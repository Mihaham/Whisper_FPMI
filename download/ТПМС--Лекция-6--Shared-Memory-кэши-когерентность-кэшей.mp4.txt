[00:00.000 --> 00:16.680]  Ну что, поехали? Последние три лекции мы с вами говорили про то, как писать код, который запускает
[00:16.680 --> 00:22.160]  потоки, который в потоках что-то делает, который конструирует сами потоки. В общем,
[00:22.160 --> 00:28.080]  мы говорили про пространство пользователей, про то, как поверх процессора описывать какие-то
[00:28.080 --> 00:33.520]  вычисления конкурентные. На два ближайших занятия жизнь у нас серьезно поменяется,
[00:33.520 --> 00:38.800]  потому что мы перестанем говорить о том, как писать код. Ну ладно, это неправда,
[00:38.800 --> 00:43.880]  будем говорить сегодня, как писать код. Но в первую очередь у нас сегодня и на следующей
[00:43.880 --> 00:49.080]  неделе задача будет другой. Мы поговорим с вами о том, что мы принимали с вами с самого начала
[00:49.080 --> 00:55.320]  курса как данность, а именно понятие разделяемой памяти. Ну да, мы себе представляем процессор,
[00:55.320 --> 01:00.640]  представляем, наверное, даже ядра себе. Эти ядра работают с общими ячейками памяти. Ну и для нас
[01:00.640 --> 01:07.080]  такой вот планка памяти, там есть ячейки, мы их читаем и пишем. Вот я покажу сегодня и особенно
[01:07.080 --> 01:11.360]  через неделю о том, что вот такого представления, конечно же, недостаточно, чтобы делать что-то
[01:11.360 --> 01:19.680]  полезное эффективно. Ну и давайте мы вспомним, с чего мы когда-то начали говорить про вообще
[01:19.680 --> 01:27.240]  многопроцессорные системы, про разделяемую память, мы вспомнили, поговорили про закон Мура,
[01:27.240 --> 01:32.920]  который говорит, что там каждые два года примерно количество транзисторов в процессоре удваивается.
[01:32.920 --> 01:39.120]  Почему? Потому что они становятся меньше, это продолжается до сих пор, и раньше они становились
[01:39.120 --> 01:45.280]  быстрее еще вместе с этим, они быстрее переключались. Но в какой-то момент эта история закончилась,
[01:45.280 --> 01:49.600]  то есть процесс транзисторов все больше и больше, но быстрее переключаться они не стали,
[01:49.600 --> 01:57.880]  тактовая частота, рост тактовой частоты остановился, и это толкнуло людей к тому,
[01:57.880 --> 02:05.880]  чтобы строить многоядерные процессоры, которые вычисление производит параллельно. То есть мы не
[02:05.880 --> 02:09.960]  можем в единицу времени больше их проводить, ну в смысле на одном вычислительном ядре, на одном
[02:09.960 --> 02:15.000]  юнити вычислительном, но мы можем реплицировать этот юнит и теперь делать там четыре раза больше
[02:15.000 --> 02:21.480]  вычислений параллельно, если, конечно, они при определенных условиях четыре раза быстрее.
[02:21.480 --> 02:29.360]  Ну вот об этом мы сегодня и хотим поговорить, собственно. Так вот, с одной стороны, то, что вот
[02:29.360 --> 02:35.200]  закон Мура выполняется, но только для количества транзисторов, но уже не для частоты, это вроде бы
[02:35.200 --> 02:40.320]  печальный факт. Ну то есть, как у вас там почти 20 лет назад были процессоры с частотой в несколько
[02:40.320 --> 02:46.800]  гигагерц, так и сейчас они остаются и быстрее не становятся. Но должно ли нас это очень сильно
[02:46.800 --> 02:54.920]  печалить? Ну вас это печалит, например? Очень печалит. А вот не должно на самом деле, потому что
[02:54.920 --> 03:00.320]  проблема не в ядрах и не в процессорах, проблема в том, что память медленная. Вот обращение к
[03:00.320 --> 03:06.600]  памяти процессора стоит, ну, примерно в сто раз дороже, чем манипуляция с регистрами. То есть,
[03:06.600 --> 03:10.400]  вы, конечно, можете ускорить процессор, но узкое место сейчас в компьютере – это не процессор для
[03:10.400 --> 03:17.680]  вас, это хранилище, но в частности память. Этот разговор можно как бы и дальше распространить там
[03:17.680 --> 03:26.000]  на внешнее хранение на диске, но вот память уже в сто раз медленнее, чем процессор. Поэтому хочется
[03:26.000 --> 03:31.320]  в этом месте что-то ускорить, чтобы программы становились быстрее. Ну и в таком случае,
[03:31.320 --> 03:36.760]  когда у нас есть очень медленные хранилища и очень быстрые клиенты, условно, есть для такой задачи
[03:36.760 --> 03:42.880]  общее решение, которое называется кэширование. Ну, тут картинка не про память совсем, конечно,
[03:42.880 --> 03:47.840]  не про процессор, а про какое-то внешнее большое хранилище, какую-то холодную базу данных,
[03:47.840 --> 03:52.720]  которая хранит все данные на диске, и к ней обращаются клиенты. Обращаются, видимо, часто. И
[03:53.240 --> 04:00.920]  мы хотели бы, чтобы этих клиентов быстро обслуживали. Для этого мы между клиентами и большим хранилищем
[04:00.920 --> 04:05.280]  помещаем компонент, который называется кэш. Ребят, потише немножко, вы меня сбиваете.
[04:05.280 --> 04:12.160]  Помещаем кэш. Кэш гораздо меньше по объему, чем хранилища, разумеется. В него все данные не
[04:12.160 --> 04:20.600]  могут поместиться, но он старается обслуживать чтения, которые выполняют клиенты, которые
[04:20.600 --> 04:25.760]  обращают клиента к хранилищу. Значит, клиент сначала идет за данными в кэш. Если он там
[04:25.760 --> 04:31.040]  находит ответ, в ситуации называется кэш хит, попадание по кэшу, то в хранилищ медленный мы
[04:31.040 --> 04:37.160]  не ходим. Если же мы в кэше не находим ничего, то мы, так уж и быть, идем в медленное хранилище,
[04:37.160 --> 04:47.240]  далеко. Получаем от него ответ и поселяем этот ответ в кэше, чтобы в следующем обращении мы
[04:47.240 --> 04:53.320]  попали в этот кэш, и ответ был быстрее. Эта ситуация называется промах по кэшу, кэш мисс.
[04:53.320 --> 04:59.840]  Конструкция понятная? Вот, конечно, очень много всего неизвестного, что это за хранилищ,
[04:59.840 --> 05:06.800]  какой у него интерфейс, как именно работает кэш, какой там механизм вытеснения, но нам сейчас это
[05:06.800 --> 05:12.320]  не очень важно и вообще сегодня не очень важно. И давайте лучше перейдем к конкретному примеру,
[05:12.400 --> 05:18.240]  а именно к нашему примеру, к процессору и памяти. У нас в качестве медленного хранилища есть
[05:18.240 --> 05:25.200]  оперативная память. Там лежит какая-то ячейка, какая-то переменная X, и мы собираемся из нее
[05:25.200 --> 05:34.600]  прочесть значение в регистр, для того чтобы что-то с ним сделать. Сначала мы эту ячейку ищем в кэше,
[05:34.600 --> 05:44.040]  который мы устанавливаем между ядром и памятью. Кэш гораздо меньше, чем память, кэш гораздо
[05:44.040 --> 05:53.640]  быстрее, чем память, и если ячейка находится в кэше, то мы в память вообще не входим в этой
[05:53.640 --> 06:00.160]  инструкции. То есть, если случается кэш хит, то это положительный сценарий, кэш мисс. Если кэш мисс,
[06:00.160 --> 06:07.680]  то мы проваливаемся в память, идем и читаем данные оттуда. Но что важно, мы читаем оттуда не одну
[06:07.680 --> 06:13.280]  ячейку памяти, это было бы очень неэффективно. Мы оттуда читаем целый блок памяти, который мы
[06:13.280 --> 06:21.920]  назовем линией. И эта линия в типичном современном процессоре составляет 64 байта, 8 машинных слов.
[06:21.920 --> 06:30.000]  Вот когда вы читаете что-то из памяти, вы читаете не одно машинное слово, а сразу 8. И эти 8 машинных
[06:30.000 --> 06:38.960]  слов эту линию помещаете в кэш. Ну вот такая запись в кэше называется кэш-линей. Ну а дальше уже вы
[06:38.960 --> 06:47.160]  оттуда берете данные. Конструкция понятна, ну пока терминология, которая возникает, понятна. Если
[06:47.160 --> 06:54.520]  понятно, то сразу нужно уточнить, что кэш между ядром и памятью разумеется не один, их целая
[06:54.520 --> 07:01.320]  иерархия. У нас есть кэш первого уровня, кэш второго уровня, кэш третьего уровня, и каждый последующий
[07:01.320 --> 07:07.960]  кэш больше, чем предыдущий, дальше от ядра, чем предыдущий, медленнее, чем предыдущий, дешевле,
[07:07.960 --> 07:19.160]  чем предыдущий. Ну и вообще поддерживается такое свойство, что если у тебя есть, если кэш-линия
[07:19.160 --> 07:27.960]  находится в кэше более младшем, то она есть и во всех старших кэша. Ну типичные размеры тут
[07:27.960 --> 07:36.040]  указаны. Для кэша первого уровня это 1632 килобайта, для кэша второго уровня это там 256 килобайт,
[07:36.040 --> 07:47.120]  512, 2 мегабайта, кэш третьего уровня 8 мегабайт. А размер памяти, ну не знаю, 100 гигабайт. Довольно
[07:47.120 --> 07:53.880]  серьезный зазор, да? Вот. Но тем не менее утверждается, что даже такие крошечные кэши,
[07:53.880 --> 08:01.120]  выстроенные перед памятью, существенно ускоряют исполнение вашей программы. Да, еще маленькое
[08:01.120 --> 08:05.080]  замечание, но не знаю, насколько полезно нам сегодня, что кэш первого уровня, как правило,
[08:05.080 --> 08:11.120]  разделяется на кэш для инструкции и кэш для данных. Для инструкции тоже нужен кэш,
[08:11.120 --> 08:17.560]  нужен кэш, чтобы программа быстрее исполнялась, чтобы просто быстрее вычитывать ее код. Так вот,
[08:17.560 --> 08:23.760]  прежде чем объяснить, почему такие маленькие кэши оказываются полезными, еще некоторые
[08:23.760 --> 08:29.600]  технические комментарии, насколько быстро они работают. Вот если мы говорим про регистры,
[08:29.600 --> 08:35.880]  то обращение, если мы говорим про кэш первого уровня, а что я, собственно, говорю вам? Давайте
[08:35.880 --> 08:43.200]  я вам покажу лучше хороший референс. Это такое известное место в интернете из одного старого
[08:43.200 --> 08:49.040]  доклада, и если вы гуглите, то гуглите вот latency numbers every programmer should know. Так оно и есть. Вы
[08:49.040 --> 08:53.720]  все должны знать эти числа, ну по крайней мере порядок этих чисел. Тут говорится, что вот L1 кэш,
[08:53.720 --> 08:59.800]  референс в него стоит 0,5 на на секунду, ну то есть вы можете миллиарды раз обращаться к L1 кэшу
[08:59.800 --> 09:07.960]  в секунду. Ну вот более-менее так же быстро, как вы молотите инструкции. L2 кэш, ну вот он уже
[09:07.960 --> 09:14.920]  медленнее в 10 раз. Референс обращения к памяти 100 на на секунду, то есть 100-200 раз медленнее,
[09:14.920 --> 09:23.280]  чем обращение к L1 кэшу. Ну если говорить про диск, то с диском тут совсем все печально,
[09:23.280 --> 09:29.880]  потому что обращение к диску, ну если у вас диск вращающийся магнитный, то там пока он
[09:29.880 --> 09:35.400]  повернется, пока рука с головкой считывающей станет на нужную дорожку, это занимает 5-10 миллисекунд
[09:35.400 --> 09:43.520]  миллисекунд. А у нас были на на секунды. Это вот, ну сколько, шесть порядков разница, да? В миллион
[09:43.520 --> 09:51.240]  раз медленнее. Но вот эти числа, они из доклада 12 года, прошло уже 10 лет, но не то чтобы кэши и
[09:51.240 --> 09:55.280]  память драматически ускорились за это время, нет, не ускорились, но все-таки числа могут меняться,
[09:55.280 --> 10:02.680]  особенно там SSD, тут наверное устаревшие данные. Но можно даже эти числа не помнить, потому что
[10:02.680 --> 10:08.800]  можно помнить скорее их отношения. И для этого есть довольно удачная аналогия, которая называется
[10:08.800 --> 10:16.160]  пивная иерархия. Смысл такой, что, ну когда вот вы пьете пиво уже из бутылки, лимонад, я пью
[10:16.160 --> 10:22.040]  лимонад вместо пива, то это вот обращение к регистру. Когда у вас вот бутылка стоит рядом с вами,
[10:22.040 --> 10:28.960]  это L1 кэш, вы протягиваете руку, берете ее. L2 кэш, когда у вас стоит холодильник рядом с креслом,
[10:28.960 --> 10:37.080]  вам нужно его открыть и достать. L3 кэш, когда вам нужно пойти к холодильнику, на кухню. Основная
[10:37.080 --> 10:44.360]  память, это примерно как спуститься вниз к магазину у подъезда. А если говорить про обращение к диску,
[10:44.360 --> 10:48.920]  если вдруг, не знаю, вашу страницу памяти вытеснили на диск, потому что вот физическая память
[10:48.920 --> 10:55.200]  закончилась, то это примерно как за лимонадом отправляться в Австралию на яхте. Вот это очень
[10:55.200 --> 11:00.640]  долго. Ну, то есть, если вы так себе это все представите, то получите, ну, будет понятно,
[11:00.640 --> 11:09.400]  с какой скоростью все работает, даже если вы не помните отдельные числа. Ну вот, значит, кэши,
[11:09.400 --> 11:14.920]  они довольно близко по скорости к процессору, но они замедляются, конечно, по мере удаления от
[11:14.920 --> 11:21.640]  процессора, но все же существенно быстрее, чем память. Но они очень маленькие по сравнению с
[11:21.640 --> 11:29.440]  памятью. И возникает вопрос, а почему же вообще они помогают? Почему же они что-то ускоряют? Ответ
[11:29.440 --> 11:36.120]  следующий. Дело в том, что программы, которые вы пишете, они обращаются с памятью не совсем
[11:36.120 --> 11:42.720]  произвольно. В них есть некоторые паттерны. А именно, программы соблюдают, как правило,
[11:42.720 --> 11:49.480]  программы ваши соблюдают два типа локальности. Ну, один из двух типов локальности. Первая – это
[11:49.480 --> 11:54.880]  пространственная локальность, вторая – это временная локальность. Пространственная локальность,
[11:54.880 --> 11:59.040]  она говорит о том, что если ваша программа обращается к ячейке памяти, к какому-то
[11:59.040 --> 12:05.000]  машинному слову, то, вероятно, она в ближайшем будущем обратится к какому-то соседнему
[12:05.000 --> 12:10.320]  машинному слову. Но это совсем простой сценарий, совсем такой очевидный, естественный. Вот вы,
[12:10.320 --> 12:16.320]  не знаю, пишете Quick Sort или Merge Sort, а вы там ходите по массивам справа-налево, слева-направо. И тут
[12:16.320 --> 12:22.960]  довольно кстати приходится то, что вы сохраняете в кэше не одну ячейку памяти, а целую кэшлинию.
[12:22.960 --> 12:30.000]  Вот вы прочитали очередной элемент массива, а дальше следующие восемь вы читаете бесплатно,
[12:30.000 --> 12:36.120]  практически. То есть паттерн очень естественный. Это пространственная локальность. И есть
[12:36.120 --> 12:40.160]  временная локальность. Временная локальность говорит о том, что если вы обратились к ячейке
[12:40.160 --> 12:47.080]  памяти, то, скорее всего, в ближайшем будущем вы обратитесь к ней же еще раз. Но опять,
[12:47.080 --> 12:52.840]  представьте себе, что у вас какое-нибудь дерево поиска, и чтобы вы там не искали,
[12:52.840 --> 12:58.720]  как правило, вы проходите через какую-то верхушку этого дерева чаще всего. Вот эта верхушка дерева
[12:58.720 --> 13:06.720]  закашируется. Ну, может быть, для деревьев поиск, где у вас R-2, это не слишком крутая оптимизация. Но
[13:06.720 --> 13:13.080]  когда, скажем, мы работаем с там B-деревьями или с деревьями вообще во внешней памяти, то это
[13:13.080 --> 13:18.360]  становится уже очень важно. Ну вот, два типа локальности. Понятно, да? Понятно сценарий.
[13:18.360 --> 13:28.200]  Зачем нам об этом знать? Ну, потому что пока мы не говорим про конкаренность и про многопоточность
[13:28.200 --> 13:32.840]  вообще, а пока мы говорим просто про какую-то оптимизацию, которая есть в процессоре. Но в
[13:32.840 --> 13:41.000]  любом случае она полезна вам, потому что вам разумно ее учитывать, когда вы пишете свой код. Вам
[13:41.000 --> 13:48.400]  разумно с помощью знания о кышах, об иерархии памяти уточнить, детализировать свою модель
[13:48.400 --> 13:54.720]  стоимости, которую вы используете при программировании. Ну вот, чему вас учат на алгоритмах? Что у вас есть,
[13:54.720 --> 14:01.160]  что у вас единица стоимости, это, грубо говоря, обращение к ячейке памяти. Ну, что-то подобное. Но это
[14:01.160 --> 14:05.040]  же воронье, потому что если вы обращаетесь к соседним ячейкам памяти, это одна история. А к далеким
[14:05.040 --> 14:14.600]  ячейкам памяти совсем другая история. Это стоит сто раз дороже. Вот разумно учитывать локальность
[14:14.600 --> 14:19.080]  в моделистой масте, когда вы оцениваете эффективность своего алгоритма. Ну, скажем,
[14:19.080 --> 14:26.240]  вы приходите на собеседование после учебы и вас спрашивают, ну, там, что быстрее, итерация по
[14:26.240 --> 14:32.800]  вектору или итерация по эстаделист? И вы отвечаете, ну, разумеется, итерация по вектору, потому что вы
[14:32.800 --> 14:39.800]  сканируете там кэшлини одно за другой. А что происходит, когда вы итерируетесь по списку? Вы
[14:39.800 --> 14:44.680]  каждый раз прыгаете по какому-то неизвестному адресу, какую-то неизвестную память и, скорее всего,
[14:44.680 --> 14:50.880]  по кэшу промахиваетесь. И вот вы вместо того, чтобы промахиваться по кэшу раз в восемь обращений,
[14:50.880 --> 14:58.400]  вы промахиваетесь на каждом обращении. Кажется, это не очень эффективно. Ну, или не знаю. Вы
[14:58.400 --> 15:04.720]  увлекаетесь машинным обучением и вот любите умножать матрицу. Ну, что вы знаете про умножение
[15:04.720 --> 15:09.760]  матрицы? Что его можно как-то оптимизировать? Какие-то рекурсивные сложные алгоритмы? Я предлагаю
[15:09.760 --> 15:18.520]  ничего не оптимизировать. Я предлагаю написать тупой код. Надеюсь, я не ошибся в умножении
[15:18.520 --> 15:26.960]  матрицы. Ну вот, просто двойной цикл. В нем еще один цикл. Считаем каждый римен. Запускаем. Вот
[15:26.960 --> 15:32.560]  матрица размера тысяча. Некоторые генерируются случайным образом. Тут не очень интересно. Запускаем
[15:32.560 --> 15:46.080]  и перемножили за одну секунду. А как это ускорить, если мы знаем про генериантность кэшей? Ну вот у нас
[15:46.080 --> 15:54.560]  по первой матрице мы ходим построчно. И это разумно. А по второй матрице мы ходим по столбцам. И вот
[15:54.560 --> 16:02.760]  каждый новый индекс K – это же прыжок в памяти, и это промах по кэшу. Вот мы давайте транспонируем
[16:02.760 --> 16:17.520]  матрицу сначала B, а потом перемножим две матрицы построчно. И что у нас получится? Стало в пять раз
[16:17.520 --> 16:24.320]  быстрее. Довольно приятно. Вот. И дело тут совсем не в алгоритмических каких-то оптимизациях. Это
[16:24.320 --> 16:28.320]  нам даром не нужно. Вот мы просто транспонировав матрицу, получили производительность в пять раз
[16:28.320 --> 16:34.840]  быстрее для однопоточного кода. Тут еще, конечно, огромное пространство для оптимизации, но вот уже
[16:34.840 --> 16:43.240]  такое базовое знание про кэши дает нам возможность программу ускорять. Вот. Есть очень красивые
[16:43.240 --> 16:47.600]  способы. Это, конечно, совсем не относится к нашему курсу. Это, скорее, такое балавство алгоритма.
[16:47.600 --> 16:54.480]  Вот скажем, представьте себе бинарный поиск на каком-то огромном массиве. Вот как можно
[16:54.480 --> 17:02.880]  оценить этот алгоритм с точки зрения промахов по кэшу? Насколько он эффективен? Но он очень плох. Мы
[17:02.880 --> 17:08.080]  берем середину, потом четверть, потом… Вот мы каждый раз прыгаем далеко. В какой-то момент, конечно,
[17:08.080 --> 17:12.960]  мы вместимся в одну кэшлинию, но там уже можно и линейный поиск запустить, уже неважно. Так вот,
[17:12.960 --> 17:19.640]  оказывается, что люди этим озаботились и придумывают, как можно уложить данные для бинарного
[17:19.640 --> 17:26.880]  поиска в памяти так, чтобы промахов по кэшу было как можно меньше, ну, просто минимальное
[17:26.880 --> 17:32.480]  возможное количество. Причем для этого даже не нужно знать размер кэшлинии. Это такая очень
[17:32.480 --> 17:40.280]  красивая рекурсивная укладка ВНМД БОСа. И если, скажем, вы пойдете в шат учиться, что довольно
[17:40.280 --> 17:44.960]  разумно, то Максим Бабенко вам там расскажет на курсе по алгоритмам о внешней памяти к
[17:44.960 --> 17:49.240]  такие штуки сделать. Ну, то есть, вы изучаете одни алгоритмы, а потом можно детализировать
[17:49.240 --> 17:54.560]  модель памяти, детализировать модель стоимости, модель вашего компьютера и придумывать новые
[17:54.560 --> 18:00.800]  алгоритмы, которые учитывают уже больше нюансов, которые больше параметров. И получать более
[18:00.800 --> 18:12.120]  разумное приближение к вот настоящей железке. Ну что, есть ли вопросы пока? Если нет, то нужно
[18:12.120 --> 18:19.400]  поговорить теперь о том, зачем вообще мы говорим про кэши в нашем курсе, потому что пока, ну, это
[18:19.400 --> 18:26.440]  все полезно, конечно, знать, но пока не относится к конкаранции, к параллельности, вообще к
[18:26.440 --> 18:35.320]  многоядерности. Ну, дело в том, что вот эта картинка, вот эта и вот эта картинка, она
[18:35.320 --> 18:41.360]  усложняется, когда мы начинаем говорить про многоядерные процессоры, потому что оказывается,
[18:41.360 --> 18:48.920]  что у каждого ядра свой собственный L1 кэш, ну, или L2 кэш тоже может быть общий, и они
[18:48.920 --> 18:55.200]  соединены вот в такой интерконнект, через который вся эта конструкция общается с памятью. То есть,
[18:55.200 --> 19:02.640]  смотрите, у каждого ядра есть собственный кэш, и дальше эти кэши связываются вот в одну систему
[19:02.640 --> 19:12.480]  вместе с памятью. Почему это добавляет нам сильно сложности? Ну, посмотрите, в чем дело. Вот раньше
[19:12.480 --> 19:17.040]  вы жили в мире, где у вас была память, планка памяти, там были ячейки, которые вы читали и
[19:17.040 --> 19:24.240]  писали. А теперь каждая ячейка может быть в памяти, а еще может быть в кэше. Ну, то есть,
[19:24.240 --> 19:30.920]  у вас теперь логическая ячейка, одна единственная, имеет потенциально несколько физических
[19:30.920 --> 19:38.080]  воплощений. Одно непосредственно в памяти, а другие в кэшах. То есть, у вас появляются копии ячейки,
[19:38.080 --> 19:43.440]  у вас теперь нет одной ячейки, у вас теперь много копий одного и того же. И эти копии, хуже того,
[19:43.440 --> 19:50.440]  могут расходиться друг с другом, они могут показывать разные значения. Ну, смотрите,
[19:50.440 --> 19:57.040]  как это выглядит. Вот пример ячейки памяти. У нас есть большой процессор, там 16 ядер. И мы
[19:57.040 --> 20:06.840]  делаем следующее. Мы берем ячейку памяти и на каждом ядре пишем туда свой номер. Вот третье
[20:06.840 --> 20:14.600]  ядро пишет в ячейку значение 3, а потом читает то, что написано в ячейке. И вот, что читают разные
[20:14.600 --> 20:21.040]  ядра. Ячейка одна. Ну, то есть, ячейка одна, а значение у нее много одновременно, у нее много
[20:21.040 --> 20:27.080]  значений при этом. Выглядит довольно странно, да? Вот, довольно сложно жить в таком мире,
[20:27.080 --> 20:32.240]  где у ячейки много одновременно значений. На самом деле, эта картинка иллюстрирует чуть больше,
[20:32.240 --> 20:36.880]  чем я говорю, потому что здесь есть некоторая структура заметная. Тут какие-то пары соседних
[20:36.880 --> 20:42.240]  ядер похожи почему-то. У этих всех историй наблюдения за ячейкой есть какая-то общая
[20:42.240 --> 20:46.920]  структура. Ну, про это мы поговорим через неделю. Это уже история больше, наверное, про модели
[20:46.920 --> 20:56.520]  памяти. Но, тем не менее, о чем я говорю? О том, что кэши в процессоре и память образуют... Ну,
[20:56.520 --> 21:03.080]  то есть, о чем я говорю? Что разделяемая память, которую мы принимаем за данность, это на самом
[21:03.080 --> 21:08.400]  деле абстракция. В компьютере никакой разделяемой памяти одной и вот единственной экономической нет.
[21:08.400 --> 21:15.600]  Вместо нее есть распределенная система, где есть ядра с отдельными кэшами и общая память.
[21:15.600 --> 21:27.000]  И, ну, видимо, содержимое этих кэшей и памяти должно как-то синхронизироваться. И вот это история
[21:27.000 --> 21:31.400]  про распределенные системы. А распределенные системы это очень сложно, оказывается. Ну,
[21:31.400 --> 21:36.880]  я про это читаю в отдельных спецкурсах, я там рассказываю очень долго про то, что очень сложно
[21:36.880 --> 21:41.360]  научить вот три компьютера в сети, которые хранят одни и те же данные, вести себя вот как
[21:41.360 --> 21:47.920]  единое целое. Вот вести себя так, чтобы пользователь снаружи не понял, что это три копии одного и того
[21:47.920 --> 21:57.680]  же, потому что копии расходятся, машины перезагружаются или отказывают. Это сложная история, но, по счастью,
[21:57.680 --> 22:03.680]  в процессоре решение все же становится проще, потому что, в отличие от распределенных систем,
[22:03.680 --> 22:09.880]  где машины могут умирать незаметно для других, все-таки в процессоре отдельные ядра сами по
[22:09.880 --> 22:18.400]  себе не отказывают. Это немного упрощает дело, поэтому все же процессору удается поддерживать
[22:18.400 --> 22:25.240]  некоторые простые варианты относительно копий одних и тех же данных в разных кэшах. Итак,
[22:25.240 --> 22:29.440]  проблема понятна, про которую я рассказываю, что у нас есть несколько копий и ядра с ними
[22:29.440 --> 22:34.840]  работают вроде бы независимо, но их нужно как-то синхронизировать между собой. Ну вот,
[22:34.840 --> 22:41.320]  эта задача называется протоколка гириантности. Как синхронизировать содержимое одних и тех же
[22:41.320 --> 22:50.200]  линий в разных ядрах с памятью. И этот протокол к гириантности поддерживает для вас очень
[22:50.200 --> 22:59.920]  простые варианты. Они выглядят так. Смотрите, если у вас какая-то линия памяти находится
[22:59.920 --> 23:09.960]  сразу в нескольких кэшах, то протокол к гириантности гарантирует вам, что все эти копии этой линии в
[23:09.960 --> 23:21.000]  разных кэшах одинаковы и совпадают с памятью. Гарантия понятна? Если линия есть в двух кэшах,
[23:21.000 --> 23:25.200]  как минимум, то гарантируется, что значение ее актуальной совпадает с памятью. Ну и все эти
[23:25.200 --> 23:31.680]  копии одинаковы. Вот мы в этом случае будем говорить, что кэш-линия в этих кэшах находится
[23:31.680 --> 23:41.000]  в состоянии Sherrod. Вот у каждой линии в каждом кэше будет отдельное состояние. Состояния будет
[23:41.000 --> 23:46.040]  четыре. Вот поэтому протокол к гириантности называется Мессия. Вот мы сейчас говорим про
[23:46.040 --> 23:58.840]  состояние S-Sherrod. Первая гарантия. Вторая гарантия. Следующая. Если кэш-линия была изменена текущим
[23:58.840 --> 24:07.760]  ядром, но изменения еще не попали в память, а разумеется, когда процессор, я не сказал об этом,
[24:07.760 --> 24:15.360]  пишет в кэш, вообще пишет в память что-то, то он пишет сначала в кэш. Он не пишет сразу в память.
[24:15.360 --> 24:23.880]  Он пишет в кэш, а потом эвенчивали данные из кэша, сбрасываются в память. Так вот, если какой-то
[24:23.880 --> 24:32.120]  процессор, какое-то ядро записало свежие данные в какую-то кэш-линию, но содержимое этой кэш-линии
[24:32.120 --> 24:43.680]  еще не уехало в память, то гарантируется, что в других кэшах копии этой линии нет. То есть эта
[24:43.680 --> 24:49.080]  линия, если у нее состояние грязное, как говорят, то она находится только в одном кэше. То есть ядро
[24:49.080 --> 24:58.480]  владеет этой линией эксклюзивно. Вот если эксклюзивное владение обозначается буквой M,
[24:58.480 --> 25:06.840]  состояние кэш-линии, когда она отсутствует в кэше, обозначается буквой I, но invalid. То есть отсутствует
[25:06.840 --> 25:18.360]  или нет одно и то же для кэша. Вообще, кстати, не поговорил, извините, я вернусь в прошлое. Секунду.
[25:18.360 --> 25:29.680]  Потерялась картинка. Что такое вообще кэш? Я как-то упустил момент. Это на самом деле железная
[25:29.680 --> 25:39.520]  хэштаблица. Она состоит из какого-то фиксированного числа бакетов с фиксированной емкостью. То есть это
[25:39.520 --> 25:45.680]  аппаратная хэштаблица, которая реализована прямо в процессоре. Вот так же, как, не знаю,
[25:45.680 --> 25:50.560]  page table – это аппаратное префиксное дерево, которое тоже реализовано прямо на схеме процесса.
[25:50.560 --> 26:02.280]  Вот. Так что кэш-линия, она вот... кэш-линия может находиться, вообще говоря, в разных... линия
[26:02.280 --> 26:07.320]  памяти может находиться в разных строчках кэша, но гарантируется, что если она грязная, то она
[26:07.320 --> 26:14.640]  находится только... только в одном кэше. Ну и есть некоторое специальное состояние,
[26:14.640 --> 26:26.120]  которое называется exclusive, которое вроде бы про то, что кэш-линия совпадает с памятью,
[26:26.120 --> 26:34.240]  как и в случае shared. Но при этом состояние exclusive означает, что кэш-линия находится
[26:34.240 --> 26:40.040]  гарантированно только в одном кэше. В других кэшах этой линии нет. Вообще говоря,
[26:40.040 --> 26:45.240]  и shared не требует, чтобы кэш-линия была одновременно в двух кэшах. Может быть,
[26:45.240 --> 26:51.400]  в одном кэше... ну, две кэш-линии были в состоянии shared в двух кэшах, а потом одну кэш-линию вытеснили,
[26:51.400 --> 26:55.640]  потому что, ну, какая-то запись... какой-то записи потребовалось места, и кэш-линия выпала из кэша.
[26:55.640 --> 27:03.480]  Но вот состояние shared не понимает, что линия находится только в одном кэше, а exclusive
[27:03.480 --> 27:09.960]  понимает. Картинка неправильности должна быть invalid, конечно. Простите, небольшой баг. То есть,
[27:09.960 --> 27:16.440]  если состояние exclusive, то оно эксклюзивное, взаимно исключающее. Modify – это то же самое,
[27:16.440 --> 27:29.880]  взаимно исключающее. В других кэшах значений быть не может. Идея понятна? Тогда я хочу вам показать,
[27:29.880 --> 27:59.440]  как это будет выглядеть. Ну вот, такой симулятор протокола когериантности.
[27:59.560 --> 28:09.000]  У нас есть память из четырех ячеек, у нас есть четыре... три, почему-то. У нас и раньше было
[28:09.000 --> 28:18.880]  три, это странно. Три ядра. У каждого из них есть кэш из двух кэш-линий. Ассоциативность кэша – один.
[28:18.880 --> 28:27.560]  То есть, ячейки 0 и 2 попадают в первую линию, 1 и 3 попадают во вторую линию, ну, в нулевую,
[28:27.560 --> 28:34.760]  в первую, соответственно. Ну, давайте что-нибудь прочитаю. Вот я читаю данные,
[28:34.760 --> 28:43.760]  мне отвечает память, и вот у меня данные в кэше в состоянии exclusive. Вот я, ядро, знаю, что других
[28:43.760 --> 28:52.560]  ядер этого состояния нет. Ну, а дальше вопрос. А как вообще, собственно, эти состояния поддерживаются,
[28:52.560 --> 29:00.720]  вот эти гарантии, про которые я говорю вот здесь? Как гарантируются вот эти варианты в кэше?
[29:00.720 --> 29:07.640]  Они гарантируются с помощью коммуникации. Можно, прежде чем я к ней перейду, я скажу,
[29:07.640 --> 29:16.080]  что вот состояния shared и exclusive, они же на самом деле очень, очень естественные, потому что, ну,
[29:16.080 --> 29:21.880]  если вы что-то знаете про Mutex, то, возможно, вы знаете про штуку, которая называется «где же она»,
[29:21.880 --> 29:37.760]  shared Mutex. Слышали про него? Какие у него любопытные. Удивительно. Shared Mutex это, ну, или RV Mutex,
[29:37.760 --> 29:44.880]  или Reader-Writer Mutex. Он про то, чтобы немного обобщить интерфейс Mutex и к методам log и
[29:44.880 --> 29:51.040]  unlock добавить методы log shared и unlock shared. Ну, идея такая. У вас есть некоторое состояние и,
[29:51.040 --> 29:57.360]  может быть, его хотят менять. Для этого у вас есть log-unlock для критических секций. Вот какой-то
[29:57.360 --> 30:04.000]  поток хочет получить эксклюзивный доступ к данным. А может быть, у вас сразу несколько потоков
[30:04.000 --> 30:11.040]  хотят читать данные. Вот они могли бы делать это параллельно. Вот читать и одновременно писать
[30:11.040 --> 30:17.640]  нельзя. Это data race, это undefined behavior стандарта C++. А вот параллельно читать можно. Поэтому
[30:17.640 --> 30:23.200]  мы к log и unlock добавляем log shared и unlock shared. И гарантия следующая, что между log и unlock может
[30:23.200 --> 30:28.160]  находиться только один поток. А между log shared и unlock shared могут находиться несколько поток
[30:28.160 --> 30:37.040]  разом. Но секции log-unlock и lock shared-unlock shared не пересекаются. Ну, то есть, потоки, которые
[30:37.040 --> 30:42.200]  пишут данные, берут эксклюзивную блокировку. Потоки, которые читают данные, берут разделяемую
[30:42.200 --> 30:53.480]  блокировку. И, по сути, состояние протоколок агерентности, вот эти вот M и S, это вот не что иное,
[30:53.480 --> 30:58.440]  как, ну, и весь этот протокол к агерентности, это не что иное, как такой вот shared butex,
[30:58.440 --> 31:09.240]  который реализован вот аппаратно прямо в процессоре. Нет, не так, разумеется. Иначе бы,
[31:09.240 --> 31:14.840]  ну, все-таки, когда мы читаем в память, читаем из памяти и пишем в память, не то, что мы берем
[31:14.840 --> 31:21.720]  блокировку, нет. Происходит не так. Мы поддерживаем, скорее про состояние. Что значит состояние? Либо у тебя
[31:21.720 --> 31:25.640]  много читателей, либо один писатель. Вот о чем я говорю. Не о том, что Mutex берется, разумеется.
[31:25.640 --> 31:31.160]  Там Mutex ни какого нет. Повтори тогда его.
[31:40.160 --> 31:47.920]  Да. Ну, и в обратную сторону тоже. И тут возникает вопрос, у кого приоритет, но это out of scope
[31:47.920 --> 31:55.040]  сегодня. Так вот, как же поддерживаются вот эти варианты протоколок агерентности про S и
[31:55.040 --> 32:02.760]  M? Они поддерживаются с помощью коммуникации. Вот скажем, у нас ячейка памяти в состоянии E,
[32:02.760 --> 32:10.480]  и ее хочет прочесть другое ядро. В каком состоянии ячейки должны оказаться, линии в кэшах должны
[32:10.480 --> 32:17.560]  оказаться после чтения? Ну, видимо, у нас будут две копии одной и той же линии. Значит, они должны
[32:17.560 --> 32:21.480]  быть в состоянии shared. Ну, а значит, когда мы читаем, нужно поговорить с другими кэшами,
[32:21.480 --> 32:30.920]  и вот мы отправили сообщение на шину, и это сообщение пришло этому кэшу, и он поменялся
[32:30.920 --> 32:36.880]  в состоянии с E на S. Раньше он знал, что только он владеет копией, а теперь он знает, что,
[32:36.880 --> 32:50.080]  возможно, есть другие. Впрочем, смотрите, что могло произойти. Я прочел ячейку A2,
[32:50.440 --> 32:57.880]  но они отобразились в одну и ту же кэшлинию, но просто в силу ассоциативности кэша, и вот эта
[32:57.880 --> 33:06.280]  копия ячейки A0 вытеснилась из кэша, но здесь S так и осталось. То есть S не означает, что кэшлини
[33:06.280 --> 33:11.200]  находятся, по крайней мере, в двух кэшах, она означает, что кэш не уверен, что кэшлини находятся
[33:11.200 --> 33:19.160]  только у него, но он уверен, что у него актуальное состояние. Так вот, если мы будем читать дальше
[33:19.160 --> 33:25.080]  ячейку памяти A0, то мы это будем делать без коммуникации, потому что мы знаем, что состояние
[33:25.080 --> 33:31.920]  shared означает, что ячейка памяти имеет актуальное состояние, что она совпадает с памятью. Вот,
[33:31.920 --> 33:39.440]  а теперь предположим, что мы хотим записать что-то в ячейку A0, но мы должны перейти после этого в
[33:39.440 --> 33:46.320]  состояние modified. У нас будет грязная копия, у других не должно быть копий. Опять, это коммуникация.
[33:46.320 --> 33:58.320]  Мы должны отправить другим кэшам сообщение, чтобы они инвалидировали копию у себя. Тут, правда,
[33:58.320 --> 34:02.480]  не modified получилось, тут сразу эксклюзив, потому что мы еще в память сбросили. Но,
[34:02.480 --> 34:06.560]  может быть, если перезаписать, ну вот да, перезаписали, получили modified.
[34:06.560 --> 34:24.800]  Так, чтение и завершает состоянием shared. Да, вот, смотри, теперь состояние modified,
[34:24.800 --> 34:33.480]  а что будет, если другое ядро прочтет ячейку 0? Вот, мы должны сказать вот этому кэшу, чтобы он
[34:33.480 --> 34:39.320]  сбросил свое значение в память, потому что нам нужно два shared, а shared требует, чтобы память и
[34:39.320 --> 34:47.320]  кэш совпадали по значению. Так что мы отправляем на месте вот этого читателя сообщение про то,
[34:47.320 --> 34:52.520]  что вот по флаж, пожалуйста, свой кэш сбрось данные в память, перейди в состояние S, а мы
[34:52.520 --> 35:13.480]  получим копию. Что за вопрос у тебя был, я, наверное, не... Еще раз, любое чтение завершается,
[35:13.480 --> 35:22.400]  по крайней мере, shared. Но если у других кэшах, когда мы читаем, мы общаемся с другими кэшами,
[35:22.400 --> 35:29.480]  потому что они должны инверидировать у себя грязные копии или сбросить exclusive. Так что любое
[35:29.480 --> 35:34.760]  чтение завершается, по крайней мере, в состоянии shared для локального кэша. Но если окажется при
[35:34.760 --> 35:39.320]  чтении, при коммуникации с другими кэшами, с другими процессорами, что копии ни у кого не было,
[35:39.320 --> 35:46.320]  то мы попадем в состояние exclusive. Вот я закрываю пальцем у себя этот modified, здесь тоже invalid
[35:46.320 --> 35:51.720]  написано. Ну, то есть, выгодно оставаться в exclusive, потому что если мы находимся в состоянии
[35:51.720 --> 36:03.400]  exclusive, ну, давайте мы прочтем какую-нибудь А1, получим exclusive, то чем он ценен? Тем, что в
[36:03.400 --> 36:10.280]  него можно писать без коммуникации, потому что мы знаем, что копии ни у кого нет. В этом его смысл.
[36:10.280 --> 36:20.360]  Ну что, мне кажется, я и писал примерно все сценарии, если они понятны, то я бы пошел дальше. Что скажете?
[36:20.360 --> 36:38.040]  А где ткнуть? Ну, а зачем? Как бы, чего ты ожидаешь от процессора? У него грязная копия, он знает,
[36:38.040 --> 36:42.360]  поэтому, что он только у него. Но она еще не сбросилась в память, с другой стороны, пока она
[36:42.360 --> 36:49.440]  никому не нужна, поэтому он может писать в нее дальше. Но если кто-то захочет прочесть, то придется
[36:49.440 --> 37:06.000]  сначала сбросить ее в память и потом получить два шерода. Как это все примерно реализовано? Ну,
[37:06.000 --> 37:10.440]  это реализовано с помощью примитива, который называется atomic broadcast, который упорядочивает
[37:10.440 --> 37:17.840]  все события во всех кышах, для всех кишей, для всех процессоров. Вот сложность в распределенных
[37:17.840 --> 37:22.520]  системах состоит в том, что такой atomic broadcast сложно сделать. В процессоре гораздо легче,
[37:22.520 --> 37:29.600]  потому что отдельно ядра не ломаются. Ну и, да, еще одно замечание, что, конечно же, вот протокол
[37:29.600 --> 37:35.520]  Месси, это не то, чтобы вот прям так процессор работает. Это некоторая модель, которой можно
[37:35.520 --> 37:41.160]  пользоваться, когда мы рассуждаем о том, как наши ядра, наши потоки на разных ядрах работают с
[37:41.160 --> 37:48.880]  памятью. В процессоре может быть сколь угодно сложнее. Но нам не нужно понимать точно, как работает.
[37:48.880 --> 37:54.440]  Нам нужна хорошая модель, в которой можно оптимизировать код. То есть модель будет хороша,
[37:54.440 --> 38:03.160]  если мы, используя ее, оптимизируем свой код в этой абстрактной машине, а дальше мы его
[38:03.160 --> 38:11.760]  компилируем, и он эффективно исполняется уже на настоящем процессоре. Вот мы сейчас собираемся
[38:11.760 --> 38:18.200]  знанием про эти киши воспользоваться для оптимизации каких-то примитивов, которые мы
[38:18.200 --> 38:25.120]  видели, либо еще увидим. Но смотрите, вот прямо сейчас уже очень важную мораль можно сформулировать
[38:25.120 --> 38:33.040]  для всего нашего курса. Что вот протокол когерентности и вот эта коммуникация между кышами, которую он
[38:33.040 --> 38:38.440]  требует в отдельных сценариях, это и есть для вас, как для разработчика, стоимость разделяемой
[38:38.440 --> 38:44.520]  памяти. Вот это и есть для вас главная статья расходов при синхронизации. Вот это коммуникация.
[38:44.520 --> 38:52.320]  Чем меньше коммуникации между кышами будет в исполнении вашего кода, тем эффективнее синхронизацию
[38:52.320 --> 38:58.880]  вы построили. Но есть еще отдельная история с моделем памяти, с упорядочиванием памяти. Это вот
[38:58.880 --> 39:04.120]  история на следующую неделю. А пока это вот половина правды, половина перформанса тратится
[39:04.120 --> 39:10.840]  здесь на коммуникацию между ядрами. Да, собственно, и модель памяти тоже самая. Короче, вот эта
[39:10.840 --> 39:17.960]  коммуникация, вот это движение данных по линии, по интерконекту, по шине между разными кышами и
[39:17.960 --> 39:25.840]  памятью – это вот главная статья расхода. И вы должны ее экономить. Ну а теперь давайте посмотрим,
[39:25.840 --> 39:34.200]  как, собственно, можно ее экономить. Мне кажется, что сейчас это может быть забавно. Итак, пример первый.
[39:34.200 --> 39:42.960]  Пусть вы пишете какую-то большую, сложную, не знаю, распределенную систему, базу данных,
[39:42.960 --> 39:53.280]  и для такой системы вам полезно за ней уметь наблюдать. Есть такая тема observability. Она включает
[39:53.280 --> 39:59.760]  себя логирование, распределенный трейсинг, а еще сбор метрик. Ну грубо говоря, вы хотите знать,
[39:59.760 --> 40:05.000]  сколько запросов пришло на вашу машину в единицу времени? Сколько задач цветпулевы запустили за
[40:05.000 --> 40:12.680]  последние там 10 секунд? Короче говоря, вы хотите что-то считать. Разумеется, вы хотите считать из
[40:12.680 --> 40:19.640]  разных потоков. Вам нужен атомарный счетчик. Ну и казалось бы, в чем вопрос вообще? У вас есть
[40:20.640 --> 40:26.400]  атомик, вот у него есть операция фичет, она атомарная, ну вот атомарный счетчик готов.
[40:26.400 --> 40:38.120]  Ну это работает. Я начну с положительного. Это работает, ну и вот какой-то тест, где мы запускаем
[40:38.120 --> 40:44.840]  10 потоков, и дальше они делают по миллиону инкрементов. И мы, видимо, ожидаем после этого
[40:44.840 --> 40:59.280]  получить значение миллион, если все пойдет хорошо. Тут тест запускается в цикле, ну вот. Вот
[40:59.280 --> 41:05.560]  пример, насколько стоит миллион инкрементов из 10 потоков. Мы сейчас хотим этот код пооптимизировать,
[41:05.560 --> 41:13.320]  но пооптимизировать не в том смысле, что мы хотим ускорить инкремент. Нет, не хотим. Мы хотим
[41:13.320 --> 41:18.640]  ускорить все инкременты. В смысле, мы хотим повысить пропускную способность. Количество
[41:18.640 --> 41:24.440]  инкрементов – единица времени. Ну и давайте подумаем, что нам мешает прямо сейчас вот с такой
[41:24.440 --> 41:34.080]  реализацией счетчика. Видимо, счетчик непараллелен. Ну разумно же, да? Каждое ядро для того,
[41:34.080 --> 41:42.240]  чтобы записать что-то, сделать фичет, это запись с точки зрения кэшей, нужно получить кэшлинию
[41:42.240 --> 41:48.560]  в монопольное владение, ну, перед записатью. То есть фактически взять на нем юдекс. То есть все
[41:48.560 --> 41:55.000]  эти инкременты на уровне процесса происходят последовательно. Так можно я немного остановлюсь
[41:55.000 --> 42:00.840]  и замечание сделаю важное. Операция ComperExchange. Вот с одной стороны, это чтение плюс запись.
[42:00.840 --> 42:06.720]  Причем чтение и, возможно, запись. Так вот, с точки зрения процессора, это всегда запись.
[42:06.720 --> 42:13.280]  То есть прежде чем исполнять ComperExchange, процессор должен получить кэшлинию в монопольное
[42:13.280 --> 42:19.840]  владение. Вот не пытайтесь это оптимизировать. Не думайте, что если у вас ComperFalse, то значит
[42:19.840 --> 42:27.200]  чтение в записи не произошло. Модель памяти здесь ни при чем абсолютно. Вообще ни при чем.
[42:27.200 --> 42:48.520]  Руководство Intel является хорошим референсом. Там это написано. Что написано на эти референсы?
[42:48.520 --> 42:56.600]  Ну, там написано, что у Uniclock нужно делать параметр шаблона при объявлении. И вы все это делаете.
[42:56.600 --> 43:06.120]  А зачем? Не нужно давно уже. Ладно, хватит нытья. Как нам ускорить atomic counter? Как нам его распараллелить?
[43:06.120 --> 43:23.800]  Ну вот, thread local сложно обходить. Это хорошая очень идея, но я реализую ее несколько по-тупому.
[43:23.800 --> 43:31.720]  Я скажу, что я хочу шардировать счетчик. Вот раньше у меня был один счетчик, а теперь у меня массив
[43:31.720 --> 43:38.280]  счетчиков. Вот этот отдельный счетчик я назову шардом. Почему приходить осенью? Каждый шард это
[43:38.280 --> 43:47.800]  отдельный счетчик, просто отдельный atomic. У него такие же операции Fichet, Lot и теперь что я делаю,
[43:47.800 --> 43:54.880]  когда я говорю «инкремент»? Я для потока выбираю некоторые шарды. Каким образом? Я беру
[43:54.880 --> 44:02.520]  идентификатор потока, хэширую его, хотя это скорее всего и так число, и беру по модулю шардов. Ну,
[44:02.520 --> 44:07.320]  то есть у меня есть несколько ячеек памяти для счетчиков, и каждый поток попадает в некоторую
[44:07.320 --> 44:12.840]  ячейку. Но может быть какие-то, конечно, какие-то потоки склеиваются по ячейкам, но вот какие-то
[44:12.840 --> 44:19.400]  потоки попадают в разные, но, казалось бы, разные ячейки – это повод для параллельности. Ну,
[44:19.400 --> 44:26.200]  и вот такой вот код. Когда я хочу прочесть значение счетчика, я просто пробегаюсь по
[44:26.200 --> 44:42.320]  количеству шардов и вот складываю значение. Конструкция понятна? Ну, на самом деле,
[44:42.320 --> 44:49.760]  конечно, нет, потому что непонятно, почему это будет эквивалентно. Вот. Почему это счетчика
[44:49.760 --> 44:52.920]  тамарный, будет ли он тамарным – это вообще-то вопрос нетривиальный, гораздо нетривиальный,
[44:52.920 --> 44:58.880]  чем мы сегодня обсуждаем с вами. Но я предлагаю пока про это не думать, тем более, скажем,
[44:58.880 --> 45:05.040]  если мы читаем запросы, то нам может быть прямо в точности правильное поведение не нужно. Ну,
[45:05.040 --> 45:08.640]  это тонкий очень вопрос, но я вас не обманываю. Мне кажется, что это счетчика тамарен, нужно
[45:08.640 --> 45:20.480]  доказывать строго. Но можно взять этот счетчик, по крайней мере, и вот воспользоваться им. У нас
[45:20.480 --> 45:35.200]  здесь сколько? Восемь шардов. Ну, давайте запустим. Как-то не стало быстрее, медленнее стало,
[45:35.200 --> 45:41.760]  интересно. Ну, это несколько неожиданно, конечно. Хотя, наверное, можно понять, стало больше работы,
[45:41.760 --> 45:47.080]  а параллельность – это больше не стало. Вот мы ожидали здесь, что будет параллельность,
[45:47.080 --> 45:53.960]  да, наверное. Но мы такого ожидали, если мы просто невнимательно слушали лекцию. Потому что, ну,
[45:53.960 --> 45:59.800]  на уровне этого кода, разумеется, параллельность есть. Ну, в смысле, вот логическая параллельность
[45:59.800 --> 46:07.480]  есть у вас в голове. У вас разные потоки работают с разными ячейками памяти в этом массиве. И могли
[46:07.480 --> 46:14.040]  бы работать параллельно. Но для чего я вам картинки-то показываю? Для того, чтобы мы знали,
[46:14.040 --> 46:21.640]  что процессор, когда он общается с памятью, для него единица доступа к памяти – это не ячей
[46:21.640 --> 46:30.040]  к памяти, это целая линия. И что происходит в этом примере? В этом примере происходит явление,
[46:30.040 --> 46:36.280]  которое называется false sharing. Вот у вас есть две ячейки памяти. Вот есть одно ядро, которое пишет
[46:36.280 --> 46:42.800]  в X, а другое пишет в Y. Это разные ячейки. Записи в них могли бы идти параллельно. Но не могут,
[46:42.800 --> 46:46.880]  потому что они находятся в одной кыш-линии. А для того, чтобы сделать запись в кыш-линию,
[46:46.880 --> 46:54.800]  нужно инвалидировать копию этой линии у другого ядра. В итоге логическая параллельность здесь
[46:54.800 --> 47:00.720]  есть, а на аппаратном уровне вы берете эксклюзивную блокировку, и все записи происходят
[47:00.720 --> 47:06.840]  последовательно. Довольно тупая ситуация, правда? Ну, то есть, мы ничего не ускорили,
[47:06.840 --> 47:14.800]  кажется, стало хуже. Ну, live примеры – это, конечно, сложно. Ну, давайте попробуем ускорить.
[47:14.800 --> 47:38.800]  Неплохо, да? Волшебство. Нет, пожалуйста, не пишите так о домашних работах. Я честно,
[47:38.800 --> 47:43.120]  по-человечески прошу, не надо, потому что это вот прям для очень специальных случаев. Вот есть у
[47:43.120 --> 47:48.880]  нас mutex, и взаимное исключение – нигде такое не нужно. Но вот здесь мы на халяву буквально
[47:48.880 --> 48:01.600]  ускорили кодки четыре раза. Я сейчас про это расскажу, да. Так вот, это просто для хохмы. На самом
[48:01.600 --> 48:07.600]  деле, конечно, в C++ нужно делать… Во-первых, что мы сделали? Мы просто взяли и каждую ячейку
[48:08.000 --> 48:15.440]  поместили в свою отдельную кашлинию. И теперь с ними можно работать параллельно. Ну вот,
[48:15.440 --> 48:27.440]  в C++ для этого есть более человеческие инструменты, а именно есть Align S. То есть,
[48:27.440 --> 48:37.400]  вы выравниваете вот это поле по 64 байта. Но это не совсем фундаментальные константы для всего
[48:37.880 --> 48:46.320]  мира, но это, вероятно, сейчас так. Ну вот, быстро работает. Да, убираем, и становится медленно. Сейчас,
[48:46.320 --> 49:05.160]  давайте… Компьютер начинает греться, это не к добру. Ну вот, воспроизводится, да. Вот он нагрелся
[49:05.160 --> 49:19.960]  и стало медленнее. Итак, есть Align S, который задает выравнивание для ваших полей. И более того,
[49:19.960 --> 49:26.720]  в C++, начиная с 17, есть вот некоторая магическая константа, которая говорит, ну вот, насколько
[49:26.720 --> 49:34.680]  нужно разнести два объекта в память, чтобы они не попадали в одну кашлиню, чтобы не было
[49:34.680 --> 49:44.600]  full sharing. Ну то есть, с одной стороны, вы живете счастливы и не знаете про каши ничего, и думаете,
[49:44.600 --> 49:51.080]  что просто под вами большая память. На самом деле, про каши полезно знать, и вот эта абстракция,
[49:51.080 --> 49:55.440]  она хоть в определенной степени… То есть, каши прозрачные, о них можно не думать, но если вы о
[49:55.440 --> 50:03.800]  них думаете, то можно излечь из этого пользу. Давайте поговорим про, может быть, менее тривиальный
[50:03.800 --> 50:14.720]  пример сейчас. Где еще можно этим воспользоваться, этим full sharing? Ну, значит, с этим мы
[50:14.720 --> 50:22.400]  разобрались. Посмотрим на такую штуку, как циклический буфер. У нас есть два потока и
[50:22.400 --> 50:32.160]  циклический буфер. Один поток пишет в него, добавляет элементы в цикле. Другой читает.
[50:32.160 --> 50:39.200]  Продюсер-консюмер. Методы тут без блокировок. То есть, мы пробуем, если нет, то немного ждем.
[50:39.200 --> 50:59.640]  Наша задача такой буфер написать. Ну, что мы пишем? Мы пишем массив из значений, и у нас есть две
[50:59.640 --> 51:09.320]  переменные head и tail. И как мы с ними работаем? Ну, вот мы в продюсе читаем head и tail и смотрим,
[51:09.320 --> 51:17.520]  если буфер не полон, head указывает на первую занятую ячейку в циклическом буфере, tail – на
[51:17.520 --> 51:27.160]  первую свободную, с учетом того, что он зацикливается. Вопрос на понимание, как отличить полный буфер от
[51:27.160 --> 51:49.480]  пустого, если он циклический. Это же и вопрос. Наоборот. Ну, когда буфер пуст, то у нас head
[51:49.480 --> 52:01.160]  равен tail. Когда буфер полон, когда head почти догнал tail. Что? Тут просто вектор два индекса по
[52:01.160 --> 52:21.760]  нему бегают. Я не понимаю, слот – это вот один элемент. Мы пишем шаблон. Так вот же.
[52:21.760 --> 52:35.840]  Мы различаем эти два случая. Так, конечно, не пишите циклический буфер никогда,
[52:35.840 --> 52:46.640]  потому что тут нужно брать степень двойки и понятно. Ну и вот такой код. Мы читаем tail,
[52:46.640 --> 52:56.000]  читаем head. Если tail почти догнал head, то мы говорим false, иначе мы в tail записываем
[52:56.000 --> 53:02.920]  новый элемент и двигаем tail вперед. Для консюмера в методе tryConsume мы читаем head и tail,
[53:02.920 --> 53:08.000]  если они совпадают, буфер пустой. Иначе мы извлекаем элементы с головы и двигаем голову
[53:08.000 --> 53:19.280]  вперед. Вот симметрично все. Видите ли вы здесь false sharing? Название false sharing, потому что,
[53:19.280 --> 53:26.480]  еще раз, у нас логически разделяемых данных нет, а на самом деле фактически на уровне процессора
[53:26.480 --> 53:33.360]  разделяемые данные есть, это кэшлиния. Вот есть ли здесь что-то, что можно… есть ли здесь лишняя
[53:33.360 --> 53:41.840]  синхронизация? Ну вот смотрите, у нас есть продюсер и консюмер, и пусть… ну буфер, конечно,
[53:41.840 --> 53:46.680]  может быть полон, может быть пуст, может быть почти полон, но если вдруг он так вот где-то
[53:46.680 --> 53:55.080]  посерединке, то вроде бы с head и tail, то есть с хвостом этого буфера и с головой можно было
[53:55.080 --> 54:04.840]  бы работать параллельно. Но скажем, что здесь мешает? Что мешает работать параллельно? С точки
[54:04.840 --> 54:22.240]  зрения памяти параллельно, разумеется. Ну не только они рядом лежат. Довольно медленно,
[54:22.560 --> 54:35.160]  работает вот этот вот с 10 миллионами продюсеров консюма. Ну сейчас давай наивно. Ты говоришь,
[54:35.160 --> 54:43.640]  что вот для того, чтобы работать с головой очередью, нужно работать с… что предлагается,
[54:43.640 --> 54:59.960]  разнести вот эти поля по разным кашлениям, да? Ну это же бесполезно. Разве нет? Ну у нас же и
[54:59.960 --> 55:11.080]  консюм и продюс обращаются к обеим переменам. Ну какая разница? Постоянно кто-то пишет. Один
[55:11.080 --> 55:19.040]  пишет, другой читает. Все, это уже коммуникация. Так что плохо. Ну по крайней мере, одно место
[55:19.040 --> 55:31.680]  можно улучшить. Возможно. Хотя не факт. Ну вот если мы работаем с хвостом, если буфер небольшого
[55:31.680 --> 55:37.960]  размера, там скажем, 8, то продюсер и консюмер пересекаются, по крайней мере, по одной кашлении
[55:37.960 --> 55:43.240]  в самом буфере. Можно разнести их. Ну вот тут очень тонкий пример, может получиться, а может
[55:43.240 --> 55:54.760]  не получиться. Давайте посмотрим. Тут у меня еще и компьютер снова. Ну стало быстрее, да? То есть
[55:54.760 --> 56:06.400]  все-таки это работает. А теперь, смотрите, довольно ловкий трюк. Вот вроде бы продюсер работает с
[56:06.400 --> 56:11.800]  тейлом очереди, а консюмер работает с хедом очереди. Но при этом и тому, и другому обе ящики нужны.
[56:11.800 --> 56:22.680]  Смотрите, что я сделаю. Я не буду в продюсере перечитывать чужой хед, который пишет консюмер
[56:22.680 --> 56:29.480]  каждый раз. Я вместо этого буду хранить последний хед, который я прочитал.
[56:29.480 --> 56:44.000]  А для консюмера я буду помнить последний tail, который я прочитал. Ну кэша тут смысле не в том
[56:44.000 --> 56:49.560]  смысле, что он в аппаратном кэше, а вот в моей логике. И что я сделаю? Смотрите, я вот эту точку
[56:49.560 --> 56:56.600]  закомментирую и раскомментирую эти строчки. Вот если у меня мой прочитанный tail до сих пор
[56:56.600 --> 57:10.520]  не догнал старый прочитанный хед, то, видимо, я не должен спойлерить модели памяти. Пока. То, видимо,
[57:10.520 --> 57:17.240]  мне и не нужно читать, точнее, если он догнал, то мне нужно перечесть хед. А если у меня он еще не
[57:17.240 --> 57:24.000]  догнал, то я могу воспользоваться просто старым хедом и чужую ящику памяти не читать. Понятная
[57:24.000 --> 57:36.000]  идея. Довольно ловкая оптимизация. Здесь можно сделать точно так же. Я читаю хед, и если он догнал
[57:36.000 --> 57:41.320]  мой закашированный tail, про который я последний раз помнил, то придется перечесть. Возможно,
[57:41.320 --> 57:49.160]  tail сдвинулся вперед. А если мы его еще не догнали, то, видимо, не страшно, можно извлекать. И вот теперь,
[57:49.160 --> 57:56.960]  смотрите, на быстром пути, когда у нас закашированных переменных хватает, продюсер работает только с
[57:56.960 --> 58:02.480]  одной разделенной ящикой памяти, а консюмер работает только с одной разделенной ящикой памяти. Вот они
[58:02.480 --> 58:10.760]  уже друг с другом не общаются. И теперь, возможно, если нам повезет, мы посмотрим, что стало. О, стало
[58:10.760 --> 58:22.600]  еще раз в два раза быстрее. Нет, не стало. В два раза быстрее. Вот такую ивристику используют,
[58:22.600 --> 58:30.760]  кажется, самая быстрая разумная реализация вот такого буфера. Можете на гитхабе найти. Ну,
[58:30.760 --> 58:35.400]  про нее автор написал небольшой блокпост, но тут, в общем-то, идею все объяснил, никаких подробностей
[58:35.400 --> 58:42.920]  новых не нужно. Мы кэшируем другую ящику памяти своего соседа, конкурента, и не пересчитываем ее каждый
[58:42.920 --> 58:49.480]  раз. Поэтому мы снижаем коммуникацию между кэшами. Вот с точки зрения корректности, с точки зрения
[58:49.480 --> 58:55.560]  синхронизации, то, что мы делаем, здесь совершенно бесполезно. То есть мы ничего, никакой новой логики
[58:55.560 --> 59:03.800]  не вносим в синхронизацию. Мы всего лишь заботимся о том, чтобы между кэшлиниями не было фоллс-шеринга.
[59:03.800 --> 59:12.720]  Вот, кстати, можно его спрятать обратно и посмотреть, замедлится ли программа. Ну вот, наша оптимизация
[59:12.720 --> 59:23.240]  кату под хвост. Вот, а если мы разделили на две, на два класса ячеек и сделали паттинг между ними, то
[59:23.240 --> 59:36.880]  работает быстрее. Это еще не все. Можно, ну не знаю, если вы не хотите спойлеров, то закройте глаза сейчас,
[59:36.880 --> 59:44.880]  а я расставлю memory-order здесь и покажу, что можно еще ускорить. Ну не смотрите, пожалуйста.
[59:44.880 --> 59:59.120]  Нет, нет, я уверяю тебя, нет. Нет, я могу поспорить ответ. Расставить memory-order правильно
[59:59.120 --> 01:00:06.000]  здесь без глубокого понимания невозможно, а у нас его точно нет сейчас. Как-то интуитивно,
[01:00:06.000 --> 01:00:10.480]  по каким-то шаблонам можно, но тут дело не в шаблонах, тут дело в понимании. По шаблонам
[01:00:10.480 --> 01:00:30.600]  memory-order использовать невозможно. Пожалуйста, не думайте, что читаем мы SPP Reference, вы что-то
[01:00:31.600 --> 01:00:42.960]  только плохой может быть. Ну что? Вот видите, вот буквально из ничего, в смысле я не меняю алгоритмы,
[01:00:42.960 --> 01:00:50.120]  никаких вот волшебных там оптимизации, не оптимизирую алгоритмы всякие, я ускорил программу в 6 раз.
[01:00:50.120 --> 01:01:14.800]  Просто оптимизирую синхронизацию. По-моему довольно ловко. Давай проверим.
[01:01:20.120 --> 01:01:41.720]  Как это делается? Вот так, да? Смотрите-ка, я написал датарейс. Это сложно, в отдали памяти
[01:01:41.720 --> 01:01:50.960]  это сложно, и как это все, короче, на семинаре проверим, насколько вы это поймете. Но пока я
[01:01:50.960 --> 01:01:55.240]  просто вам показываю это как некоторое волшебство, как можно, зная про когнитность кэшей, про модели
[01:01:55.240 --> 01:02:02.480]  памяти, ускорить программу синхронизацию в 6 раз. Ну что ж, с этим примером мы закончили,
[01:02:02.480 --> 01:02:10.840]  и давайте посмотрим на еще один пример на, собственно, на синхронизацию, на спинлог. Вот этот
[01:02:10.840 --> 01:02:19.160]  спинлог совершенно наивен, и вот такой код мы написали на первой лекции с вами. Вот его тоже
[01:02:19.160 --> 01:02:25.080]  нужно улучшать, зная про протокол когнитности, правда, немного другими механизмами. Тут дело уже
[01:02:25.080 --> 01:02:31.920]  не фолдшеринги, фолдшеринга тут нет. Тут есть другое явление, которое называется... Нет,
[01:02:31.960 --> 01:02:41.040]  во-первых, давайте запустим этот код. Он снова в десяти потоках делает 100-500 раз lock-on-lock,
[01:02:41.040 --> 01:02:53.440]  и ну вот как-то работает. Первое, что с ним следует сделать, конечно же, написать что-то вот в этот
[01:02:53.440 --> 01:03:17.480]  цикл. Написали, запустили. Стало совокупно быстрее, способность повысилась. Хорошо,
[01:03:17.680 --> 01:03:25.760]  на самом деле, отдельные потоки могут замедлиться, но в единицу времени секция выполняется больше.
[01:03:25.760 --> 01:03:35.760]  А что тут еще есть плохого с точки зрения протоколок когнитности кэшей? Очень неэффективного.
[01:03:35.760 --> 01:03:45.280]  Тут есть явление, которое называется cache-пин-понг. Давайте ему... Нет, здесь есть. Вот здесь
[01:03:45.280 --> 01:03:53.760]  происходит cache-пин-понг. Почему? Но представьте, что у вас было три ядра. Странно. Четыре ядра,
[01:03:53.760 --> 01:04:01.200]  три потока. Вот на первом ядре поток захватил spin-lock. В каком состоянии сейчас кэшлиния с ячейкой
[01:04:01.200 --> 01:04:17.960]  locked у него? У него modified. У других ничего нет. У других invalid. Ну вот давайте... У других invalid.
[01:04:17.960 --> 01:04:24.440]  Потом пришел второй поток, он сделал exchange. Довольно бесполезный, потому что он прочел один,
[01:04:24.440 --> 01:04:29.960]  записал один. Но, тем не менее, он записал. Кэшлиния у него в состоянии modified. Она переехала сюда.
[01:04:29.960 --> 01:04:37.800]  Он пообщался с этим кэшом, инвалидировал у него копию, получил ее себе. Потом пришел третий поток.
[01:04:37.800 --> 01:04:44.040]  Он тоже сделал exchange. Снова прочел единицу, записал единицу. Таким образом, инвалидировал
[01:04:44.040 --> 01:04:49.920]  копию здесь, пообщался с другим кэшом, подождал, пока ему ответят на эту инвалидацию. Получил себе
[01:04:49.920 --> 01:04:58.600]  значение. Ну и опять бесполезно для себя. А что происходит дальше? Ну дальше это ядро, оно же по-прежнему
[01:04:58.600 --> 01:05:07.560]  ждет спинлока. Оно снова делает exchange и получает копию себе. И вот два этих ядра, которые ждут,
[01:05:07.560 --> 01:05:13.200]  пока спинлок освободится, просто двигают между собой эту одну кэшлиню несчастную, без всякой
[01:05:13.200 --> 01:05:21.220]  пользы. Они нагружают общую шину данных, по которой общаются все ядра процессора. Они отвлекают
[01:05:21.220 --> 01:05:25.440]  компьютер от полезной работы, просто передвигая строчку с единицей туда-сюда и без всякой пользы.
[01:05:25.440 --> 01:05:32.520]  Ну просто потому, что состояние modified, запись в ячейку памяти, требует инвалидации кэшлини
[01:05:32.520 --> 01:05:41.100]  и эксклюзивного монопольного владения. Как это оптимизировать? Я ж про это говорил.
[01:05:41.100 --> 01:05:49.640]  Compare exchange это запись всегда, независимо от исхода. Процессор, он не угоден… короче,
[01:05:49.640 --> 01:05:57.920]  запись. Как это ускорить? Это написано, как ускорить. Не нужно постоянно… ну, то есть,
[01:05:57.920 --> 01:06:02.840]  если вы сделали exchange, сделали его неуспешным, то зачем дальше мучить шину памяти? Зачем дальше
[01:06:02.840 --> 01:06:11.880]  мучить протокол гигрианта стекошей? Просто ждите, пока в ячейке не станет ноль. Вот что будет после
[01:06:11.880 --> 01:06:19.400]  такой неудачной попытки на этом ядре? Ну, точнее, не так. Вот у нас три потока сделали exchange,
[01:06:19.400 --> 01:06:25.120]  этот был успешен, этот неуспешен, неуспешен, а потом второй поток на втором ядре после exchange
[01:06:25.120 --> 01:06:32.600]  делает load уже. Ну да, он снова общается с другими кэшами, он получает себе копию,
[01:06:32.600 --> 01:06:41.120]  эта копия сбрасывается в память, и теперь у нас два шерда. А чем хороши шерда? Тем, что из них
[01:06:41.120 --> 01:06:47.560]  можно читать без коммуникации. И дальше два ядра просто дальше крутятся в этих лодах, читают кэшлинию
[01:06:47.560 --> 01:06:53.320]  без коммуникации с другими кэшами, не отвлекая процессор. Ну, то есть, что я хочу сделать? Я хочу
[01:06:53.320 --> 01:07:16.920]  написать здесь следующее. Я забыл, сколько было. Больше двухсот, но сейчас стало в два раза быстрее.
[01:07:16.920 --> 01:07:24.600]  Ну, вот опять, некоторое волшебство, но просто зная про то, за что мы платим при синхронизации,
[01:07:24.600 --> 01:07:30.720]  что синхронизация это на самом деле стоит нам коммуникации между кэшами, мы получаем такую
[01:07:30.720 --> 01:07:40.160]  штуку. А еще, если вы опять закроете глаза, я расставлю здесь memory-ордеры. Код выглядит,
[01:07:40.160 --> 01:07:44.760]  как хороший спинлок, но только такой нужно писать. Вот любые другие спинлоки писать не стоит,
[01:07:44.760 --> 01:08:05.600]  нужно ударять сразу. Синхронизация это не обычный код, это особенный код. Ну что, надеюсь, я был
[01:08:05.600 --> 01:08:17.800]  убедительным. Вот такой вот магии. Вот этим циклом мы уже ускоряем что-то. Тут как бы два
[01:08:17.800 --> 01:08:23.640]  ускорения комбинируются, то есть два типа знаний про память. С одной стороны про memory-ordеры,
[01:08:23.640 --> 01:08:29.440]  там барьеры и модели памяти, и с другой про когенетность кэшей. Но вот, используя ОВА,
[01:08:29.440 --> 01:08:34.040]  используя правильно, аккуратно, можно сильно выиграть. Правда, тут еще есть одна проблема,
[01:08:34.040 --> 01:08:47.920]  она называется Thundering Heart. Идея такая, вот был у вас спинлок, были у вас на него претенденты,
[01:08:47.920 --> 01:08:56.520]  они крутились на своем состоянии S, а потом поток, который спинлоком владел, его отпустил. Что
[01:08:56.520 --> 01:09:04.680]  произошло? Он сделал запись памяти, вот здесь вот. Он у других инвалидировал кэшлинию. Другие
[01:09:04.680 --> 01:09:13.140]  потоки перечитали ее, увидели там ноль, и потом снова каждый из них делает эксченж. Это так
[01:09:13.140 --> 01:09:17.520]  называется, потому что вот напоминает стады бизонов, которые прочли ноль и снова ломанули
[01:09:17.520 --> 01:09:24.200]  сделать эксченжи, и снова нагружают компьютер. Вот у вас сегодня будет маленькая задачка, маленькая,
[01:09:24.200 --> 01:09:38.600]  но странная. Не знаю, компенсирует ли одно другое. Давайте я покажу. Про очень затейливый спинлок,
[01:09:38.600 --> 01:09:50.560]  который используется в ядре линукса, и который как раз оптимизирует и пинг-понг, и вот этот
[01:09:51.240 --> 01:09:57.480]  который оптимизирует промахи по кэшу с помощью очень ловкой интрузивной очереди, что бы это ни значило,
[01:09:57.480 --> 01:10:03.560]  но интрузивность у нас по многим причинам полезна, поэтому вот этот спинлок тоже можно
[01:10:03.560 --> 01:10:09.160]  написать, а потом можно под файберами переписать красиво. Ну короче, попробуйте, разберетесь еще
[01:10:09.160 --> 01:10:15.040]  лучше в том, где вы платите за протокол к гирятости. В этом коде, в этой задачке вас потребует
[01:10:15.040 --> 01:10:21.520]  написать, от вас потребуется написать больше сложного кода, спинлок станет сложнее, там будет
[01:10:21.520 --> 01:10:27.240]  больше действий, больше объектов, односвязанные списки, но при этом он будет лучше, что довольно
[01:10:27.240 --> 01:10:35.880]  неожиданно. Ну это все была в ствол на самом деле, потому что, смотрите, сейчас я скажу, наверное,
[01:10:35.880 --> 01:10:44.840]  самое важное, что можно сказать на этой лекции и вообще может быть в курсе. Ну да, мы можем
[01:10:44.840 --> 01:10:52.040]  оптимизировать спинлок, мы можем в нем оптимизировать синхронизацию, там какие-то разделяемые ячейки
[01:10:52.040 --> 01:11:03.480]  памяти, но если вы используете мьютексы, просто вот вы используете мьютексы в программе, то вам уже
[01:11:03.480 --> 01:11:09.560]  плохо, потому что что такое мьютексы, ну что такое использование мьютекса, у вас есть разные ядра,
[01:11:09.560 --> 01:11:17.400]  на них есть потоки и они берут лог и работают с какими-то данными. Ну да, вы пооптимизировали
[01:11:17.400 --> 01:11:24.680]  что-то в спинлоке, но в конце концов зачем вы брали блокировку, чтобы работать с данными,
[01:11:24.680 --> 01:11:30.640]  и если у вас сначала критическая секция была на одном ядре, а потом она запускается на другом
[01:11:30.640 --> 01:11:36.440]  ядре, то что происходит, вы запустились на одном ядре, записали данные, теперь они у вас в вашем
[01:11:36.440 --> 01:11:43.040]  кэше, вот на этом ядре в состоянии modified, а потом вы взяли блокировку на другом ядре, и вот в этом
[01:11:43.040 --> 01:11:47.200]  кэше сейчас данных нет, просто по инварианту протоколок гириантности, по состоянию modified,
[01:11:47.200 --> 01:11:55.560]  и вам нужно пойти в другой кэш и эти данные к себе привести, и вот если вы используете мьютекс
[01:11:55.560 --> 01:12:01.320]  обычным способом, запуская потоки на разных ядрах, то вы, как бы вы не оптимизировали
[01:12:01.320 --> 01:12:09.640]  саму блокировку, двигаете данные туда-сюда, понимаете проблему, вот мьютекс вас заставляет работать
[01:12:09.640 --> 01:12:16.160]  с процессором так, как он работать не хочет, как ему неэффективно работать, просто мьютекс by
[01:12:16.160 --> 01:12:22.240]  design навязывает процессору очень неэффективный сценарий работы, это очень глубокая мораль на самом
[01:12:22.240 --> 01:12:38.680]  деле, потому что, ну я не знаю, знаете ли вы что-нибудь про продюс? Ну да, знаю, это больше чем ничего,
[01:12:38.680 --> 01:12:48.640]  безусловно, медленно загрузится, ну давайте я пока нагрузится или загрузилась расскажу коротко,
[01:12:48.800 --> 01:12:56.240]  вот у вас есть очень много машин, тысячи, сотни тысяч, может быть даже, десятки, сотни тысяч,
[01:12:56.240 --> 01:13:05.200]  очень много данных на этих машинах, и вы хотите их параллельно обрабатывать, и при этом вы не хотите
[01:13:05.200 --> 01:13:11.760]  думать о том, где данные лежат, что делать с упавшими машинами, как перезапускать фрагменты
[01:13:11.760 --> 01:13:22.320]  вычислений, которые разломались, для этого Google в 2000 каком-то древнем году, ст. 4 года,
[01:13:22.320 --> 01:13:26.960]  придумал такой фреймворк, ну такой вообще парадигму описания вычислений, мы продюс,
[01:13:26.960 --> 01:13:33.400]  вы описываете свое вычисление над большими данными в виде композиции вот таких вот операторов,
[01:13:33.400 --> 01:13:41.120]  а дальше сам фреймворк эффективно исполняет ваши задачи на большом кластере машин
[01:13:41.240 --> 01:13:48.360]  скрывая от вас отказы, так вот в чем идея там, ну представьте у вас есть много данных,
[01:13:48.360 --> 01:13:53.360]  они как-то разложены по машинам, на каких-то они есть, на каких-то нет, и у вас есть вычисления,
[01:13:53.360 --> 01:14:02.320]  где разумно запускать вычисления, ну вот разумно не запускать вычисления где-то, а потом вести
[01:14:02.320 --> 01:14:07.200]  к этому вычислению данные, разумно отправить вычисления вашему программу «Бинарник» туда,
[01:14:07.200 --> 01:14:16.100]  где просто данные лежат. К чему я это говорю? К тому, что распределенная система большая и ваш
[01:14:16.100 --> 01:14:23.380]  процессор мало чем отличаются. У нас есть потоки, которые работают с разрядивыми данными, и у нас
[01:14:23.380 --> 01:14:30.180]  есть эти данные. И вот вместо того, чтобы двигать данные между ядрами, между кышами, гораздо разумнее,
[01:14:30.180 --> 01:14:36.460]  критические секции, которые обрабатывают эти данные, привести на одно ядро и выполнить их там
[01:14:36.460 --> 01:14:44.480]  подряд просто. Тогда не будет промахов по кышу. Это основная статья расхода для вас. Не будет
[01:14:44.480 --> 01:14:51.320]  навалидаций, данные будут горячими локально, и просто между критическими секциями будет
[01:14:51.320 --> 01:14:59.920]  меньше синхронизации. Она не нужна будет. Вот мы через субботу, через выходные поговорим про
[01:14:59.920 --> 01:15:05.740]  фьючи, и там будут экзекьюторы, и там будет асинхронный мьютекс. Так вот, асинхронный мьютекс
[01:15:05.740 --> 01:15:13.020]  звучит довольно странно, но в общем, можно написать так, чтобы он работал эффективнее, чем обычный
[01:15:13.020 --> 01:15:17.940]  синхронный мьютекс. Потому что асинхронному мьютексу не нужно будет возить данные между кышами.
[01:15:17.940 --> 01:15:25.420]  Вот обычный мьютекс для потоков – это сценарий неэффективный. И когда мы будем писать файберы
[01:15:25.420 --> 01:15:31.320]  со своим планировщиком и со своим мьютексом, то можно будет делать так, чтобы критические секции
[01:15:31.320 --> 01:15:39.800]  для файберов кластеризовались на одном ядре, чтобы они запускались вот подряд. Или, скажем, вы
[01:15:39.800 --> 01:15:45.840]  пишете какие-нибудь горутины, и одна горутина шлёт, данные другая их получает. Вот было бы глупо
[01:15:45.840 --> 01:15:51.640]  запускать их на разных ядрах, разумно запустить их одну за другой на том же ядре, потому что данные
[01:15:51.640 --> 01:15:59.620]  уже в кэше. Вот компьютер ваш, STD-mutex и планировщик операционной системы, такую локальность
[01:15:59.620 --> 01:16:06.740]  не учитывает. Но если вы пишете свой планировщик и свои мьютексы там в ГОИ или вот мы пишем в своем
[01:16:06.740 --> 01:16:13.500]  фреймворке, то мы можем эту локальность учесть кластеризовать секции на одном ядре, в одном
[01:16:13.500 --> 01:16:18.660]  потоке, и получить просто большую пропускную способность. Вот это такая очень хитрая мораль,
[01:16:18.660 --> 01:16:27.280]  надеюсь, я смог вам её рассказать. Прокажите на сегодня всё. В следующий раз мы поговорим про
[01:16:27.280 --> 01:16:34.080]  модели памяти, это будет гораздо более сложная и совершенно убийственная история, и я вас немного
[01:16:34.080 --> 01:16:38.880]  обману, потому что я вместо одной лекции прочитаю вам две, потому что за одну лекцию невозможно
[01:16:38.880 --> 01:16:47.120]  уместить, и лучше делать это разом. Вот как именно мы организуемся, мы обсудим в чате, но вот на
[01:16:47.140 --> 01:16:51.060]  следующей неделе мы продолжим разговоры про процессоры и про memory order, и там у нас будет
[01:16:51.060 --> 01:16:58.620]  три часа сложный разговор, где мы даже не узнаем, что такое release-acquire. Вот, так что на сегодня
[01:16:58.620 --> 01:17:00.180]  всё, приходите через неделю, спасибо.

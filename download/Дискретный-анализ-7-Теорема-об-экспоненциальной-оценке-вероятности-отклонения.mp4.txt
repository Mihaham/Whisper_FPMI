[00:00.000 --> 00:04.000]  Редактор субтитров Е.Воинова Корректор А.Кулакова
[00:30.000 --> 00:33.000]  Корректор А.Кулакова
[01:00.000 --> 01:03.000]  Корректор А.Кулакова
[01:30.000 --> 01:33.000]  Корректор А.Кулакова
[02:00.000 --> 02:03.000]  Корректор А.Кулакова
[02:30.000 --> 02:33.000]  Корректор А.Кулакова
[03:00.000 --> 03:03.000]  Корректор А.Кулакова
[03:30.000 --> 03:33.000]  Корректор А.Кулакова
[04:00.000 --> 04:03.000]  Корректор А.Кулакова
[04:30.000 --> 04:33.000]  Корректор А.Кулакова
[05:00.000 --> 05:03.000]  Корректор А.Кулакова
[05:30.000 --> 05:33.000]  Корректор А.Кулакова
[06:00.000 --> 06:03.000]  Корректор А.Кулакова
[06:30.000 --> 06:33.000]  Корректор А.Кулакова
[07:00.000 --> 07:03.000]  Корректор А.Кулакова
[07:04.000 --> 07:07.000]  Корректор А.Кулакова
[07:08.000 --> 07:11.000]  Корректор А.Кулакова
[07:12.000 --> 07:15.000]  Корректор А.Кулакова
[07:16.000 --> 07:19.000]  Корректор А.Кулакова
[07:20.000 --> 07:23.000]  Корректор А.Кулакова
[07:24.000 --> 07:27.000]  Корректор А.Кулакова
[07:27.000 --> 07:30.000]  Корректор А.Кулакова
[07:31.000 --> 07:34.000]  Корректор А.Кулакова
[07:35.000 --> 07:38.000]  Корректор А.Кулакова
[07:39.000 --> 07:42.000]  Корректор А.Кулакова
[07:43.000 --> 07:46.000]  Корректор А.Кулакова
[07:47.000 --> 07:50.000]  Корректор А.Кулакова
[07:51.000 --> 07:54.000]  Корректор А.Кулакова
[07:54.000 --> 07:57.000]  Корректор А.Кулакова
[07:58.000 --> 08:01.000]  Корректор А.Кулакова
[08:02.000 --> 08:05.000]  Корректор А.Кулакова
[08:06.000 --> 08:09.000]  Корректор А.Кулакова
[08:10.000 --> 08:13.000]  Корректор А.Кулакова
[08:14.000 --> 08:17.000]  Корректор А.Кулакова
[08:18.000 --> 08:21.000]  Корректор А.Кулакова
[08:21.000 --> 08:24.000]  Корректор А.Кулакова
[08:25.000 --> 08:28.000]  Корректор А.Кулакова
[08:29.000 --> 08:32.000]  Корректор А.Кулакова
[08:33.000 --> 08:36.000]  Корректор А.Кулакова
[08:37.000 --> 08:40.000]  Корректор А.Кулакова
[08:41.000 --> 08:44.000]  Корректор А.Кулакова
[08:45.000 --> 08:48.000]  Корректор А.Кулакова
[08:48.000 --> 08:51.000]  Корректор А.Кулакова
[08:52.000 --> 08:55.000]  Корректор А.Кулакова
[08:56.000 --> 08:59.000]  Корректор А.Кулакова
[09:00.000 --> 09:03.000]  Корректор А.Кулакова
[09:04.000 --> 09:07.000]  Корректор А.Кулакова
[09:08.000 --> 09:11.000]  Корректор А.Кулакова
[09:12.000 --> 09:15.000]  Корректор А.Кулакова
[09:15.000 --> 09:18.000]  Корректор А.Кулакова
[09:19.000 --> 09:22.000]  Корректор А.Кулакова
[09:23.000 --> 09:26.000]  Корректор А.Кулакова
[09:27.000 --> 09:30.000]  Корректор А.Кулакова
[09:31.000 --> 09:34.000]  Корректор А.Кулакова
[09:35.000 --> 09:38.000]  Корректор А.Кулакова
[09:39.000 --> 09:42.000]  Корректор А.Кулакова
[09:42.000 --> 09:45.000]  Корректор А.Кулакова
[09:46.000 --> 09:49.000]  Корректор А.Кулакова
[09:50.000 --> 09:53.000]  Корректор А.Кулакова
[09:54.000 --> 09:57.000]  Корректор А.Кулакова
[09:58.000 --> 10:01.000]  Корректор А.Кулакова
[10:02.000 --> 10:05.000]  Корректор А.Кулакова
[10:06.000 --> 10:09.000]  Корректор А.Кулакова
[10:09.000 --> 10:12.000]  Корректор А.Кулакова
[10:13.000 --> 10:16.000]  Корректор А.Кулакова
[10:17.000 --> 10:20.000]  Корректор А.Кулакова
[10:21.000 --> 10:24.000]  Корректор А.Кулакова
[10:25.000 --> 10:28.000]  Корректор А.Кулакова
[10:29.000 --> 10:32.000]  Корректор А.Кулакова
[10:33.000 --> 10:36.000]  Корректор А.Кулакова
[10:36.000 --> 10:39.000]  Корректор А.Кулакова
[10:40.000 --> 10:43.000]  Корректор А.Кулакова
[10:44.000 --> 10:47.000]  Корректор А.Кулакова
[10:48.000 --> 10:51.000]  Корректор А.Кулакова
[10:52.000 --> 10:55.000]  Корректор А.Кулакова
[10:56.000 --> 10:59.000]  Корректор А.Кулакова
[11:00.000 --> 11:03.000]  Корректор А.Кулакова
[11:03.000 --> 11:06.000]  Корректор А.Кулакова
[11:07.000 --> 11:10.000]  Корректор А.Кулакова
[11:11.000 --> 11:14.000]  Корректор А.Кулакова
[11:15.000 --> 11:18.000]  Корректор А.Кулакова
[11:19.000 --> 11:22.000]  Корректор А.Кулакова
[11:23.000 --> 11:26.000]  Корректор А.Кулакова
[11:27.000 --> 11:30.000]  Корректор А.Кулакова
[11:30.000 --> 11:33.000]  Корректор А.Кулакова
[11:34.000 --> 11:37.000]  Корректор А.Кулакова
[11:38.000 --> 11:41.000]  Корректор А.Кулакова
[11:42.000 --> 11:45.000]  Корректор А.Кулакова
[11:46.000 --> 11:49.000]  Корректор А.Кулакова
[11:50.000 --> 11:53.000]  Корректор А.Кулакова
[11:54.000 --> 11:57.000]  Корректор А.Кулакова
[11:57.000 --> 12:00.000]  Корректор А.Кулакова
[12:01.000 --> 12:04.000]  Корректор А.Кулакова
[12:05.000 --> 12:08.000]  Корректор А.Кулакова
[12:09.000 --> 12:12.000]  Корректор А.Кулакова
[12:13.000 --> 12:16.000]  Корректор А.Кулакова
[12:17.000 --> 12:20.000]  Корректор А.Кулакова
[12:21.000 --> 12:24.000]  Корректор А.Кулакова
[12:24.000 --> 12:30.220]  конкретный пример. Возьмите n равное 10 в шестой, чтобы было как-то более впечатляюще, так всем
[12:30.220 --> 12:37.980]  рассказываю. Возьмите a равное 10, например, в четвертый, то есть он делает миллион шагов,
[12:37.980 --> 12:48.420]  пьяница делает миллион шагов и хочет, надеется, удалиться от центра притяжения на расстояние
[12:48.420 --> 12:55.180]  всего лишь 10 тысяч или больше. Ну то, что здесь стоит именно такой знак или такой, я надеюсь,
[12:55.180 --> 13:01.660]  вы понимаете, без разницы. Больше, больше, либо равно это не имеет никакого значения. Значит,
[13:01.660 --> 13:09.840]  вот он хочет за миллион шагов удалиться хотя бы на 10 тысяч вправо. Вероятность этого это 10 в шестой
[13:09.840 --> 13:19.000]  поделить на 10 в восьмой, а на меньше одной сотой. То есть, если он 100 раз попробует сходить на
[13:19.000 --> 13:25.840]  расстояние миллион, то скорее всего все равно ничего не получится. Ну, 100 раз делает миллион шагов,
[13:25.840 --> 13:35.280]  100 миллионов шагов. Ну, такое нет, понимаете, но программа. Программа-то может делать много шагов,
[13:35.440 --> 13:51.960]  а вот человек реальный, квадриллион километров во мраке. Но, понимаете, то есть это скорее даже
[13:51.960 --> 14:02.080]  не то, что пить вредно, а то, что такое объяснение алкоголизма. Человек не может уйти далеко от
[14:02.080 --> 14:14.280]  кабака. Ну, вредно, конечно, алкоголизм даже плохо. Ну вот, ладно. Значит, к чему я клоню? Клоню
[14:14.280 --> 14:22.080]  я всегда к следующей теореме, которая забивает крышку, последний гвоздь в крышку гроба этого
[14:22.080 --> 14:36.680]  несчастного пьянице. Да. Нет, но она же меньше. Ну, зачем делить пополам? Она меньше. Ну, то есть,
[14:36.680 --> 14:42.520]  не то, что если формально писать, а если пытаться оценить лучше, то да, конечно, можно поделить пополам.
[14:42.520 --> 15:02.200]  Да. Ну, тем лучше. Я же оцениваю это сверху. Но если пополам разделить, это же лучше будет. Ну,
[15:02.200 --> 15:07.440]  то есть, мое неравенство правильное. Просто может быть завышенное. Но я вот сейчас хочу сформулировать
[15:07.440 --> 15:13.040]  теорему, которая говорит, что оно не в два раза завышенное, а в дофигищий раз. Вот, извините меня
[15:13.040 --> 15:30.080]  за грубое выражение. Значит, утверждается, что можно вот такую штуку доказать. Вероятность
[15:30.080 --> 15:38.760]  не линейна или там квадратична, а экспоненциально мала. Вот смотрите на тот же самый пример. А квадрат
[15:38.760 --> 15:46.360]  поделить на n, это перевернутая n поделить на а квадрат. То есть, это 100. Но вот здесь как раз
[15:46.360 --> 15:54.520]  пополам делится это 100, получается 50. Но это, конечно, хорошо. Но согласитесь, что e в минус 50 и
[15:54.520 --> 16:00.760]  гораздо меньше, чем 1 сотая. Это означает, что уже действительно хуже, чем квадриллион километров
[16:00.760 --> 16:16.120]  пресловутой. У меня каждый год такое исследование про квадриллион километров. Это цитата, на самом
[16:16.120 --> 16:23.320]  деле. Она кому-то заходит или нет? Кто-то понимает, откуда эта цитата про квадриллион километров
[16:23.320 --> 16:29.640]  во мраке. Ладно. Раз все молчат, значит, никто не понимает. Ну ничего странного. Может, там кто-нибудь
[16:29.640 --> 16:39.320]  понимает. Просто там, в смысле, вот там. За камерой. Не оператор, а те, кто будут смотреть потом. Нет,
[16:39.320 --> 16:48.000]  нет. Хуже. Ну, узл. Не хуже, конечно. Больше. 20 тысяч льи, это всего 80 тысяч километров. А тут,
[16:48.000 --> 16:55.240]  понимаете, квадриллион. Квадриллион это сколько? Это 10 в 15, да? Триллион это 10 в 12, квадриллион
[16:55.240 --> 17:02.920]  10 в 15. Ну ладно, это все не важно. В общем, совершенно обалденная вероятность. Значит, дорогие друзья,
[17:02.920 --> 17:08.760]  поскольку у вас действительно еще мало было теории вероятностей, дайте я чуть-чуть вам буквально
[17:08.760 --> 17:15.560]  пару слов скажу по поводу того, с чем вот это связано. С тому функцию вы, наверное, узнаете,
[17:15.560 --> 17:27.740]  да? Она похожа на что-то, да? Встречали вот такую функцию? Это нормальное распределение называется,
[17:27.740 --> 17:33.280]  но если не встречали, вы ее встретите. Все равно обязательно. Это прям очень важная функция,
[17:33.280 --> 17:38.420]  которая есть в теории вероятностей. Называется плотность нормального распределения. Там надо
[17:38.420 --> 17:46.860]  еще на корень из 2p поделить, чтобы интеграл по всей прямой равнялся единицей. Тогда это действительно
[17:46.860 --> 17:52.060]  будет плотность. Но если вы не знаете, что это, забейте. Я сейчас вот эту теорему докажу,
[17:52.060 --> 17:58.660]  не ссылаясь ни на какие интегралы, ни на какие ваши знания из теории вероятностей. Я сейчас
[17:58.660 --> 18:04.500]  докажу, честно докажу, и вам надо будет уметь воспроизводить. Это гениальное доказательство.
[18:04.500 --> 18:12.660]  Ну а это вы потом меня вспомните. У вас будет предельная теорема, там какая-нибудь типа
[18:12.660 --> 18:19.380]  центральная. Можете просто вот так где-то отложить у себя в памяти, что когда-то в теории вероятностей
[18:19.380 --> 18:25.820]  будет центральная предельная теорема. Там будут вот эти интегралы. Но из нее, строго говоря,
[18:25.820 --> 18:31.060]  вот это не будет следовать, потому что она предельная. Там какой-то предельный переход. А здесь
[18:31.060 --> 18:37.900]  вот прям вот совершенно явное неравенство, верное для всех n и для всех а. Поехали гениальное
[18:37.900 --> 18:54.420]  доказательство. Ну, нас интересует вот такая вероятность. Во-первых, давайте вот в этом
[18:54.420 --> 19:00.500]  неравенстве слева и справа произведем умножение на одну и ту же положительную константу.
[19:00.500 --> 19:13.380]  Ну, зачем-то. Константу обозначим лямбда. Лямбда больше нуля. Мы ее потом подберем. Я могу
[19:13.380 --> 19:18.140]  сразу сказать, чему она в оптимальном случае равна, но мы ее подберем, чтобы вас порадовать.
[19:18.140 --> 19:32.660]  Ну, можем так сделать. Это не самый гениальный ход. Самый гениальный ход — это как бы,
[19:32.660 --> 19:38.900]  зная ответ, подогнать под него. То есть давайте вот в этом неравенстве возьмем от левой и правой
[19:38.900 --> 19:46.340]  части экспоненту. И е возведем в степень левой части, правой части. Экспонента — функция
[19:46.340 --> 20:03.180]  монотонная, правильно? Поэтому это тоже верное будет неравенство. Больше либо равняется е в
[20:03.180 --> 20:11.700]  степени лямбда. Теперь применим банальное неравенство Маркова, от которого я уворачивался,
[20:11.700 --> 20:17.660]  помните, довольно усиленно. Я сказал, что не хочу вот здесь применять неравенство Маркова,
[20:17.660 --> 20:23.540]  хочу применить его именно в форме ЧБШ. А вот здесь применим прям неравенство Маркова. Видите,
[20:23.540 --> 20:29.620]  вот это же положительное число, а это функция, которая принимает только положительные значения.
[20:29.620 --> 20:38.220]  Поэтому можно применять неравенство Маркова. Я пишу нм неравенства Маркова, согласно которому
[20:38.220 --> 20:54.900]  будет вот так. Мат ожидания экспоненты. Ну поделить на е в степени лямбда, ну давайте я напишу
[20:54.900 --> 21:02.260]  умножить на е в степени минус лямбда. Поделить на е в степени лямбда, поделить на правую часть
[21:02.260 --> 21:08.460]  мат ожидания левой части, поделить на правую часть. Это неравенство Маркова. Ну поделил вот так,
[21:08.460 --> 21:17.260]  написал. Так, все успевают? Нормально? Так, смотрите, экспоненты от суммы, это произведение экспонент.
[21:17.260 --> 21:28.620]  Сами величины независимы, поэтому экспоненты от них, очевидно, тоже независимы. А значит,
[21:28.760 --> 21:35.020]  мат ожидания их произведения, это произведение их мат ожидания. Если вдруг вы не знаете такого
[21:35.020 --> 21:39.980]  свойства, уверяю вас, оно точно будет, я его не буду доказывать. Еще раз, если случайные величины
[21:39.980 --> 21:48.400]  независимы, то мат ожидания их произведения – это произведение их мат ожидания. Но тут важно,
[21:48.400 --> 21:53.420]  чтобы они были независимы. Линейность верна всегда, мат ожидания суммы – это всегда сумма.
[21:53.420 --> 21:58.940]  мат ожидания. А вот мат ожидания произведения равно произведению мат
[21:58.940 --> 22:07.440]  ожидания только в случае, если величины независимы. Так, ну пишу. Мат ожидания
[22:07.440 --> 22:16.100]  е в степени лямбда х1 и так далее. Мат ожидания е в степени лямбда хn.
[22:16.140 --> 22:24.660]  Ну и вот этот прилипало, который никуда не девается, е в степени минус лямбда. Но это
[22:24.660 --> 22:27.860]  хорошо. Нам надо оценить сверху. Отрицательные экспоненты нам явно в
[22:27.860 --> 22:33.780]  этом деле только помощницы. Наша цель оценить вероятность сверху. У нас уже есть
[22:33.780 --> 22:41.740]  отрицательные экспоненты. Как здорово. Так, давайте я пойду справа налево.
[22:46.100 --> 22:48.700]  О-хо-хо.
[22:54.940 --> 23:00.060]  Ну, беру любое из этих мат ожиданий. Поскольку иксы-то и все одинаково распределены,
[23:00.060 --> 23:04.660]  они все принимают вот два значения, с вероятностью одна вторая, то какая разница,
[23:04.660 --> 23:08.460]  какое брать. Они все одинаковые. С чему они равны?
[23:08.460 --> 23:19.980]  Е в степени лямбда плюс е в степени минус лямбда пополам. Ну, это даже называется
[23:19.980 --> 23:25.100]  чосинус там какой-то. Косинус гиперболический. Вот, но я лично не помню
[23:25.100 --> 23:31.020]  формулу Тейлора для косинуса гиперболического. Может, вы помните?
[23:31.020 --> 23:36.060]  Ну, я лично помню вообще только для экспонента. Вы меня извините. Я вот помню
[23:36.060 --> 23:39.900]  для экспонента, а дальше я могу через экспонента что угодно записать. Поэтому,
[23:39.900 --> 23:44.300]  с вашего позволения, я традиционно для себя просто напишу ряд Тейлора для
[23:44.300 --> 23:49.580]  каждой из экспонента, а потом их сложу. Одну-вторую я пока вынесу за скобку. Тут
[23:49.580 --> 23:54.780]  будет сумма пока от нуля до бесконечности лямбда вкатой поделить на к факториал.
[23:54.780 --> 24:01.500]  Плюс сумма пока от нуля до бесконечности минус лямбда вкатой поделить на к
[24:01.500 --> 24:11.340]  факториал. Так, ну вот, я легко соображаю, что если k нечетная, но одно и то же в
[24:11.340 --> 24:17.580]  двух суммах и при этом нечетная, то они друг друга кокают. А если k четная, то они
[24:17.580 --> 24:23.340]  наоборот складываются, потом делятся пополам и дают то, что было в исходном.
[24:23.340 --> 24:30.740]  Так, я не очень быстро говорю. Нечетные аннигилируют, а четные складывают, потом
[24:30.740 --> 24:37.060]  делятся пополам. То есть у меня получается вот так. Сумма пока от нуля до
[24:37.060 --> 24:43.860]  бесконечности лямбда в степени 2k поделить на 2k факториал. Я надеюсь, что мне
[24:43.860 --> 24:50.740]  не надо делать замену переменных. Ну, k и k, какая разница? Только четкие выжили, но с коэффициентом 1,
[24:50.740 --> 24:58.900]  потому что двое здесь поделились пополам. Так, теперь я замечаю глупость, но она... Как глупость?
[24:58.900 --> 25:05.380]  Она очень простая, конечно, но сильно лучше и не сделать. k факториал на 2 в степени k.
[25:05.380 --> 25:13.700]  Ну, это очевидно. Ну, вроде смешно, а при k равном нулю это точное равенство. То есть улучшить нельзя,
[25:13.700 --> 25:21.140]  например. Поэтому при k равном единице, кстати, тоже. Поэтому в каком-то смысле это не
[25:21.140 --> 25:29.780]  улучшаемая история. Значит, у меня получается меньше либо равно сумма. Ну, а дальше они уже
[25:29.780 --> 25:36.420]  маленькие становятся. Сумма пока от нуля до бесконечности лямбда в степени 2k поделить
[25:36.420 --> 25:48.660]  на 2k факториал и на 2 в степени k. А теперь я замечаю, что это опять ряд Тейлора. Ой, на k,
[25:48.660 --> 25:54.380]  конечно. Да, извините. Конечно, на k, иначе это не будет ряд Тейла. На k факториал. Да-да-да,
[25:54.380 --> 26:07.140]  вот я подставляю вот это неравенство. Потому что вот так. О, называется это орешек арахис.
[26:07.140 --> 26:17.300]  Вот видите, какой арахис. Ладно, шутка глупая. Но он все время у меня рисуется, как такой арахис.
[26:17.300 --> 26:23.500]  Не, ну короче говоря, видите, тут лямбда квадрат пополам и все вкатый. Я специально вот этот
[26:23.500 --> 26:28.820]  лямбда квадрат пополам в этот арахис обвел. Лямбда квадрат пополам вкатый и поделить на k
[26:28.820 --> 26:35.220]  факториал. Что, плохо видно? Лямбда квадрат пополам и все это вкатый. То есть это снова ряд для
[26:35.220 --> 26:47.660]  экспонента только лямбда квадрат пополам. Вот так. А теперь я здесь продолжаю. У меня получается вот
[26:47.660 --> 27:00.260]  что. У меня получается е в степени лямбда квадрат н пополам. Что? Да-да-да, вот мне уже сказали,
[27:00.260 --> 27:06.580]  я зачеркнул, но зачеркнул плохо. Вы не увидели. Ну надо было просто стереть, да, но это долго стирать,
[27:06.580 --> 27:12.700]  надо скобки тогда стирать. Ну как-то, в общем, я решил. Ну, впрочем, и сейчас их надо как бы стирать,
[27:12.700 --> 27:18.740]  поскольку я двойку зачеркнул. Можно не стирать, да, в скобках k, но мне скажут, что в скобках k это
[27:18.740 --> 27:24.780]  наверно какое-то произведение. Ладно, друзья, я шучу, но понятно. k факториал, правильно. Именно
[27:24.780 --> 27:30.300]  поэтому получается такая экспонента. Вы видели, да, что там е в степени лямбда квадрат пополам?
[27:30.300 --> 27:38.540]  Видели? Но это вот n таких одинаковых экспонент получается. Ну то есть надо умножить на n в
[27:38.540 --> 27:43.860]  показатели, потому что они перемножаются, в показатели складываются, ну получается n вот раз,
[27:43.860 --> 27:50.500]  один и тот же показатель. И минус лямбда а, товарищи. Ну я думаю, что уже многие поняли,
[27:50.500 --> 27:58.060]  что произошло. У меня в показателе экспоненты находится квадратный, ну, трехчлен, двучлен,
[27:58.060 --> 28:11.500]  я уже не знаю. Парабола, короче говоря. Парабола усами вверх. У нее есть точка минимума по лямбда.
[28:11.500 --> 28:18.340]  Помните, я сказал, что я в принципе могу сказать, чему равняется лямбда, но мы его подберем. Нам же
[28:18.340 --> 28:25.060]  нужна оценка сверху. Мы подберем лямбда таким образом, чтобы она была самой лучшей. Чтобы вот
[28:25.060 --> 28:29.940]  это выражение было минимально, то есть чтобы парабола минимизировалась. Ну в какой точке
[28:29.940 --> 28:36.140]  минимизируется? Ну вы понимаете, там производная равна нулю, да? Ну, со школы еще многие изобрят
[28:36.140 --> 28:47.420]  минус b поделить на 2a. Тут минус b это a, а 2a это n. Ну то есть надо взять лямбда равная a поделить на n.
[28:47.420 --> 28:57.540]  Это точка минимума, где производная ноль. И подставить сюда будет a квадрат поделить на 2n
[28:57.540 --> 29:05.100]  квадрат и умножить на n. А тут будет a квадрат поделить на n с минусом. То есть тут на 2n,
[29:05.100 --> 29:09.740]  а с минусом просто на n. Ну вот и будет вот это. Прямо в точность.
[29:14.100 --> 29:19.140]  А? В каком-то смысле это, знаете, как доказательство из книги. То есть,
[29:19.140 --> 29:30.140]  пойди додумайся до такого. Три строчки, по сути, короткие. Никаких вам там центральных
[29:30.140 --> 29:36.020]  предельных теорем ничего. И эта оценка, по сути, не улучшаемая. То есть вот лучше сделать нельзя.
[29:36.020 --> 29:48.100]  Ну там, точностью, да, мизер. Принципиально это не улучшаемая оценка. Вот такое вот. Называется
[29:48.100 --> 29:54.780]  неравенство большого уклонения. Друзья, вы только осознайте, значит, мы сожрали с потрохами неравенство
[29:54.780 --> 30:02.020]  Чебышова, но только в этом специальном случае. То есть важно, что суммируются независимые величины,
[30:02.020 --> 30:09.540]  которые еще там какие-то очень специфические значения принимают плюс-минус один с вероятностью
[30:09.540 --> 30:15.620]  одна-вторая. Мы на этом очень существенно сыграли. Конечно, я не утверждаю, что неравенство Чебышова
[30:15.620 --> 30:24.180]  всегда можно вот так вот улучшить. То есть здесь неравенство Чебышова стер, да, давало что-то
[30:24.180 --> 30:31.260]  типа n поделить на а в квадрате, а тут вот такая вот экспонентность перевернутая величиной в показатель.
[30:31.260 --> 30:38.220]  Я не говорю, что в любом случае можно сделать такое улучшение, но вот для такого набора
[30:38.220 --> 30:43.220]  случайных величин можно. И в этом некий пафос, о котором мы еще будем говорить на протяжении
[30:43.220 --> 30:54.420]  этого курса. Так, понятно я сказал? Так, теперь для вот этой теоремы нужно некое обобщение,
[30:54.420 --> 31:07.060]  которое похоже придется давать после перерыва. Давайте сейчас перерыв тогда 6-5 минут. Так, ну давайте
[31:07.060 --> 31:14.500]  я обобщу. Все, я продолжаю. Давайте я обобщу немножко или ну как обобщу, ну просто перенесу на некий
[31:14.500 --> 31:20.460]  другой случай вот этот результат. Наверное без доказательства, потому что там оно немножко
[31:20.460 --> 31:36.740]  более техничное, но в общем суть та же самая. Так, вот такая вот случайная величина немножко
[31:36.740 --> 31:52.540]  отличающаяся на самом деле от пришественницы. Сейчас скажу какая. Вот такая вот случайная
[31:52.540 --> 32:01.020]  величина, на самом деле вот она и называется биномиальная. Называется биномиальная
[32:01.020 --> 32:22.060]  случайная величина с параметрами n и p. Ну понятно, что такое параметры, а смысл очень простой. Вместо
[32:22.060 --> 32:27.140]  нет, можно говорить про пьяницу, конечно. В каком смысле можно говорить и про пьяницу,
[32:27.140 --> 32:34.460]  которая по наклонной двигается, которая на горе. Да, но тут видите я немножко по-другому, 0 и 1,
[32:34.460 --> 32:41.300]  но можно из этого что-нибудь по вычитать. Нам все равно сейчас вычитать придется. Среднее у него
[32:41.300 --> 32:46.500]  не в кабаке, понимаете. Вот если вот такую величину брать, то среднее в точке n умножить на p,
[32:46.500 --> 32:53.540]  правильно? Среднее в точке n умножить на p. Вот, ну можно вычесть это n умноженное на p,
[32:53.540 --> 33:00.540]  тогда будет пьяница как раз со среднем в кабаке. То есть вот если из это n-ого вычесть np,
[33:00.540 --> 33:08.700]  то это фактически пьяница, которая двигается по наклону. Давай. np это мат ожидания, это n.
[33:08.700 --> 33:16.420]  А мат ожидания вот такой величины это 0, то есть мы возвращаемся к истории про пьяницу. Но сама
[33:16.420 --> 33:22.260]  исходная величина называется биномиальная, я просто расскажу как бы заново то, что вам и так расскажут,
[33:22.260 --> 33:35.700]  конечно, но вот применительно к нашей ситуации. Обозначается это вот так, достаточно традиционно,
[33:35.700 --> 33:41.460]  ну может вам будут писать просто b от np или bin от np, но я пишу bin, чтобы было прям видно,
[33:42.260 --> 33:51.100]  так друзья, почему она так называется? Какие значения принимает такая величина на каком-то
[33:51.100 --> 33:58.980]  там своем омега? Какие она значения принимает? Ну понятно 0, 1 и так далее n. Это все значения,
[33:58.980 --> 34:12.580]  какие она может принимать. Ну вот, чтобы это был пьяница, нужно писать 1-p, а тут
[34:12.580 --> 34:20.460]  minus p. Вот здесь писать 1-p, а тут minus p. Тогда в сумме как раз получится вот то, что здесь написано.
[34:20.460 --> 34:30.460]  Ну не важно, в общем, вот есть такая случайная величина. Она принимает значения от 0 до
[34:30.460 --> 34:40.540]  какой вероятностью она принимает свое конкретное значение k? Ну надо, чтобы k раз выпало 1,
[34:40.540 --> 34:51.900]  cn-k раз выпало 0. Ну конечно, да, c и cn пока на p вкатый на q в n-катый. Я просто поясняю,
[34:51.900 --> 35:01.340]  почему она называется биномиальной. Binom это сумма всех вероятностей и ее значений. Ну binom
[35:01.340 --> 35:09.100]  p-q в n-й степени равны единице. Что конечно и должно быть, но должна сумма вероятностей равняться
[35:09.100 --> 35:25.180]  единице. Это понятно. Ну вот. Значит, аналог теоремы, какую мы там доказали, это утверждение о том,
[35:25.180 --> 35:34.220]  что вероятность, с которой это n-np больше либо равняется а, не превосходит е в степени
[35:34.220 --> 35:43.900]  минус 2а квадратная. Двойка прыгнула из знаменателя в числитель. Вот эта вот двойка,
[35:43.900 --> 35:59.980]  тут она в знаменателе, а тут она в числитель. Ну, не зависит. От p не зависит.
[36:04.220 --> 36:26.460]  Что будет? Чем плохо p1? Пока мне не очевидно. Ну, в общем, это можно доказать, но это довольно
[36:26.460 --> 36:33.940]  такая рутинная деятельность. Ну, я бы оставил это без доказательства, это просто аналог вот
[36:33.940 --> 36:39.380]  этого. Чуть более сложными аналитическими выкладками. То есть если тут красиво очень,
[36:39.380 --> 36:44.540]  прям вот вся идея видна на трех строчках, то там надо повозиться подольше, там строчек шесть.
[36:44.540 --> 36:51.300]  Надо еще какие-то мерзкие аналитические неравенства писать. На поля не уместятся,
[36:51.300 --> 36:55.660]  а мне кажется, ну что я вас буду перегружать техникой? Смысл-то уже понятен.
[36:56.620 --> 37:03.820]  Похожая величина тоже уклоняется от среднего с очень маленькой вероятностью. Такой вот хороший
[37:03.820 --> 37:11.500]  результат. Так, ну все, я готов, товарищи, доказывать вот эту теорию, наконец-то.
[37:11.500 --> 37:23.020]  Ну, я надеюсь, вы тоже готовы воспринимать. Никого не кокнул, там биномбат, все понятно. Ничего
[37:23.020 --> 37:52.660]  сложного. Так, сейчас нам надо кое-что придумать. Доказательство пошло вот той теории.
[37:52.660 --> 37:59.700]  Сейчас мы кое-что придумаем. Вот пусть нам дан какой-то конкретный граф G.
[37:59.700 --> 38:08.420]  Ну, у нас множество вершин, это будет, конечно, набор чисел от единицы до N,
[38:08.420 --> 38:19.060]  что мы работаем с обычным случайным графом, наверное, на вершинах. Ну, давайте запустим
[38:19.060 --> 38:28.460]  на этом графе некий процесс. Я очень стараюсь, во-первых, сделать так, чтобы не запутаться,
[38:28.460 --> 38:34.820]  потому что я не повторил, но я вроде помню. А во-вторых, чтобы вас не кокнуть. Ну вот,
[38:34.820 --> 38:39.820]  но я вроде помню. Рассуждение, оно не очень сложное. Оно очень красивое, на самом деле. Надо
[38:39.820 --> 38:46.260]  некий процесс запустить на графе, и тогда все-все получится. Ну, давайте я нарисую какой-нибудь
[38:46.260 --> 38:58.900]  конкретный пример графа, неважно. Вот есть какой-то граф. Заметьте, я его нарисовал несвязанным,
[38:58.900 --> 39:04.380]  потому что у нас, как правило, несвязанные графы будут получаться в рамках нашего пункта. Поэтому
[39:04.380 --> 39:11.340]  я для примера нарисовал несвязанный граф. Теперь я беру какую-то его вершину в качестве стартовой.
[39:11.340 --> 39:20.660]  Вот какая-то его вершина. Я в каком-то смысле запускаю такой процесс рождения и гибели. То есть
[39:20.660 --> 39:30.900]  я интерпретирую эту вершину как прародительницу некоего рода. Я понятно говорю, да? Ну, наверное,
[39:30.900 --> 39:34.620]  записывать невозможно, но вот это не надо записывать, потому что если вы все это будете
[39:34.620 --> 39:41.460]  записывать в тетрадку, ну у вас трактат получится. Сейчас вы увидите, я совершенно формально опишу
[39:41.460 --> 39:48.020]  этот процесс. Вам главное понять, чего происходит, ну и мне тоже не запутаться. Вот есть прародительница
[39:48.020 --> 39:56.100]  рода. Она интерпретируется как единственная живая в начале всех времен. Вот давайте в каждый
[39:56.100 --> 40:03.060]  момент времени через у с индексом t, t это дискретное время, обозначать число живых вершин.
[40:03.060 --> 40:22.540]  Число живых вершин. Ну, то есть вот y0 равно единице, потому что у нас в начале всех времен есть вот
[40:22.540 --> 40:30.820]  эта вершина v, и она единственная живая. Все остальные вершины мы называем нейтральными.
[40:30.820 --> 40:38.420]  Пока что. Вот когда у нас есть только одна живая вершина, все остальные, давайте,
[40:38.420 --> 40:51.940]  число нейтральных вершин обозначать n с индексом t. Число нейтральных вершин в момент времени t,
[40:51.940 --> 41:02.620]  ну то есть n1, ну или n0. Давайте, пусть будет nt плюс один, что ли. Мне почему-то так хочется.
[41:02.620 --> 41:10.860]  n1 равняется единице. Тут время стартуется единице, тут время стартуется нуля. Н1, ну или с нуля
[41:10.860 --> 41:17.820]  все стартовать. Ладно, попробуем с нуля, вот не запутаюсь. Н0, ой, только не единица, а n-1, конечно.
[41:17.820 --> 41:27.220]  Да, вы, наверное, на это смотрите. Нет, тогда я n1 напишу. Все, хочу n1. Н0, ну ладно. Ладно,
[41:27.220 --> 41:32.780]  время одинаковое. Все, все, давайте время одинаковое. Н0 равняется n-1, а вот все получится. Давайте,
[41:32.780 --> 41:41.580]  н0 равняется n-1. Ну или я nt плюс один напишу, тогда все будет хорошо. Если я тут nt плюс один
[41:41.580 --> 41:47.740]  напишу, тогда все будет хорошо. t равно нулю будет n1. Ладно, все, все, пусть будет так. Не хочу никого путать, пусть
[41:47.740 --> 41:59.420]  будет так. Значит, y равно 1, n0 равно n-1. Вот, теперь вот эта вот единственная живая вершина порождает
[41:59.420 --> 42:04.540]  потомков. Ну, товарищи, я думаю, совершенно понятно, кого она порождает. Она порождает
[42:04.540 --> 42:10.860]  множество своих соседей. То есть потомки этой вершины – это те вершины, которые соединены с
[42:10.860 --> 42:23.380]  ней ребрами. Так, вот я прямо на этом рисунке изображаю. Она порождает потомков и умирает.
[42:23.380 --> 42:37.220]  Да, и умирает. Теперь смотрите, у нас у2. Так, внимание, товарищи, чтобы было просто все понятно.
[42:37.220 --> 42:52.580]  у2 равняется 2. Ой, у2, у1. Так, друзья, нам бы, конечно, успеть времени не так много,
[42:52.580 --> 42:56.100]  не хотелось бы переносить на следующую лекцию завершения доказательств. Давайте
[42:56.100 --> 43:06.260]  сосредоточимся. Согласны, да, живых – это вот ее потомки. Она померла. Да, да, да, да, да. То есть,
[43:06.260 --> 43:11.460]  смотрите, у меня пока граф не случайный. Я нарисовал совершенно конкретный граф, чтобы было абсолютно
[43:11.460 --> 43:18.420]  понятно, как устроен процесс на конкретном графе. Нет, воскресать она не умеет, к сожалению. Все, вот
[43:18.420 --> 43:30.980]  она умерла и умерла. Значит, теперь у1 у нас равняется 2, а n1 чему равняется? 1, конечно. Дальше мы
[43:30.980 --> 43:38.860]  совершенно наугад, без разницы, обзываем, например, вот эту вершину живой. Вернее, не обзываем, вот они
[43:38.860 --> 43:46.060]  две живые. Берем вот эту вершину и смотрим на ее потомство. Не есть потомство? Нету, потому что
[43:46.060 --> 43:51.860]  потомство выбирается, внимание, товарищи, только из нейтральных всегда. Потомство выбирается из
[43:51.860 --> 44:08.620]  нейтральных. Это живая, поэтому никак. Ну, да, виноват. n1 равно 4. Согласен, да, да, да. Забыли мы про
[44:08.620 --> 44:13.700]  эту компоненту, но мы-то ее видим, а компьютер не видит. То есть, вот компьютер на вход поступил
[44:14.060 --> 44:19.420]  и он действительно не понимает. Это отдельная компонента, не отдельная. Пока просто 4. Все правильно,
[44:19.420 --> 44:27.380]  извините. Конечно, 4. Тем не менее, если мы выбрали вот эту вершину как продолжательницу рода, то она
[44:27.380 --> 44:45.620]  умирает, к сожалению, ничего не породив. Зря появилась на свет. Так, идем дальше, друзья. Кто у
[44:45.620 --> 44:54.260]  нас остался в живых? Вот эта подруга. Она порождает. Ну, можно я уже Y писать не буду? Дальше все понятно.
[44:55.260 --> 45:02.660]  Она порождает, помирает, остается живая. Да, у нее там нейтральных 3, но фиг она до них дотянется,
[45:02.660 --> 45:11.580]  поэтому она тоже помирает. И в этот момент Y с каким-то индексом становится равным нулю. Ну,
[45:11.580 --> 45:22.580]  понятно, с каким индексом. Если Y0 равно 1, там Y1 равно 2, Y2 равно тоже 2, Y3 равно 1.
[45:22.580 --> 45:34.380]  Y4 равно 0. Все. Род выродился. В момент времени 4. Что такое, товарищи, 4? Это количество вершин
[45:34.380 --> 45:46.500]  вот в этой компоненте связности. Да, одну из живых, любую совершенно, неважно в каком порядке их
[45:46.500 --> 45:52.220]  выбирать, в конце концов процесс остановится, и остановится он в тот момент времени, когда мы
[45:52.220 --> 46:00.540]  наберем вот ровно эту компоненту связности, в которую входила исходная вершина. Так, друзья,
[46:00.540 --> 46:05.780]  но мне казалось, что вот на этом примере очень хорошо. Не на этом конкретном, я его сейчас выдумал
[46:05.780 --> 46:10.420]  из головы, но если вы берете какой-то конкретный пример, то совершенно понятно, как идет процесс.
[46:10.420 --> 46:20.060]  Теперь мы можем считать, друзья, внимание, вот самый главный момент, что граф случайен, и этот
[46:20.060 --> 46:29.440]  процесс тоже состоит не из конкретных чисел, а из случайных величин. Вот эти все Y, N, это
[46:29.440 --> 46:36.060]  случайные величины. Конечно, зависимые, да, да, но вот мы сейчас разберемся, на самом деле все очень
[46:36.260 --> 46:47.540]  неплохо. То, что зависимые, безусловно. Значит, давайте Y с индексом T. Во-первых, можно представить
[46:47.540 --> 46:56.900]  вот так. Это Y с индексом T минус 1. Это сколько было живых на предыдущий момент времени. Может,
[46:56.980 --> 47:07.220]  придется переносить. Посмотрим. Так, что надо прибавить? Ну, надо прибавить число потомков вершины,
[47:07.220 --> 47:14.020]  которую мы выбрали как живую. То есть число ее соседей среди нейтральных. Давайте вот это
[47:14.020 --> 47:22.380]  число обозначим просто Z с индексом T. То есть это число соседей очередной живой вершины среди
[47:22.540 --> 47:34.060]  нейтральных на этот момент времени. Можно я не буду писать, а просто скажу. Так, ну и надо
[47:34.060 --> 47:38.700]  вычислить единицу, потому что умерла вот эта вот вершина, которая столько вершин породила.
[47:38.700 --> 47:47.860]  Вообще, в науке в теории вероятности в случайных процессах называется ветвящимся процессом,
[47:47.860 --> 47:55.500]  когда каждая очередная частица что-то порождает, сама, допустим, умирает, и дальше идет вот этот
[47:55.500 --> 48:06.660]  процесс размножения. Такой ветвящийся процесс размножения. Так, что еще мы можем сказать? Как
[48:06.660 --> 48:14.220]  ZT, ну скажем, так распределена. Вот, смотрите, вот, допустим, у нас была в начале одна вершина.
[48:21.300 --> 48:29.660]  Выражение случайная величина имеет распределение. Вас не смущает? Ну, случайная
[48:29.660 --> 48:34.620]  величина имеет распределение, это значит, мы знаем с какими вероятностями она принимает
[48:34.620 --> 48:41.340]  свои значения. Вот, например, она имеет биномиальное распределение, если вероятности всех ее значений
[48:41.340 --> 48:47.300]  вот такие. То есть, если значения это целые числа от нуля до n, и их вероятности выражают с какой-то
[48:47.300 --> 48:53.900]  такой формы. Вот, может, конечно, да, но я утверждаю, что ZT тоже биномиальный.
[48:53.900 --> 49:02.940]  Но с каким параметром? С какими параметрами? Я же не зря старался, бином, писал вам, товарищи.
[49:02.940 --> 49:08.340]  Сейчас он появится во всей красе. Я утверждаю, что она имеет биномиальное распределение.
[49:08.340 --> 49:18.860]  С какими параметрами? Ну, давайте посмотрим, Z1, она на ком распределена? Она распределена на n0,
[49:18.860 --> 49:30.220]  то есть, тут надо писать тогда nt-1 и p. Но, смотрите, вот, если здесь единичка, то тут n0,
[49:30.220 --> 49:38.700]  которая равно n-1. Ну, почему я так пишу? Потому что вот у меня одна живая вершина в случайном
[49:38.700 --> 49:45.460]  графе, в случайном, не вот в этом конкретном, а в случайном графе. Она присоединяется к каждой из
[49:45.460 --> 49:53.620]  оставшихся n-1 вершин с вероятностью p и не присоединяется с вероятностью q. То есть, мы
[49:53.620 --> 50:02.300]  n-1 раз бросаем вот такую монетку. 1 проводим ребро, 0 не проводим ребро.
[50:09.740 --> 50:15.220]  Друзья, давайте вот это осознаем, понятно или не очень? Ну, конечно, это при условии,
[50:15.220 --> 50:23.460]  что мы знаем, чему равняется nt-1, то есть, конечно, это зависимые величины. Если нам nt-1 до
[50:23.460 --> 50:29.620]  no уже, на каком-то графе оно реализовалось, то zt имеет биномиальное распределение вот с такими
[50:29.620 --> 50:37.820]  параметрами. Очередное число потомков, мы этих потомков выбираем только из нейтральных вершин,
[50:37.820 --> 50:43.980]  которые были на предыдущем шаге. И каждое из них мы выбираем с вероятностью p, потому что граф
[50:43.980 --> 50:52.060]  случайный, не конкретный, а случайный. Каждое ребро в нем возникает с вероятностью p. Сейчас,
[50:52.060 --> 51:02.060]  друзья, вот это точно понятно? Умел я как-то объяснить? Хорошо, теперь давайте еще одну рекурсию
[51:02.060 --> 51:11.820]  напишем последнюю. Значит, как выражается n с индексом t? Ну, наверное, надо что сделать? Надо
[51:11.820 --> 51:24.500]  из общего числа вершин вычесть количество живых в тот же самый момент времени и вычесть еще что-то.
[51:24.500 --> 51:40.380]  Что еще надо вычесть? Количество мертвых, правильно. Ну, то есть t. Например, в нулевой
[51:40.380 --> 51:50.260]  момент времени это n-1, n-y0, ну и минус 0. Первый момент времени это n-y1-1, вроде,
[51:50.260 --> 51:58.380]  правильно. Вот, я думаю, что этого нам хватит. Черт, наверное, все-таки не успеем. Ну, ничего страшного,
[51:58.380 --> 52:03.820]  значит, закончим в следующий раз. Главное, чтобы вы все осознали, потому что, видите, вы немножко
[52:03.820 --> 52:09.220]  не знаете еще вот этих всех вещей, связанных со распределением, мне приходится немножко напоминать.
[52:09.220 --> 52:16.420]  Ну, ничего страшного, тут все просто. Так, значит, ну давайте леммой, что ли, это обзовем.
[52:16.420 --> 52:30.740]  Я утверждаю, что yt имеет вот такое замечательное распределение. О, я придумал, как сделать так,
[52:30.740 --> 52:38.420]  чтобы успеть хотя бы такую содержательную часть, сейчас увидите, катартическую, катарсисную.
[52:39.140 --> 52:42.620]  Катартическую это как-то неоднозначно звучит.
[52:50.580 --> 53:01.900]  Во, во, во. Ну, лучше не тильду, наверное, рисовать, лучше написать вот так, чтобы понять не было.
[53:01.900 --> 53:09.340]  Имеется в виду, что yt вкладывается как сумма фиксированной части, ну, конечно, от t зависящей,
[53:09.340 --> 53:15.780]  но при данном t фиксированной, и случайной. А вот эта случайная часть имеет вот такое
[53:15.780 --> 53:22.100]  биномиальное распределение. То есть n-1 раз бросаем монетку, и вот такая хитрая вероятность успеха.
[53:22.100 --> 53:33.420]  Само утверждение чисто формально понятно. Я утверждаю, что число живых вершин в момент времени t,
[53:33.420 --> 53:40.660]  это случайная величина, конечно, она зависит от графа, это случайное число, но его можно получить
[53:40.660 --> 53:47.220]  так. Взять вот это вот фиксированное число и к нему прибавить число раз, когда вот эта
[53:47.220 --> 53:52.740]  монетка выпадает единицей кверху. Монетка, это вот мы бросаем монетку, либо она падает решкой,
[53:52.740 --> 53:58.340]  с вероятностью p добавляется 1, либо она падает орлом, с вероятностью q добавляется 0.
[53:58.340 --> 54:06.660]  Ну, наверное, вы знаете эту интерпретацию с монетками, да? Монетку бросаем n раз.
[54:06.660 --> 54:14.540]  Если в очередном бросании решка, добавляем единичку. Нет? Ну, не добавляем ничего.
[54:14.540 --> 54:21.100]  Поэтому я говорю вот в терминах монеток. Это просто утверждение, мы его докажем,
[54:21.100 --> 54:28.140]  но похоже, что мы его докажем в следующий раз. Ну, просто потому что, если я сейчас в это закопаюсь,
[54:28.140 --> 54:33.940]  вы катарсис сегодня не испытаете и в следующий раз вы уже вообще забудете. Поэтому я предлагаю
[54:33.940 --> 54:41.380]  пока отложить доказательства этого факта и воспользоваться ими. Хорошо? Сейчас мы применим
[54:41.380 --> 54:54.060]  эту лимму для завершения доказательства теории. Итак, смотрите, вероятность того, что y,
[54:54.060 --> 55:11.140]  t больше нуля, ну, то есть процесс еще не прервался. Так, помните, что когда y,
[55:11.140 --> 55:20.980]  t обратится в ноль, это мы исчерпали всю компоненту для вершины, с которой стартовали. Хорошо, да? Да,
[55:20.980 --> 55:26.540]  друзья, заметьте, вершину мы зафиксировали. Когда мы вот в общем случае вот это все писали,
[55:26.540 --> 55:32.180]  мы зафиксировали какую-то вершину, например, один. Давайте считать, что мы зафиксировали один.
[55:32.180 --> 55:39.940]  Дальше пошел вот этот процесс. Вероятность того, что y, t больше нуля, это, конечно, вероятность того,
[55:39.940 --> 55:55.380]  что binom от этих параметров, да что ж такое, n минус 1, 1 минус больше, чем t минус 1. Ну,
[55:55.380 --> 56:05.460]  перенес сразу вправо вот эту 1 минус t. Понимаете? Тут вроде все просто. Если поверить в справедливость
[56:05.460 --> 56:09.900]  леммы, то вот так. Ну, лемму еще надо будет доказать, но это в следующий раз. Значит,
[56:09.900 --> 56:23.420]  дайте я не буду переписывать, сварить какая ловкость рук. Ну, было вот так, а стало вот так. Ну,
[56:23.420 --> 56:29.220]  это правда, потому что t это целое число. Быть строго больше, чем t минус 1, это то же самое,
[56:29.220 --> 56:36.420]  чтобы быть больше либо равном t. Ну, тут такая ловкость рук невеликая. Но это, в общем,
[56:36.420 --> 56:48.500]  ладно, это не очень интересно. Теперь смотрите. 1 минус 1 минус p в степени t меньше либо равняется
[56:48.500 --> 57:01.060]  pt, кажись. Ну, 1 минус p в степени t больше либо равняется 1 минус pt. Знаем такое неравенство?
[57:01.060 --> 57:10.180]  Ну, bernoulli, наверное, оно называется, но вроде бы это ясно просто. Ну, из binomo сразу следует.
[57:10.180 --> 57:18.340]  Вот. Меньше либо равняется pt. Ну, а n минус 1 меньше, чем n. Это как-то совсем очевидно,
[57:18.340 --> 57:23.540]  вот то, что n минус 1 меньше, чем n. Я утверждаю, что из этого, конечно, следует следующее неравенство.
[57:23.540 --> 57:32.420]  Давайте вот так сотру. Вот так. Меньше либо равно, интересующая нас вероятность, не больше,
[57:32.420 --> 57:43.140]  если я такую врезку сделаю, просто пояснение, не больше, чем вероятность того, что binom от n
[57:43.140 --> 57:56.140]  pt больше либо равняется t. Но я заменил число бросаний монетки на большее и вероятность заменил
[57:56.140 --> 58:04.420]  на большую величину вероятность единицы. Но, разумеется, суммарное число единиц большое в
[58:04.420 --> 58:12.700]  этом случае с большей вероятностью, чем в этом. Если я и бросаний увеличил количество, и вероятность
[58:12.700 --> 58:21.260]  успеха в каждом бросании тоже увеличил вероятность единички. Согласны? Но я могу строго меньше даже
[58:21.260 --> 58:27.340]  написать. Тоже будет правда, но неважно. Так, друзья, слушайте, мы, по-моему, пришли, наконец,
[58:27.340 --> 58:40.820]  вот к этой теории. Смотрите, это с индексом n. Это вот она прямо, только тут p, а не ppt. Вот тут
[58:40.820 --> 58:51.140]  у нас p. Где? Где? Вот тут p. Вот тут p. А тут у нас вместо p, p умножить на t. Ну какая разница? Сейчас
[58:51.140 --> 59:05.820]  применим. Ну, надо переписать. Надо переписать вот так. Значит binom от npt минус npt больше либо
[59:05.820 --> 59:13.260]  равняется t минус npt. Я просто лево и справа вычил npt, чтобы подогнать под ту теорему,
[59:13.260 --> 59:28.860]  которая вот там нарисована. Видите, да, друзья? Видно. Теперь давайте вот на это посмотрим. У нас
[59:28.860 --> 59:39.140]  пет какое. Вон там оно. C поделить на n. Сейчас катарсис будет. Значит, что такое npt? Вот давайте
[59:39.140 --> 59:51.260]  я так подчеркну. Что такое npt? Это n. C делить на n на t. Это ct, правильно? А c у нас, туда,
[59:51.260 --> 01:00:02.940]  видите, да, меньше единицы. То есть вот так число t минус ct, оно больше нуля. Вот можно применить то,
[01:00:02.940 --> 01:00:13.500]  что написано в теореме. Что это не превосходит e в какой степени минус, а не влезет сейчас.
[01:00:13.500 --> 01:00:27.260]  Ну давайте вот сюда. Не превосходит e в степени минус. Так, 2a квадрат. Это t минус ct квадрате
[01:00:27.260 --> 01:00:51.620]  поделить на n. Что такое n? Плохо. Сейчас. Я понял, где я ошибся. Я хотел вот в эту теорему подставить,
[01:00:51.620 --> 01:01:10.580]  черт, я вот здесь наврал. Я понял. Наврал. Вот вы правильно меня спросили, а я все-таки
[01:01:10.580 --> 01:01:20.660]  наврал. Я так уперся, говорю, что от p не зависит, а от p зависит. Ах ты Господи. Так, сейчас, или это неважно.
[01:01:20.660 --> 01:01:46.060]  Сейчас. Значит, секунду. А, так это все равно. А, все-все-все, да, это правильно. Ну ладно,
[01:01:46.060 --> 01:01:52.220]  тогда может ничего страшного. Не, правильная теорема. Уперся-то нормально. Значит, смотрите,
[01:01:52.220 --> 01:01:58.180]  давайте я чуть прокомментирую, особенно для тех, кто там в записи будет слушать. Я сначала подумал,
[01:01:58.180 --> 01:02:04.100]  что все-таки здесь стоит сделать зависимость справа от p. Содержательная зависимость должна
[01:02:04.100 --> 01:02:10.860]  быть устроена чисто интуитивно. Это понятно так. Вот здесь вместо n написать дисперсию вот этой
[01:02:10.860 --> 01:02:17.900]  вот величины. Ну дисперсия это n, это известно, она равна npq. Я думаю, ну дай я отсюда запишу
[01:02:17.900 --> 01:02:25.660]  npq. Ну можно, конечно, написать npq, но дело в том, что npq, очевидно, меньше, чем n, дробь больше,
[01:02:25.660 --> 01:02:30.900]  чем если я сюда подставлю n, а со знаком минус, ну получается как раз то, что я здесь написал.
[01:02:30.900 --> 01:02:37.020]  Причем p, умноженное на q еще можно заменить на одну четверть. Вот у вас тут будет четверка,
[01:02:37.020 --> 01:02:44.100]  поделенная пополам, как в той теореме, которую мы доказали, и будет как раз 2a2n. Я заодно вспомнил,
[01:02:44.100 --> 01:02:49.100]  как доказывать эту теорему, потому что ну я как бы не собирался вам ее доказывать и не вспоминал,
[01:02:49.100 --> 01:02:54.060]  как ее доказывать. А доказывать ее по сути надо именно так. То есть вот здесь будет e в степени
[01:02:54.060 --> 01:03:02.020]  минус a2 на npq и на 2. Ну а поскольку p и q у нас одна-вторая в худшем случае, то будет как раз
[01:03:02.020 --> 01:03:06.820]  два квадрата подережная. Короче, это правильный результат. Короче, это правильный результат,
[01:03:06.820 --> 01:03:17.340]  ничего я тут вам не наврал, это я просто зарапортовался немножко. А оно вот здесь важно,
[01:03:17.340 --> 01:03:23.100]  на самом деле, потому что больше чего-то вот, ну слушайте, давайте вот это надо написать для
[01:03:23.100 --> 01:03:29.300]  любого а больше нуля, конечно. Это иначе просто очевидно неверно, посмотрите, потому что если вы
[01:03:29.300 --> 01:03:36.660]  возьмете отрицательное какое-то а, то это у вас получается просто некая величина лежит на правой
[01:03:36.660 --> 01:03:45.700]  половине прямой. Это вероятность высокая. Надо уйти вправо от среднего, ну там больше либо равняется,
[01:03:45.700 --> 01:04:01.300]  наверное, ну да, да, да, да, да, да, да, да, да, да, да, да, то есть вероятность вот это превратится
[01:04:01.300 --> 01:04:07.260]  просто в единицу, если тут брать отрицательные числа, а в конце концов превратится в единицу. Ну
[01:04:07.260 --> 01:04:12.500]  слушайте, ну уйти от, вот если мы находимся не в этой ситуации, а в простейшей, когда кабак в нуле,
[01:04:12.500 --> 01:04:21.900]  уйти вправо на расстояние отрицательное, ну вообще невозможно, а наоборот оказаться больше,
[01:04:21.900 --> 01:04:31.620]  чем минус один, ну это вероятность один. В этом смысл, понимаете, вот уйти от кабака так,
[01:04:31.620 --> 01:04:41.700]  чтобы оказаться правее точки минус один, вот поэтому тут важно, конечно, чтобы она была больше
[01:04:41.700 --> 01:04:47.860]  нуля, вот здесь мы используем очень по существу 100с меньше единицы, ну я могу здесь написать на n,
[01:04:47.860 --> 01:04:57.740]  давайте сообразим тогда, что у меня получится, значит, неужели я все равно не успею, ну что ж такое,
[01:04:57.740 --> 01:05:20.700]  очень долго рассуждаю, так, ой, черт возьми, все-таки мне хочется как-то это, почему,
[01:05:20.700 --> 01:05:32.140]  что ж такое-то, я хочу, почему-то мне все время хочется вот здесь это npq написать,
[01:05:32.140 --> 01:05:39.020]  вот в этом месте все-таки я запутался, черт возьми, npq хочется написать, чтобы катарсис-то получился,
[01:05:39.020 --> 01:05:45.540]  времени осталось мало, черт, я не знаю, что ж такое, то есть я хотел, чтобы здесь получилась величина
[01:05:45.540 --> 01:05:54.420]  порядка t, тогда вот это t квадрат с t сократится до t, и это будет то, что мне нужно, я вот что-то
[01:05:54.420 --> 01:06:03.940]  сейчас не соображу, почему я могу n заменить на t, или это очевидно, подождите, наверное очевидно,
[01:06:03.940 --> 01:06:20.740]  подождите, подождите, а? чего хорошего-то, не, ну подождите, n больше чем t это понятно,
[01:06:20.740 --> 01:06:37.380]  но знак неравенства будет не в ту сторону, что-что, что такое сейчас, ай-яй-яй-яй-яй-яй,
[01:06:37.500 --> 01:06:51.140]  ой-яй-яй, как нехорошо, ой, что ж ты будешь делать, ай-яй-яй, почти все получилось-то, так, смотрите,
[01:06:51.140 --> 01:07:03.100]  вот еще раз, я-то чего хочу сказать, что это вот такая вот величина на два npq, ну, конечно,
[01:07:03.100 --> 01:07:12.700]  npq не превосходит, ой, n поделить на 4, поэтому тут верно, что отсюда следует вот это, да,
[01:07:12.700 --> 01:07:20.500]  поскольку npq не превосходит n поделить на 4, p у нас, в худшем случае, одна вторая, то, конечно,
[01:07:20.500 --> 01:07:26.740]  вот это следует, применить я все-таки хочу вот это, знаете почему, потому что p у меня сейчас это
[01:07:26.740 --> 01:07:33.620]  не константа, как одна вторая, а c поделить на n, и это, конечно, мне очень сильно сейчас улучшит жизнь,
[01:07:33.620 --> 01:07:42.260]  понимаете, одно дело грубо оценить p, умноженное на q, как 1 четверть сверху и получить вот это
[01:07:42.260 --> 01:07:48.500]  общее неравенство, которое я пытался применить, и совсем другое, при нашем конкретном p равном
[01:07:48.500 --> 01:07:54.740]  c поделить на n, применить вот это неравенство, оно гораздо сильнее, чем то, что сверху, следствие
[01:07:54.740 --> 01:08:00.380]  это правильное, я его держал в голове, вот что значит не повторить, готовьтесь к лекциям, товарищи,
[01:08:00.380 --> 01:08:06.340]  и к экзамену, вот, понимаете, это правильное утверждение, но мне нужно вот это, оно тоже правильное,
[01:08:06.340 --> 01:08:14.540]  вот мне нужно вот это, к сожалению, для меня, что я затянул, вот, поэтому я здесь пишу не n,
[01:08:14.540 --> 01:08:26.700]  а все-таки 2pq, что такое pq, 2npq, значит, np это c,
[01:08:26.700 --> 01:08:49.660]  да что такое-то, а, все-все-все, вот у меня p, это же pt, да-да-да, простите, пожалуйста, да,
[01:08:49.660 --> 01:08:57.060]  вот pt, конечно, вероятность успеха-то это p умножить на t, то есть это опять вот это npt,
[01:08:57.060 --> 01:09:10.580]  вот это npq, которое здесь, ой, позорище, это 2npt, это np я написал, и еще умноженное на q,
[01:09:10.580 --> 01:09:21.060]  ну, то есть на 1-pt, на 1-pt, вот так вот, это все в знаменателе, тут вот, да, вот эту двойку,
[01:09:21.060 --> 01:09:27.620]  вот эту двойку, конечно, надо убрать, да, двойка теперь она в знаменателе, то есть вот это np,
[01:09:27.620 --> 01:09:39.940]  потому что p у меня, p умножить на t, а q это 1-pt, вот это вот n2npq, вот так вот, npq, ну, я надеюсь,
[01:09:39.940 --> 01:09:46.220]  что сейчас я выправился, ну, в том смысле, что я не запутал окончательно, нормально,
[01:09:46.220 --> 01:09:55.420]  понятно, что происходит, да, ой, какое счастье, слушайте, я еще кое-чем похваст, то есть 1-pt можно
[01:09:55.420 --> 01:10:02.260]  убрать, знаете почему, потому что он меньше единицы, в дроби он больше, а со знаком минус снова меньше,
[01:10:02.260 --> 01:10:08.900]  поэтому я его зачеркну, он мне не принесет никакой пользы, вот, короче говоря, у меня получается вот
[01:10:08.900 --> 01:10:14.660]  так, ну, чуть-чуть на пару минут задержу, извините, сейчас доведем до результата, значит, у меня
[01:10:14.660 --> 01:10:27.780]  получается вот так, равняется е, в какой степени, вот смотрите сюда, тут t квадрат, тут, так, npt это
[01:10:27.780 --> 01:10:39.500]  ct, вот npt это ct, вот так, npt это ct, в нашем специальном случае, поэтому мы t квадрат делим на t,
[01:10:39.500 --> 01:10:49.980]  у нас получается минус t, и это минус t, на что умножается, на 1 минус c в квадрате, на 1 минус
[01:10:49.980 --> 01:11:08.420]  c в квадрате, и делится просто пополам, да, и на c, да, на 2c, а теперь смотрите, берем t в виде бета на
[01:11:08.420 --> 01:11:14.460]  логориф men, ну, бета можете считать не констант, а там какая-то функция, так, чтобы это получилось
[01:11:14.460 --> 01:11:21.740]  целое число, или рисуете тут целую часть, это уже не важно, вот берете t в виде бета на логориф men,
[01:11:21.740 --> 01:11:32.700]  можно же так подобрать бета, чтобы это было меньше, чем 1 поделить на n в квадрате, ну как, тут е в
[01:11:32.700 --> 01:11:38.060]  степени минус логориф men, тут какая-то конкретная константа, она еще умножается на бета, просто
[01:11:38.060 --> 01:11:44.380]  компенсируем эту константу так, чтобы получилось 1 на n в квадрате, очевидно можно, да, добрать
[01:11:44.380 --> 01:11:50.700]  положительное бета, или не очевидно, но тут логориф men, е в степени минус логориф men, это 1n,
[01:11:50.700 --> 01:11:59.140]  но возводим ее так, чтобы получилось 1 на n в квадрате, так, теперь смотрите, это вероятность того,
[01:11:59.140 --> 01:12:06.820]  что стартуя, внимание, вот сейчас будет катарс, вероятность того, что стартуя с конкретной вершины,
[01:12:06.820 --> 01:12:17.900]  вы остановитесь позже, чем в момент времени бета логориф men, правильно, ну то есть, что в этот
[01:12:17.900 --> 01:12:27.540]  момент времени yt все еще будет больше нуля, все еще будет живая популяция, что момент остановки будет
[01:12:27.540 --> 01:12:34.660]  дальше, чем вот эта величина, вот вероятность настолько маленькая, теперь вероятность того,
[01:12:34.740 --> 01:12:45.060]  что существует вершина, существует вершина такая, что вот это вот выполнено, вот это вот yt,
[01:12:45.060 --> 01:12:55.900]  уй, yt больше нуля, вот, что существует такая вершина, но она меньше либо равна n поделить на n в
[01:12:55.900 --> 01:13:06.780]  квадрате, потому что вершина всего n, это объединение событий, нет, зависимые, зависимые,
[01:13:06.780 --> 01:13:11.380]  конечно, но это объединение, вероятность объединения не больше, чем сумма вероятностей,
[01:13:11.380 --> 01:13:26.740]  нет, нет, мы вообще запускаем независимо от каждой вершины, мы берем объединение просто событий,
[01:13:26.740 --> 01:13:32.380]  смотрите, мы берем одну вершину и смотрим множество графов, для которых стартуя с этой вершины,
[01:13:32.380 --> 01:13:37.300]  мы остановимся позже вот этого момента, потом все забываем, берем другую вершину,
[01:13:37.460 --> 01:13:43.580]  может быть даже из той же компоненты и снова все запускаем заново, то есть мы сильно мажорируем
[01:13:43.580 --> 01:13:51.820]  интересующую нас вероятность, мы на это не смотрим вообще, мы отдельно смотрим,
[01:13:51.820 --> 01:14:00.780]  что будет если запустить процесс отсюда, мы вообще про все забываем, еще раз, что такое событие,
[01:14:01.260 --> 01:14:08.060]  это множество графов, вот множество графов, для которых стартуя отсюда, мы остановимся
[01:14:08.060 --> 01:14:13.440]  позже, чем вот в этот момент времени, оно имеет вероятность меньше, чем один на n в квадрате,
[01:14:13.440 --> 01:14:19.560]  точно также множество графов, для которых стартуя от двойки и делая все то же самое,
[01:14:19.560 --> 01:14:24.220]  опять остановимся позже, чем в этот момент времени, оно естественно снова имеет вероятность
[01:14:24.220 --> 01:14:27.900]  меньше, чем один на n в квадрате, ну какая разница с какой вершиной начать,
[01:14:27.900 --> 01:14:34.540]  Вот, но я и говорю, что вероятность того, что тортуя хотя бы с одной вершины мы
[01:14:34.540 --> 01:14:39.860]  остановимся поздно, она маленькая, она меньше, чем n поделитель на n в квадрате, она стремится к нулю.
[01:14:39.860 --> 01:14:49.020]  Ну а значит, с вероятностью стремящейся к единице нет ни одной компоненты размера больше, чем
[01:14:49.020 --> 01:14:57.620]  battle-agorifmen. Всё. Вот это, что существует компонент размера больше, и эта вероятность стремится к нулю.
[01:14:57.620 --> 01:15:01.660]  Так, друзья, я вот в ту сторону не смотрю, понятно или не очень?
[01:15:01.660 --> 01:15:09.980]  Ну я готов ещё раз пояснить, да. Всё. Слушайте, я на 5 минут задержал,
[01:15:09.980 --> 01:15:11.900]  то свинство с моей стороны, прошу прощения.

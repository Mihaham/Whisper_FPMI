[00:00.000 --> 00:10.920]  Так, добрый день, всем ли меня слышно? На всякий случай спрашиваю. Всем хорошо слышно. Замечательно.
[00:10.920 --> 00:20.120]  Тогда давайте начинать. Извините за такую задержку. Значит, я на всякий случай напоминаю,
[00:20.120 --> 00:28.360]  что лекции по материалу этого семестра у нас начались в прошлом семестре. Правильно? Многочлены
[00:28.360 --> 00:35.880]  мы там уже обсудили. К счастью, они записаны, поэтому можно этот материал освежить. А я сейчас
[00:35.880 --> 00:42.640]  небольшое напоминание сделаю по поводу того, что в прошлом семестре рассказалось и что нам
[00:42.640 --> 00:49.680]  сейчас непосредственно будет нужно на всякий случай. И о многочленах, и о чём-то, что в материале
[00:49.680 --> 00:59.200]  прошлого семестра было. Итак, небольшая напоминалка. Во-первых, многочлены. Вот я не
[00:59.200 --> 01:10.000]  буду формальных всех определений давать, но я просто напомню, что если у нас F это поле,
[01:10.000 --> 01:27.920]  то у нас появляется кольцо многочленов. Каждый многочлен единственным образом записывается
[01:27.920 --> 01:40.160]  вот в таком вот виде, где PIT из F. Единственным образом с точностью до добавления нулевых слагаемых
[01:40.160 --> 01:45.600]  в начало, естественно. Правильно? То есть каждый многочлен единственным образом раскладывается
[01:45.600 --> 01:53.560]  в линейную комбинацию степеней вот этого самого загадочного X. Дальше я напоминаю, что у нас,
[01:53.560 --> 02:01.680]  понятно, раз у нас есть кольцо многочленов, то существует понятие делимости. И в частности
[02:01.680 --> 02:14.680]  существует понятие наибольшего общего делителя двух многочленов PQ, которое определено с точностью
[02:14.680 --> 02:26.560]  до умножения на ненулевой скаляр. На скаляр из F со звездой, правильно? И кроме того, этот нот
[02:26.560 --> 02:38.840]  линейно выражается через многочлены P и Q. P на S плюс Q на T, где S и T тоже многочлены над нашим полем.
[02:39.400 --> 02:47.400]  Так далее у нас существует основная теорема арифметики для многочленов, то есть каждый многочлен,
[02:47.400 --> 02:56.880]  давайте я скажу ненулевой, раскладывается на неприводимые сомножители.
[02:56.880 --> 03:15.360]  На неприводимые сомножители. Так давайте я лучше не P напишу, а скажем A. Гамма на P1 и так далее
[03:15.360 --> 03:28.600]  на ПКТ, где гамма это ненулевой скаляр, P и T неприводимые. И это разложение единственно с точностью
[03:28.600 --> 03:37.280]  до, понятно, перестановки сомножителей раз и домножение каких-то сомножителей на константы
[03:37.280 --> 03:47.320]  ненулевые, так чтобы в целом домножили на единицу. Такое разложение по существу единственное.
[03:47.320 --> 04:00.240]  Вот так вот давайте я скажу. Детали можно, еще раз говорю, вспомнить из хотя бы даже видео.
[04:00.240 --> 04:16.280]  Ну и третье, что мы знаем, что у многочленов существуют корни, это означает, что просто P от C равно 0,
[04:16.280 --> 04:27.200]  и это равносильно тому, что х-с делит наш многочлен. С это корень многочленов, равносильно тому,
[04:27.200 --> 04:41.680]  что х-с делит наш многочлен. Ну и соответственно, С будет корень кратности, давайте я так скажу,
[04:41.680 --> 04:48.520]  С корень кратности хотя бы К, если х-с вкатый делит наш многочлен. Ну а соответственно,
[04:48.520 --> 04:54.440]  кратность этого корня, это наибольшая такая К будет, для которого х-с вкатый делит многочлен.
[04:54.440 --> 05:02.280]  Ну и отсюда и из теоремы о единственности разложения на неприводимые множители следует,
[05:02.280 --> 05:17.840]  что количество корней с учетом кратности, то есть по сути дела сумма кратностей всех корней не превосходит
[05:17.840 --> 05:25.800]  в степени многочленов. Так, это необходимые воспоминания про многочлены. Многие из этих фактов
[05:25.800 --> 05:38.440]  мы знали и до этого, но возможно не над любым полем. И давайте я еще напомню некоторые
[05:38.440 --> 05:45.560]  сведения, связанные пролинейные операторы. Давайте я так скажу, пролинейные преобразования,
[05:45.560 --> 05:55.600]  потому что в этом семестре мы будем заниматься линейными преобразованиями, а не отображениями.
[05:55.600 --> 06:03.840]  Когда мы будем говорить про линейные операторы, чаще всего мы будем иметь в виду именно линейное
[06:03.840 --> 06:12.080]  преобразование. Ну во-первых, что такое линейное преобразование? Это отображение из линейного
[06:12.080 --> 06:25.240]  пространства в себя, которое при этом еще и линейно. Фет у плюс в это фет у плюс фет в,
[06:25.240 --> 06:45.200]  фет лямбда на у это лямбда на фет у. Если есть базис в, то немедленно появляется у каждого
[06:45.200 --> 06:57.080]  преобразования у каждого оператора его матрица размера n на n, в столбцах которой, напоминаю,
[06:57.080 --> 07:08.920]  если нужно, стоят координатные столбцы. Здесь вот у нас в столбцах этой матрицы это координатные
[07:08.920 --> 07:22.800]  столбцы, фет е1, фет е2 и так далее. Ну и если у нас вектор в имеет координатный столбец х в нашем
[07:22.800 --> 07:30.840]  базисе, то фет в будет иметь координатный столбец а на х в этом же самом базисе. Базис 1 и тот же,
[07:30.840 --> 07:38.160]  потому что, напоминаю, мы у преобразования как бы и в этом и в этом пространстве выбираем один
[07:38.160 --> 07:44.280]  и тот же базис. Мы берем фе-образы базисных векторов и расписываем их по этому же самому
[07:44.280 --> 07:51.920]  базису в случае преобразования. То есть наше линейное преобразование заключается в координатной
[07:51.920 --> 08:04.800]  форме, просто в умножении на нашу матрицу А. Если базис у нас будет меняться, если у нас есть
[08:04.800 --> 08:14.960]  замена базиса, то матрица, естественно, тоже будет меняться. Кто мне напомнит, кстати? Вот я
[08:14.960 --> 08:26.400]  напоминаю, давайте. С-1 на А на С, правильно? У нас оба базиса в обоих пространствах меняются,
[08:26.400 --> 08:35.720]  поэтому вот таким вот образом у нас происходит замена матрицы нашего преобразования. И тут,
[08:35.720 --> 08:41.760]  давайте я сразу сделаю что-нибудь, скажу что-нибудь новенькое, я скажу определение,
[08:41.760 --> 08:56.520]  что вот такие вот матрицы называются подобными. Матрицы А и А' одного и того же размера n на n
[08:56.520 --> 09:08.800]  над полем называются подобными. Если существует такая матрица замены базиса, то есть, напоминаю,
[09:08.800 --> 09:16.480]  невырожденная матрица S. Так, невырожденность, напоминаю, мы записываем вот таким вот образом.
[09:16.480 --> 09:23.760]  GLn от f это как раз множество, а точнее, группа всех невырожденных, они же обратимые, матриц размера n
[09:23.760 --> 09:36.280]  на полем S. Итак, если существует матрица S, такая, что А' это С-1 на А на С. В этом случае
[09:36.280 --> 09:44.520]  матрицы называются подобными, ну и вот из этого вот факта вытекает, что матрицы подобные, это означает,
[09:44.520 --> 09:50.720]  что это матрицы одного и того же оператора в каких-то двух разных базисах. Если А это матрица
[09:50.720 --> 09:59.240]  какого-то оператора, то применим замену базиса с матрицы перехода S, получим матрицу А' того же
[09:59.240 --> 10:13.840]  самого оператора. Правильно? Вот, значит, здесь же я должен сказать, что если мы обозначим через L от V
[10:13.840 --> 10:28.480]  это множество всех линейных преобразований из V в V, тогда на этом множестве мы можем
[10:28.480 --> 10:35.800]  определить кучу операций. Операторы, вот эти преобразования нашего пространства V мы можем
[10:35.800 --> 10:46.000]  складывать. Естественно, фи плюс си от V это фи от V плюс си от V. Мы можем их умножать на константу
[10:46.000 --> 10:55.400]  лямда фи от V, это просто лямда на фи от V. И таким образом это множество становится линейным
[10:55.400 --> 11:04.080]  пространством над нашим полем. Также мы имеем право, конечно же, их перемножать. Фи на си от V это
[11:04.080 --> 11:11.560]  просто композиция этих операторов. Обратите внимание, что это действительно фи от си от V, то есть
[11:11.560 --> 11:19.360]  к вектору V сначала применяется правый си, потом левый фи. Правильно? Ну и в таком, вот с такими
[11:19.360 --> 11:32.640]  тремя операциями l от V, таким образом l от V это алгебра над полем f. Алгебра, напоминаю,
[11:32.640 --> 11:38.520]  это значит, что это кольцо и одновременно с этим линейное пространство. Правильно? Согласованными
[11:38.520 --> 11:46.160]  операциями. l от V это алгебра над f. Ну а когда мы каждому оператору в каком-то фиксированном
[11:46.160 --> 11:59.760]  базисе сопоставляем его матрицу, то это изоморфизм между алгеброй операторов l от V и алгеброй
[11:59.760 --> 12:07.280]  матрицы на n над нашим полем f. То есть это сопоставление, это биекция, и она сохраняет все
[12:07.280 --> 12:12.880]  операции, о которых мы говорим, в частности, операцию композиции операторов, она переводит в
[12:12.880 --> 12:22.000]  произведение наших матриц. Вот, это второй факт, который здесь нужно вспомнить. Ну и третий факт,
[12:22.000 --> 12:28.960]  который связывает все наши напоминания друг с другом, говорит вот о чем. Если у вас есть
[12:28.960 --> 12:43.720]  многочлен P над полем f, если у вас есть оператор phi на пространстве V над полем f, естественно,
[12:43.720 --> 12:51.920]  то тогда мы имеем право взять этот многочлен от этого оператора. Если у нас есть многочлен над
[12:51.920 --> 12:57.400]  полем, мы имеем право брать вот такое вот выражение для любой алгебры над этим же самым полем.
[12:57.400 --> 13:06.000]  Ну что это такое? Если у нас многочлен имеет вот такой вот ответ, как мы написали, то здесь мы
[13:06.000 --> 13:12.960]  можем написать просто Pn на phi в n, что такое phi в n, мы знаем, это n-кратная итерация нашего phi,
[13:12.960 --> 13:26.240]  плюс Pn-1 phi в n-1, плюс и так далее, плюс P0 на единицу, правильно? В любой алгебре,
[13:27.000 --> 13:34.960]  в которой мы такое вот дело берем, мы берем P0 умножить на единицу, то есть на тождественное
[13:34.960 --> 13:41.300]  отображение. Вот иногда его обозначают вот таким вот образом. 1 с индексом V это означает
[13:41.300 --> 13:50.880]  тождественное отображение пространства V в себя. Ну и ровно также, если у нас есть матрицы,
[13:50.880 --> 13:57.960]  если у нас есть произбольная матрица n на n над полем f, то мы точно также можем взять многочлен
[13:57.960 --> 14:04.880]  от матрицы, ну и это будет как бы вот операция, которая получается нашим изоморфизмом из
[14:04.880 --> 14:12.720]  многочлена от оператора. То есть это будет Pn-я в n, плюс и так далее. P0 будет умножаться опять же
[14:12.720 --> 14:21.720]  таки на единицу в этой алгебре, то есть на единичную матрицу E. Вот, наверное, это все напоминания,
[14:21.720 --> 14:29.400]  которые хотелось бы сказать. Да, ну и естественно, с этими многочленами от оператора или от матрицы мы
[14:29.400 --> 14:34.160]  можем работать также, как с обычными многочленами, складывать их, перемножать, получать результаты
[14:34.160 --> 14:41.920]  применения соответствующих многочленов. Так, вот это вот у нас напоминание. Если вдруг у кого-то
[14:41.920 --> 14:49.840]  какие-то вопросы, давайте я спрошу, есть ли какие-то вопросы, а то вдруг что-то совсем забылось. Все
[14:49.840 --> 15:00.320]  понятно, что я сказал, да? Тогда давайте двигаться дальше. Теперь уже будет что-то новое. Итак,
[15:00.320 --> 15:06.280]  мы начинаем с вами первую большую тему. Ну и как нетрудно понять, вот первая большая
[15:06.280 --> 15:16.400]  тема у нас будет связана с линейными операторами. Линейные операторы это совсем большая тема,
[15:16.400 --> 15:35.120]  а тема поменьше называется инвариантное подпространство. Итак, что это такое, даю
[15:35.120 --> 15:50.680]  определение. Пусть у нас есть, как обычно, линейное пространство над полем f, и есть
[15:50.680 --> 15:59.560]  линейный оператор на нем, то есть линейное преобразование этого самого пространства. Мы с
[15:59.560 --> 16:09.560]  вами говорим, что подпространство этого пространства, напоминаем, что вот это вот меньше или равно,
[16:09.560 --> 16:15.640]  но этот значок обозначает, что мы взяли не просто подможество, а подпространство оператора, пространство
[16:15.640 --> 16:20.640]  Вы, извините, говорим, что подпространство этого пространства
[16:24.640 --> 16:29.640]  давайте даже без тире, инвариантно относительно фи,
[16:29.640 --> 16:45.640]  если, формально я могу написать так, фи от у содержится в у.
[16:45.640 --> 16:50.640]  То есть, иначе говоря, если любой вектор нашего подпространства
[16:50.640 --> 16:57.640]  при действии оператором фи переходит тоже в какой-то вектор подпространства у,
[16:57.640 --> 17:01.640]  то оно обязательно в тот же самый, естественно.
[17:01.640 --> 17:07.640]  Обратите внимание еще здесь, включение, а не равенство,
[17:07.640 --> 17:11.640]  то есть, не обязательно в каждый вектор этого подпространства что-то должно переходить,
[17:11.640 --> 17:16.640]  может быть, что не в каждый.
[17:16.640 --> 17:24.640]  Оно естественно будет подпространством, потому что фи от любого подпространства это подпространство,
[17:24.640 --> 17:29.640]  что он в своё время доказывает.
[17:29.640 --> 17:36.640]  Здесь я могу написать, если хотите, и вот так вот тоже, это тоже будет правдой.
[17:36.640 --> 17:40.640]  Но это равносильные понятия.
[17:40.640 --> 17:46.640]  Раз уж включаются, то и подпространство.
[17:46.640 --> 17:54.640]  Сразу давайте мы приведем какие-нибудь примеры.
[17:54.640 --> 18:05.640]  Во-первых, если фи – это нулевой оператор,
[18:05.640 --> 18:17.640]  то любое подпространство инвариантно.
[18:17.640 --> 18:25.640]  Любое подпространство переходит в ноль, нулевой вектор лежит в любом подпространстве,
[18:25.640 --> 18:30.640]  поэтому любое подпространство инвариантно.
[18:30.640 --> 18:36.640]  Второй пример, наверное, более интересный.
[18:36.640 --> 18:40.640]  Давайте мы, наверное, начнём с геометрического.
[18:40.640 --> 18:46.640]  Давайте посмотрим на двумерную нашу плоскость.
[18:46.640 --> 18:50.640]  В – это наша стандартная В2.
[18:50.640 --> 18:56.640]  Фи от В – это проекция В на какую-то ось.
[18:56.640 --> 19:04.640]  То есть я зафиксировал А, скажем, вот он направлен для любого вектора В.
[19:04.640 --> 19:12.640]  Артагональная проекция на эту ось, фи от В.
[19:12.640 --> 19:22.640]  Тогда утверждаю я, у этого отображения есть вот такие, в частности, инвариантные подпространства.
[19:22.640 --> 19:28.640]  Подпространство, порождённое А, будет инвариантным,
[19:28.640 --> 19:32.640]  просто потому что каждый вектор из него переходит в себя, правильно?
[19:32.640 --> 19:39.640]  И если я возьму вектор В, артагональный А,
[19:39.640 --> 19:45.640]  то подпространство, порождённое В, также будет инвариантным,
[19:45.640 --> 19:51.640]  просто потому что каждый вектор этого вот пространства переходит в ноль, правильно?
[19:51.640 --> 19:59.640]  Ну и, наверное, на всякий случай я скажу более общё.
[19:59.640 --> 20:07.640]  Если любое наше пространство разложено в прямую сумму двух подпространств,
[20:07.640 --> 20:19.640]  то, напоминаю, у нас возникает оператор проекции на подпространство У вдоль подпространства В.
[20:19.640 --> 20:31.640]  Фи – это проекция на У вдоль В.
[20:31.640 --> 20:33.640]  Ну, то есть давайте я напомню так.
[20:33.640 --> 20:42.640]  Если вектор В раскладывается по векторам вот этих подпространств как У плюс В,
[20:42.640 --> 20:50.640]  У лежит в У, В лежит в В, то фи от В – это просто-напросто У.
[20:50.640 --> 20:58.640]  В этой ситуации У и В, разумеется, оба инвариантны относительно фи.
[21:05.640 --> 21:14.640]  Ну, много других… Да, давайте всё-таки ещё одно другое от пространства я приведу.
[21:21.640 --> 21:35.640]  Третье. Если В – это пространство многочленов в степени не выше n, многочлены в степени не выше n.
[21:35.640 --> 21:46.640]  И я предъявлю вам линейное отображение, заключающееся во взятии производной многочлены.
[21:46.640 --> 21:50.640]  Напоминаю, что формальная производная у нас определена над любым полем,
[21:50.640 --> 21:54.640]  поэтому на самом деле над любым полем это можно сделать.
[21:54.640 --> 22:04.640]  То тогда инвариантными подпространствами будут, давайте я скажу в частности, хотя практически все я перечислю,
[22:04.640 --> 22:12.640]  любое подпространство многочленов в степени не выше k окажется инвариантным относительно этого оператора.
[22:13.640 --> 22:19.640]  Многочлены в степени не выше k при дифференцировании переходят в многочлены в степени не выше k-1.
[22:19.640 --> 22:25.640]  Поэтому любое ПКТ окажется инвариантным.
[22:25.640 --> 22:35.640]  Ну вот мы с вами уже на этих примерах не трудно почувствовать, что инвариантные подпространства
[22:35.640 --> 22:41.640]  имеют достаточно большую роль в жизни линейного оператора.
[22:41.640 --> 22:48.640]  Они много о нем говорят, о каких-то вещах, связанных с геометрией этого оператора.
[22:48.640 --> 22:53.640]  Ну и мы сейчас еще это увидим с алгебравической точки зрения тоже.
[22:53.640 --> 23:01.640]  Я надеюсь, этих примеров у нас достаточно. А, да, я забыл привести самый важный пример.
[23:01.640 --> 23:06.640]  У каждого оператора есть точно два инвариантных подпространства.
[23:06.640 --> 23:13.640]  Все В и ноль. В и ноль. Инвариантная всегда.
[23:16.640 --> 23:25.640]  Так, ну давайте теперь переходить к обсуждению свойств наших инвариантных подпространств.
[23:25.640 --> 23:33.640]  Прежде чем я сформулирую какие-то утверждения, я сделаю еще одно идеологическое замечание.
[23:33.640 --> 23:43.640]  Если У инвариантное подпространство относительно Ф, что это означает?
[23:43.640 --> 23:51.640]  Это означает, что Ф переводит любой вектор из У в вектор из У.
[23:51.640 --> 24:01.640]  Ну и тогда можно посмотреть на то, как действует Фи только на У, и это окажется тоже линейным преобразованием У.
[24:01.640 --> 24:07.640]  То есть в этом случае можно рассмотреть, вот какой оператор Фи, ограниченный на У,
[24:07.640 --> 24:11.640]  и он окажется линейным преобразованием этого самого пространства.
[24:11.640 --> 24:17.640]  Если У будет неинвариантным, такого сделать нельзя, потому что Фи, какие-то векторы из У,
[24:17.640 --> 24:21.640]  будут переводить вовне У, правильно?
[24:21.640 --> 24:29.640]  Так что вот это можно сделать ровно в том случае, когда У инвариантно, и это тоже бывает полезным.
[24:29.640 --> 24:37.640]  Очень часто действия Фи на каких-то инвариантных подпространствах тоже много о самом преобразовании, о самом операторе говорит.
[24:38.640 --> 24:46.640]  Так, ну поехали изучать свойства этого нового введенного понятия.
[24:46.640 --> 24:56.640]  Утверждение первое. Давайте мы поймем, как строить новое инвариантное подпространство из старых.
[24:56.640 --> 25:11.640]  Итак, пусть Фи это линейный оператор на В, а У1 и У2 это инвариантное подпространство относительно Фи.
[25:11.640 --> 25:28.640]  Тогда их пересечение и их сумма также инвариантны. Естественно, тоже относительно Фи.
[25:28.640 --> 25:35.640]  Ну это пока что достаточно тривиально, но давайте проговорим на всякий случай.
[25:35.640 --> 25:53.640]  Что делать с пересечением? Если какой-то вектор лежит в пересечении, то естественно Фи от У будет лежать в У1,
[25:53.640 --> 25:59.640]  Вектор У лежит в У1, значит и Фи от У лежит в У1. У1 инвариантно, правильно?
[25:59.640 --> 26:09.640]  Фи от У лежит в У2 по той же самой причине, ну а значит Фи от У лежит в пересечении.
[26:09.640 --> 26:18.640]  Вот мы и доказали, что пересечение инвариантно. Любой вектор из этого пересечения переходит тоже в вектор из пересечения.
[26:18.640 --> 26:30.640]  Для суммы не намного сложнее. Если какой-то вектор лежит в сумме двух инвариантных подпространств,
[26:30.640 --> 26:39.640]  то мы с вами знаем, что он раскладывается в сумму двух векторов из этих подпространств.
[26:40.640 --> 26:44.640]  Это разложение не обязательно единственное, но нам это не важно.
[26:44.640 --> 26:57.640]  Ну а тогда Фи от В это будет Фи от У1 плюс Фи от У2. Фи от У1 лежит в У1, поскольку У1 инвариантно.
[26:57.640 --> 27:05.640]  Фи от У2 аналогичным образом лежит в У2, ну а значит эта сумма лежит в сумме наших У1 и У2.
[27:05.640 --> 27:10.640]  И таким образом утверждение мы тоже уже доказали.
[27:10.640 --> 27:28.640]  Так, следующее... О, вот я как раз стираю матрицы наших операторов.
[27:28.640 --> 27:39.640]  Давайте мы поймем, как инвариантные подпространства влияют на структуру матрицы оператора в соответствующем базе.
[27:39.640 --> 28:01.640]  Итак, пусть у нас Фи это линейный оператор на В, а Е, состоящий из этих векторов Е1,
[28:01.640 --> 28:06.640]  и так далее, и ЕН, это базис В.
[28:11.640 --> 28:22.640]  Тогда давайте мы поймем, что означает, что первые К векторов в этом базисе порождают инвариантное подпространство.
[28:22.640 --> 28:35.640]  Вот такое вот подпространство с базисом Е1 и так далее ЕКТ окажется инвариантным тогда и только тогда,
[28:35.640 --> 28:43.640]  когда в нашем базисе Фи имеет вот такую вот блочную матрицу.
[28:43.640 --> 28:52.640]  Значит, что у нас тут будут за блоки? Вот размеры блоков будет здесь КН-К, здесь тоже КН-К.
[28:52.640 --> 28:58.640]  Вот здесь будут стоять какие-то три матрицы, а здесь будет стоять ноль.
[29:04.640 --> 29:07.640]  Доказательства опять же такие очень простые.
[29:07.640 --> 29:12.640]  Давайте мы выясним, что означает, что это у инвариантно.
[29:16.640 --> 29:28.640]  Как в терминах базиса сформулировать, что подпространство у вот с таким вот базисом или с такой порождающей системой является инвариантным?
[29:28.640 --> 29:38.640]  Очень просто. Нам достаточно проверить, что Фи, что образы всех вот этих вот базисных векторов попадают в У, правильно?
[29:38.640 --> 29:47.640]  Естественно, если У инвариантно, то Фи от яитых будут попадать в У при И от одного до К.
[29:47.640 --> 29:51.640]  Это вот я объяснил, почему сверху следует нижнее.
[29:51.640 --> 29:59.640]  Ну и наоборот, естественно, тоже, если образы всех базисных векторов будут лежать в У, то и образы их линейных комбинаций тоже, правильно?
[29:59.640 --> 30:01.640]  А это все векторы из У.
[30:01.640 --> 30:06.640]  Так, ну а это что как раз означает?
[30:06.640 --> 30:19.640]  Фи от яитых лежит в У, это означает, что Фи от яитого это линейная комбинация первых К базисных векторов в нашем базисе.
[30:19.640 --> 30:24.640]  Так, давайте я не буду мельчить, я перейду на следующую часть, может быть.
[30:34.640 --> 30:45.640]  Итак, Фи от яитого должно раскладываться только по первым К базисам, по первым К базисным векторам, естественно, извините.
[30:45.640 --> 30:51.640]  Сумма пожелает одного до К какая-то альфа ежи ежи т.
[30:51.640 --> 30:58.640]  Ну а эти коэффициенты разложения это как раз элементы нашей матрицы.
[30:58.640 --> 31:06.640]  В первых К столбцах нашей матрицы должны стоять координатные столбцы ровно вот этих вот товарищей, правильно?
[31:06.640 --> 31:11.640]  Так здесь у нас И тоже от одного до К.
[31:11.640 --> 31:15.640]  В первых К столбцах должны стоять координатные столбцы вот этих товарищей.
[31:15.640 --> 31:22.640]  И вот здесь мы написали, что в этих координатных столбцах только первые К элементов могут отличаться от нуля.
[31:22.640 --> 31:27.640]  Потому что только по первым К базисным векторам они тут у нас раскладываются.
[31:27.640 --> 31:36.640]  Это как раз и означает, что Фи в базе Се имеет матрицу, в которой здесь стоят нули.
[31:36.640 --> 31:40.640]  Ниже вот этой вот планки в К элементов стоят нули.
[31:40.640 --> 31:44.640]  Ну а здесь стоит все, что угодно.
[31:44.640 --> 31:49.640]  Вот мы наше утверждение и доказали.
[31:49.640 --> 31:56.640]  Ну и это, в общем, еще раз показывает, что инвариантные подпространства могут быть полезными.
[31:56.640 --> 32:01.640]  Поскольку матрица нашего оператора, вот если мы отловили какое-то инвариантное подпространство
[32:01.640 --> 32:09.640]  и его базис поставили на первые места, то мы уже приходим к хорошей матрице, про которую мы даже что-то знаем.
[32:09.640 --> 32:16.640]  Напоминаю, что в свое время у нас было утверждение про определитель такой матрицы.
[32:16.640 --> 32:20.640]  Это как раз матрица с углом нулей.
[32:20.640 --> 32:25.640]  Здесь уместно сделать замечание.
[32:25.640 --> 32:35.640]  Естественно, вот если я тут написал матрицу вот в таком виде А, Б, С, то А имеет ясный геометрический смысл.
[32:35.640 --> 32:38.640]  Что такое А?
[32:38.640 --> 32:46.640]  Это матрица того самого преобразования подпространства У, правильно?
[32:46.640 --> 32:50.640]  А это матрица ограничения Фи на У.
[32:50.640 --> 32:59.640]  Потому что здесь как раз написано, как образы базисных векторов раскладываются по тому же самому базису этого подпространства, правильно?
[32:59.640 --> 33:08.640]  Матрица этого ограничения в базисе Е1 и так далее Ек.
[33:08.640 --> 33:14.640]  Тут я сразу хочу сказать, что матрица С тоже может придать геометрический смысл.
[33:14.640 --> 33:21.640]  Но мы этого сейчас делать не будем, потому что это требует некоторых дополнительных понятий, которые мы пока не ввели.
[33:21.640 --> 33:25.640]  Но через некоторое время мы об этом тоже попытаемся поговорить.
[33:25.640 --> 33:36.640]  То есть у матрицы С тоже есть смысл как у матрицы какого-то преобразования, но чуть похитрее определенного.
[33:36.640 --> 33:41.640]  Так, давайте я лучше перейду на следующую доску.
[33:41.640 --> 33:46.640]  Ну и следующее утверждение, которым мы часто будем пользоваться.
[33:46.640 --> 33:55.640]  Еще один важный источник наших инвариантных подпространств.
[33:55.640 --> 34:07.640]  Вот каков утверждение.
[34:07.640 --> 34:20.640]  Пусть у нас есть на сей раз два линейных оператора на пространстве В, два линейных преобразования В.
[34:20.640 --> 34:27.640]  Причем важные условия, они коммутируют.
[34:27.640 --> 34:36.640]  То есть если их перемножить в любом порядке, получится один и тот же оператор.
[34:36.640 --> 34:52.640]  Тогда ядро Пси и образ Пси инвариантны относительно Фи.
[34:52.640 --> 35:00.640]  Прежде чем доказывать, давайте я спрошу, а относительно Пси они будут инвариантными?
[35:00.640 --> 35:04.640]  Будут ли они инвариантными относительно Пси?
[35:04.640 --> 35:09.640]  Конечно да, потому что мы можем тоже утверждение применить к Фи равному Пси.
[35:09.640 --> 35:13.640]  Насколько Пси, они тоже инвариантными будут.
[35:13.640 --> 35:18.640]  Так, ну раз у нас прозвенел звонок, давайте мы...
[35:18.640 --> 35:28.640]  Или как вы смотрите, можем мы первый перерыв немножко съесть в счет того, что у нас происходило в начале пары?
[35:28.640 --> 35:31.640]  Задержка у нас происходила.
[35:31.640 --> 35:35.640]  Работаем без одного перерыва или не работаем?
[35:35.640 --> 35:37.640]  Без проблем.
[35:37.640 --> 35:41.640]  То есть если у кого-то есть проблемы, говорите, сделаем перерыв.
[35:41.640 --> 35:46.640]  Нет проблем. Хорошо, тогда доказываем.
[35:46.640 --> 35:57.640]  Опять же таки нужно нам разобраться с двумя случаями.
[35:57.640 --> 36:02.640]  Давайте докажем, что ядро Пси инвариантно относительно Фи.
[36:02.640 --> 36:06.640]  Если какой-то вектор лежит в ядре Пси,
[36:06.640 --> 36:11.640]  ну здесь у нас будет все практически автоматически, главное определение помнить.
[36:11.640 --> 36:15.640]  Что означает, что какой-то вектор лежит в ядре Пси?
[36:15.640 --> 36:20.640]  Это означает, что Пси от У это ноль.
[36:20.640 --> 36:27.640]  А теперь нам хочется проверить, что Фи от У тоже лежит в ядре Пси, правильно?
[36:27.640 --> 36:31.640]  Что нам для этого нужно выяснить?
[36:31.640 --> 36:36.640]  Что Пси от Фи от У это ноль, правильно?
[36:36.640 --> 36:46.640]  Хотелось бы нам понять, что это ноль, но мы-то с вами знаем, что Фи и Пси перестановочны, правильно?
[36:46.640 --> 36:54.640]  Поэтому здесь мы можем переставить их местами и сказать, что это то же самое, что Фи от Пси от У.
[36:54.640 --> 37:00.640]  Но Пси от У был нулем, значит и Фи от него это тоже ноль.
[37:00.640 --> 37:05.640]  Равно Фи от нуля, то есть равно нулю.
[37:05.640 --> 37:10.640]  Все, мы с вами получили, что Пси от Фи от У это ноль,
[37:10.640 --> 37:17.640]  значит Фи от У лежит в ядре Пси просто по определению.
[37:17.640 --> 37:24.640]  Мы проверили, что любой, если какой-то вектор лежит в ядре Пси, то и Фи от него тоже лежит в ядре Пси.
[37:24.640 --> 37:30.640]  Значит первая часть у нас доказана.
[37:30.640 --> 37:37.640]  Ну и вторая часть делается аналогичной манипуляцией.
[37:37.640 --> 37:43.640]  Давайте мы ее, естественно, все равно проговорим.
[37:43.640 --> 37:53.640]  Значит, если у нас есть ядро Пси, если мы взяли некоторые векторы, извините, образ Пси, конечно,
[37:53.640 --> 37:58.640]  если мы взяли некоторые векторы из образа Пси, что это означает?
[37:58.640 --> 38:12.640]  Это означает, что этот вектор есть Пси от кого-то другого, правильно, от какого-то W, где W лежит в В.
[38:12.640 --> 38:18.640]  Ну а тогда что такое Фи от У?
[38:18.640 --> 38:25.640]  Раз У это Пси от В, то это Фи от Пси от В.
[38:25.640 --> 38:29.640]  Ну и опять же таки вспоминаю, что наши операторы перестановочные.
[38:29.640 --> 38:35.640]  Мы можем сказать, что это Пси от Фи от В.
[38:35.640 --> 38:42.640]  Да, равенства уже не нужно. Мы уже говорим, это Пси от кого-то, правда?
[38:42.640 --> 38:46.640]  А значит он лежит в образе Пси.
[38:46.640 --> 38:57.640]  И таким образом мы опять же таки доказали, что если У лежал в образе, то и Фи от У тоже лежит в образе, что и требовалось доказать.
[38:57.640 --> 39:08.640]  Так, утверждение интересно само по себе. Чаще всего, я сразу скажу, мы будем использовать его вот в какой форме.
[39:08.640 --> 39:16.640]  Где взять запас таких Пси, которые коммутируют с данным нам Фи?
[39:16.640 --> 39:20.640]  Фи, говорят нам, Фи квадрат и так далее.
[39:20.640 --> 39:43.640]  На самом деле в качестве Пси можно взять, давайте я просто скажу, что можно положить Пси равным любому многочлену от Фи.
[39:43.640 --> 39:51.640]  Глядите, если я возьму П от Фи умножено Фи, это будет то же самое, что я Фи умножено П от Фи.
[39:51.640 --> 39:55.640]  Можно это проверить непосредственно, просто скобки раскрыть, правильно?
[39:55.640 --> 40:07.640]  Можно сказать, что мы работаем с многочленами от Фи, а они ведут себя, вот и алгебравические действия над ними, это то же самое, что алгебравические действия просто над многочленами.
[40:07.640 --> 40:11.640]  Но в частности они коммутативны.
[40:11.640 --> 40:16.640]  То есть если я возьму Пси равным П от Фи, то я могу спокойно применять это утверждение.
[40:16.640 --> 40:34.640]  То есть ядро произвольного многочлена от Фи и образ произвольного многочлена от Фи инвариантная относительно Фи.
[40:42.640 --> 41:06.640]  Ну и еще, наверное, одно, еще более частное следствие, но тоже из тех, которые нам будут нужны сильнее всего.
[41:07.640 --> 41:20.640]  Самый простой многочлен, который нам окажется полезным, это многочлен вида Фи минус лямбда, ну то есть Х минус лямбда.
[41:20.640 --> 41:27.640]  Здесь, конечно же, лямбда это скаляр, это элемент поля.
[41:27.640 --> 41:36.640]  Ядро Фи минус лямбда и образ Фи минус лямбда инвариантная относительно Фи.
[41:36.640 --> 41:40.640]  Что такое Фи минус лямбда?
[41:40.640 --> 41:43.640]  Как я из оператора вычитаю скаляр?
[41:43.640 --> 41:47.640]  Это Фи минус лямбда на тождественный оператор, естественно, правильно?
[41:47.640 --> 41:52.640]  Я многочлен Х минус лямбда подставил Фи.
[41:52.640 --> 42:02.640]  То есть каждый раз, когда мы пишем Фи минус лямбда, мы имеем в виду, естественно, Фи минус лямбда на тождественный оператор на пространстве В.
[42:02.640 --> 42:08.640]  Так вот, ядро и образ таких операторов всегда инвариантны относительно Фи.
[42:08.640 --> 42:16.640]  Ну и, наконец, еще одно утверждение, чуть более общее.
[42:16.640 --> 42:24.640]  Пусть Фи это линейный оператор на В,
[42:24.640 --> 42:36.640]  тогда любое подпространство в ядре Фи...
[42:36.640 --> 42:50.640]  Ну хорошо, оставим пока так.
[42:50.640 --> 43:07.640]  Любое подпространство в ядре Фи и, внимание, любое надпространство образа Фи инвариантны относительно Фи.
[43:07.640 --> 43:15.640]  Это тоже еще пока что очень просто.
[43:15.640 --> 43:33.640]  Почему это так? Потому что если У лежит в таком вот У, которое подпространство в ядре Фи, то Фи от У это, конечно же, ноль, и потому лежит в У, правильно?
[43:33.640 --> 43:48.640]  Первое утверждение я уже доказал. Если В лежит в В большом, то Фи от В, даже если бы он не лежал в В, лежал бы просто в В,
[43:48.640 --> 43:57.640]  Фи от В все равно лежит в образе Фи, правда?
[43:57.640 --> 44:09.640]  И реальность верная всегда, ну а образ Фи это подпространство в В, и второе утверждение тоже верное.
[44:09.640 --> 44:19.640]  Ну и несложное упражнение, которое я предлагаю всем желающим на самостоятельную разработку.
[44:19.640 --> 44:27.640]  Это утверждение останется верным, если я здесь вместо Фи подставлю Фи минус лямбда.
[44:27.640 --> 44:49.640]  То есть любое подпространство в ядре Фи минус лямбда и любое надпространство образа Фи минус лямбда также будут инвариантны относительно Фи.
[44:50.640 --> 44:58.640]  Относительно Фи минус лямбда это они точно инвариантны, правильно? Оказывается и относительно Фи тоже.
[44:58.640 --> 45:08.640]  Так, ну вот это небольшой блок, касающийся произвольных инвариантных подпространств.
[45:08.640 --> 45:15.640]  Они нам еще будут встречаться этим понятием, мы естественно будем очень усиленно пользоваться.
[45:16.640 --> 45:24.640]  Но дальше мы переходим к изучению самой простой ситуации инвариантного подпространства.
[45:24.640 --> 45:32.640]  Тут у нас появляется новое понятие, которое называется собственные векторы.
[45:32.640 --> 45:44.640]  Ну давайте прежде чем я дам определение, я спрошу об этом.
[45:44.640 --> 45:58.640]  Что означает, что подпространство, порожденное одним вектором, инвариантно относительно Фи?
[45:58.640 --> 46:08.640]  Как по вектору понять, что подпространство, порожденное им одним, ну естественно вектор, давайте мы возьмем не нулевой,
[46:08.640 --> 46:14.640]  как по вектору понять, что подпространство, порожденное им одним, инвариантно?
[46:14.640 --> 46:26.640]  Это означает, что когда мы к нему применяем Фи, получается что-то ему пропорциональное, правильно?
[46:26.640 --> 46:32.640]  Это лямбда на В, где лямбда это какой-то элемент нашего поля.
[46:32.640 --> 46:38.640]  В должен переходить тоже в эту линейную оболочку, то есть вектор себе пропорциональный.
[46:38.640 --> 46:42.640]  Ну а больше никаких условий у нас и нет. Если это будет так, то все в порядке.
[46:42.640 --> 46:46.640]  Вот такой вектор и называется собственным вектором.
[46:46.640 --> 46:54.640]  Итак, определение, пусть Фи, это линейное преобразование В,
[46:54.640 --> 47:22.640]  значит, говорим, внимание, что не нулевой вектор В является
[47:22.640 --> 47:28.640]  собственным вектором оператора Фи.
[47:28.640 --> 47:32.640]  Внимание, еще одно понятие, мы сразу два вводим.
[47:32.640 --> 47:42.640]  Собственным значением лямбда, лямбда это как раз скаляр,
[47:42.640 --> 47:50.640]  если выполнено ровно вот это вот свойство.
[47:50.640 --> 47:56.640]  Еще раз обращаю ваше внимание, что это определение работает только с не нулевыми векторами.
[47:56.640 --> 48:02.640]  Если бы мы убрали это условие, то нулевой вектор оказался бы собственным,
[48:02.640 --> 48:04.640]  с любым собственным значением, правильно?
[48:04.640 --> 48:08.640]  Этого не хочется, потому что сейчас мы на самом деле увидим,
[48:08.640 --> 48:12.640]  что вот с таким определением собственных значений у нас будет немного.
[48:12.640 --> 48:17.640]  То есть весьма избранные лямбды окажутся собственными значениями.
[48:17.640 --> 48:22.640]  Естественно, давайте я это отдельно сформулирую.
[48:22.640 --> 48:41.640]  Скаляр лямбда называется собственным значением оператора Фи,
[48:41.640 --> 48:45.640]  если для него есть собственный вектор.
[48:45.640 --> 48:49.640]  Если он хотя бы в одной такой ситуации оказывается,
[48:49.640 --> 48:59.640]  если для него есть собственный вектор с этим самым собственным значением.
[48:59.640 --> 49:03.640]  Этими понятиями мы сейчас будем пользоваться так часто,
[49:03.640 --> 49:06.640]  что давайте мы их будем сокращать.
[49:06.640 --> 49:11.640]  Собственный вектор у нас будет обозначаться СВ,
[49:11.640 --> 49:17.640]  собственным значением у нас, естественно, будет обозначаться СЗ.
[49:17.640 --> 49:23.640]  Разумеется.
[49:23.640 --> 49:29.640]  Давайте сразу примеры.
[49:29.640 --> 49:39.640]  Если вектор лежит в ядре Фи и при этом не нулевой, правильно?
[49:39.640 --> 49:49.640]  То он собственный вектор с собственным значением ноль, правильно?
[49:49.640 --> 49:58.640]  Второй пример, наверное, стоит сказать, что если, например,
[49:58.640 --> 50:12.640]  Фи – это проекция на У вдоль В, мы с вами уже сказали, по сути дела,
[50:12.640 --> 50:17.640]  что все векторы В собственные собственным значением ноль, правильно?
[50:17.640 --> 50:29.640]  Все не нулевые векторы в В – это собственные векторы с собственным значением ноль,
[50:29.640 --> 50:42.640]  а все векторы в У собственные собственным значением 1, правильно?
[50:42.640 --> 50:51.640]  Собственные с собственным значением 1, опять же такие, нужно сказать, слова не нулевые.
[50:51.640 --> 51:04.640]  На самом деле давайте я сразу скажу, что такое будет множество всех векторов,
[51:04.640 --> 51:10.640]  удовлетворяющих вот тому вот самому свойству, правильно?
[51:10.640 --> 51:17.640]  Фи от В равно лямда В. Давайте сразу заметим вот что.
[51:17.640 --> 51:23.640]  Что означает, что Фи от В равно лямда В?
[51:23.640 --> 51:28.640]  Мы можем перенести правую часть в левую и перегруппировать.
[51:29.640 --> 51:35.640]  Давайте я даже так сделаю. Сначала Фи от В минус лямда В – это ноль.
[51:35.640 --> 51:44.640]  А здесь у нас написано, что Фи минус лямда на В, Фи минус лямда, примененное к В – это ноль.
[51:44.640 --> 51:53.640]  Иначе говоря, В лежит в ядре оператора Фи минус лямда.
[51:53.640 --> 51:57.640]  Фи минус лямда – это Фи минус лямда на тождественный.
[51:57.640 --> 52:03.640]  Поэтому стоит сразу сделать вот какое определение.
[52:03.640 --> 52:13.640]  Итак, если лямда – это собственное значение оператора Фи,
[52:13.640 --> 52:19.640]  то мы можем ввести вот такое вот подпространство.
[52:19.640 --> 52:24.640]  Оно у нас будет обозначаться В с нижним индексом лямда.
[52:24.640 --> 52:27.640]  Это ядро Фи минус лямда.
[52:27.640 --> 52:34.640]  Оно называется собственным подпространством.
[52:34.640 --> 52:41.640]  Ну, соответствующее собственному значению лямда.
[52:41.640 --> 52:43.640]  Что такое собственное подпространство?
[52:43.640 --> 52:48.640]  Это множество всех собственных векторов, соответствующих лямда.
[52:48.640 --> 52:51.640]  И еще это добавление нуля, естественно.
[52:51.640 --> 52:56.640]  Нулевой вектор не собственный, но он здесь тоже будет, чтобы оно оказалось подпространством.
[52:56.640 --> 53:03.640]  Вот эти вот подпространства будут играть у нас, конечно же, важную роль.
[53:03.640 --> 53:09.640]  Мы определили важное понятие собственного вектора.
[53:09.640 --> 53:16.640]  Через некоторое время мы увидим, насколько оно важно, особенно когда этих собственных векторов много.
[53:16.640 --> 53:28.640]  Но прежде чем это понимать, еще раз, мы вроде как это только что говорили, правильно?
[53:28.640 --> 53:37.640]  Еще раз, Фи – это оператор, лямда – это скаляр, Фи минус лямда – это Фи минус лямда на тождественный оператор на В.
[53:37.640 --> 53:46.640]  То есть Фи минус лямда – это Фи минус лямда на тождественный оператор на пространстве В.
[53:46.640 --> 53:56.640]  Ну, соответственно, давайте я для большей ясности сразу скажу, чему он соответствует.
[53:56.640 --> 54:08.640]  Если Фи имеет в каком-то базе матрицу А, Фи минус лямда будет иметь матрицу А минус лямда умноженная на Е.
[54:08.640 --> 54:33.640]  Так, ну и давайте мы сразу уж раз об этом заговорили, то спросим себя, как искать собственные векторы и собственные значения.
[54:33.640 --> 54:40.640]  А потом увидим, зачем они в первую очередь нам будут нужны.
[54:40.640 --> 54:48.640]  Собственные векторы и собственные значения искать просто после того, как мы сказали то, что мы уже сказали.
[54:48.640 --> 55:00.640]  Лямда является собственным значением, когда у нас есть хотя бы один не нулевой собственный вектор, соответствующий этому собственному значению.
[55:00.640 --> 55:05.640]  То есть, когда ядро Фи минус лямда не нулевое, правильно?
[55:05.640 --> 55:20.640]  Ядро Фи минус лямда не нулевое, а это означает, что Фи минус лямда – вырожденный оператор.
[55:20.640 --> 55:41.640]  То есть, если Фи имеет в некотором базе матрицу А, то мы уже сказали, какая матрица у Фи минус лямда, А минус лямда Е – это вырожденная матрица.
[55:41.640 --> 55:53.640]  А это означает, что определитель А минус лямда Е равен нулю.
[55:53.640 --> 56:05.640]  Ну и таким образом, если у нас оператор Фи задан в каком-то базисе матрицей А, то как мы можем найти все его собственные значения?
[56:05.640 --> 56:13.640]  Найти вот этот определитель – это будет что, кстати, такое? Это будет какой-то многочлен от лямды, правильно?
[56:13.640 --> 56:21.640]  Найти вот этот определитель, как многочлен от лямды, его корни в точности и будут собственными значениями.
[56:21.640 --> 56:41.640]  Итак, лямда – это собственное значение оператора Фи тогда и только тогда, когда вот этот самый определитель равен нулю, ну и если А – это матрица нашего Фи.
[56:41.640 --> 56:51.640]  Тут сразу стоит сделать определение, обозначить вот этот определитель каким-то хорошим образом.
[56:51.640 --> 57:19.640]  Если А – это матрица N на N над полем F, то вот такой вот многочлен, который мы только что написали,
[57:19.640 --> 57:31.640]  я сразу пишу, как он называет, он обозначается, греческая буква Хи. Давайте я тут Х напишу.
[57:31.640 --> 57:55.640]  Определитель А минус Х на Е, вот этот многочлен – это характеристический многочлен матрицы А.
[57:55.640 --> 58:05.640]  Таким образом, лямда – это собственное значение оператора Фи тогда и только тогда, когда лямда – это корень характеристического многочлена матрицы этого оператора.
[58:05.640 --> 58:25.640]  На самом деле здесь, конечно, хочется сказать, что очень странно у нас получается, у одного и того же оператора в разных базисах будут разные матрицы,
[58:25.640 --> 58:33.640]  и могут получиться разные характеристические многочлены у этих матриц. На самом деле это не так.
[58:33.640 --> 58:53.640]  Если Фи имеет матрицу А в каком-то базе СЕ и имеет матрицу А' в каком-то базе СЕ', то характеристические многочлены этих матриц совпадают.
[58:55.640 --> 59:12.640]  Доказательства. Давайте мы увидим это. Да, давайте я сначала напомню, что означает, что А и А' – это матрицы одного и того же Фи в разных базисах.
[59:12.640 --> 59:32.640]  Это означает, что они подобны. Мы можем сказать, что А' имеет вид С-1 на АС, для некоторой, естественно, обратимой матрицы С, раз у нас встречается С-1.
[59:32.640 --> 59:42.640]  Ну и если это так, то мы можем понять, как у нас выглядит характеристический многочлен А'.
[59:42.640 --> 01:00:10.640]  Тогда характеристический многочлен А' – это что такое? Я беру определитель А'-хE, то есть я беру определитель вот какой матрицы С-1,
[01:00:10.640 --> 01:00:20.640]  вот какой матрицы С-1, АС, минус Х на Е. Но давайте я сразу это Е тоже немножко распишу.
[01:00:20.640 --> 01:00:36.640]  Минус Х на вот что? С-1 ЕС. С-1 на С это же Е, правильно? Ну а значит у нас здесь написано определитель вот какого произведения.
[01:00:36.640 --> 01:00:48.640]  С-1 я могу вынести слева, а С я могу вынести справа, правильно? И в скобках у меня останется А-хE.
[01:00:48.640 --> 01:01:02.640]  Ну определитель произведения мы с вами знаем, что равен произведению определителей.
[01:01:02.640 --> 01:01:10.640]  Ну и это, естественно, два крайних сомножителя в произведении дают единицы, правильно?
[01:01:10.640 --> 01:01:24.640]  Так же определители взаимно обратных матриц, так же взаимно обратных. Ну и мы получили то, что нам нужно, потому что мы получили характеристический многочлен матрицы А.
[01:01:24.640 --> 01:01:44.640]  Ну и как следствием, коль скоро любая матрица нашего оператора имеет один и тот же характеристический многочлен,
[01:01:44.640 --> 01:01:54.640]  то естественным образом мы можем сказать, что этот многочлен можно назвать характеристическим многочленом этого оператора.
[01:01:54.640 --> 01:02:20.640]  Характеристическим многочленом оператора А-хE теперь мы просто назовем характеристический многочлен любой его матрицы.
[01:02:20.640 --> 01:02:30.640]  Неважно какой, потому что многочлен получится один и тот же.
[01:02:30.640 --> 01:02:50.640]  Так, ну и коль скоро мы с вами заговорили про характеристический многочлен и поняли, что он не зависит, то есть он не меняется при подобии.
[01:02:50.640 --> 01:03:00.640]  Стоит, прежде чем двигаться дальше, немножко посмотреть вблизи на этот самый характеристический многочлен.
[01:03:00.640 --> 01:03:16.640]  Давайте мы сразу это сделаем. Пусть А имеет элементы А и Ж, давайте я даже ее в натуральную величину напишу.
[01:03:16.640 --> 01:03:40.640]  А2-1, А2-2, здесь А1-2, тогда ее характеристический многочлен получается каким образом?
[01:03:40.640 --> 01:03:50.640]  Я должен из А вычесть х на Е, то есть по сути дела вычесть из диагональных элементов по х, правильно?
[01:03:50.640 --> 01:04:06.640]  Здесь у меня будет А1-1-х, А2-2-х, АНН-х, а остальные элементы остаются такими же, как были.
[01:04:06.640 --> 01:04:24.640]  АН1, АН2 и так далее. И вот такой у нас получается многочлен. Давайте мы на него внимательно посмотрим, какая у него будет степень.
[01:04:24.640 --> 01:04:36.640]  Разумеется, Н. У нас всего Н линейных многочленов, которые зависит от х, все остальные константы, правильно?
[01:04:36.640 --> 01:04:44.640]  Больше Н получится в принципе не может, но все эти товарищи у нас конечно перемножатся и дадут нам х в Н, правильно?
[01:04:44.640 --> 01:04:54.640]  Точнее дадут нам х в Н с каким коэффициентом? Минус 1 в Н, потому что перед всеми этими х-ами стоят минусы, правильно?
[01:04:54.640 --> 01:05:02.640]  По сути дела у нас будет минус х в Н. Старший член нашего многочлена это минус 1 в Н на х в Н.
[01:05:02.640 --> 01:05:12.640]  Единственным образом вот таким вот из произведения всех диагональных элементов может получиться х в Н.
[01:05:12.640 --> 01:05:20.640]  Давайте я сразу спрошу, а каким образом может получиться х в Н-1?
[01:05:20.640 --> 01:05:22.640]  Неправда.
[01:05:26.640 --> 01:05:30.640]  Откуда может получиться х в Н-1? Если у нас...
[01:05:30.640 --> 01:05:44.640]  Абсолютно верно. Если мы хотим получить в полном разложении этого определителя, когда мы раскроем все скобки,
[01:05:44.640 --> 01:05:52.640]  моном х в Н-1, он может получиться тоже только из произведения вот этих диагональных элементов,
[01:05:52.640 --> 01:06:00.640]  потому что если у нас Н-1х набрался, то Н-1х был взят из диагональных элементов, правильно?
[01:06:00.640 --> 01:06:06.640]  А тогда и Н-ный элемент тоже будет диагональным, раз уж мы Н-1 элемент в произведении знаем,
[01:06:06.640 --> 01:06:12.640]  и знаем, что они должны быть из разных строк и разных столбцов. Последний тоже должен быть из диагонали.
[01:06:12.640 --> 01:06:19.640]  Х в Н-1 тоже должен взяться из вот этого самого произведения, но только когда мы там скобки раскроем,
[01:06:19.640 --> 01:06:24.640]  у нас же будут члены, в которых Н-1х, а Н-ая константа, правильно?
[01:06:24.640 --> 01:06:29.640]  И, значит, давайте понимать, что у нас там будет. Там будет минус 1 в степени Н-1,
[01:06:29.640 --> 01:06:35.640]  вот эти вот Н-1х возьмутся со знаками минус, правильно?
[01:06:35.640 --> 01:06:42.640]  А дальше будет... а дальше может оказаться любой из вот этих вот диагональных элементов,
[01:06:42.640 --> 01:06:47.640]  то есть здесь будет А-1-1 плюс А-2-2 плюс и так далее, плюс АНН.
[01:06:47.640 --> 01:07:00.640]  Так, дальнейшие члены я вычислять не буду, хотя в принципе некоторые их описания тоже существуют,
[01:07:00.640 --> 01:07:06.640]  но я их оставлю на самостоятельное изучение всем желающим.
[01:07:06.640 --> 01:07:10.640]  Скажу я только, какой будет свободный член. А какой будет свободный член?
[01:07:10.640 --> 01:07:14.640]  Конечно, просто детерминат нашей матрицы, правильно?
[01:07:14.640 --> 01:07:20.640]  Хотя бы потому, что такое свободный член вот этого вот многочлена.
[01:07:20.640 --> 01:07:24.640]  Свободный член любого многочлена, это его значение в нуле.
[01:07:24.640 --> 01:07:30.640]  Если мы сюда подставим ноль, то мы, конечно же, получим просто определитель матрицы А.
[01:07:30.640 --> 01:07:34.640]  Свободный член у нас это определитель А.
[01:07:34.640 --> 01:07:44.640]  Так, ну и настало нам время обозначить вот это вот выражение тоже каким-то образом.
[01:07:44.640 --> 01:07:54.640]  Оно имеет специальное название, определение.
[01:07:54.640 --> 01:08:16.640]  Если А это квадратная матрица с элементами А и житами, то ее след это ровно вот то, что у нас там написано.
[01:08:16.640 --> 01:08:24.640]  Это сумма всех ее диагональных элементов. Не всех, а только диагональных элементов.
[01:08:24.640 --> 01:08:34.640]  Так, ну и следствие из нашего предыдущего утверждения, которое у нас здесь было,
[01:08:34.640 --> 01:08:48.640]  если одно и то же преобразование имеет матрицы А и а' в двух разных базисах,
[01:08:48.640 --> 01:08:58.640]  то след А равен следу а', ну а детерминат А равен детерминату а',
[01:08:58.640 --> 01:09:04.640]  просто потому что это коэффициенты их характеристических многочленов, правильно?
[01:09:04.640 --> 01:09:12.640]  А мы доказали только что, что характеристические многочлены совпадают, значит и соответствующие их коэффициенты тоже совпадают.
[01:09:12.640 --> 01:09:24.640]  Мы знаем, для следа мы знаем, что минус, давайте я для следа напишу, мы знаем, что характеристические многочлены совпадают,
[01:09:24.640 --> 01:09:32.640]  то есть мы знаем в частности, что минус 1 в n-минус 1 на след А равен минус 1 в n-минус 1 на след а',
[01:09:32.640 --> 01:09:40.640]  ну а это и означает, что след А равен следу а'. С детерминатом абсолютно аналогично.
[01:09:40.640 --> 01:09:52.640]  Детерминатом мы могли и раньше сразу доказать, но уж из этого оно следует, правильно?
[01:09:52.640 --> 01:09:58.640]  Другие коэффициенты характеристических многочленов, естественно, с ними тоже можно то же самое сказать,
[01:09:58.640 --> 01:10:04.640]  просто вот эти вот имеют достаточно ясный или известный нам смысл.
[01:10:04.640 --> 01:10:14.640]  Ну и естественно, коль скоро такие понятия, не зависит от того, в каком базе мы берем матрицу нашего оператора,
[01:10:14.640 --> 01:10:40.640]  то мы можем сказать, что эти значения называются следом и детерминатом фи.
[01:10:40.640 --> 01:10:48.640]  То есть мы можем писать понятия след фи и можем писать определение, понятие детерминат фи,
[01:10:48.640 --> 01:10:54.640]  как след любой матрицы этого фи или детерминат любой матрицы этого фи.
[01:10:54.640 --> 01:11:02.640]  Они не зависят от того, в каком базе мы эту матрицу записываем.
[01:11:02.640 --> 01:11:14.640]  Ну что ж, про характеристический многочлен мы с вами поговорили и про то, как собственные значения и собственные векторы искать мы тоже поговорили.
[01:11:14.640 --> 01:11:22.640]  Ну точнее мы сказали, как искать собственные значения, решить вот это вот уравнение, правильно?
[01:11:22.640 --> 01:11:25.640]  Ну а собственные векторы это как после этого искать?
[01:11:25.640 --> 01:11:37.640]  Собственные векторы это поиск ядра известного вам оператора.
[01:11:37.640 --> 01:11:45.640]  Вы нашли конкретную лямду, после этого вам нужно найти ядро фи-лямда, мы это уже знаем как делать, правильно?
[01:11:45.640 --> 01:12:03.640]  Так что собственные значения, собственные векторы мы тоже искать уже должны уметь.
[01:12:03.640 --> 01:12:12.640]  Если хотите, можем устроить перерыв раньше, но я бы хотел доказать еще одно важное подтверждение.
[01:12:12.640 --> 01:12:16.640]  А что, уже устали?
[01:12:16.640 --> 01:12:20.640]  Да, давайте хорошо, раз у нас все равно полторы пары, да?
[01:12:20.640 --> 01:12:26.640]  Давайте мы сейчас сделаем 10 минутный перерыв, а после этого уже двинемся дальше.
[01:12:26.640 --> 01:12:33.640]  Раз уж мы сделали паузу, то давайте прежде чем переходить дальше, я еще пару...
[01:12:33.640 --> 01:12:37.640]  Одно упражнение, одно замечание оставлю.
[01:12:37.640 --> 01:12:50.640]  Так, нехитрое упражнение, которое порой бывает полезным, говорит нам, что...
[01:12:50.640 --> 01:12:57.640]  Раз уж мы ввели новое понятие, то я предлагаю всем желающим доказать вот это вот равенство
[01:12:57.640 --> 01:13:04.640]  для любых матриц, для которых это равенство имеет смысл.
[01:13:04.640 --> 01:13:06.640]  На самом деле, для каких матриц имеет смысл?
[01:13:06.640 --> 01:13:12.640]  Какие матрицы можно перенажать и в этом, и в том порядке?
[01:13:12.640 --> 01:13:17.640]  Не только квадратные.
[01:13:17.640 --> 01:13:31.640]  Вы можете взять матрицу N на K в качестве матрицы A, и тогда матрица B придется вам брать размера K на N, правильно?
[01:13:31.640 --> 01:13:36.640]  А получаются квадратные хоть и разного размера.
[01:13:36.640 --> 01:13:41.640]  Вот даже в такой общности это утверждение все равно верно.
[01:13:41.640 --> 01:13:51.640]  Ну и замечание, которое я, может быть, сказал бы позже, но уж давайте, раз так пришлось, то скажу сейчас.
[01:13:51.640 --> 01:14:00.640]  Естественно, из того, что мы вывели, следует, что у оператора не слишком много собственных значений.
[01:14:00.640 --> 01:14:20.640]  Если phi – это линейное преобразование пространства V, размерность V равна N, то у phi не более N собственных значений есть.
[01:14:20.640 --> 01:14:27.640]  Потому что все собственные значения являются корнями вот этого характеристического многочлена.
[01:14:27.640 --> 01:14:35.640]  Его степень равна N, ну а значит и корней тоже не больше, чем N.
[01:14:35.640 --> 01:14:42.640]  Ну и вот теперь давайте мы пойдем дальше.
[01:14:42.640 --> 01:14:49.640]  Сначала у нас важное, то самое важное утверждение, которым я хотел сказать, уже не связанное с характеристическим многочленом.
[01:14:49.640 --> 01:14:57.640]  Мы выяснили, что собственные векторы и собственные значения мы все-таки уже понимаем более-менее, как искать.
[01:14:57.640 --> 01:15:02.640]  Практически это тоже будет реализовано.
[01:15:02.640 --> 01:15:05.640]  Как они расположены друг относительно друга?
[01:15:05.640 --> 01:15:09.640]  Верен следующий важный факт.
[01:15:09.640 --> 01:15:27.640]  Пусть у нас phi – это линейный оператор на пространстве V, а λ1 и так далее, лямдокаты различны его собственное значение.
[01:15:27.640 --> 01:15:36.640]  Тогда мы для каждого из этих лямд можем взять собственное подпространство.
[01:15:36.640 --> 01:15:41.640]  То есть множество всех собственных факторов, соответствующих этому лямду, пополненное нулем.
[01:15:41.640 --> 01:15:47.640]  И вот утверждение заключается в том, что если мы возьмем все эти собственные подпространства,
[01:15:47.640 --> 01:15:53.640]  то их сумма окажется прямой суммой.
[01:16:01.640 --> 01:16:06.640]  Что это значит, мы обсудим немножко позже.
[01:16:06.640 --> 01:16:11.640]  А пока что давайте мы это докажем.
[01:16:11.640 --> 01:16:15.640]  Как нам это доказать?
[01:16:15.640 --> 01:16:20.640]  Давайте предположим противное.
[01:16:20.640 --> 01:16:28.640]  Я, как обычно, напомню, что у нас происходило.
[01:16:28.640 --> 01:16:37.640]  Сумма называется прямой суммой, если каждый вектор из этой суммы раскладывается по компонентам единственным образом.
[01:16:37.640 --> 01:16:47.640]  И мы знаем критерий, что это происходит тогда и только тогда, когда нулевой вектор раскладывается по этим компонентам не единственным образом.
[01:16:47.640 --> 01:16:59.640]  То есть когда мы предполагаем противное, мы можем считать, что ноль оказался суммой k векторов из этих подпространств.
[01:16:59.640 --> 01:17:09.640]  Это v1 плюс и т.д. плюс vkt, где vi и т.д. это элемент v лямбда и т.д.
[01:17:09.640 --> 01:17:13.640]  Но и при этом не все эти векторы нули.
[01:17:13.640 --> 01:17:19.640]  Единственное разложение нуля было, конечно же, 0 плюс 0 плюс и т.д. плюс 0, в случае когда сумма прямая.
[01:17:19.640 --> 01:17:24.640]  Если сумма не прямая, то есть вот такое нетривиальное разложение.
[01:17:24.640 --> 01:17:28.640]  Не все vt нули.
[01:17:28.640 --> 01:17:38.640]  Давайте мы сразу ограничим себя только на те подпространства, из которых vt не нулевые.
[01:17:38.640 --> 01:17:47.640]  То есть если у нас, скажем, vkt оказалось нулем, то давайте просто забудем про эту лямбду кату и будем смотреть на остальные лямбы.
[01:17:47.640 --> 01:17:52.640]  Мы это сделать спокойно можем. Я же не сказал, что это все собственные значения.
[01:17:52.640 --> 01:18:17.640]  Так что мы можем считать, забывая про лишние индексы, мы считаем, что все vt не нули.
[01:18:17.640 --> 01:18:33.640]  Ну и давайте мы предположим, мы по-прежнему считаем, что лямбд собственных значений у нас k и все vt не нули.
[01:18:33.640 --> 01:18:43.640]  Ну и наконец давайте, пусть k у нас наименьшее число, для которого это возможно.
[01:18:43.640 --> 01:18:53.640]  По сути дела, что у нас возможно? Давайте смотреть.
[01:18:53.640 --> 01:19:02.640]  vt не нулевые, то есть они все являются собственными векторами с собственными значениями лямбд, правильно?
[01:19:02.640 --> 01:19:10.640]  И их сумма равна нулю. То есть мы говорим, что пусть k это наименьшее число.
[01:19:10.640 --> 01:19:19.640]  Такое, что мы можем взять k собственных векторов с различными собственными значениями, у которых сумма нулевая.
[01:19:19.640 --> 01:19:25.640]  Ну и давайте посмотрим, что нам дает вот это вот равенство.
[01:19:25.640 --> 01:19:33.640]  У нас есть равенство 0 равно v1 плюс и так далее, плюс vkt.
[01:19:33.640 --> 01:19:37.640]  Естественно, я должен сказать, что k больше единицы.
[01:19:37.640 --> 01:19:44.640]  Потому что если k равно единице, то у нас тут написано 0 равно v1, а v1 не нулевой.
[01:19:44.640 --> 01:19:47.640]  Этого быть не может, правильно?
[01:19:48.640 --> 01:19:56.640]  И мы можем к обеим частям этого равенства применить наш любимый phi.
[01:19:56.640 --> 01:20:04.640]  Что у нас получится? Слева у нас получится phi от 0, то есть, естественно, 0, правильно?
[01:20:04.640 --> 01:20:12.640]  А справа у нас получится phi от v1, плюс и так далее, плюс phi от vkt.
[01:20:13.640 --> 01:20:18.640]  Но все они собственные с известными нам собственными значениями.
[01:20:18.640 --> 01:20:27.640]  Поэтому на самом деле здесь написано лямбда 1 v1, плюс и так далее, плюс лямбда kt на vkt.
[01:20:27.640 --> 01:20:34.640]  Вот такая вот их линейная комбинация равна 0, и вот такая вот их линейная комбинация равна 0.
[01:20:34.640 --> 01:20:37.640]  Но из этого легко получить противоречие.
[01:20:37.640 --> 01:20:40.640]  Давайте мы отсюда...
[01:20:44.640 --> 01:20:49.640]  Вот у нас есть сумма phi от 1 до k лямбда i t v i t.
[01:20:49.640 --> 01:20:51.640]  Это мы взяли вот эту нулевую сумму.
[01:20:51.640 --> 01:20:56.640]  Давайте мы отсюда вычтем лямбда kt, умноженная на первую сумму.
[01:20:57.640 --> 01:21:05.640]  Сумма phi от 1 до k просто v i t.
[01:21:05.640 --> 01:21:08.640]  Что это у нас такое?
[01:21:08.640 --> 01:21:12.640]  Лямбда kt на vkt у нас сократится, правильно?
[01:21:12.640 --> 01:21:15.640]  Ну а остальные члены как раз не сократятся.
[01:21:15.640 --> 01:21:22.640]  Сумма phi от 1 до k-1, kт и член здесь, и kт и член здесь сократились.
[01:21:22.640 --> 01:21:24.640]  Что у нас здесь будет?
[01:21:24.640 --> 01:21:30.640]  Лямбда i t минус лямбда kt на v i t, правда?
[01:21:30.640 --> 01:21:33.640]  Что у нас получилось?
[01:21:33.640 --> 01:21:38.640]  Каждое такое слагаемое, оно не нулевое,
[01:21:38.640 --> 01:21:42.640]  потому что я взял не нулевой вектор и умножил его на не нулевой скаляр.
[01:21:42.640 --> 01:21:45.640]  Мы считаем все лямбда i t различными, правильно?
[01:21:45.640 --> 01:21:50.640]  Каждый такой вектор лежит, естественно, в v лямбда i t,
[01:21:50.640 --> 01:21:53.640]  и я взял собственный вектор и умножил его на скаляр.
[01:21:53.640 --> 01:21:59.640]  И значит у нас получилась сумма k-1 не нулевого собственного вектора
[01:21:59.640 --> 01:22:02.640]  с различными собственными значениями, которая тоже равна нулю.
[01:22:02.640 --> 01:22:04.640]  Этого быть не может.
[01:22:04.640 --> 01:22:11.640]  Это противоречие с выбором k.
[01:22:11.640 --> 01:22:17.640]  Мы сказали, что только k собственных векторов с различными собственными значениями
[01:22:17.640 --> 01:22:19.640]  могут в сумме давать ноль.
[01:22:19.640 --> 01:22:22.640]  А сейчас мы получили k-1 такой вектор,
[01:22:22.640 --> 01:22:27.640]  и при этом это не ноль векторов, потому что мы сказали, что k больше единицы.
[01:22:27.640 --> 01:22:33.640]  Таким образом, наша теорема доказана.
[01:22:33.640 --> 01:22:36.640]  И что это означает?
[01:22:36.640 --> 01:22:41.640]  Это означает, в частности, важную вещь.
[01:22:41.640 --> 01:22:45.640]  Давайте я скажу сразу про нашу теорему.
[01:22:45.640 --> 01:22:53.640]  Давайте мы вспомним еще один критерий прямой суммы.
[01:22:53.640 --> 01:23:06.640]  Если взять базисы в v лямбда i t,
[01:23:06.640 --> 01:23:10.640]  ну и составить их вместе, взять их инкотинацию,
[01:23:10.640 --> 01:23:18.640]  как мы помним, получится базис вот этой вот суммы пространств,
[01:23:18.640 --> 01:23:24.640]  то есть получится линейно-независимая система.
[01:23:24.640 --> 01:23:29.640]  Я взял базисы всех этих товарищей и составил их вместе,
[01:23:29.640 --> 01:23:32.640]  e1 и так далее, elt.
[01:23:32.640 --> 01:23:35.640]  Первая часть это базис v лямбда 1,
[01:23:35.640 --> 01:23:38.640]  следующая часть базис v лямбда 2 и так далее.
[01:23:38.640 --> 01:23:55.640]  Давайте мы дополним ее до базиса
[01:23:55.640 --> 01:24:03.640]  и спросим себя, любую линейно-независимую систему можно дополнить до базиса,
[01:24:03.640 --> 01:24:09.640]  и спросим себя, какая же будет матрица у нашего φ в этом базисе.
[01:24:23.640 --> 01:24:28.640]  Тогда в этом самом базисе у φ будет вот какая матрица.
[01:24:28.640 --> 01:24:30.640]  Давайте смотреть.
[01:24:30.640 --> 01:24:36.640]  Первые несколько векторов это собственные векторы с собственным значением лямбда 1.
[01:24:36.640 --> 01:24:41.640]  То есть каждый из них переходит в лямбда 1 на него,
[01:24:41.640 --> 01:24:45.640]  скажем там вот φ от e1, это лямбда 1 на e1.
[01:24:45.640 --> 01:24:49.640]  Что это нам говорит про матрицу?
[01:24:49.640 --> 01:24:53.640]  Конечно, мы должны в первый столбец нашей матрицы
[01:24:53.640 --> 01:25:00.640]  поставить φ от e1, расписанный по тому же самому базису e1 и так далее, e1.
[01:25:00.640 --> 01:25:02.640]  Но он расписан очень просто.
[01:25:02.640 --> 01:25:06.640]  Лямбда 1 и куча нулей.
[01:25:06.640 --> 01:25:13.640]  Если вектор с собственным значением лямбда 1 перейдет в лямбда 1 на e2,
[01:25:13.640 --> 01:25:18.640]  то здесь будет еще одно лямбда 1 и так далее.
[01:25:18.640 --> 01:25:23.640]  То есть в этой матрице будет некоторый блок из лямбда 1,
[01:25:23.640 --> 01:25:28.640]  затем некоторый блок из лямбда 2 и так далее.
[01:25:28.640 --> 01:25:31.640]  На диагонали будут стоять наши лямбды,
[01:25:31.640 --> 01:25:35.640]  а во всех остальных местах будут стоять просто-напросто нули.
[01:25:35.640 --> 01:25:41.640]  Ну и так будет продолжаться первая l столбцов, естественно.
[01:25:41.640 --> 01:25:46.640]  Что будет дальше, никому не известно.
[01:25:46.640 --> 01:25:51.640]  Вот такой вид будет у этой матрицы.
[01:25:51.640 --> 01:25:57.640]  Но это все равно очень здорово, потому что начало этой матрицы выглядит совсем хорошо.
[01:25:57.640 --> 01:26:08.640]  Ну и в частности, будет верно вот какое утверждение.
[01:26:08.640 --> 01:26:18.640]  Пусть фи – это линейный оператор на пространстве размерности n,
[01:26:18.640 --> 01:26:28.640]  и пусть у фи есть n различных собственных значений.
[01:26:28.640 --> 01:26:33.640]  Лямбда 1 и так далее, лямбда n.
[01:26:33.640 --> 01:26:45.640]  Тогда существует базис, в котором матрица фи имеет очень простой вид.
[01:26:45.640 --> 01:26:52.640]  По диагонали стоят лямбда 1 и лямбда 2.
[01:26:52.640 --> 01:26:57.640]  В котором матрица фи имеет очень простой вид.
[01:26:57.640 --> 01:27:04.640]  По диагонали стоят лямбда 1, лямбда 2 и так далее, лямбда n, а все остальные нули.
[01:27:04.640 --> 01:27:09.640]  Такая матрица называется диагональной и даже имеет специальное название.
[01:27:09.640 --> 01:27:14.640]  Часто пишут диаг, лямбда 1 и так далее, лямбда n.
[01:27:14.640 --> 01:27:20.640]  Диагональная матрица с вот такими элементами на диагонали.
[01:27:20.640 --> 01:27:26.640]  Из того, что мы сказали, это уже должно быть, наверное, очевидно.
[01:27:26.640 --> 01:27:29.640]  Как взять базис?
[01:27:31.640 --> 01:27:38.640]  Взять для каждого лямбда i по одному собственному вектору, правильно?
[01:27:38.640 --> 01:27:52.640]  Доказательство. Пусть i и t это собственный вектор с собственным значением лямбда i и t.
[01:27:52.640 --> 01:27:58.640]  Для каждого i мы возьмем по одному собственному вектору, больше нам и не надо.
[01:27:58.640 --> 01:28:09.640]  Е и т у нас все не нулевые по определению собственного вектора.
[01:28:09.640 --> 01:28:16.640]  Ну и более того, е1 и так далее, еn окажется линейной независимой системой.
[01:28:16.640 --> 01:28:21.640]  Это непосредственно следует вот из этой же самой теоремы, правильно?
[01:28:21.640 --> 01:28:31.640]  Если мы соберем по одному не нулевому вектору из прямых слагаемых, эта система будет линейно независима.
[01:28:31.640 --> 01:28:42.640]  Следовательно, раз размерность нашего пространства равна n, то это базис.
[01:28:42.640 --> 01:28:51.640]  Ну и наша φ имеет в этом базисе ровно такую матрицу, как нам обещали.
[01:28:51.640 --> 01:29:00.640]  Просто вот в этой вот самой матрице никакой звездочки у нас не будет, правильно?
[01:29:00.640 --> 01:29:06.640]  Тут может возникнуть какое-то сомнение.
[01:29:06.640 --> 01:29:14.640]  А что произойдет, если, скажем, для лямбда первого у вас будет не одномерное, а двумерное собственное пространство?
[01:29:14.640 --> 01:29:18.640]  Такого быть на самом деле не может.
[01:29:18.640 --> 01:29:29.640]  Потому что если в этой ситуации, когда у вас n различных собственных значений, хотя бы одно из собственных пространств окажется больше, чем одномерным,
[01:29:29.640 --> 01:29:34.640]  то вот эта прямая сумма будет иметь размерность больше, чем n, правильно?
[01:29:34.640 --> 01:29:35.640]  Этого быть не может.
[01:29:35.640 --> 01:29:40.640]  Так что, конечно, в этом случае все эти собственные пространства будут одномерными.
[01:29:40.640 --> 01:29:49.640]  И то, что мы берем по одному собственному вектору для каждого из лямбд, это никакое неупущение, больше мы взять и не сможем.
[01:29:49.640 --> 01:29:52.640]  Правильно?
[01:29:52.640 --> 01:30:08.640]  Давайте я тут уже сразу скажу немного более общее определение.
[01:30:08.640 --> 01:30:22.640]  Вы с вами увидели, что вот в этом случае, когда есть n различных собственных значений, у нас существует базис, в котором phi имеет диагональную матрицу.
[01:30:22.640 --> 01:30:28.640]  Это на самом деле очень хорошая ситуация, ее нужно как-то назвать.
[01:30:28.640 --> 01:30:46.640]  Мы говорим, что оператор phi на пространстве V диагонализуем,
[01:30:46.640 --> 01:31:03.640]  если в некотором базисе он имеет диагональную матрицу.
[01:31:03.640 --> 01:31:12.640]  Но в частности, то есть вот что мы сказали, что если у нас есть n различных собственных значений, то такой оператор обязательно диагонализуем.
[01:31:12.640 --> 01:31:20.640]  Иногда это же понятие применяют не к оператору, а к его матрице.
[01:31:20.640 --> 01:31:24.640]  В таком случае, наверное, стоит сказать вот какую вещь.
[01:31:24.640 --> 01:31:52.640]  Говорим, что матрица А диагонализуема, если она подобна диагональной матрице.
[01:31:52.640 --> 01:31:58.640]  Напоминаю, что значит она подобна диагональной матрице.
[01:31:58.640 --> 01:32:03.640]  Это означает, что она и какая-то диагональная матрица, это матрица одного и того же оператора.
[01:32:03.640 --> 01:32:10.640]  Так что по сути дела, матрица диагонализуема, если это матрица диагонализуемого оператора.
[01:32:10.640 --> 01:32:14.640]  Два понятия, одно и то же.
[01:32:14.640 --> 01:32:28.640]  Я сразу хочу заметить, что это понятие диагонализуемости матрицы не оператора, а матрицы.
[01:32:28.640 --> 01:32:44.640]  Диагонализуемость матрицы может зависеть от поля F.
[01:32:44.640 --> 01:32:52.640]  Если у вас есть, например, матрица с рациональными элементами, вы ее можете воспринимать как матрицу над Q,
[01:32:52.640 --> 01:32:56.640]  а можете ее, например, воспринимать как матрицу над R или над C.
[01:32:56.640 --> 01:33:04.640]  Вот может так случиться, что над Q она не диагонализуема, то есть нельзя придумать рациональную обратимую матрицу,
[01:33:04.640 --> 01:33:11.640]  такую, что подобие переводит ее в диагональную, а над R или на C она диагонализуемой оказаться может.
[01:33:11.640 --> 01:33:15.640]  Примеры скоро воспоследуют.
[01:33:15.640 --> 01:33:21.640]  Ну, например, пример я могу дать даже сейчас, конечно же, правильно?
[01:33:21.640 --> 01:33:29.640]  Если у вас у матрицы есть характеристический многошлен, у которого N различных собственных значений,
[01:33:29.640 --> 01:33:34.640]  и они не рациональны, то у нас уже окажется беда, конечно же.
[01:33:34.640 --> 01:33:47.640]  Так, давайте мы сразу немножко поговорим, прежде чем говорить о том, что означает диагонализуемость оператора,
[01:33:47.640 --> 01:33:53.640]  поговорим о том, когда она есть, поговорим о том, что она означает как раз.
[01:33:53.640 --> 01:34:08.640]  Давайте мы сразу скажем, что значит, что оператор Фи в некотором базе Се имеет диагональную матрицу.
[01:34:08.640 --> 01:34:14.640]  Обратите, пожалуйста, внимание, лямды у нас уже не обязательно различны, правильно?
[01:34:14.640 --> 01:34:21.640]  Он диагонализуем, если существует такой базис с не обязательно различными лямдами.
[01:34:21.640 --> 01:34:23.640]  Чем хороша эта ситуация?
[01:34:23.640 --> 01:34:28.640]  Ну, во-первых, такой оператор имеет ясное геометрическое описание.
[01:34:28.640 --> 01:34:30.640]  Что это означает?
[01:34:30.640 --> 01:34:41.640]  Это означает, если мы берем произвольный вектор, берем его координатный столбец в базе Се,
[01:34:41.640 --> 01:34:53.640]  это будет x1 и так далее, xn, тогда Фи от В будет иметь координатный столбец a на x,
[01:34:53.640 --> 01:34:57.640]  вот это вот у нас x, это у нас, конечно, a.
[01:34:57.640 --> 01:35:03.640]  Координатный столбец Фи от В получается умножением на матрицу a,
[01:35:03.640 --> 01:35:15.640]  то есть это будет просто-напросто лямда 1x1, лямда 2x2 и так далее, лямда nxn.
[01:35:15.640 --> 01:35:20.640]  То есть каждая координата спокойно себе умножается на свою соответствующую лямду.
[01:35:20.640 --> 01:35:22.640]  Ну что это означает?
[01:35:22.640 --> 01:35:26.640]  Геометрическая интуиция здесь, конечно, очень простая.
[01:35:26.640 --> 01:35:31.640]  Давайте представим себе, что n равно 3 для совсем наглядности.
[01:35:31.640 --> 01:35:35.640]  У нас есть 3 оси соответствующих вот этим вот 3-м базисным векторам.
[01:35:35.640 --> 01:35:40.640]  И наше преобразование состоит в растяжении в лямда 1 раз вдоль первой оси,
[01:35:40.640 --> 01:35:45.640]  в лямда 2 раз вдоль второй оси, в лямда 3 раз вдоль третьей оси.
[01:35:45.640 --> 01:35:51.640]  То есть геометрический смысл диагонализуемого оператора у нас достаточно простой,
[01:35:51.640 --> 01:35:58.640]  и поэтому имеет смысл понять, когда же оператор диагонализуем.
[01:35:58.640 --> 01:36:15.640]  Ну и, значит, еще, наверное, одно, два замечания, которые хочется сказать
[01:36:15.640 --> 01:36:22.640]  про диагонализуемый оператор, прежде чем мы будем говорить об условиях диагонализуемости.
[01:36:22.640 --> 01:36:27.640]  Вот я там написал лямда 1 и так далее, лямда n.
[01:36:27.640 --> 01:36:38.640]  Далее, в этом случае, в случае, когда оператор диагонализуем,
[01:36:38.640 --> 01:36:42.640]  какой будет характеристический многочлен нашего оператора?
[01:36:42.640 --> 01:36:54.640]  Ну я должен взять определитель A-XE, где A – это вот эта вот самая диагональная матрица,
[01:36:54.640 --> 01:36:58.640]  ну и получится, естественно, вот что такое.
[01:36:58.640 --> 01:37:05.640]  Лямда 1 минус X, лямда 2 минус X и так далее, лямда n-й минус X,
[01:37:05.640 --> 01:37:12.640]  потому что это вот и есть просто диагональные элементы той матрицы,
[01:37:12.640 --> 01:37:16.640]  у которой мы берем определитель, и она сама диагональна.
[01:37:16.640 --> 01:37:23.640]  Напоминаю, что вот это еще вот таким вот образом обозначается.
[01:37:23.640 --> 01:37:28.640]  То есть, естественно, в том случае, когда оператор диагонализуем,
[01:37:28.640 --> 01:37:33.640]  на диагонале у нас будут стоять именно его собственные значения,
[01:37:33.640 --> 01:37:37.640]  причем, обратите внимание, с учетом кратности, правильно?
[01:37:37.640 --> 01:37:45.640]  С учетом кратности их как корней характеристического многочлена.
[01:37:45.640 --> 01:38:01.640]  Ну и наконец, мы с вами можем сказать, что сумма этих лямдаитых равна следу нашего оператора,
[01:38:01.640 --> 01:38:04.640]  ну потому что она равна следу вот этой матрицы, правильно?
[01:38:04.640 --> 01:38:09.640]  А след любой матрицы один и тот же.
[01:38:09.640 --> 01:38:17.640]  А определитель phi будет равен произведению этих самых лямдаитов.
[01:38:20.640 --> 01:38:27.640]  Так, ну что ж, я разрекламировал диагональный вид оператора достаточно сильно вроде как,
[01:38:27.640 --> 01:38:31.640]  много про него хорошего сказал.
[01:38:31.640 --> 01:38:36.640]  Остается вопрос, когда же он существует?
[01:38:36.640 --> 01:38:39.640]  Всегда ли он существует?
[01:38:39.640 --> 01:38:42.640]  Вот как вы думаете, давайте я к залу обращусь.
[01:38:42.640 --> 01:38:48.640]  Любой ли оператор, любое ли линейное преобразование диагонализуемо?
[01:38:48.640 --> 01:38:55.640]  Матрица поворота, наверное, нет, там вообще нет собственных векторов.
[01:38:55.640 --> 01:39:02.640]  Значит, давайте я даже этот вопрос немножко вынесу.
[01:39:03.640 --> 01:39:11.640]  Любой ли оператор диагонализуем?
[01:39:11.640 --> 01:39:16.640]  Ну, кстати, матрица поворота, она не диагонализуема не над любым полем.
[01:39:16.640 --> 01:39:21.640]  Над r она не диагонализуема, а над c она диагонализуема.
[01:39:21.640 --> 01:39:24.640]  На c она как раз будет диагонализуема.
[01:39:24.640 --> 01:39:36.640]  И еще раз, глядите, говорят нам нет, и толстая причина для этого заключается в том,
[01:39:36.640 --> 01:39:42.640]  что, например, у характеристического многочлену может не быть корней.
[01:39:42.640 --> 01:39:49.640]  Если у него корней нет, то нет собственных значений, некого ставить на диагональ, правильно?
[01:39:49.640 --> 01:39:59.640]  Например, характеристический многочлен может не иметь корней.
[01:40:03.640 --> 01:40:11.640]  Более того, мы с вами только что увидели, что если фи диагонализуем,
[01:40:11.640 --> 01:40:15.640]  то не просто корни должны быть у характеристического многочлена,
[01:40:15.640 --> 01:40:19.640]  а он должен раскладываться на n линейных сомножителей, правильно?
[01:40:19.640 --> 01:40:23.640]  Как только у нас есть диагональная матрица, так это вот так вот.
[01:40:23.640 --> 01:40:33.640]  Или не раскладываться на линейные сомножители.
[01:40:33.640 --> 01:40:42.640]  К сожалению или к счастью, мир сложнее порой, чем кажется, и это не единственная причина.
[01:40:42.640 --> 01:40:53.640]  То есть я обращаю внимание, что существуют и другие причины,
[01:40:53.640 --> 01:40:57.640]  по которым оператор может быть недиагонализуем.
[01:40:57.640 --> 01:41:03.640]  То есть даже в том случае, когда характеристический многочлен таким вот образом на ноле раскладывается,
[01:41:03.640 --> 01:41:07.640]  может оказаться, что он недиагонализуем.
[01:41:07.640 --> 01:41:17.640]  Вот сейчас мы как раз начнем говорить о том, почему это может так случиться.
[01:41:22.640 --> 01:41:33.640]  Так, ну давайте, наверное, я сразу дам важное определение, которое нам пригодится в дальнейшем,
[01:41:33.640 --> 01:41:37.640]  а потом мы его немножко поисследуем.
[01:41:37.640 --> 01:41:53.640]  Итак, определение, пусть λ, это собственное значение оператора phi на пространстве V.
[01:41:53.640 --> 01:42:01.640]  То есть давайте я сразу скажу, что это значит.
[01:42:01.640 --> 01:42:09.640]  Это означает попросту, что она корень характеристического многочлена этого оператора, правильно?
[01:42:09.640 --> 01:42:35.640]  Так вот, тогда алгебраической кратностью этого самого лямда собственного значения лямда называется его кратность в алгебраическом смысле.
[01:42:35.640 --> 01:42:43.640]  У нас есть характеристический многочлен, у него есть корень лямда, у этого корня есть некоторая кратность, правильно?
[01:42:43.640 --> 01:42:54.640]  То есть кратность корня лямда в характеристическом многочлене.
[01:42:54.640 --> 01:43:04.640]  Ну, то есть напоминаю, какая степень х-л делит этот самый характеристический многочлен.
[01:43:04.640 --> 01:43:18.640]  А есть еще вторая кратность этого самого собственного значения.
[01:43:18.640 --> 01:43:39.640]  Геометрической кратностью собственного значения лямда называется размерность V лямда.
[01:43:39.640 --> 01:43:48.640]  То есть размерность вот этого самого собственного подпространства, соответствующего нашему лямду.
[01:43:48.640 --> 01:43:59.640]  То есть количество линейно-независимых собственных векторов с этим собственным значением, которое мы можем выбрать максимальное.
[01:43:59.640 --> 01:44:14.640]  Оказывается, что эти кратности связаны между собой. Ну и вот чтобы это доказать, давайте я сначала докажу чуть более общее утверждение.
[01:44:14.640 --> 01:44:33.640]  Значит, пусть phi это линейный оператор, а u это инвариантное подпространство относительно phi.
[01:44:33.640 --> 01:44:50.640]  Тогда, как мы говорили уже, можно взять ограничение phi на u и получить новый линейный оператор уже на подпространстве u.
[01:44:50.640 --> 01:45:05.640]  Так вот, в этом случае характеристический многочлен ограничения делит характеристический многочлен исходного оператора.
[01:45:05.640 --> 01:45:23.640]  На самом деле доказательство очень простое.
[01:45:23.640 --> 01:45:32.640]  Для него достаточно выбрать правильный базис.
[01:45:32.640 --> 01:45:53.640]  Давайте мы выберем базис e1 и так далее, en в v такой, что его начало, его префикс длины, скажем, k, это базис в нашем инвариантном подпространстве.
[01:45:53.640 --> 01:46:03.640]  Но мы с вами знаем, что такой базис всегда выбрать можно. Нам достаточно выбрать базис в u, а после этого дополнить его до базиса v.
[01:46:03.640 --> 01:46:21.640]  Тогда мы с вами прекрасно знаем, мы это уже говорили. Этот базис, как обычно, мы обозначаем через e. В этом базисе phi будет иметь следующую матрицу.
[01:46:21.640 --> 01:46:31.640]  Здесь у нас будут стоять какие-то три матрицы A, B, C, а здесь будет стоять ноль.
[01:46:31.640 --> 01:46:48.640]  А это не что иное, как матрица оператора psi в базисе e1 и так далее. Здесь я должен поставить базис уже подпространство u, вот в этом базисе у нас это и будет.
[01:46:48.640 --> 01:47:03.640]  Это мы все уже с вами говорили. Ну и значит, что такое характеристический многочлен нашего оператора phi?
[01:47:03.640 --> 01:47:23.640]  Это определитель матрицы минус хe. Как мы здесь будем вычитать хe? Напоминаю, у нас здесь вот матрица A имеет размеры k на k, матрица C имеет размеры n-k на n-k.
[01:47:23.640 --> 01:47:35.640]  Поэтому, когда мы здесь будем вычитать хe, мы вычтем матрицу хe размера k на k из A. Вот первая kx мы вычтем из диагонали матрицы A.
[01:47:35.640 --> 01:47:44.640]  Остальные n-kx мы вычтем из диагонали матрицы C. Поэтому здесь будет определитель вот какой блочной матрицы.
[01:47:44.640 --> 01:47:57.640]  Здесь я вычел A минус хe, здесь у меня осталась матрица B нетронутая, х туда не залезли, здесь остался по-прежнему 0, а здесь будет C минус х на e.
[01:47:57.640 --> 01:48:06.640]  Эти х вышлись как раз по диагонали вот этих двух матриц. Ну и следовательно, у нас получился определитель с углом нулей.
[01:48:06.640 --> 01:48:14.640]  А мы знаем, чему он равен. Он равен произведению вот этих вот двух матриц, определителей вот этих вот двух матриц.
[01:48:14.640 --> 01:48:31.640]  Определитель A минус хe на определитель C минус хe. Ну так и замечательно, потому что определитель A минус хe это и есть характеристический многочлен psi.
[01:48:31.640 --> 01:48:35.640]  Ну а определитель C минус хe это еще какой-то многочлен.
[01:48:35.640 --> 01:48:44.640]  Все, мы доказали, мы сказали, что характеристический многочлен phi есть характеристический многочлен psi умножить на еще какой-то многочлен.
[01:48:44.640 --> 01:48:46.640]  Этого нам и было нужно.
[01:48:46.640 --> 01:49:07.640]  Это утверждение, разумеется, полезно само по себе, но сейчас для нас пока что наиболее важным будет вот какое следствие из него.
[01:49:07.640 --> 01:49:36.640]  Пусть λ это собственное значение оператора phi, тогда его алгебравическая кратность не меньше геометрической.
[01:49:36.640 --> 01:50:03.640]  Доказательства очень простые. Теперь уже достаточно понять, какому инвариантному подпространству нам вот это утверждение применить.
[01:50:03.640 --> 01:50:11.640]  Какое у нас появляется инвариантное подпространство, если у нас есть собственное значение лямбда?
[01:50:11.640 --> 01:50:24.640]  В лямбда, конечно же. Давайте возьмем в качестве нашего инвариантного подпространства В лямбда, собственное подпространство собственным значением лямбда.
[01:50:24.640 --> 01:50:31.640]  Оно естественно инвариантно, потому что каждый его вектор переходит в лямбда, умноженный на него.
[01:50:31.640 --> 01:50:35.640]  Разумеется, оно инвариантно.
[01:50:35.640 --> 01:50:47.640]  А кто тогда такое будет ограничение phi на это самое подпространство?
[01:50:47.640 --> 01:50:56.640]  Что делает phi со всеми элементами этого подпространства? Умножает их на лямбду, это подпространство из собственных векторов.
[01:50:56.640 --> 01:51:05.640]  Поэтому наша psi в нашем утверждении это будет просто оператор, который умножает все векторы на лямбду.
[01:51:05.640 --> 01:51:20.640]  Ну и тогда матрица этого самого psi будет просто диагональной с лямбдами на диагонали.
[01:51:20.640 --> 01:51:30.640]  И характеристический многочлен этого самого psi будет равен чему?
[01:51:31.640 --> 01:51:39.640]  Лямдо минус х вентой, правильно. Мы вычитаем х по диагонали и берем определитель этой матрицы.
[01:51:39.640 --> 01:51:45.640]  Конечно, получается лямдо минус х вентой.
[01:51:45.640 --> 01:51:53.640]  Применяем наше утверждение, получаем, что вот этот вот самый характеристический многочлен,
[01:51:53.640 --> 01:52:01.640]  то есть лямдо минус х вентой, не вентой, давайте только. У нас здесь матрица размера k на k,
[01:52:01.640 --> 01:52:07.640]  n мы привыкли обозначать размером всего v, давайте мы скажем, что здесь у нас будет kt.
[01:52:07.640 --> 01:52:15.640]  Итак, этот характеристический многочлен, то есть лямдо минус х в кт, делит характеристический многочлен
[01:52:15.640 --> 01:52:21.640]  всего преобразования всего оператора phi.
[01:52:21.640 --> 01:52:33.640]  Ну а это означает, что кратность корня лямдо вот в этом вот многочлене,
[01:52:33.640 --> 01:52:37.640]  она уже никак не меньше, чем k.
[01:52:37.640 --> 01:52:50.640]  То есть алгебраическая кратность лямдо, то есть кратность его как корня в характеристическом многочлене,
[01:52:50.640 --> 01:53:00.640]  как корня вот этого самого характеристического многочлена, никак не меньше, чем k.
[01:53:00.640 --> 01:53:15.640]  Ну а k это и есть его геометрическая кратность, это размерность v лямдо, то есть его геометрическая кратность.
[01:53:15.640 --> 01:53:19.640]  И наше утверждение уже доказано.
[01:53:19.640 --> 01:53:27.640]  Итак, смотрите, что у нас получилось. Для каждого собственного значения, для каждого корня нашего характеристического многочлена,
[01:53:27.640 --> 01:53:36.640]  у нас алгебраическая кратность его, то есть кратность как корня, никак не меньше, чем геометрическая кратность.
[01:53:36.640 --> 01:53:45.640]  И если она окажется строго больше, то вот тут-то, как мы очень скоро увидим, и наступит проблема.
[01:53:45.640 --> 01:53:57.640]  Ну и давайте я сразу приведу пример того, что стать больше она может.
[01:53:57.640 --> 01:54:08.640]  Давайте мы рассмотрим, забегая вперед, эту матрицу я обозначу через g.
[01:54:08.640 --> 01:54:15.640]  Давайте даже так сделаем. Ну ладно, пусть будет так.
[01:54:15.640 --> 01:54:27.640]  Рассмотрим вот такую вот матрицу, и давайте поймем, что у нее происходит с собственными значениями.
[01:54:27.640 --> 01:54:34.640]  Ее характеристический многочлен это определитель вот такой вот матрицы.
[01:54:34.640 --> 01:54:40.640]  Я учитаю x на е, поэтому по диагонали стоит минус x.
[01:54:40.640 --> 01:54:46.640]  Это x квадрат. И следовательно, какие у нас получаются собственные значения?
[01:54:46.640 --> 01:54:52.640]  У нас собственное значение только 0, правильно? И он алгебраической кратности 2.
[01:54:52.640 --> 01:55:02.640]  λ равно 0. Это собственное значение алгебраической кратности 2.
[01:55:02.640 --> 01:55:19.640]  Какая у него геометрическая кратность? Геометрическая его кратность это просто размерность ядра нашего оператора,
[01:55:19.640 --> 01:55:25.640]  то есть размерность пространства решений системы с такой вот матрицей.
[01:55:25.640 --> 01:55:33.640]  Это размерность пространства решений системы уравнений жина x равно 0.
[01:55:33.640 --> 01:55:43.640]  И она, естественно, равна 1. Следовательно, здесь геометрическая кратность меньше, чем алгебрическая.
[01:55:43.640 --> 01:55:56.640]  Ну и давайте, раз уж мы до этого дошли, я теперь сформулирую теорему,
[01:55:56.640 --> 01:56:02.640]  а доказывать будем уже в следующий раз, потому что не успеем.
[01:56:02.640 --> 01:56:08.640]  С высоты нашего обзора доказательство будет уже на самом деле несложным,
[01:56:08.640 --> 01:56:16.640]  поэтому желающие могут понять самостоятельно, что они все это уже могут соединить.
[01:56:16.640 --> 01:56:21.640]  Но в любом случае в следующий раз мы, конечно, это докажем.
[01:56:21.640 --> 01:56:35.640]  Итак, теорема критерий диагонализуемости, критерии даже, потому что у нас их будет несколько.
[01:56:35.640 --> 01:56:50.640]  Пусть Фи это линейное преобразование пространства В, тогда равносильны следующие утверждения.
[01:56:50.640 --> 01:56:58.640]  Первое утверждение Фи диагонализуемо.
[01:56:58.640 --> 01:57:04.640]  Второе утверждение, практически сразу эквивалентное этому.
[01:57:04.640 --> 01:57:20.640]  В В существует базис, состоящий из собственных векторов, оператора Фи.
[01:57:20.640 --> 01:57:24.640]  Такой базис часто просто называется собственным базисом.
[01:57:32.640 --> 01:57:45.640]  Третье условие, то, что все пространство В раскладывается в прямую сумму собственных подпространств.
[01:57:50.640 --> 01:58:01.640]  Где лямбда 1 и так далее, лямбда kt, это, естественно, собственное значение оператора Фи.
[01:58:01.640 --> 01:58:04.640]  Ну то есть, глядите, что означает третье условие.
[01:58:04.640 --> 01:58:10.640]  Нам уже сказали, что сумма всех этих подпространств прямая, правильно?
[01:58:10.640 --> 01:58:14.640]  А тут нам говорят дополнительно, что она еще и В.
[01:58:14.640 --> 01:58:18.640]  То есть нетривиальная часть в том, что эта прямая сумма равна В.
[01:58:18.640 --> 01:58:23.640]  И четвертое условие, которое у меня сюда не влезет, поэтому я перейду дальше.
[01:58:29.640 --> 01:58:36.640]  Оно немножко более техничное, но зато как бы наиболее непосредственно проверяемое на практике.
[01:58:36.640 --> 01:58:50.640]  Характеристический многочлен нашего Фи раскладывается на линейные сомножители.
[01:58:50.640 --> 01:59:12.640]  И при этом у любого собственного значения, то есть у любого, разумеется, корня нашего характеристического многочлена,
[01:59:12.640 --> 01:59:21.640]  то есть геометрическая кратность равна алгебрической.
[01:59:29.640 --> 01:59:41.640]  Ну то есть тривиальное следствие из вот этого, вот эта матрица диагонализуемой быть не может, правильно?
[01:59:41.640 --> 01:59:48.640]  Потому что здесь есть собственное значение 0, у которого алгебрическая кратность равна 2, а геометрическая равна 1.
[01:59:50.640 --> 01:59:56.640]  В следующий раз мы начнем с доказательства всей этой теоремы. Я ее еще раз хотя бы вкратце выпишу.
[01:59:56.640 --> 02:00:00.640]  А на сегодня все. Вопросы, естественно, приветствуются.

[00:00.000 --> 00:12.320]  Сегодня мы поговорим про систему, которая, в отличие от прошлой лекции, помогает работать с данными.
[00:12.320 --> 00:18.480]  Помните, в прошлый раз мы говорили про ZooKeeper, и эта система позволяла строить, мы говорили,
[00:18.480 --> 00:22.760]  зачем она нужна, чтобы строить другие распределённые системы. Вот опачкавка,
[00:22.760 --> 00:27.040]  сегодняшняя наша система как раз использует ZooKeeper, мы сегодня свяжем прошлую лекцию с
[00:27.040 --> 00:32.600]  прошлой лекции с сегодняшней. В общем, следите за этим, там будет много забавных вещей.
[00:32.600 --> 00:40.720]  Итак, представим себе задачу. Вот мы большая компания, у нас есть сервис такси, у нас есть
[00:40.720 --> 00:47.520]  сервис доставки еды, у нас есть поиск, у нас есть... ну что у нас ещё есть? У нас есть рекламная сеть.
[00:47.520 --> 00:54.240]  В общем, у нас очень много разных сервисов, и, разумеется, это разные системы, они обслуживаются,
[00:54.240 --> 01:02.560]  разрабатываются разными подразделениями, деплоатируются на разных компьютерах. В общем,
[01:02.560 --> 01:10.080]  это такой огромный феодальный мир, где каждый усочек живёт более-менее, может быть, обособлено.
[01:10.080 --> 01:19.000]  И представим себе такую задачу. Вот вы пишете поиск, и для того, чтобы этот поиск был
[01:19.000 --> 01:24.400]  персенализирован, вам нужно агрегировать очень много разных источников данных. Вам полезно знать
[01:24.400 --> 01:30.760]  про пользователей, что он заказал себе, не знаю, суши или что-то другое заказал, или гамбургер.
[01:30.760 --> 01:35.840]  Вам может быть интересно, куда он поехал, вам важно, что он искал в поиске совершенно недавно,
[01:35.840 --> 01:41.520]  вам важно, на какие рекламные объявления он кликнул. Вам это всё нужно агрегировать в
[01:41.520 --> 01:47.000]  реальном времени для того, чтобы по вот этим данным рассчитывать какие-то профили пользователей,
[01:47.000 --> 01:53.800]  ну и персенализировать выдачу для него в поиске. И представим себе, что мы занимаемся машинным
[01:53.800 --> 02:00.640]  обучением и готовим поисковую формулу, которая будет ранжировать выдачу. Для этого нам нужны,
[02:00.640 --> 02:07.360]  в принципе, те же самые данные, нам нужны действия всех пользователей, все их клики в поисковой
[02:07.360 --> 02:12.440]  сессии, но, конечно же, они нужны не в реальном времени уже, а в системе, которая позволяет
[02:12.440 --> 02:18.000]  выполнять батч-убработку этих данных. Грубо говоря, нам нужно положить те же самые данные
[02:18.000 --> 02:24.320]  куда-то в reproduce для того, чтобы раз в неделю или раз в день или ещё чаще эти данные обрабатывать
[02:24.320 --> 02:32.080]  с помощью конвейера и высчитывать какие-то коэффициенты. Ну или можно придумать себе много разных
[02:32.080 --> 02:35.880]  таких приложений, в конце концов, не знаю, хочется для аудито просто иметь логи, что делали
[02:35.880 --> 02:43.120]  пользователи ваших сервисов. То есть у вас есть очень-очень много источников данных, данных
[02:43.120 --> 02:47.840]  разной природы, разной структуры, которые пишут с разными, которые генируются с разными сервисами,
[02:47.840 --> 02:52.720]  за которые отвечают совершенно разные люди. И есть довольно много потребителей этих данных,
[02:52.720 --> 02:57.280]  которые тоже очень по-разному устроены. Где-то real-time обработка, где-то batch обработка,
[02:57.280 --> 03:04.360]  где-то просто хранение. Ну а теперь представим, что вот этих всех производителей данных и этих
[03:04.600 --> 03:11.620]  потребителей данных нужно связать друг с другу. Это сделать, ну на прямую это сделать, конечно,
[03:11.620 --> 03:16.760]  не получится. Потому что каждый продюсер данных должен знать про каждого потребителя этих данных,
[03:16.760 --> 03:21.760]  они должны договориться о протоколе, они должны договориться о транспорте, они должны договориться
[03:21.760 --> 03:28.880]  там о сроках хранения данных. Они должны подобрать какие-то коэффициенты,大哥, не коэффициенты,
[03:28.880 --> 03:34.800]  должны настроить скорость отправки данных, какие-то лимиты. В общем, огромное количество
[03:34.800 --> 03:39.640]  технической работы, которыми ни продюсеры, ни консюмеры заниматься вообще-то не хотят,
[03:39.640 --> 03:44.600]  потому что зачем продюсеру поиску думать про там какого-то, какую-то подсистему,
[03:44.600 --> 03:50.280]  которая только начинает обрабатывать эти данные. И с другой стороны, если вы занимаетесь
[03:50.280 --> 03:54.640]  персонализацией, то вам будет очень сложно ходить и договариваться с каждым источником данных,
[03:54.640 --> 04:05.080]  делать это напрямую. Вот задача понятна, надеюсь, ну и она должна быть довольно понятна и за
[04:05.080 --> 04:10.080]  пределами какой-нибудь большой компании, и за пределами IT, потому что, ну в самом деле,
[04:10.080 --> 04:14.440]  представим себе такую задачу. У вас есть много компаний, которые просто торгуют своими акциями,
[04:14.440 --> 04:18.600]  у вас есть много покупателей потенциальных этих акций. Но покупатели, они очень маленькие,
[04:18.600 --> 04:26.640]  отдельные люди. Компания очень большие, и конечно же, Microsoft не зачем продавать акции каждому
[04:26.640 --> 04:33.120]  отдельному человеку. Для этого продавцов и покупателей, компаний и акционеры, они связаны
[04:33.120 --> 04:38.640]  с помощью брокера, который вот может дегрегировать покупки и покупать сразу много акций, ну в общем-то
[04:38.640 --> 04:50.720]  уменьшать накладные расходы на все эти операции. В нашей задаче, в нашем мире, где есть очень много
[04:50.720 --> 04:54.880]  потребителей данных и производителей, возникает та же самая задача, и можно решать это точно таким же
[04:54.880 --> 05:00.720]  образом. Можно производителей и потребителей, продюсеров и консюмеров, как мы будем называть
[05:00.720 --> 05:06.800]  сегодня, объединить с помощью брокера. Вот продюсеры и консюмеры друг по другу напрямую не знают,
[05:06.800 --> 05:11.360]  но, разумеется, они знают, что там, не знаю, персонализация знает про то, что там пользователь
[05:11.360 --> 05:17.080]  ездит на такси и хочет получить эти данные. Но напрямую сервисы, вот эти вот напрямую
[05:17.080 --> 05:24.560]  персонализация и такси друг с другом не договариваются. Они общаются друг с другом через
[05:24.560 --> 05:30.400]  некоторую шину сообщений, через некоторого брокера, который позволяет доставлять данные от
[05:30.400 --> 05:39.320]  продюсеров консюмеров. И этот брокер называется в нашем случае Apache Kafka. Kafka предоставляет вам
[05:39.320 --> 05:47.320]  довольно простую модель данных, а именно понятие данных, которое называется LOG. LOG — это Append-Only
[05:47.320 --> 05:52.320]  структура данных, в нее можно добавлять сообщения, ну и, в общем-то, все, что с ней можно делать, в смысле,
[05:52.320 --> 05:58.800]  из мутации. Нельзя исправлять сообщения, нельзя удалять сообщения, в середине, по крайней мере,
[05:58.800 --> 06:06.280]  нельзя их менять. Мы просто добавляем сообщения в LOG, и ему присваивается новый порядковый номер. А дальше
[06:06.280 --> 06:14.960]  эти сообщения могут читать клиенты. Это LOG — это не очередь, в том смысле, что клиенты, консюмеры данных,
[06:14.960 --> 06:22.760]  не достают эти самые сообщения, которые отправляет продюсер. Они их просто читают. Вот если мы говорим,
[06:23.120 --> 06:33.440]  про сессии пользователей или про клики на выдаче поисковой или на рекламу, или про заказы в сервисе
[06:33.440 --> 06:42.120]  доставки, то для каждого продюсера данных разумно завести свой собственный LOG, так чтобы в пределах
[06:42.120 --> 06:47.680]  этого LOGа хранились какие-то гомогенные сообщения. То есть, вот пользователь кликнул по такому
[06:47.680 --> 06:53.880]  урлу в выдаче. И вот можно себе представить, что огромный поисковый кластер, где многие тысяч,
[06:53.880 --> 06:59.160]  десятки тысяч машин обрабатывают клики и запросы пользователей, вот эти машины пишут свои логи и
[06:59.160 --> 07:07.240]  отправляют их в эту самую кавку, этот самый брокер. А кавка, в свою очередь, всем этим событиям
[07:07.240 --> 07:12.320]  назначает порядковые номера и укладывает вот один такой общий, видимо, отказоустойчивый
[07:12.320 --> 07:24.080]  распределенно хранящийся LOG. И дальше сервис, на котором работают поисковые демоны, он забывает
[07:24.080 --> 07:29.880]  про все, он же не думает про отдельных консюмеров, потому что теперь задача кавки, чтобы консюмеры
[07:29.880 --> 07:37.040]  могли эти данные вычитывать. Персонализация или какие-нибудь процессы, которые загружают данные
[07:37.040 --> 07:44.960]  в таблице, в produce, это все будет независимо уже от продюсера происходить. Консюмеров у одних
[07:44.960 --> 07:49.240]  этих данных, разумеется, может быть, много. Система batch обработки, система real-time обработки,
[07:49.240 --> 07:55.760]  с точки зрения кавки, с точки зрения лога, это отдельный консюмер, отдельный клиент. И у каждого
[07:55.760 --> 08:02.280]  клиента есть своя собственная позиция в логе, до которой он в данный момент дочитал. Разумеется,
[08:02.280 --> 08:08.360]  консюмеры могут читать в разном темпе, и никто из них данные не достает, они просто их учитывают,
[08:08.360 --> 08:17.240]  продвигают свой курсор дальше. В кавке лог, ну лог это такая абстрактная структура данных,
[08:17.240 --> 08:25.920]  в кавке этот лог называется топиком. И разумно, если мы говорим про хранение разных данных,
[08:25.920 --> 08:30.720]  для разного источника данных, для разных продюсеров заводить разные топики, чтобы в пределах
[08:30.720 --> 08:35.440]  топика данные были какими-то однородными. Клики, или поиски, или заказы, или что-то подобное.
[08:35.440 --> 08:48.920]  Давайте посмотрим на то, как с этими топиками дальше можно работать. Вот в кавке все клиенты,
[08:48.920 --> 08:53.200]  все пользователи выступают в двух ролях. Либо продюсеры, либо консюмеры. Продюсеры
[08:53.200 --> 09:00.360]  загружают данные в топики, консюмеры читают данные из топиков. Вот, пожалуйста,
[09:00.360 --> 09:07.440]  пример консюма, пример продюсера. Вот мы создаем клиента и в цикле отправляем в какой-то топик,
[09:07.440 --> 09:16.600]  который называется mytopic. Давайте это схлопнем. Отправляем 100 записей. Записи – это пара ключа
[09:16.600 --> 09:22.600]  значения, как здесь видно. Вот ключ, вот значение. Но это всего лишь такая фиксированная структура
[09:22.600 --> 09:28.760]  записи. Семантик этих ключей и значений в кавке, ну в данном по крайней мере случае, в общем случае,
[09:28.760 --> 09:35.560]  не фиксировано. Это просто некоторые блогики, и кавка их внутри себя никак не интерпретирует.
[09:35.560 --> 09:43.080]  Пусть продюсер и консюмер за пределами системы договорятся о том, каково же содержимое этих
[09:43.080 --> 09:48.840]  записей, в каком они формате. Там может быть JSON, там может быть протобув, там могут быть просто
[09:48.840 --> 09:55.400]  какие-то строчки, могут быть бинарные данные, что угодно. Про конкретный формат, про протокол
[09:55.400 --> 09:59.720]  договариваются снаружи системы, а кавка выступает здесь просто посредником.
[09:59.720 --> 10:08.480]  Вот мы видим, что есть интересного, что мы в клиенте указываем точку входа, какую-то машину,
[10:08.480 --> 10:15.280]  через которую мы узнаем дальше, где же кавка будет данные этого топика хранить. Вот мы здесь
[10:15.280 --> 10:21.160]  указываем число ретроев. Что будет, если мы ретрои повысим, но в случае, если вдруг что-то где-то
[10:21.160 --> 10:26.200]  залипнет, мы можем сделать ретрои одного и того же сэнда, и в результате мы можем получить
[10:26.200 --> 10:34.200]  потенциально в топике дубли этих же данных. В случае ретрои 0 мы говорим, что мы ретрои делать не хотим.
[10:34.200 --> 10:38.840]  Но мы можем, конечно, бачить отправку, то есть мы можем отправлять не по одной записи в кавку,
[10:38.840 --> 10:45.360]  это было бы абсолютным безумием в высоконагруженной системе. Мы хотим группировать, накапливать пачки
[10:45.360 --> 10:53.960]  сообщений и отправлять их уже вот такими большими блоками. Ну, serializer, я уже сказал,
[10:53.960 --> 11:00.000]  что кавка не интерпретирует содержимое этих данных. Про протокол договариваются вот здесь,
[11:00.000 --> 11:07.360]  в клиенте продюсер и в клиенте консюмер. Вот продюсер занимается тем, что он отправляет
[11:07.360 --> 11:13.920]  данные в кавку. Консюмер занимается тем, что он читает данные из кавки и из топика. Вот мы
[11:14.240 --> 11:26.160]  давайте пример попроще найдем. Вот мы здесь создаем консюмера. Мы подписываемся на топике и дальше в цикле,
[11:26.160 --> 11:32.520]  ну потому что сам топик это бесконечный лог, он где-то начинается, когда-то дальше растет
[11:32.520 --> 11:38.840]  непрерывно. Мы в этом бесконечном цикле обрабатываем данные из топика, не знаю, читаем какие-нибудь
[11:38.840 --> 11:48.000]  запросы пользователей, как-то их анализируем. Для этого у нас есть вызов пол. Консюмер использует
[11:48.000 --> 11:54.800]  пол, то есть мы опрашиваем кавку, когда у нее появляются данные. Пол означает, что мы готовы
[11:54.800 --> 12:03.520]  ждать 100 миллисекунд, пока кавка не наберет либо достаточно много сообщений, мы ждем пока,
[12:03.520 --> 12:09.160]  либо сама кавка не наберет достаточно много сообщений для нас, либо просто истечет тайм-аут,
[12:09.160 --> 12:16.840]  и мы получим очередную пачку записи. Ну а дальше мы их как-то обработаем, в данном случае просто
[12:16.840 --> 12:30.000]  напечатаем на экран. У консюмера, разумеется, есть свое собственное состояние, потому что что
[12:30.000 --> 12:38.640]  делать, если консюмер вдруг перезагрузится? С какого места он начнет читать? Это, собственно,
[12:38.640 --> 12:47.320]  стейт консюмера, его состояние, и этот стейт называется оффсетами, то есть для топика мы знаем,
[12:47.320 --> 12:54.520]  до какого индекса мы уже дочитали, с какого индекса нам нужно начать в следующий раз. И в данном
[12:54.520 --> 13:03.600]  примере эти оффсеты фиксируют, ну кто отвечает за хранение этих самых оффсетов? Это достаточно
[13:03.600 --> 13:11.360]  тонкий момент, важный момент, за хранение оффсетов отвечает сама система. Клиент не должен помнить,
[13:11.360 --> 13:17.280]  на каких позициях он остановился, просто потому что клиент, ну понятно, мы говорим про определенную
[13:17.280 --> 13:25.200]  систему, клиент может отказывать, клиент может переехать на другую машину, и пусть его оффсеты,
[13:25.200 --> 13:35.520]  его состояние хранится централизовано в кавке. И вот эта настройка отвечает за то, что сам клиент
[13:35.520 --> 13:42.720]  кавки, то есть сам консюмер, будет периодически в кавку фиксировать текущий прогресс клиента. Ну
[13:42.720 --> 13:51.520]  какой позиции он начитал. Мы доверяем здесь самому клиенту и не забоимся вообще об этом. Ну вот такая
[13:51.520 --> 13:58.920]  общая модель, есть продюсеры, есть консюмеры, продюсеры добавляют в лог, в топик, консюмеры из
[13:58.920 --> 14:05.840]  этого лога топика читают записи, ну и периодически передвигают свой курсор персистентный вперед.
[14:05.840 --> 14:27.840]  А как связаны консюмеры и клиента, не очень понятно. Клиент кавки, он может быть в двух ролях,
[14:27.840 --> 14:33.160]  либо продюсер, либо генерирует данные, пишет их в кавку, либо он консюмер, то есть он читает из
[14:33.160 --> 14:48.520]  кавки. Был еще какой-то вопрос с эхом. Это он и был. Ну вот такая модель данных, но на самом деле
[14:48.520 --> 14:56.160]  модель данных чуть сложнее, потому что кавка, конечно, не смогла бы для каждого логического
[14:56.160 --> 15:02.680]  потока данных, логического потока записи заводить просто вот отдельный такой лог-топик.
[15:02.680 --> 15:07.240]  Ну представьте себе, вот вы пишете поисковые системы, сколько там поисков секунды происходит,
[15:07.240 --> 15:12.520]  и сложно представить, что кавка сможет все эти там запросы пользователей упорядочивать,
[15:12.520 --> 15:20.120]  каждому из них присваивать такой вот порядковый номер, который еще непрерывно растут. Понятно,
[15:20.120 --> 15:24.920]  что чтобы присваивать записям, сообщениям продюсеров порядковые номера, нужно вот где-то
[15:24.920 --> 15:30.800]  завести машину, которая через себя будет все это пропускать и упорядочивать. Вот разумеется,
[15:30.800 --> 15:38.000]  одна машина с этим не справится. Кроме того, с другой стороны, у нас есть консюмеры, и они тоже
[15:38.000 --> 15:43.640]  читают из топика, и разумеется, они хотят читать параллельно. Ну то есть у вас опять много данных,
[15:43.640 --> 15:49.600]  и вы готовы учитывать и обрабатывать их на многих машинах. Ну скажем, вы загружаете данные в
[15:49.600 --> 15:54.000]  mupreduce, почему бы вам не писать в mupreduce параллельно? Сам mupreduce это прекрасно позволяет.
[15:54.000 --> 16:05.320]  Так что в модели данных кавки топик, в свою очередь, делится на партиции. Вот топик это,
[16:05.320 --> 16:13.680]  логически, это такие однородные данные, но физически эти данные, эти сообщения одного
[16:13.680 --> 16:21.800]  топика, представлены в виде набора партиций. И каждая партиция уже является логом. Вот внутри
[16:21.800 --> 16:28.440]  каждой партиции номерация сообщений, такая сквозная, непрерывная, но между партициями никакой связи
[16:28.440 --> 16:36.500]  нет. Партиции более-менее независимы. То есть мы ожидаем, что в пределах одного топика однородные
[16:36.500 --> 16:43.700]  данные и на уровне, и внутри кавки, эти данные хранятся в виде набора партиции, в виде набора логов.
[16:43.700 --> 16:54.460]  Когда мы пишем в топик, вот, например, в этом примере, то мы отправляем очередное сообщение
[16:54.460 --> 17:00.940]  в какую-то партицию. В самом простом случае это происходит просто по-хорошему от ваших данных.
[17:06.500 --> 17:13.820]  Ну и таким образом вы можете параллелить работу продюсеров легко. У вас много продюсеров, там, не знаю,
[17:13.820 --> 17:19.540]  тысячи машин, они пишут в кавку, они пишут в разные партиции, ну и вы выбираете число партий,
[17:19.540 --> 17:25.140]  вы их конфигурируете исходя из ожидаемой нагрузки на запись и на чтение. Если у вас данных мало,
[17:25.140 --> 17:32.380]  ваш топик может быть одной партицией. Если у вас данных очень-очень много, то вы этот топик
[17:32.380 --> 17:39.460]  дробите нам много партиций. Нужна ли вам сквозная нумерация? Ну, вопрос. Как правило, вы можете без
[17:39.460 --> 17:44.580]  нее все же обойтись. Вы можете, там, не знаю, вам в пределах пользователя может быть нужна одна
[17:44.580 --> 17:49.780]  сквозная нумерация. Ну, тогда вы сделаете так, чтобы записи одного пользователя были в одной
[17:49.780 --> 17:55.940]  партиции. Но сами партиции могут быть друг от друга, сами партиции друг от друга независимы.
[17:55.940 --> 18:04.780]  Это такой способ промасштабироваться в пределах одного топика. Ну, и если у вас много партиций,
[18:04.780 --> 18:10.820]  то, разумеется, писать в эти партиции параллельно очень легко. А вот читать чуть сложнее,
[18:10.820 --> 18:18.180]  потому что, ну представьте, у каждого клиента есть прогресс, у каждого клиента есть позиция в
[18:18.180 --> 18:25.700]  этом логе, на который он остановился. И сложно представить себе, как могли бы несколько независимых
[18:25.700 --> 18:29.940]  клиентов параллельно читать одну партицию. Ну, то есть, им же нужно друг с другом данные,
[18:29.940 --> 18:39.260]  им нужно поделить между собой данные, которые они вычитывают. Вот кавка позволяет вам из топика
[18:39.260 --> 18:46.940]  читать параллельно, просто распределяя партиции топика между консюмерами. Один консюмер может
[18:46.940 --> 18:53.380]  читать несколько партиций, но одну партицию читает только один консюмер. Ну, разумеется,
[18:53.380 --> 18:59.220]  если у вас консюмеры просто логически разные, там реал-тайм какой-нибудь, процессинг и бач
[18:59.220 --> 19:05.060]  процессинг, то это вот разные группы консюмеров. И, конечно же, они могут читать одни те же партиции.
[19:05.060 --> 19:12.020]  Но если у вас консюмер логически один, просто он представлен в виде набора машин, то в этом
[19:12.020 --> 19:19.020]  случае кавка сама умеет распределить партиции топика между вот конкретными узлами клиентами
[19:19.020 --> 19:26.260]  и консюмерами. Вы можете не заботиться о том, как это распределение сделать, потому что,
[19:26.260 --> 19:32.700]  понятно, у вас отдельные узлы, на которые запущены консюмеры, могут отказывать, могут
[19:32.700 --> 19:37.540]  перезагружаться, консюмеры могут пропадать, могут добавляться новые. Так вот, в кавке для этого
[19:37.540 --> 19:43.180]  устроен довольно нетривиальный протокол, который обеспечивает балансировку партий
[19:43.180 --> 19:49.820]  между консюмерами. Когда вы подключаетесь к кавке, вы задаете группа ID. И если вы параллельно
[19:49.820 --> 19:56.300]  с разных машин подключаетесь к кавке к одному и тому же топику, ну, с одним и тем же группой,
[19:56.300 --> 20:03.860]  к одному и тому же топику, с одним и тем же группой ID, то кавка считает, что это логически один клиент,
[20:03.860 --> 20:10.260]  и между всеми вот конкретными инстанциями этого клиента нужно партиции распределить. Для этого
[20:10.260 --> 20:15.500]  в кавке выбирается, среди узлов кавки, про которые мы пока не говорили, выбирается узел,
[20:15.500 --> 20:26.980]  который играет роль координатора, координатора группы, и дальше координатор группы вот в одиночку
[20:26.980 --> 20:33.860]  распределяет партиции между конкретными узлами-консюмерами. Если вдруг появился новый консюмер,
[20:33.860 --> 20:39.220]  или если вдруг отказал один из существующих консюмеров, ну, просто потому что машина
[20:39.220 --> 20:46.420]  взорвалась его, то в первом случае координатор получает от клиента сообщение, что вот я новый
[20:46.420 --> 20:51.340]  консюмер, я готов присоединиться к группе, или в случае, когда клиент-консюмер умирает,
[20:51.340 --> 20:59.460]  брокер-координатор, на брокере-координаторе истекает тайм-аут, ну, в общем, по одному из этих событий
[20:59.460 --> 21:05.020]  координатор понимает, что нужно пересмотреть распределение партиции между консюмерами одной
[21:05.020 --> 21:11.940]  группы и проводит их там, ну, можно сказать, что через барьер. Вот, если вы помните, мы писали
[21:11.940 --> 21:16.940]  барьер на кондварах, но вот тут такой же барьер нужно, чтобы все консюмеры перестали читать,
[21:16.940 --> 21:26.020]  и мы с ними передоговоримся о составе, о наборе живых консюмеров и о распределении партий.
[21:26.020 --> 21:32.700]  Вот просто так добавить нового консюмера, конечно же, нельзя, потому что он начнет читать
[21:32.700 --> 21:37.140]  какой-то топик, а в то же время какой-то другой консюмер его уже раньше читал,
[21:37.140 --> 21:42.460]  и он начнет обновлять офсеты для данной партиции, ну, и в общем, тут случится какая-то
[21:42.460 --> 21:47.500]  согласованность, непонятно, кто именно читает партицию, кто именно двигает для нее курсор.
[21:47.500 --> 21:56.460]  Вот курсор при чтении из топика, он опять же связан с этим самым группой ID,
[21:56.460 --> 22:03.860]  то есть с логическим консюмером, и он свой для каждой партиции. Вот, то есть машины – это
[22:03.860 --> 22:08.740]  отдельные машины, отдельные узлы, которые запускают на себе клиентов, это уже следующий
[22:08.740 --> 22:14.700]  уровень, и офсеты хранятся не для машин, разумеется, а для таких вот логических клиентов. Поэтому
[22:14.700 --> 22:20.180]  координатор нужен здесь за тем, чтобы аккуратно перераспределять партиции и не допускать вот
[22:20.180 --> 22:25.820]  таких вот гонок, когда один консюмер, казалось бы, умер, у него протухла сессия, а потом он появился
[22:25.820 --> 22:33.500]  и закомитил старый офсет. Проблема понятна, идея понятна? Что будет, если координатор умрет?
[22:33.500 --> 22:43.420]  Если координатор умрет, то это не страшно, потому что выберется другой. Это узел системы,
[22:43.420 --> 22:50.740]  и эта роль, конечно, не закреплена за каким-то конкретным узлом, потому что клиентов более-менее
[22:50.740 --> 22:59.260]  произвольное число. То есть они опять сами между собой общаются, посылают храбиты и выбирают лидеров?
[22:59.260 --> 23:06.980]  Ну, это сделано не так, и давай мы, наверное, если про модель данных понятно, что вот есть
[23:06.980 --> 23:13.020]  топики, в которые должны лежать однородные сообщения, эти топики делятся на партиции
[23:13.020 --> 23:17.740]  для параллелизма, для масштабирования, есть продюсеры-консюмеры, продюсеры пишутся,
[23:17.740 --> 23:23.380]  топики добавляют в конец, консюмеры читают из топика и фиксируют свой прогресс, фиксируют
[23:23.380 --> 23:31.540]  позицию курсора в каждом топике, в каждой партиции, и есть балансировка партиции между экземплярами,
[23:31.540 --> 23:36.980]  физическими экземплярами одного логического клиента. Вот если с моделью данных понятно,
[23:36.980 --> 23:41.980]  то можно перейти к тому, как это все физически выпущено самой системе, потому что вопрос уже
[23:41.980 --> 23:49.620]  про реализацию. Но реализовано это более-менее стандартным образом.
[23:53.060 --> 23:57.220]  И давайте все же про модель данных еще одну вещь поговорим, прежде чем говорить,
[23:57.220 --> 24:06.140]  как это все хранится и как там кто выбирается. Поговорим про консюмера. Вот здесь мы сказали,
[24:06.140 --> 24:13.980]  что консюмер просто автоматически фиксирует свой прогресс. Сам кавка-консюмер где-то в фоне,
[24:13.980 --> 24:18.620]  где-то в фоновом потоке периодически уведомляет систему о том, что вот клиент дочитал до какой-то
[24:18.620 --> 24:25.580]  позиции и можно передвинуть его offset вперед. Но может быть мы хотим действовать аккуратнее,
[24:25.580 --> 24:35.180]  мы хотим вручную управлять фиксацией текущих offset. Представим себе вот такой код. Консюмер
[24:35.180 --> 24:41.220]  здесь устроен чуть сложнее, он опять в бесконечном цикле читает данные, он из них накапливает какие-то
[24:41.220 --> 24:49.300]  пачки и если он накопил пачку какого-то достаточного размера для каких-то своих целей, то он эти данные
[24:49.300 --> 24:58.380]  перекладывает в базу. Ну то есть вот этот код, этот клиент это просто он использует кавку как такой
[24:58.380 --> 25:04.740]  персистентный буфер для данных, для того чтобы продюсер туда писали данные, а вот этот клиент
[25:04.740 --> 25:09.100]  будет их перекладывать в базу для быстрой аналитики. Ну представим себе какой-нибудь
[25:09.100 --> 25:14.180]  криг хаус, который не тормозит как известно. Так вот представим, что мы здесь пишем самый
[25:14.180 --> 25:22.420]  условный криг хаус в базу данных, а потом мы фиксируем offset. Вот здесь мы делаем это вручную.
[25:22.420 --> 25:30.780]  Подумаем, что происходит с этим кодом в случае отказа, в случае рестарта клиента, рестарта машины,
[25:30.780 --> 25:37.100]  на которой запущен клиент или в случае, когда вот машина целиком умирает и этот клиент для вот
[25:37.100 --> 25:48.340]  тех же топиков, тех же партий, переезжает на другую машину. Ну вот представим себе,
[25:48.340 --> 25:55.940]  что машина перезагрузилась после этой строчки и до этой строчки. В этом случае после перезапуска
[25:55.940 --> 26:06.740]  клиент, до запуска клиента, не знаю, прочел 100500 записей из партий из топика и записал их в базу,
[26:06.740 --> 26:15.540]  но не успел сделать последний commit sync и не зафиксировал, что последние 100 записей действительно
[26:15.540 --> 26:23.300]  были им прочитаны и обработаны. Он перезагрузился и его offset равен сейчас 100400, но при этом вот в
[26:23.300 --> 26:31.300]  этой базе лежит уже 100500 записей из топика. Он перезапустится и вот последние 100 записей в базе
[26:31.300 --> 26:39.260]  получается продублируют. Так мы получаем семантику at least once. Мы гарантируем, что каждое сообщение из
[26:39.260 --> 26:49.180]  топика, по крайней мере один раз, окажется в этой внешней базе данных. Если нас это не устраивает,
[26:49.180 --> 26:54.860]  то мы можем использовать то, что мы можем сделать. Мы можем переставить эти строчки местами. Мы сначала
[26:54.860 --> 27:01.420]  зафиксируем offset, до которых мы дочитали, а потом положим данные в базу. В этом случае,
[27:01.420 --> 27:07.860]  если мы перезагрузимся после commit sync, но до insert ntdb, то мы получим семантику at most once,
[27:07.860 --> 27:14.780]  то есть каждая запись из топика будет обработана, попадет в базу не более одного раза.
[27:14.780 --> 27:25.620]  Ну вот тут мы в таком API, то есть при синхронном комите offset, мы хотя бы управляем этим. В случае,
[27:25.620 --> 27:33.900]  когда мы ставим галку autocommit, то commit offset происходит вообще фоново и без понятных для нас гарантий.
[27:33.900 --> 27:42.580]  Ну вот, конечно же, нам хотелось бы иметь семантику, как мы понимаем, exactly once, когда каждая
[27:42.580 --> 27:48.300]  запись из топика будет обработана ровно один раз. Ну вот как этого достичь, мы поговорим чуть позже.
[27:48.300 --> 27:55.460]  А пока, как все эти топики, партиции хранятся внутри кавки? Ну как обычно, кавка — это система,
[27:55.460 --> 28:01.900]  которая состоит из некоторого набора узлов. Эти узлы находятся в каких-то стойках, и каждый
[28:01.900 --> 28:10.260]  узел называется брокера. Наши топики поделены на партиции, и каждый брокер отвечает за хранение
[28:10.260 --> 28:16.860]  какого-то набора партиции. Партиции мы считаем небольшими, то есть каждая партиция должна
[28:16.860 --> 28:26.860]  умещаться в одну машину, в одного брокера, в один диск. Ну и разумеется, реплики партиции разумно
[28:26.860 --> 28:34.660]  раскладывать по разным стойкам, чтобы учитывать домины отказов, использовать, реализовывать
[28:35.260 --> 28:43.380]  для того, чтобы повысить доступность наших данных. На что это похоже? Ну это такой более-менее
[28:43.380 --> 28:51.220]  стандартный дизайн. Вот представьте себе HDFS. Вот там же что-то похожее. У вас есть много датанод,
[28:51.220 --> 29:00.700]  они хранят кусочки файлов, в чанке файлов. Ну и как и в HDFS, для такой раскладки нужен какой-то
[29:00.700 --> 29:06.020]  узел, который будет координировать хранение этих данных, вообще понимать, где что лежит,
[29:06.020 --> 29:15.940]  на каких машинах брокерах хранятся какие партиции, каких топиков. Если мы вспомним HDFS, то вот за это
[29:15.940 --> 29:26.620]  отвечает у нас кто? Узел NameNode. Такой вот мастер, который хранит метаданные. Карту кластера. В кавке
[29:26.620 --> 29:33.540]  тоже есть подобная централизация. Тоже есть узел, который называется координатором, который как
[29:33.540 --> 29:41.180]  раз и следит за всеми партициями, за всеми брокерами и знает про расположение, про отображение партиции
[29:41.180 --> 29:48.540]  топиков в физические машины. Но, разумеется, кавка должна быть готова к тому, что вот такой узел
[29:48.540 --> 29:55.060]  координатора откажет. Поэтому этот узел нефиксированный. Поэтому координатор – это не конкретный узел,
[29:55.060 --> 30:01.900]  это роль. И в случае отказа координатора любой другой узел системы может на себя взять его
[30:01.900 --> 30:11.140]  функции. Каким образом выбирается координатор? Для этого кавке нужен зукипер. Но не только для этого,
[30:11.140 --> 30:17.580]  но для разных вещей. Вот кавке нужен зукипер и один из поводов использовать зукипер – это выбор
[30:17.580 --> 30:24.380]  лидера, то есть выбор координатора. Координатор просто берет зукипер и блокировку. То есть у него узел,
[30:24.380 --> 30:28.060]  который хочет стать координатором, берет зукипер и блокировку и становится координатором.
[30:28.060 --> 30:36.180]  Если возвращаться к вопросу про координатора группы клиентов, то опять такая же переходящая роль.
[30:36.180 --> 30:42.580]  Она не прибита жесткоконкретному узлу, это всего лишь роль отдельного брокера. Каждый брокер может
[30:42.580 --> 30:51.260]  стать координатором для группы клиентов. Идея понятна?
[30:51.260 --> 31:03.620]  Понятно. Тогда следующий вопрос, который нас волнует. Что же делать? Отлично, любой узел может
[31:03.620 --> 31:08.700]  стать координатором, но координатор уже нужно знать, что в системе происходит. Ему нужно знать про
[31:08.700 --> 31:15.380]  все брокеры, про все партиции, где что лежит. Так вот, координатор это все на своем жестком диске,
[31:15.380 --> 31:20.060]  конечно, не хранит, потому что координатор может умереть. Мы используем довольно стандартную схему,
[31:20.060 --> 31:30.540]  мы отделяем точку обслуживания от самих данных. Вот метаданные про кавку, не все метаданные, но часть.
[31:30.540 --> 31:38.940]  Кавка хранит в зукипере. И когда узел становится координатором, он просто из зукипера учитывает
[31:38.940 --> 31:45.820]  нужные ему для работы данные. Если координатор меняет распределение данных на кластере, то он
[31:45.820 --> 31:51.420]  записывает это в зукипер. Ну а зукипер здесь уже является такой отказоустойчивой памятью,
[31:51.420 --> 32:00.220]  отказоустойчивым диском. Он не ломается. Ну и разумеется, зукипер помогает нам отслеживать,
[32:00.220 --> 32:04.860]  то есть делать фенсинг, про который мы в прошлый раз говорили. Что делать, когда у нас старый
[32:04.860 --> 32:10.500]  координатор на самом деле не умер за рип, и мы уже выбрали нового. Ну зукипер с разрывами сессии,
[32:10.500 --> 32:19.420]  вот опять с этим всем помогаем. Как устроена отдельная партиция, как устроена отдельная
[32:19.420 --> 32:30.180]  партиция топика? Ну вот есть брокеры, у них есть реплики партиции. Вот у нас красная партиция,
[32:30.180 --> 32:36.740]  хранится на трех узлах. И эти партиции нужно друг с другом как-то синхронизировать. Вот давайте
[32:36.740 --> 32:45.020]  посмотрим, как происходит общение с клиентом. Когда продюсер отправляет в партицию сообщения,
[32:45.020 --> 32:51.740]  ну он отправляет в топик сообщения, но скажу аккуратно, продюсер может отправлять данные просто
[32:51.740 --> 32:59.060]  в топик, и в этом случае сообщение отправится просто в одну из партиций по хэшу. Либо же продюсер
[32:59.060 --> 33:04.580]  может делать аккуратнее и направлять данные в конкретную партицию топика, ну потому что он
[33:04.580 --> 33:14.100]  хочет, чтобы, например, действия одного клиента были строго упорядочены. Вот одна партиция,
[33:14.100 --> 33:24.420]  это вот логически одно целое, но физически это реплики. И только одна из этих реплик обрабатывает
[33:24.420 --> 33:30.220]  запрос клиента. Эта реплика называется лидером. Вот на этой картинке реплика один лидер. Она
[33:30.220 --> 33:37.300]  получает сообщение от клиентов, она присваивает им порядковые номера, записывает их в свою реплику
[33:37.300 --> 33:47.420]  партиции. А дальше другие реплики. Вот это по смыслу похоже чем-то на RAFT, но организованно немного не
[33:47.420 --> 33:56.380]  так. В RAFT лидер получает команду от клиента, кладет в свой лог и раздает всем. Здесь же просто
[33:56.380 --> 34:04.740]  каждая реплика партиции, независимо от других реплик, в своем темпе пулит данные из лидера. И
[34:04.740 --> 34:13.180]  когда она запрашивает в него, скажем, offset с третьей позиции, хотя не похоже, что третья позиция под
[34:13.180 --> 34:19.700]  линей лога, то лидер понимает, что эта реплика дошла уже до такого префикса, ну и фиксирует ее
[34:19.700 --> 34:27.700]  прогресс и понимает, что сообщение лежит на большинстве. Ну аккуратно, в случае кавки мы говорим не
[34:27.700 --> 34:37.100]  про большинство, мы говорим про некоторые quorum синхронных реплик. Вот с каждым набором реплик,
[34:37.100 --> 34:43.180]  с каждой партицией связано такое состояние, называется quorum state кавки, и она образована
[34:43.180 --> 34:52.900]  тройкой. Кто сейчас лидер? В какой эпохе это лидер? Ну потому что опять, как и в рафте,
[34:52.900 --> 34:58.420]  как и в мультипакс, во всех протоколах консенсуса нужно блокировать старых лидеров, и мы их
[34:58.420 --> 35:05.860]  блокируем просто по порядковому номеру эпохи. Каждый лидер, он лидер в своей эпохе. И мы считаем,
[35:05.860 --> 35:14.980]  что запись надежно зафиксирована в партиции топика, когда лидер отреплицировал ее на конкретный
[35:14.980 --> 35:25.100]  quorum, он называется ISR. Вот этот ISR, это вот в данном случае первые две реплики, это не просто там
[35:25.100 --> 35:31.140]  любое большинство, а вот конкретное большинство, ну точнее даже не обязательно большинство,
[35:31.140 --> 35:39.620]  просто конкретный набор реплик. И если лидер умирает, если он отказывает, то перевыбирается новый
[35:39.620 --> 35:46.100]  лидер вот из этого quorum, потому что в этом quorum-е гарантированно есть все зафиксированные
[35:46.100 --> 35:51.340]  записи, все записи, которые подтверждены клиентам. Идея понятна?
[35:51.340 --> 36:01.740]  Окей, тогда как именно выбираются лидеры, как это все работает, где хранится этот quorum
[36:01.740 --> 36:10.140]  стоит, вот вся эта информация хранится в зоокипере. И лидеры выбираются, точнее не так, нужно выбирать,
[36:10.140 --> 36:16.620]  нужно обнаруживать, что в системе отказал лидер какой-то партийцы, ну то есть допустим отказывает
[36:16.620 --> 36:23.180]  машина брокер, она являлась лидером для какого-то количества партийций, вот нужно об этом узнать
[36:23.180 --> 36:30.420]  всем партийцам, которые были задействованы, и перевыбрать нового лидера. Так вот, за это все
[36:30.420 --> 36:38.020]  отвечает координатор, и он это все делает с помощью зоокипера. Вот как в прошлый раз рассказывал,
[36:38.020 --> 36:43.180]  зоокипер может применяться для задачи обнаружения сбоев, там есть эфемерные узлы, которые живут до
[36:43.180 --> 36:49.940]  тех пор, пока жива сессия клиента, который эти эфемерные узлы создал. Так вот, каждый брокер
[36:49.940 --> 36:57.220]  создает в зоокипере в директории с кавкой эфемерный узел, который говорит, что я брокер жив. Сам
[36:57.220 --> 37:02.780]  координатор с помощью зоокипера опять же хранит информацию о том, какие партийцы, каких топиков
[37:02.780 --> 37:10.140]  лежат на каких брокерах, и если вдруг координатор, подписавшись на директорию с эфемерными узлами
[37:10.140 --> 37:18.820]  брокеров, узнает о том, что какой-то брокер отказал, то он понимает, для каких партийцей этот
[37:18.820 --> 37:29.620]  брокер был лидером, читает их аэсары из зоокипера, и для каждой пострадавшей партийцы просто бампает
[37:29.620 --> 37:38.780]  эпоху и перевыбирает нового лидера. То есть детектор отказов и перевыбор лидера он централизован
[37:38.780 --> 37:45.540]  в кавке для разных партийцей, и всем этим занимается координатор с помощью зоокипера.
[37:45.540 --> 37:59.060]  Вот такая конструкция. То есть это по смыслу похоже на мультипаксис или RAF, но техники те же,
[37:59.060 --> 38:04.260]  просто декомпозировано все немного по-другому. То есть у нас централизованный тектор сбоев,
[38:04.260 --> 38:10.620]  у нас централизован перевыбор лидера для разных партийцей, и состояние каждой
[38:10.620 --> 38:17.820]  партийцы хранится надежно еще в зоокипере. Метод данной каждой партийцы QuorumState хранится
[38:17.820 --> 38:31.260]  централизованно надежно в зоокипере. Любопытное замечание про нынешнее и будущее состояние
[38:31.260 --> 38:39.500]  капки. Вот как я сказал, координатор играет очень важную роль. Он отслеживает отказ узлов
[38:39.500 --> 38:46.140]  лидеров партийцей, переназначает новых лидеров, распределяет партийцы между брокерами,
[38:46.140 --> 38:53.020]  и все свое состояние для отказа устойчивости координатор хранит в зоокипере. Если узел,
[38:53.020 --> 38:58.780]  который играет роль координатора, отказывает, то новый узел подхватывает освободившуюся блокировку
[38:58.780 --> 39:07.780]  координатора в зоокипере и загружает в себя все это состояние. Так вот, есть идея, сейчас мы найдем.
[39:17.660 --> 39:27.300]  Кавка хочет в конце концов отказаться от зоокипера. С одной стороны, вот начинать строить сложную
[39:27.300 --> 39:32.900]  систему, когда зоокипер есть, гораздо проще. Мы ему делегируем задачу консенсусом. Мы там
[39:32.900 --> 39:39.460]  поддерживаем монотонную историю QuorumState для реплик, мы там выбираем лидера, обнаруживаем отказы.
[39:39.460 --> 39:48.420]  В общем, с помощью этого одного зоокипера мы реализуем много-много разных согласованно реплицированных
[39:48.420 --> 39:58.500]  партиций. Но в то же время у этого дизайна есть свои недостатки. Во-первых, такая банальная проблема,
[39:58.500 --> 40:04.460]  что зоокипер это просто довольно большая зависимость кавкина, то есть вы должны поддерживать не одну
[40:04.460 --> 40:13.020]  систему кавку, а две системы кавка плюс зоокипер. А во-вторых, вот такой вот дизайн, он вам увеличивает
[40:13.020 --> 40:23.220]  время восстановления после сбоев. Если вдруг координатор отказал, то выбрать нового, допустим,
[40:23.220 --> 40:29.540]  не так уж и сложно, но новый координатор должен загрузить довольно большое состояние себе в свою
[40:29.540 --> 40:42.460]  машину зоокипера. Это не то чтобы очень страшно, это не влияет на корректность. Мы
[40:42.460 --> 40:48.020]  никакие данные не потеряем, потому что зоокиперы они хранятся надежно, ну и система доступна до тех
[40:48.020 --> 40:53.820]  пор, пока доступно большинство узлов зоокипера. Но все же восстановление после сбоев замедляется.
[40:53.820 --> 41:01.220]  Так вот, идея в том, чтобы просто вместо зоокипера использовать рафт, чтобы координатор был
[41:01.220 --> 41:07.580]  реплицирован с помощью рафта, и тогда, если один узел в рафте отказывает, то другой узел он уже
[41:07.580 --> 41:15.260]  в себе содержит копию всех данных, ну просто потому что так рафт устроен, как вы помните, и время
[41:15.260 --> 41:21.740]  восстановления после сбоя центрального узла, оно уменьшится. Но что любопытно, вообще говоря,
[41:21.740 --> 41:28.500]  отказ координатора, он не является, координатор здесь не является прям вот точкой отказа всей
[41:28.500 --> 41:33.700]  системы, потому что если координатор умер, в принципе, это не мешает реплике обслуживать записи,
[41:33.700 --> 41:41.100]  не мешает зоокиперу обслуживать записи в отдельные партиции. Вот на быстром пути,
[41:41.100 --> 41:47.940]  на пути записи данных зоокипер не участвует. Зоокипер участвует тогда, когда отказывает лидер,
[41:47.940 --> 41:55.660]  или когда отказывает какая-нибудь из реплик ASR. Вот если отказов нет, просто отказал координатор,
[41:55.660 --> 42:01.140]  то система может продолжать работу и обслуживать записи чтения клиентов.
[42:01.140 --> 42:11.620]  Теперь по поводу состояния клиентов. Ну вот что хранится в зоокипере? Карта кластера,
[42:11.620 --> 42:17.340]  распределение партийцы по брокерам, хранится метаинформация для каждой партийцы о статусе
[42:17.340 --> 42:22.620]  репликации. Кстати, пока мы о репликации не ушли, если кому интересно, кто ходит в субботу меня
[42:22.620 --> 42:29.100]  слушать, то вот для кавки разработчики написали спецификацию на тело и плюс, как именно устроена
[42:29.100 --> 42:34.580]  репликация. Там можно посмотреть на все события, которые могут происходить, и как именно кавка их
[42:34.580 --> 42:41.700]  вырабатывает. Вот в зоокипере хранится карта распределения данных, хранится блокировка
[42:41.700 --> 42:52.300]  координатора, хранится состояние репликации для каждой партиции. Вот то, что было нарисовано
[42:52.300 --> 42:58.660]  вот этот лидер эпоха и ISR. Не расшифровал, кажется, ISR, это инсинг-реплики, то есть реплики,
[42:58.660 --> 43:04.540]  которые синхронизированы с лидером, которые содержат все эти же данные. И на старте, но вот
[43:04.540 --> 43:11.420]  давным-давно кавка, помимо вот этих данных, хранила в зоокипере еще и оффсеты клиентов.
[43:11.420 --> 43:19.380]  Но мы договорились, что мы делегируем хранение стейта каждого клиента самой кавки, но вот кавка
[43:19.380 --> 43:25.060]  тоже должна это хранить отказаустойчиво. Почему бы ей не хранить это в зоокипере? Но клиентов
[43:25.060 --> 43:33.380]  становилось много, партийцы становилось много, оффсеты обновляются часто. Вот вот это состояние
[43:33.380 --> 43:40.700]  обновляется, тогда как кто-то отказывает. Довольно редко может быть. А клиенты продвигают вперед
[43:40.700 --> 43:47.460]  оффсеты часто. И в какой-то момент система просто перестала справляться с нагрузкой. Ее нужно
[43:47.460 --> 43:54.060]  было масштабировать в этом месте, в месте хранения партийцы. Так вот, кавка идет по интересному
[43:54.060 --> 43:58.220]  пути и по важному для нас, потому что это нам позволит сделать в конечном итоге семантику
[43:58.220 --> 44:07.100]  и экзорпеванс. Кавка говорит, давайте мы будем хранить данные, оффсеты для клиента для партийцы
[44:07.100 --> 44:16.900]  в виде топика. То есть у нас будет специальный служебный топик, в котором данными будут не вот
[44:16.900 --> 44:24.220]  какие-то записи пользователя, а наши служебные данные мы будем для пользователя, запятая партийцы,
[44:24.220 --> 44:34.820]  хранить там оффсеты. То есть когда пользователь, когда клиент говорит commit sync, вот здесь вот,
[44:34.820 --> 44:43.700]  то кавка в служебный топик записывает текущий оффсет для данного клиента, для партийц,
[44:43.700 --> 44:49.660]  которыми он работает. Но поскольку топики это сущность, которая масштабируется внутри кавки,
[44:49.660 --> 44:59.140]  то значит хранение оффсетов не будет больше узким местом. Правда есть один нюанс, если мы просто
[44:59.140 --> 45:05.940]  записываем апдейты в такой монотонно растущий топик, то чтобы прочесть текущие оффсеты клиента,
[45:05.940 --> 45:12.260]  нужно будет этот топик прочесть с самого начала, что конечно не эффективно. Поэтому в кавке есть
[45:12.260 --> 45:20.260]  служебные топики, какой-то служебный механизм компактификации. И вот это одна из причин,
[45:20.260 --> 45:28.460]  по которой в модели данных кавки лог это не просто массив из сообщений, это массив из записей,
[45:28.460 --> 45:36.140]  ключ значения. Вот в случае компактификации внутри топика работает фоновая процедура,
[45:36.620 --> 45:46.820]  внутри партиции топика работает фоновая процедура, которая в этой партиции забывает старые значения
[45:46.820 --> 45:54.220]  по одному и тому же ключу. Вот мы можем забыть вот такую запись, потому что у нас есть K1V2,
[45:54.220 --> 46:04.420]  и K1V2 есть. Эту запись из топика можно ударить. Таким образом, чтобы прочесть оффсеты клиента,
[46:04.420 --> 46:09.820]  нам не нужно читать гигантский топик, нам достаточно читать топик, который будет уже гораздо меньше,
[46:09.820 --> 46:14.980]  потому что оффсеты там случаются. Комит оффсетов может происходить каждые несколько секунд,
[46:14.980 --> 46:22.020]  но для одного клиента, для одной партиции у нас будет одна запись в этом топике.
[46:22.020 --> 46:30.020]  Почему это важно? Потому что, во-первых, этот механизм позволяет кавке масштабироваться,
[46:30.020 --> 46:38.020]  а во-вторых, он нам поможет достичь семантики exactly once, которой нам не хватало вот здесь.
[46:38.020 --> 46:45.940]  Вот у нас есть две строчки, мы пишем что-то в базу данных и мы комитим оффсета, то есть пишем что-то
[46:45.940 --> 46:54.940]  в топик кавки. Мы можем упорядочивать две эти строчки двумя способами получить семантику,
[46:54.940 --> 47:01.140]  либо at most once, либо at least once. В идеальном мире мы бы хотели сделать и то, и другое атомарно,
[47:01.140 --> 47:06.660]  мы бы хотели записать в базу, плюс закомитить оффсеты, то есть сделать запись в служебный
[47:06.660 --> 47:13.660]  топик кавки. Но эта задача безнадежная, то есть система, в которой мы пишем данные про кавку,
[47:13.660 --> 47:19.900]  ничего не знает, а мы хотим какую-то распределенную транзакцию, эта система транзакций может не уметь.
[47:19.900 --> 47:30.580]  Но все же мы можем достичь exactly once в некотором ограниченном наборе случаев, когда мы читаем из
[47:30.580 --> 47:38.380]  топика, обрабатываем данные и их пишем не в какую-то внешнюю систему, а в другой топик в самой кавке.
[47:38.380 --> 47:52.540]  Вот в этом случае мы уже можем достичь семантики exactly once, причем смотрите каким способом. Вот если мы
[47:52.540 --> 47:58.020]  читаем данные из топика и пишем с другой, читаем данные из топика и комитимом оффсета, сейчас
[47:58.020 --> 48:04.900]  аккуратно, читаем данные из топика, процессируем их, пишем выходной топик и фиксируем прогресс,
[48:04.900 --> 48:13.700]  комитим оффсеты, то что это на уровне кавки, внутри кавки происходит? Когда мы пишем данные выходной
[48:13.700 --> 48:18.980]  топика, мы пишем в какой-то там какие-то партиции того самого топика, когда мы комитим оффсеты для
[48:18.980 --> 48:27.300]  входного топика, то мы снова пишем какие-то записи уже в служебные партиции, в партиции служебного
[48:27.300 --> 48:36.220]  топика. То есть комит оффсетов и запись данных выходной топик – это на уровне кавки просто
[48:36.220 --> 48:44.540]  записи в два разных топика. Так что если вдруг мы на уровне кавки научимся делать транзакции,
[48:44.540 --> 48:52.340]  то есть мы научимся атомарно писать в несколько топиков, несколько партиций, то тем самым мы
[48:52.340 --> 48:58.180]  сможем достичь семантика exactly once при процессинге из кавки в кавку. Понятно?
[48:58.180 --> 49:11.340]  Да. Отлично, спасибо за обратную связь. Тогда следующий шаг – мы хотим сделать транзакции в
[49:11.340 --> 49:21.180]  кавке. Ну, давайте сначала делать их как-то наивно. Вот можно про правую часть акценки не думать
[49:21.180 --> 49:27.300]  пока. Вот мы продюсер. Ну, во-первых, как вообще выглядит работа с транзакцией в кавке? Вот можно
[49:27.300 --> 49:34.900]  посмотреть еще раз на продюсера, на транзакционный пример. Вот в случае, когда мы писали просто данные
[49:34.900 --> 49:42.340]  в кавку, то вот мы в цикле делали send. Мы можем делать аккуратнее. Ну, во-первых, можно завести в
[49:42.340 --> 49:46.820]  кавке так называемого idempotentного продюсера, который даже в случае ретраев не будет дублировать
[49:46.820 --> 49:51.780]  данные. Но это похоже все на протокол TCP, когда у нас есть sequential number, когда у нас есть
[49:51.780 --> 50:00.460]  идентификатор клиента, и партия запоминает для каждого продюсера до какого своего логического
[50:00.460 --> 50:05.820]  индекса он записал. Но про это мы не успеем сейчас. Давайте лучше про транзакции. Вот в случае,
[50:05.820 --> 50:14.340]  вот самый общий API, этот транзакционный API, мы на месте продюсера начинаем транзакцию. И мы в
[50:14.340 --> 50:20.380]  пределах этой транзакции пишем какие-то топики. Сейчас надо найти какой-то неинтересный пример,
[50:20.380 --> 50:31.980]  потому что здесь нет аффсетов. Давайте найдем пример поинтереснее. Да, вот здесь уже поинтереснее.
[50:31.980 --> 50:40.380]  Вот смотрите, у нас есть клиент, который является одновременно и консюмером, и продюсером. То есть
[50:40.380 --> 50:47.980]  он читает данные из какого-то топика, а потом он начинает транзакцию. Он данные, которые он
[50:47.980 --> 50:57.660]  прочел, пишет выходной топик, и он в рамках своей транзакции фиксирует аффсеты для себя в роли
[50:57.660 --> 51:10.300]  консюмера и говорит commit transaction. Тут разные API, send и send-offset, но под капотом и то и другое,
[51:10.300 --> 51:16.860]  это просто записи в топике. Это в топик с данными пользователей, а это в служебный топик с аффсетами
[51:16.860 --> 51:24.700]  и компактификацией. Так вот, как же сделана запись от амарной в несколько топиков?
[51:24.700 --> 51:37.660]  Протокол простой. Когда мы под транзакцией делаем сенды, то мы эти сенды маркируем с помощью
[51:37.660 --> 51:45.420]  флажка транзакционности. То есть мы просто добавляем в партизу выходного топика очередное
[51:45.420 --> 51:53.020]  сообщение. Просто ставим на нем зарубку такую, что это запись под транзакцией. И когда консюмер
[51:53.020 --> 52:01.860]  будет этот топик читать, он не имеет права вот такую транзакционную запись сразу выдать клиенту.
[52:01.860 --> 52:10.140]  Потому что непонятно вообще, закомитится ли эта запись или нет. Что если продюсер просто откажет
[52:10.140 --> 52:24.180]  до выполнения строчки 20. Поэтому когда консюмер пишет данные под транзакцией, он просто помечает
[52:24.180 --> 52:30.180]  эти записи как транзакционные. Когда консюмер их читает, он их не выбрасывает, разумеется,
[52:30.180 --> 52:43.220]  но он их накапливает у себя в памяти. Как же происходит фиксация транзакции? Продюсер должен
[52:43.220 --> 52:50.820]  просто в каждую транзакцию поместить специальный маркер. Маркер комита. Для каждой транзакции
[52:50.820 --> 52:57.340]  должен быть выбран уникальный идентификатор, который отличает транзакцию других. И продюсер,
[52:57.340 --> 53:07.820]  когда он делает commit transaction, в каждую партицию каждого топика, который он писал,
[53:07.820 --> 53:14.180]  должен положить маркер комита, который говорит, что теперь транзакционные записи, которые шли
[53:14.180 --> 53:20.380]  раньше, которые были помечены ID транзакции, теперь они закомечены. И логически все эти записи
[53:20.380 --> 53:34.700]  попадают разом вот сюда. Идея понятна? Ну да. Тогда нужно подумать, что в этом подходе не работает.
[53:34.700 --> 53:44.340]  Ну, прям так делать нельзя, конечно. Ну, клиент может commit не записать, а перезагрузиться. Ну,
[53:44.340 --> 53:50.100]  во-первых, клиент, во-первых, продюсер может не завершить транзакцию и отказать. В этом случае
[53:50.100 --> 53:57.900]  в топике останутся транзакционные записи, которые в памяти накапливают клиент,
[53:57.900 --> 54:04.820]  но и у клиента растет расход памяти. Это неудобно. Память у него может просто переполниться. То есть
[54:04.820 --> 54:09.660]  для каждой транзакции нужно вообще принять решение. Она все-таки в конце концов докатится,
[54:09.660 --> 54:18.340]  или все-таки ее нужно откатить и эти записи выкинуть. Другая проблема. Ну вот, продюсер,
[54:18.340 --> 54:23.940]  который делает commit транзакции, то есть если продюсер не сделал, вообще не дошел до этой строчки,
[54:23.940 --> 54:31.140]  то транзакцию нужно откатить. А вот если продюсер дошел до этой строчки и, скажем, в две партиции
[54:31.140 --> 54:37.020]  успел записать маркеры commit, а в третью не успел, то получилась довольно странная ситуация,
[54:37.580 --> 54:46.580]  что какие-то констюмеры транзакции уже видят, а какие-то еще нет. Ну и в третьих, то есть это
[54:46.580 --> 54:52.020]  проблема с отказоустойчивостью, и в третьих есть проблема с атомарностью. Вот представим,
[54:52.020 --> 54:59.340]  что у нас есть два продюсера, которые выполняют две транзакции, и вот они пишут в одни и те же
[54:59.340 --> 55:06.940]  партиции. Но так получилось, что первый продюсер свой комит, свой маркер комита написал раньше,
[55:06.940 --> 55:12.100]  чем второй продюсер, в эту партицию, а в эту партицию сначала написал маркер комита второй
[55:12.100 --> 55:21.180]  продюсер, а потом первый. В итоге у нас как будто бы в две партиции комиты случились в разном порядке.
[55:21.180 --> 55:33.780]  Понятно проблема? Просто потому, что каждая партиция, это вот такой отдельный, отдельная
[55:33.780 --> 55:41.220]  группа реплик, и в ней лидеры по-разному упорядочивают эти самые комиты. Вот все эти
[55:41.220 --> 55:49.380]  проблемы решаются тем, что мы переносим роль координатора транзакции, то есть роль координатора,
[55:49.380 --> 55:55.380]  тот кто управляет комитом. Вот мы переносим роль координатора транзакции с продюсера,
[55:55.380 --> 56:02.740]  который неотказа устойчивый, на узел брокер. Мы просто говорим, что один из брокеров является
[56:02.740 --> 56:11.140]  координатором транзакции. И если мы пишем в одни и те же топики, то это будет один и тот же
[56:11.140 --> 56:27.620]  аккуратнее скажу. У нас есть партиции, в которые мы пишем, и оффсеты этих партий хранятся в служебных
[56:27.620 --> 56:42.700]  партициях. У нас есть для транзакции набор партий, в которые мы пишем вот эти маркеры,
[56:42.700 --> 56:53.900]  данные и маркеры. И для одних и тех же партий координатором будет выбран один и тот же узел,
[56:53.900 --> 57:04.020]  один и тот же брокер кластера кавки. И что делает теперь продюсер, когда он говорит комит? Он говорит
[57:04.020 --> 57:11.460]  этому брокеру координатору транзакции, что я готов закомитить свою транзакцию. И теперь,
[57:11.460 --> 57:19.500]  ну во-первых, когда продюсер начинает транзакцию, он с самого начала говорит об этом координатору.
[57:19.500 --> 57:24.860]  Зачем? Затем, что если вдруг продюсер напишет данные в пределах транзакции, а потом откажет,
[57:24.860 --> 57:30.180]  то транзакцию рано или поздно нужно откатить. Нужно все-таки в топик написать, что транзакция
[57:30.180 --> 57:36.420]  с таким ID просто отменилась, и, пожалуйста, консюмеры, забудьте про те записи, которые вы прочитали
[57:36.420 --> 57:43.300]  и бафилизировали у себя. Вот, поэтому координатор, вообще говоря, должен запомнить, что транзакция
[57:43.300 --> 57:51.500]  началась. Потому что сам координатор – это же тоже не конкретный узел. Брокер, который является
[57:51.500 --> 57:56.340]  координатором, может это показать, и координатор переедет в другое место кластера, на другого
[57:56.340 --> 58:04.700]  брокера. Так вот, новый брокер должен вспомнить, что была транзакция, и что продюсер рано или поздно
[58:04.700 --> 58:11.580]  должен ее закомитить. А если он этого не сделает, то вот я, новый координатор, новый брокер, должен
[58:11.580 --> 58:18.540]  эту транзакцию отменить. Поэтому для транзакции снова нужно иметь некоторое надежное состояние,
[58:18.540 --> 58:24.780]  нужно поддерживать ее статус. И, опять же, можно было бы это сделать из-у киперя, но это было бы
[58:24.780 --> 58:32.620]  снова не масштабируемо, поэтому координатору транзакции нужен собственный лог, отказу устойчивый,
[58:32.620 --> 58:37.900]  в котором он будет писать события, что вот транзакция началась, или вот транзакция готова
[58:37.900 --> 58:46.220]  закомититься, или вот транзакция закомитилась, потому что мы записали все маркеры. И вот этот лог событий
[58:46.220 --> 58:52.100]  для транзакции снова нужно хранить надежным, масштабируемо, отказу устойчиво, ну а для всего
[58:52.100 --> 58:59.620]  этого мы снова можем использовать топик кавки служебной. Ну то есть кавка, она здесь поддерживает
[58:59.620 --> 59:06.100]  сама себя. Вот ей для функционирования нужно хранить аффсеты, нужно хранить лог транзакции,
[59:06.100 --> 59:20.020]  ну и вот все это в свою очередь тоже топики кавки. Окей, теперь продюсер на старте транзакции говорит
[59:20.020 --> 59:26.140]  координатору, пожалуйста, зафиксируй мою транзакцию в логе. Когда продюсер говорит commit, он говорит
[59:26.140 --> 59:31.660]  координатору, что вот я готов закомититься, и координатор пишет сообщение, пишет, что транзакция
[59:31.660 --> 59:41.180]  перешла в статус prepare. После этого координатор пишет флажки commit в каждую выходную партизу.
[59:41.180 --> 59:49.620]  Если вдруг координатор отказал, написал два флажка и отказал, не написал в третий, то выберется
[59:49.620 --> 59:57.340]  новый координатор, он из этого лога прочтет, что транзакция была на фазе prepare уже, прошла
[59:57.340 --> 01:00:10.100]  через нее, и ее нужно докатить, и запишет флажок в третью партизу. Ну и поскольку координатор будет
[01:00:10.100 --> 01:00:17.140]  один для разных транзакций, то этот координатор гарантирует, что флажки разных транзакций будут
[01:00:17.140 --> 01:00:31.780]  записаны в эти партиции в одном и том же порядке. Ну что, мы получили таким образом отказоустойчивые
[01:00:31.780 --> 01:00:37.860]  распределенные транзакции на несколькими топиками, и по модулю того, что мы хранили
[01:00:37.860 --> 01:00:44.780]  оффсеты для клиента в отдельном топике, мы получили и семантику exactly once. То есть мы
[01:00:44.780 --> 01:00:54.140]  здесь гарантируем, что для каждой записи входного топика мы гарантированно пройдем
[01:00:54.140 --> 01:01:13.380]  только одну запись выходного топика. Ну как, понятна идея? Понятно. Вот, но у нас остается все еще
[01:01:13.380 --> 01:01:20.420]  ограничение, что мы не можем работать так с внешним состоянием, но... где это было? Вот здесь. Но
[01:01:20.420 --> 01:01:28.820]  кавка и здесь нам кое-что предлагает на самом деле. Вот пусть мы хотим делать процессинг, вот что мы
[01:01:28.820 --> 01:01:34.580]  умеем сейчас с помощью кавки делать? Мы можем читать из топика, обрабатывать в памяти и класть в
[01:01:34.580 --> 01:01:42.740]  топик выходной, и иметь семантику exactly once. А что если наш процессинг сложнее? Что если мы
[01:01:42.740 --> 01:01:52.100]  хотим читать из топика, накапливать какое-то сложное состояние, именно накапливать его? Ну,
[01:01:52.100 --> 01:02:00.780]  то есть, не знаю, высчитывать какие-то средние по потоку, а потом с учетом этого писать данный выходной
[01:02:00.780 --> 01:02:07.780]  топик. То есть у нас узел, который находится между входным и выходным топиком, он имеет свое
[01:02:07.780 --> 01:02:17.580]  состояние, он не stateless. Вот кавка позволяет организовать и такие вычисления. То есть,
[01:02:17.580 --> 01:02:23.260]  где каждый узел, каждый узел процессинга, он обладает собственным персистентным статусом.
[01:02:23.260 --> 01:02:28.660]  Каким образом это делается? Ну, это делается с помощью отдельного framework, который называется
[01:02:28.660 --> 01:02:36.940]  Kafka Streams. Вот, и в этом framework вы можете писать, ну, вот такие вот программы. То есть,
[01:02:36.940 --> 01:02:43.580]  у вас есть некоторый топик с, не знаю, с документами, и вы начинаете его процессить,
[01:02:43.580 --> 01:02:51.700]  разбивать их на слова и делать count by key. Ну, то есть, вы делаете такой word count,
[01:02:51.700 --> 01:02:59.460]  который вы, наверное, делали уже в GoProduce, но только в кавке без бачинга. То есть вы читаете
[01:02:59.460 --> 01:03:05.980]  топики, процессите их, сплитите сразу, и все эти записи текут дальше. Ну, вот для того, чтобы,
[01:03:05.980 --> 01:03:13.260]  то есть, для вот такой программы мы на уровне Kafka Streams встроим такой граф вычислений,
[01:03:13.260 --> 01:03:20.500]  и в этом графе вычислений есть узлы, которые просто читают входной топик и сплитят его. Ну,
[01:03:20.500 --> 01:03:39.100]  какую-то картинку сейчас можно показать. А есть какие-то узлы, которые считают в каком-то
[01:03:39.100 --> 01:03:48.700]  скользящем окне частоту слов. И вот для случая, ну, и в модели Kafka Streams есть два понятия stream
[01:03:48.700 --> 01:03:56.780]  и table. Вот stream, вот вы можете создать stream, который получается обработкой данных какого-то
[01:03:56.780 --> 01:04:04.380]  другого стрима. Ну, есть один топик на выходе физически, есть другой топик на выходе. И где-то
[01:04:04.380 --> 01:04:10.020]  на узле вашего кластера запускается такая stateless задачка, которая читает один топик,
[01:04:10.020 --> 01:04:16.860]  процессит его, поднимает в uppercase какие-то поля и записей. Ну, потому что теперь уже топики будут
[01:04:16.860 --> 01:04:22.820]  в Kafka Streams схематизированы, то есть у каждого топика, у каждого сообщения будет какая-то
[01:04:22.820 --> 01:04:29.820]  фиксированная схема. Вот. И пишет на выход и делает это с помощью транзакций, то есть семантических
[01:04:29.820 --> 01:04:37.820]  execuants. Мы можем легко такое вычисление запустить на кластере. И Kafka Streams нам дает декларативный
[01:04:37.820 --> 01:04:44.940]  язык для того, чтобы удобно описывать эти вычисления. Чтобы вы не код на джаве писали, а вот такое
[01:04:44.940 --> 01:04:52.700]  декларативное описание. Но что если мы хотим между двумя топиками поместить узел, который будет
[01:04:52.700 --> 01:05:03.020]  уже вычислять что-то с учетом, если здесь есть пример, с учетом средних. Вот ему нужно более
[01:05:03.020 --> 01:05:21.340]  сложное состояние поддерживать. Как сделать это? Понятно ли вам? Возможно, свои промежуточные
[01:05:21.340 --> 01:05:29.300]  расчеты писать в отдельный топик? Вот смотрите, как организован в общем случае такой, если мы
[01:05:29.300 --> 01:05:34.220]  говорим про произвольный процессинг, то у нас есть граф, где истоки это какие-то топики,
[01:05:34.220 --> 01:05:43.780]  промежуточные узлы это вычисления, которые читают из входного топика, пишут что-то в выходной топик,
[01:05:43.780 --> 01:05:52.540]  и в общем случае накапливают какое-то состояние у себя. Но вот если состояния бы не было, то мы
[01:05:52.540 --> 01:05:56.420]  просто использовали бы транзакции, которые мы только что обсудили, и получили бы execuants
[01:05:56.420 --> 01:06:01.940]  процессинг. Такой граф, в котором каждая запись исходная проходит через граф ровно один раз.
[01:06:01.940 --> 01:06:09.440]  Но если у нас есть стейт, то при рестарте, казалось бы, мы не можем просто перезапустить
[01:06:09.440 --> 01:06:16.460]  этот процессинг на другой машине, потому что там стейт потерялся. Поэтому стейт тоже должен быть
[01:06:16.460 --> 01:06:22.860]  отказоустойчивым. Ну а этот стейт, он вообще говоря произвольно устроен, таблица со средними
[01:06:22.860 --> 01:06:32.220]  значениями, как здесь. Но опять же, даже произвольное состояние можно представить в виде топика.
[01:06:32.220 --> 01:06:39.100]  Вот мы offset представили в виде топика, мы состояние transaction manager представили в виде топика,
[01:06:39.100 --> 01:06:45.540]  мы теперь и произвольную таблицу со средними значениями тоже представили в виде топика. Каким
[01:06:45.540 --> 01:06:53.660]  образом? Ну очень просто, у нас любое состояние, любое состояние конкретное, там таблица может
[01:06:53.660 --> 01:07:02.780]  быть представлено просто в виде changelog. Вот скажем, у нас есть состояние таблицы счетчика,
[01:07:02.780 --> 01:07:12.180]  сколько встречается каждое слово. Вот с одной стороны, это вот такая вот таблица, с другой
[01:07:12.180 --> 01:07:21.860]  стороны, это вот такой changelog. И если мы теперь представим вот здесь изменение этой таблицы,
[01:07:21.860 --> 01:07:32.180]  этого состояния в виде changelog, где мы добавляем очередное значение к сумме и очередное значение
[01:07:32.180 --> 01:07:41.140]  к знаменателю, то мы получим состояние этой таблицы со средними значениями сенсоров в виде просто
[01:07:41.140 --> 01:07:46.780]  служебного топика. Этот топик, опять же, пользователю не виден, пользователь просто создает таблицу,
[01:07:49.060 --> 01:07:56.780]  а под капотом вот этот create table разворачивается в создание служебного топика. И когда
[01:07:56.780 --> 01:08:11.940]  когда мы теперь хотим пропустить очередную запись через узел обработки, то мы с одной стороны должны
[01:08:11.940 --> 01:08:23.340]  записать данный выходной топик, обновить состояние узла и зафиксировать offset на входном топике. И вот
[01:08:23.340 --> 01:08:33.380]  три этих действия, это на самом деле всего лишь три записи в три разных топика, в changelog состояние,
[01:08:33.380 --> 01:08:45.580]  в топик offset исходного входных данных и в топик с данными для выходных данных, в топик выходных
[01:08:45.580 --> 01:08:52.380]  данных. И мы должны сделать все это автоматно. Но для этого у нас уже есть транзакции, мы их
[01:08:52.380 --> 01:09:00.180]  поддержали. Так что мы теперь используем транзакционные IP, мы используем служебные
[01:09:00.180 --> 01:09:07.700]  топики для транзакций, для offset транзакций и стейта, и таким образом мы можем делать отказоустойчивую
[01:09:07.700 --> 01:09:17.780]  stateful вычисления. То есть мы можем, и Kafka это делает в виде framework Kafka Streams,
[01:09:17.780 --> 01:09:27.700]  который позволяет вам более-менее декларативно описывать граф вычислений, а дальше это декларативное
[01:09:27.700 --> 01:09:36.580]  описание развернется в конкретный граф с процессингом на отдельных узлах кластера, и отказ любого из
[01:09:36.580 --> 01:09:42.300]  этих узлов Kafka переживет. Просто вычисление перейдет на другой узел и продолжит ровно с того
[01:09:42.300 --> 01:09:47.540]  места, где оно остановилось. Восстановит свой стейт, там, не знаю, таблицу со средними значениями и побежит
[01:09:47.540 --> 01:09:55.460]  дальше. Вот, ну и все потому, что мы научились делать транзакции, и мы все свои структуры данных
[01:09:55.460 --> 01:10:03.140]  представили в виде топиков. Вот одно плюс второе дает нам в общем случае stateful exactly once вычисления.
[01:10:03.140 --> 01:10:09.580]  Что скажете?
[01:10:09.580 --> 01:10:23.740]  Я бы сказал, что на сегодня это все. Я, в смысле, по плану рассказал все, что хотел и так израсходовал
[01:10:23.740 --> 01:10:30.940]  времени больше, чем у меня было. Если у вас есть вопросы, то самое время их задать.
[01:10:30.940 --> 01:10:37.260]  Я хотел точнить, для чего ключи нужны, когда мы пишем именно как какой-то пользовательский код.
[01:10:37.260 --> 01:10:41.500]  Записи с одним ключом удаляются старые или нет?
[01:10:41.500 --> 01:10:48.380]  Ну ключи интерпретируются, вот смотри, есть механизм компактификации, когда вот записи,
[01:10:48.380 --> 01:10:55.820]  когда новые записи по тому же ключу стирают старые записи. Ну не то чтобы прямо стирают,
[01:10:55.820 --> 01:11:00.420]  они добавляются, а потом фоновый процесс старые записи выкидывает, сборку мусора делает.
[01:11:00.420 --> 01:11:06.620]  Если же мы говорим про просто произвольный топик с данными, то никакой семантики у ключа значения нет.
[01:11:06.620 --> 01:11:09.660]  Это просто такая фиксированная структура из двух полей.
[01:11:09.660 --> 01:11:17.220]  Ну можно себе представить, что ты, например, если мы собираем логи, скажем, в кавке,
[01:11:17.220 --> 01:11:24.180]  вот логи всех там действий пользователя, которые пишутся на сотнях, на тысячах,
[01:11:25.140 --> 01:11:34.260]  то можно писать в value значение записи, саму записи с лога, а в ключ, там, не знаю,
[01:11:34.260 --> 01:11:42.020]  метаинформацию, с какой машины она доехала. Или если у тебя, скажем, в топике все-таки
[01:11:42.020 --> 01:11:45.700]  разнородные данные хранятся, в смысле схема данных разная, то есть ты не знаешь заранее,
[01:11:45.700 --> 01:11:50.500]  как десеревизовать эти данные, то есть там разные данные могут быть, то ты можешь
[01:11:50.500 --> 01:11:56.740]  положить в ключ, там, схему этих данных. Ну короче говоря, как ты именно используешь
[01:11:56.740 --> 01:12:03.620]  вот этот ключ значения, это уже твоя воля. Да, а можно сделать так, чтобы пользовательский
[01:12:03.620 --> 01:12:10.340]  топик обрабатывался тоже с семантикой уникальных ключей? По-моему, да, можно, но вроде нет никаких
[01:12:10.340 --> 01:12:14.380]  причин, почему вы это будете сделать. Ну, например, складывать вместо ключа ID-шники,
[01:12:14.380 --> 01:12:23.260]  таким образом добиваться уникальности данных. Ну, смотри, кавка, я не помню, она гарантирует ли
[01:12:23.260 --> 01:12:29.700]  тебе, что она прям, вот, при чтении ты не увидишь дубль, и кажется, что такой гарантии у тебя все же
[01:12:29.700 --> 01:12:36.900]  не будет. Ну, потому что ты начал читать, прочел старую версию, потом положили новую, ты прочел
[01:12:36.900 --> 01:12:42.860]  еще раз более новую версию. Ну, то есть это такая оптимизация, она просто позволяет тебе схлопывать
[01:12:42.860 --> 01:12:50.860]  лишние данные. Ну, это все-таки не стоит думать в этом смысле про кавку, про киварию хранилища,
[01:12:50.860 --> 01:12:58.020]  это не киварию хранилища совсем, а на другой сценарии, когда ты очень-очень много пишешь подряд,
[01:12:58.020 --> 01:13:04.580]  поэтому это ивристика для того, чтобы компактить данные, которые вот так хорошо компактятся. Если же
[01:13:04.580 --> 01:13:10.260]  мы говорим про произвольные данные, то, разумеется, они тоже расти бесконечно не могут, и ты должен
[01:13:10.260 --> 01:13:17.580]  настроить, но вот как именно ты готов про старые записи забывать. Ну, скажем, ты готов хранить
[01:13:17.580 --> 01:13:23.820]  данные в топике, данные в партиции до тех пор, пока размеры партийцы не превысит, там не знаю,
[01:13:23.820 --> 01:13:35.860]  100 гигабайт или три дня. Вот что наступит раньше. А можно настроить, чтобы до чтения, например,
[01:13:35.860 --> 01:13:46.500]  они хранятся? Честно говоря, я в смысле по аффсетам клиента, честно говоря, я не помню,
[01:13:46.500 --> 01:13:57.380]  но честно говоря, по своему опыту я не видел, чтобы кто-то как-то так делал. Просто настраивают,
[01:13:57.380 --> 01:14:03.100]  ну не знаю, если у тебя данных относительно немного, ты хранишь там три дня эти данные, и если там ты
[01:14:03.100 --> 01:14:06.860]  за три дня не способен их прочесть, то значит у тебя что-то с процессом сильно не так, ну то есть
[01:14:06.860 --> 01:14:13.820]  у тебя просто есть запас по времени, и если ты прямо его израсходовал, то значит, что у тебя твой
[01:14:13.820 --> 01:14:20.380]  сервис несколько дней не работает, это довольно печально. Я правильно понимаю, что Kafka в основном
[01:14:20.380 --> 01:14:25.380]  используется для записи каких-нибудь real-time данных, каталоги, какие-то статистики, которые
[01:14:25.380 --> 01:14:33.180]  просто нужны там для аналитики в первую очередь. Используется для разных вещей, для разных, не
[01:14:33.180 --> 01:14:49.380]  только для, ну да, если у тебя вот прям, не знаю, какие-то данные, какие-то транзакции пользователя,
[01:14:49.380 --> 01:14:54.660]  ну транзакции пользователя ты, наверное, вырабатываешь как-то иначе. Да, это в первую очередь
[01:14:54.660 --> 01:14:58.820]  какие-то статистики, аналитические данные, какие-то сенсоры, ну вот что-то подобное.
[01:14:58.820 --> 01:15:08.340]  Если у тебя задача более, ну это такая надежная труба, по которой ты данные переправляешь. Если у
[01:15:08.340 --> 01:15:14.180]  тебя какая-то специфичная работа с этими данными, то ты просто, ну не знаю, работаешь с какой-то
[01:15:14.180 --> 01:15:18.460]  системой напрямую, с базой данных напрямую, транзакции выполняешь. Kafka тебе здесь не нужно.
[01:15:18.460 --> 01:15:22.420]  Да, спасибо.

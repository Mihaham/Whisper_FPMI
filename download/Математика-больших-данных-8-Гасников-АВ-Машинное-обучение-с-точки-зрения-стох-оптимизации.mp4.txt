[00:00.000 --> 00:13.560]  Сегодня у нас будет лекция от в формате онлайн. Немного необычный формат,
[00:13.560 --> 00:21.840]  но надеюсь, что непривычный для конкретно этого курса. Надеюсь, что какие-то основные моменты
[00:21.840 --> 00:32.640]  мы не упустим. Начну я, наверное, с обещанного примера, который называется Multi-Level Monte Carlo.
[00:32.640 --> 00:34.760]  Лекция 8.
[00:34.760 --> 00:47.200]  Первая тема, которую мы пройдем. Multi-Level Monte Carlo.
[00:47.200 --> 00:57.640]  Monte Carlo. На русском языке это пишется через Defic Monte Carlo, а на английском просто Monte Carlo.
[00:57.640 --> 01:03.760]  Такая традиция. То, что мы будем сейчас рассказывать, это работа где-то 20-летней
[01:03.760 --> 01:09.640]  давности. Ну, может быть, плюс-минус, я так точно уж не скажу. Майкла Гайлса.
[01:09.640 --> 01:32.560]  Извините, у меня сейчас лекция. Да, я сейчас все сделаю, я помню. Свет выключу.
[01:32.560 --> 01:39.600]  Спасибо. Прошу прощения. Мы берем стахастическое дифференциальное уравнение.
[01:39.600 --> 01:50.840]  Dspdt равняется a sat t. В общем виде записанное. Это, так называемый, диффузионный процесс.
[01:50.840 --> 01:56.520]  Некая коэффициент b. Иногда его sigma обозначают. В общем там непринципиально.
[01:56.520 --> 02:10.320]  В самом общем виде и wat это Винеровский процесс. Вот это есть объект изучения наш.
[02:10.320 --> 02:18.720]  Ну, можно как-то задать точку старта s от нуля. Допустим, равняется s0. Можно
[02:18.720 --> 02:23.560]  считать ее не случайно, это сейчас не очень важно. Ну и, в общем, интересоваться поведением
[02:23.560 --> 02:30.880]  траектории этого процесса на промежутке времени от нуля до t большого. В чем задача? Задача заключается
[02:30.880 --> 02:40.720]  в том, чтобы посчитать мат ожидания, то есть вот некое число y, которое есть мат ожидания f от s от t.
[02:40.720 --> 02:46.600]  То есть есть какая-то функция, которая, например, есть цена опционы или там, я не знаю,
[02:46.600 --> 02:54.200]  фьючерса, еще что-то, которое зависит от цены акции. Цена акции, в свою очередь, подчиняется
[02:54.200 --> 03:02.480]  вот такому вот стахастическому дифференциальному уравнению. И, ну, например, для, действительно,
[03:02.480 --> 03:10.120]  для классической постановки здесь просто будет s от t, ну такой вот, а здесь постоянная sigma.
[03:10.120 --> 03:17.400]  Ну то есть, прошу прощения, непостоянная sigma здесь будет ну как бы а на s от t, а здесь будет
[03:17.400 --> 03:24.040]  sigma, константы на s от t, то есть это все константы. Ну то есть от t они не зависят, а от s от t они
[03:24.040 --> 03:29.920]  зависят линейно. Но это для вот так называемого геометрического броновского движения будет так
[03:29.920 --> 03:37.080]  получаться. Так вот наша цель посчитать мат ожидания. Есть две схемы. Первая схема это схема
[03:37.080 --> 03:44.160]  Эйлера и, в общем-то, в каком-то смысле вычисления в лоб. Ну вот вычисления в лоб это значит,
[03:44.160 --> 03:57.240]  что мы берем как бы сетку. Значит сетка берется ну вот точками значит 1 и так далее t на h,
[03:57.240 --> 04:07.040]  t большой на h, h это шаг сетки, шаг сетки, шаг сетки. И пытаемся аппроксимировать вот этот вот
[04:07.040 --> 04:16.520]  стог дифур, пытаемся аппроксимировать его в дискретном времени, шаг сетки. Вот мы разбили
[04:16.520 --> 04:23.480]  промежуток времени от нуля до t большого, от нуля вот на такие вот промежуточки, длина которых,
[04:23.480 --> 04:31.840]  ну соответственно произвольного отрезочка h. Ну и говорим, что соответственно накатом промежутки,
[04:31.840 --> 04:40.280]  накатом отрезки вот у нас значение вот этой величины будет sk. Но чтобы не путать это с
[04:40.280 --> 04:44.880]  настоящим значением, мы будем сверху ставить крышечку. То есть мы можем написать такую динамику
[04:44.880 --> 04:57.760]  s с крышечкой k плюс 1 равняется s с крышечкой k плюс a на s с крышечкой k на tk на дельта t ну дельта
[04:57.760 --> 05:07.960]  t понятно чему равно, оно равно h вот в нашем случае плюс соответственно b на s с крышечкой k на tk на
[05:07.960 --> 05:18.280]  дельта wk. Ну чему равняется дельта wk, наверное тоже более-менее понятно, это есть w в точке как бы
[05:18.280 --> 05:28.040]  объяснить, отвечающем как бы kk плюс 1 минус wk. Ну и все наши обозначения, это на самом деле вот
[05:28.040 --> 05:34.040]  если так вот как бы пытаться аппроксимировать, что это должно аппроксимировать, то можно это
[05:34.040 --> 05:49.800]  понимать так, что w как бы от h на k это вот wk, ну и s от h на k это вот s с крышечкой k. То есть мы
[05:49.800 --> 05:57.320]  вот значит надеемся, что есть какое-то такое соответствие. Но вообще говоря, это просто
[05:57.320 --> 06:02.520]  отдельная история, просто какой-то отдельный процесс. Ну и если вот скажем здесь я могу
[06:02.520 --> 06:07.920]  писать на самом деле точное равенство, просто считаю, что у меня вот здесь равенство, ну какой-то
[06:07.920 --> 06:17.320]  Винеровский процесс, так вот он, ну хотя тоже нет на самом деле, на самом деле конечно это так тоже
[06:17.320 --> 06:22.600]  нельзя писать точно, это как бы мы хотим, чтобы это было как-то похоже. То вот надежда на то,
[06:22.600 --> 06:30.480]  что вот это вот дело, которое я обвожу, будет неплохо аппроксимировать вот это вот дело.
[06:30.480 --> 06:38.720]  Ну и значит в итоге, если мы сделаем таких траекторий много, то есть мы возьмем,
[06:38.720 --> 06:46.000]  значит я даже другим цветом выделю и здесь запишем индекс i и сделаем такие независимые
[06:46.000 --> 06:55.920]  траектории. Ну то есть в принципе, если это iid, то есть каждый раз вы независима. Ну по сути
[06:55.920 --> 07:02.240]  речь идет о независимости вот этого блока, вот этого блока. Потому что, по сути, если это независимо,
[07:02.240 --> 07:08.280]  все остальное как бы порождено независимостью процесса вот этого. То мы можем в принципе оценить
[07:08.280 --> 07:14.800]  интересующую нас величину, которую мы назвали y. Напомню, что y этому от ожидания, вот она написана,
[07:14.800 --> 07:20.400]  этому от ожидания f от st в конечном момент времени. Ну мы можем оценить тогда, например,
[07:20.400 --> 07:30.920]  так, если возьмем n число траектории. Значит это есть от 1 до i, от 1 до n. Ну f от то, что получается
[07:30.920 --> 07:36.040]  s крышкой. Ну в последний момент времени, я не знаю, как мы его там обозначали, ну t на h,
[07:36.040 --> 07:39.960]  то есть мы не ввели никакого специального обозначения. Вот мы использовали t на h,
[07:39.960 --> 07:45.920]  ну значит ничего не остается, как написать t на h. Вот t на h. Вот что получается, естественно,
[07:45.920 --> 07:52.520]  тут i. И эта вот независимость вот этих вот реализаций порождает независимость траектории,
[07:52.520 --> 08:01.280]  значит это должно быть хорошей величиной. То есть закон больших чисел для нее должен выполняться,
[08:01.280 --> 08:06.440]  или там центрально-предельная теорема. Ну то есть это означает, что существуют такие константы c1,
[08:06.440 --> 08:13.840]  c2 больше нуля. Такие что, значит, вот это вот мат ожидания y. Давайте только это y
[08:13.840 --> 08:21.200]  с чертой назовем, y настоящего минус y с чертой в квадрате. Вот, естественно, это все под мат
[08:21.200 --> 08:29.560]  ожиданием. Что это будет иметь вид c1 на n-1. Это вот как раз следствие того, что вот дисперсия вот
[08:29.560 --> 08:35.160]  этой случайной величины, а это и есть дисперсия, но она пропорциональна 1 на n. Ну с каким-то
[08:35.160 --> 08:40.600]  коэффициентом пропорциональности, который зависит от свойств функции. Но это еще не все. Это так
[08:40.600 --> 08:45.360]  было бы, если бы тут были независимые траектории реально этого метода. То есть если бы мы реально
[08:45.360 --> 08:51.000]  запускали независимые траектории, так что-то я стер. Если бы мы реально запускали независимые
[08:51.000 --> 08:56.440]  траектории, то этим бы делом все и ограничилось. Тогда было бы просто вот такая формула и все. Но
[08:56.440 --> 09:01.960]  у нас не совсем такие траектории. У нас траектории, которые опроксимирующие систему. Ну не сложно
[09:01.960 --> 09:08.920]  сообразить, что если шаг h, то по сути речь идет о схеме опроксимации Эллера. И если t у нас
[09:08.920 --> 09:15.200]  ну не какой-то большой, мы там не сильно думаем о том, что вот этот вот фактор c2 там может зависеть
[09:15.200 --> 09:21.080]  от t как e в степени t. Нас сейчас это не интересует. Мы просто говорим, что c2 да от t зависище, но как нас
[09:21.080 --> 09:27.520]  не сильно волнует. Нам важно, что от h какая зависимость. То понятно, что bias, смещение,
[09:27.520 --> 09:33.040]  которое дает эта схема разностная, подобно тому, как это оценивается в чистых методах оптимизации.
[09:33.040 --> 09:42.400]  Так вот этот bias как раз он просто пропорционален h. Но это значит, что когда мы будем возводить
[09:42.400 --> 09:48.160]  в квадрат этот bias, он же не случайный, мы получим просто h2. То есть подобно тому, как в
[09:48.160 --> 09:55.360]  вычислительной математике мы имеем результат о том, что схема Эллера имеет порядок опроксимации
[09:55.360 --> 10:03.520]  h, то при тех же предположениях липшцевости мы получаем вот такую вот формулу. Ну ясно дальше,
[10:03.520 --> 10:11.840]  что если от нас требуется, чтобы вот эта вся штука равнялась ε2, то есть мы хотим
[10:11.840 --> 10:20.480]  опроксимировать, чтобы у с чертой у отличались в ε1. Это значит, что вот эту штуку надо сделать
[10:20.480 --> 10:30.080]  масштаба, ну как бы разница у и минус у с чертой, она характеризуется по сути корнем из как бы
[10:30.080 --> 10:38.480]  корнем из вот этого вот выражения у минус у с чертой в квадрате. Мы хотим, чтобы это было
[10:38.480 --> 10:45.400]  Эпсилон. Вот отсюда следует, что во-первых n должно быть пропорционально Эпсилон минус значит
[10:45.400 --> 10:52.680]  второй вот, потому что иначе мы не получим значит вот вот вот тут Эпсилон. Ну и второй
[10:52.680 --> 11:00.400]  результат, который нужно тут сделать, что h равняется пропорционально Эпсилон. Ну и тогда
[11:00.400 --> 11:07.880]  общее число вычислений значит если хотите вот таких вот шагов, то есть по сути по сути
[11:07.880 --> 11:15.680]  сэмплирований сэмплирований гауссовского вот это винеровского блуждания, то таких сэмплирований
[11:15.680 --> 11:21.920]  надо сделать и соответствующих вычислений вот которые здесь стоят вот этого дела. Их надо
[11:21.920 --> 11:28.160]  сделать то столько сколько итераций, ну не итерации а сэмплов, ну что естественно сэмплов 1 на
[11:28.160 --> 11:36.160]  Эпсилон квадрате. И соответственно понимать, что на каждом на каждом вот этом вот траектории нам
[11:36.160 --> 11:41.920]  еще один на Эпсилон. Итерации надо сделать, потому что это t на h, h пропорционально Эпсилон. Значит
[11:41.920 --> 11:49.720]  итогов получается total от Эпсилон равняется n умножить на t на h, ну и это пропорционально Эпсилон
[11:49.720 --> 11:55.840]  минус треть. Можно ли эту оценку улучшить? Вот в этом вопрос, то есть вопрос, который правильно
[11:55.840 --> 12:04.960]  тут задать, можно ли улучшить? Можно ли сделать лучше? Можно ли сделать лучше? Сделать лучше? Ну
[12:04.960 --> 12:12.480]  вообще такой довольно частный вопрос, потому что вроде как кажется, что это какая-то не очень
[12:12.480 --> 12:17.800]  важная задача. На самом деле ничего подобного, огромное количество задач финансовой математики,
[12:17.800 --> 12:22.720]  это вот в общем задачи, ну оценки финансовых инструментов, это как раз вот как это делать,
[12:22.720 --> 12:27.200]  то что вот я написал, то есть решать вот такую задачу. То есть это не то что не частный вопрос,
[12:27.200 --> 12:34.520]  это один из преугольных таких вопросов, главных вообще много где. И вот собственно то, что я хочу
[12:34.520 --> 12:40.480]  рассказать, это схема Гайлса, который называется метод multilevel Monte Carlo, который более изощренный,
[12:40.480 --> 12:47.160]  более хитрый и позволяет фактически выиграть одну размерность по Эпсилон, то есть Эпсилон минус
[12:47.160 --> 12:56.440]  второй получить результат, что достаточно в общем оптимистично на мой взгляд. Вот ну то есть
[12:56.440 --> 13:02.240]  в общем-то немножко даже неожиданно. Для того чтобы значит этот результат получить, значит схема
[13:02.240 --> 13:15.400]  Гайлса, схема M. Гайлса. Делается следующее, то есть у нас уже сделано обозначение, вот я напомню его,
[13:15.400 --> 13:23.920]  это вот что такое вот такая вот траектория. Только здесь важно понимать с каким шагом мы идем.
[13:23.920 --> 13:33.080]  Вот и давайте мы сделаем значит следующее, мы сделаем набор шагов HL, которая есть значит просто
[13:33.080 --> 13:38.960]  вот такая формула. T делить на некая константа E, ну и просто умножить на M в степени минус L.
[13:38.960 --> 13:47.360]  Ну то есть например если H0 выбирается, да, то например H0 это просто T, то есть шаг вообще
[13:47.360 --> 13:56.960]  ну как бы совпадает с последней точкой, то есть мы H0 берем вот собственно 0T. Значит H1 получается
[13:56.960 --> 14:08.160]  T делить на M, ну и получается вот у нас здесь M, M засечек на отрезке 0T. Если мы делаем шаг,
[14:08.160 --> 14:17.800]  который например H2 равняется T на M в квадрате, да, то получается засечек сильно больше. То есть
[14:17.800 --> 14:24.360]  засечек будет M в квадрате от 0 до T. Ну я думаю понятно схема, ну и соответственно каждому такому
[14:24.360 --> 14:35.240]  HL мы ставим в соответствие S, крышка, значит и ну тоже как бы разные реализации на T на HL.
[14:35.240 --> 14:42.280]  Ну по идее я еще должен был конечно здесь пояснить, что сама процедура идет шагом H,
[14:42.280 --> 14:48.840]  то есть по сути вот здесь дельта T берется как HL. Ну и соответствующая разница вот этих вот значений,
[14:48.840 --> 14:54.920]  то есть это что такое? Это просто берется значение Винеровского процесса в точке,
[14:54.920 --> 15:00.120]  ну или если хотите, на самом деле это на практике сэмплируется не так. На практике просто берется
[15:00.120 --> 15:08.400]  нормальная случайная величина с мат ожиданием 0 и дисперсией H, H просто и все. То есть вместо того,
[15:08.400 --> 15:13.400]  чтобы реально как бы гонять тиректорию случайного процесса, мы просто берем эту разницу, мы знаем,
[15:13.400 --> 15:17.920]  что она должна распределена быть по N0H, N нормальное распределение, H дисперсия,
[15:17.920 --> 15:23.560]  ну и сэмплируем собственно из нее. Вот собственно этим фактом мы пользуемся и каждый раз это делаем
[15:23.560 --> 15:30.960]  IID, независимо одинаково распределенными вот эти вот величины. Ну и в общем-то понятно,
[15:30.960 --> 15:36.520]  что нам просто надо уметь сэмплировать стандартные нормальные, не стандартные, а нормальные случайные
[15:36.520 --> 15:41.800]  величины вот с такой дисперсией. И по-хорошему я должен конечно здесь как-то вот зашивать,
[15:41.800 --> 15:48.580]  что от H зависит. Но я думаю, что не будет недоразумения, чтобы громоздкими не делать
[15:48.580 --> 15:54.080]  обозначения. Я просто буду обозначать вот так результат, который получается еще раз, вот это вот,
[15:54.080 --> 16:00.840]  это есть результат, который получается вот в данном процессе, вот в данном процессе, когда H равняется
[16:00.840 --> 16:08.040]  HL, то есть H тут равняется HL. Ну вот тогда это мы будем обозначать выход, то есть в выходную точку,
[16:08.040 --> 16:17.760]  мы будем обозначать просто скорректировав HL. Ну вот так. Ну и таких схем. Но теперь нам
[16:17.760 --> 16:26.240]  хочется по-другому оценивать Y, а именно мы хотим оценивать Y следующим образом,
[16:26.240 --> 16:37.360]  как Y с чертой равняется сумма YL, L от 0 до L, мы подберем это L, где, соответственно,
[16:37.360 --> 16:48.400]  Y0 равняется 1 на N0, мы подберем эти числа, значит сумма по И от 1 до N0 на F от, соответственно,
[16:48.400 --> 16:58.200]  S с крышкой И на T на H0. Вот это просто вот то, что получается, если делать вот такую вот схему,
[16:58.200 --> 17:03.920]  то есть просто за один шаг. Но это довольно дурацкая оценка, но тем не менее вот мы ее вводим.
[17:03.920 --> 17:11.160]  Ну и дальше вводим YL с чертой, естественно, это все. Вот YL с чертой равняется уже более
[17:11.160 --> 17:20.240]  что-то интересной единице на NL, это то, что на L там шаге, значит, подбирается сумма по И от 1 до NL.
[17:20.240 --> 17:31.040]  И вот здесь стоит такая разница F от S с крышкой И T H, соответственно, L, а здесь стоит минус F,
[17:31.040 --> 17:46.400]  S с крышкой и T на H на L-1. Вот такая разница. Обратите внимание, что мы считаем среднеаррифметическое
[17:46.400 --> 17:58.080]  таких вот траекторий, но как бы теперь мы делаем проход с разными шагами, то есть мы берем,
[17:58.080 --> 18:08.840]  как бы сказать, то есть мы берем траекторию вот с шагом с каким-то вот, не знаю, там HL и HL-1,
[18:08.840 --> 18:14.360]  то есть по сути вот с таким шагом и вот с таким шагом. И проходим на одной реализации с этими
[18:14.360 --> 18:21.320]  траекториями. Просто считаем две разницы. Считаем вот такую величину и вот такую величину,
[18:21.320 --> 18:31.040]  но как бы на одной траектории. И также делаем для всех L, то есть это L от 1 до некого L.
[18:31.040 --> 18:39.080]  Что можно сказать про эти схемы, про вот эту схему? Ясно, что есть телескопическое свойство,
[18:39.080 --> 18:49.360]  вообще говоря. То есть во всяком случае мы как бы тут надеемся, что есть такая вот,
[18:49.360 --> 18:57.880]  что ли, то есть то, что мы здесь посчитали, должно сократиться с тем, что в каком-то смысле вот с
[18:57.880 --> 19:03.960]  тем, что здесь будет. Ну и так они будут друг на друга, то есть оценки должны быть, но если не в
[19:03.960 --> 19:11.960]  точности, то хотя бы вот по порядку должны как-то соответствовать вот этому делу. Ну вот оказывается,
[19:11.960 --> 19:19.960]  что bias, который получается от такой формулы, ну то есть bias имеется в виду, что мы берем от
[19:19.960 --> 19:28.120]  ожидания, ну от y с чертой минус y, но только нам интересно, значит модуль этого величины, он будет,
[19:28.120 --> 19:34.600]  вот для такой формулы, он будет о большой от a shale, где a shale это последняя точка. То есть почему?
[19:34.600 --> 19:46.920]  Потому что просто траектория, которые здесь вылезают, они, как сказать, ну то есть как бы вот тот bias,
[19:46.920 --> 19:52.440]  который, вот эта грубость, которая вот тут есть, она сокращается вот с траекторией, которая здесь,
[19:52.440 --> 20:03.680]  то есть у нас будет наблюдаться, соответственно, что ли такой эффект, ну что ли телескопичность,
[20:03.680 --> 20:10.680]  о котором я уже говорил, то есть фактически, самая, так сказать, последняя составляющая,
[20:10.680 --> 20:16.720]  то есть та, которая в сумме идет с плюсом, она и будет давать вклад, а все остальные вклады давать
[20:16.720 --> 20:25.880]  не будут, то есть a shale это вот как бы, как объяснить, ну из-за того, что у нас формула вот такая,
[20:25.880 --> 20:33.280]  вот мы редуцируем на самом деле вот эту байсную часть, опираясь фактически на последнее слагаемое.
[20:33.280 --> 20:39.200]  Ну с дисперсией тут ситуация немножко сложнее, чем была в предыдущей истории, потому что дисперсия,
[20:39.200 --> 20:44.840]  ну я знаю, давайте d назовем это, вот, она же считается вот от этой штуки s с крышкой,
[20:44.840 --> 20:51.640]  значит, соответственно, и на t на shale, и потом надо будет еще нормировать на число слагаемых,
[20:51.640 --> 20:56.960]  ну потому что у нас среднее арифметическое берется, значит s с крышкой, и вот здесь будет f,
[20:56.960 --> 21:06.760]  значит, прошу прощения, надо аккуратно сделать, f s с крышкой и на t на shale минус 1, shale минус 1,
[21:06.760 --> 21:15.280]  и просто по построению, как мы это сделали, у нас эта дисперсия, она определяется масштабом вот
[21:15.280 --> 21:21.000]  в этой схеме, масштабом дисперсии вот этой случайности, которая тут есть, то есть,
[21:21.000 --> 21:29.800]  как бы масштаб неточности, который есть на этой траектории, он будет иметь масштаб вот этого h,
[21:29.800 --> 21:36.040]  потому что это есть дисперсия, это именно дисперсия вот этой вот штуки, ну и мы получаем,
[21:36.040 --> 21:43.400]  что дисперсия вот всей этой разницы, ну которая на последней точке, это будет что-то,
[21:43.400 --> 21:52.960]  о большое какое-то от тоже, ну в данном случае, shale, потому что здесь вот берется, shale не последняя,
[21:52.960 --> 22:00.280]  а она как бы промежуточная, ну и чтобы посчитать дисперсию уже оценки dy с чертой, чтобы посчитать
[22:00.280 --> 22:08.240]  оценку dysперсии y с чертой, нам надо, значит, сделать следующее, нам надо просуммировать вот эти
[22:08.240 --> 22:15.400]  вот дисперсии vl, умножить каждую из них на nl минус 1, потому что здесь есть вот такая вот
[22:15.400 --> 22:22.920]  нормировка, ну и суммировать это от l до единицы до l большого, ну и заметить, что в данном случае
[22:22.920 --> 22:28.080]  средне квадратичное отклонение, которое мы до этого считали, ну вот я напомню, давайте введем
[22:28.080 --> 22:39.360]  для него обозначение mse, вот это mse, я сейчас перепишу, mse равняется мат ожидания, ну там,
[22:39.360 --> 22:52.080]  значит, это у меня, да, квадрат, значит, это есть модуль y минус y с чертой, там, я не знаю,
[22:52.080 --> 22:56.880]  модуля как ставил, но можно, конечно, их, да, тут раз квадрат ставится, модуль можно не ставить,
[22:56.880 --> 23:07.760]  значит, это квадрат, ну и в данном случае это есть bias в квадрате, значит, это есть bias в квадрате,
[23:07.760 --> 23:19.360]  плюс вот эта дисперсия y с чертой, ну и, естественно, мы должны тоже требовать,
[23:19.360 --> 23:24.360]  чтобы это было, как и раньше, имело масштаб epsilon в квадрате, раньше мы это требовали неявно,
[23:24.360 --> 23:30.720]  вот как бы корень не равняется epsilon, ну значит, это должно равняться epsilon в квадрате, вот, то есть,
[23:30.720 --> 23:35.920]  мы отсюда и находили всякие вот выражения, но теперь, значит, что это означает, это означает,
[23:35.920 --> 23:41.920]  что первое условие, что bias должен равняться epsilon, то что bias должен равняться epsilon,
[23:41.920 --> 23:50.640]  ну по порядку, да, означает, что h, ну то есть, o большое от hl должно равняться epsilon,
[23:50.640 --> 23:57.640]  ну o большое от hl это есть, значит, t на h в степени, не h, а что там у нас, m,
[23:57.640 --> 24:04.480]  m в степени минус l, ну m мы можем условно выбрать, не знаю, двойкой, там это неважно, это просто какое-то
[24:04.480 --> 24:15.720]  число, вот, поэтому мы отсюда можем заключить, что l большое должно равняться по порядку, значит,
[24:15.720 --> 24:24.200]  логарифм epsilon минус первое на логарифм делить m какое-то, ну, то есть, по порядку это
[24:24.200 --> 24:29.720]  всё логарифм epsilon минус первое, вот, что такое l большое, теперь с дисперсией разбираемся,
[24:29.720 --> 24:36.120]  дисперсия y с чертой, она уже нами была выписана, но мы же можем подставить, что есть vl,
[24:36.120 --> 24:42.960]  и vl есть вот эта штука hl, ну, значит, дисперсия y с чертой, она, во-первых,
[24:42.960 --> 24:51.480]  равняется, как я уже писал, nl в минус первый на vl сумма по l от 1 до l большого, ну, и значит,
[24:51.480 --> 24:57.800]  это пропорционально сумма по l от 1 до, ну, только не от 1, от нуля, от нуля, тут, надеюсь,
[24:57.800 --> 25:05.260]  я тоже написал, от нуля надо писать, от нуля до l большого, значит, вот, пишу nl минус первое на
[25:05.260 --> 25:11.840]  vl, и тоже хочу, чтобы это было пропорционально epsilon квадрате, теперь напишем, что такое total,
[25:11.840 --> 25:20.520]  но total теперь оно считается посложнее, потому что у нас как бы вот, как устроена схема,
[25:20.520 --> 25:30.880]  мы делаем единицы, мы делаем nl, nl сэмплов с шагом hl, да, поэтому нам надо как бы суммировать,
[25:30.880 --> 25:38.560]  то есть total будет равняться nl на hl минус первое, это значит nl, это сколько сэмплов делается,
[25:38.720 --> 25:44.240]  сколько сэмплов, а hl минус первое, сколько на траекторию попадает, ну, на значение,
[25:44.240 --> 25:49.160]  то есть это как бы стоимость одной траектории, вот, это l тоже суммируется от нуля до l,
[25:49.160 --> 25:54.160]  значит, нам надо решать задачу, вот это вот на минимум, при условии, что l равна
[25:54.160 --> 26:00.400]  пропорционально log минус epsilon минус первое, при ограничении, вот это и звездочка, то есть при
[26:00.400 --> 26:09.480]  ограничении, что сумма nl минус первое на vl имеет масштаб epsilon квадрате, вот, значит,
[26:09.480 --> 26:15.920]  пропорционально epsilon квадрате l от нуля до l большого, вот, надо решать такую задачу оптимизации,
[26:15.920 --> 26:23.960]  вот, ну и можно эту задачу отрешать и получить, что nl равняется, там, о, большое, значит,
[26:23.960 --> 26:31.800]  epsilon минус второе l на hl, вот, вот какое будет у него решение, ну и тогда, если это решение
[26:31.800 --> 26:38.640]  подставить, значит, в total, то total epsilon будет равняться, значит, ну, естественно,
[26:38.640 --> 26:46.600]  большое от epsilon минус второй на логариф в квадрате epsilon минус первое, ну, то есть вы получаете
[26:46.600 --> 26:57.360]  как бы такой эффект, что, сделав вот такую хитрую схему, о которой я сейчас сказал,
[26:57.360 --> 27:06.080]  хитрую схему, ну, вот, связанную с, значит, ну, со специальным сэмплированием, то есть со специальной
[27:06.080 --> 27:18.480]  генерацией траектории, вот, мы, извините меня, до 20-30, да, ну, до 8-30 занятий, я, я, я сотру все,
[27:18.480 --> 27:31.360]  да, сам сотру все, да, извините меня, лекция, я могу продолжить, да, спасибо, значит, прошу
[27:31.360 --> 27:39.440]  прощения, да, что, значит, отвлекся, так вот, у нас получается два результата, вот этот вот результат,
[27:39.440 --> 27:46.640]  это было до этого, как бы классика, то есть просто обычная стандартный подход, ну, и если мы начинаем
[27:46.640 --> 27:55.320]  хитрить, то есть мы начинаем, значит, вычислять уже по другой схеме, то есть мы сэмплируем как бы,
[27:55.320 --> 28:01.960]  видите, тут размер сэмпла, он меняется, то есть nl, давайте посмотрим на результат,
[28:01.960 --> 28:09.120]  который получился, у нас в принципе получается, что ε-2, оно всегда сидит в сэмплах, но обратите
[28:09.120 --> 28:14.520]  внимание, что тут присутствует ашель, и, например, если l равняется нулю, то это просто
[28:14.520 --> 28:23.280]  ε-2, а вот если это уже там, не знаю, ну, короче говоря, ашель растет с геометрической,
[28:23.280 --> 28:30.280]  ну, бывает точнее, с геометрической прогрессией, то есть вы как бы в каком-то смысле, вот эта ашель,
[28:30.280 --> 28:35.840]  давайте еще раз посмотрим, чему она равняется, вы его все убываете, убываете, убываете,
[28:35.840 --> 28:41.720]  убываете до момента, когда ашель просто станет пропорционально ε, значит, это означает, что вот
[28:41.720 --> 28:48.680]  эта nl, которая здесь стоит, оно изначально было ε-2, а уже под конец вы можете сэмплить все,
[28:48.680 --> 28:55.920]  все как бы меньше и меньше, то есть вы в этом вот выражении берете размер сэмпла, а меньше,
[28:55.920 --> 29:01.480]  на этом идет некоторая игра, то есть не надо каждый раз сэмплировать ε-2, и заметьте,
[29:01.480 --> 29:07.160]  что число таких итераций, оно тоже вот как бы логарифмическое, то есть в смысле не тоже,
[29:07.160 --> 29:15.160]  а как бы оно логарифмическое, это позволяет потерять не сильно много, потерять вот логарифм,
[29:15.160 --> 29:22.640]  и это вот специальная такая схема, довольно эффективная, ее сейчас активная, но не сейчас,
[29:22.640 --> 29:29.040]  уже там давно, то есть это была такая кульминация финансовой математики где-то лет 15 назад,
[29:29.040 --> 29:35.080]  вот был относительный бум, может даже поменьше, побольше времени, для 20 назад, ну где-то так,
[29:35.080 --> 29:41.800]  20-15 лет назад было очень популярно все это, вот, и вот такими вещами люди активно занимались,
[29:41.800 --> 29:49.880]  это порождало, значит, вот такие вот исследования, ну вот к чему это приводило,
[29:49.880 --> 29:57.880]  я здесь описал некоторые результаты, вот, по-моему довольно интересно, значит понятно,
[29:57.880 --> 30:02.800]  что я не очень аккуратно всю эту схему обосновал, она требует, ну на самом деле более
[30:02.800 --> 30:09.200]  стругих доказательств, но качественно, я надеюсь, что вы более-менее поняли, значит,
[30:09.200 --> 30:18.680]  о чем идет речь, вот, ну, собственно, наверное, про multilevel Monte Carlo у меня все, если какие-то
[30:18.680 --> 30:29.400]  вопросы. То есть единственная разница, что раньше мы били на одинаково маленькие кусочки, теперь мы
[30:29.400 --> 30:33.840]  как бы сначала одно разбиение, потом другое, потом третий и так далее. Да, ну, то есть раньше мы
[30:33.840 --> 30:39.200]  использовали одну схему, то есть мы использовали схему с постоянным шагом, то есть h было одинаково
[30:39.200 --> 30:44.400]  для всех реализаций, то есть сделав большое количество реализаций, вы делали все их на одном шаге,
[30:44.400 --> 30:53.880]  а теперь мы действуем по-другому, мы делаем схемы с разным шагом и оцениваем вот это y по одной
[30:53.880 --> 31:01.000]  схеме, потом этот y оцениваем по другой схеме, ну и делаем это еще вот, как бы сказать, вот с такой
[31:01.000 --> 31:07.880]  разностью, чтобы был какой-то эффект, ну, эффект одной траектории, что ли, что на одной траектории у
[31:07.880 --> 31:22.000]  нас в принципе, как объяснить, ну, ну, то есть мы, мы теряем, что ли, вот эту проблему большого
[31:22.000 --> 31:29.520]  байса за счет вот, за счет того, что фактически все сводится к тому, что там на последней траектории,
[31:29.520 --> 31:38.800]  потому что есть некоторая такая телескопичность этих вот, значит, дел, ну, потому что когда мы
[31:38.800 --> 31:45.840]  говорим о байсе, мы по сути, то есть нам не важно, что тут разный показатель 1 на 0, 1 на nl, для нас важно
[31:45.840 --> 31:50.720]  в среднем, что это такое, то есть вы, конечно, можете посчитать, какой байс будет вот здесь,
[31:50.720 --> 31:56.120]  но ровно такой же байс будет, если возьмете мат ожидания от этой штуки, которая будет сокращаться
[31:56.120 --> 32:01.280]  вот с этим по байсу, не по дисперсии, еще раз, здесь очень важно понимать, что мы специально разделяем
[32:01.280 --> 32:07.640]  байс и variance, и вот по байсу вы как бы берете, считаете какую-то величину, ну блин, она очень грубая,
[32:07.640 --> 32:14.480]  то есть здесь как бы байс отдельного слагаемого, он ужасен, потому что, ну, понятно, что если вы
[32:14.480 --> 32:20.360]  будете байс y0 считать, он там колоссальный, потому что у вас шаг имеет сетка h0, она
[32:20.440 --> 32:27.160]  совпадает с t, у вас одна интерация происходит, и это глупый метод, если бы так делали, но вы сделали
[32:27.160 --> 32:31.840]  хитрее, вы как бы вот эту штуку вот сюда засунули, это еще очень важный момент, и поэтому,
[32:31.840 --> 32:35.160]  когда вы будете считать у мат ожидания вот этой штуке, а потом мат ожидания вот этой штуки,
[32:35.160 --> 32:39.160]  то мат ожидания вот этой штуки будут в точности равно мат ожидания вот этой штуки,
[32:39.160 --> 32:45.640]  оно сократится, но при этом, как бы вот то что вы считаете на одной траектории, вот эту разность,
[32:45.640 --> 32:53.240]  вот эту разность позволяет вам как бы сказать дисперсию вот этой штуке
[32:53.240 --> 32:58.680]  определять ну и точнее вот всякие ее характеристики вот связаны с отклонением
[32:58.680 --> 33:03.780]  около мат ожидания позволяет вот как раз оценивать эту штуку исходя из
[33:03.780 --> 33:08.820]  свойств вот этого винаровского ну дефузионного процесса то есть когда вы
[33:08.820 --> 33:13.800]  берете траекторию то масштаб ее как раз определяется масштаб вот невязкий
[33:13.800 --> 33:17.560]  который тут приобретается масштаб определяется вот этой вот штукой потому
[33:17.560 --> 33:22.680]  что это одна траектория то есть как бы объяснить у вас есть траектория и вы ее
[33:22.680 --> 33:28.360]  берете с частотой 2 исчезну то есть просто делите на отрезки да а потом в
[33:28.360 --> 33:34.080]  два раза чаще эти отрезки делаете и как бы расхождение этих траектории ну
[33:34.080 --> 33:39.520]  грубо говоря потому что масштаб вот вот как бы масштаб именно вот с точки
[33:39.520 --> 33:43.760]  зрения случайности дисперсии он вот такой вот это вы наблюдаете вот в этом
[33:43.760 --> 33:47.320]  месте то есть это просто все так специально подобрана то есть вот вот в
[33:47.320 --> 33:51.520]  этом месте вы наблюдаете что масштаб у атошель потому что это одна траектория
[33:51.520 --> 33:56.720]  просто в одном случае шаги в два раза чаще чем в другом и за счет вот
[33:56.720 --> 34:00.280]  специфики то сказать ну это вот такое несложно упражнение вы можете проверить
[34:00.280 --> 34:06.000]  что и это и это верно все остальное дело техники как только вы догадались да вот
[34:06.000 --> 34:14.600]  такой конструкции, ну все, победа. Дальше вы можете просто подбирать, исходя из требования,
[34:14.600 --> 34:20.840]  вот то, что надо, то есть просто действительно поставить задачу оптимизации, как подобрать,
[34:20.840 --> 34:26.040]  ну распорядиться той степенью свободы, которая есть. Ну понятно, что L это уже не степень свободы,
[34:26.040 --> 34:31.080]  вы однозначно ее определяете, но еще осталась степень свободы с выбором размеров сэмплов.
[34:31.080 --> 34:35.880]  Ну вот ими вы и распоряжаетесь так, чтобы наилучшим образом это все стало. Ну вот-ка,
[34:35.880 --> 34:41.480]  что это получится? Это надо уже считать, вот можно посчитать, что будет так. То есть на самом деле это
[34:41.480 --> 34:47.440]  достаточно такой необычный сюжет, но повторю, требует более аккуратного обоснования. Ну если
[34:47.440 --> 34:52.800]  с байсом еще тут все как-то совсем очевидно, то уже с variance надо чуть поаккуратнее, конечно,
[34:52.800 --> 34:57.880]  тут объяснять, почему этот variance равняется у атошель, но более-менее соображение,
[34:57.880 --> 35:07.440]  вот какие приводят к этому я объяснил. Теперь у нас есть такой сюжет, он называется,
[35:07.440 --> 35:18.720]  ну я не знаю, в общем есть такая книжка, значит книжка Райгородского литвак. Ну вот название
[35:18.720 --> 35:24.600]  раздела можно понимать как счетчики с короткой памятью. А М. Райгородский давайте я все-таки
[35:24.600 --> 35:32.000]  сделаю. Красный цвет. А. М. Райгородский. Нелли Литвак. Ну я не знаю, они пишут без имен,
[35:32.000 --> 35:38.680]  я к сожалению отчества Нелли Литвак не знаю, поэтому будет А. Райгородский и Н. Литвак. Райгородский.
[35:38.680 --> 35:46.560]  Очень хорошая книжка, называется математика компьютерного века. Вот, она естественно такая,
[35:46.560 --> 35:51.520]  ну судя, вот наверное можете судить, потому что формул почти нет. Она написана для, в общем,
[35:51.520 --> 35:57.160]  широкой аудитории, то есть естественно не для нашего курса, потому что у нас такой продвинутый
[35:57.160 --> 36:02.240]  как бы курс, вроде там такая продвинутая математика. Но тем не менее, мне бы хотелось вот прям не
[36:02.240 --> 36:08.640]  залезая в дебри, рассказать вот приблизительно на таком уровне, как в этой книжке написано,
[36:08.640 --> 36:15.480]  об одном сюжете, вот который здесь вот называется счетчики с короткой памятью. Ну и фактически
[36:15.480 --> 36:22.240]  речь идет о том, чтобы решить очень такую несложную, на первый взгляд, задачу посчитать,
[36:22.240 --> 36:28.280]  сколько разных чисел встречается в каком-то массиве чисел, то есть у вас есть массив чисел,
[36:28.280 --> 36:35.280]  вот не знаю, вот он допустим приведен, и сколько раз тут встречается, ну не сколько раз, а сколько
[36:35.280 --> 36:40.760]  разных чисел здесь есть. Ну вообще говоря, вот собственно мы как делаем, 15 первый раз встречается,
[36:40.760 --> 36:46.280]  48 первый раз, 32 первый раз, 31 первый раз, 48 ранее встречалось, но это вот я как бы,
[36:46.280 --> 36:52.440]  так сказать, иду с точки зрения человеческого мозга, то есть я иду и каким-то образом называю
[36:52.440 --> 36:57.440]  вам, что ага, значит 50 уже встречалось, у меня в памяти, потому что есть, что 50 уже встречалось,
[36:57.440 --> 37:02.280]  а как вы это запрограммируете, то есть это что надо делать, все это хранить в памяти каждый раз,
[37:02.280 --> 37:06.920]  когда появляется новое число, сравнивать со всеми, которые до этого были, то есть уникальными,
[37:06.920 --> 37:12.760]  ну знаете, это очень долго, потому что если действительно вы имеете огромный массив,
[37:12.760 --> 37:19.880]  например, айпи адресов, и значит вам интересно, сколько там транзакций с уникальными картами было,
[37:19.880 --> 37:26.360]  или с айпи, ну и с картами банковскими, или сколько вообще значит айпи адресов там,
[37:26.360 --> 37:34.120]  на этот сервер уникальных обращалось, да, за n log n это решается, значит это вот в том-те дело,
[37:34.120 --> 37:40.080]  что нам не хочется, чтобы n вылезало, потому что ну вот комментарий был в чате, что я не хочу,
[37:40.080 --> 37:45.960]  чтобы это n вылезало, это n огромная размерность, я хочу, чтобы ответ был log log n, чтобы моя
[37:45.960 --> 37:51.320]  процедура позволяла как-то, может быть, неточно, вероятностно за log log n оценивать,
[37:51.320 --> 37:58.000]  знаете, вот настолько хорошо, что было, понятно, что это вообще говоря кажется немножко фантастичным,
[37:58.000 --> 38:03.040]  и явно здесь есть какой-то обман, но вот мы давайте попробуем разобраться, как это возможно,
[38:03.040 --> 38:10.480]  то есть чтобы n не вылезало уж точно в как бы, как n, ну и даже как логарифм n не здорово,
[38:10.480 --> 38:15.600]  чтобы оно вылезало, значит еще раз повторю постановку задачи, значит она заключается в том,
[38:15.600 --> 38:23.280]  чтобы посчитать число уникальных адресов чисел в массиве, огромном массиве из чисел, вот так,
[38:23.280 --> 38:27.920]  что там еще, а как вообще сделать быстрее, чем за n, если нам нужно минимум n единиц в
[38:27.920 --> 38:37.560]  времени просто в виде числа, так смотрите, речь идет о том, что, значит, ну, как сказать,
[38:37.560 --> 38:44.120]  речь идет о памяти, то есть речь идет не о том, сколько мы должны пройти, значит, вот n, да,
[38:44.120 --> 38:48.920]  мы должны пройти весь массив, это правда, то есть если понимать под сложностью, сколько мы должны
[38:48.920 --> 38:55.720]  пройти как бы элементов, то массив мы должны пройти, вопрос другом, как бы, как вы это будете
[38:55.720 --> 39:00.040]  делать, что вы будете использовать, какие вы будете структуру данных создавать и что вы в этих
[39:00.040 --> 39:04.760]  структурах данных будете хранить, если что с чем сравнивать, то есть сложность задач она измеряется
[39:04.760 --> 39:10.160]  не только тем, что вы должны как бы n чисел считать, но вы же этого числа не просто считываете,
[39:10.160 --> 39:17.800]  вы его должны сравнивать соответственно, и как бы вот это, значит, соответственно надо его
[39:17.800 --> 39:24.640]  хранить все, что у вас есть в памяти или как-то их, может быть, как-то разрежено, но в любом случае
[39:24.960 --> 39:31.240]  это некоторая проблема, что организовать вычисление таким образом, чтобы и память
[39:31.240 --> 39:37.720]  была эффективно использована, и время было действительно не сильно больше, чем просто прямой
[39:37.720 --> 39:43.480]  проход по всем числам, это некоторый челлендж, да, ну вы правы, что конечно пройти-то по всем
[39:43.480 --> 39:51.680]  числам надо, значит, давайте сначала просто предположим вот для простоты, что эти числа случайные,
[39:51.680 --> 40:01.600]  что эти числа случайные, ну и какое-то количество чисел там, я не знаю, вот выбросили,
[40:01.600 --> 40:12.000]  ну в данном случае здесь случайно-равновероятно выбирается число от 1 до 50, да, вот от 1 до 50,
[40:12.000 --> 40:20.360]  ну вот здесь такая постановка задачи, вот, и дальше мы говорим, что если числа случайные и нам
[40:20.400 --> 40:24.640]  интересно число уникальных чисел, ну или не обязательно, ну как бы вот просто есть огромный
[40:24.640 --> 40:31.800]  массив, давайте я как бы нарисую, есть огромный массив данных, и они более-менее случайно
[40:31.800 --> 40:38.840]  выбираются как от отрезка там от 1 или от 0 до некоторого большого числа, вот эти числа как-то
[40:38.840 --> 40:49.480]  случайно выбираются, возможно с повторениями возникает вопрос, вот если, значит,
[40:49.480 --> 40:58.200]  чисел n большое, а наименьшее число, которое встретилось, например, равняется 7, то что можно
[40:58.200 --> 41:05.200]  сказать относительно того, насколько, не знаю, там не обязательно 7, ну просто какому-то числу равно,
[41:05.200 --> 41:11.840]  то что можно сказать относительно того, сколько всего было уникальных чисел, но если мы считаем,
[41:11.840 --> 41:20.000]  что как бы вот эта вот величина, она характерна для, ну поскольку числа случайные,
[41:20.000 --> 41:28.720]  для, значит, как объяснить-то, ну для равномерного распределения на отрезке,
[41:28.720 --> 41:38.000]  значит, сказать там, ну то есть вы бросили какое-то количество чисел случайно равномерно,
[41:38.000 --> 41:43.320]  и какие-то из них повторяются, вот сколько независимых было чисел брошено, которые уникальные,
[41:43.320 --> 41:52.000]  ну как бы, если совсем по-простому, то эти числа, они ну как-то равномерно лягут, вот да, и по идее,
[41:52.000 --> 41:59.520]  если они вот действительно совсем по-простому равномерно легли, то вот как раз такой вот отрезочек
[41:59.520 --> 42:05.560]  позволяет вам оценить, с какой плотностью они легли, то есть если n большое, это число всего
[42:05.560 --> 42:14.760]  позиции, а у вас остался зазор, вот, например, там 0,7, наверное, вы вправе там ожидать, что n на 7,
[42:14.760 --> 42:21.880]  да, где-то будет, вот, как бы такое число уникальных величин, но, конечно, это очень грубая идея,
[42:21.880 --> 42:29.240]  и вообще говоря, на самом деле, конечно, надо делать намного аккуратнее, хитрее алгоритм,
[42:29.240 --> 42:36.080]  который вот так в таком ключе работает, ну, то есть он просто пытается отследить самое минимальное
[42:36.080 --> 42:42.400]  число, то есть он идет, значит, по списку и следит за минимальным числом, но вот мы видим в этом списке,
[42:42.400 --> 42:48.200]  что минимальное число равняется 0,1, и это не работает, ну, то есть вы просто получили 0,1,
[42:48.200 --> 42:54.200]  и все, а как бы, значит, что у вас по такой стратегии, ну, то есть стратегия простая, каждый раз хранить
[42:54.200 --> 42:59.960]  в массиве, ну, вот есть как бы ячейка памяти, и мы проходим по вот этим элементам, идем-идем,
[42:59.960 --> 43:05.320]  и каждый раз записываем вот в эту ячейку памяти самое наименьшее число, пока оно 15,
[43:05.320 --> 43:10.840]  пока оно 15, но вот в какой-то момент оно стало 0,1, и, собственно, после этого момента оно не меняется,
[43:10.840 --> 43:17.600]  вот это 0,1, ну, и мы доходим до конца, и мы живем с этим 0,1, вот если эти данные просто какие-то
[43:17.600 --> 43:25.320]  числа, то такая идея не работает, и вообще говоря, она плохая, но если вы действительно как бы
[43:25.320 --> 43:32.560]  работаете, ну, так сказать, с нормальной компьютерной программой, то никто не заставляет вас работать с
[43:32.560 --> 43:37.440]  первозданными IP-адресами, которые есть, или с названиями там, я не знаю, вот как тут пример
[43:37.440 --> 43:42.680]  у Райгородского, с названиями магазинов там каких-то, ну, или вот сайтов, которые вот посещаются,
[43:42.680 --> 43:48.760]  вы можете ставить им в соответствие случайный этап значения в некое вот хэш, хэшированием
[43:48.760 --> 43:56.280]  заниматься, то есть, по сути, как бы просто ставить соответствие каким-то там буквам или каким-то
[43:56.280 --> 44:02.480]  IP-адресам по определенным правилам, ну, практически вот, можно сказать, случайная независимая величину,
[44:02.480 --> 44:09.040]  а там, на самом деле, не совсем это как бы делается случайно, то есть, как бы некоторые
[44:09.040 --> 44:14.080]  закономерности там, в том числе, связанные с теоретико-числовыми какими-то вот вещами,
[44:14.080 --> 44:19.600]  там это все есть, но нас сейчас это не сильно беспокоит, для нас важно, чтобы мы с хорошим
[44:19.600 --> 44:24.840]  приближением могли считать, что вот эти вот последовательности условных нулей и единиц,
[44:24.840 --> 44:30.880]  которые кодируют встречающиеся нам элементы множества, ну, что они в каком-то смысле случайны,
[44:30.880 --> 44:37.920]  вот в хорошем смысле, вот с хорошей степенью, они вот случайны, каждое число, оно имеет
[44:37.920 --> 44:45.320]  свой уникальный адрес, понятно, что даже если элементов много, за счет того, что мы как бы
[44:45.320 --> 44:50.560]  битого, например, кодируем, мы можем очень большие числа закодировать, и длина описания у них
[44:50.560 --> 44:58.920]  будет не очень большая, ну и дальше, собственно, идея лок-лок вот этого алгоритма Флажале,
[44:58.920 --> 45:05.520]  вот он Флажале, Филипп Флажале, которая позволила просто некоторую революцию совершить,
[45:05.680 --> 45:13.760]  во всяких алгоритмах там, в том числе работы в социальных сетях, как там считать число рукопожатия,
[45:13.760 --> 45:20.640]  тому подобное, вот такого типа вспомогательной задачи там возникают, и идея Флажале заключается в том,
[45:20.640 --> 45:28.880]  чтобы хранить не просто минимальное число, а хранить минимальное число бит, которые равны нулю,
[45:28.880 --> 45:33.960]  ну не минимальное, а наоборот максимальное, то есть хранить число бит, которые вот мы идем по списку,
[45:34.280 --> 45:37.820]  вот первое число, вот второе, вот третье, вот четвертое, вот пятое.
[45:37.820 --> 45:46.200]  После того как мы посмотрели первое число мы храним 1 бит нулевой, после того как мы посмотрели второе число мы храним
[45:46.200 --> 45:52.060]  3 бита нулевых, то есть число 3 закодировали, ну потом это число не поменялось, тут оно тоже не
[45:52.060 --> 45:58.440]  поменялось, ну в конце оно тоже не поменялось, то есть и того, мы имеем, что у нас как-бы минимальные
[45:58.440 --> 46:05.320]  элемент имеет 3 бита в начале ну то есть это означает что разреженность в среднем одна восьмая
[46:05.320 --> 46:13.400]  поэтому если у нас есть понимание ну то есть если у нас есть например 8 бит да допустим 8 бит 2
[46:13.400 --> 46:20.960]  в смысле получается 2 восьмой то с учетом того что у нас первые 3 бита значит как бы
[46:20.960 --> 46:30.680]  получаются нулевыми ну вот как здесь мы можем рассчитывать на то что число уникальных элементов
[46:30.680 --> 46:37.000]  повторю очень грубо 2 восьмой на 2 третий понятно что на самом деле там какой-то коридор и так вот
[46:37.000 --> 46:42.400]  делать как я ну наверное аккуратно надо как-то точнее но во всяком случае с точностью до масштаба
[46:42.400 --> 46:48.440]  2 вы таким образом оцениваете то есть плюс-минус два раза вы ошибаетесь как бы но это уже хотя
[46:48.440 --> 46:53.920]  приближенно оценивают число вот с точностью до двойки но это можно дальше уточнять как-то это
[46:53.920 --> 46:59.800]  все делать более вот эту идею использовать но обратите внимание что вы не просто храните
[46:59.800 --> 47:06.840]  минимальный элемент а вы даже храните число просто бит которые нулевые в минимальном элементе это
[47:06.840 --> 47:14.760]  получается поэтому лок-лок того что надо как бы хранить про эти числа знать и замечу что на
[47:14.760 --> 47:22.120]  самом деле это все действительно ну в общем-то как сказать ну осуществимо на практике то есть
[47:22.120 --> 47:27.840]  это не есть какая-то такая совершенно выдуманная история и вот что замечательно вот в этой книжке
[47:27.840 --> 47:33.400]  райгородского литвак приводится очень такой яркий пример как с помощью вот этого алгоритма
[47:33.400 --> 47:41.360]  удалось компании фейсбук привлекая математиков значит да кстати правильно название хипер лок-лок
[47:41.360 --> 47:48.960]  этого счетчика как удалось значит ну просто посчитать число рукопожатия среднее число
[47:48.960 --> 47:54.920]  рукопожатия вот в социальных сетях вот также вот объединяя то есть мы берем каждого человека
[47:54.920 --> 48:02.760]  складываем его друзей массив вот который и считаем число уникальных друзей потом берем значит
[48:02.760 --> 48:09.320]  всех друзей друзей складываем массив получаем новый массив но уже можно посчитать сколько именно
[48:09.320 --> 48:14.440]  друзей через два рукопожатия выкидывая числа уникальных ну как бы мы можем посчитать число
[48:14.440 --> 48:21.940]  друзей первого ранга число друзей второго ранга ну а ну вот иughtersry посчитается как бы
[48:21.940 --> 48:30.080]  и вычитанием того сколько у него сколько с какой-то на всякой вершины входят множество друзья первого
[48:30.080 --> 48:34.160]  второго ранга завыч slides dernier первая ранга и вот вот это возможно считать именно 0 du
[48:34.160 --> 48:38.300]  в 팀ей она важна потому что один и тот же элемент например через два рукопожатия можно
[48:38.300 --> 48:44.900]  прийти к этому и через своего другого мы вот дружим в четвером вот я дружу с ним и с ним вот
[48:44.900 --> 48:50.460]  но они тоже дружат вот с этим еще одним человеком я с ним напрямую не дружу но я к нему через два
[48:50.460 --> 48:54.740]  рукопожатия могу прийти через своего одного друга и через другого но мне его надо в итоге
[48:54.740 --> 49:00.540]  учитывать один раз вот это уникальная возможность считать сколько различных элементов множестве она
[49:00.540 --> 49:06.500]  позволяет вот все это делать за реальное время и с реальной памятью и вот это действительно было
[49:06.600 --> 49:13.340]  революция к сожалению вот тут написано что флажоле не дожил до момента когда это все набрело ну
[49:13.340 --> 49:19.300]  так сказать стало вот настолько популярно ему буквально не хватило там несколько лет что вот
[49:19.300 --> 49:25.100]  в одиннадцатом году и не стало его буквально в тринадцатом году в двенадцатом это уже в общем-то
[49:25.100 --> 49:30.140]  но вот в четырнадцатом году это было внедрение понимание что это все будет классно работать это
[49:30.140 --> 49:35.500]  тут уже было в 12-13 году вы можете посмотреть ссылки я эту книжку сброшу и мне кажется это
[49:35.500 --> 49:42.740]  очень поучительные примеры очень простых идей собственно на похожие идеи много чего вот базируется
[49:42.740 --> 49:49.420]  мы можете посмотреть это вот есть книжка блюма хопрофта канана то есть это недалеко не единственный
[49:49.420 --> 49:57.860]  так сказать такой сюжет вокруг вот этой всей темы но он достаточно простой чтобы его за небольшое
[49:57.860 --> 50:02.620]  время рассказать и удивительным образом он у нас по моему особо никуда не входит ни в какие курсы
[50:02.620 --> 50:12.220]  такие вот по моему школьникам можно более чем рассказывать вот и в общем-то на этом вот та часть
[50:12.220 --> 50:18.860]  которая не связана с ну как бы не непосредственно математикой машинного обучения в нашем курсе
[50:18.860 --> 50:26.780]  но не то чтобы заканчивается в моей части она заканчивается вот еще будет вот в частях там
[50:26.780 --> 50:32.060]  алексея фролова математика которая не напрямую связана с машинным обучением может быть у
[50:32.060 --> 50:36.860]  максима рахубы вот у александра но иксана безносикова по многом связана с машинным
[50:36.860 --> 50:44.420]  обучением и вот теперь у меня цель перейти уже непосредственно к тому что ну давайте мы прям
[50:44.420 --> 50:48.980]  назовем следуя вот этой книжке которая тоже сброшу вы сможете ее смотреть это называется
[50:48.980 --> 50:55.660]  алгоритм икс та хасте конвекс оптимизейша но мы конкретно будем на это смотреть сквозь призму
[50:55.660 --> 51:02.060]  приложение к машинному обучению то есть нас будет интересовать не что-нибудь такое абстрактное
[51:02.060 --> 51:08.460]  смысле стахастической оптимизации а вот стахастическая оптимизация на службе у машинного
[51:08.460 --> 51:14.300]  обучения я извиняюсь что тороплюсь потому что ну я даже не столько тороплюсь сколько ну стараюсь
[51:14.300 --> 51:19.260]  не отвлекаться на какие-то такие детали потому что мне очень хочется успеть вам проговорить вот
[51:19.260 --> 51:25.580]  всю эту первую главу ну хотя бы как-то концептуально хотя бы до там до concluding remarks то
[51:25.580 --> 51:32.940]  есть 23 страницы как-то главные моменты вам рассказать на чем строится современное как бы
[51:32.940 --> 51:39.060]  машинное обучение если смотреть на нее с позиции специалиста постов оптимизации вот мы должны это
[51:39.060 --> 51:45.140]  прочувствовать очень частично у нас это было уже в курсе ну совсем чуть-чуть когда мы говорили
[51:45.140 --> 51:50.140]  про принцип максимум правдоподобия вот сейчас наша задача пройтись поэтому всему за оставшийся
[51:50.140 --> 51:57.820]  ну вот полтора часа пройтись поэтому ну более основательно и начнем мы просто с постановки
[51:57.820 --> 52:03.620]  задачи стог оптимизации то есть нам интересно задача стог оптимизации вот как она формулируется
[52:03.620 --> 52:10.140]  минимум некоторые функции при вот где функционал этом от ожидания по кси вот сейчас мы столкнемся
[52:10.140 --> 52:16.780]  ситуации когда это кси нам как раз будет неизвестно и вот собственно пример вот очень яркий пример я
[52:16.780 --> 52:23.020]  очень рекомендую как минимум его понять потому что ну с этого все начинается с какого-то совсем
[52:23.020 --> 52:30.060]  простого примера вот и если он будет понятен дальше будет уже попроще вот эти примеры значит
[52:30.060 --> 52:35.700]  заключается в следующем у нас есть низ прошу прощения у нас есть неизвестный параметр x звездой
[52:35.700 --> 52:42.580]  вы видите вот на экране вот он он из множества действительных чисел у нас есть шум который
[52:42.580 --> 52:48.900]  вот имеет гауссовский закон с параметрами 0 сигма в квадрате и мы можем сэмплить мы можем
[52:48.900 --> 52:55.820]  сэмплить случайной величины ксика которые есть и к со звездой неизвестный параметр скалярный
[52:55.820 --> 53:01.220]  плюс шум в общем-то проще схемы по моему пока не придумали ну разве что схемы испытаний
[53:01.220 --> 53:08.180]  вернули вот с неизвестной вероятностью выпадения орла значит все эти этак а я иди но здесь
[53:08.180 --> 53:13.860]  расшифровано что это означает ну и поставлена задача оценить оценить к со звездой вот из-за
[53:13.860 --> 53:21.660]  по вот этим наблюдением то есть по сути построить оценку x с крышкой я не знаю как там это обозначать
[53:21.660 --> 53:28.500]  вот этой выборке кси катая вот такая типичная задача казалось бы причем здесь тох оптимизация да
[53:28.500 --> 53:33.380]  то есть еще раз мы начали говорить про анализ данных и начали говорить что анализ данных это вот
[53:33.380 --> 53:38.740]  об этом но я сразу с места в карьер начал говорить что хорошо давайте рассмотрим конкретную задачу
[53:38.740 --> 53:43.900]  мат-статистики оценить неизвестный параметр и сразу же привожу задачу оптимизации стох
[53:43.900 --> 53:51.060]  оптимизации я говорю что меня вот это случайно величина кси имеет мат ожидания смотрите какое
[53:51.060 --> 53:57.380]  она мат ожидания имеет ну дровно то которое имеет ксикатая то есть я как бы по имея модель вот эту
[53:57.380 --> 54:03.300]  задав модель я в общем и пользуюсь этим тем что это она такая ну и говорю что вот почему-то
[54:03.300 --> 54:08.820]  именно квадратичную функцию надо минимизировать и минимум этой функции оказывается то само икса
[54:08.820 --> 54:14.300]  звездой то есть если мне хочется оценить икса звездой то мне надо решить задачу сток оптимизации
[54:14.300 --> 54:20.420]  дальше вы уже там можете сами додумывать как решить эту задачу например вот заменив мат
[54:20.420 --> 54:25.620]  ожидания выборочным средним или каким-то стахастическим градиентным спуском а-ля да
[54:25.620 --> 54:32.940]  это уже дело 10 самое главное сейчас понять что мы действительно можем заменить задачу оценки
[54:32.940 --> 54:38.780]  неизвестного параметра задачи сток оптимизации почему это так ну как бы самый простой вариант
[54:38.780 --> 54:45.140]  ну раскрыть мат ожидания получить соответствующую функцию и проверить что эта функция действительно
[54:45.140 --> 54:50.860]  достигает минимумов точки икса звездой ничего проще тут как бы доказать не получится вот давайте
[54:50.860 --> 54:55.780]  это сделаем вот написанная выкладка она следует просто из того что у нас задана
[54:55.780 --> 55:01.180]  раз закон распределения кси то есть мы считаем квадрат мат ожидания кси он складывается из квадра
[55:01.180 --> 55:08.580]  из квадрата мат ожидания и соответственно дисперсии ну и вот значит из-за того что тут минус берется
[55:08.580 --> 55:13.780]  смешанное такое слагаемо то есть я надеюсь что как-то сильно пояснять вот эту выкладку не надо
[55:13.780 --> 55:23.220]  она довольно очевидно я просто раскрыл квадрат и применил мат ожидания ну как бы сказать каждому
[55:23.220 --> 55:28.660]  слагаемому ну и воспользовался тем что мат ожидания кси в квадрате это равняется ну как бы
[55:28.660 --> 55:35.860]  это равняется икс со звездой в квадрате плюс дисперсия в квадрате вот что я сделал почему это
[55:35.860 --> 55:41.580]  так потому что ну в общем так у меня это определяется дело то есть это как бы мат ожидания плюс дисперсии
[55:41.580 --> 55:47.940]  хорошо значит это действительно так меня вот эта функция достигает минимума ну что дальше но дальше
[55:47.940 --> 55:56.100]  можно заметить что мы можем заменить эту задачу вот эту задачу на такую задачу которая просто есть
[55:56.100 --> 56:02.540]  ну грубо говоря метод монте карло для вот вот этого функционала то есть мы заменяем от ожидания на
[56:02.540 --> 56:08.500]  выборочное среднее ну и решаем задачу которая получается с выборочным среднем она дает решение
[56:08.500 --> 56:12.700]  этой задачи дает такую оценку совершенно естественную оценку которую самое начало мы
[56:12.700 --> 56:17.500]  хотели получить ну то есть как бы интуиция подсказывала что просто надо взять и усреднить
[56:17.500 --> 56:22.380]  потому что этих нулевой мат ожидания и они взаимокомпенсируются ну и это на самом деле
[56:22.380 --> 56:28.740]  ну в каком смысле наилучшая оценка с точки зрения минимальности дисперсии в классе несмещённых
[56:28.740 --> 56:35.380]  оценок ну в общем другой способ решение той же самой задачи вот тут как бы концептуально важно
[56:35.380 --> 56:41.780]  понимать что заменять мот ожидания выборочном среднем это подход оффлайн то есть это подход
[56:41.780 --> 56:46.300]  который например позволяет хранить разные ксика на разных устройствах потом как-то они
[56:46.300 --> 56:52.020]  обмениваются это вот такой как бы распределённый потенциально там всякие параллельные сетапы
[56:52.020 --> 56:56.260]  а вот это и другая история это история которая в каком смысле онлайн получает информацию
[56:56.260 --> 57:02.940]  о выборке то есть ксика это элемент выборки она берет вводит функцию ну то есть мы вводим
[57:02.940 --> 57:08.540]  функцию f от x которая у нас здесь задана вот это самая функция ф от x кси мы считаем 100
[57:08.540 --> 57:15.580]  градиент ну то есть по сути мы считаем градиент функции f от x кси по x ну и как бы мотнут градиент
[57:15.580 --> 57:21.380]  он перестановочен с будет данном случае с взятием мат ожидания потому что распределение кси не
[57:21.380 --> 57:28.020]  зависит от x ну и мы можем вот записать какой-нибудь такой процедуры сгд и заметить что она приводит
[57:28.020 --> 57:32.700]  вот если именно так выбрать шаги то она приводит ровно к такой же оценке после n итерации ну то
[57:32.700 --> 57:38.420]  есть последняя точка этой процедуры даже не надо брать средне арифметическое но она даст ровно ровно
[57:38.420 --> 57:44.100]  оценку 1 4 то есть вот как бы две дороги приводящие к одному результату только в одном случае нам
[57:44.100 --> 57:48.700]  потребовалось как бы формально решать задачу оптимизации в другом случае никакую задачу
[57:48.700 --> 57:53.940]  оптимизации решать не надо и мы просто корректируем это называется агрегирование оценки то есть мы
[57:53.940 --> 57:59.300]  агри как бы сказать по мере получения информации корректируем свое текущее представление об оценке
[57:59.300 --> 58:05.620]  и когда у нас произойдет что мы всю выборку про просмотрим но только тут выборка обозначается
[58:05.620 --> 58:14.420]  вот стоит обратить внимание что она вот как бы пошла с этого дела с как бы с нуля мы немножко
[58:14.420 --> 58:21.620]  сдвинули это все вот то есть здесь как объяснить ну просто дань традиции что итерации чистых
[58:21.620 --> 58:27.500]  методов часто с нуля обозначают но а тут с нуля неудобно суммировать вот поэтому тут ну может
[58:27.540 --> 58:33.060]  быть тут даже на напечатка есть потому что здесь кси 0 кси 1 а потом к седин они неодинаковые то
[58:33.060 --> 58:39.580]  есть здесь здесь просто сдвиг здесь надо было написать ксикап плюс 1 но это не важно да то есть
[58:39.580 --> 58:46.700]  здесь можно было написать ксика плюс 11 ты сейчас это я потом исправлю вот в итоге версия пришлю вам
[58:46.700 --> 58:51.580]  правильно вот то есть мы получается две дороги рассмотрели которые ведут в одну в одно направление
[58:51.580 --> 58:54.580]  Но возникает вопрос, а почему это функция квадрат?
[58:54.580 --> 58:56.580]  Как я угадал?
[58:56.580 --> 58:59.580]  Как я смог взять и сразу сказать, что это квадрат?
[58:59.580 --> 59:03.580]  Хочется этот момент не оставлять за скобками,
[59:03.580 --> 59:05.580]  потому что он и есть главный на самом деле,
[59:05.580 --> 59:07.580]  а не то, что нам повезло.
[59:07.580 --> 59:09.580]  Наверное, всегда можно что-нибудь такое подобрать.
[59:09.580 --> 59:12.580]  И возникает вопрос сразу, а как это подбирать?
[59:12.580 --> 59:13.580]  Единственное ли это?
[59:13.580 --> 59:15.580]  Как наилучшим образом это подбирать?
[59:16.580 --> 59:22.580]  И вот мы сразу переходим к тому, что у нас задача вот этой есть.
[59:22.580 --> 59:26.580]  В этой задаче у нас есть плотность распределения случайной величины кси.
[59:26.580 --> 59:28.580]  Обратите внимание, что здесь сидит квадрат.
[59:28.580 --> 59:33.580]  Вот ровно тот квадрат, который сидит вот здесь, вот здесь, вот здесь.
[59:33.580 --> 59:35.580]  Наверное, это не случайно.
[59:35.580 --> 59:38.580]  Действительно, давайте заметим следующее,
[59:38.580 --> 59:42.580]  что если взять функцию плотности, вероятности,
[59:42.580 --> 59:46.580]  что имеет место х, а выпадет кс.
[59:46.580 --> 59:48.580]  То есть что это такое? Что вот тут написано?
[59:48.580 --> 59:55.580]  Это вероятность того, что мы пронаблюдаем согласно вот этой вот модели,
[59:55.580 --> 59:58.580]  мы пронаблюдаем набор случайных величин кс,
[59:58.580 --> 01:00:01.580]  если истинное значение параметра х.
[01:00:01.580 --> 01:00:03.580]  То есть если истинное значение параметра х,
[01:00:03.580 --> 01:00:06.580]  какова вероятность того, что мы пронаблюдаем набор значений кс?
[01:00:06.580 --> 01:00:10.580]  Вот это и есть так называемое правдоподобие, likely put.
[01:00:10.580 --> 01:00:15.580]  И дальше, естественно, рождается такая как бы гипотеза,
[01:00:15.580 --> 01:00:19.580]  интуиция, что ну хорошо, вот этот likely put,
[01:00:19.580 --> 01:00:25.580]  он дает нам как бы понимание, что он правдоподобие описывает.
[01:00:25.580 --> 01:00:30.580]  Но если мы хотим, чтобы х каком-то смысле наилучшим образом описывал данные,
[01:00:30.580 --> 01:00:33.580]  то, наверное, это правдоподобие должно быть максимально.
[01:00:33.580 --> 01:00:38.580]  Ну так, по логике, что если вот данные кск при заданном х
[01:00:38.580 --> 01:00:40.580]  имеют наибольшую вероятность,
[01:00:40.580 --> 01:00:43.580]  наверное, этот х как бы наилучшим образом им и соответствует.
[01:00:43.580 --> 01:00:46.580]  Но поэтому мы, ну и поскольку мы любим задачи на минимум,
[01:00:46.580 --> 01:00:50.580]  мы переписываем эту задачу как задачу вот на минимум от логарифма.
[01:00:50.580 --> 01:00:53.580]  Ну какая разница, максимизировать likely put
[01:00:53.580 --> 01:00:56.580]  или минимизировать минус лог likely put.
[01:00:56.580 --> 01:00:59.580]  Лог — это функция монотонная, монотонно возрастающая,
[01:00:59.580 --> 01:01:02.580]  поэтому там, где вот эта штука достигает максимума,
[01:01:02.580 --> 01:01:05.580]  там вот эта штука достигает минимума.
[01:01:05.580 --> 01:01:08.580]  Также я могу выбрать нормировочный множитель, если мне хочется.
[01:01:08.580 --> 01:01:11.580]  Логарифм берется для того, чтобы независимые случайные величины,
[01:01:11.580 --> 01:01:14.580]  их плотность расщепилась в сумму.
[01:01:14.580 --> 01:01:17.580]  Но так если мы сейчас возьмем вот эту плотность,
[01:01:17.580 --> 01:01:20.580]  отдельного слагаемого, напишем эту сумму,
[01:01:20.580 --> 01:01:25.580]  мы как раз получим ту самую задачу, которая и была записана.
[01:01:25.580 --> 01:01:28.580]  То есть мы получим вот такую постановку.
[01:01:28.580 --> 01:01:32.580]  Если мы берем гауссовский модель, мы получим вот такую задачу.
[01:01:32.580 --> 01:01:37.580]  Она соответствует вот этой вот постановке,
[01:01:37.580 --> 01:01:44.580]  за исключением того, что здесь нет слагаемых лишних и нормировочного множителя.
[01:01:44.580 --> 01:01:49.580]  Но поскольку задача — найти точку минимума, а не значение функции,
[01:01:49.580 --> 01:01:52.580]  то мы можем вполне себе позволить все это не писать.
[01:01:52.580 --> 01:01:56.580]  Вот это вот не писать. Нас это не интересует.
[01:01:56.580 --> 01:02:00.580]  Поэтому понятно, откуда квадрат появился в этой задаче.
[01:02:00.580 --> 01:02:03.580]  И вот оказывается, что то, что я вам сейчас рассказал,
[01:02:03.580 --> 01:02:05.580]  это на самом деле общая схема.
[01:02:05.580 --> 01:02:10.580]  Это общая схема, и эта общая схема — она вот откуда следует.
[01:02:10.580 --> 01:02:13.580]  Значит, берем точку х звездой,
[01:02:13.580 --> 01:02:18.580]  которой есть минимизатор вот этого как бы функции правдоподобия.
[01:02:18.580 --> 01:02:21.580]  Только в данном случае, если мы считаем выборку iid,
[01:02:21.580 --> 01:02:24.580]  нам нет необходимости вот это минимизировать.
[01:02:24.580 --> 01:02:27.580]  То есть это уже как бы лишнее.
[01:02:27.580 --> 01:02:29.580]  Ну то и зачем?
[01:02:29.580 --> 01:02:34.580]  У нас есть и так уже то, что надо.
[01:02:34.580 --> 01:02:37.580]  То есть у нас вот это есть, но от того, что я здесь просэмплирую,
[01:02:37.580 --> 01:02:41.580]  у меня как бы ничего не поменяется, у них это одинаково будет.
[01:02:41.580 --> 01:02:44.580]  Это вот задача, когда мы имперический делаем.
[01:02:44.580 --> 01:02:48.580]  Это вот я про имперический говорил, там да, там надо ксикат.
[01:02:48.580 --> 01:02:51.580]  А здесь у нас просто как бы мы исследуем вопрос в теории.
[01:02:51.580 --> 01:02:56.580]  Верно ли, что х со звездой будет принадлежать аркминему вот этой штуке?
[01:02:56.580 --> 01:02:58.580]  Ну действительно, это будет верно,
[01:02:58.580 --> 01:03:00.580]  потому что имеет место неравенство,
[01:03:00.580 --> 01:03:03.580]  которое получается из неотрицательности расстояния кульбока Лебера.
[01:03:03.580 --> 01:03:05.580]  По сути это неравенство Йенсона,
[01:03:05.580 --> 01:03:08.580]  ну или если хотите, неравенство выпуклости.
[01:03:08.580 --> 01:03:12.580]  Неравенство выпуклости функции лагарифм.
[01:03:13.580 --> 01:03:16.580]  То есть если для функции лагарифм вы напишите,
[01:03:16.580 --> 01:03:20.580]  что эта функция она выпуклая, она вогнутая точнее,
[01:03:20.580 --> 01:03:22.580]  вот что функция лагарифм вогнутая,
[01:03:22.580 --> 01:03:27.060]  Вот вы можете из этого неравенства, которое формулируется таким образом,
[01:03:27.060 --> 01:03:36.140]  что мат ожидания от вогнутой функции от x, x случайная величина,
[01:03:36.140 --> 01:03:49.500]  значит сейчас соображу, но будет меньше либо равняется, чем мат ожидания для f вогнутая,
[01:03:49.500 --> 01:04:02.420]  чем f от мат ожидания x. Вот собственно вариация на эту тему и дает возможность устанавливать
[01:04:02.420 --> 01:04:09.380]  нужное нам неравенство. Это не отрицательность расстояния кульбока лебера, это то же самое,
[01:04:09.380 --> 01:04:16.500]  что дивергенция, не расстояние, а дивергенция. Что условие, что x звездой является точкой минимум
[01:04:16.500 --> 01:04:23.780]  вот этой штуки. Здесь просто по определению написано, что у нас мат ожидания берется по
[01:04:23.780 --> 01:04:31.060]  мере, которая настоящая, ксито имеет настоящее распределение p от x звездой. И соответственно
[01:04:31.060 --> 01:04:35.780]  логарифм берется по произвольному x, то есть вероятность это считается по x звездой,
[01:04:35.780 --> 01:04:40.940]  которая настоящая, а тут произвольный x. И мы видим, что наименьшее значение как раз тогда,
[01:04:40.940 --> 01:04:46.300]  когда x совпадает с x звездой. Вот все это и написано. Это не значит, что оно единственное,
[01:04:46.300 --> 01:04:51.660]  но это значит, что оно содержится среди минимизаторов, что истинное значение содержится
[01:04:51.660 --> 01:04:56.780]  среди минимизаторов. Ну и собственно оценка максимального правдоподобия,
[01:04:56.780 --> 01:05:02.060]  которую мы уже с вами рассматривали, и она была выборочная средняя. Это оценка,
[01:05:02.060 --> 01:05:09.500]  которая получается, если вот записать вот это все дело, оценку правдоподобия, ну просто вот
[01:05:09.500 --> 01:05:17.340]  записать ее таким образом. То есть что мы уже делали. Только теперь обращаю внимание, вы теперь
[01:05:17.340 --> 01:05:22.500]  работаете с реальной выборкой. То есть у вас дана выборка, и вы пишете как бы не в теории,
[01:05:22.500 --> 01:05:28.740]  что x со звездой есть арк минимум вот такого матожедания, потому что оно невычислимо. У вас нет
[01:05:28.740 --> 01:05:34.020]  возможности считать x со звездой. Вы не знаете x со звездой. Это есть задача. То есть здесь написано
[01:05:34.020 --> 01:05:38.860]  как бы ерунда по большому счету. То есть здесь написано какая-то тавтологическая вещь, что x со
[01:05:38.860 --> 01:05:44.660]  звездой неизвестная является решением некой задачи. Но чтобы посчитать матожедание, вам тоже нужно
[01:05:44.660 --> 01:05:50.420]  как бы брать матожедание по неизвестному x со звездой. Ничего как бы такого великого. Просто тут
[01:05:50.420 --> 01:05:55.900]  x со звездой, тут и тут x со звездой. Но как только вы переписываете это уже, так сказать, апроксимируя
[01:05:55.900 --> 01:06:06.140]  вот это матожедание, ну или заменяя его просто вот этим вот вероятностью, то есть не добавляя
[01:06:06.140 --> 01:06:12.260]  здесь матожедания, понимая, что как бы если объем выборки большой, то вот этот вот арк максимум
[01:06:12.260 --> 01:06:17.180]  должен по идее быть сконцентрирован около того, где там матожедание максимум достигает. То вот
[01:06:17.180 --> 01:06:23.300]  вы получаете способ, который называется maximum likelihood estimation. И для него работает теорема,
[01:06:23.300 --> 01:06:29.060]  которая часто ее называют теоремой Фишера. В старой литературе, даже не в старой, но вот в общем
[01:06:29.060 --> 01:06:33.460]  Фишер к этому приложил руку. Но иногда просто теорема о свойствах оценки максимального
[01:06:33.460 --> 01:06:44.340]  правдоподобия. И теорема заключается в том, что как бы вы не оценивали неизвестные параметры,
[01:06:44.340 --> 01:06:51.940]  как бы спозируясь на выборке, то вот тот способ, который как бы описывается формулой 1.7, он
[01:06:51.940 --> 01:06:57.980]  асимпатический, когда объем выборки, а это мы обозначали n, он асимпатический большой,
[01:06:57.980 --> 01:07:02.340]  ну в смысле асимпатический стремится к бесконечности, то он асимпатический наилучший. То есть вот
[01:07:02.340 --> 01:07:07.420]  асимптотика имеется в виду по объему выборки. То такой способ, наилучший это значит, что у него
[01:07:07.420 --> 01:07:12.180]  наименьшая как бы корреляционная матрица, ну или по-другому говоря дисперсия по любому направлению.
[01:07:12.180 --> 01:07:18.220]  И вот эта матрица, она задается информационной матрицей Фишера, так называемой информационной
[01:07:18.220 --> 01:07:23.900]  матрицей Фишера. Вот это вот будет корреляционная матрица вот этой вот штуки. Ну и видно, что по
[01:07:23.900 --> 01:07:28.580]  сути дисперсия, об этом идет речь, она пропорциональна 1 на n. То есть то, о чем мы много раз
[01:07:28.580 --> 01:07:34.260]  говорили, качество оценки в плане дисперсии 1 на n. Значит отклонение среднеквадратичное будет
[01:07:34.260 --> 01:07:40.780]  1 на корень из n, что ожидаемо. И борьба идет не за зависимость от n, а за коэффициент при 1 на корень из n.
[01:07:40.780 --> 01:07:48.060]  Вот такой способ он максимум likely put on, ну и наилучший. Ну и мы с вами как бы качественно
[01:07:48.060 --> 01:07:53.060]  поняли откуда он возникает. И качественно поняли, что это все как бы возникает как некоторый
[01:07:53.060 --> 01:07:58.220]  эмпирический подход к задачи минимизации риска. Ну к задачи минимизации некого,
[01:07:58.220 --> 01:08:05.500]  так сказать, стог оптимизации по сути. То есть к решению задачи стог оптимизации. Ну здесь в общем-то
[01:08:05.500 --> 01:08:12.100]  неформально сформулировано, что мы не просто знаем, что у нее дисперсия, так сказать,
[01:08:12.100 --> 01:08:17.020]  минимальная, а у нее она еще нормально, ну то есть асимпатически оценка максимального
[01:08:17.020 --> 01:08:22.380]  правдоподобия ведет себя как нормальный случайный вектор, вот с как раз той
[01:08:22.380 --> 01:08:29.860]  самой корреляционной матрицей. То есть получается, что у нас в общем-то имеет
[01:08:29.860 --> 01:08:37.900]  место достаточно такой сильный результат. Ну и надо сказать, что для вот этих вот онлайн процедур,
[01:08:37.900 --> 01:08:44.160]  которые я приводил, вот ключевые фамилики, фамилик Полик, юдитский Руперт, который в конце 80-х,
[01:08:44.160 --> 01:08:50.340]  начале 90-х годов, они в общем-то вот то, что я здесь отмечал, как в общем-то, так сказать,
[01:08:50.340 --> 01:08:57.220]  что то же самое можно достичь некой процедуры типа SGD, вот они это сделали для общих задач,
[01:08:57.220 --> 01:09:03.180]  не обязательно вот значит таких вот простых, они это сделали вот в большей общности, по сути,
[01:09:03.180 --> 01:09:09.620]  для произвольных задач с некоторыми общими условиями, типа локальные там квадратичности,
[01:09:09.620 --> 01:09:16.020]  в общем это это локальные там сильные выпуклости, в общем это все можно прочитать, там ссылки будут,
[01:09:16.020 --> 01:09:22.260]  вот детали, вот уже надо смотреть отдельно работы, но идея та же самая, что пишется правильная версия
[01:09:22.260 --> 01:09:28.780]  с правильными весами вот этого стахастического градиентного спуска и получается аналогичный
[01:09:28.780 --> 01:09:33.700]  результат без всяких имперических средней, то есть без отрешиваний вспомогательных задач,
[01:09:33.700 --> 01:09:41.100]  можно-то добиться того же результата. Ну и в общем-то, идя дальше в этом направлении,
[01:09:41.100 --> 01:09:48.460]  мы можем и даже должны заметить следующее, что в постановке задач, о которой я говорил,
[01:09:48.460 --> 01:10:04.060]  вообще говоря не все было сделано, как бы сказать, ну что ли, не все возможности исчерпаны в плане
[01:10:04.060 --> 01:10:09.980]  постановки, то есть мы говорили про задачу, когда х со звездой ну просто неизвестен, а что если на
[01:10:09.980 --> 01:10:17.100]  х со звездой можно задать какое-то априорное распределение p от x, то есть если у вас есть
[01:10:17.100 --> 01:10:25.540]  априорное распределение на p от x, то вы можете на самом деле задаться вопросом, а какая будет
[01:10:25.540 --> 01:10:32.460]  тогда наилучшая оценка, которую можно получить, потому что это уже другая постановка, то есть вы
[01:10:32.460 --> 01:10:37.580]  как бы сэмплите, давайте мы прям немножко скакнем и рассмотрим вот какую-нибудь такую схему, да,
[01:10:37.960 --> 01:10:45.540]  то есть если например вы знаете, что ну в свою очередь, ну чуть усложнил модель, что х со звездой
[01:10:45.540 --> 01:10:51.060]  генерируется из какого-то распределения, а потом к с и к генерируется из какого-то рассвет.
[01:10:51.060 --> 01:10:57.480]  То есть х со звездой случайной, это ка случайно, то как бы вся случайность, которая есть в модели,
[01:10:57.480 --> 01:11:02.220]  она складывается из того, что это сначала разыграли, а потом начинают из этого сэмплить,
[01:11:02.220 --> 01:11:07.500]  естественно возникает вопрос а как тогда оценивать икса звездой как оценивать икса звездой если у вас
[01:11:07.500 --> 01:11:13.420]  есть принципе априорная информация из какого закона оно распределено вы не знаете как оно
[01:11:13.420 --> 01:11:18.300]  реализовалось но вы знаете уже какие-то последствия которые будут в результате измерений того икса
[01:11:18.300 --> 01:11:23.300]  звезды который реализовалась и вот здесь таких постановках естественно помогает ну во-первых
[01:11:23.300 --> 01:11:31.180]  prior информация о том что есть икса звездой ну и естественный такой принцип что вы как и раньше
[01:11:31.220 --> 01:11:37.080]  выписываете вероятность некого события что выпадет соответственно икса звездой только теперь
[01:11:37.080 --> 01:11:42.500]  надо поэтому интегрировать вот разные значения может выпасть зет ну и что при этом z и релизуется
[01:11:42.500 --> 01:11:48.100]  ксикат и то есть это как бы у это это вот условная вероятность а это формула полной вероятности
[01:11:48.100 --> 01:11:52.180]  я сейчас просто проинтегрировал а меня была к� heat hello вероятность но я хочу получить
[01:11:52.180 --> 01:11:58.940]  оценку я хочу получить оценку этого параметра z который является как бы наиболее вероятным
[01:11:58.940 --> 01:12:07.920]  Поэтому я как бы беру вот эту плотность, плотность вот это вот как называется апостериорная плотность распределения,
[01:12:07.920 --> 01:12:15.300]  то есть вероятность того, что выпал конкретный х, вероятность того, что выпал конкретный х, — это есть вот что такое.
[01:12:15.300 --> 01:12:22.480]  И эту вероятность я как бы оценку пишу, что это мат ожидания по вот этой аппостериорной вероятности,
[01:12:22.480 --> 01:12:31.480]  того, что выпал конкретный х, имея в виду, что априорная вот такая, а это вот условная вероятность, что при заданном y выпадет ксикатой.
[01:12:31.480 --> 01:12:35.480]  Ну и вот это будет как бы формула байса, по-моему, называется.
[01:12:35.480 --> 01:12:40.480]  Вот вы можете воспользоваться формулой байса и получить так называемую байсовскую оценку.
[01:12:40.480 --> 01:12:47.480]  И вот на эту байсовскую оценку вы можете написать аналогичные неравенства, ну прям вот с точностью до всяких ипы,
[01:12:47.480 --> 01:12:50.480]  так же вот с этими матрицами, которые вы писали раньше.
[01:12:50.480 --> 01:12:55.480]  И тоже с той же концентрацией, ну то есть с некоторыми оговорками, о которой я сейчас не хочу говорить.
[01:12:55.480 --> 01:13:09.480]  Но в целом, в общем, важно понимать, что сейчас мы говорим уже, на самом деле, о том, что мат ожидания берется полная.
[01:13:09.480 --> 01:13:15.480]  То есть мат ожидания берется как по х, потому что оно случайно, так и по ксикате.
[01:13:15.480 --> 01:13:18.480]  То есть это немножко другая постановка. Там-то х был параметром.
[01:13:18.480 --> 01:13:28.480]  Но при этом, когда мы говорим о том, что х нормально распределено относить, ну то есть вот такого типа результат, он условный.
[01:13:28.480 --> 01:13:37.480]  То есть это conditional распределение. То есть вот эта байсовская оценка при условии, что мы заморозили х со звездой, она будет иметь вот такое распределение.
[01:13:37.480 --> 01:13:41.480]  Это уже как бы результат, надо понимать, на условное распределение.
[01:13:41.480 --> 01:13:45.480]  Условное приз в том, что мы заморозили реализацию х со звездой.
[01:13:45.480 --> 01:13:47.480]  Но как бы эффект тот же самый.
[01:13:47.480 --> 01:13:53.480]  Но тут еще есть всякие варианты более, так сказать, тонкие, типа теориям Берштейна фон Мизеса.
[01:13:53.480 --> 01:14:04.480]  Одно они заключается в том, что вот это вот пастериорное распределение асимпатически нормально и сосредоточено около, ну сконцентрировано около maximum likelihood estimation.
[01:14:04.480 --> 01:14:08.480]  Вот такой коверационный мат, но это уже какие-то отдельные вопросы там.
[01:14:08.480 --> 01:14:15.480]  То есть для наших целей вполне достаточно того, что написано вот в основной теореме без этих тонкостей.
[01:14:15.480 --> 01:14:25.480]  Ну и замечу, что помимо всяких байсовских оценок, совершенно естественно, если мы берем maximum likelihood estimation, то вот так.
[01:14:25.480 --> 01:14:36.480]  А если мы берем, добавляем в maximum likelihood estimation P от X и формулируем задачу не максимум правдоподобия, а максимум полной вероятности, то надо prior учесть.
[01:14:36.480 --> 01:14:38.480]  И тогда оценка получается вот такой.
[01:14:38.480 --> 01:14:42.480]  Вообще говоря, это не то же самое, что байсовская оценка.
[01:14:42.480 --> 01:14:46.480]  Это maximum posterior estimation, то есть байсовская оценка.
[01:14:46.480 --> 01:14:50.480]  Это как бы average posterior, а это maximum posterior.
[01:14:50.480 --> 01:14:54.480]  То есть это не то же самое, но в ряде постановок они просто совпадают.
[01:14:54.480 --> 01:14:57.480]  И они асимпатически одинаково себя ведут.
[01:14:57.480 --> 01:15:01.480]  То есть в общем-то это дело вкуса, с чем тут лучше вам работать.
[01:15:02.480 --> 01:15:10.480]  Но чисто аналогия с maximum likelihood, она конечно больше вот здесь прослеживается, чем когда я пишу какую-то такую формулу.
[01:15:10.480 --> 01:15:13.480]  Это, наверное, менее прослеживается.
[01:15:13.480 --> 01:15:15.480]  Вот здесь она больше прослеживается.
[01:15:15.480 --> 01:15:17.480]  Давайте конкретно пример разберем.
[01:15:17.480 --> 01:15:20.480]  Rich regression и LASSO.
[01:15:20.480 --> 01:15:23.480]  Ну пускай, как и раньше, у нас неизвестный параметр.
[01:15:23.480 --> 01:15:25.480]  Вектор теперь уже X звездой.
[01:15:25.480 --> 01:15:27.480]  Есть шум.
[01:15:27.480 --> 01:15:38.480]  И пускай каждая вот эта ксика, скалярная величина, она есть, значит, ака, некая матрица, на, значит, X звездой и зашумляется.
[01:15:38.480 --> 01:15:41.480]  Вот X звездой тоже случайно как-то, вообще говоря, разыгрывается.
[01:15:41.480 --> 01:15:44.480]  Ну, вообще говоря, это зависит от схемы эксперимента.
[01:15:44.480 --> 01:15:46.480]  Вот, матрица известна.
[01:15:46.480 --> 01:15:48.480]  Цель оценить X звездой.
[01:15:48.480 --> 01:15:52.480]  Значит, к чему приводит просто maximum likelihood estimation.
[01:15:53.480 --> 01:15:58.480]  Если мы говорим о maximum likelihood estimation, то X звездой это не есть случайно разыгранная величина.
[01:15:58.480 --> 01:16:01.480]  У нас по постановке задачи мы считаем, что оно задано.
[01:16:01.480 --> 01:16:03.480]  Все. Мы не знаем, как оно задано.
[01:16:03.480 --> 01:16:05.480]  Мы не вносим сюда случайности.
[01:16:05.480 --> 01:16:09.480]  Поэтому maximum likelihood estimation это просто метод наименьших квадратов.
[01:16:09.480 --> 01:16:11.480]  То есть AX минус X.
[01:16:11.480 --> 01:16:13.480]  X это вектор вот из этих составлений.
[01:16:13.480 --> 01:16:18.480]  AX это вот, по сути, значит, то, что получится, если вот эти строки записать матрично.
[01:16:18.480 --> 01:16:21.480]  То есть взять вот такую матрицу и записать это векторно.
[01:16:21.480 --> 01:16:23.480]  Ну, вот это будет maximum likelihood estimation.
[01:16:23.480 --> 01:16:28.480]  Это будет то, что дает вот тот способ, который я говорил.
[01:16:28.480 --> 01:16:33.480]  Выписывание правдоподобия и поиск параметра, который его минимизирует.
[01:16:34.480 --> 01:16:42.480]  Bias'овская оценка в данном случае совпадает с оценкой maximum posterior estimation.
[01:16:42.480 --> 01:16:43.480]  То есть они одинаковые.
[01:16:43.480 --> 01:16:45.480]  Они будут иметь вот такой вид.
[01:16:45.480 --> 01:16:47.480]  То есть bias приводит к регуляризации.
[01:16:47.480 --> 01:16:49.480]  Вот что такое bias.
[01:16:49.480 --> 01:16:52.480]  Bias' это регуляризация в методе наименьших квадратов.
[01:16:52.480 --> 01:16:54.480]  Замечательно, правда?
[01:16:54.480 --> 01:16:55.480]  То есть откуда берется bias?
[01:16:55.480 --> 01:16:56.480]  Это просто prior.
[01:16:56.480 --> 01:17:00.480]  Масштаб bias'а, то есть если вы как бы будете здесь смотреть,
[01:17:00.480 --> 01:17:06.480]  то дисперсия bias'а, чем она меньше, то есть если у вас прям точная информация,
[01:17:06.480 --> 01:17:08.480]  то это прям бесконечность будет стремиться к бесконечности,
[01:17:08.480 --> 01:17:10.480]  значит она prior у вас информативен.
[01:17:10.480 --> 01:17:15.480]  Если sigma бесконечности стремится, то есть по сути режим переходит в этот,
[01:17:15.480 --> 01:17:17.480]  то есть не информативный prior,
[01:17:17.480 --> 01:17:20.480]  то вы как бы имеете, что это вымывается, это почти ноль,
[01:17:20.480 --> 01:17:22.480]  потому что в знаменателе бесконечность.
[01:17:22.480 --> 01:17:25.480]  Бесконечность знаменателе это вымывается и регуляризатор исчезает.
[01:17:25.480 --> 01:17:26.480]  Очень естественно.
[01:17:26.480 --> 01:17:28.480]  То есть понятно, что есть предельный переход.
[01:17:28.480 --> 01:17:32.480]  Но помимо вот этих ситуаций мы же можем рассмотреть,
[01:17:32.480 --> 01:17:35.480]  да, это все называется rich regression,
[01:17:35.480 --> 01:17:45.480]  но мы можем рассмотреть вот тогда ситуацию такую,
[01:17:45.480 --> 01:17:51.480]  что если у нас оценка L1, то тогда LASSO,
[01:17:51.480 --> 01:17:54.480]  который будет соответствовать этому,
[01:17:54.480 --> 01:17:56.480]  ну то есть как бы объяснить сейчас,
[01:17:56.480 --> 01:18:01.480]  да, то есть если у нас плотность prior не нормальный,
[01:18:01.480 --> 01:18:04.480]  не такой, а экспоненциальный,
[01:18:04.480 --> 01:18:05.480]  лопласовский он называется,
[01:18:05.480 --> 01:18:08.480]  то тогда вот то, что сюда вносится,
[01:18:08.480 --> 01:18:10.480]  во всяком случае в подходе LASSO,
[01:18:10.480 --> 01:18:13.480]  он будет вот такой, то есть это другая задача.
[01:18:13.480 --> 01:18:17.480]  Это задача, которая в оптимизации называется LASSO,
[01:18:17.480 --> 01:18:19.480]  и она как бы отвечает разреженности,
[01:18:19.480 --> 01:18:21.480]  выделению основных признаков.
[01:18:21.480 --> 01:18:24.480]  И эта задача порождается тоже как байсовская,
[01:18:24.480 --> 01:18:26.480]  но просто с другим prior.
[01:18:26.480 --> 01:18:28.480]  Ну и тут дальше можно замечать,
[01:18:28.480 --> 01:18:32.480]  что все эти штуки вырождаются к чему-то такому,
[01:18:32.480 --> 01:18:36.480]  то есть если, например, лямбда стремится к нулю,
[01:18:36.480 --> 01:18:39.480]  то это дело стремится в свою очередь к нулю,
[01:18:39.480 --> 01:18:41.480]  но лямбда стремится к нулю означает,
[01:18:41.480 --> 01:18:43.480]  что у нас prior неинформативен,
[01:18:43.480 --> 01:18:45.480]  то есть вы фактически имеете равномерное распределение,
[01:18:45.480 --> 01:18:49.480]  но оно как бы и сводится тогда к тому случаю,
[01:18:49.480 --> 01:18:51.480]  когда нет никакой информации об X звездой.
[01:18:51.480 --> 01:18:54.480]  То есть в общем мысль здесь очень простая.
[01:18:54.480 --> 01:18:58.480]  Prior это регуляризатор в этих во всех подходах,
[01:18:58.480 --> 01:19:00.480]  связанных со стох-оптимизацией,
[01:19:00.480 --> 01:19:03.480]  но в данном случае мы на это смотрим не с точки зрения стох-оптимизации,
[01:19:03.480 --> 01:19:10.480]  а с точки зрения мы смотрим имперического риска,
[01:19:10.480 --> 01:19:14.480]  то есть это все отвечает минимизации имперического риска.
[01:19:14.480 --> 01:19:20.480]  Ну и вот яркий пример, который на самом деле уже совсем неочевиден,
[01:19:20.480 --> 01:19:23.480]  если до этого, я думаю, вы где-то сталкивались с тем, что я рассказывал,
[01:19:23.480 --> 01:19:27.480]  то теперь у нас SVM, support vector machine,
[01:19:27.480 --> 01:19:30.480]  и здесь мы имеем вот такую постановку,
[01:19:30.480 --> 01:19:32.480]  что есть какая-то разделяющая гиперплоскость,
[01:19:32.480 --> 01:19:38.480]  и у нас не собственное распределение вероятности есть,
[01:19:38.480 --> 01:19:40.480]  то есть данные сэмпливаются таким образом,
[01:19:40.480 --> 01:19:43.480]  что здесь они имеют плотность вероятности 1,
[01:19:43.480 --> 01:19:45.480]  но поскольку это бесконечное множество,
[01:19:45.480 --> 01:19:48.480]  то задать равномерную плотность на некомпактном множестве,
[01:19:48.480 --> 01:19:50.480]  это непонятно, что такое, оно не собственное,
[01:19:50.480 --> 01:19:54.480]  а вот здесь оно как бы экспоненциально убывающее,
[01:19:54.480 --> 01:19:59.480]  то есть вероятность того, что мы где-то себя здесь найдем,
[01:19:59.480 --> 01:20:03.480]  она экспоненциально убывает по мере отдаления от этой прямой,
[01:20:03.480 --> 01:20:07.480]  то есть фактически, если лейблы правильно там ставить,
[01:20:07.480 --> 01:20:10.480]  то мы должны попадать вот сюда,
[01:20:10.480 --> 01:20:15.480]  но с некоторой маленькой вероятностью точки сэмпливаются и здесь,
[01:20:15.480 --> 01:20:17.480]  и чем дальше, тем меньше вероятность,
[01:20:17.480 --> 01:20:21.480]  ну и дальше, веря в то, что реально точки сэмпливаются вот так,
[01:20:21.480 --> 01:20:25.480]  что существует, короче говоря, нормаль ak,
[01:20:25.480 --> 01:20:28.480]  то есть существует какой-то вектор ak,
[01:20:29.480 --> 01:20:33.480]  и существует вот метка y, которая классифицирует,
[01:20:33.480 --> 01:20:36.480]  что данные сэмплируются по такому образу,
[01:20:36.480 --> 01:20:40.480]  то есть мы говорим, что у нас там мы хотим сэмплировать,
[01:20:40.480 --> 01:20:43.480]  то есть какого типа данные,
[01:20:43.480 --> 01:20:47.480]  ну и в зависимости от метки y, либо та картинка, которую я нарисовал,
[01:20:47.480 --> 01:20:49.480]  либо с точностью до наоборот симметричная,
[01:20:49.480 --> 01:20:53.480]  то есть это соответствует метке y единица,
[01:20:53.480 --> 01:20:55.480]  вот такая плотность распределения,
[01:20:55.480 --> 01:20:58.480]  если у нас y равняется минус единица,
[01:20:58.480 --> 01:21:00.480]  вот у нас два значения она приобретает,
[01:21:00.480 --> 01:21:01.480]  то плотность будет другая,
[01:21:01.480 --> 01:21:03.480]  то есть плотность распределения зависит от y,
[01:21:03.480 --> 01:21:05.480]  ну естественно она зависит и от ak,
[01:21:05.480 --> 01:21:08.480]  ak это нормаль, которая задает эту плоскость,
[01:21:08.480 --> 01:21:10.480]  таким образом у нас есть два параметра,
[01:21:10.480 --> 01:21:15.480]  точнее не так, я неправильно говорю,
[01:21:15.480 --> 01:21:17.480]  прошу прощения, ak это данные,
[01:21:17.480 --> 01:21:20.480]  ak это данные, а y это метки, это все данные,
[01:21:20.480 --> 01:21:22.480]  а и нормаль задается x,
[01:21:22.480 --> 01:21:24.480]  то есть у нас есть x, который задает нормаль,
[01:21:24.480 --> 01:21:26.480]  вот этот x нам и надо определить,
[01:21:26.480 --> 01:21:28.480]  то есть x задает нормаль,
[01:21:28.480 --> 01:21:30.480]  и вот это и есть параметры,
[01:21:30.480 --> 01:21:31.480]  а все остальное это данные,
[01:21:31.480 --> 01:21:33.480]  и здесь написано плотность,
[01:21:33.480 --> 01:21:36.480]  какая плотность будет у данных в зависимости от их параметров,
[01:21:36.480 --> 01:21:40.480]  вот метки y и вот вектора ak,
[01:21:40.480 --> 01:21:42.480]  ну и если у вас такая плотность,
[01:21:42.480 --> 01:21:44.480]  она вполне естественная для классификации,
[01:21:44.480 --> 01:21:46.480]  то оценка, которая map,
[01:21:46.480 --> 01:21:48.480]  которая максимума posterior,
[01:21:48.480 --> 01:21:50.480]  она как раз и есть то, что в SVM,
[01:21:50.480 --> 01:21:52.480]  но регуляризованном SVM,
[01:21:52.480 --> 01:21:54.480]  вот эта как бы часть,
[01:21:54.480 --> 01:21:58.480]  то есть вот это сочетание приводит к такой штуке,
[01:21:58.480 --> 01:22:01.480]  ну а prior, который гауссовский,
[01:22:01.480 --> 01:22:03.480]  если мы его вводим,
[01:22:03.480 --> 01:22:06.480]  он приводит к такой регуляризации,
[01:22:06.480 --> 01:22:08.480]  ну и если prior не информативен,
[01:22:08.480 --> 01:22:09.480]  это бесконечная дисперсия,
[01:22:09.480 --> 01:22:11.480]  он вымывается, его нет,
[01:22:11.480 --> 01:22:13.480]  и вы получаете просто задачу SVM,
[01:22:13.480 --> 01:22:14.480]  support vector machine,
[01:22:14.480 --> 01:22:18.480]  то есть это задача минимизации вот такой негладкой функции,
[01:22:18.480 --> 01:22:19.480]  но выпуклой,
[01:22:19.480 --> 01:22:21.480]  потому что это максимум из нуля,
[01:22:21.480 --> 01:22:23.480]  и какой-то линейной функции,
[01:22:23.480 --> 01:22:24.480]  если максимум берется,
[01:22:24.480 --> 01:22:26.480]  то что это означает?
[01:22:26.480 --> 01:22:27.480]  Вот это максимум,
[01:22:27.480 --> 01:22:29.480]  вот максимум это выпуклая функция,
[01:22:29.480 --> 01:22:30.480]  вот это ноль,
[01:22:30.480 --> 01:22:32.480]  а вот это вот опять линейная функция,
[01:22:32.480 --> 01:22:34.480]  вы можете линейную функцию провести вот так,
[01:22:34.480 --> 01:22:35.480]  это неважно,
[01:22:35.480 --> 01:22:38.480]  тоже максимум будет выпуклая функция.
[01:22:38.480 --> 01:22:43.480]  Ну теперь мы переходим,
[01:22:43.480 --> 01:22:45.480]  я скоро уже заканчиваю вот эту часть,
[01:22:45.480 --> 01:22:47.480]  она компактная,
[01:22:47.480 --> 01:22:48.480]  мы переходим к тому,
[01:22:48.480 --> 01:22:52.480]  что собственно мы сейчас делали,
[01:22:52.480 --> 01:22:55.480]  мы считали f от x к си
[01:22:55.480 --> 01:22:57.480]  минус логариф mp от x,
[01:22:57.480 --> 01:22:58.480]  где p это плотность,
[01:22:58.480 --> 01:23:00.480]  и исходя из этой философии,
[01:23:00.480 --> 01:23:03.480]  мы там брали вот такую разницу,
[01:23:03.480 --> 01:23:05.480]  что-то там смотрели,
[01:23:05.480 --> 01:23:07.480]  но, прошу прощения,
[01:23:07.480 --> 01:23:09.480]  а зачем, зачем все это,
[01:23:09.480 --> 01:23:11.480]  если можно без этого,
[01:23:11.480 --> 01:23:12.480]  то есть вообще говоря,
[01:23:12.480 --> 01:23:14.480]  не вводить никакие плотности,
[01:23:14.480 --> 01:23:15.480]  и сразу написать,
[01:23:15.480 --> 01:23:17.480]  что у нас функция регрета вот такая,
[01:23:17.480 --> 01:23:19.480]  и в этом есть смысл,
[01:23:19.480 --> 01:23:20.480]  мы вообще не знаем ничего
[01:23:20.480 --> 01:23:22.480]  про параметрическую зависимость,
[01:23:22.480 --> 01:23:24.480]  тут же как бы предполагается,
[01:23:24.480 --> 01:23:26.480]  что есть какой-то параметрический закон,
[01:23:26.480 --> 01:23:28.480]  и все это статистика.
[01:23:28.480 --> 01:23:29.480]  Но машина обучения это не любит,
[01:23:29.480 --> 01:23:31.480]  и оно и задается,
[01:23:31.480 --> 01:23:32.480]  машина обучения задается вопросом,
[01:23:32.480 --> 01:23:33.480]  зачем все это,
[01:23:33.480 --> 01:23:35.480]  зачем все эти параметры,
[01:23:35.480 --> 01:23:36.480]  зачем все эти гаусяны,
[01:23:36.480 --> 01:23:37.480]  зачем все это,
[01:23:37.480 --> 01:23:38.480]  если я просто могу,
[01:23:38.480 --> 01:23:39.480]  с самого начала сказать,
[01:23:39.480 --> 01:23:41.480]  что у меня функция риска,
[01:23:41.480 --> 01:23:44.480]  какая-то функция потерь,
[01:23:44.480 --> 01:23:46.480]  это и есть квадратичная не вязка,
[01:23:46.480 --> 01:23:55.480]  Просто я зашиваю специфику задачи не в закон распределения шума, а в штраф, который мне интересен.
[01:23:55.480 --> 01:23:58.480]  Я ничего не знаю о законе распределения кси.
[01:23:58.480 --> 01:24:03.480]  Тем не менее, я также могу ставить задачу. Я имею в виду задачу мат ожидания.
[01:24:03.480 --> 01:24:10.480]  Но теперь мат ожидания может браться по ИА в том числе, потому что я не знаю, как эти данные генерируются.
[01:24:10.480 --> 01:24:13.480]  И мне надо определить тот параметр, который сидит в этой модели.
[01:24:13.480 --> 01:24:17.480]  Но я вполне допускаю, что и А может быть случайно разыгран.
[01:24:17.480 --> 01:24:21.480]  Если в предыдущей модели мне надо было, чтобы А было фиксировано,
[01:24:21.480 --> 01:24:25.480]  сейчас я могу считать, что у меня вот здесь мат ожидания берется по ИА.
[01:24:25.480 --> 01:24:31.480]  И то же самое. Я также могу брать офлайн подход, то есть брать империческую штуку.
[01:24:31.480 --> 01:24:34.480]  Вот эти сэмплы. Будет получаться вот такая задача.
[01:24:34.480 --> 01:24:44.480]  Для классификации, для hinge loss, которая здесь фигурирует, я получу другую задачу,
[01:24:44.480 --> 01:24:49.480]  которая там будет. Вот такого типа она будет.
[01:24:49.480 --> 01:24:53.480]  Ее также можно имперически переписать. Я уже не буду это делать.
[01:24:53.480 --> 01:24:56.480]  И вам надо отрешивать вот такие задачи.
[01:24:57.480 --> 01:25:05.480]  Дальше, собственно, если мы так смотрим на проблематику, то все.
[01:25:05.480 --> 01:25:10.480]  У нас ушла какая-то специфика с законами распределения.
[01:25:10.480 --> 01:25:13.480]  И она нам на самом деле не нужна была.
[01:25:13.480 --> 01:25:17.480]  Потому что где мы здесь пользовались каким-то конкретным законом распределения?
[01:25:17.480 --> 01:25:19.480]  Давайте вернемся к самому началу.
[01:25:19.480 --> 01:25:24.480]  Вот мы задали функцию f от x равняется epsilon-x в квадрате.
[01:25:24.480 --> 01:25:27.480]  В принципе, мы могли сразу сказать, что это и есть функция потерь.
[01:25:27.480 --> 01:25:30.480]  Дальше мы сказали, что нам нужен 100 градиент.
[01:25:30.480 --> 01:25:33.480]  Пожалуйста, кто же мешает его посчитать? Вот он считается.
[01:25:33.480 --> 01:25:37.480]  И нам не нужно вот это знать.
[01:25:37.480 --> 01:25:41.480]  То есть нам все, что нужно, это чтобы оценивать скорость сходимости таких процедур.
[01:25:41.480 --> 01:25:43.480]  Но это какие-то свойства более общие.
[01:25:43.480 --> 01:25:46.480]  То есть чему равняется дисперсия, чем она ограничена.
[01:25:46.480 --> 01:25:48.480]  Но нам не нужен конкретный закон.
[01:25:48.480 --> 01:25:51.480]  Мы эту параметрическую модель, что именно нормальный закон,
[01:25:51.480 --> 01:25:53.480]  поэтому построили квадрат.
[01:25:53.480 --> 01:25:58.480]  Но могли про это не знать и просто изначально рассматривать вот такую задачу.
[01:25:58.480 --> 01:26:01.480]  И когда мы делали с такой моделью,
[01:26:01.480 --> 01:26:04.480]  то этот функционал был однозначно связан с тем,
[01:26:04.480 --> 01:26:08.480]  что мы хотим, чтобы это была оценка не смещена.
[01:26:08.480 --> 01:26:11.480]  В общем, вот это дело сыграло свою роль.
[01:26:11.480 --> 01:26:14.480]  Вот то, что я здесь рассказывал.
[01:26:14.480 --> 01:26:21.480]  Но это потому, что мы имели параметрическую вероятностную модель.
[01:26:21.480 --> 01:26:24.480]  А если ее нет, то мы можем просто говорить,
[01:26:24.480 --> 01:26:27.480]  что нам в целом интересна вот такая задача стокоптимизации,
[01:26:27.480 --> 01:26:33.480]  как вот здесь это вылезает, вот подобно тому,
[01:26:33.480 --> 01:26:36.480]  как это вылезает, допустим, вот здесь.
[01:26:36.480 --> 01:26:38.480]  То есть мы не знаем закон распределения,
[01:26:38.480 --> 01:26:42.480]  мы не знаем вообще ничего про то, откуда Y сэмплится.
[01:26:42.480 --> 01:26:44.480]  Но нас это вообще не смущает.
[01:26:44.480 --> 01:26:46.480]  Нас это не смущает.
[01:26:46.480 --> 01:26:49.480]  Мы продолжаем рассматривать эту задачу как задачу стокоптимизации.
[01:26:49.480 --> 01:26:52.480]  Мы вводим шары в соответствующих нормах.
[01:26:52.480 --> 01:26:56.480]  Rp – это ограничение на параметры.
[01:26:56.480 --> 01:26:59.480]  То есть rp – это радиус шара в p-норме.
[01:26:59.480 --> 01:27:02.480]  То есть, если, например, мы работаем с шаром в 1-норме,
[01:27:02.480 --> 01:27:07.480]  то это будет, так сказать, какое-то такое множество.
[01:27:07.480 --> 01:27:10.480]  Ну и теперь мы, в общем, предполагаем какие-то свойства
[01:27:10.480 --> 01:27:15.480]  о классе функций, чтобы получить какие-то гарантированные результаты.
[01:27:15.480 --> 01:27:20.480]  Значит, это свойства выпуклости, липшцевости, равномерной эпокси.
[01:27:20.480 --> 01:27:24.480]  Иногда будет использоваться generalized linear structure.
[01:27:24.480 --> 01:27:27.480]  Это вот как раз в SVM, но это вообще везде пока.
[01:27:27.480 --> 01:27:30.480]  То, что мы смотрели, это везде generalized linear structure.
[01:27:30.480 --> 01:27:32.480]  Вот она, generalized linear structure.
[01:27:32.480 --> 01:27:34.480]  Вот она, generalized linear structure.
[01:27:34.480 --> 01:27:37.480]  То есть что-то, что, так сказать, афинно,
[01:27:37.480 --> 01:27:39.480]  вот такое скалярное произведение входит,
[01:27:39.480 --> 01:27:42.480]  и некоторая такая функция от скалярного произведения.
[01:27:42.480 --> 01:27:45.480]  То есть это как бы позволяет размерность снизу снижать.
[01:27:45.480 --> 01:27:49.480]  Ну и дальше мы говорим, что можно ввести функцию эмпирическую,
[01:27:49.480 --> 01:27:52.480]  ну и определить минимум вот этой эмпирической функции.
[01:27:52.480 --> 01:27:55.480]  Все, это и есть как бы подход offline.
[01:27:55.480 --> 01:27:58.480]  Это off-line подход.
[01:27:58.480 --> 01:28:04.480]  Ну и, собственно, теорема, которую можно доказать,
[01:28:04.480 --> 01:28:09.480]  она для случая, когда у нас, значит, мы предполагаем
[01:28:09.480 --> 01:28:13.480]  одновременно выполненными вот эти два соотношения.
[01:28:13.480 --> 01:28:16.480]  То есть есть generalized linear structure
[01:28:16.480 --> 01:28:19.480]  и есть выпуклость и липшцевость.
[01:28:19.480 --> 01:28:23.480]  И в этом случае мы получаем, что имеет место равномерная аппроксимация
[01:28:23.480 --> 01:28:27.480]  эмпирической функции, исходной функции.
[01:28:27.480 --> 01:28:29.480]  И порядок аппроксимации один на коре низен.
[01:28:29.480 --> 01:28:31.480]  Повторю, равномерная.
[01:28:31.480 --> 01:28:34.480]  Что, вообще говоря, довольно круто.
[01:28:34.480 --> 01:28:38.480]  Ну и из равномерной аппроксимации мы можем получить вот, так сказать,
[01:28:38.480 --> 01:28:43.480]  два пресеста, что сначала, значит, нам надо, значит,
[01:28:43.480 --> 01:28:48.480]  просто, ну, то есть чтобы записать, чему равняется эта разность,
[01:28:48.480 --> 01:28:50.480]  мы, значит, пишем что?
[01:28:50.480 --> 01:28:55.480]  Что, значит, это есть...
[01:28:55.480 --> 01:29:03.480]  Сейчас, f от x минус f от x со звездой.
[01:29:03.480 --> 01:29:05.480]  Значит, это что такое?
[01:29:05.480 --> 01:29:09.480]  Это меньше либо равняется, чем f от x с чертой.
[01:29:09.480 --> 01:29:15.480]  Это оценка минус f с чертой от x с крышкой n.
[01:29:15.480 --> 01:29:18.480]  Это минимизатор f от x с чертой.
[01:29:18.480 --> 01:29:30.480]  Плюс, значит, f от чертой x с крышкой n минус f от x с крышкой n.
[01:29:30.480 --> 01:29:33.480]  Вот такая вот штука.
[01:29:33.480 --> 01:29:35.480]  Ну и плюс...
[01:29:35.480 --> 01:29:38.480]  Плюс, значит...
[01:29:38.480 --> 01:29:40.480]  Сейчас, минутку.
[01:29:40.480 --> 01:29:42.480]  Минутку, минутку.
[01:29:42.480 --> 01:29:48.480]  Плюс f от f от...
[01:29:54.480 --> 01:29:56.480]  Сейчас, сейчас, сейчас.
[01:29:56.480 --> 01:29:58.480]  Значит, это как-то тривиально делается.
[01:29:58.480 --> 01:30:00.480]  Я что-то немножко запутался.
[01:30:00.480 --> 01:30:01.480]  Сейчас мы это сделаем.
[01:30:01.480 --> 01:30:03.480]  То есть два раза надо вот этим вот воспользоваться.
[01:30:03.480 --> 01:30:05.480]  Несколько раз, который уйдет вот сюда.
[01:30:05.480 --> 01:30:08.480]  И, значит, надо представить эту разность через...
[01:30:08.480 --> 01:30:11.480]  Значит, тут будет тогда f от x со звездой.
[01:30:11.480 --> 01:30:15.480]  Здесь должно быть f от x.
[01:30:15.480 --> 01:30:17.480]  Плюс...
[01:30:17.480 --> 01:30:23.480]  Так, это будет f от x минус f от xn.
[01:30:23.480 --> 01:30:26.480]  Плюс f от...
[01:30:26.480 --> 01:30:28.480]  Минутку.
[01:30:28.480 --> 01:30:30.480]  Минутку.
[01:30:30.480 --> 01:30:32.480]  f от...
[01:30:34.480 --> 01:30:36.480]  Так, что-то я туплю.
[01:30:36.480 --> 01:30:39.480]  Надо точно, чтобы здесь было минус f от x со звездой,
[01:30:39.480 --> 01:30:41.480]  потому что она здесь присутствует.
[01:30:41.480 --> 01:30:46.480]  И, значит, надо точно, чтобы присутствовало f от x.
[01:30:46.480 --> 01:30:50.480]  Вот, значит, f от x.
[01:30:55.480 --> 01:30:57.480]  Так, f от xn.
[01:30:57.480 --> 01:30:58.480]  Сейчас.
[01:30:58.480 --> 01:31:03.480]  То есть надо собрать оценку, которая здесь написана,
[01:31:03.480 --> 01:31:05.480]  из вот этой штуки.
[01:31:05.480 --> 01:31:07.480]  И, соответственно, вот такого типа штуки,
[01:31:07.480 --> 01:31:09.480]  где правильно подставить аргументы.
[01:31:09.480 --> 01:31:11.480]  Значит, схема понятна.
[01:31:11.480 --> 01:31:14.480]  Просто надо сейчас эти аргументы подобрать.
[01:31:14.480 --> 01:31:16.480]  Минутку.
[01:31:16.480 --> 01:31:18.480]  Минутку.
[01:31:18.480 --> 01:31:22.480]  Значит, f от xn.
[01:31:22.480 --> 01:31:30.480]  Дальше мы, значит, делаем f от xn с крышкой f от f.
[01:31:30.480 --> 01:31:40.480]  С крышкой f от x со звездой, наверное, надо сделать вот эту штуку f от f от x.
[01:31:44.480 --> 01:31:45.480]  Минимум.
[01:31:45.480 --> 01:31:47.480]  А здесь будет...
[01:31:49.480 --> 01:31:51.480]  Сейчас, сейчас, сейчас, сейчас.
[01:31:53.480 --> 01:31:56.480]  Так, наверное, вот так проще сделать будет.
[01:31:56.480 --> 01:32:03.480]  Значит, это будет минус f с чертой от x со звездой.
[01:32:05.480 --> 01:32:10.480]  Плюс f с чертой от x со звездой, минус f от x.
[01:32:10.480 --> 01:32:11.480]  Вот так вот.
[01:32:11.480 --> 01:32:12.480]  Да?
[01:32:12.480 --> 01:32:17.480]  Вот эта штука, она меньше либо равняется f с чертой от x
[01:32:17.480 --> 01:32:21.480]  минус f с чертой от x с крышкой n,
[01:32:21.480 --> 01:32:24.480]  потому что x со звездой это просто какая-то точка,
[01:32:24.480 --> 01:32:27.480]  а x с крышкой n это минимизатор f с чертой.
[01:32:27.480 --> 01:32:28.480]  Поэтому это неравенство верно.
[01:32:28.480 --> 01:32:30.480]  А вот это неравенство...
[01:32:30.480 --> 01:32:33.480]  Значит, прошу прощения, здесь x со звездой, конечно.
[01:32:33.480 --> 01:32:35.480]  Значит...
[01:32:35.480 --> 01:32:37.480]  x со звездой, да.
[01:32:37.480 --> 01:32:38.480]  x со звездой.
[01:32:38.480 --> 01:32:42.480]  Да, ну и здесь, конечно, я тоже забыл написать f от x
[01:32:42.480 --> 01:32:44.480]  минус f от x с чертой от x.
[01:32:44.480 --> 01:32:45.480]  Вот, да.
[01:32:45.480 --> 01:32:47.480]  То есть теперь все сходится.
[01:32:47.480 --> 01:32:50.480]  То есть, смотрите, мы берем первую разницу.
[01:32:51.480 --> 01:32:53.480]  Вот f от x соответствует f от x.
[01:32:53.480 --> 01:32:56.480]  Мы берем вот эту штуку, она сокращается вот с этой штукой.
[01:32:56.480 --> 01:33:00.480]  Берем вот эту штуку, она сокращается вот с этой штукой.
[01:33:00.480 --> 01:33:01.480]  Ну и, соответственно, вот эта остается,
[01:33:01.480 --> 01:33:19.480]  вот она тоже здесь остается.
[01:33:19.480 --> 01:33:20.480]  Вот это вот дело, вот это вот дело,
[01:33:20.480 --> 01:33:26.480]  результат позволяет оценивать но тогда оно и получается вот вот вот здесь значит прошу
[01:33:26.480 --> 01:33:33.880]  прощения так да вот мы сюда и приходим вот отсюда и вот сюда мы приходим но это вот дело
[01:33:33.880 --> 01:33:39.480]  оно превращается вот в это дело то есть мы действительно можем аппроксимировать в таком
[01:33:39.480 --> 01:33:45.960]  случае мы можем аппроксимировать невязку по прямой функции в этом цель то есть у нас есть
[01:33:45.960 --> 01:33:52.040]  настоящая функция и мы можем ее аппроксимировать невязкой по имперической функции плюс вот это вот
[01:33:52.040 --> 01:33:58.800]  оценка которая есть на аппроксимацию то есть это хороший результат это уже лучше не бывает но
[01:33:58.800 --> 01:34:05.160]  это все получено в предположении что у нас есть generalized linear structure это на самом деле очень
[01:34:05.160 --> 01:34:10.200]  сильное предположение ну и если есть сильная выпуклость дополнительно причем равномерная
[01:34:11.200 --> 01:34:17.320]  то оценку можно усилить но аналогично это получается я сейчас не буду здесь ушу так
[01:34:17.320 --> 01:34:24.080]  сказать этим заниматься вот к чему это все значит приводит нет извините вот к чему это
[01:34:24.080 --> 01:34:34.040]  приводит если все то же самое но если нет условия два если условия два нет тогда на самом деле все
[01:34:34.040 --> 01:34:44.560]  не так весело вот и и оценка которая значит здесь появляется она будет другая она будет вот такая и
[01:34:44.560 --> 01:34:50.440]  это надо в общем-то доказывать это уже по-другому доказывается это более общий результат то есть
[01:34:50.440 --> 01:34:56.240]  что в сильно именно подчеркиваю сильно выпуклым случае вырожденном случае вот это я такого не
[01:34:56.240 --> 01:35:00.680]  говорил что это будет верно я именно сказал сильно выпуклым случае останется верно вот такой
[01:35:00.680 --> 01:35:07.640]  результат но испортится часть которая связана вот с вот этой вот оценкой ну на самом деле это
[01:35:07.640 --> 01:35:14.240]  просто означает сколько вам надо брать слагаемых в этой сумме то есть вот вот результат о том что
[01:35:14.240 --> 01:35:21.840]  если n берется так что вот эта штука равняется epsilon то и отлично да то есть если например n
[01:35:21.840 --> 01:35:29.320]  брать пропорционально 1 на mu epsilon то здорово но при этом надо еще решить задачу вспомогательную
[01:35:29.440 --> 01:35:35.480]  вот эту вот задачу достаточно точно то есть вот эту задачу минимизации вот вот она написано ее
[01:35:35.480 --> 01:35:40.360]  надо достаточно точно решить а почему это так ну потому что вот видите это тоже должно быть
[01:35:40.360 --> 01:35:46.520]  epsilon если мы хотим чтобы это было epsilon значит это означает что невязка по вот этому решению
[01:35:46.520 --> 01:35:51.980]  должно быть epsilon в квадрате на мю то есть короче говоря вот эту задачу мы должны решить с
[01:35:51.980 --> 01:35:58.080]  точностью epsilon в квадрате на му этот этот этот как бы если хотим чтобы решение вот этой вспомогательной
[01:35:58.080 --> 01:36:04.320]  задача было Эпсилон решением исходной. Вот что такая наука говорит. И более того, доказано, что это не
[01:36:04.320 --> 01:36:10.280]  улучшаемо. То есть ничего не улучшаемо. Не эта часть не улучшаема, не эта часть не улучшаема. Точнее, эта
[01:36:10.280 --> 01:36:17.880]  часть улучшаема, если есть предположение 2. Но если Generalized Linear Structure нет, то оно не улучшаемо. И это все
[01:36:17.880 --> 01:36:26.320]  результаты относительно вот как раз офлайн подхода для вот таких постановок. Но, идя дальше, мы как бы
[01:36:26.320 --> 01:36:32.160]  знаем, что офлайн подход это не конец как бы истории. У нас есть онлайн подход. И вот про этот онлайн подход
[01:36:32.160 --> 01:36:39.760]  мы с вами говорили. То есть, офлайн подход это вот об этом, что надо решать эту задачу. Но есть и онлайн
[01:36:39.760 --> 01:36:45.560]  подход. То есть, не обязательно решать вот эту задачу с какой-то нужной точностью. А чем онлайн и
[01:36:45.560 --> 01:36:52.360]  офлайн подход отличаются? Это вопрос был задан. Значит, офлайн подход это то, что мы как бы рассматривали,
[01:36:52.360 --> 01:36:59.960]  когда заменяли мат ожидания. Вот он, значит, этот офлайн подход. Мы заменили мат ожидания выборочным
[01:36:59.960 --> 01:37:05.520]  средним. То есть, вот у нас задача сток оптимизации. Теперь уже мы не говорим, что она порождена
[01:37:05.520 --> 01:37:10.400]  какими-то плотностями. Мы просто постулируем сам функционал, как это делается в машинном обучении.
[01:37:10.400 --> 01:37:17.600]  Мы постулируем регрет или не регрет, там какой-то лоз. И дальше мы просто отрешим задачу сток
[01:37:17.600 --> 01:37:22.400]  оптимизации, заменяем от ожидания выборочным средним. И все, что я сейчас рассказывал на данный
[01:37:22.400 --> 01:37:28.400]  момент, это как бы насколько точно позволяет отрешивание вот этой задачи. То есть, если мы
[01:37:28.400 --> 01:37:33.520]  решим эту задачу с такой-то точностью, что можно сказать про решение исходной задачи сток
[01:37:33.520 --> 01:37:40.680]  оптимизации? То есть, это вообще говоря, наука о чем? Что вы решаете задачу империческую, а качество,
[01:37:40.680 --> 01:37:45.960]  которое вам нужно, оно меряется по вот этому функционалу. И что можно сказать, если вы там с
[01:37:45.960 --> 01:37:53.160]  такой-то точностью решите вот эту задачу минимизации? Вот она написана. Вот относительно критерия вот
[01:37:53.160 --> 01:37:59.360]  этой задачи. Вот это есть офлайн подход, он тут описан. Вот грубо говоря, к таким же оценкам мы
[01:37:59.360 --> 01:38:04.120]  сейчас придем в онлайн подходе, но здесь сделаем это без решения вспомогательной задачи с какой-то
[01:38:04.120 --> 01:38:09.560]  там повышенной точностью. А именно, мы берем онлайн подход, просто метод проекции сток градиента.
[01:38:09.560 --> 01:38:15.360]  Ну, мы считаем, что множество ку есть. Вот выкладки тривиальные, я их рассказывал на одной из лекций,
[01:38:15.360 --> 01:38:21.520]  когда мы рассказывали про SGD. И эти выкладки, но единственное, что там может быть и новое будет,
[01:38:21.520 --> 01:38:27.400]  что в сильно выпуклом случае шаг надо выбирать зависище мотка. Не фиксированным, а зависище
[01:38:27.400 --> 01:38:33.600]  мотка. Ну, вот нас это приводит, по сути, к таким двум результатам. Я даже про онлайн говорил на
[01:38:33.600 --> 01:38:38.760]  лекции. Ну, и здесь как раз онлайн формулировки написаны. То есть, нас это приводит вот к таким
[01:38:38.760 --> 01:38:44.640]  двум результатам, из которых уже можно вычислить результаты для выпуклой оптимизации, не для онлайн
[01:38:44.640 --> 01:38:54.080]  подхода. Когда все эти функции, ну, значит, как бы сказать, ну, когда этот х выбирается вот таким вот
[01:38:54.080 --> 01:39:03.160]  образом, мы можем, значит, выбирать х со звездой, можно выбирать фиксированный, а можно выбирать,
[01:39:03.160 --> 01:39:11.640]  вот, например, таким образом, когда вот минимизируется вот такая правая часть. И это дает нам,
[01:39:11.640 --> 01:39:21.280]  на самом деле, нужную оценку на вот большие отклонения и на сходимость. То есть, вот такие
[01:39:21.280 --> 01:39:27.000]  процедуры можно установить, как они сходятся. В выпуклом и в сильно выпуклом случае. И замечу,
[01:39:27.000 --> 01:39:32.600]  что эти оценки в точности соответствуют, ну, там с точностью до логарифмов, они соответствуют вот
[01:39:32.600 --> 01:39:37.320]  тому, что было раньше. То есть, вот этим вот оценкам они соответствуют. Видите, m в квадрате на
[01:39:37.320 --> 01:39:45.600]  mu n, вот m в квадрате на mu n, ну, вот здесь m r на корень из n, ну, и здесь то же самое. То есть,
[01:39:45.600 --> 01:39:50.840]  здесь такие же рейты мы получаем, m, ну, тут r задана вот как эта штука на корень из n,
[01:39:50.840 --> 01:39:56.280]  m r на корень из n, ну, а здесь m в квадрате на mu n тоже, тут просто логариф немножко другой
[01:39:56.280 --> 01:40:04.200]  получился, там не было фактора n. Вот, ну, хорошо, то есть, вот мы сейчас с вами посмотрели на два
[01:40:04.200 --> 01:40:12.200]  подхода, которые приводят, в общем-то, к одному и тому же. Но они как бы приводят к одному и тому
[01:40:12.200 --> 01:40:20.960]  же почти, потому что вот если я с оффлайн подходом мог явно выписать результат, вот он, то с онлайн
[01:40:20.960 --> 01:40:26.560]  подходом, с оффлайн подходом я этого не могу сделать. Я сказал следующее, я сказал, что если
[01:40:26.560 --> 01:40:35.200]  condition 2, это вот это condition, оно не выполняется, то все, что мне можно установить, это то, что на
[01:40:35.200 --> 01:40:40.400]  самом деле только вот эта вот штука имеет место. Это сильно выпуклый случай, я уже это говорил,
[01:40:40.400 --> 01:40:46.280]  то есть в оффлайн подходе существенно сильная выпуклость. Я не имею аналога прямого вот такого
[01:40:46.280 --> 01:40:52.320]  результата, если не выполняется ограничительное предположение 2. У меня получается, что в оффлайн
[01:40:52.320 --> 01:40:57.560]  подходе не только, значит, не только получается, что он требует решения вспомогательной задачи
[01:40:57.560 --> 01:41:02.840]  достаточно точно, но еще он не дает в выраженном случае, когда нет сильной выпуклости, вот такой
[01:41:02.840 --> 01:41:08.640]  оценки без предположения generalized linear model, а это очень специфический класс задач. То есть
[01:41:08.640 --> 01:41:16.200]  общая задача обучения выпуклым, ну, например, это какого, логистическая регрессия, свм, вот они
[01:41:16.200 --> 01:41:25.880]  подходят под generalized linear model, но как бы есть более общие модели, где прям совсем как бы ничего
[01:41:25.880 --> 01:41:31.760]  мы не знаем про просто выпуклость. Вот и вот онлайн подход ему все равно, а оффлайн подход ему не
[01:41:31.760 --> 01:41:38.600]  все равно. Значит, какой дальнейший план? У нас остался меньше часа времени, значит, я давайте
[01:41:38.600 --> 01:41:45.200]  наверное, не знаю, имеет ли смысл делать на 40 минут перерыв на 50 или просто уже 40 минут договорить
[01:41:45.200 --> 01:41:51.800]  и на этом закончить. Вот, потому что мы обычно делаем перерыв, когда, значит, вот студенты сидят
[01:41:51.800 --> 01:41:57.520]  в аудитории, но сейчас каждый может сделать перерыв так, как вот считает нужно в какой момент и
[01:41:57.520 --> 01:42:03.440]  ну, разве что с точки зрения восприятия, просто делать какую-то паузу, что вы могли немножко
[01:42:03.440 --> 01:42:10.720]  поспрашивать о происходящем, потому что я действительно немножко быстро иду, но резюме очень простое,
[01:42:10.720 --> 01:42:17.920]  что онлайн и оффлайн подходы, с которых я начал, они отличаются тем, что в онлайн подходе вы делаете
[01:42:17.920 --> 01:42:24.040]  вот это, то есть вы решаете задачу стох-оптимизации, ну как бы просто, как вот, как это мы с вами до
[01:42:24.040 --> 01:42:31.280]  этого рассматривали, как с SGD, это онлайн подход, а оффлайн подход вы заменяете задачу стох-оптимизации,
[01:42:31.280 --> 01:42:39.800]  задачей оптимизации близкой к ней, но опроксимирующей ее, вот такой вот. Ну и соответственно, нужна теория,
[01:42:39.800 --> 01:42:45.080]  показывающая сколько там чего, так сразу несколько вопросов, а какие недостатки могут быть
[01:42:45.080 --> 01:42:50.720]  онлайн подхода, да-да, во-первых, это та же версия книги, которую я скинул 1 сентября, это ответ на
[01:42:50.720 --> 01:42:56.360]  вопрос, только это, нет, это чуть-чуть новая версия, она не сильно отличается, но я ее сброшу еще раз,
[01:42:56.360 --> 01:43:02.160]  вот сегодня, а какие недостатки могут быть онлайн подхода, это хороший и правильный вопрос, ну
[01:43:02.160 --> 01:43:13.200]  представьте себе вот, что, значит, градиент, который здесь считается, например, да, он является
[01:43:13.200 --> 01:43:18.000]  градиентом какой-то сложной функции, такое может быть, например, когда вы Баррицентр Вассерштейна
[01:43:18.000 --> 01:43:23.440]  считаете, то есть это градиент функций, который требует, ну вообще говоря, очень больших вычислений,
[01:43:23.440 --> 01:43:28.880]  ну вот прям больших, а бывает такая ситуация, что двой, градиент двойственной функции считается
[01:43:28.880 --> 01:43:33.920]  сильно быстрее, ну, например, вот вы берете задачу поиска Баррицентра Вассерштейна, если вы ее
[01:43:33.920 --> 01:43:39.840]  оффлайн решаете, вы ее запишете в таком виде, и вы можете построить двойственную задачу, двойственная
[01:43:39.840 --> 01:43:44.760]  задача, и решать двойственную задачу каким-то распределенным алгоритмом, еще как-то, а прямой
[01:43:44.760 --> 01:43:51.000]  подход, во-первых, заставляет вас работать с прямыми градиентами, что может быть дороже, ну и во-вторых,
[01:43:51.000 --> 01:43:55.920]  он не позволяет хранить эти картинки, то есть он в стрим режиме, вам эти картинки, то есть вы
[01:43:55.920 --> 01:44:01.440]  должны их либо на каком-то сервере хранить и последовательно обрабатывать, ну либо, я не знаю,
[01:44:01.440 --> 01:44:06.680]  либо просто как-то они должны откуда-то скачиваться там по мере того, что вот они к вам приходят,
[01:44:06.680 --> 01:44:12.600]  но в любом случае это может быть проблема хранения этих картинок, то есть их содержать на одном узле,
[01:44:12.600 --> 01:44:18.800]  на котором происходит обучение. Вот в этой постановке вы можете распределить, разбив части суммы в узлах
[01:44:18.800 --> 01:44:23.920]  и хранить это в узлах, построить двойственную задачу, решать двойственную задачу. Вот об этом будет
[01:44:23.920 --> 01:44:28.760]  Александр Безносиков рассказывать, но не столько двойственная задача, сколько вообще вот как решать
[01:44:28.760 --> 01:44:34.320]  такого типа задач распределенными алгоритмами, то есть это своя жизнь, это своя жизнь связана с тем,
[01:44:34.320 --> 01:44:40.400]  что big data задача обучения нейронных сетей, огромный датасет, вы храните его на нескольких узлах,
[01:44:40.400 --> 01:44:46.400]  вы в голову даже вам не приходит пересылать ксикаты, ксикаты это картинки, кошечки, собачки,
[01:44:46.400 --> 01:44:51.480]  это как бы в вашем понимании выборка, да, ну то есть, но это большая, много весит каждая картинка,
[01:44:51.600 --> 01:44:58.400]  их много этих картинок, огромный датасет, миллиард сэмплов, ну невозможно все эти миллиард картинок
[01:44:58.400 --> 01:45:05.400]  значит мегабайтных хранить в памяти, в которой есть быстрый доступ, оперативный метод, а нам
[01:45:05.400 --> 01:45:10.280]  хочется, чтобы это все-таки было все в раме, происходило random access memory, мы значит как-то
[01:45:10.280 --> 01:45:15.440]  их распределили на узлы, и вот значит это все не так много в узле, и вот они значит пересылают
[01:45:15.440 --> 01:45:20.080]  какой-то агрегат вычислений, эти вычисления могут быть сильно дешевле, чем стоимость самой картинки,
[01:45:20.080 --> 01:45:24.560]  это сто градиент, то есть посчитать сто градиент может быть или двойственной, сильно дешевле, чем
[01:45:24.560 --> 01:45:30.720]  вообще разобраться с тем, что это за картинка, саму ее пересылать, поэтому когда мы говорим о онлайн
[01:45:30.720 --> 01:45:36.640]  обучении, это на самом деле вполне нас ограничивает даже вот как бы совсем такими современными
[01:45:36.640 --> 01:45:41.120]  ну приложениями, хотя вот явно тут много преимуществ, но вот как обучение нейронных
[01:45:41.120 --> 01:45:48.000]  сетей, потому что не совсем понятно как организовать, значит эту процедуру не распределенно,
[01:45:48.000 --> 01:45:53.680]  чисто вот вычислительно непонятно, а распределенно понятно, что хранятся части задачи и вот они
[01:45:53.680 --> 01:45:59.240]  коммуницируют, ну на самом деле здесь есть некий обман, потому что вы эту задачу можете решать
[01:45:59.240 --> 01:46:04.240]  более-менее этим же методом, так и делают, то есть вы как бы берете сто градиент, просто берете его
[01:46:04.240 --> 01:46:10.080]  случайно выбирая слагаемое, это немного другая история, но как бы в конечном итоге вы получите
[01:46:10.080 --> 01:46:15.120]  тоже качество, если напишете эту задачу и будете решать ее вот таким методом, где сто градиент
[01:46:15.120 --> 01:46:20.800]  выбирается случайно равновероятно, и это как бы восстановит справедливость, то есть вы как бы из
[01:46:20.800 --> 01:46:26.720]  офлайн подхода сделаете онлайн, ну практически такое же по качеству, но тогда вот вы будете терять какие-то
[01:46:26.720 --> 01:46:32.080]  свойства при редукции дисперсии, которую здесь можно сделать, о которой мы практически ничего не
[01:46:32.080 --> 01:46:36.800]  говорили, но вот какой-то некий прием, учитывающий структуру суммы, который позволяет такие задачи
[01:46:36.800 --> 01:46:45.680]  решать, значит хорошо теперь у нас, собственно, вопрос в том, что sample average approximation, то есть то,
[01:46:45.680 --> 01:46:53.240]  что мы называли, значит, я уже забыл, значит, ну короче, Монте-Карло и онлайн, оффлайн и онлайн
[01:46:53.240 --> 01:46:58.600]  подходы, вот видите, тут написано off-line, это sample average approximation, то есть это Монте-Карло,
[01:46:58.600 --> 01:47:05.320]  а это онлайн подход, то есть это SGD, но в западной литературе вот это вот называется stochastic
[01:47:05.320 --> 01:47:12.160]  approximation, ну а это вот так и называется, ну мы называем это Монте-Карло или оффлайн,
[01:47:12.160 --> 01:47:21.480]  оффлайн, вот и, значит, давайте просто ведем понятие вот такое, вот что, значит, решение вот
[01:47:21.480 --> 01:47:27.800]  этой задачи с точностью там дельта, бета, дельта это точность решения, бета это доверительный
[01:47:27.800 --> 01:47:32.560]  уровень, вот это будет такая точка, которая удовлетворяет вот такому условию, то есть
[01:47:32.560 --> 01:47:37.840]  мы решили задачу минимизации имперической риска, функции риска, то есть это оптимум,
[01:47:37.840 --> 01:47:43.040]  я напомню, а это вот то, что мы приближенно нашли, это вот дельта, которая обеспечивается таким
[01:47:43.040 --> 01:47:47.440]  доверительным уровнем, ну и это обозначение мы будем использовать далее, как бы сказать то,
[01:47:47.440 --> 01:47:53.640]  что является опроксиматором для задачи минимизации имперического риска, ну и теперь вот есть результаты
[01:47:53.640 --> 01:48:02.640]  довольно, так сказать, интересные, показывающий, что если задача не сильно выпуклая, не сильно
[01:48:02.640 --> 01:48:10.160]  выпуклая, то обратите внимание, что оценка, которая получается, она портится в n раз, n размерность
[01:48:10.160 --> 01:48:15.240]  пространства, то есть вы действительно имеете аналогичный результат, но в n раз хуже, если
[01:48:15.240 --> 01:48:21.520]  вот заниматься тем, чтобы решать задачу минимизации имперического риска, то есть число сэмплов надо
[01:48:21.520 --> 01:48:27.440]  брать в отличие от онлайн подхода в n раз больше, это вообще удивительно, потому что если бы вы решали
[01:48:27.440 --> 01:48:32.920]  задачу сток оптимизации, вам бы n надо было взять 1 на епсилон квадрате, и от размерности бы это не
[01:48:32.920 --> 01:48:38.920]  зависело, то есть давайте это прям зафиксирую, n пропорционально 1 на епсилон квадрате, это онлайн,
[01:48:38.920 --> 01:48:45.960]  а если оффлайн, то n пропорционально n размерность пространства на епсилон квадрате, это оффлайн,
[01:48:45.960 --> 01:48:51.720]  и разница большая, если размерность пространства большая, вот это как бы необычная ситуация,
[01:48:51.720 --> 01:48:57.440]  похожая на мини бочирование, ну я не знаю, как это прокомментировать, ну наверное, что-то
[01:48:57.440 --> 01:49:05.240]  похоже, значит, и в чем тут проблема, ну проблема ровно в том, что нет вот этой вот как бы сильной
[01:49:05.240 --> 01:49:11.080]  выпуклости, то есть нет гарантии, что сходимость по решению будет иметь место, ну то есть по аргументу,
[01:49:11.800 --> 01:49:20.080]  я доказательства естественные опускаю, оно громоздкое, но замечу, что и с игрой на невклидовость я
[01:49:20.080 --> 01:49:26.680]  тоже это опускаю в детали, но замечу, что вообще говоря, в случае, когда сильно выпуклость есть,
[01:49:26.680 --> 01:49:30.640]  у вас справедливость восстанавливается, то есть вы как раз можете получить тот результат,
[01:49:30.640 --> 01:49:35.960]  который я уже упоминал, только даже в более общем сетапе со всякой невклидовостью, и получается,
[01:49:35.960 --> 01:49:44.080]  что главное, что нужно сделать, это регулировать задачу, то есть она не сильно выпуклая, а нам
[01:49:44.080 --> 01:49:49.240]  надо ее регулировать, и тогда проблема решится сама собой, то есть задача станет сильно выпуклой,
[01:49:49.240 --> 01:49:55.040]  но просто регулировать надо правильно, и если мы это правильно сделаем, то все схлопнется,
[01:49:55.040 --> 01:50:00.440]  то есть можно будет использовать результат, который как бы для сильно выпуклых задач
[01:50:00.440 --> 01:50:05.720]  дает правильную оценку, он же даст правильную оценку при регулиризации для просто выпуклых
[01:50:05.720 --> 01:50:11.720]  задач, то есть онлайн и офлайн подходы совпадут, но чтобы это сделать мачинг, нам надо вести
[01:50:11.720 --> 01:50:18.680]  регулиризацию, здесь она рассказана в самом простом ключе, а именно вот в таком, когда эта регулиризация
[01:50:18.680 --> 01:50:25.000]  делается просто квадратичной, давайте выберем mu равняется epsilon r2, тогда утверждается,
[01:50:25.000 --> 01:50:30.680]  что если сглажен вот эта вот регулиризованная функция, отрешивается, отминимизируется с
[01:50:30.680 --> 01:50:36.840]  точностью epsilon пополам, и mu равняется epsilon на r2, это размер решения в квадрате, но то есть размер
[01:50:36.840 --> 01:50:41.560]  того шара, в котором множество кул лежит, тут есть предположение, что кул лежит в этом шаре,
[01:50:41.560 --> 01:50:48.440]  то тогда мы можем обеспечить, что эта же самая точка будет решением исходной задачи с точностью
[01:50:48.440 --> 01:50:54.800]  epsilon, то есть нам достаточно регулиризовать с таким параметром, а меньше не надо, можно и меньше,
[01:50:54.800 --> 01:50:59.840]  но меньше уже не надо, потому что и так будет выполняться нужная точность, но сильно выпуклась
[01:50:59.840 --> 01:51:05.120]  пострадает, поэтому начну, тут тривиальная выкладка, почему это выполняется, вы можете потом
[01:51:05.120 --> 01:51:10.080]  посмотреть, это в общем я рассказываю в курсе оптимизации, поэтому не хочется здесь повторяться,
[01:51:10.080 --> 01:51:16.560]  тем более, что это вы можете посмотреть потом, но эффект от этого колоссальный, то есть как бы вы
[01:51:16.560 --> 01:51:21.600]  берете задачу минимизации имперического риска, добавляете аккурат тот самый регулиризатор,
[01:51:21.600 --> 01:51:27.040]  который соответствует вот этой теории Митиханова, ну и получаете, что вы уже вот для этой задачи,
[01:51:27.040 --> 01:51:32.520]  не для исходной, вот как бы оценка числа слагаемых вот такая, то есть разница в том,
[01:51:32.520 --> 01:51:36.920]  что если бы я не писал вот это, то у меня бы здесь вылезла размерность пространства, если бы я не
[01:51:36.920 --> 01:51:42.360]  писал, понимаете, это на самом деле очень мощно, то есть еще раз, я этого не пишу, значит здесь оценка
[01:51:42.360 --> 01:51:49.080]  в n раз хуже, а как только я это пишу, у меня оценка в n раз лучше, вот зачем нужна регулиризация,
[01:51:49.080 --> 01:51:58.680]  то есть вот как бы регулиризация обеспечивает то, что число и число слагаемых, которые надо взять в
[01:51:58.680 --> 01:52:05.160]  подходе оффлайн, оно соответствует числу итерации онлайн процедуры, оно и не удивительно, как бы это
[01:52:05.160 --> 01:52:10.400]  правильный результат, другое дело, что здесь еще надо решать задачу, а там она как бы автоматически
[01:52:10.400 --> 01:52:14.880]  агрегируется, но мы с вами уже говорили, что организовать процедуру, когда автоматически
[01:52:14.880 --> 01:52:18.760]  агрегируется, может быть технически сложно, потому что это значит надо всю эту выборку,
[01:52:18.760 --> 01:52:24.360]  где-то схоронить и как-то доступ к ней обеспечивать, тут как бы это все распределено можно делать,
[01:52:24.360 --> 01:52:28.600]  они могут коммуницировать, как бы так и не так и не собирая в одном месте все ксяка,
[01:52:28.600 --> 01:52:38.880]  или там соображение приватности, еще что-то, ну и замечу, что вот когда у нас условия дополнительно
[01:52:38.880 --> 01:52:44.040]  известны, какие-то вот так называемые с growth condition, то есть условия, обычно это возникает
[01:52:44.040 --> 01:52:49.880]  условия, когда вот есть функция, и она минимум имеет вот как бы в граничной точке на множестве
[01:52:49.880 --> 01:52:54.600]  q, и там есть условия какого-то острого минимума, типа острого минимума, например s единица,
[01:52:54.600 --> 01:53:01.320]  это называется острый минимум, тогда в этом случае оценки на число итерации, они становятся
[01:53:01.320 --> 01:53:07.040]  удивительно хорошими, в частности для острого минимума s единица, то есть мы имеем, что если
[01:53:07.040 --> 01:53:12.600]  положить единицы, число итерации будет пропорционально, некая константа, ну короче,
[01:53:12.600 --> 01:53:19.080]  пропорционально один, ну логарифм на епсилон, то есть число не итерации, число слагаемых
[01:53:19.080 --> 01:53:23.800]  пропорционально один на епсилон, то есть от желаемой точности практически никак не зависит,
[01:53:23.800 --> 01:53:29.840]  это потому что вот это обнулится, показатель епсилон, и это хороший результат, это мощный
[01:53:29.840 --> 01:53:34.920]  результат, потому что он по сути означает, что для острого минимума не надо много сэмплов,
[01:53:34.920 --> 01:53:41.560]  но то же самое получается и для подхода, который онлайн, но я не буду контрпример приводить,
[01:53:41.560 --> 01:53:48.200]  вот для подхода онлайн у нас получается, значит прошу прощения, такой же результат, только там
[01:53:48.200 --> 01:53:53.920]  немножко другие логарифмы, получается он как раз рестартами из метода стахастического
[01:53:53.920 --> 01:54:02.320]  градиентного спуска, вот значит здесь есть оценка снизу, оценка сверху, и мы как бы выбираем число
[01:54:02.320 --> 01:54:10.120]  итерации так, чтобы вот эта штука, ну норма невязки, уполовинилась, вот это и есть, вот она идея,
[01:54:10.120 --> 01:54:16.080]  значит, и вот если так выбирать n, мы собственно получим, что число таких рестартов должно быть
[01:54:16.080 --> 01:54:21.280]  логарифмическим, ну и можно оценить каким должен быть размер одной такой итерации
[01:54:21.280 --> 01:54:27.240]  уполовинивающей, и мы можем из любой процедуры, в которую входит расстояние от точки старта до
[01:54:27.240 --> 01:54:34.840]  решения получить процедуру, которая уже имеет правильные оценки, что мы и сделали, вот, ну и как бы
[01:54:34.840 --> 01:54:41.000]  если говорить про всякие релаксации того, что я сейчас рассказывал, то можно обобщать все эти
[01:54:41.000 --> 01:54:50.640]  результаты на случай хвостов тяжелых, в частности, все это верно, если, например, у нас дисперсия
[01:54:50.640 --> 01:54:56.480]  ограничена, но хвосты тяжелые, вот то, что я рассказывал, будет верно, то есть несущественно,
[01:54:56.480 --> 01:55:01.640]  что как бы хвосты не субгаусовские или там нефинитные, то есть нам это не нужно, на самом деле,
[01:55:01.640 --> 01:55:07.000]  вот много и не все, но много из того, что я сказал, будет верно, но вот если дисперсия не ограничена,
[01:55:07.000 --> 01:55:13.800]  но ограничен какой-то момент между двум и один, тогда результат ухудшается, то есть эпсилон минус
[01:55:13.800 --> 01:55:20.440]  второй становится вот так таким, и это, ну точнее вот таким, там, p это у нас двойка, на p можете не
[01:55:20.440 --> 01:55:24.920]  смотреть, я Евклида в случае рассматриваю, ну то есть, вообще говоря, если альфа маленькая,
[01:55:24.920 --> 01:55:33.400]  то результат становится плохим, вот, значит, условия выпуклости можно заменять, можно рассматривать
[01:55:33.400 --> 01:55:40.960]  более слабые условия, например, полика Лоисеевича, и на эту тему тоже есть результаты, что не обязательно
[01:55:40.960 --> 01:55:48.200]  все это делать в условиях выпуклости, значит, все это можно сделать и в каком-то смысле адаптивно,
[01:55:48.200 --> 01:55:53.960]  идея адаптивности, она заключается в том, что вот шаг метода, который раньше выбирался вот так,
[01:55:53.960 --> 01:56:01.240]  теперь его надо выбирать вот, как бы, во-первых, так, чтобы не было зависимости от числа желаемых
[01:56:01.240 --> 01:56:07.040]  итераций, это, конечно, приводит к лишним проблемам, но главное, что вот в этой схеме вы можете сделать,
[01:56:07.040 --> 01:56:14.000]  это заменить m константу Липшица на вот такие штуки, то есть вы фактически вот апраксимируете вот эту
[01:56:14.000 --> 01:56:21.160]  штуку, как r на mk, на m корень из k, но так оно и есть, потому что это и есть какая-то оценка m в квадрате,
[01:56:21.160 --> 01:56:27.440]  и число таких слагаемых k, и вот эта идея, она легла в основу адаграда, то есть вместо того,
[01:56:27.440 --> 01:56:35.960]  чтобы сначала заменить это вот этим, а потом с этим работать, ну было принято, было как бы понято,
[01:56:35.960 --> 01:56:41.840]  что m на корень из k проще вот как бы, не проще, а адаптивно можно вычислять вот так вот, это и есть
[01:56:41.840 --> 01:56:47.560]  m на корень из k, ну какая-то оценка, и это работает, доказана даже соответствующая теорема, что это
[01:56:47.560 --> 01:56:53.880]  должно для выпуклых задач работать. Перепараметризацию мы с вами рассматривали, то есть мы с вами
[01:56:53.880 --> 01:57:01.520]  рассматривали случаи, когда дисперсия задана в нуле, и это, соответственно, порождает оценку,
[01:57:01.520 --> 01:57:08.640]  вообще говоря, линейной исходимости с конкретности, но надо заметить, что вот результат для
[01:57:08.640 --> 01:57:18.760]  эмпирических задач, когда есть перепараметризация, это уже другое, он выглядит как бы хуже, то есть у вас
[01:57:18.760 --> 01:57:24.240]  уже нет линейной исходимости, то есть если вы будете использовать вот эмпирический подход,
[01:57:24.240 --> 01:57:29.000]  то вот эта перепараметризация, она действительно тоже есть некий эффект быстрой исходимости,
[01:57:29.000 --> 01:57:36.840]  оценки, но он вот другой, он не такой быстрый, как в случае онлайн подхода, то есть онлайн подход
[01:57:36.840 --> 01:57:43.520]  здесь, во всяком случае, на данный момент по теории лучше. Самый как бы такой естественный способ
[01:57:43.520 --> 01:57:50.440]  дальнейшего развития всей этой науки, это вот уже теперь преимущество онлайн подхода. Онлайн подход
[01:57:50.440 --> 01:57:55.840]  бочируется, то есть параллелится, когда есть гладкость. Под гладкостью мы понимаем, что целевая
[01:57:55.840 --> 01:58:00.720]  функция имеет констант улипшится градиента. Тогда можно решать задачу ускоренными методами, они
[01:58:00.720 --> 01:58:05.240]  сходятся вот так, что, конечно, лучше, чем всякие оценки один на корень и зен, которые мы писали,
[01:58:05.240 --> 01:58:11.160]  но у нас задача 100-х оптимизации, то есть у нас 100-х аракул, и поэтому мы работаем с так называемым
[01:58:11.160 --> 01:58:16.400]  пробатченным 100-градиентом. И вот можно показать, что если размер батча должным образом выбирать,
[01:58:16.400 --> 01:58:22.880]  вот здесь есть некая наука, как это делать, причем из элементарных соображений, то будет допускаться
[01:58:22.880 --> 01:58:27.560]  вот такой параллелизм, то есть размер батча выбирается, ну в зависимости того ускоренный
[01:58:27.560 --> 01:58:33.520]  там не ускоренный метод, ну вот, либо так, либо так, ну и оценка скорости сходимости будет,
[01:58:33.520 --> 01:58:39.240]  значит, ну вот, допустим, для ускоренного метода вот такая, то есть число последовательных
[01:58:39.240 --> 01:58:43.680]  итераций будет вот таким, размер батча будет вот таким, ну а, соответственно,
[01:58:43.680 --> 01:58:48.440]  общее число итерации это будет не итерация аракульных вызовов 100 градиентов, это будет
[01:58:48.440 --> 01:58:54.080]  произведение двух чисел, оно будет таким, каким было раньше, то есть вы в этом смысле ничего не
[01:58:54.080 --> 01:59:01.440]  улучшили, вы получили такую же оценку, как раньше, но сделали лучше то, что число последовательных
[01:59:01.440 --> 01:59:05.920]  итераций, которые не параллелятся, оно будет меньше, а в предыдущих подходах все это
[01:59:05.920 --> 01:59:12.480]  последовательно делалось, ну то есть здесь борьба за параллелизм. Ну то, что задача вида суммы
[01:59:12.480 --> 01:59:19.200]  обладает специфическими особенностями, которые надо учитывать и решать, это вот так называемые
[01:59:19.200 --> 01:59:24.720]  методы редукции дисперсии, они позволяют эффективнее решать такие задачи, чем, ну чем
[01:59:24.720 --> 01:59:29.320]  какими-то стандартными методами типа ускоренного, вот это вот тоже важное замечание, я не буду
[01:59:29.320 --> 01:59:35.160]  подробно этому останавливаться, вот здесь есть идея метода редукции дисперсии, которую вы можете
[01:59:35.160 --> 01:59:41.560]  посмотреть, в чем она заключается, как выбирать редукцию дисперсии, также вы можете, ну мы уже
[01:59:41.560 --> 01:59:47.000]  с этим сталкивались, например, лассо проблем и так далее, есть композиты, то есть в задачи есть
[01:59:47.000 --> 01:59:52.400]  какие-то, в самой задачи оптимизации есть какие-то, например, регуляризаторы вот такого типа,
[01:59:52.400 --> 01:59:58.600]  и их необязательно брать градиент, потому что, например, лассо в том же лассо плюс лямбда х1,
[01:59:58.600 --> 02:00:03.960]  оно не гладкое, если брать его градиент, то все как бы наука теряется, наука, которая гладко
[02:00:03.960 --> 02:00:10.200]  издает, поэтому идея заключается в том, чтобы брать вместо проекции, значит понимать проекцию,
[02:00:10.200 --> 02:00:14.600]  как отрешивание вспомогать на задачи и, соответственно, смотреть на задачу обучения,
[02:00:14.600 --> 02:00:19.880]  как на задачу с композитом, и вот этот композит оставлять вот в этой модели, которая здесь
[02:00:19.880 --> 02:00:24.440]  присутствует, то есть у нас есть модель квадратичная функционала, вот ее мы сохраняем,
[02:00:24.440 --> 02:00:30.600]  а композит не заменяет ничем, просто оставляем как есть, ну и такие подходы действительно для
[02:00:30.600 --> 02:00:36.320]  лассо дают хорошие результаты, и потому что есть ускорение, есть как бы гладкость, а не гладкость,
[02:00:36.320 --> 02:00:42.560]  она зашивается вот сюда, ну и это усложняет вспомогательную задачу, метода, но вот сохраняет
[02:00:42.560 --> 02:00:48.560]  в онлайн подходе число итерации хорошим, значит я уже упоминал про раннюю, нет не упоминал,
[02:00:48.560 --> 02:00:55.520]  но говорил про трюк, что можно решать задачу минимизации эмпирического риска sgd, и вот здесь
[02:00:55.520 --> 02:01:03.280]  очень важно решать ее, вовремя остановившись, то есть если мы делаем число итерации sgd
[02:01:03.280 --> 02:01:09.240]  пропорционально n, то качество будет n-1,2, это ровно то, что надо, но тут важно, что это n,
[02:01:09.240 --> 02:01:15.600]  оно ровно совпадает с тем n, которое здесь, то есть тут важно, что число итерации sgd надо брать
[02:01:15.600 --> 02:01:21.360]  не больше, не меньше, а порядка n, это то, что вы на практике скорее всего используете, когда проходите
[02:01:21.360 --> 02:01:27.320]  датасет несколько раз, но вы его хотя бы раз проходите, но не проходите его бесконечно много раз,
[02:01:27.320 --> 02:01:32.720]  это если вы не делаете регуляризацию, если вы ее не делаете, то тогда вот это золотое правило,
[02:01:32.720 --> 02:01:38.320]  нельзя его нарушать, иначе будет переобучение, но есть более хитрые всякие механизмы, значит
[02:01:38.320 --> 02:01:43.960]  что будет если, например, в ВТ выбираете немножко по-другому, какая будет, ну то есть здесь,
[02:01:43.960 --> 02:01:49.880]  в общем-то, скорее результаты о том, что ничего хорошего ожидать не следует, и вот это, наверное,
[02:01:49.880 --> 02:01:55.400]  максимально яркий результат, а все остальное это некоторые, так сказать, окрестности, которые в
[02:01:55.400 --> 02:02:02.720]  целом, ну они такие, достаточно пессимистичны, но, например, если вы будете решать задачу вот эту
[02:02:02.720 --> 02:02:09.720]  вот не стахастическим градиентным спуском, а градиентным спуском, понимаете, это как бы круче,
[02:02:09.720 --> 02:02:14.920]  как бы с точки зрения скорости сходимости для этой задачи, градиентный спуск, естественно,
[02:02:14.920 --> 02:02:19.080]  будет сходиться быстрее, но вот я это пишу, вот градиентный спуск, то на самом деле,
[02:02:19.080 --> 02:02:23.440]  сколько бы итераций градиентного спуска вы не делали, вот неважно, какой ВТ возьмете,
[02:02:23.440 --> 02:02:31.880]  вы не получите качество лучше, чем n-5 в 12, но это хуже, это хуже, это хуже, чем n в степени
[02:02:31.880 --> 02:02:38.440]  минус одна вторая, я имею в виду качество на исходной задачи, а можно подробнее немного про
[02:02:38.440 --> 02:02:46.640]  результаты переобучения, ну конечно, пожалуйста, то есть, смотрите, вам надо решать вот задачу
[02:02:46.640 --> 02:02:54.040]  минимизации имперического риска, решение этой задачи как бы дальше исследуется на предмет того,
[02:02:54.040 --> 02:02:58.240]  насколько оно там соответствует настоящему функционалу, а настоящий функционал, это мы от
[02:02:58.240 --> 02:03:04.520]  ожидания, и понятно, что даже если вы сколь угодно точно отрешиваете эту задачу, это еще не значит,
[02:03:04.520 --> 02:03:09.800]  что вы что-то хорошее сделали для исходной задачи, и вот оно об этом, то есть, когда вы очень
[02:03:09.800 --> 02:03:15.000]  точно решаете задачу, допустим, градиентным спуском, вы в принципе не можете гарантировать
[02:03:15.000 --> 02:03:21.440]  аппроксимацию, которую вы можете получить, решая эту задачу стахастическим градиентным спуском,
[02:03:21.440 --> 02:03:26.680]  делая всего n итераций, столько-сколько слагаемых, ну пропорционально, это немного удивительно,
[02:03:26.720 --> 02:03:32.480]  потому что вы как бы нацелены на то, чтобы решать эту задачу, но повторю, если у вас нет регуляризации,
[02:03:32.480 --> 02:03:37.360]  если нет сильной выпуклости, то эта задача плохо аппроксимирует исходную задачу, вам тогда надо
[02:03:37.360 --> 02:03:43.240]  было брать, чтобы была аппроксимация n пропорционально n маленькой на ε2, вот,
[02:03:43.240 --> 02:03:52.440]  ну то есть, здесь как бы момент такой, что как бы достаточно для других процедур 1 на
[02:03:52.440 --> 02:03:58.760]  ε2 итерации, а вот как бы без регуляризации итерации будет больше, это не очень хорошо,
[02:03:58.760 --> 02:04:05.040]  это как бы означает, что эта задача в целом без регуляризации, она, она довольно дурацкая сама
[02:04:05.040 --> 02:04:11.880]  по себе, то есть, ее нельзя просто так решать, например, LBFJS, каким методом Ньютона, ее надо
[02:04:11.880 --> 02:04:17.880]  решать, либо регуляризовав предварительно, и тогда уже можно точно решать, либо решать с остановкой,
[02:04:17.880 --> 02:04:23.080]  early stopping, это early stopping, оно как раз и контролирует переобучение, потому что, смотрите,
[02:04:23.080 --> 02:04:28.200]  вы запускаете какой-то итерационный процесс решения этой задачи, но regret или loss вы
[02:04:28.200 --> 02:04:34.840]  меряете по, то есть, loss вы меряете по невязке по прямой задаче, то есть, f от xk, у вас минус f
[02:04:34.840 --> 02:04:39.960]  от x со звездой по прямой задаче, xk считается исходя из итерационного процесса для этой задачи,
[02:04:39.960 --> 02:04:48.360]  например, xk плюс 1 равняется xkt минус h на градиен, допустим, f с чертой от xk, вот такая процедура,
[02:04:48.360 --> 02:04:55.280]  и вот это xk, оно как себя будет вести, оно будет убывать, убывать, убывать, а проблема в том,
[02:04:55.280 --> 02:05:00.720]  что потом оно начнет возрастать, это потому что вы решаете задачу по вот этому критерию,
[02:05:00.720 --> 02:05:05.640]  и по критерию этому она будет дальше убывать, но по критерию вот этому она начнет возрастать,
[02:05:05.640 --> 02:05:14.120]  уобразной кривой поедет, это будет переобучение как раз, то есть, если бы у нас было вот красное,
[02:05:14.120 --> 02:05:21.760]  это отвечает зависимости f с чертой от xk минус f с чертой от соответственно x с чертой со звездой,
[02:05:21.760 --> 02:05:27.800]  то вы бы действительно убывали так, как полагается там с рейтом там 1 на k, допустим,
[02:05:27.800 --> 02:05:32.400]  то здесь будет как бы так, что вы будете по началу похоже, а потом вот это загнется,
[02:05:32.400 --> 02:05:37.160]  просто потому что задача другая, то есть вы как бы хотите точно решать одну задачу,
[02:05:37.160 --> 02:05:43.480]  то есть хотите точно минимизировать f, а решаете реально другую задачу, вот в этом проблема,
[02:05:43.480 --> 02:05:49.080]  и эта проблема отчасти решается просто непосредственно онлайн подходом, потому что у нас нет вот этого вообще
[02:05:49.080 --> 02:05:56.320]  дела, но с другой стороны вот можно и как бы из оффлайн подхода что-то вытащить, если есть желание
[02:05:56.320 --> 02:06:03.440]  распределенного что-то делать, значит хорошо идем дальше, у нас теперь сюжет вот с распределенной
[02:06:03.440 --> 02:06:09.280]  оптимизацией, но он не очень как бы такой существенный, потому что ну в общем-то дальше
[02:06:09.280 --> 02:06:14.360]  Александр Безносиков про это расскажет на следующем занятии, то есть фактически эта сумма,
[02:06:14.360 --> 02:06:20.080]  она собирается вот в такие подсуммы, и дальше мы можем использовать два фактора, либо редукция
[02:06:20.080 --> 02:06:25.360]  дисперсии, о которой я говорил, либо similarity, то есть тот факт, что слагаемые оказываются
[02:06:25.520 --> 02:06:30.960]  статистически близкими, если это одинаковые сэмплы, вот как математически использовать тот факт,
[02:06:30.960 --> 02:06:37.640]  что в задачах обучения они близки, близкие эти функции, оказывается это можно делать, и здесь
[02:06:37.640 --> 02:06:42.560]  возникает очень красивая математика, красивая наука, я не буду сейчас про это рассказывать,
[02:06:42.560 --> 02:06:48.800]  здесь более-менее написано, но эффект заключается в том, что вот такую задачу, если это хранится на
[02:06:48.800 --> 02:06:56.960]  разных серверах, вот это разные сервера, хранят разные, можно решить за число вычислений 1 на корень
[02:06:56.960 --> 02:07:04.400]  из s, 1 на корень из s, это степень similarity, поделить на μ константа сильной выпуклости, то есть имеется
[02:07:04.400 --> 02:07:09.720]  в виду, что вместо того, чтобы как было раньше число коммуникации с центральным узлом было
[02:07:09.720 --> 02:07:14.720]  пропорционально константе липшица на константу сильной выпуклости, это надо было коммуницировать
[02:07:14.720 --> 02:07:21.480]  столько раз, мы константу липшица редуцируем в s раз, ну то есть как бы s это число слагаемых,
[02:07:21.480 --> 02:07:28.000]  потому что закон больших чисел, то есть как бы эта штука, она имеет, реально как бы они схожие,
[02:07:28.000 --> 02:07:33.800]  из-за этого там возникает не худшая константа, а как бы константа липшица разницы, то есть возникает
[02:07:33.800 --> 02:07:40.400]  ну как бы константа липшицы, не знаю где-то мне это написано, которая вот отвечает такой вот разнице
[02:07:40.400 --> 02:07:46.760]  функции, ну в общем здесь есть детали, это можно посмотреть, это довольно сильно сокращает объем
[02:07:46.760 --> 02:07:54.080]  вычислений, вот, значит можно пойти дальше и рассуждать о так называемых ускоренных методах,
[02:07:54.080 --> 02:08:01.880]  тензорных, то есть не о методах, когда мы берем модель, ну в общем первого порядка, а метода,
[02:08:01.880 --> 02:08:06.600]  когда мы берем модель второго типа Ньютона, то есть можно ли использовать метод Ньютона для вот
[02:08:06.600 --> 02:08:11.880]  этих задач, я имею ввиду задачу минимизации суммы, но если она сильно выпуклая, то можно,
[02:08:11.880 --> 02:08:16.520]  мы с вами про это недавно говорили, то есть если она вырождена, нельзя, ее надо регулиризовать,
[02:08:16.520 --> 02:08:22.200]  а тогда уже можно и методы высокого порядка применять, вот, ну естественно тут есть всякие
[02:08:22.200 --> 02:08:27.360]  отдельные сюжеты про седла, как это переносится на седла, можно посмотреть ссылки, как это все
[02:08:27.360 --> 02:08:32.680]  переносится на, значит, ну какие-то конкретные неевклидовые проксы, примеры, вот Виссерштейн,
[02:08:32.680 --> 02:08:40.560]  Борис Энтер, экзампов, ну и в целом, ну надо, конечно, отметить уже в заключение, что люди,
[02:08:40.560 --> 02:08:47.480]  которые, в общем, ну внесли большой вклад в эту науку, ну вот началось все там около 70 лет
[02:08:47.480 --> 02:08:54.240]  назад статья Робинсона Монро, Робинс Монро, это в общем классики данной области, но вот, конечно,
[02:08:54.240 --> 02:08:58.480]  особо хочется отметить Аркадия Семеновича Немировского, где уже очень многое из того,
[02:08:58.480 --> 02:09:05.480]  что я рассказывал, было получено, это 79 год, книжка, вот, потом были разные, конечно,
[02:09:05.480 --> 02:09:12.040]  всякие, так сказать, результаты, но в основном эта наука начала так серьезно двигаться только
[02:09:12.040 --> 02:09:17.880]  20 лет назад, то есть вот где-то начало двухтысячных годов, ну может быть чуть раньше, и вот многие
[02:09:17.880 --> 02:09:22.200]  результаты, которые же вот мы так современно смотрим, это вот результаты последних 20 лет,
[02:09:22.200 --> 02:09:29.640]  здесь есть большое число ссылок на литературные источники, но думаю, что мы, наверное, на этом
[02:09:29.640 --> 02:09:37.000]  остановимся, потому что, в общем-то, ну не хочется здесь перегружать вас какими-то деталями,
[02:09:37.000 --> 02:09:43.720]  а хотелось просто вот немножко так по диагонали сверху пройтись по вот всем этим постановкам,
[02:09:43.720 --> 02:09:49.000]  вот, ну повторю, что мне кажется, наиболее важный результат, который вот из-за того,
[02:09:49.000 --> 02:09:55.640]  что я рассказывал, вы можете, как бы, ну какой вывод сделать, что либо надо регулировать задачу
[02:09:55.640 --> 02:10:01.540]  минимизации имперического риска, либо надо и ранее остановка соответствующего процедуры типа
[02:10:01.540 --> 02:10:07.920]  SGD, даже не типа, а просто SGD, и что еще важно, что есть две концепции, извините, что я так мотаю,
[02:10:07.920 --> 02:10:15.240]  просто хочу начало лекции, есть две концепции, как строить задачи обучения, концепция статистики,
[02:10:15.720 --> 02:10:23.120]  мат-статистики, которая, по сути, требует вероятностную модель, если есть вероятностная модель,
[02:10:23.120 --> 02:10:30.680]  вот она, значит, есть функционал, значит, есть правдоподобие, и вы решаете задачу стох
[02:10:30.680 --> 02:10:36.560]  оптимизации. Дальше вся та же самая наука, будь то, соответственно, имперический риск минимизации,
[02:10:36.560 --> 02:10:41.840]  или, соответственно, онлайн какой-то подход агрегирования, это уже не так сейчас важно,
[02:10:41.840 --> 02:10:46.000]  более-менее они одинаковые. Ну так, в первом приближении, вопрос только в том,
[02:10:46.000 --> 02:10:51.920]  что онлайн подход, он не требует отрешивания задачи с какой-то высокой точностью, а, соответственно,
[02:10:51.920 --> 02:10:56.480]  оффлайн подход требует. Ну дальше возникают технические вопросы, с какой точностью,
[02:10:56.480 --> 02:11:01.840]  все это сейчас нас не беспокоит, но важно, что именно такой способ оценивания, он, ну,
[02:11:01.840 --> 02:11:07.520]  грубо говоря, наилучший. Но грубость тут заключается в том, что я должен сказать асимпатический,
[02:11:07.520 --> 02:11:12.280]  при каких-то оговорках, и наилучший в каком смысле? В смысле наиболее компактного,
[02:11:12.280 --> 02:11:17.280]  доверительного множества, построенного вокруг вот такой оценки. То есть, если у нас есть оценка,
[02:11:17.280 --> 02:11:23.360]  полученная по принципу максимум правдоподобия, то, значит, построенный вокруг нее доверительный
[02:11:23.360 --> 02:11:29.400]  интервал на основе ее вероятностных свойств, он будет наикрочайший. Ну и доверительное множество,
[02:11:29.400 --> 02:11:33.880]  но для множества уже что такое наикрочайшее, это более интересный вопрос, поэтому тут надо как-то
[02:11:33.880 --> 02:11:39.120]  немножко тоньше говорить. Но все это упирается в каком-то смысле вот в то, что это неравенство
[02:11:39.120 --> 02:11:44.040]  Раукрамера, верное для всех оценок, оно достигает равенства в симптотике на оценке максимального
[02:11:44.040 --> 02:11:49.320]  правдоподобия. Вот в этом результат, что тут равенство достигается в неравенстве Раукрамера,
[02:11:49.320 --> 02:11:53.080]  именно на оценке максимального правдоподобия. Возможно, я, кстати, это плохо проговорил,
[02:11:53.080 --> 02:11:58.080]  но оценка максимального правдоподобия, она обладает наимнейшей дисперсией, потому что в
[02:11:58.080 --> 02:12:03.320]  неравенстве Раукрамера достигается равенство. Аналогом такого неравенства для байсовской
[02:12:03.320 --> 02:12:07.240]  постановки является неравенство Вантриса. Оно просто чуть-чуть прайером скорректировано.
[02:12:07.240 --> 02:12:12.760]  Вот он у меня где-то здесь, должен быть этот сюжет. Немножко скорректировано прайером. Ну и мы
[02:12:12.760 --> 02:12:21.000]  поняли, что байсовская регулизация, это по сути, байсовский анализ, это по сути регулизация сама
[02:12:21.000 --> 02:12:27.480]  собой естественная такая. Не всегда обязательно квадратичная, Лапласова плотность априорно
[02:12:27.480 --> 02:12:32.200]  приводит к L1. Но, в общем-то, все это сохраняет как бы вот эту философию,
[02:12:32.200 --> 02:12:38.400]  ну, философию статистики. Но мы от этой философии можем уйти в машины обучения,
[02:12:38.400 --> 02:12:43.120]  где надо просто пастулировать функционал. Вот мы пастулируем какой функционал и забываем про
[02:12:43.120 --> 02:12:48.320]  всякую вероятностную природу. Мы нигде здесь и дальше в рассуждениях не требовали закон на
[02:12:48.320 --> 02:12:52.840]  распределение вероятности. Все, что мы требовали, это возможность посчитать 100 градиент. А посчитать
[02:12:52.840 --> 02:12:59.320]  100 градиент, это значит просто получить сэмпл, дата, данные получить. По сути, это означает,
[02:12:59.320 --> 02:13:04.920]  что у вас есть картинки, вам даже не нужно знать, из какого закона распределения они. Просто надо,
[02:13:04.920 --> 02:13:10.840]  чтобы они были IID, то есть более-менее независимы из одного закона. И на этом сидит все машина
[02:13:10.840 --> 02:13:17.680]  обучения, потому что вот эти суммы, это и есть проявление как бы IID с результатом. То есть,
[02:13:17.760 --> 02:13:23.120]  то есть, что мы заменяем от ожидания выборочным средним, но это можно делать,
[02:13:23.120 --> 02:13:28.360]  если есть независимая схема. Тогда выборочное среднее, это хорошая оценка. Мы же с вами про это
[02:13:28.360 --> 02:13:33.760]  уже говорили. Если как бы есть зависимость в выборке, то это все не так про... То есть,
[02:13:33.760 --> 02:13:38.720]  так нельзя вообще, говорят, делать. Ну и дальше вот ключевой результат был в том, что если есть
[02:13:38.720 --> 02:13:44.960]  сильная выпуклость, тогда все хорошо. Если ее нет, то надо регулиризовать, либо ранняя остановка. В общем-то,
[02:13:44.960 --> 02:13:52.800]  все. То есть, как ни странно, программа какая-то, которую мы хотели выполнить, она вот так вот
[02:13:52.800 --> 02:13:59.600]  относительно быстро может быть выполнена. Но по модулю того, что вы, конечно, сами прочитаете
[02:13:59.600 --> 02:14:05.240]  детали, которые я опустил, моя задача была вас как-то сориентировать, на что надо обратить
[02:14:05.240 --> 02:14:16.560]  внимание. Вот еще раз, внимание надо обратить на то, что, в общем-то, есть естественная процедура
[02:14:16.560 --> 02:14:22.880]  связана с принципом максимума правдоподобия. И она оптимальная. Эта процедура сразу вам говорит
[02:14:22.880 --> 02:14:27.640]  о том, какой функционал. А есть процедура, где вы постулируете функционал, это называется машин
[02:14:27.640 --> 02:14:33.120]  леонинг. А там, где, значит, функционал определяется по плотности и вероятности, это статистика
[02:14:33.120 --> 02:14:37.480]  математическая. Но и там, и там речь идет об обратных задачах в каком-то смысле теории вероятности.
[02:14:37.480 --> 02:14:43.320]  То есть, есть какая-то модель, и вы пытаетесь вскрыть, какие параметры порождает эту модель.
[02:14:43.320 --> 02:14:47.520]  Только в случае статистики это буквально вероятностная модель, а в случае машинного обучения
[02:14:47.520 --> 02:14:54.320]  хитрее. У вас нет параметрической модели, но у вас есть критерий качества, и все зашивается в
[02:14:54.320 --> 02:15:00.200]  критерии качества. То есть, все вот эти вот какие-то там лопласты априорные или еще что-то, все это
[02:15:00.200 --> 02:15:06.620]  вы как бы выдумываете сами. Но как бы статистика вас наводит на мысли, что, ага, квадратичный
[02:15:06.620 --> 02:15:12.560]  штраф отвечает нормальному шуму, L1 какая-то невязка, отвечает лопластову шуму и так далее.
[02:15:12.560 --> 02:15:17.840]  Далее вы, в общем, смотрите, что там как бы более, а какой-нибудь hinge loss там тоже отвечает, ну вот
[02:15:17.840 --> 02:15:23.240]  мы рассматривали какую-то вероятностную модель, по-моему. И как бы дальше весь вопрос в том, что
[02:15:23.240 --> 02:15:29.160]  надо делать, как решать задачу. Стох оптимизации. Есть два подхода. Имперический риск минимизировать,
[02:15:29.160 --> 02:15:37.200]  это выгодный подход, если задача огромных размеров, а хранить ее на одном узле невозможно,
[02:15:37.200 --> 02:15:42.560]  всю эту выборку. Либо задача вот онлайн, когда, например, есть возможность хорошо параллелить,
[02:15:42.560 --> 02:15:48.080]  когда проблем вот с таким стримом нет. То есть, вы получаете поток, обрабатываете,
[02:15:48.080 --> 02:15:51.920]  корректируете оценку. Ну какие же проблемы. Тем более, что это параллельно можно делать.
[02:15:51.920 --> 02:15:56.400]  И вот в зависимости от контекста, что именно вам надо делать, дальше тут начинается наука,
[02:15:56.400 --> 02:16:02.560]  что онлайн подход можно бочировать, допустим, когда гладкость есть, когда гладкости нет,
[02:16:02.560 --> 02:16:07.640]  бочировать нельзя. Можно, значит, какие-то дополнительные трюки там делать, связанные
[02:16:07.640 --> 02:16:14.200]  с выбором прокса, дивергенции. Ну, в общем, мы там какие-то вещи рассматривали. Стендзорные методы
[02:16:14.200 --> 02:16:20.520]  можно использовать, типа Ньютона. То есть, делать как бы шаг какого-нибудь метода Ньютона условно,
[02:16:20.520 --> 02:16:25.120]  но бочировать. Что-то там еще с гессианом связано. Это тоже можно делать. К слову, сказать,
[02:16:25.120 --> 02:16:30.280]  гессиан надо бочировать меньшее число раз, чем градиент. Что интересно, для вот этих тендзорных
[02:16:30.280 --> 02:16:37.040]  методов, это есть некоторое такое наблюдение, чтобы вот было нужное качество, то есть разные
[02:16:37.040 --> 02:16:42.400]  размеры бача для градиента. Ну и неважно, это сейчас я в сторону. Для оффлайн подхода там в
[02:16:42.400 --> 02:16:47.840]  основном проблемы аппроксимации, которые тяжелые, не все до конца сделано, особенно в категории
[02:16:47.840 --> 02:16:53.480]  больших отклонений. Но тоже эти проблемы решаются. Ну и основной тут отрицательный результат,
[02:16:53.480 --> 02:16:59.480]  что есть оценка, которая не улучшаема. Эта оценка говорит о том, что вылезает размерность.
[02:16:59.480 --> 02:17:05.360]  Это оценка на число слагаемых, которые надо брать в негладком случае, для выпуклой задачи
[02:17:05.360 --> 02:17:11.200]  сток-оптимизации. И вылезает фактор N, чтобы сумма аппроксимировала ее идеальное решение,
[02:17:11.200 --> 02:17:18.480]  давала решение исходной задачи сток-оптимизации. Это фактор N, он плохой. И ровно потому,
[02:17:18.480 --> 02:17:25.200]  что он плохой, задачу регулирует, что он мешает сохранить тот же эффект, что и в онлайн-оптимизации,
[02:17:25.200 --> 02:17:30.800]  чтобы вот этого было достаточно. В оффлайн подходе этого недостаточное, возникает этот фактор. Но
[02:17:30.800 --> 02:17:37.680]  если сделать раннюю остановку процедуры, которая для суммы SGD, то годится. Либо регулиризовать
[02:17:37.680 --> 02:17:42.600]  задачу, и тоже вот этот фактор исчезнет. Вот это яркий сюжет, как бы совершенно неожиданный,
[02:17:42.600 --> 02:17:48.040]  и он много объясняет в реальном машинном обучении, почему люди регулируют, почему они
[02:17:48.040 --> 02:17:53.680]  делают раннюю остановку. Но вот это связано просто с математикой, в том числе с нижними оценками,
[02:17:53.680 --> 02:17:59.560]  о которых мы сегодня с вами поговорили. Поскольку мы не делали перерыва, то заканчивать мне надо не в
[02:17:59.560 --> 02:18:07.120]  20-30, а в 20-20. Ну что, в общем, к чему я и стремился. Ну еще раз покажу вам книжку Рыгородского Литвак,
[02:18:07.120 --> 02:18:14.280]  я ее сброшу. Вот книжка выглядит, значит, таким образом. Математика информационного века,
[02:18:14.280 --> 02:18:21.920]  ну вот как-то так. Но это книжка, которая в электронном виде. Почему-то в бумажном виде ее назвали,
[02:18:21.920 --> 02:18:27.800]  кому нужна математика или зачем нужна математика. Еще бы назвали, кому вообще нужна эта ваша
[02:18:27.800 --> 02:18:33.240]  математика. Ну как-то странно немножко. Но вот в электронном виде то, что версия у меня, название,
[02:18:33.240 --> 02:18:38.800]  по-моему, очень симпатичное. Математика компьютерного века. Но как бы повторю,
[02:18:38.800 --> 02:18:46.560]  что если искать ее по такому названию бумажную версию, вы, скорее всего, не найдете. Потому что
[02:18:46.560 --> 02:18:52.560]  она называется кому нужна математика. Замечу, что тут есть еще другие сюжеты про бигдата,
[02:18:52.560 --> 02:18:57.880]  там миллион аукционов в минуту. Очень красивый сюжет, как устроена реклама в Яндексе и других
[02:18:57.880 --> 02:19:04.240]  сетях. То есть, в принципе, про Тер-САК, там кодирование. Ну в общем, я думаю, что-то из
[02:19:04.240 --> 02:19:11.840]  этого вы, конечно, уже знаете, но не все. Далеко не все. И мне кажется, что, в общем, довольно
[02:19:11.840 --> 02:19:20.720]  интересная эта книжка. И я вам ее рекомендую, естественно. Как вот тоже сюжетная линия. Несколько
[02:19:20.720 --> 02:19:28.000]  сюжетных линий к тому курсу, который мы изучаем. Значит, заметку я сбрасываю, естественно, в нашу
[02:19:28.000 --> 02:19:35.400]  группу, которая у нас математика больших данных, ну телеграм-канал. Ну и на этом, наверное,
[02:19:35.400 --> 02:19:43.680]  лекция заканчивается. Где у нас эта математика? Стальные материалы к лекции я чуть
[02:19:43.680 --> 02:19:56.080]  позже пришлю. Но тут все просто, потому что мы шли по книжке. Если вопросы... Хорошо. Значит,
[02:19:56.120 --> 02:20:00.040]  если вопросов нет, тогда еще раз, на всякий случай, анонс того, что будет дальше. Дальше
[02:20:00.040 --> 02:20:06.640]  следующая среда очная лекция Александра Безносикова. Потом очная лекция Алексея Фролова.
[02:20:06.640 --> 02:20:17.600]  Потом две очных лекции Максима Рахубе. Потом очная онлайн лекция Алексея Наумова. И замечу,
[02:20:17.600 --> 02:20:24.920]  что начало всех лекций в пять часов, как и положено. Это просто... Мне лекция сейчас,
[02:20:25.400 --> 02:20:31.280]  из-за того, что я нахожусь не в Москве, пришлось немножко сместить расписание. Так, в общем,
[02:20:31.280 --> 02:20:35.480]  все будет в пять часов. Только лекция Наумова будет онлайн, а лекции остальных будут очна и,
[02:20:35.480 --> 02:20:39.960]  соответственно, они будут транслироваться онлайн на YouTube. Значит, что касается записи
[02:20:39.960 --> 02:20:44.840]  этого выступления, через буквально, не знаю, полчаса, там может побольше, я ее выложу,
[02:20:44.840 --> 02:20:51.680]  она когда обработается, в варианте зумовской записи с кодом выложу тоже в группу. На YouTube
[02:20:51.680 --> 02:20:56.440]  она будет залита чуть попозже, как я понимаю. Пока придется пользоваться записяю через
[02:20:57.640 --> 02:21:07.320]  соответственно Zoom. Ну хорошо, на этом, наверное, мы на какое-то время прощаемся до того,
[02:21:07.320 --> 02:21:14.560]  что уже начнутся задачи проектов или какие-то вопросы. Я буду доступен для ответов на эти
[02:21:14.560 --> 02:21:19.760]  вопросы, каких-то комментариев, но в целом уже теперь лекции будут читать другие лекторы,
[02:21:20.160 --> 02:21:25.360]  вплоть до конца, до последней лекции, которую прочтет Алексей Наумов по Reinforcement Learning. Я надеюсь,
[02:21:25.360 --> 02:21:30.440]  что если вот все это вы вместе соберете, как-то прочитаете материал, у вас действительно получится
[02:21:30.440 --> 02:21:36.280]  такая некоторая общая картинка, потому что вот те детали, которые я опускал, это ровные детали,
[02:21:36.280 --> 02:21:41.520]  как раз связанные со всякими неравенствами концентрации, вот то, что мы проходили. То есть
[02:21:41.520 --> 02:21:48.280]  опущенные детали, это не пемпердикулярно другим частям курса. Я их опускал потому,
[02:21:48.760 --> 02:21:54.080]  собственно, зачем 10 раз одно и то же. Можно было, конечно, это закрепить, но этим занимаются в
[02:21:54.080 --> 02:21:58.800]  отдельных курсах. В курсах, например, статистической теории обучения, которая читается у нас на кафедре
[02:21:58.800 --> 02:22:04.840]  Moe, там Никита Пучкин, Владимир Вячеславович Югин, допустим, читает что-то похожее. Мне кажется,
[02:22:04.840 --> 02:22:13.680]  что нет цели воспроизвести такие курсы, а цель именно связать разные вещи единой математикой.
[02:22:13.680 --> 02:22:19.520]  Математика машинного обучения — это во многом математика стахастической оптимизации. О чем я хотел
[02:22:19.520 --> 02:22:24.240]  сказать? А математику стахастической оптимизации мы с вами уже немножко поисследовали. И в основном
[02:22:24.240 --> 02:22:29.880]  это вокруг свойств условного мат. ожидания, неравенств концентрации для мартингала разности,
[02:22:29.880 --> 02:22:35.720]  вот и каких-то тривиальных идей оптимизации, типа выпуклости, там гладкости, липшицевости. Вот,
[02:22:35.720 --> 02:22:42.720]  собственно, вся сюжетная линия, которую я хотел обрисовать вокруг этого и дальше,
[02:22:42.720 --> 02:22:46.840]  будут немножко другие сюжеты, даже не немножко, сильно другие. Но, впрочем, сюжет,
[02:22:46.840 --> 02:22:51.120]  который будет Александром Безносиком, он по-прежнему связан с тем, о чем я сейчас говорил.
[02:22:51.120 --> 02:22:58.800]  Тоже будет машинное обучение. Хорошо, коллеги, если нет вопросов, тогда, наверное, мы заканчиваем
[02:22:58.800 --> 02:23:03.760]  лекцию. Вроде всё. Спасибо. Спасибо. До свидания. До свидания.

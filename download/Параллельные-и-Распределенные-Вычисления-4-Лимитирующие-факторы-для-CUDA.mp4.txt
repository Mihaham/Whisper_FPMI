[00:00.000 --> 00:11.000]  Всем доброго дня! Мы с вами опять настраивали технику. Точнее, сегодня мы не настраивали технику,
[00:11.000 --> 00:17.680]  а дожидались, пока эта техника доедет. У нас в процессе съемок присутствует некоторая конвейерность.
[00:17.680 --> 00:22.960]  Раз мы говорим про конвейерность, то давайте поговорим сегодня про то,
[00:22.960 --> 00:27.240]  какие лимитирующие факторы присутствуют в написании приложений на Куте.
[00:27.240 --> 00:35.240]  Мы с вами в прошлый раз ввели логические и физические абстракции, при помощи которых мы с вами можем
[00:35.240 --> 00:43.240]  замерять, так сказать, некоторую производительность. И, по большей части, на семинарах вам, по идее,
[00:43.240 --> 00:50.240]  должны были показать работу профилировщика. Я на своем семинаре показывал. Я запускал Antelita NvProv,
[00:50.240 --> 00:59.240]  в которой мы смотрели скорость работы программы. Но, как известно, начиная с видеокарты NVIDIA 30
[00:59.240 --> 01:07.240]  поколений вида ампер, возникла есть такая история, что этот профилировщик вырубили из стандартной поставки.
[01:07.240 --> 01:13.240]  То есть классической профилировки нет, но нам все-таки необходимо замерить время работы нашей программы.
[01:13.240 --> 01:21.240]  И сразу скажу, что как раз с временем замера программы возникают первые проблемы, потому что мы с вами
[01:21.240 --> 01:27.240]  выясним, что код ядра, который у нас есть, обычно запускается именно ассихронно.
[01:27.240 --> 01:33.240]  Как замерить скорость работы программы на видеокарте? Для этого есть специальный счетчик под названием
[01:33.240 --> 01:43.240]  KudoEventT. Создание события времени – это KudoEventCreate. Оно принимает объект на вход KudoEventT.
[01:43.240 --> 01:50.240]  Дальше у нас с вами есть событие KudoEventRecord. Мы с вами записываем определенное событие.
[01:50.240 --> 01:58.240]  А дальше есть еще одна команда, которая называется KudoEventSynchronize, которая ожидает исполнения события,
[01:58.240 --> 02:06.240]  то есть ставит блокировку на события. Почему это важно? KudoEventRecord и KudoEventSynchronize.
[02:06.240 --> 02:13.240]  Вот смотрите, есть два именно способа замеры, точнее две единицы замера.
[02:13.240 --> 02:21.240]  Почему нам не хватит замерить точку старта, точку конца, вычесть из разницы начала и кажется все хорошо?
[02:21.240 --> 02:36.240]  У кого-то есть какие-нибудь мысли по этому поводу? Почему нельзя взять две точки и замерить разницу между ними?
[02:36.240 --> 02:43.240]  У нас ядро на самом деле выполняется асинхронно, вот это надо понимать.
[02:43.240 --> 02:52.240]  И вот давайте, чтобы вы понимали, как запускается код на видеокарту.
[02:52.240 --> 02:57.240]  Минутка компилятора KudoEventRecord.
[02:57.240 --> 03:04.240]  Значит, смотрите, когда мы с вами говорим про код ядра, то вы видели...
[03:04.240 --> 03:13.240]  Так, а есть здесь те, кто не на мысль многое? Ага, вам просматривали код ядра на KudoEventRecord?
[03:13.240 --> 03:18.240]  Нет, у вас еще не было, да? Ну давайте тогда я расскажу.
[03:18.240 --> 03:25.240]  Значит, у нас есть код функции IntMain.
[03:25.240 --> 03:33.240]  И дальше вам нужно объявить некоторую функцию, которая называется GlobalVoid.
[03:33.240 --> 03:37.240]  Значит, это код ядра.
[03:37.240 --> 03:40.240]  Что-то одно, допустим, принимает количество элементов.
[03:40.240 --> 03:42.240]  А, в этом смысле, да.
[03:42.240 --> 03:47.240]  Вот, вот эта штука называется ядром.
[03:52.240 --> 03:57.240]  И когда вы запускаете код, то на самом деле происходит следующее.
[03:57.240 --> 04:03.240]  Вы вызываете функцию это с некоторыми параметрами.
[04:03.240 --> 04:06.240]  Значит, это параметры конфигурации ядра.
[04:06.240 --> 04:09.240]  Значит, здесь мы указываем число блоков.
[04:09.240 --> 04:12.240]  Вот, число потоков.
[04:12.240 --> 04:17.240]  Это у нас называется блок DIMM.
[04:17.240 --> 04:23.240]  А вот это у нас количество этих блоков.
[04:27.240 --> 04:28.240]  Гринди.
[04:28.240 --> 04:32.240]  Значит, теперь как происходит процесс компиляции в Kudo программу.
[04:32.240 --> 04:35.240]  На самом деле, он заключается в следующем.
[04:35.240 --> 04:39.240]  Что здесь код по факту делится на две основных части.
[04:39.240 --> 04:43.240]  Одна часть едет в код компиляции под видеокарту.
[04:43.240 --> 04:47.240]  Вторая часть едет в код компиляции под CPU.
[04:47.240 --> 04:52.240]  То есть, на самом деле, что у нас происходит под капотом.
[04:52.240 --> 04:58.240]  То, что помещается в модификатор Global и Local, это на самом деле
[04:58.240 --> 05:02.240]  суммарный модификатор device plus host.
[05:05.240 --> 05:08.240]  Просто модификатор host обычно опускается.
[05:08.240 --> 05:13.240]  То, что помещено идентификатором device, едет на само устройство.
[05:13.240 --> 05:16.240]  И компилируется именно внутри видеокарты.
[05:16.240 --> 05:18.240]  То есть, для запуска на видеокарту.
[05:18.240 --> 05:25.240]  То, что помещено ключевым словом host, появляется как доступ кода на ЦПУ.
[05:25.240 --> 05:29.240]  Причем, когда мы указываем код именно самого ядра,
[05:29.240 --> 05:34.240]  по факту у вас происходит что-то похожее на процесс линковки.
[05:35.240 --> 05:38.240]  Вспоминаем курс по технологиям программирования.
[05:38.240 --> 05:40.240]  Как происходит линковка, берегляйте.
[05:40.240 --> 05:46.240]  Тут статическая все-таки линковка.
[05:46.240 --> 05:48.240]  Что у нас происходит?
[05:48.240 --> 05:52.240]  Мы с вами продолжаем. У нас сегодня день технических проблем.
[05:52.240 --> 05:53.240]  Но ничего страшного.
[05:53.240 --> 05:57.240]  Смотрите еще раз. Что у нас с вами происходит?
[05:57.240 --> 06:01.240]  Когда мы запускаем код на девайсе, он компилируется под девайс.
[06:01.240 --> 06:04.240]  То есть, именно код будет именно самой библиотеки.
[06:04.240 --> 06:10.240]  Здесь у нас просто будет declaration функции add, что она у нас есть и она есть на видеокарте.
[06:10.240 --> 06:15.240]  И по факту, когда мы запускаем функцию add, мы создаем объект типа future.
[06:18.240 --> 06:20.240]  Только этот объект типа future неявный.
[06:20.240 --> 06:25.240]  То есть, мы с вами не сможем явно контролировать результат работы этого ядра.
[06:26.240 --> 06:28.240]  То есть, явно получить результат.
[06:28.240 --> 06:31.240]  То есть, мы сможем получить результат только в том случае,
[06:31.240 --> 06:36.240]  когда мы с вами поставим какую-то точку синхронизации или какой-то барьер.
[06:39.240 --> 06:41.240]  Это необходимо понимать.
[06:41.240 --> 06:44.240]  Поэтому нам необходимо всегда ждать события.
[06:44.240 --> 06:48.240]  Либо после точки установки события делать некоторую синхронную операцию.
[06:50.240 --> 06:53.240]  Давайте код посмотрим еще раз.
[06:53.240 --> 06:55.240]  Ядро у нас выполняется синхронно.
[06:55.240 --> 07:02.240]  И смотрите, если вы напишите, образно говоря, здесь создадите рекорд.
[07:03.240 --> 07:05.240]  Здесь сделайте рекорд.
[07:08.240 --> 07:17.240]  Если вы сразу возьмете delta равно t-end-t-start, то он вам покажет время 0.
[07:17.240 --> 07:23.240]  Потому что истинные моменты t-start и t-end у нас здесь не появятся.
[07:23.240 --> 07:29.240]  Давайте как раз чтобы вот это все визуально было, я нарисую картинку.
[07:35.240 --> 07:38.240]  Которая покажет всю магию нашего процесса.
[07:38.240 --> 07:40.240]  То есть, у нас с вами...
[07:40.240 --> 07:42.240]  Это у нас хост.
[07:44.240 --> 07:46.240]  Это у нас девайс.
[07:49.240 --> 07:51.240]  Нарисуем диаграмму последовательности.
[07:53.240 --> 07:59.240]  У нас с вами, когда мы запускаем какое-то ядро,
[08:02.240 --> 08:06.240]  в нашем случае функция add, это функция,
[08:07.240 --> 08:09.240]  в нашем случае функция add.
[08:09.240 --> 08:11.240]  Эта функция является синхронной.
[08:11.240 --> 08:13.240]  То есть, у нас не будет никакого результата.
[08:13.240 --> 08:20.240]  После этого, в какой-то момент времени, мы можем вызвать еще одну функцию add2.
[08:23.240 --> 08:28.240]  И она именно станет именно в этот момент времени.
[08:28.240 --> 08:34.240]  То есть, у нас получается время на cpu, время на gpu идет не таким образом, как обычно.
[08:34.240 --> 08:37.240]  Соответственно, если мы ставим здесь рекорд,
[08:41.240 --> 08:43.240]  а после этого ставим здесь рекорд,
[08:46.240 --> 08:49.240]  то есть, это у нас t1, а вот это t2,
[08:49.240 --> 08:56.240]  то понятно, что у нас t1 и t2 фактически на видеокарте еще не были исполнены.
[08:56.240 --> 09:00.240]  То есть, момент t1, здесь он будет t1 со звездочкой,
[09:00.240 --> 09:04.240]  а вот здесь момент t2 будет здесь, находится здесь.
[09:04.240 --> 09:06.240]  То есть, этот момент t2 со звездочкой.
[09:06.240 --> 09:08.240]  То есть, он у нас по факту не наступил.
[09:08.240 --> 09:14.240]  Оно будет обновляться только в тот момент времени, когда вы на видеокарте получите событие.
[09:16.240 --> 09:21.240]  То есть, вначале оно заполняется текущим временем, но после этого оно заполняется актуальным временем.
[09:21.240 --> 09:26.240]  Соответственно, какая из операций является блокирующей, которую мы с вами прошли?
[09:26.240 --> 09:28.240]  Первое, это CUDA event synchronize.
[09:35.240 --> 09:36.240]  Это раз.
[09:36.240 --> 09:40.240]  Тогда вы дождетесь завершения события, поставите блокировку.
[09:40.240 --> 09:44.240]  Но лучше, опять же, эту блокировку ставить после выполнения всяких ядер,
[09:44.240 --> 09:47.240]  потому что иначе это нарушает очередь исполнения.
[09:47.240 --> 09:53.240]  А второй способ, есть еще полный синхронайз, называется CUDA синхронайз.
[09:53.240 --> 09:55.240]  Я не советую его делать.
[09:55.240 --> 10:00.240]  В какой-то момент времени из-за этого очень сильно страдал библиотека PyTorch.
[10:00.240 --> 10:07.240]  Потому что после вызова ядра там была функция .cpu, то есть тендер вернуть на цепу.
[10:07.240 --> 10:09.240]  И что там, фактически, происходило?
[10:09.240 --> 10:12.240]  Там просто CUDA синхронайз на всю видеокарту.
[10:12.240 --> 10:16.240]  Ставился глобальный блокировщик и скидывал спавнную цепу.
[10:16.240 --> 10:19.240]  Из-за этого конвертация была очень тяжелой.
[10:19.240 --> 10:24.240]  А в TensorFlow в тот момент времени все было написано достаточно аккуратно через механизм стримов.
[10:24.240 --> 10:28.240]  Вот как раз механизм стримов мы через лекцию посмотрим.
[10:28.240 --> 10:35.240]  И оказалось следующее, что если мы поставим после этого какую-то синхронную операцию,
[10:35.240 --> 10:39.240]  то, в принципе, синхронайз делать не надо.
[10:39.240 --> 10:45.240]  И, как ни странно, одной из такой синхронных операций это MemCopy.
[10:45.240 --> 10:51.240]  То есть после MemCopy, если вы обращаетесь с каким-то элементом массива,
[10:51.240 --> 10:56.240]  а там именно вот эти вот события будут записаны на массивы.
[10:56.240 --> 11:01.240]  То есть у вас по факту построится граф вычислений под капотом видеокарты, потому что компилятор другой.
[11:01.240 --> 11:06.240]  И, собственно, когда вы запросите MemCopy определенного куска массива,
[11:06.240 --> 11:12.240]  с которым вы производили операцию, то для этого массива будет как раз блокирующая операция.
[11:12.240 --> 11:21.240]  То есть у нас получается, допустим, у нас есть массив dA, мы делаем MemCopy,
[11:21.240 --> 11:36.240]  дальше kernel, еще kernel, и дальше MemCopy.
[11:36.240 --> 11:43.240]  Вот, тогда после этого MemCopy у нас возникнет синхронная операция,
[11:43.240 --> 11:49.240]  при помощи которой мы уже получим завершение всех событий, которые были до этого момент.
[11:49.240 --> 11:55.240]  Так, понятен ли вот этот тезис?
[11:55.240 --> 12:00.240]  Ну, что, типа, если умеряем время, то нужно именно использовать копирование времени.
[12:00.240 --> 12:06.240]  Вот, то есть мы ставим трекер в прохождении дистанции, а потом на финише уже сверяем часы.
[12:06.240 --> 12:12.240]  Вот приблизительно вот такой вот код мы с вами увидим, если мы напишем на видеокарте.
[12:12.240 --> 12:19.240]  То есть мы ставим куды event start, куды event start, куды event start.
[12:19.240 --> 12:24.240]  Значит, создаем два события, записываем точку старта, записываем точку остановки,
[12:24.240 --> 12:29.240]  после этого делаем куды MemCopy, потом делаем куды event synchronize,
[12:29.240 --> 12:34.240]  и дальше есть точка за мерами времени, куды event elapsed time.
[12:34.240 --> 12:39.240]  Скажите, пожалуйста, какая строчка на самом деле здесь является лишней?
[12:39.240 --> 12:44.240]  В этом коде.
[12:44.240 --> 12:49.240]  Ну вот, оно на самом деле не нужно.
[12:49.240 --> 12:52.240]  То есть зачем нам синхронизировать события stop?
[12:52.240 --> 13:00.240]  Если у нас копирование массива dy уже произойдет после выполнения гидра.
[13:00.240 --> 13:08.240]  То, что dy у нас как раз завязано на копирование, и мы в результате тоже скопируем dy.
[13:08.240 --> 13:13.240]  Так, понятно ли вот это вот код?
[13:13.240 --> 13:18.240]  Вам скорее всего очень много раз придется им пользоваться.
[13:18.240 --> 13:23.240]  А?
[13:23.240 --> 13:25.240]  Какая строка?
[13:25.240 --> 13:30.240]  Cuda event record stop.
[13:30.240 --> 13:35.240]  Cuda event record stop.
[13:35.240 --> 13:39.240]  А затем, что все-таки нам нужно поставить отсечку времени.
[13:39.240 --> 13:48.240]  То есть мы выполняем гидро, нам нужно отставить момент времени, когда оно закончилось.
[13:48.240 --> 13:53.240]  То есть мы здесь замеряем время гидра под названием edit.
[13:53.240 --> 13:58.240]  Или я иначе не понял?
[13:58.240 --> 14:03.240]  А?
[14:03.240 --> 14:05.240]  Хорошо.
[14:05.240 --> 14:10.240]  А теперь давайте посчитаем количество операций, которые у нас используются.
[14:10.240 --> 14:13.240]  И здесь будет еще один интересный факт.
[14:13.240 --> 14:26.240]  Вам по идее на семинарах должны были показать, что код сложения двух массивов работает приблизительно за 10 с количеством итераций, равняемым 10 десятой.
[14:26.240 --> 14:32.240]  То есть 10 десятой, а мощность видеокарты все-таки не 10 десятой, а 10 двенадцатой.
[14:32.240 --> 14:36.240]  Вот наша цель как раз разобраться с тем, а где здесь возникает проблема.
[14:36.240 --> 14:44.240]  Итак, смотрите, на видеокарте RTX 2080 Ti, которая стоит на нашем кластере, 4352 ядра.
[14:44.240 --> 14:52.240]  Частота ядра 1,5 ГГц.
[14:52.240 --> 15:01.240]  На самом деле видеокарты настроены таким образом, что в целом они могут прогонять две операции за один цикл.
[15:01.240 --> 15:03.240]  За счет дублирования.
[15:03.240 --> 15:06.240]  И в итоге мы получаем вот такую мощность видеокарты.
[15:06.240 --> 15:12.240]  То есть мы можем количество ядер умножить на частоту нашего одного ядра, умножить на количество тактов.
[15:12.240 --> 15:15.240]  А операции за один такт.
[15:15.240 --> 15:20.240]  Получаем 13,5 Tera floating operations per second.
[15:20.240 --> 15:22.240]  Flops.
[15:25.240 --> 15:29.240]  Это сколько получается?
[15:29.240 --> 15:32.240]  Tera, да.
[15:32.240 --> 15:34.240]  10 двенадцатый.
[15:34.240 --> 15:39.240]  13 на 10 двенадцатый, мы должны получить 10 тринадцатой операции в секунду.
[15:45.240 --> 15:50.240]  В смысле переполнить?
[15:50.240 --> 15:52.240]  В смысле переполнить?
[15:52.240 --> 15:55.240]  10 тринадцатый, 14 битный, счетчика не ждет.
[15:55.240 --> 15:58.240]  10 восемнадцатый.
[15:58.240 --> 16:03.240]  Ну да, достаточно быстро.
[16:03.240 --> 16:06.240]  Не, я говорю к тому, что у нас все равно 10 десятый.
[16:06.240 --> 16:08.240]  Получилось на замерах.
[16:08.240 --> 16:10.240]  Чем приходится платить?
[16:15.240 --> 16:18.240]  Памяти необходимо платить.
[16:18.240 --> 16:20.240]  Скорость доступа к памяти.
[16:20.240 --> 16:24.240]  То есть скорость доступа к памяти начинает играть в лимитирующий фактор.
[16:24.240 --> 16:25.240]  А что с памяти?
[16:25.240 --> 16:28.240]  Давайте посчитаем такую величину, как пропускная способность.
[16:28.240 --> 16:32.240]  Значит, у нас с вами пример с AXP.
[16:32.240 --> 16:36.240]  Значит, это прямо...
[16:36.240 --> 16:40.240]  Расшифровываться как S равно AX плюс Y.
[16:40.240 --> 16:43.240]  Итак, мы с вами векторно выкопали.
[16:43.240 --> 16:48.240]  Итак, мы с вами векторно выполняем операцию AX плюс Y.
[16:48.240 --> 16:52.240]  Давайте посчитаем количество операций, которые у нас с вами здесь происходят.
[16:52.240 --> 16:55.240]  X, Y и S это массивы.
[16:59.240 --> 17:03.240]  Сколько операций чтения массива у нас произойдет здесь?
[17:03.240 --> 17:05.240]  Во-первых, нам нужно будет прочитать...
[17:05.240 --> 17:08.240]  Первое, что нам нужно будет прочитать, X.
[17:10.240 --> 17:11.240]  Да.
[17:11.240 --> 17:14.240]  Read Y, Write S.
[17:16.240 --> 17:19.240]  Что мы с вами получаем?
[17:19.240 --> 17:25.240]  Значит, здесь у нас получается N, здесь N, здесь N.
[17:25.240 --> 17:28.240]  В сумме у нас получается 3 N операции.
[17:28.240 --> 17:32.240]  Но тут важно не количество операций, а количество байт.
[17:32.240 --> 17:33.240]  Которые мы используем.
[17:33.240 --> 17:38.240]  Сразу говорю, что мы работаем с вами с 32-битными числами.
[17:38.240 --> 17:43.240]  С 34-битными числами не надо работать, потому что они все ломают.
[17:43.240 --> 17:47.240]  В итоге мы получаем с вами 12 N операций.
[17:47.240 --> 17:52.240]  12 N байт, которые необходимо прогнать через код нашей видеокарты.
[17:53.240 --> 17:57.240]  12 N байт, которые необходимо прогнать через код нашей видеокарты.
[17:59.240 --> 18:00.240]  Логично?
[18:03.240 --> 18:06.240]  Давайте посчитаем, сколько это будет.
[18:08.240 --> 18:12.240]  Мы с вами получаем следующую величину...
[18:12.240 --> 18:19.240]  А да, это сколько мы на практике будем с вами должны прочитать операции, 12 N байт, зафиксировать.
[18:19.240 --> 18:22.240]  Теперь посчитаем пропускную способность видеокарты.
[18:22.240 --> 18:26.240]  Это на самом деле максимальная величина по объему памяти,
[18:26.240 --> 18:29.240]  которая видеокарта может прогонять за единицу времени.
[18:29.240 --> 18:31.240]  Точнее, за секунду.
[18:31.240 --> 18:33.240]  Значит, как она измеряется?
[18:33.240 --> 18:35.240]  Она измеряет следующим образом.
[18:35.240 --> 18:41.240]  Она измеряет частота оперативной памяти на ширину шины данных.
[18:41.240 --> 18:45.240]  То есть у нас данные в оперативную память гоняются по шине.
[18:45.240 --> 18:49.240]  Попробуем положить на эффективность этой памяти.
[18:49.240 --> 18:55.240]  Первая. Частота оперативной памяти видеокарты где-то 1600-1700 ГГц.
[18:57.240 --> 19:06.240]  На ЦПУ мощность частота памяти где-то 2,6-3 ГГц.
[19:06.240 --> 19:10.240]  Здесь меньше, но сравнимо с частотой процессора.
[19:15.240 --> 19:21.240]  В раме 2,6 ГГц.
[19:31.240 --> 19:34.240]  Тут говорят 1750 ГГц.
[19:38.240 --> 19:42.240]  В раме частоту 1,75 ГГц.
[19:45.240 --> 19:50.240]  Вот. То есть видно, что мощность.
[19:50.240 --> 19:53.240]  Давайте теперь сравним еще по одному фактору.
[19:53.240 --> 19:56.240]  Какая мощность частота ЦПУ?
[20:00.240 --> 20:03.240]  Где-то 3 ГГц сейчас стабильно можно выжимать.
[20:03.240 --> 20:05.240]  На десктопах версии.
[20:05.240 --> 20:12.240]  Здесь у нас мощность ГПУ это 1,5 ГГц.
[20:12.240 --> 20:16.240]  А теперь посчитаем количество.
[20:16.240 --> 20:20.240]  Сколько ядер там? Справа на ЦПУ.
[20:20.240 --> 20:23.240]  Ну 8-24.
[20:26.240 --> 20:28.240]  Да нет.
[20:28.240 --> 20:31.240]  Сейчас можно AMD.
[20:33.240 --> 20:36.240]  Да-да-да. Мы именно такие трэду считаем.
[20:36.240 --> 20:39.240]  Значит здесь у нас получается 4352.
[20:39.240 --> 20:41.240]  Ну вот.
[20:41.240 --> 20:45.240]  Если у нас на центральных компьютерах ЦПУ и RAM
[20:45.240 --> 20:48.240]  могут достаточно быстро друг друга и обеспечивать
[20:48.240 --> 20:52.240]  по ресурсам, потому что пропускной способности хватает,
[20:52.240 --> 20:57.240]  потому что они могут кормить эти 8 ядер,
[20:57.240 --> 21:01.240]  то здесь кормить с такой же скоростью 4000 ядер
[21:01.240 --> 21:05.240]  это большой-большой вопрос.
[21:06.240 --> 21:10.240]  То есть вы не успеете просто с такой же частотой
[21:10.240 --> 21:12.240]  обеспечивать ресурсами.
[21:12.240 --> 21:15.240]  А главное это подносить ресурсы для вычислений.
[21:15.240 --> 21:17.240]  Вот. Поэтому здесь возникает проблема.
[21:17.240 --> 21:20.240]  И как раз пропускная способность
[21:20.240 --> 21:23.240]  она собирается таким образом.
[21:23.240 --> 21:26.240]  Я показываю специфику видеокарты 2080Т
[21:26.240 --> 21:28.240]  и можно посчитать.
[21:28.240 --> 21:32.240]  Значит частота данных битной шины это 352,
[21:32.240 --> 21:34.240]  но при этом бита.
[21:34.240 --> 21:38.240]  А эффективность видеокарты равняется 8.
[21:38.240 --> 21:43.240]  И здесь нужно лезть именно в железки компьютера
[21:43.240 --> 21:48.240]  и понимать, что мы говорим про память.
[21:48.240 --> 21:52.240]  Что такое DDR, я хочу вас спросить.
[21:52.240 --> 21:54.240]  Знаете ли вы?
[21:54.240 --> 22:01.240]  Да, DDR2, DDR3, DDR4.
[22:01.240 --> 22:03.240]  А?
[22:03.240 --> 22:07.240]  Как расшифровывается DDR?
[22:07.240 --> 22:13.240]  Так, там три.
[22:13.240 --> 22:19.240]  Последний это rate.
[22:19.240 --> 22:21.240]  Double data rate.
[22:21.240 --> 22:23.240]  То есть за один такт прогона
[22:23.240 --> 22:28.240]  мы можем доставить память по факту по шине данных.
[22:28.240 --> 22:30.240]  Мы можем проставить две операции.
[22:30.240 --> 22:31.240]  Итак, смотрите.
[22:31.240 --> 22:33.240]  На самом деле здесь получается,
[22:33.240 --> 22:36.240]  что в современных видеокартах 2080Т
[22:36.240 --> 22:43.240]  используется видеокарт памяти DDR6.
[22:43.240 --> 22:45.240]  О, Господи.
[22:45.240 --> 22:49.240]  Сейчас я отвечу на звонок.
[22:49.240 --> 22:54.240]  Да вы задолбали, ладно.
[22:54.240 --> 23:00.240]  DDR6, 4 передачи за цикл у нас получаются.
[23:00.240 --> 23:02.240]  Вот.
[23:02.240 --> 23:04.240]  И double data rate у нас возникает.
[23:04.240 --> 23:06.240]  Особенности DDR передач мы еще получаем,
[23:06.240 --> 23:08.240]  две передачи дополнительно.
[23:08.240 --> 23:11.240]  То есть шестая, шестой поколение DDR
[23:11.240 --> 23:14.240]  нам дает на один цикл, так сказать, 4 передачи,
[23:14.240 --> 23:17.240]  а double data rate нам дает еще две передачи.
[23:17.240 --> 23:21.240]  В итоге получаем 4 на 2, 8.
[23:21.240 --> 23:23.240]  То есть за один такт на самом деле мы можем
[23:23.240 --> 23:27.240]  перегнать в 8 раз что-то по шине данных.
[23:27.240 --> 23:29.240]  Опять же, где это читать, это нужно читать
[23:29.240 --> 23:32.240]  в спецификах архитектуры видеокарт.
[23:32.240 --> 23:34.240]  Точнее, оперативной памяти видеокарт.
[23:34.240 --> 23:37.240]  И в итоге мы с вами получаем следующую величину.
[23:37.240 --> 23:39.240]  616 ГБ в секунду.
[23:39.240 --> 23:43.240]  Давайте, чтобы вы меня не считали арманщиком.
[23:43.240 --> 23:49.240]  1750 на 352.
[23:51.240 --> 23:53.240]  616, это в гигабайт.
[23:59.240 --> 24:01.240]  Ну что, кажется ли вам эта скорость большой?
[24:05.240 --> 24:06.240]  А?
[24:06.240 --> 24:08.240]  Вообще да, но для видеокарты это мало.
[24:08.240 --> 24:10.240]  А теперь давайте посчитаем следующую вещь.
[24:10.240 --> 24:11.240]  Очень простую.
[24:11.240 --> 24:13.240]  Эффективная пропускная способность.
[24:13.240 --> 24:15.240]  Это количество байт на единицу времени,
[24:15.240 --> 24:17.240]  которое мы с вами перегоняем.
[24:17.240 --> 24:21.240]  И получается, что нам нужно посчитать количество байт,
[24:21.240 --> 24:23.240]  которые необходимы для всех операций
[24:23.240 --> 24:24.240]  чтения и записи.
[24:24.240 --> 24:26.240]  И здесь мы получаем следующую основную часть.
[24:26.240 --> 24:30.240]  Мы читаем X, читаем Y, пишем Y.
[24:30.240 --> 24:33.240]  Получаем 3n операции, 12n байт.
[24:33.240 --> 24:36.240]  А теперь смотрите, 228 байт.
[24:36.240 --> 24:38.240]  28 на 3 на 4.
[24:40.240 --> 24:42.240]  Сколько это у нас получается?
[24:42.240 --> 24:44.240]  Это у нас в байтах.
[24:44.240 --> 24:48.240]  Делим на 1024, делим на 1024.
[24:54.240 --> 24:56.240]  Получаем, нам надо перегнать 3 гигабайта.
[24:59.240 --> 25:01.240]  Для того, чтобы вычислить эту задачу.
[25:01.240 --> 25:03.240]  3 гигабайта.
[25:03.240 --> 25:07.240]  Мощность этой оперативной памяти,
[25:07.240 --> 25:11.240]  частота, с которой мы гоним эту оперативную память,
[25:13.240 --> 25:15.240]  это количество элементов массива.
[25:15.240 --> 25:17.240]  Здесь примеры будут для N228.
[25:17.240 --> 25:19.240]  И примеры семинаров тоже.
[25:19.240 --> 25:21.240]  Все для N228.
[25:21.240 --> 25:23.240]  А теперь давайте посмотрим,
[25:23.240 --> 25:25.240]  что у нас получается.
[25:25.240 --> 25:27.240]  Это количество элементов массива.
[25:27.240 --> 25:29.240]  Здесь примеры будут для N228.
[25:29.240 --> 25:31.240]  И примеры семинаров тоже.
[25:31.240 --> 25:33.240]  Все для N228.
[25:33.240 --> 25:35.240]  То есть мы складываем 2,28,
[25:35.240 --> 25:37.240]  вектор размера 2,28,
[25:37.240 --> 25:39.240]  вот таким вот способом.
[25:41.240 --> 25:43.240]  Ну и что? Давайте посчитаем.
[25:43.240 --> 25:45.240]  Теперь следующая вещь.
[25:45.240 --> 25:47.240]  Мы берем 3, делим на 616.
[25:49.240 --> 25:51.240]  Сколько мы получаем?
[25:51.240 --> 25:53.240]  Мы получаем с вами 4,8 мс.
[25:55.240 --> 26:01.240]  А время работы этого ядра
[26:01.240 --> 26:03.240]  по замерам?
[26:03.240 --> 26:05.240]  Смотрите, сколько.
[26:09.240 --> 26:11.240]  5,7 мс.
[26:13.240 --> 26:15.240]  То есть какую долю времени
[26:15.240 --> 26:17.240]  мы тратим на то, чтобы посчитать
[26:19.240 --> 26:23.240]  просто брать элементы из памяти?
[26:25.240 --> 26:31.240]  Ну, что нам нужно сделать?
[26:33.240 --> 26:35.240]  Одно на второе поделить?
[26:41.240 --> 26:43.240]  Да, то есть смотрите, 83% времени
[26:43.240 --> 26:45.240]  во время вычислений ядра
[26:45.240 --> 26:47.240]  мы занимаемся тем,
[26:47.240 --> 26:49.240]  что читаем элементы из памяти.
[26:49.240 --> 26:51.240]  То есть это очень медленная операция.
[26:51.240 --> 26:53.240]  То есть как минимум там
[26:53.240 --> 26:55.240]  можно еще быстрее сделать,
[26:55.240 --> 26:57.240]  и в итоге у нас получается,
[26:57.240 --> 26:59.240]  что вот по факту перещелкиванием
[26:59.240 --> 27:01.240]  элементов, складыванием, перемножением
[27:01.240 --> 27:03.240]  мы занимаемся на самом деле
[27:03.240 --> 27:05.240]  от силы там 1 мс.
[27:05.240 --> 27:07.240]  То есть как минимум вот то 10,10
[27:07.240 --> 27:09.240]  можно легко превратить 10 в 11,
[27:09.240 --> 27:11.240]  а то и даже в 10,12.
[27:11.240 --> 27:13.240]  Потому что мы владеем
[27:13.240 --> 27:15.240]  немедленностью.
[27:15.240 --> 27:17.240]  То есть смотрите, сейчас что произойдет.
[27:17.240 --> 27:19.240]  Если вы на видеокарте запустите
[27:19.240 --> 27:21.240]  тот же самый код,
[27:21.240 --> 27:23.240]  ну а просто сделаете фор,
[27:23.240 --> 27:25.240]  эффективно, так сказать,
[27:25.240 --> 27:27.240]  запустите всю ту же сетку,
[27:27.240 --> 27:29.240]  то работать это будет в разы быстрее,
[27:29.240 --> 27:31.240]  а не 10,10 это раза в секунду.
[27:33.240 --> 27:35.240]  То есть память у нас является
[27:35.240 --> 27:37.240]  лимитирующим фактором для работы
[27:37.240 --> 27:39.240]  с видеокартой.
[27:39.240 --> 27:41.240]  Так, это понятно?
[27:41.240 --> 27:43.240]  А значит, что если у нас память
[27:43.240 --> 27:45.240]  является лимитирующим фактором,
[27:45.240 --> 27:47.240]  то что нам нужно сделать с памятью?
[27:49.240 --> 27:51.240]  Нет, детально изучить.
[27:53.240 --> 27:55.240]  Поэтому следующий фактор, который у нас
[27:55.240 --> 27:57.240]  будет, это следующий
[27:57.240 --> 27:59.240]  способ. Значит, первый способ,
[27:59.240 --> 28:01.240]  который позволяет это все сделать,
[28:01.240 --> 28:03.240]  это заблокировать память
[28:03.240 --> 28:05.240]  на хосте. Что это означает?
[28:05.240 --> 28:07.240]  Это означает, что мы с вами можем
[28:07.240 --> 28:09.240]  по факту сказать, что вот
[28:09.240 --> 28:11.240]  та память, которая используется
[28:11.240 --> 28:13.240]  там для трансферинга данных
[28:13.240 --> 28:15.240]  видеокарт, она у нас
[28:15.240 --> 28:17.240]  блокируется,
[28:17.240 --> 28:19.240]  и мы выполняем ядро. Это каким
[28:19.240 --> 28:21.240]  образом можно ускорить
[28:21.240 --> 28:23.240]  кудомимцпай операцию?
[28:23.240 --> 28:25.240]  Потому что операция кудомимцпай тоже работает очень долго.
[28:31.240 --> 28:33.240]  Во-первых, чтобы не делать синхронизацию,
[28:33.240 --> 28:35.240]  да, именно для того,
[28:35.240 --> 28:37.240]  чтобы не делать синхронизацию на кудомимцпай,
[28:37.240 --> 28:39.240]  использовать операцию кудомимцпай оси.
[28:39.240 --> 28:41.240]  Но
[28:41.240 --> 28:43.240]  как работает классическая
[28:43.240 --> 28:45.240]  модель в памяти?
[28:45.240 --> 28:47.240]  Вы знаете, наверное, из курса
[28:47.240 --> 28:49.240]  операционных систем, что вся память
[28:49.240 --> 28:51.240]  оперативная, она бьется на куски страниц.
[28:53.240 --> 28:55.240]  Эта память называется пейджибл.
[28:55.240 --> 28:57.240]  Так вот, видеокарта позволяет сделать так,
[28:57.240 --> 28:59.240]  чтобы те куски памяти,
[28:59.240 --> 29:01.240]  которые у нас есть, были не пейджибл,
[29:01.240 --> 29:03.240]  а были пинт памятью.
[29:03.240 --> 29:05.240]  Что это позволяет делать?
[29:05.240 --> 29:07.240]  Это аналог функции mlock.
[29:07.240 --> 29:09.240]  Я знаю,
[29:09.240 --> 29:11.240]  рассказывали вам про нее
[29:11.240 --> 29:13.240]  на курсе операционных систем.
[29:15.240 --> 29:17.240]  Прямо вызов mlock функции,
[29:17.240 --> 29:19.240]  memorylock.
[29:25.240 --> 29:27.240]  По факту, что позволяет сделать
[29:27.240 --> 29:29.240]  mlock? Она говорит вам следующее,
[29:29.240 --> 29:31.240]  что ваша память не может быть выгружена
[29:31.240 --> 29:33.240]  в swap.
[29:33.240 --> 29:35.240]  То есть у вас, по факту, память
[29:35.240 --> 29:37.240]  не модифицируется
[29:37.240 --> 29:39.240]  вне механизма
[29:39.240 --> 29:41.240]  чтения.
[29:41.240 --> 29:43.240]  И тогда вы можете сделать следующее.
[29:43.240 --> 29:45.240]  Вы можете создать
[29:45.240 --> 29:47.240]  классическую пейджибл память,
[29:47.240 --> 29:49.240]  далее вы делаете
[29:49.240 --> 29:51.240]  пинт память параллельно с ней
[29:51.240 --> 29:53.240]  и делаете копирование
[29:53.240 --> 29:55.240]  из пинт памяти
[29:55.240 --> 29:57.240]  в память видеокарты.
[29:57.240 --> 29:59.240]  Пинт память — это память,
[29:59.240 --> 30:01.240]  которая не подвержена
[30:01.240 --> 30:03.240]  механизму
[30:03.240 --> 30:05.240]  работы со страницами.
[30:05.240 --> 30:07.240]  То есть вы ее фиксируете
[30:07.240 --> 30:09.240]  в определенном месте
[30:09.240 --> 30:11.240]  и по факту отдаете ее
[30:11.240 --> 30:13.240]  на откуп управления
[30:13.240 --> 30:15.240]  не уже операционной системой.
[30:15.240 --> 30:17.240]  Как это выглядит?
[30:23.240 --> 30:25.240]  Да, ее прям многому не выделить.
[30:25.240 --> 30:27.240]  Но это не может быть
[30:27.240 --> 30:29.240]  измена,
[30:29.240 --> 30:31.240]  измена,
[30:31.240 --> 30:33.240]  измена,
[30:33.240 --> 30:35.240]  измена,
[30:35.240 --> 30:37.240]  измена,
[30:37.240 --> 30:39.240]  измена,
[30:39.240 --> 30:41.240]  измена,
[30:41.240 --> 30:43.240]  измена,
[30:43.240 --> 30:45.240]  ее прям многому не выделить.
[30:45.240 --> 30:47.240]  Потому что у нас по факту
[30:47.240 --> 30:49.240]  мы уходим из механизма виртуальной памяти.
[30:49.240 --> 30:51.240]  Вот.
[30:51.240 --> 30:53.240]  Это недостаток этой штуки.
[30:53.240 --> 30:55.240]  Их надо использовать только в тех местах, которые...
[30:55.240 --> 30:57.240]  Значит, смотрите, у вас появляется
[30:57.240 --> 30:59.240]  пейджибл память
[30:59.240 --> 31:01.240]  и появляется пинт память.
[31:03.240 --> 31:05.240]  А дальше у вас
[31:05.240 --> 31:07.240]  есть этот в рам.
[31:09.240 --> 31:11.240]  Собственно, вот эта память
[31:11.240 --> 31:13.240]  можно выделить при помощи механизма
[31:13.240 --> 31:15.240]  kudo-host-lock.
[31:17.240 --> 31:19.240]  Причем ее можно запустить
[31:19.240 --> 31:21.240]  с похожим параметром, как mmap.
[31:23.240 --> 31:25.240]  Вот.
[31:25.240 --> 31:27.240]  А здесь вы копируете
[31:27.240 --> 31:29.240]  при помощи memcpy.
[31:29.240 --> 31:31.240]  А здесь вы копируете при помощи
[31:31.240 --> 31:33.240]  kudo-memcpy-i-sync.
[31:37.240 --> 31:39.240]  При этом пинт память
[31:39.240 --> 31:41.240]  можно напрямую писать тоже элементы.
[31:41.240 --> 31:43.240]  Я вот как раз вчера сидел, разбирался
[31:43.240 --> 31:45.240]  и были такие примеры.
[31:45.240 --> 31:47.240]  Это ускоряет программу
[31:47.240 --> 31:49.240]  в том случае, если вам
[31:51.240 --> 31:53.240]  лимитирует именно скорость переброски данных
[31:53.240 --> 31:55.240]  из CPU-памяти
[31:55.240 --> 31:57.240]  с GPU-памяти.
[31:57.240 --> 31:59.240]  Такое бывает чтение.
[31:59.240 --> 32:01.240]  Потому что, несмотря на то, что код
[32:01.240 --> 32:03.240]  нашего ядра, который мы с вами
[32:03.240 --> 32:05.240]  запускали, работает 5.77 мс.,
[32:05.240 --> 32:07.240]  а копирование памяти
[32:07.240 --> 32:09.240]  работает 300-400 мс.
[32:11.240 --> 32:13.240]  Зачем копировать память?
[32:13.240 --> 32:15.240]  Зачем выполнять ядро, когда
[32:15.240 --> 32:17.240]  копирование памяти просто сжирает
[32:17.240 --> 32:19.240]  все время работы видеокарты?
[32:25.240 --> 32:27.240]  Вот это понятно?
[32:29.240 --> 32:31.240]  Хорошо.
[32:31.240 --> 32:33.240]  Вот она картинка, кстати.
[32:33.240 --> 32:35.240]  Все равно
[32:35.240 --> 32:37.240]  дататрансферы он как раз все делает
[32:37.240 --> 32:39.240]  через пейджел памяти,
[32:39.240 --> 32:41.240]  а здесь у нас как раз все работает
[32:41.240 --> 32:43.240]  через пинт памяти.
[32:43.240 --> 32:45.240]  А теперь давайте разберемся наконец-таки
[32:45.240 --> 32:47.240]  с иерархией наших
[32:47.240 --> 32:49.240]  учтительных устройств.
[32:49.240 --> 32:51.240]  Не знаю, видели ли вы эту картинку
[32:51.240 --> 32:53.240]  или нет. Скорее всего, где-то ее
[32:53.240 --> 32:55.240]  рассказывали, что на самом деле скорость
[32:55.240 --> 32:57.240]  доступа к памяти, которая у нас
[32:57.240 --> 32:59.240]  имеется, она
[32:59.240 --> 33:01.240]  на самом деле
[33:01.240 --> 33:03.240]  сначала она является слишком медленной,
[33:03.240 --> 33:05.240]  объем у нас достаточно большой.
[33:05.240 --> 33:07.240]  После этого, чем
[33:07.240 --> 33:09.240]  больше скорость доступа
[33:09.240 --> 33:11.240]  хотим, тем
[33:11.240 --> 33:13.240]  стоимость наших ресурсов
[33:13.240 --> 33:15.240]  экспоненциально растет,
[33:15.240 --> 33:17.240]  но при этом получается
[33:17.240 --> 33:19.240]  объем памяти
[33:19.240 --> 33:21.240]  становится сильно меньше.
[33:21.240 --> 33:23.240]  И здесь как раз можно попытаться
[33:23.240 --> 33:25.240]  пересчитать количество ресурсов,
[33:25.240 --> 33:27.240]  которые нам приходят сюда секунду.
[33:27.240 --> 33:29.240]  То есть объем памяти на секунду,
[33:29.240 --> 33:31.240]  получается,
[33:31.240 --> 33:33.240]  сколько мы платим денег за 1 гигабайт памяти
[33:33.240 --> 33:35.240]  и сколько
[33:35.240 --> 33:37.240]  мы скорости платим
[33:37.240 --> 33:39.240]  в гигабайтах на секунду.
[33:39.240 --> 33:41.240]  Значит,
[33:41.240 --> 33:43.240]  самым медленным здесь является лента.
[33:43.240 --> 33:45.240]  Вы когда-нибудь
[33:45.240 --> 33:47.240]  видели ленты магнитные?
[33:51.240 --> 33:53.240]  Вы не поверите,
[33:53.240 --> 33:55.240]  до сих пор бэкапы хранят на магнитных лентах.
[33:55.240 --> 33:57.240]  Да, в том же самом
[33:57.240 --> 33:59.240]  Амазоне.
[34:09.240 --> 34:11.240]  Вот они.
[34:11.240 --> 34:13.240]  Так, магнитные ленты
[34:13.240 --> 34:15.240]  для хранения данных.
[34:15.240 --> 34:17.240]  Вот так они выглядят.
[34:17.240 --> 34:19.240]  Ну, это старые какие-то,
[34:19.240 --> 34:21.240]  но есть достаточно много
[34:21.240 --> 34:23.240]  из них.
[34:23.240 --> 34:25.240]  Это именно система хранения данных.
[34:25.240 --> 34:27.240]  Это самый тяжелый бэкап.
[34:27.240 --> 34:29.240]  Но почему работать с лентами
[34:29.240 --> 34:31.240]  очень неудобно?
[34:31.240 --> 34:33.240]  Потому что совсем
[34:33.240 --> 34:35.240]  непроизвольный доступ к памяти.
[34:37.240 --> 34:39.240]  Он линейный, да.
[34:39.240 --> 34:41.240]  Но при этом, как ни странно,
[34:41.240 --> 34:43.240]  Linux разработчики
[34:43.240 --> 34:45.240]  часто работают
[34:45.240 --> 34:47.240]  с форматом хранения данных
[34:47.240 --> 34:49.240]  именно для этих лент.
[34:49.240 --> 34:51.240]  Все очень просто.
[34:55.240 --> 34:57.240]  Обычно, где хранят
[34:57.240 --> 34:59.240]  разные данные?
[35:01.240 --> 35:03.240]  В каком смысле разные данные?
[35:03.240 --> 35:05.240]  Данные, которые слишком много места
[35:05.240 --> 35:07.240]  обычно занимают.
[35:09.240 --> 35:11.240]  В каком формате хранят?
[35:13.240 --> 35:15.240]  В каком формате хранят?
[35:15.240 --> 35:17.240]  В каком формате хранят?
[35:17.240 --> 35:19.240]  В каком формате хранят?
[35:21.240 --> 35:23.240]  Ну, пример.
[35:23.240 --> 35:25.240]  У нас есть какой-нибудь
[35:25.240 --> 35:27.240]  набор текстовых файлов.
[35:27.240 --> 35:29.240]  Они занимают слишком много места.
[35:29.240 --> 35:31.240]  Архивируем.
[35:31.240 --> 35:33.240]  Хорошо.
[35:33.240 --> 35:35.240]  Ну, теперь смотрите.
[35:35.240 --> 35:37.240]  Пишем R.
[35:37.240 --> 35:39.240]  Архив.
[35:39.240 --> 35:41.240]  Ленточный формат.
[35:41.240 --> 35:43.240]  Ой, я кажется...
[35:43.240 --> 35:45.240]  Что сломал?
[35:45.240 --> 35:47.240]  Делаю экран.
[35:47.240 --> 35:49.240]  Ленточный
[35:49.240 --> 35:51.240]  формат
[35:51.240 --> 35:53.240]  называется Tape.
[35:59.240 --> 36:01.240]  Tape-архив.
[36:01.240 --> 36:03.240]  Да, то есть самый первый
[36:03.240 --> 36:05.240]  формат архивирования, который появился,
[36:05.240 --> 36:07.240]  это формат Tar.
[36:07.240 --> 36:09.240]  И он был предназначен на том, чтобы хранить
[36:09.240 --> 36:11.240]  архивы именно
[36:11.240 --> 36:13.240]  на магнитных лентах.
[36:15.240 --> 36:17.240]  Да-да-да.
[36:17.240 --> 36:19.240]  Это не ваши зипы, рары и так далее.
[36:21.240 --> 36:23.240]  Именно в Tar-формате
[36:23.240 --> 36:25.240]  ленточные штуки позволяют хранить данные.
[36:25.240 --> 36:27.240]  Дальше, если мы с вами поднимаемся
[36:27.240 --> 36:29.240]  по ленте, то у нас появляются
[36:29.240 --> 36:31.240]  разные диски.
[36:31.240 --> 36:33.240]  У нас появляются диски жесткие
[36:33.240 --> 36:35.240]  и диски Solid State Drive.
[36:35.240 --> 36:37.240]  То есть диски, которые...
[36:37.240 --> 36:39.240]  Как это называется?
[36:39.240 --> 36:41.240]  SSD, да.
[36:41.240 --> 36:43.240]  Flash-накопители.
[36:43.240 --> 36:45.240]  Обычно твердотельные накопители.
[36:45.240 --> 36:47.240]  И мы с вами
[36:47.240 --> 36:49.240]  можем даже посмотреть
[36:49.240 --> 36:51.240]  в магазинах, сколько
[36:51.240 --> 36:53.240]  твердотельный накопитель стоит за терабайт
[36:53.240 --> 36:55.240]  и сколько обычный накопитель стоит за терабайт.
[36:57.240 --> 36:59.240]  Хотите эксперимент?
[37:07.240 --> 37:09.240]  Так, какой магазин
[37:09.240 --> 37:11.240]  вы полюбите?
[37:13.240 --> 37:15.240]  Хорошо.
[37:17.240 --> 37:19.240]  Так, хорошо.
[37:19.240 --> 37:21.240]  Магазин CityLink.
[37:21.240 --> 37:23.240]  Господи.
[37:23.240 --> 37:25.240]  Товары, товары.
[37:27.240 --> 37:29.240]  Да.
[37:29.240 --> 37:31.240]  SSD-накопители, давайте смотреть,
[37:31.240 --> 37:33.240]  сколько он стоит.
[37:35.240 --> 37:37.240]  Да,
[37:37.240 --> 37:39.240]  нативную рекламу.
[37:39.240 --> 37:41.240]  Вот, давайте NVMe возьмем.
[37:43.240 --> 37:45.240]  У нас получается, смотрите,
[37:45.240 --> 37:47.240]  семь с половиной тысяч
[37:47.240 --> 37:49.240]  за терабайт.
[38:01.240 --> 38:03.240]  А?
[38:05.240 --> 38:07.240]  Да.
[38:07.240 --> 38:09.240]  При этом скорость доступа,
[38:09.240 --> 38:11.240]  какая у нас получается?
[38:11.240 --> 38:13.240]  Три с половиной гигабайта в секунду,
[38:13.240 --> 38:15.240]  да, где-то.
[38:15.240 --> 38:17.240]  Ну, два гигабайта в секунду.
[38:19.240 --> 38:21.240]  Так.
[38:21.240 --> 38:23.240]  Хорошо.
[38:23.240 --> 38:25.240]  Теперь пойдем смотреть жесткие диски.
[38:35.240 --> 38:37.240]  Давайте первое.
[38:37.240 --> 38:39.240]  Что?
[38:41.240 --> 38:43.240]  Вэдэблю?
[38:47.240 --> 38:49.240]  Вэдэблю?
[38:51.240 --> 38:53.240]  Ну, блю.
[38:53.240 --> 38:55.240]  Тут ценное качество, конечно.
[38:55.240 --> 38:57.240]  Лучше вот это. Давайте возьмем.
[38:59.240 --> 39:01.240]  А?
[39:03.240 --> 39:05.240]  Сколько?
[39:05.240 --> 39:07.240]  Восемь терабайт.
[39:07.240 --> 39:09.240]  Нормально.
[39:09.240 --> 39:11.240]  У меня дома восьми терабайт на ДИК ставит.
[39:11.240 --> 39:13.240]  И тут заканчивается.
[39:13.240 --> 39:15.240]  Так, значит, HDD.
[39:17.240 --> 39:19.240]  Сколько тысяч рублей за терабайт
[39:19.240 --> 39:21.240]  получается?
[39:23.240 --> 39:25.240]  22 делить
[39:25.240 --> 39:27.240]  на 8, это кажется 3,75.
[39:31.240 --> 39:33.240]  О, Господи, да.
[39:33.240 --> 39:35.240]  Три раза дешевле.
[39:39.240 --> 39:41.240]  А?
[39:43.240 --> 39:45.240]  Ну, слушайте, да.
[39:45.240 --> 39:47.240]  Ну, на самом деле,
[39:47.240 --> 39:49.240]  тут мощность хранения данных есть.
[39:49.240 --> 39:51.240]  Так, а какая скорость у него доступа?
[39:55.240 --> 39:57.240]  Вряд ли.
[39:59.240 --> 40:01.240]  Я думаю,
[40:01.240 --> 40:03.240]  SDD будет под сотню стоить.
[40:05.240 --> 40:07.240]  Все характеристики.
[40:07.240 --> 40:09.240]  Максимальная скорость интерфейса.
[40:09.240 --> 40:11.240]  600 мегабайт в секунду.
[40:19.240 --> 40:21.240]  То есть мы видим с вами цену ресурсов.
[40:21.240 --> 40:23.240]  Давайте теперь поднимемся выше.
[40:27.240 --> 40:29.240]  Сильно дешевле.
[40:31.240 --> 40:33.240]  На терабайт, но и сильно медленнее
[40:33.240 --> 40:35.240]  по скорости.
[40:35.240 --> 40:37.240]  Так, дальше у нас идет кто?
[40:37.240 --> 40:39.240]  Оперативная память.
[40:47.240 --> 40:49.240]  Так, давайте
[40:49.240 --> 40:51.240]  оперативная память.
[40:53.240 --> 40:55.240]  Оперативная память.
[40:55.240 --> 40:57.240]  Какую возьмем?
[40:57.240 --> 40:59.240]  Давайте DDR5.
[40:59.240 --> 41:01.240]  Капец они мощные стали.
[41:11.240 --> 41:13.240]  О!
[41:13.240 --> 41:15.240]  Пойдет, короче говоря.
[41:15.240 --> 41:17.240]  Итак, что мы здесь видим?
[41:17.240 --> 41:19.240]  Объем памяти 32 гигабайта.
[41:19.240 --> 41:21.240]  На 10, сколько?
[41:21.240 --> 41:23.240]  На 11 тысяч.
[41:23.240 --> 41:25.240]  Чтобы получить твердую память.
[41:25.240 --> 41:27.240]  На 11 тысяч.
[41:29.240 --> 41:31.240]  Чтобы получить стоимость на терабайт,
[41:31.240 --> 41:33.240]  что нам нужно сделать?
[41:33.240 --> 41:35.240]  Нам нужно эту 10 тысяч бумаж на 32.
[41:37.240 --> 41:39.240]  Получаем 352 тысячи рублей за терабайт.
[41:45.240 --> 41:47.240]  Потому что у нас 32 гигабайта
[41:47.240 --> 41:49.240]  объем оперативной памяти.
[41:49.240 --> 41:51.240]  Это ну за терабайт, конечно.
[41:51.240 --> 41:53.240]  То есть, видите, порядок
[41:53.240 --> 41:55.240]  возрос 40 раз.
[41:55.240 --> 41:57.240]  Но при этом, что у нас
[41:57.240 --> 41:59.240]  происходит со скоростью?
[42:05.240 --> 42:07.240]  Битность шины.
[42:07.240 --> 42:09.240]  Здесь есть...
[42:09.240 --> 42:11.240]  О, пропускная способность.
[42:13.240 --> 42:15.240]  Сколько это получается?
[42:15.240 --> 42:17.240]  38,4 гигабайта в секунду.
[42:19.240 --> 42:21.240]  Ну, видно разницу.
[42:21.240 --> 42:23.240]  То есть, получается,
[42:23.240 --> 42:25.240]  у нас и здесь выросло в 20 раз,
[42:25.240 --> 42:27.240]  и здесь приблизительно в 40 раз
[42:27.240 --> 42:29.240]  ценник поднялся.
[42:29.240 --> 42:31.240]  Дальше что идет?
[42:31.240 --> 42:33.240]  Кэш.
[42:33.240 --> 42:35.240]  Значит, как оценивать кэш?
[42:35.240 --> 42:37.240]  А?
[42:39.240 --> 42:41.240]  Не знаю.
[42:41.240 --> 42:43.240]  Не знаю.
[42:43.240 --> 42:45.240]  Не знаю.
[42:45.240 --> 42:47.240]  Не знаю.
[42:47.240 --> 42:49.240]  Да.
[42:49.240 --> 42:51.240]  Он там есть многоуровневый.
[42:51.240 --> 42:53.240]  И нам просто нужно, на самом деле,
[42:53.240 --> 42:55.240]  зашить некоторую величину относительно
[42:55.240 --> 42:57.240]  скорости центрального процессора.
[42:57.240 --> 42:59.240]  То есть, относительно стоимости центрального процессора.
[42:59.240 --> 43:01.240]  Давайте возьмем какой-нибудь
[43:01.240 --> 43:03.240]  центральный процессор.
[43:03.240 --> 43:05.240]  Какой мы с вами возьмем?
[43:09.240 --> 43:11.240]  Вот этот, допустим, да?
[43:11.240 --> 43:13.240]  Так.
[43:13.240 --> 43:15.240]  У нас мощность 3,9 гигагерца.
[43:15.240 --> 43:17.240]  Число ядер потока
[43:17.240 --> 43:19.240]  столько.
[43:19.240 --> 43:21.240]  Кэш.
[43:21.240 --> 43:23.240]  Ну, сколько кэш у нас получается?
[43:23.240 --> 43:25.240]  20 мегабайт?
[43:25.240 --> 43:27.240]  Плюс-минус, да?
[43:27.240 --> 43:29.240]  А?
[43:29.240 --> 43:31.240]  Ну,
[43:31.240 --> 43:33.240]  это некорректно.
[43:33.240 --> 43:35.240]  Я согласен, что это некорректно сравнивать.
[43:35.240 --> 43:37.240]  Нам нужно плюс-минус прикидочное значение
[43:37.240 --> 43:39.240]  получить. А?
[43:41.240 --> 43:43.240]  Ну, 20...
[43:43.240 --> 43:45.240]  Так. L2 кэш у нас на каждое ядро
[43:45.240 --> 43:47.240]  получается.
[43:47.240 --> 43:49.240]  16 плюс 12 на 3...
[43:49.240 --> 43:51.240]  Так.
[43:51.240 --> 43:53.240]  Плюс 3 на 6. 34
[43:53.240 --> 43:55.240]  получается, да?
[43:55.240 --> 43:57.240]  34 мегабайта.
[43:59.240 --> 44:01.240]  Нам нужно 1024
[44:01.240 --> 44:03.240]  на 1024 поделить на 132.
[44:05.240 --> 44:07.240]  1024 на 1024
[44:07.240 --> 44:09.240]  поделить на 34.
[44:09.240 --> 44:11.240]  Ну, вот.
[44:11.240 --> 44:13.240]  Получаем 30 тысяч
[44:13.240 --> 44:15.240]  и вот эту величину,
[44:15.240 --> 44:17.240]  которую нам нужно
[44:17.240 --> 44:19.240]  умножить на 14 тысяч.
[44:19.240 --> 44:21.240]  Давайте не на 14 тысяч.
[44:21.240 --> 44:23.240]  Предположим, что, как вы думаете,
[44:23.240 --> 44:25.240]  кэш сколько стоит
[44:25.240 --> 44:27.240]  относительно цены процессора?
[44:27.240 --> 44:29.240]  Ну, давайте даже скажем
[44:29.240 --> 44:31.240]  5 процентов.
[44:33.240 --> 44:35.240]  Ну, сколько?
[44:35.240 --> 44:37.240]  15. Хорошо.
[44:39.240 --> 44:41.240]  Так.
[44:59.240 --> 45:01.240]  Получаем с вами 65 миллионов рублей
[45:01.240 --> 45:03.240]  на турабайт информации.
[45:05.240 --> 45:07.240]  Скорая доступа.
[45:07.240 --> 45:09.240]  Ну, давайте тут напишем порядка
[45:15.240 --> 45:17.240]  гигабайта в секунду.
[45:17.240 --> 45:19.240]  То есть, за очень быструю скорость
[45:19.240 --> 45:21.240]  нам действительно нужно очень много платить.
[45:21.240 --> 45:23.240]  Ну, и самый верхний уровень
[45:23.240 --> 45:25.240]  этой всей пирамиды
[45:25.240 --> 45:27.240]  это регистры.
[45:29.240 --> 45:31.240]  Сколько у нас регистров?
[45:33.240 --> 45:35.240]  Ну, штук 20.
[45:35.240 --> 45:37.240]  Соответственно, здесь уже
[45:37.240 --> 45:39.240]  почти под бесконечно все будет.
[45:45.240 --> 45:47.240]  Ну, да, потому что сколько?
[45:47.240 --> 45:49.240]  6 регистров это, образно говоря,
[45:49.240 --> 45:51.240]  даже на каждое ядро,
[45:51.240 --> 45:53.240]  если у нас там 20 регистров,
[45:53.240 --> 45:55.240]  то 20 на 12 получается 240,
[45:55.240 --> 45:57.240]  там на 4,
[45:57.240 --> 45:59.240]  ну, где-то получается
[45:59.240 --> 46:01.240]  тысячу байтов
[46:01.240 --> 46:03.240]  получается где-то.
[46:03.240 --> 46:05.240]  1 килобайт получается где-то.
[46:05.240 --> 46:07.240]  1 килобайт, собственно,
[46:07.240 --> 46:09.240]  здесь будет сложно считать,
[46:09.240 --> 46:11.240]  а скорость доступа, ну,
[46:11.240 --> 46:13.240]  по турабайт, наверное, можно получить.
[46:17.240 --> 46:19.240]  Вот такая пирамида.
[46:19.240 --> 46:21.240]  Это пирамида именно на ЦПУ,
[46:21.240 --> 46:23.240]  как она у нас работает.
[46:23.240 --> 46:25.240]  То есть, на классическом компьютере.
[46:25.240 --> 46:27.240]  Так, понятно ли вот это?
[46:27.240 --> 46:29.240]  Вот это вот разъяснение.
[46:29.240 --> 46:31.240]  То есть, мы с вами даже провели эксперимент
[46:31.240 --> 46:33.240]  в том, как это считается.
[46:33.240 --> 46:35.240]  Мне кажется, просто это
[46:35.240 --> 46:37.240]  никто не показал.
[46:37.240 --> 46:39.240]  Вот, если мы говорим про РАФИС
[46:39.240 --> 46:41.240]  и НОЦПУ, то
[46:41.240 --> 46:43.240]  здесь, наверное, немножко ошибся
[46:43.240 --> 46:45.240]  в какой-то степени.
[46:45.240 --> 46:47.240]  У нас с вами есть регистры,
[46:47.240 --> 46:49.240]  из которых, если у нас данных нет,
[46:49.240 --> 46:51.240]  то мы доходим до уровня L1-L2 cache,
[46:51.240 --> 46:53.240]  за 5-12 тактов процессора
[46:53.240 --> 46:55.240]  получаем память.
[46:55.240 --> 46:57.240]  Дальше, если у нас чего-то нет в L1-L2 cache,
[46:57.240 --> 46:59.240]  мы идем в L3 cache, который уже общий
[46:59.240 --> 47:01.240]  и получаем информацию
[47:01.240 --> 47:03.240]  о том,
[47:03.240 --> 47:05.240]  уже со скоростью где-то
[47:05.240 --> 47:07.240]  200-400 гигабайт в секунду.
[47:07.240 --> 47:09.240]  А дальше, если
[47:09.240 --> 47:11.240]  нам не повезло с Cache,
[47:11.240 --> 47:13.240]  то мы уже идем в оперативную память
[47:13.240 --> 47:15.240]  и скорость доступа падает
[47:15.240 --> 47:17.240]  достаточно сильно.
[47:17.240 --> 47:19.240]  Ну да, где-то мы по оценкам, кстати,
[47:19.240 --> 47:21.240]  не ошиблись.
[47:21.240 --> 47:23.240]  То есть, чем дальше,
[47:23.240 --> 47:25.240]  чем ближе мы делаем запрос
[47:25.240 --> 47:27.240]  в оперативной памяти,
[47:27.240 --> 47:29.240]  тем меньше объем мы с вами можем перегонять.
[47:29.240 --> 47:31.240]  Что касается иерархии
[47:31.240 --> 47:33.240]  самой видеокарты,
[47:33.240 --> 47:35.240]  здесь у нас есть 3-ми мультипроцессоры,
[47:35.240 --> 47:37.240]  к каждому из 3-ми мультипроцессоров
[47:37.240 --> 47:39.240]  подключена L1-Cache,
[47:39.240 --> 47:41.240]  он имеет определенный объем,
[47:41.240 --> 47:43.240]  и дальше между ними есть L2-Cache,
[47:43.240 --> 47:45.240]  между ними всеми,
[47:45.240 --> 47:47.240]  общий L2-Cache.
[47:47.240 --> 47:49.240]  И мы с вами
[47:49.240 --> 47:51.240]  в какой-то момент времени
[47:51.240 --> 47:53.240]  должны сказать, что у центрального
[47:53.240 --> 47:55.240]  процессора всегда есть
[47:55.240 --> 47:57.240]  линей, в которых мы можем
[47:57.240 --> 47:59.240]  загружать последовательно, и дальше
[47:59.240 --> 48:01.240]  все к ней могут обращаться.
[48:01.240 --> 48:03.240]  Если мы говорим про ГПУ,
[48:03.240 --> 48:05.240]  то ширина окрашения ГПУ 128 байт.
[48:15.240 --> 48:17.240]  Нет, это для НЛИДИСКИ.
[48:17.240 --> 48:19.240]  Мы тут говорим именно про...
[48:19.240 --> 48:21.240]  Да, для других
[48:21.240 --> 48:23.240]  надо смотреть спецификацию.
[48:25.240 --> 48:27.240]  Да, они зафиксировали это
[48:27.240 --> 48:29.240]  под свою архитектуру.
[48:33.240 --> 48:35.240]  Так, а теперь магия.
[48:35.240 --> 48:37.240]  Значит, заключается следующее.
[48:39.240 --> 48:41.240]  Вмещается ровно 32 флота.
[48:45.240 --> 48:47.240]  Да, совпадение недуманное.
[48:47.240 --> 48:49.240]  Вообще, смотрите,
[48:49.240 --> 48:51.240]  cache-линия подгружается на один ворб.
[48:51.240 --> 48:53.240]  А это означает следующее,
[48:53.240 --> 48:55.240]  что все элементы, которые обращаются
[48:55.240 --> 48:57.240]  внутри одного ворпа,
[48:57.240 --> 48:59.240]  могут обращаться как раз
[48:59.240 --> 49:01.240]  внутри одной кашлини.
[49:01.240 --> 49:03.240]  Если вы вдруг пойдете
[49:03.240 --> 49:05.240]  в другой объем памяти,
[49:05.240 --> 49:07.240]  то, увы, у вас
[49:07.240 --> 49:09.240]  кашлиня сломается.
[49:09.240 --> 49:11.240]  В другой кусок памяти.
[49:13.240 --> 49:15.240]  Кстати, там будет про это еще слайд.
[49:15.240 --> 49:17.240]  Теперь про ГПУ.
[49:17.240 --> 49:19.240]  В чем заключается?
[49:19.240 --> 49:21.240]  Особенность в том, что здесь L1 L2 cache
[49:21.240 --> 49:23.240]  и доступ к L1 cache
[49:23.240 --> 49:25.240]  намного медленнее, чем доступ
[49:25.240 --> 49:27.240]  к оперативной памяти.
[49:27.240 --> 49:29.240]  При этом видно, что пропускная способность
[49:29.240 --> 49:31.240]  сильно больше, и наша цель
[49:31.240 --> 49:33.240]  как разработчиков видеокарты
[49:33.240 --> 49:35.240]  на самом деле уметь
[49:35.240 --> 49:37.240]  эффективно обращаться
[49:37.240 --> 49:39.240]  либо к L1 cache
[49:39.240 --> 49:41.240]  либо к
[49:41.240 --> 49:43.240]  участку памяти, который очень сильно
[49:43.240 --> 49:45.240]  собой напоминает L1 cache.
[49:45.240 --> 49:47.240]  То,
[49:47.240 --> 49:49.240]  что именно у нас
[49:49.240 --> 49:51.240]  может находиться на чипе.
[49:51.240 --> 49:53.240]  То есть у видеокартов,
[49:53.240 --> 49:55.240]  у ГПУ, на самом деле,
[49:55.240 --> 49:57.240]  есть несколько типов памяти.
[49:57.240 --> 49:59.240]  Значит, есть общая память, глобальная.
[49:59.240 --> 50:01.240]  Есть память, которая внутри находится
[50:01.240 --> 50:03.240]  на одном чипе.
[50:03.240 --> 50:05.240]  Внутри одного стриминга мультипроцессора.
[50:05.240 --> 50:07.240]  А теперь
[50:07.240 --> 50:09.240]  смотрите интересную картинку,
[50:09.240 --> 50:11.240]  которая у нас здесь есть.
[50:11.240 --> 50:13.240]  У нас с вами есть, так сказать,
[50:13.240 --> 50:15.240]  он-чип-мемори.
[50:15.240 --> 50:17.240]  Это shared память,
[50:17.240 --> 50:19.240]  local памяти и регистр.
[50:19.240 --> 50:21.240]  То есть она находится на самом чипе,
[50:21.240 --> 50:23.240]  и размер ее не очень большой.
[50:23.240 --> 50:25.240]  При этом мы видим с вами, что shared память
[50:25.240 --> 50:27.240]  выделяется на блок,
[50:27.240 --> 50:29.240]  локальная память выделяется на один поток,
[50:29.240 --> 50:31.240]  регистры тоже выделяются на один поток.
[50:31.240 --> 50:33.240]  А глобальная память,
[50:33.240 --> 50:35.240]  константная память и текстурная память,
[50:35.240 --> 50:37.240]  они выделяются
[50:37.240 --> 50:39.240]  целиком.
[50:39.240 --> 50:41.240]  Текстурная память это та, в которой можно
[50:41.240 --> 50:43.240]  использовать текстуры,
[50:43.240 --> 50:45.240]  это отрисовки компьютерной графики.
[50:45.240 --> 50:47.240]  Константная память
[50:47.240 --> 50:49.240]  это память, в которой вы можете хранить константы.
[50:49.240 --> 50:51.240]  Размер ее тоже не очень большой.
[50:51.240 --> 50:53.240]  И есть глобальная память, в которую мы обычно
[50:53.240 --> 50:55.240]  закидываем массивы.
[50:55.240 --> 50:57.240]  И что такое
[50:57.240 --> 50:59.240]  разделяемая память?
[50:59.240 --> 51:01.240]  Это особенность видеокарты.
[51:01.240 --> 51:03.240]  Мы можем использовать память
[51:03.240 --> 51:05.240]  на уровне чипа.
[51:05.240 --> 51:07.240]  Если мы посмотрим
[51:07.240 --> 51:09.240]  про центральный процессор, у нас такой возможности нет.
[51:09.240 --> 51:11.240]  Единственное, что нам позволяет
[51:11.240 --> 51:13.240]  каким-то образом процессор управлять,
[51:13.240 --> 51:15.240]  это использовать ASCII и AVX инструкции.
[51:15.240 --> 51:17.240]  Более ничего.
[51:17.240 --> 51:19.240]  Здесь же мы можем управлять
[51:19.240 --> 51:21.240]  внутри чипа.
[51:21.240 --> 51:23.240]  И для того, чтобы такую память объявить,
[51:23.240 --> 51:25.240]  нам нужно объявить ключевое слово shared.
[51:25.240 --> 51:27.240]  При этом данные
[51:27.240 --> 51:29.240]  между всеми потоками
[51:29.240 --> 51:31.240]  распространяются именно в одном блоке.
[51:31.240 --> 51:33.240]  И появляются
[51:33.240 --> 51:35.240]  локальные массивы
[51:35.240 --> 51:37.240]  с быстрой скоростью доступа, несколько
[51:37.240 --> 51:39.240]  секунд. Единственная проблема
[51:39.240 --> 51:41.240]  shared памяти заключается в том, что сначала память
[51:41.240 --> 51:43.240]  не нужно скопировать.
[51:43.240 --> 51:45.240]  У вас есть глобальный массив, у вас внутри каждого
[51:45.240 --> 51:47.240]  блока выделяется маленький локальный
[51:47.240 --> 51:49.240]  участок, в который вы можете обращаться.
[51:49.240 --> 51:51.240]  То есть у вас появляется
[51:51.240 --> 51:53.240]  свойство локальности данных.
[51:57.240 --> 51:59.240]  Правда, удовольствие ограниченное.
[51:59.240 --> 52:01.240]  Всего на архитектуре
[52:01.240 --> 52:03.240]  видеокарты можно выделить порядка
[52:03.240 --> 52:05.240]  48 килобайт на блок.
[52:05.240 --> 52:07.240]  Но смотрите, давайте посчитаем.
[52:07.240 --> 52:09.240]  Размер блока у нас обычно какой?
[52:09.240 --> 52:11.240]  В видеокарте.
[52:17.240 --> 52:19.240]  Обычно, я не знаю, проводили ли вам
[52:19.240 --> 52:21.240]  замеры, максимальный размер блока
[52:21.240 --> 52:23.240]  это 1024 байта.
[52:25.240 --> 52:27.240]  1024 элемента на 4 байта.
[52:27.240 --> 52:29.240]  То есть у нас получается
[52:29.240 --> 52:31.240]  4096 байта.
[52:31.240 --> 52:33.240]  4 килобайта.
[52:33.240 --> 52:35.240]  То есть по факту, если у вас размер
[52:35.240 --> 52:37.240]  блока 4 килобайта,
[52:37.240 --> 52:39.240]  то вы в разделяемой памяти
[52:39.240 --> 52:41.240]  можете хранить
[52:41.240 --> 52:43.240]  X12 от этого значения.
[52:43.240 --> 52:45.240]  То есть у вас получается на каждый элемент
[52:45.240 --> 52:47.240]  вы можете хранить дополнительно
[52:47.240 --> 52:49.240]  12 вы разделяемой памяти.
[52:49.240 --> 52:51.240]  Если вы делаете меньше размер блока,
[52:51.240 --> 52:53.240]  то вы можете большее количество элементов
[52:53.240 --> 52:55.240]  хранить на одном блоке
[52:55.240 --> 52:57.240]  в разделяемой памяти.
[52:57.240 --> 52:59.240]  То есть это по факту такая буферная зона, в которой
[52:59.240 --> 53:01.240]  вы можете обращаться.
[53:05.240 --> 53:07.240]  Как объявлять размер
[53:07.240 --> 53:09.240]  shared памяти?
[53:09.240 --> 53:11.240]  Вот это тоже важный момент. Ее можно объявлять либо статически, либо динамически.
[53:11.240 --> 53:13.240]  Статически
[53:13.240 --> 53:15.240]  это мы делаем следующее.
[53:15.240 --> 53:17.240]  Мы объявляем вот в этом моменте времени
[53:17.240 --> 53:19.240]  shared какой-нибудь int
[53:19.240 --> 53:21.240]  x
[53:21.240 --> 53:23.240]  и дальше пишете размер массива.
[53:23.240 --> 53:25.240]  Там, допустим, 128.
[53:25.240 --> 53:27.240]  То есть вы определяете это
[53:27.240 --> 53:29.240]  статическим образом.
[53:31.240 --> 53:33.240]  Второй способ, который нам может быть
[53:33.240 --> 53:35.240]  полезен, это
[53:37.240 --> 53:39.240]  следующий.
[53:39.240 --> 53:41.240]  Вы объявляете extern
[53:49.240 --> 53:51.240]  и в квадратных скобках указываете
[53:51.240 --> 53:53.240]  то, что вам необходимо.
[53:57.240 --> 53:59.240]  А как это определить размер массива?
[54:05.240 --> 54:07.240]  У кого-нибудь есть мысли?
[54:21.240 --> 54:23.240]  У кого есть мысли, куда
[54:23.240 --> 54:25.240]  ее можно засунуть?
[54:27.240 --> 54:29.240]  Она должна объявляться динамически.
[54:33.240 --> 54:35.240]  Кто у нас вызывает
[54:35.240 --> 54:37.240]  ядро?
[54:37.240 --> 54:39.240]  Где у нас
[54:39.240 --> 54:41.240]  в этом моменте времени
[54:41.240 --> 54:43.240]  вызывается ядро?
[54:49.240 --> 54:51.240]  Видите?
[54:51.240 --> 54:53.240]  Это оператор тройной скобки.
[54:53.240 --> 54:55.240]  Это
[54:55.240 --> 54:57.240]  операторы параметров ядра.
[55:19.240 --> 55:21.240]  У параметров вызовов ядра есть
[55:21.240 --> 55:23.240]  третий параметр.
[55:23.240 --> 55:25.240]  Это количество байтов,
[55:25.240 --> 55:27.240]  которое вы делаете в shared memory
[55:27.240 --> 55:29.240]  на каждый блок.
[55:29.240 --> 55:31.240]  Именно количество байтов.
[55:35.240 --> 55:37.240]  Таким образом, когда вы
[55:37.240 --> 55:39.240]  обращаетесь к разделаемой памяти здесь,
[55:39.240 --> 55:41.240]  вам нужно будет автоматически
[55:41.240 --> 55:43.240]  отсчитывать указатели на следующий элемент.
[55:45.240 --> 55:47.240]  Потому что глобальная shared память общая,
[55:47.240 --> 55:49.240]  если вы хотите передать туда
[55:49.240 --> 55:51.240]  два массива в shared памяти,
[55:51.240 --> 55:53.240]  правильно настраивать индексацию,
[55:53.240 --> 55:55.240]  потому что это будет последний блок.
[55:57.240 --> 55:59.240]  Это можно делать
[55:59.240 --> 56:01.240]  третьим параметром вызов ядра
[56:01.240 --> 56:03.240]  передать количество байтов.
[56:03.240 --> 56:05.240]  Примеры с разделяемой памяти
[56:05.240 --> 56:07.240]  будут на семинарах.
[56:11.240 --> 56:13.240]  Теперь давайте
[56:13.240 --> 56:15.240]  поговорим про то,
[56:15.240 --> 56:17.240]  где располагаются разные памяти.
[56:17.240 --> 56:19.240]  Вид памяти у нас
[56:19.240 --> 56:21.240]  есть типы памяти
[56:21.240 --> 56:23.240]  Register, Local, Shared, Global
[56:23.240 --> 56:25.240]  и Constant.
[56:25.240 --> 56:27.240]  И дальше указывается,
[56:27.240 --> 56:29.240]  где эта память сохраняется.
[56:29.240 --> 56:31.240]  Register и Shared находятся
[56:31.240 --> 56:33.240]  на самом чипе видеокарты.
[56:33.240 --> 56:35.240]  Local, Global и Constant
[56:35.240 --> 56:37.240]  находятся вне чипа.
[56:37.240 --> 56:39.240]  При этом та память,
[56:39.240 --> 56:41.240]  которая не находится на чипе,
[56:41.240 --> 56:43.240]  она может быть закэширована.
[56:45.240 --> 56:47.240]  Вот это важно.
[56:47.240 --> 56:49.240]  Остальная память не кэширована.
[56:49.240 --> 56:51.240]  Смысл ее кэшировать,
[56:51.240 --> 56:53.240]  если она и так уже находится на чипе.
[56:53.240 --> 56:55.240]  Про доступ.
[56:55.240 --> 56:57.240]  Все они работают в режиме read-write,
[56:57.240 --> 56:59.240]  кроме Constant.
[56:59.240 --> 57:01.240]  Вы туда можете числа передавать
[57:01.240 --> 57:03.240]  установленное какое-то значение
[57:03.240 --> 57:05.240]  и его использовать.
[57:05.240 --> 57:07.240]  Тут разные уровни кэша.
[57:07.240 --> 57:09.240]  Constant хранится
[57:09.240 --> 57:11.240]  на уровне L1 кэша.
[57:11.240 --> 57:13.240]  Зачастую локальные переменные
[57:13.240 --> 57:15.240]  обычно хранятся на уровне L1.
[57:15.240 --> 57:17.240]  На уровне L1 кэша
[57:17.240 --> 57:19.240]  глобальные все-таки скидываются
[57:19.240 --> 57:21.240]  на размер L2 кэша.
[57:21.240 --> 57:23.240]  В какой области видимости
[57:23.240 --> 57:25.240]  они находятся?
[57:25.240 --> 57:27.240]  Регистр и локальные переменные
[57:27.240 --> 57:29.240]  можно будет прочитать
[57:29.240 --> 57:31.240]  на уровне одного потока.
[57:31.240 --> 57:33.240]  Shared видит все потоки
[57:33.240 --> 57:35.240]  внутри блока.
[57:35.240 --> 57:37.240]  Глобальная память видит Device и Host.
[57:37.240 --> 57:39.240]  И Constant тоже видит Device и Host.
[57:39.240 --> 57:41.240]  Как CPU, так и GPU их видят.
[57:41.240 --> 57:43.240]  При этом жительный цикл,
[57:43.240 --> 57:45.240]  где они определяются.
[57:45.240 --> 57:47.240]  Регистр и локальные переменные
[57:47.240 --> 57:49.240]  определяются внутри потока.
[57:49.240 --> 57:51.240]  Shared определяются внутри блока.
[57:51.240 --> 57:53.240]  Константные глобальные памяти
[57:53.240 --> 57:55.240]  определяются на Host.
[57:55.240 --> 57:57.240]  И дальше функциями куда
[57:57.240 --> 57:59.240]  перекидываются либо на блок,
[57:59.240 --> 58:01.240]  либо перекидываются
[58:01.240 --> 58:03.240]  внутри гидрата.
[58:03.240 --> 58:05.240]  Вот такая сводная таблица
[58:05.240 --> 58:07.240]  по типу памяти,
[58:07.240 --> 58:09.240]  которая у нас существует.
[58:09.240 --> 58:11.240]  Давайте остановлю здесь
[58:11.240 --> 58:13.240]  и спрошу, понятно ли это табличка.
[58:17.240 --> 58:19.240]  Хорошо.
[58:19.240 --> 58:21.240]  И теперь надо рассказать
[58:21.240 --> 58:23.240]  про синхронизацию внутри
[58:23.240 --> 58:25.240]  shared памяти.
[58:25.240 --> 58:27.240]  Почему это важно?
[58:27.240 --> 58:29.240]  У нас есть такой код,
[58:29.240 --> 58:31.240]  и здесь появляется
[58:31.240 --> 58:33.240]  одна важная инструкция.
[58:33.240 --> 58:35.240]  Кто ее может найти?
[58:35.240 --> 58:37.240]  Синк тредс.
[58:37.240 --> 58:39.240]  Что делает синк тредс?
[58:39.240 --> 58:41.240]  Синк тредс – это
[58:41.240 --> 58:43.240]  барьер внутри одного блока.
[58:43.240 --> 58:45.240]  Это означает,
[58:45.240 --> 58:47.240]  что все операции,
[58:47.240 --> 58:49.240]  которые вы сделали внутри одного блока,
[58:49.240 --> 58:51.240]  доходят до этого момента.
[58:51.240 --> 58:53.240]  И ставится глобально в блокировку.
[58:53.240 --> 58:55.240]  Зачем это необходимо?
[58:55.240 --> 58:57.240]  Это необходимо для того,
[58:57.240 --> 58:59.240]  чтобы избежать датарейса.
[58:59.240 --> 59:01.240]  Когда у вас
[59:01.240 --> 59:03.240]  часть потоков, допустим,
[59:03.240 --> 59:05.240]  прошла в следующее вычисление,
[59:05.240 --> 59:07.240]  тут, видите,
[59:07.240 --> 59:09.240]  в коде уаттит,
[59:09.240 --> 59:11.240]  это ut от treddx
[59:11.240 --> 59:13.240]  минус ut от предыдущего treddx,
[59:13.240 --> 59:15.240]  у нас значения какие-то могли
[59:15.240 --> 59:17.240]  еще не записаться,
[59:17.240 --> 59:19.240]  и с этим возникли проблемы.
[59:19.240 --> 59:21.240]  В какой-то ячейке запись у нас есть
[59:21.240 --> 59:23.240]  внутри блока, какой-то нет.
[59:23.240 --> 59:25.240]  Здесь ставится глобальный барьер
[59:25.240 --> 59:27.240]  на блок.
[59:27.240 --> 59:29.240]  Это делается всегда
[59:29.240 --> 59:31.240]  на случай работы
[59:31.240 --> 59:33.240]  с разделяемой памятью.
[59:33.240 --> 59:35.240]  Вопрос.
[59:35.240 --> 59:37.240]  Каким образом можно легко
[59:37.240 --> 59:39.240]  сделать
[59:39.240 --> 59:41.240]  дедлог
[59:41.240 --> 59:43.240]  на видеогарте?
[59:47.240 --> 59:49.240]  У кого есть мысли
[59:53.240 --> 59:55.240]  по этому поводу?
[59:55.240 --> 59:57.240]  Какой код вызовет нам
[59:57.240 --> 59:59.240]  дедлог?
[01:00:01.240 --> 01:00:03.240]  Это типичная ошибка,
[01:00:03.240 --> 01:00:05.240]  которая иногда
[01:00:05.240 --> 01:00:07.240]  допускает люди
[01:00:07.240 --> 01:00:09.240]  смотреть.
[01:00:09.240 --> 01:00:11.240]  Пример очень простой.
[01:00:31.240 --> 01:00:33.240]  Вот такой код вызовет
[01:00:33.240 --> 01:00:35.240]  дедлог.
[01:00:35.240 --> 01:00:37.240]  Да,
[01:00:37.240 --> 01:00:39.240]  не все потоки
[01:00:39.240 --> 01:00:41.240]  внутри блока его вызовут.
[01:00:41.240 --> 01:00:43.240]  Потому что только один поток
[01:00:43.240 --> 01:00:45.240]  внутри этого блока вызовет
[01:00:45.240 --> 01:00:47.240]  сингтресс.
[01:00:47.240 --> 01:00:49.240]  Да, это барьер.
[01:00:49.240 --> 01:00:51.240]  Ну вот.
[01:00:51.240 --> 01:00:53.240]  То есть аккуратнее.
[01:00:53.240 --> 01:00:55.240]  Хочется только на эффективную
[01:00:55.240 --> 01:00:57.240]  операцию ставить,
[01:00:57.240 --> 01:00:59.240]  а не только на эффективную.
[01:00:59.240 --> 01:01:01.240]  Хочется только на эффективную операцию ставить
[01:01:01.240 --> 01:01:03.240]  сингтресс, нет.
[01:01:03.240 --> 01:01:05.240]  Внутри ифоксингтресс, пожалуйста, не ставьте.
[01:01:07.240 --> 01:01:09.240]  По уровню доступа к данным,
[01:01:09.240 --> 01:01:11.240]  давайте тоже поймем
[01:01:11.240 --> 01:01:13.240]  по регистрам следующее.
[01:01:13.240 --> 01:01:15.240]  Как раз в прокашлине.
[01:01:15.240 --> 01:01:17.240]  Представьте себе, что у нас есть размер блока
[01:01:17.240 --> 01:01:19.240]  256 и мы хотим одним потоком
[01:01:19.240 --> 01:01:21.240]  обрабатывать два элемента.
[01:01:21.240 --> 01:01:23.240]  Этот пример я уже на семинаре показывал.
[01:01:23.240 --> 01:01:25.240]  Сейчас покажу еще всем.
[01:01:25.240 --> 01:01:27.240]  Способ первый.
[01:01:27.240 --> 01:01:29.240]  Пусть у нас каждый поток
[01:01:29.240 --> 01:01:31.240]  будет обрабатывать элементы
[01:01:31.240 --> 01:01:33.240]  в последнем порядке.
[01:01:33.240 --> 01:01:35.240]  То есть 0.1, потом 2.3, потом 4.5,
[01:01:35.240 --> 01:01:37.240]  потом 6.7 и так далее.
[01:01:37.240 --> 01:01:39.240]  Второй способ, который у нас
[01:01:39.240 --> 01:01:41.240]  будет, это следующий.
[01:01:41.240 --> 01:01:43.240]  Обрабатывать элементы вот таким образом.
[01:01:43.240 --> 01:01:45.240]  То есть делать перескок через размер блока.
[01:01:47.240 --> 01:01:49.240]  То есть первый поток будет обрабатывать
[01:01:49.240 --> 01:01:51.240]  0.256,
[01:01:51.240 --> 01:01:53.240]  первый 1.257,
[01:01:53.240 --> 01:01:55.240]  третий, то второй 2.258
[01:01:55.240 --> 01:01:57.240]  и так далее.
[01:01:57.240 --> 01:01:59.240]  То есть со сдвигом на размер блока.
[01:01:59.240 --> 01:02:01.240]  Вопрос.
[01:02:01.240 --> 01:02:03.240]  Как вы думаете,
[01:02:03.240 --> 01:02:05.240]  какой способ будет работать
[01:02:05.240 --> 01:02:07.240]  на CPU быстрее?
[01:02:11.240 --> 01:02:13.240]  Какой?
[01:02:13.240 --> 01:02:15.240]  Первый.
[01:02:15.240 --> 01:02:17.240]  Потому что у нас элементы подряд,
[01:02:17.240 --> 01:02:19.240]  каждый поток считывает последним.
[01:02:19.240 --> 01:02:21.240]  Да, почему на GPU работает
[01:02:21.240 --> 01:02:23.240]  второй быстрее?
[01:02:23.240 --> 01:02:25.240]  Давайте посмотрим код, кстати,
[01:02:25.240 --> 01:02:27.240]  который это показывает.
[01:02:27.240 --> 01:02:29.240]  То есть есть функция add,
[01:02:29.240 --> 01:02:31.240]  которая как раз перескакивает
[01:02:31.240 --> 01:02:33.240]  через размер блока и получает
[01:02:33.240 --> 01:02:35.240]  итоговые результаты.
[01:02:35.240 --> 01:02:37.240]  Есть функция stupid add,
[01:02:37.240 --> 01:02:39.240]  которая выделяет и складывает по
[01:02:39.240 --> 01:02:41.240]  8 элементов последовательно.
[01:02:41.240 --> 01:02:43.240]  А теперь
[01:02:43.240 --> 01:02:45.240]  вспоминаем, что я говорил.
[01:02:45.240 --> 01:02:47.240]  Я говорил про то, что размер кэшлини
[01:02:49.240 --> 01:02:51.240]  равен размеру варпа.
[01:02:53.240 --> 01:02:55.240]  То есть у нас получается,
[01:02:55.240 --> 01:02:57.240]  если у нас с вами есть
[01:02:57.240 --> 01:02:59.240]  какой-то нулевой элемент массива,
[01:03:05.240 --> 01:03:07.240]  у нас с вами есть нулевой элемент массива
[01:03:07.240 --> 01:03:09.240]  и он запросил кэшлини,
[01:03:09.240 --> 01:03:11.240]  то он весь варп
[01:03:11.240 --> 01:03:13.240]  запросил кэшлини
[01:03:13.240 --> 01:03:15.240]  с элементом 0.31.
[01:03:17.240 --> 01:03:19.240]  Соответственно, элементы внутри
[01:03:19.240 --> 01:03:21.240]  кэшлини могут общаться как угодно,
[01:03:21.240 --> 01:03:23.240]  но главное не выходить за эти пределы.
[01:03:25.240 --> 01:03:27.240]  Когда мы складываем 0.256,
[01:03:27.240 --> 01:03:29.240]  то у нас
[01:03:29.240 --> 01:03:31.240]  для вот этих ребят подгружается
[01:03:31.240 --> 01:03:33.240]  свое собственное кэшлини,
[01:03:33.240 --> 01:03:35.240]  и все эффективно считают,
[01:03:35.240 --> 01:03:37.240]  потому что нулевой с нулевым,
[01:03:37.240 --> 01:03:39.240]  первый с первым и так далее.
[01:03:39.240 --> 01:03:41.240]  А 256 и 257 и так далее.
[01:03:41.240 --> 01:03:43.240]  Для них
[01:03:43.240 --> 01:03:45.240]  подгрузится свое кэшлини.
[01:03:47.240 --> 01:03:49.240]  То есть на каждую операцию у нас идет
[01:03:49.240 --> 01:03:51.240]  кэшлини. А теперь смотрите
[01:03:51.240 --> 01:03:53.240]  альтернативный способ.
[01:03:53.240 --> 01:03:55.240]  У нас будет нулевой элемент,
[01:03:59.240 --> 01:04:01.240]  который будет подгружать значение.
[01:04:01.240 --> 01:04:03.240]  Нулевой элемент подгрузит
[01:04:03.240 --> 01:04:05.240]  себе нулевое значение,
[01:04:05.240 --> 01:04:07.240]  потом в одно и то же кэшлини первое значение
[01:04:07.240 --> 01:04:09.240]  и так далее. 31.
[01:04:09.240 --> 01:04:11.240]  Отлично, но в такой концепции
[01:04:11.240 --> 01:04:13.240]  четвертый поток,
[01:04:13.240 --> 01:04:15.240]  в то время как
[01:04:15.240 --> 01:04:17.240]  напоминаю, что это происходит
[01:04:17.240 --> 01:04:19.240]  в первом уровне, то есть все операции будут
[01:04:19.240 --> 01:04:21.240]  отшлины в один такт,
[01:04:21.240 --> 01:04:23.240]  четвертый поток
[01:04:23.240 --> 01:04:25.240]  внутри ворпа
[01:04:25.240 --> 01:04:27.240]  пойдет и запросит значение
[01:04:27.240 --> 01:04:29.240]  из 32-го элемента.
[01:04:29.240 --> 01:04:31.240]  Вот в самый момент времени.
[01:04:31.240 --> 01:04:33.240]  То есть он выйдет
[01:04:33.240 --> 01:04:35.240]  за пределы кэшлини,
[01:04:37.240 --> 01:04:39.240]  которого подгрузили на этот ворп.
[01:04:39.240 --> 01:04:41.240]  И у нас возникнет кэшмисс.
[01:04:41.240 --> 01:04:43.240]  И нам придется лезть
[01:04:43.240 --> 01:04:45.240]  в глобальную память для того,
[01:04:45.240 --> 01:04:47.240]  чтобы подгрузить эти значения.
[01:04:51.240 --> 01:04:53.240]  Понятен тезис.
[01:04:53.240 --> 01:04:55.240]  Почему это
[01:04:55.240 --> 01:04:57.240]  лучше писать
[01:04:57.240 --> 01:04:59.240]  с пропуском черед размер блока?
[01:04:59.240 --> 01:05:01.240]  Естественно, если размер блока делится
[01:05:01.240 --> 01:05:03.240]  на 32.
[01:05:03.240 --> 01:05:05.240]  В прошлом раз говорили, что размер блока
[01:05:05.240 --> 01:05:07.240]  должен делиться на размер ворпа.
[01:05:07.240 --> 01:05:09.240]  И тот самый пример,
[01:05:09.240 --> 01:05:11.240]  который у нас был.
[01:05:11.240 --> 01:05:13.240]  Нормальный способ
[01:05:13.240 --> 01:05:15.240]  сложения будет работать за 5,7 мс.
[01:05:15.240 --> 01:05:17.240]  То есть у нас
[01:05:17.240 --> 01:05:19.240]  никаким образом не меняется результат.
[01:05:19.240 --> 01:05:21.240]  А медленный способ начинает работать
[01:05:21.240 --> 01:05:23.240]  в 3-4 раза медленнее.
[01:05:25.240 --> 01:05:27.240]  То есть на семинарах
[01:05:27.240 --> 01:05:29.240]  до 10 раз
[01:05:29.240 --> 01:05:31.240]  замедление получалось.
[01:05:31.240 --> 01:05:33.240]  То есть этим нужно быть аккуратнее.
[01:05:33.240 --> 01:05:35.240]  Способ складывать элементы таким
[01:05:35.240 --> 01:05:37.240]  образом быстрее.
[01:05:37.240 --> 01:05:39.240]  И я про это хотел
[01:05:39.240 --> 01:05:41.240]  сказать.
[01:05:41.240 --> 01:05:43.240]  Все данные, которые
[01:05:43.240 --> 01:05:45.240]  обращаются к ипотокам
[01:05:45.240 --> 01:05:47.240]  могут обращаться
[01:05:47.240 --> 01:05:49.240]  внутри одного ворпа, чтобы не сломать
[01:05:49.240 --> 01:05:51.240]  кашливую.
[01:05:51.240 --> 01:05:53.240]  Чем больше выгрузок в ней кашлять,
[01:05:53.240 --> 01:05:55.240]  тем больше время работает наша программа.
[01:05:55.240 --> 01:05:57.240]  Это важно учить.
[01:05:57.240 --> 01:05:59.240]  И давайте
[01:05:59.240 --> 01:06:01.240]  последний тезис, который мы сегодня успеем
[01:06:01.240 --> 01:06:03.240]  разобрать. В следующий раз мы тогда про синхронизацию
[01:06:03.240 --> 01:06:05.240]  будем детально говорить.
[01:06:05.240 --> 01:06:07.240]  В видеокарте есть механина
[01:06:07.240 --> 01:06:09.240]  предсказательного вычисления.
[01:06:09.240 --> 01:06:11.240]  Что это означает?
[01:06:11.240 --> 01:06:13.240]  Это означает следующее.
[01:06:13.240 --> 01:06:15.240]  Если вы хотите вычислить какое-то значение
[01:06:15.240 --> 01:06:17.240]  и вы вычисляете ветку
[01:06:17.240 --> 01:06:19.240]  с if и с else,
[01:06:19.240 --> 01:06:21.240]  дальше вы вычисляете
[01:06:21.240 --> 01:06:23.240]  значение в
[01:06:23.240 --> 01:06:25.240]  то, что у вас
[01:06:25.240 --> 01:06:27.240]  находится внутри
[01:06:27.240 --> 01:06:29.240]  if.
[01:06:29.240 --> 01:06:31.240]  То есть у вас получается следующее.
[01:06:31.240 --> 01:06:33.240]  if x,
[01:06:33.240 --> 01:06:35.240]  дальше else.
[01:06:35.240 --> 01:06:37.240]  Вы стараетесь вычислить ветку,
[01:06:37.240 --> 01:06:39.240]  которая находится внутри
[01:06:39.240 --> 01:06:41.240]  здесь и здесь, параллельно.
[01:06:41.240 --> 01:06:43.240]  Потому что скорее всего у вас
[01:06:43.240 --> 01:06:45.240]  распределение произойдет по потокам.
[01:06:45.240 --> 01:06:47.240]  То есть у вас запускается одновременно две ветки вычисления
[01:06:47.240 --> 01:06:49.240]  и
[01:06:49.240 --> 01:06:51.240]  после того, как вы посчитали значение в
[01:06:51.240 --> 01:06:53.240]  x, вы подставляете маску
[01:06:53.240 --> 01:06:55.240]  по тем результатам, которые у вас
[01:06:55.240 --> 01:06:57.240]  получаются здесь.
[01:06:57.240 --> 01:06:59.240]  То есть те ветки, в которых x был равен true,
[01:06:59.240 --> 01:07:01.240]  подставляет результат,
[01:07:01.240 --> 01:07:03.240]  который здесь.
[01:07:03.240 --> 01:07:05.240]  Если с else, то подставляет результат,
[01:07:05.240 --> 01:07:07.240]  который здесь.
[01:07:07.240 --> 01:07:09.240]  Поэтому вы на самом деле
[01:07:09.240 --> 01:07:11.240]  не увидите где-то функции
[01:07:11.240 --> 01:07:13.240]  if или еще что-то там,
[01:07:13.240 --> 01:07:15.240]  даже вычисления инструментов.
[01:07:15.240 --> 01:07:17.240]  То есть у вас скорее всего будет функция
[01:07:17.240 --> 01:07:19.240]  под названием tf.v
[01:07:19.240 --> 01:07:21.240]  или там pytorch.v,
[01:07:21.240 --> 01:07:23.240]  где у вас пишется функция, которую вы проверяете
[01:07:23.240 --> 01:07:25.240]  и дальше одновременно два массива,
[01:07:25.240 --> 01:07:27.240]  которые вы должны вернуть.
[01:07:27.240 --> 01:07:29.240]  Массив, который выполняет значение true
[01:07:29.240 --> 01:07:31.240]  при условии того,
[01:07:31.240 --> 01:07:33.240]  что у вас значение верно,
[01:07:33.240 --> 01:07:35.240]  у вас значение false,
[01:07:35.240 --> 01:07:37.240]  там, где у вас значение неверно.
[01:07:37.240 --> 01:07:39.240]  То есть вы сразу готовите два массива
[01:07:39.240 --> 01:07:41.240]  и подставляете результаты по матке.
[01:07:41.240 --> 01:07:43.240]  И за счет этого
[01:07:43.240 --> 01:07:45.240]  ускоряется время работы на видеокарте сильно.
[01:07:45.240 --> 01:07:47.240]  И вот такая вот
[01:07:47.240 --> 01:07:49.240]  финальная картинка,
[01:07:49.240 --> 01:07:51.240]  которая у нас возникает.
[01:07:51.240 --> 01:07:53.240]  Не пишите, пожалуйста, такой код.
[01:07:53.240 --> 01:07:55.240]  Он нужен только для отладки.
[01:07:55.240 --> 01:07:57.240]  То есть у вас получается один поток внутри варпа
[01:07:57.240 --> 01:07:59.240]  выполняет все операции,
[01:07:59.240 --> 01:08:01.240]  а остальные простаивают в это время.
[01:08:01.240 --> 01:08:03.240]  То есть у нас получается
[01:08:03.240 --> 01:08:05.240]  время варпа,
[01:08:05.240 --> 01:08:07.240]  время работы процесса
[01:08:07.240 --> 01:08:09.240]  внутри варпа.
[01:08:09.240 --> 01:08:11.240]  Это как раз, так сказать,
[01:08:11.240 --> 01:08:13.240]  самое долгое время работы потока
[01:08:13.240 --> 01:08:15.240]  внутри этого варпа.
[01:08:15.240 --> 01:08:17.240]  Вроде латничное.
[01:08:17.240 --> 01:08:19.240]  Все равно
[01:08:19.240 --> 01:08:21.240]  видеокарта не может вам
[01:08:21.240 --> 01:08:23.240]  сделать undefined behavior,
[01:08:23.240 --> 01:08:25.240]  поэтому с этим будут проблемы.
[01:08:27.240 --> 01:08:29.240]  Все, про особенности
[01:08:29.240 --> 01:08:31.240]  организации тогда мы, наверное,
[01:08:31.240 --> 01:08:33.240]  будем говорить в следующий раз.
[01:08:33.240 --> 01:08:35.240]  И, наверное,
[01:08:35.240 --> 01:08:37.240]  в следующий раз я еще
[01:08:37.240 --> 01:08:39.240]  принесу примеры для отладки
[01:08:39.240 --> 01:08:41.240]  программ. Посмотрим, собственно, как отлаживать
[01:08:41.240 --> 01:08:43.240]  программу, потому что
[01:08:43.240 --> 01:08:45.240]  с этим тоже бывают некоторые проблемы.
[01:08:45.240 --> 01:08:47.240]  Потому что
[01:08:47.240 --> 01:08:49.240]  вот какое поведение
[01:08:49.240 --> 01:08:51.240]  у нас будет в случае
[01:08:51.240 --> 01:08:53.240]  того, если ядро не выполняется?
[01:08:55.240 --> 01:08:57.240]  Да, ошибка в ГПУ ядре.
[01:08:59.240 --> 01:09:01.240]  Паника?
[01:09:01.240 --> 01:09:03.240]  В каком смысле паника?
[01:09:07.240 --> 01:09:09.240]  Нет, нет, нет.
[01:09:09.240 --> 01:09:11.240]  Там возникнет следующее.
[01:09:11.240 --> 01:09:13.240]  У вас ядро такое скажет
[01:09:13.240 --> 01:09:15.240]  и вам ничего, там не кинется
[01:09:15.240 --> 01:09:17.240]  никакой exception, ничего не кинется.
[01:09:17.240 --> 01:09:19.240]  У вас просто получается, вы замеряете
[01:09:19.240 --> 01:09:21.240]  время работы ядра, у вас получается ноль
[01:09:21.240 --> 01:09:23.240]  по времени ядра.
[01:09:23.240 --> 01:09:25.240]  Ну или какие-то там миллисекунды.
[01:09:25.240 --> 01:09:27.240]  То есть у вас никаким образом не уведомляется
[01:09:27.240 --> 01:09:29.240]  напрямую о том, что у вас
[01:09:29.240 --> 01:09:31.240]  происходит. А по факту
[01:09:31.240 --> 01:09:33.240]  в результате вызова ядра
[01:09:33.240 --> 01:09:35.240]  хранится флаг Ярно.
[01:09:35.240 --> 01:09:37.240]  И именно его надо доставать,
[01:09:37.240 --> 01:09:39.240]  потому что видеокарты это больше сишные
[01:09:39.240 --> 01:09:41.240]  инструкции.
[01:09:41.240 --> 01:09:43.240]  Вот, давайте
[01:09:43.240 --> 01:09:45.240]  наверное вопрос. Сегодня мы с вами как раз
[01:09:45.240 --> 01:09:47.240]  посмотрели на особенности
[01:09:47.240 --> 01:09:49.240]  работы с памяти
[01:09:49.240 --> 01:09:51.240]  на видеокарт и научились
[01:09:51.240 --> 01:09:53.240]  замерять время работы программы.
[01:09:53.240 --> 01:09:55.240]  Но поняли, какие виды памяти
[01:09:55.240 --> 01:09:57.240]  существуют.
[01:10:01.240 --> 01:10:03.240]  Все, надеюсь, что
[01:10:03.240 --> 01:10:05.240]  в следующий раз у нас будет все.
[01:10:07.240 --> 01:10:09.240]  К меньшим количествам косяков.

[00:00.000 --> 00:12.840]  Ну чего, нам осталось добить в каком-то смысле результат, но там еще так порядком. Давайте я
[00:12.840 --> 00:22.560]  вот здесь напомню лемму, которую мы доказываем, она называлась лемма в лемме. Тверждение было
[00:22.560 --> 00:32.760]  такое. Математическое ожидание y-катого больше либо равняется асимпатически. Вечины
[00:32.760 --> 00:40.400]  m в квадрате поделить на 2k в четвертой степени, а y-катый это размер самой большой цепочки,
[00:40.400 --> 00:46.600]  ну так сказать, порёберно не пересекающихся, не пересекающихся, ка вершинных независимых множеств.
[00:46.600 --> 00:57.000]  Ну вот, товарищи, если вы следите за лекциями, то вы понимаете, что такое y-катый. Ну ладно,
[00:57.000 --> 01:04.040]  я всё равно выпишу, бог с ним, это пригодится. Y-катый, для восприятия просто доказательства,
[01:04.040 --> 01:10.160]  это, наверное, всё-таки полезно. Максимальное, не помню, что я там какую букву писал, t что-ли,
[01:10.160 --> 01:19.360]  t писал. Максимальное t, такое, что существует. Давайте я их сегодня буду не s обозначать,
[01:19.360 --> 01:30.280]  а k. Так хочется, k большое, 1 и так далее. k большое t такие, что для любого i мощность
[01:30.280 --> 01:39.160]  каитова равняется k маленькому, мощность каитова пересечённого скажи там, то что любых и и же,
[01:39.160 --> 01:46.200]  конечно, не больше единицы. Можно вот здесь написать, для любых и и же, вот так, чтобы было
[01:46.200 --> 01:53.800]  корректно. Мощность каита пересечённого скажит не больше, чем единица, и каитое, к большое,
[01:53.800 --> 02:05.560]  итое независимое множество. Вершин в графе g. Вот так. Пусть это будет, это полезно. Вот,
[02:05.560 --> 02:13.280]  теперь доказываем лему. Ну, граф у нас, вообще говоря, случайный, но давайте просто зафиксируем
[02:13.280 --> 02:18.080]  какой-то граф, всё равно, когда он случайный, это просто значит, что мы ему вероятность какую-то
[02:18.080 --> 02:33.080]  присвоили, а так-то он граф и граф. Так, пока фиксируем. Граф g, в нашем случае me вершинах,
[02:33.080 --> 02:42.320]  я напоминаю, что у нас графы на me вершинах сейчас все. Фиксируем граф g на me вершинах. Пуф,
[02:42.320 --> 02:59.720]  у него есть какие-то независимые множества. Независимые множества вершин, мощность ка.
[03:03.080 --> 03:13.480]  Они же нас интересуют, вообще говоря, нам потом усреднять по всем графам, но пока зафиксировали
[03:13.480 --> 03:24.360]  граф. Давайте как-нибудь их тоже обзовем. Ну, как мы их обзовем? Да, тоже к большое. К большое 1,
[03:24.360 --> 03:34.160]  к большое 2. А сколько их всего? Ну, у нас есть обозначение для этого, и даже не одно,
[03:34.160 --> 03:49.960]  а целых два уже. Сегодня будет третье. Вот, их x с индексом k от g. Вот так. Ну, конечно,
[03:49.960 --> 03:55.480]  очень громоздкая запись, но важно, чтобы вы просто вспоминали. x с индексом k маленькая от g,
[03:55.480 --> 04:05.000]  это k маленькая. Это количество независимых множеств мощности k в графе g. Вот в графе g есть
[04:05.000 --> 04:10.360]  какие-то независимые их множества, их, конечно, столько. Вот мы их как-то перечислили в определенном
[04:10.360 --> 04:18.520]  порядке. Все. Ну, просто ввел обозначение, так? Я обещал в прошлый раз, что в доказательстве
[04:18.520 --> 04:23.600]  этой замечательной леммы будет присутствовать дополнительная рандомизация. То есть мы еще
[04:23.600 --> 04:31.080]  дополнительно почему-то усредним, чтобы получить итоговую оценку. Но нас-то интересует, понимаете,
[04:31.080 --> 04:36.920]  не сами независимое множество, а гирлянды вот эти, которые здесь выписаны, гирлянды из не сильно
[04:36.920 --> 04:43.080]  пересекающихся независимых множеств. Поэтому мы сделаем такую дополнительную рандомизацию,
[04:43.080 --> 04:49.040]  которая позволит двойной счет или усреднение какое-то организовать. Мы возьмем пока непонятно,
[04:49.040 --> 05:00.040]  какую чиселку, назовем ее Q со звездочкой. Ну, чтобы она не путалась с буквой P, которая воспринимается
[05:00.040 --> 05:05.920]  как вероятность ребра случайного графа, у нас-то, конечно, P равно 1 и 2. Я бы мог здесь просто P
[05:05.920 --> 05:12.440]  написать, но может возникнуть путаница. Если я напишу Q, вы подумаете, что это 1 минус P. Поэтому
[05:12.440 --> 05:18.000]  я пишу Q со звездочкой. Но это будет тоже некая вероятность. А именно это число должно быть,
[05:18.000 --> 05:26.360]  конечно, из отрезка 0 и 1, как и всякая вероятность. И мы это будем интерпретировать как вероятность того,
[05:26.360 --> 05:46.560]  что мы выберем k и t и от единицы до х маленькое от g. То есть мы зафиксировали пока что граф,
[05:46.560 --> 05:54.040]  перечислили все его независимые множества. А теперь как бы над каждым из этих независимых множеств
[05:54.040 --> 06:03.040]  подбрасываем монетку, у которой вот такая вот вероятность отвечает за одну из сторон. И вот
[06:03.040 --> 06:08.760]  если одна из сторон, которая отвечает, вот эта вероятность выпадает, то мы выбираем соответствующая
[06:08.760 --> 06:15.120]  k и t. Подбросили первый раз монетку, реализовалась нам условная решка с вероятностью Q со звездочкой,
[06:15.120 --> 06:22.600]  выбираем k1. Подбросили еще раз монетку, с той же самой вероятностью выбираем k2. Или не выбираем,
[06:22.600 --> 06:28.200]  с противоположной вероятностью. Мы запускаем схему испытаний Бернули, в которой вот столько
[06:28.200 --> 06:41.440]  испытаний, а вероятность успеха вот такая. Можно так? Прореживаем в некотором смысле вот эти
[06:41.440 --> 06:53.200]  независимые множества. Вот третье обозначение сейчас будет. Давайте сейчас совсем временно до
[06:53.200 --> 07:00.520]  конца доказательства этой леммы через mu обозначим как раз xkt от g. Не, не так, не xkt,
[07:00.520 --> 07:14.320]  а mat ожидания xkt. Mu это mat ожидания xkt. Ну, то есть это, конечно, то, что мы раньше обозначали fkt от m.
[07:14.320 --> 07:24.960]  Вот так мы это обозначали на прошлой лекции. Mat ожидания xkt это c из m по k на 1 вторая в степени
[07:24.960 --> 07:31.840]  c из k по 2. Если по максимуму брать, то есть четыре варианта записать одно и то же. Можно
[07:31.840 --> 07:38.080]  писать явную формулу, можно написать обозначение прошлого раза, можно просто как это определяется
[07:38.080 --> 07:44.840]  чисто с вероятностной точки зрения. Ну а я хочу совсем одну буковку сейчас mu, но виноват. Так
[07:44.840 --> 07:57.320]  это удобнее, сию секунду mu и все. Ведем просто такое обозначение. Так, сейчас я соображу, чего я
[07:57.320 --> 08:05.240]  теперь хочу сделать. Так, товарищи, с той стороны кто-нибудь есть? Если что-то вдруг непонятно,
[08:05.240 --> 08:10.320]  не стесняйтесь, пожалуйста, задавайте вопросы. Я только не понимаю, мы сейчас в зуме работаем или
[08:10.320 --> 08:20.120]  в ютубе? Первокурсники работали в зуме. У меня сейчас виден зум, но там пока видны только, а в зуме
[08:20.120 --> 08:26.280]  никого. Но у меня виден чат зума, в котором очень много вопросов от первокурсников и никого из
[08:26.280 --> 08:31.880]  второкурсников. Я понял, ну хорошо. Ну а тогда в ютуб, если вы задаете вопросы, мне просто оператор
[08:31.880 --> 08:45.080]  соответственно транслирует, что непонятно. Так, сейчас я хочу ввести некоторое множество.
[08:45.080 --> 08:58.600]  Давайте так, как бы мне его обозначить. Смотрите, идея очень простая. Сейчас обозначение я придумаю,
[08:58.600 --> 09:03.440]  конечно. Идея очень простая. Нас интересуют такие независимые множества, которые мало
[09:03.440 --> 09:10.800]  пересекаются. Давайте, наоборот, попробуем посчитать те пары независимых множеств,
[09:10.800 --> 09:16.440]  которые вредят вот этому нашему желанию, то есть в которых элементы, то есть независимое
[09:16.440 --> 09:29.520]  множество пересекаются не меньше, чем по двум элементам. Вот как-то так. Так, я, кажется,
[09:29.520 --> 09:37.960]  понял. Давайте я еще введу вот тут вот обозначение. Я добавлю сюда фигурные скобки и для этого
[09:37.960 --> 09:45.600]  множества введу обозначение k красивое от g. Ну это просто на несколько минут нам, наверное,
[09:45.600 --> 09:51.680]  поможет. А именно сейчас я буду определять вот эти вредоносные пары, которые сильно пересекаются
[09:51.680 --> 09:56.720]  вопреки нашему желанию, и мне будет удобно отсылать вас просто к кокалиографическому.
[09:56.720 --> 10:22.560]  Так. Ладно, давайте притормаживаю w. Ну давайте w от g. Я просто думаю,
[10:22.560 --> 10:26.120]  как мне обозначить, чтобы это коррелировалось тем, как я это говорил в прошлом году,
[10:26.120 --> 10:38.960]  но это может быть не так уж важно. В конце концов, какая разница. Вот w от g. Это будет
[10:38.960 --> 10:58.200]  количество таких пар k и t, k житая, что k и t, k житая принадлежит k красивому от g, и мощность
[10:58.200 --> 11:08.640]  пересечения k и t и k житого больше либо равняется двойке. Ну как я обещал. Меня чуть-чуть смущает
[11:08.640 --> 11:12.520]  обозначение w от g. Мне чуть-чуть казалось, что я его раньше как-то по-другому писал. Ну какая
[11:12.520 --> 11:20.720]  в чем разница. Ну w и w. w от g это количество пар или может быть не количество лучше, а просто множество,
[11:20.720 --> 11:26.160]  а мощность мы потом сюда навесим. Это неважно. Пусть это будет просто множество таких пар,
[11:26.160 --> 11:32.920]  не количество, а множество. Множество пар, которые нехороши с нашей точки зрения. Так,
[11:32.920 --> 11:44.680]  действительно мат ожидания мощности. Нет, это вы не поймете сходу. Сейчас я пытаюсь понять,
[11:44.680 --> 11:50.080]  чего я хочу сделать. Смотрите, вот здесь был фиксированный граф. Я хочу, чтобы было понятно.
[11:50.080 --> 11:54.520]  Я хочу сделать, чтобы было понятно. Сейчас был фиксированный граф. Мы выделили в нем все его
[11:54.520 --> 12:00.840]  независимое множество. Вот так обозначили их совокупность. А дальше устроили прореживание.
[12:00.840 --> 12:16.200]  Дальше устроили прореживание. Давайте знаете, что через c от g обозначим вот это вот прореженное
[12:16.200 --> 12:21.600]  множество после того, как мы провели испытание Бернули. То есть это случайное множество,
[12:21.600 --> 12:32.240]  являющееся под множеством k от g, коль скоро g фиксирован. Понятно? Ну можно написать распределение
[12:32.240 --> 12:37.200]  вероятностей, конечно. То есть можно написать, с какой вероятностью это множество состоит из
[12:37.200 --> 12:45.200]  конкретного набора k больших с какими-то индексами. Ну понятно, с какой. q со звездочкой в степени
[12:45.200 --> 12:51.600]  количество этих k больших умножить на 1 минус q со звездочкой в степени xk от g минус
[12:51.600 --> 12:58.480]  количество этих k больших. Ну это понятно. Вот. А дальше мы еще навешиваем случайность обратно
[12:58.480 --> 13:04.840]  на граф. То есть если угодно у нас получается такое вероятностное пространство, в котором пары
[13:04.840 --> 13:15.160]  возникают граф, он уже становится случайным, и его вот эта вот случайная совокупность. То есть
[13:15.160 --> 13:20.640]  вероятность этой пары мы воспринимаем просто как произведение вероятности этого графа. Ну она равна
[13:20.640 --> 13:26.400]  1 поделить на 2 в степени c из m под 2, в ней вообще ничего умного нет, на вероятность вот этого c от g,
[13:26.400 --> 13:35.640]  которую я сейчас озвучил. Я понятно сейчас сказал? То есть у нас двойная случайность. У нас и граф
[13:35.640 --> 13:41.880]  случайный, но и вот это множество, которое является под множеством k красивого, оно тоже случайное.
[13:41.880 --> 13:48.040]  А вероятность пары это просто произведение вероятности графа на вероятность вот этого
[13:48.040 --> 13:57.360]  случайного под множество. Так, ну и тогда, черт, мне бы какую-нибудь другую букву что-ли использовать,
[13:57.360 --> 14:10.200]  да. Давайте w штрих o от g запятая c от g. Это будет множество совершенно такое же к аитах, кожитах,
[14:10.200 --> 14:23.000]  но здесь будут к аиты и кожиты, принадлежащие c от g. Ну и то же самое вот это вот условие. Не лезет,
[14:23.000 --> 14:32.880]  но оно то же самое. Так, интересное сообщение появилось или что? Там кто-то есть, да? Это круто.
[14:32.880 --> 14:39.540]  Это товарища задержка из-за того, что мне стали все-таки показывать непосредственно
[14:39.540 --> 14:47.560]  ютуб. Вернее, а в ютубе 7 человек. Я даже вижу сколько человек в ютубе. Ну если есть какие-то
[14:47.560 --> 14:53.340]  вопросы в чат, то можете их задавать уже теперь мне непосредственно, но не суть. То есть два
[14:53.340 --> 14:59.040]  множества отличаются, вот два этих множества, да, это случайные множества, но в первом случае
[14:59.040 --> 15:06.360]  случайен только граф, и мы смотрим на те его независимые множества, которые плохо пересекаются,
[15:06.360 --> 15:11.680]  сильно пересекаются, то есть одновременно не могут присутствовать в той гирлянде, которая
[15:11.680 --> 15:19.120]  вычисляется вот этим y-катом. А во втором случае у нас другое вот это вот вероятностное пространство,
[15:19.120 --> 15:27.160]  и w штрих вычисляет количество вредных пар, уже попавших в прорежанное, дополнительно рандомизированное
[15:27.160 --> 15:41.440]  множество. Понятно сказал, нет? Ну если понятно, если понятно, а единственно, что я еще вот чего
[15:41.440 --> 15:49.600]  хочу сказать, важный момент, может быть пары надо было рисовать не в круглых скобках, а в фигурных,
[15:49.600 --> 15:53.600]  потому что я хочу подчеркнуть, что эти пары у нас будут неупорядоченными.
[15:57.160 --> 16:05.400]  Неважно в каком порядке смотреть на два множества. Подчеркиваю, рисую не кортежи, так сказать,
[16:05.400 --> 16:26.720]  а множество просто. Так, если C,E понятно, то давайте введем обозначение, господи помилуй,
[16:26.720 --> 16:38.680]  сейчас для математического ожидания мощности w обозначим его дельта пополам. Пополам это вот
[16:38.680 --> 16:44.320]  указывает на то, что нам все равно в каком порядке считать пары, а дельта это как раз когда не все
[16:44.320 --> 16:53.480]  равно. Ну так, немножко замудренно сказал, но мне так удобнее просто обозначать и привычнее. Ладно,
[16:53.480 --> 17:02.120]  но давайте начнем с более простого. Вот математическое ожидание мощности множества C. Может с этого
[17:02.120 --> 17:07.680]  и надо было начать, а потом уже w всякие вводить, но неважно. Вот математическое ожидание мощности
[17:07.680 --> 17:16.720]  множества C. У нас есть все обозначения для того, чтобы написать, чему оно равняется. Если вы
[17:16.720 --> 17:22.120]  понимаете, как устроен процесс прореживания, он не зависит от того, как мы выбираем граф. Вы
[17:22.120 --> 17:31.600]  выбрали граф, а дальше уже прорежем. Последовательное усреднение идет и по графам, по всем,
[17:31.600 --> 17:41.200]  и по прореживаниям на множествах, независимых множеств этих графов. Посмотрите, давайте начнем
[17:41.200 --> 17:49.360]  с более простого. Математическое ожидание мощности k. Сейчас вы меня убьете. Что такое
[17:49.360 --> 17:59.840]  математическое ожидание мощности k? Конечно, это мю. То есть это шестое, что ли, или какое там
[17:59.840 --> 18:05.880]  подсчет? Пятое, да? Пятый вариант того, как можно написать любой элемент из этой строчки. Это
[18:05.880 --> 18:13.800]  мат ожидание мощности k, товарищи. Мощность k это xkt от g, значит его математическое ожидание это мю,
[18:13.800 --> 18:21.640]  да, это любая из этих штуковин. Ну виноват. Что ж такого-то? Пятое, да? Может быть с этой
[18:21.640 --> 18:30.760]  подсказкой понятно, каково мат ожидание мощности c. Если мы каждый объект из k красивого выбираем
[18:30.760 --> 18:36.240]  с вероятностью куса звездочкой и не выбираем с вероятностью 1 минус куса звездочкой. Схема
[18:36.240 --> 18:52.080]  Бернули. Что здесь надо написать? Я хочу от вас добиться. Это вроде очевидно. Как считается
[18:52.080 --> 19:01.040]  математическое ожидание числа успехов в схеме Бернули? То, что написано в YouTube,
[19:01.040 --> 19:06.800]  разложить по индикаторам, это доказывать можно и так, конечно. Я бы, по-моему, тут
[19:06.800 --> 19:11.960]  очевидную вещь написал. То есть представьте себе, что само вот это k красивое не случайно,
[19:11.960 --> 19:17.600]  граф g фиксированный. Вот представьте, что граф g фиксированный, с чего мы и начинали. Ну тогда
[19:17.600 --> 19:24.520]  математическое ожидание мощности c. Это что такое? Это очевидно куса звездочкой умножить на вот это,
[19:24.520 --> 19:34.000]  на xk, tg. А дальше мы усредняем по всем графам. Ну куса звездочка это общее для них. Усредняем
[19:34.000 --> 19:40.320]  по всем графам, мю получаем, мю на куса звездочка. Все, что я хочу сказать. Ну да, конечно, это из
[19:40.320 --> 19:50.840]  линейности получается, правда? Сначала одно усреднение проводим, потом другое. Знаете,
[19:50.840 --> 19:58.760]  как представляют порядки суммирования? В сущности оно и есть. Нет вопросов. Более сложный вопрос,
[19:58.760 --> 20:06.440]  но, по-моему, тоже понятный. Мат ожидания мощности w штрих. Коль скоро мат ожидания мощности w такое.
[20:06.440 --> 20:14.600]  Но сейчас уже у вас опыт есть, я думаю, вы должны правильно ответить сразу. Мы делаем то же самое.
[20:14.600 --> 20:22.160]  Мы здесь берем просто граф конкретный, а здесь к этому конкретному графу добавляем еще случайное
[20:22.160 --> 20:29.240]  прореживание. Но при этом мы выбираем множество не по одному, а парами. То есть нам нужно, чтобы
[20:29.240 --> 20:36.400]  каждое множество попало в c от g. Вероятность того, что каитое попадает в c от g, это куса звездочкой.
[20:36.400 --> 20:42.920]  И вероятность того, что кожитое сюда попадает, это тоже куса звездочкой. То, что они оба одновременно
[20:42.920 --> 20:49.160]  попадают сюда, это куса звездочкой в квадрате. Поэтому ответ, ладно уж, дельта пополам умножить на
[20:49.160 --> 21:02.640]  куса звездочкой в квадрате. Понятно? Представляете себе именно так. Мы сначала фиксируем граф,
[21:02.640 --> 21:09.000]  потом из графа выбираем вот эти вот случайные под множество прореживания. Так, чтобы получилось c от g.
[21:09.000 --> 21:15.560]  Усредняя по ним, мы получаем куса звездочкой в квадрате, а затем усредняем по всем графам и
[21:15.560 --> 21:20.840]  просто по определению получаем дельта пополам. Потому что вот этот квадрат, он общий для всех.
[21:20.840 --> 21:32.160]  Ну, я надеюсь, что я понятно объяснил. Прекрасно. Теперь делаем еще одно последнее множество c со
[21:32.160 --> 21:44.800]  звездочкой от g. Мы его получаем так, мы удаляем, это очень стандартная для нас идея, удалить что-то,
[21:44.800 --> 21:53.480]  что нам вредит. У нас такое было, например, когда мы доказывали теорему Эрдаша про хроматическое
[21:53.480 --> 22:01.680]  число и обхват. Помните? Вот мы тоже там чего-то случайно набрали, а потом подправили. Вот здесь
[22:01.680 --> 22:22.120]  также удаляем из c от g по одному множеству из каждой пары, вот здесь видно, почему надо пары считать
[22:22.120 --> 22:38.320]  неупорядоченными. Из каждой пары принадлежащий w' от g. Нам надо разорвать плохие пары. Зачем нам
[22:38.320 --> 22:44.280]  их брать упорядоченными? Не нужно, конечно. Берем их без учета порядка и выкидываем любое множество
[22:44.280 --> 22:50.480]  из такой вот неупорядоченной пары. Одно. И так из каждой пары. Ну, может, мы лишнее повыкидываем,
[22:50.480 --> 22:55.960]  потому что вполне может случиться, что выкидывая одно множество из какой-то пары, мы уничтожим
[22:55.960 --> 23:02.920]  сразу десяток пар, например. Может же такое случиться, что мы лишнее выкинем. Но понятно,
[23:02.920 --> 23:19.040]  что уж во всяком случае мощность c со звездочкой от g точно не меньше, чем мощность c от g минус
[23:19.040 --> 23:35.200]  мощность w' от g c от g. Вот так. Мощность совсем не лезет. Так, друзья, успеваете за мыслью,
[23:35.200 --> 23:42.520]  да? Почему больше либо равняется? Мы выкинули отсюда, может быть, больше множеств. Нет,
[23:42.520 --> 23:50.320]  почему бы сейчас? А, ну да, да, да, потому что выкинув одно множество, мы могли разорвать несколько пар.
[23:50.320 --> 24:03.520]  Так, прекрасно. Ну, давайте усредним все это и по g, и по c от g. То есть у нас получится,
[24:03.520 --> 24:12.520]  что, на самом деле, ну ладно, не важно, мощность c со звездочкой, а мат ожидания мощности c со звездочкой
[24:12.520 --> 24:23.520]  больше либо равняется, нежели мат ожидания мощности c минус мат ожидания мощности w'. Ну, то есть это
[24:23.520 --> 24:36.240]  у нас есть mu на q со звездочкой минус дельта пополам на q со звездочкой в квадрате. Ну, просто
[24:36.240 --> 24:44.000]  подставил то, что мы с вами обсуждали. Так, подставил то, что мы с вами обсуждали. А зачем я это
[24:44.000 --> 24:55.720]  сделал? Так, мат ожидания у Катова, оно-то не меньше, чем мат ожидания c со звездочкой. Ну, у Катой от g просто не меньше,
[24:55.720 --> 25:02.000]  чем c со звездочкой от g. Потому что в c со звездочкой нет ни одной вредной пары, а у Катой это размер
[25:02.000 --> 25:08.760]  самой длинной гирлянды без вредных пар. А c со звездочкой это некий алгоритмический такой вероятностный
[25:08.760 --> 25:15.160]  способ построить конкретную гирлянду. Ну, значит, средняя от максимума не меньше, чем средняя от этой
[25:15.160 --> 25:21.960]  конкретной конструкции. Это и есть наша нижняя оценка, которую мы хотели получить. Но она пока вот в таком
[25:21.960 --> 25:29.800]  виде. Это что-то пока ничего, да? Не, ну, дайте я напомню. Нам-то нужно доказать, я не буду переходить к той
[25:29.800 --> 25:36.840]  доске, что ейк больше либо равняется один плюс умалое от единицы. Это в первую очередь нужно тем,
[25:36.840 --> 25:49.120]  кто сидит в ютубе, на 2k в четвертой. Но, елки-палки, вы мне не сказали, наверное, что там k1 вообще писать надо,
[25:49.120 --> 26:01.360]  да? Давайте я тогда вернусь к той доске. Я забыл. Вот здесь надо писать вот так же. Давайте всюду
[26:01.360 --> 26:10.000]  здесь считаем, что k это то же самое, что k1. Ну, виноват. У нас в прошлый раз было обозначение для параметра k1.
[26:10.000 --> 26:16.960]  А сегодня я чуть-чуть про это забыл. Ну, давайте считать, что все буковки k, которые здесь присутствуют,
[26:16.960 --> 26:23.200]  просто более короткая запись для того параметра k1, который у нас был на прошлой лекции. Естественно,
[26:23.200 --> 26:28.200]  потому что мы доказываем лему в лемме именно для этого случая. Все, а то иначе я должен везде
[26:28.200 --> 26:34.280]  сейчас переписывать, добавлять какой-то лишний индекс, это скучно. Конечно, и вообще этот индекс
[26:34.280 --> 26:39.240]  действительно не смотрелся бы. Ну, в общем, короче, мы вот это хотим доказать, а доказали вот это.
[26:39.240 --> 26:50.720]  Вообще, как вы думаете, ведь кусать звездочкой это параметр, который в нашей власти. Это параметр
[26:50.720 --> 26:56.800]  прореживания, который просто какая-то чиселка из отрезка 0,1. Мы же, наверное, можем его сейчас
[26:56.800 --> 27:06.040]  выбрать по оптимальнее, по возможности. А как его выбрать по оптимальнее, чтобы оценка
[27:06.040 --> 27:13.880]  дальше была еще получше? По параметру q со звездочкой, по этому параметру, то, что мы видим, это парабола.
[27:13.880 --> 27:29.920]  Парабола? Значит, она такая, да? Но это и хорошо, да, потому что... Сейчас. Нет, что-то я...
[27:29.920 --> 27:49.440]  Дождите. Что-то меня заскокло. Парабола, что за смотр? Что-то я не понял. Правда же? А? Я хотел...
[27:59.920 --> 28:06.400]  А, все, да, у меня заскок совершенно тупого характера. Конечно, надо выбрать q со звездочки так,
[28:06.400 --> 28:11.080]  чтобы достигался вот этот максимум, но нельзя писать больше либо равно. Я написал больше либо равно,
[28:11.080 --> 28:17.840]  и стою туплю как идиот. Ну, потому что это не больше либо равно максимума, а просто наилучшим вот
[28:17.840 --> 28:23.920]  это значение будет, когда мы возьмем q со звездочки, где максимум достигается. Ну, конечно, да. То есть
[28:23.920 --> 28:31.920]  надо взять q со звездочкой max, которая равняется минус b поделить на 2a, 2a это минус дельта,
[28:31.920 --> 28:43.560]  минус b это минус mu, то есть mu поделить на дельта. Вот такое вот. Ну, давайте возьмем mu поделить на
[28:43.560 --> 28:51.920]  дельта. Тогда у нас, конечно, здесь получится mu квадрат поделить на 2 дельта вроде бы. Правильно?
[28:51.920 --> 28:58.320]  mu квадрат поделить на дельта, да, минус mu квадрат поделить на 2 дельта. То есть разность будет как
[28:58.320 --> 29:05.720]  раз mu квадрат поделить на 2 дельта. Короче говоря, нам нужно вот это, а мы написали вот так.
[29:05.720 --> 29:31.760]  Прекрасно. Так, ну и что? А все не так плохо. Я вот сюда вот двигаюсь так потихонечку. Я
[29:31.760 --> 29:39.360]  же не зря вспомнил, что это k1. Давайте посмотрим на то, как мы выбирали параметры в самом начале
[29:39.360 --> 29:44.840]  доказательства теоремы в прошлый раз. В прошлый раз это копилось там на какой-то доске, ну а в
[29:44.840 --> 29:51.920]  этот раз оно уже пропало, потому что аудитория другая. В общем, что у нас получалось за счет того,
[29:51.920 --> 30:15.120]  что k это k1? Не, не помните уже. У нас вообще вот про эту штуку кое-что говорилось. Это вот
[30:15.120 --> 30:25.200]  сейчас ружье выстрелит, это будет полный катарсис. А если дельта ноль? Ну, Александр, находящийся в
[30:25.200 --> 30:32.600]  ютубе, ну как дельта может равняться нулю? Не, ну скажите, дельта это математическое ожидание
[30:32.600 --> 30:41.880]  очевидно положительной величины. Александр меня спрашивает, а если дельта ноль? Это, мне кажется,
[30:41.880 --> 30:45.960]  не очень содержательное замечание, потому что дельта, конечно, не может равняться нулю. Это мат
[30:45.960 --> 30:51.760]  ожидания случайной величины, которая не тождественно нулевая и уж точно принимает
[30:51.760 --> 30:58.080]  неотрицательные значения. Дельта это удвоенное мат ожидания мощности некоего множества пар,
[30:58.080 --> 31:06.800]  но понятно, что в очень многих графах это множество пар не пустое, поэтому мат ожидания точно не ноль.
[31:06.800 --> 31:12.600]  На случайном графе, который принимает с положительной вероятностью любое свое
[31:12.600 --> 31:17.680]  конкретное значение. Нет-нет, дельта, конечно, не ноль, там есть другой более тонкий вопрос,
[31:17.680 --> 31:25.680]  и вот этот более тонкий вопрос нам сейчас предстоит обсудить. Там параметр вот этот k1 был
[31:25.680 --> 31:31.440]  с одной стороны выбран таким образом, чтобы оказаться асимпатически равным, но здесь не важно,
[31:31.440 --> 31:39.280]  что писать m или n, m такое было, что это неважно, 2 лог 2 ич на n, но это сейчас тоже не так существенно,
[31:39.280 --> 31:46.080]  скорее вот в этом месте мы писали m в степени 3 плюс о малой от единицы, и это не шестое обозначение
[31:46.080 --> 31:54.600]  для одного и того же, это просто его выражение в более грубом варианте записи. Мы так подобрали,
[31:54.600 --> 32:01.800]  но все-таки возвращаясь вот сюда, нам хочется получить такое неравенство, мы имеем неравенство
[32:01.800 --> 32:13.280]  такого вида, но, наверное, из этого следует как должно выглядеть дельта. Смотрите, если мы знаем,
[32:13.280 --> 32:25.360]  что mu это... сейчас, что-то мне как-то... да-да-да-да, я понимаю, сейчас я, по-моему, немножко не в том
[32:25.360 --> 32:31.200]  порядке излагаю логику, это все то, что я сказал правильно, то есть вот тоже на той доске написано,
[32:31.200 --> 32:38.560]  это все правильно, конечно, было выбрано таким образом, но это сейчас, секунду, что-то я чуть-чуть
[32:38.560 --> 32:48.960]  притормаживаю... а, да не, ну что я действительно притормаживаю? Вот если мне нужна такая оценка,
[32:48.960 --> 32:56.440]  а я написал вот так, то, наверное, это означает, что дельта асимпатически ведет себя каким-то очень
[32:56.440 --> 33:02.120]  понятным образом. Ну что ж я так притормаживаю, действительно, каким образом? То есть мне нужно
[33:02.120 --> 33:10.360]  сказать, что mu2 поделить на 2 дельта асимпатически равно m2 поделить на 2k в четвертой, если я хочу
[33:10.360 --> 33:15.960]  завершить доказательство Лемма в Лене. Я должен убедиться в том, что вот выполнено такое асимпатическое
[33:15.960 --> 33:25.200]  равенство. mu2 на 2 дельта sim tilde m2 поделить на 2k в четвертой. Ну то есть, что отсюда следует
[33:25.200 --> 33:41.080]  дельта асимпатически равно, так это сокращается, mu2 на k в четвертой на m2. Вот давайте временно
[33:41.080 --> 33:49.400]  поверим, что дельта действительно такое. Это последнее, что надо будет проверять, самое последнее,
[33:49.400 --> 33:54.840]  что останется. Помните, я обещал Лемма, Лемма в Лене, потом еще отсылка, вот это еще отсылка. Ну вот
[33:54.840 --> 33:59.760]  сейчас последнее, что мы проверим, что это правда. Но пока это непонятно, почему так, это надо считать
[33:59.760 --> 34:06.400]  честно. Давайте поверим, что это так, тогда все получилось. Что мы доказали Лемму в Лене или нет?
[34:06.400 --> 34:14.440]  Это провокационный вопрос. Если поверить вот в это, это останется проверить, как Лемму в Лене в
[34:14.440 --> 34:23.360]  Лене. Но допустим, мы эту Лемму в Лене в Лене доказали. Замечание Александра про то, что дельта
[34:23.360 --> 34:28.040]  может оказаться нулем, оно не очень состоятельное, потому что, ну не может оно оказаться нулем,
[34:28.040 --> 34:35.400]  хотя это была хорошая попытка. Я предлагаю гораздо более прозрачную и забавную попытку предпринять сейчас.
[34:35.400 --> 34:46.240]  Вот, правильно, да. Кто нам сказал, что вот эта вот штука меньше единицы? Почему это меньше единицы,
[34:46.240 --> 34:54.520]  с какой радости? А вдруг это не так, тогда все наши вероятностные рассуждения мук леш. Смотрите,
[34:54.520 --> 34:59.160]  но мы в это по-прежнему верим, сейчас нам это предстоит как-то проверять, но мы в это верим.
[34:59.160 --> 35:08.200]  μ поделить на дельта асимптатически равно μ поделить на μ квадрат к в четвертый умножить на
[35:08.200 --> 35:17.760]  м в квадрате, шлеп-шлеп, получается м в квадрате поделить на к в четвертый мю. И вот тут, наконец,
[35:17.760 --> 35:27.040]  возвращаемся опять к первой доске, выстреливает вот это. Мы подобрали параметры так, чтобы мю было
[35:27.040 --> 35:33.040]  м в кубе, ну там с какими-то логарифмами, но неважно с какими логарифмами, к это тоже логарифм,
[35:33.040 --> 35:41.600]  но тут м в кубе, то есть это м в квадрате поделить на к в четвертый на м в кубе, но опять же с точностью
[35:41.600 --> 35:46.560]  до логарифмов, все равно это стремится к нулю, а нам достаточно, чтобы при больших значениях
[35:46.560 --> 35:57.480]  m все было меньше единицы, как и получается. Осталось как-то убедиться вот в этом. Сейчас я
[35:57.480 --> 36:06.920]  напишу некую жуткую сумму, которая выражает дельту и постараюсь убедить вас в том, что да,
[36:06.920 --> 36:14.520]  получится так, но кусок доказательства оставлю за рамками, то есть не буду давать его экзамени,
[36:14.520 --> 36:19.760]  конечно, тоже, и сейчас не буду, потому что просто скучные выкладки будут. Так, давайте, наверное,
[36:19.760 --> 36:23.840]  где-нибудь на левой, что ли, доске на самой, вот здесь вот напишем.
[36:49.760 --> 37:03.640]  Так, мат ожидания мощности w, но это, конечно, по линейности считается. Мощность w это количество
[37:03.640 --> 37:14.600]  этих неупорядоченных пар независимых множеств, то есть мы суммируем по всем. Так, тут длинный-длинный
[37:14.840 --> 37:21.440]  вопрос. А почему, если e y k t больше либо равняется константа от x, а константа от x tilde x,
[37:21.440 --> 37:32.440]  то e y k t больше либо равняется f от x? Не понял, а к чему это? Я, к сожалению, стер e y k t,
[37:32.440 --> 37:42.640]  я чуть не понял. Не понял в чем вопрос. Не понял. Но это не константа никакая, дельта совершенно не
[37:42.640 --> 37:49.160]  константа. Это некая функция, которая ведет себя асимпатически вот так. Дельта себя ведет
[37:49.160 --> 37:57.080]  асимпатически вот так, но это в точности означает просто, что дельта это 1 плюс о малое от единицы
[37:57.080 --> 38:07.200]  на μ квадрат k в четвертой на m в квадрате. Это просто одна и та же запись. Вернее, разные
[38:07.200 --> 38:12.200]  записи одного и того же явления. Дельта тильда и дельта равно 1 плюс о малое от единицы.
[38:12.200 --> 38:22.000]  Ну, я просто подставляю вот это все вот сюда вот, и я получаю ровно то, чего хочу. Ну,
[38:22.000 --> 38:28.320]  там будет 1 плюс о малое от единицы в знаменателе, но 1 поделить на 1 плюс о малое от единицы это
[38:28.320 --> 38:33.520]  тоже, конечно, 1 плюс о малое от единицы с какой-то другой конкретной функцией, которая тут стремится
[38:33.520 --> 38:42.960]  к нулю. В общем, я не очень понимаю, в чем этот вопрос. Ну, вот я вроде пояснил. Ренес,
[38:42.960 --> 38:48.280]  я сумел пояснить? На правой доске мы заменили. Да, вот я сейчас это вот как раз и пояснял.
[38:48.280 --> 38:54.560]  Тильда это и есть 1 плюс о маленькой от единицы умножить на. И если это 1 плюс о маленькой от
[38:54.560 --> 38:59.160]  единицы стоит в знаменателе, но это все равно, что написать 1 плюс о маленькой от единицы в
[38:59.160 --> 39:08.600]  числителе. Я надеюсь, что это стало понятно. Вот, я возвращаюсь сюда. Ну, как по линейности это
[39:08.600 --> 39:14.520]  посчитать? Надо просуммировать, сейчас туда-сюда бегаю, надо просуммировать вообще по всем парам
[39:14.520 --> 39:23.400]  множеств мощности K, которые вот так пересекаются, а потом умножить на вероятность того, что оба
[39:23.400 --> 39:39.000]  этих множества являются независимыми в граффиже. То есть картина такая. Есть мэ вершин. Дальше мы
[39:39.000 --> 39:47.080]  фиксируем среди них K вершин какого-то множества K большое и Т. Потом мы фиксируем сколько-то общих
[39:47.080 --> 39:53.400]  вершин, которых будет не меньше двух. К этим общим добавляем еще сколько-то, ну пусть здесь
[39:53.400 --> 40:00.960]  Т вершин, тогда здесь будет K минус Т. Вот эти две сардельки вместе дают К же Т, а вот это вот множество
[40:00.960 --> 40:07.720]  К и Т. А две сардельки вместе это К же Т. И вот надо перебрать все такие пары множеств и для каждого
[40:07.720 --> 40:15.560]  из них проверить оба ли они являются независимыми в случайном графе. То есть просуммировать надо
[40:15.560 --> 40:25.360]  так одну вторую мы тут не пишем или пишем одну вторую, а ну мы же Дельта считаем, тогда одну вторую
[40:25.360 --> 40:39.200]  не пишем. Надо просуммировать по Т от двойки до К без единицы С из М по К на С из К по Т на С из
[40:39.200 --> 40:58.160]  М минус К по К минус Т. И дальше одна вторая в степени 2 С из К по 2 минус С из Т по 2. Вот какая хрень получается.
[40:58.160 --> 41:10.800]  Абсолютная. Не, ну еще раз Т вот это вот количество общих элементов Каитова-Кожитова, поскольку мы
[41:10.800 --> 41:16.200]  сейчас в определении Дельта считаем, что пары все-таки упорядоченные, то на двойку делить не нужно.
[41:16.200 --> 41:24.120]  Вот мы сначала выбираем К вершин, вот эти К вершин для множества Каитова, потом последовательно
[41:24.120 --> 41:31.760]  выбираем из вот этих К вершин Т для кусочка множества Кожитова, который как раз находится в пересечении,
[41:31.760 --> 41:40.520]  потом из оставшихся М минус К вершин выбираем вот эти К минус Т. После чего отсутствующих в обоих множествах
[41:40.520 --> 41:47.760]  суммарно ребер должно быть ровно столько, как я написал. 2 С из К по 2 это если бы они не пересекались,
[41:47.760 --> 41:54.000]  но они имеют Т общих вершин, поэтому надо вычесть еще С из Т по 2. У меня почему-то опять
[41:54.000 --> 42:04.760]  зум появился на экране. Он мигал-мигал и появился, да? Еще раз? Мат ожидания, да, ой, елки-палки,
[42:04.760 --> 42:13.200]  а мощность В, да, это дважды мат ожидания, то есть это Дельта, да, мы Дельта считаем. Ну, я хочу
[42:13.200 --> 42:21.240]  доказать, что Дельта ведет себя симпатически вот так, как здесь подчеркнутым написано. Да, да, да,
[42:21.240 --> 42:29.840]  это мы Дельта считаем. Вот она. Так, все согласны, что Дельта такая? Это понятно? Мерзость абсолютная,
[42:29.840 --> 42:38.040]  понимаете? Значит, что я сейчас сделаю? Я вам покажу, что при Т равном двойке, именно двойке,
[42:38.040 --> 42:46.160]  при начальном слагаемом, вот эта вся хрень, она попроще выглядит, но вот именно она и равна симпатически
[42:46.160 --> 42:55.040]  вот тому, что здесь написано. А больше ничего доказывать не буду. Ну, то есть дальше я скажу,
[42:55.040 --> 43:01.240]  что в принципе надо действовать примерно так же, как мы делали, когда, если помните, доказывали С
[43:01.240 --> 43:08.680]  больше единицы в теореме про связанность случайного графа. Там тоже была жуткая сумма, в которой все
[43:08.680 --> 43:16.560]  было симпатически сконцентрировано в начале. Ну, там была какая-то сумма по К от единицы до N пополам,
[43:16.560 --> 43:25.200]  С из N по К на единицы минус П в степени К на N минус К. Ну, была там какая-то такая сумма. Неважно,
[43:25.200 --> 43:30.680]  можете не вспоминать. Но мы там разбивали суммирование на несколько частей, в одном оценивали
[43:30.680 --> 43:36.320]  геометрической прогрессии, в другом еще там как-то как хвост. В принципе, здесь можно сделать так же,
[43:36.320 --> 43:43.440]  но здесь еще сложнее. Если бы здесь было так же, я бы сделал, но здесь сложнее. Останется просто
[43:43.440 --> 43:49.720]  поверить уже вот в этом месте, останется поверить, что все остальные слагаемые тонут вот в этом начальном.
[43:49.720 --> 43:56.360]  Но у нас такое бывало, поэтому поверить легче. То есть какое-то время дальше вот в этом суммировании
[43:56.360 --> 44:01.240]  все будет геометрически убывать, как геометрическая прогрессия, экспоненциально убывать,
[44:01.240 --> 44:07.480]  потом оно начнет снова возрастать вот к этому К-1, но настолько вырасти не сможет, чтобы
[44:07.480 --> 44:13.120]  достигнуть значения при t равном двойке. Все равно будет бесконечно мало. Но это очень мучительно
[44:13.120 --> 44:19.520]  считать асимптотически, я не буду это делать, потому что это не помогает увеличить сумму ваших знаний.
[44:19.520 --> 44:27.960]  Вот все, что вы понимаете, этим вполне могло бы ограничиться. Это кажется, что уже есть полное
[44:27.960 --> 44:33.400]  доказательство того, что нам нужно. Сумму ваших знаний увеличило бы, знаете, что, если бы я
[44:33.400 --> 44:39.000]  доказал неравенство азумы для липшицевых функций, вот это бы увеличило. Но это еще три лекции,
[44:39.000 --> 44:44.040]  но я же не спецкурс читаю про хроматические числа случайных графов, поэтому давайте все-таки
[44:45.040 --> 44:57.840]  соразмеряться. Ну вот, при t равном двойке мы получаем c из m по k на c из k по 2 на c из m
[44:57.840 --> 45:07.720]  минус k по k минус 2 на одну вторую в степени 2 c из k по 2 минус 1. И это надо поделить,
[45:07.720 --> 45:15.360]  ну поделить в том смысле, что мы же доказываем асимптотическое равенство, давайте поделим на вот
[45:15.360 --> 45:22.560]  эту дробь и убедимся, что в пределе будет единица. Поделим на вот эту дробь и получим
[45:22.560 --> 45:29.320]  предель единица, μ квадрат. Ну ладно, давайте я напишу честно μ квадрат, а потом раскрою,
[45:29.320 --> 45:42.200]  μ квадрат, k в четвертой, а здесь m в квадрате. Так, напоминаю, что μ это просто c из m по k на
[45:42.200 --> 45:55.400]  одна вторая в степени c из k по 2 математическое ожидание x катова. Нет, Александр, двойку перед
[45:55.400 --> 46:01.760]  суммой уже не нужно, потому что если бы вот этой двойки не было, то перед суммой была бы одна
[46:01.760 --> 46:09.040]  вторая, а когда мы на двойку домножили, одна вторая пропала. Дельта это количество упорядоченных пар,
[46:09.040 --> 46:15.600]  а дельта пополам это количество неупорядоченных пар. Я виноват, я немножко запутал, но смысл очень
[46:15.600 --> 46:22.800]  простой. Двойка перед суммой уже не нужна. Так, м квадрат на k в четвертой, а м в квадрате все
[46:22.800 --> 46:33.240]  отлично. Так, и вот оно такое, поэтому давайте еще раз перепишем. Это равняется c из m по k на c из k по 2
[46:33.240 --> 46:45.480]  на c из m минус k по k минус 2. Одна вторая 2 в степени c из k по 2 минус 1. Поделить на c из m по k
[46:45.480 --> 46:56.000]  в квадрате на одна вторая в степени 2 c из k по 2 и на k в четвертой, а потом еще умножить на
[46:56.000 --> 47:01.520]  м квадрат. Вот так вот. Следите только, пожалуйста, очень внимательно, что вот тут я где-нибудь могу
[47:01.520 --> 47:11.000]  наложать и после этого будет ой, не сошлось. Ужас какой, но вроде пока правильно. И главное тут есть
[47:11.000 --> 47:21.840]  элемент радости. Смотрите, шлёп-шлёп, хлоп-хлоп. Видите, как хорошо. Кое-что сокращается. В общем,
[47:21.840 --> 47:29.800]  довольно пакостное сокращается как раз. Сейчас я сотру на центральной доске, себе место освобожу.
[47:41.000 --> 48:00.560]  Так, что нам делать? Давайте я еще на всякий случай напомню, что k это тильда 2 лог 2-ичный
[48:00.560 --> 48:05.920]  m. Я не думаю, что в таком виде нам это понадобится. Важно только, чтобы вы понимали, что k само
[48:05.920 --> 48:12.240]  стремится к бесконечности. У нас вся асимптотика по м, м стремится к бесконечности, но вот k тоже вместе
[48:12.240 --> 48:18.320]  с ним растет, поэтому кое-что тут можно наверняка упростить, например, сказать вот так. Это тильда,
[48:18.320 --> 48:28.000]  так, k квадрат пополам. Я заменил c из k по 2. Но мне асимптотика нужна. Да, и тут еще одна вторая,
[48:28.000 --> 48:34.840]  но в минус первой степени, то есть умножить на 2. Это вот эта одна вторая в минус первый. И дальше
[48:34.840 --> 48:46.440]  умножаем на c из m минус k по k минус 2. Я пока это трогать не буду. Делим на c из m по k. И дальше
[48:46.440 --> 48:57.720]  еще вот это, m в квадрате. Ой, я написал на k в четвертый. Вроде все, да? Все остальное кокнули. Вот
[48:57.720 --> 49:06.680]  эти двойки пропали. Так, k квадрат тоже сокращается вот здесь. В итоге у нас остается
[49:06.680 --> 49:16.720]  c из m минус k по k минус 2 поделить на c из m по k, m в квадрате на k в квадрате. Ну, кажется,
[49:16.720 --> 49:22.240]  я сейчас должен победить. Очень надеюсь на это, потому что все, что мне осталось доказать,
[49:22.240 --> 49:28.240]  это что вот эта первая дробь а симпатически равна k квадрата на m в квадрате. Ну, давайте я ее
[49:28.240 --> 49:42.640]  перепишу. c из m минус k по k минус 2 на c из m по k. Ну, конечно, она довольно противная. Что
[49:42.640 --> 49:50.360]  говорите? Нет, по стирлингу не будем. По стирлингу не будем. И, кстати, воспользуемся все-таки тем,
[49:50.360 --> 49:55.960]  что k логеретмическая. По стирлингу не будем. Мы факториалы честно напишем. m минус k факториал,
[49:55.960 --> 50:06.360]  часть из них сократим. k минус 2 факториал, m минус 2 k плюс 2 факториал. Да, так ведь. Так,
[50:06.360 --> 50:17.400]  умножить на... Тут m факториал, тут k факториал. Так, k минус 2 факториал, m минус k факториал.
[50:17.400 --> 50:23.360]  Снова что-ли? Ой, какая прелесть. Не, ну, на самом деле ничего страшного тут нет. Значит,
[50:23.360 --> 50:30.920]  это равняется, смотрите, k факториал поделить на k минус 2 факториал, это k умножить на k минус 1.
[50:30.920 --> 50:36.920]  Поэтому я напишу, не равняется отиль, да? Мне все с точностью да симптотики достаточно. Я напишу k квадрат.
[50:36.920 --> 50:43.800]  Так, k квадрат нарисовалось. Это очень приятно осознавать, товарищи. k квадрат нарисовалось. А тут
[50:43.800 --> 50:54.600]  остается m минус k факториал. Еще раз, m минус k факториал поделить на m факториал и на
[50:54.600 --> 51:07.680]  m минус 2 k плюс 2 факториал. Ой, боже ж ты ж мой. Ну, что делать? Что делать? Надо сокращать факториалы.
[51:07.680 --> 51:17.760]  Не, ну давайте сокращать, иначе я вас запутаю. Что поделать? k квадрат, оно осталось по-прежнему,
[51:17.760 --> 51:22.720]  ну а факториалы тут, по-моему, все равно какие сокращать. Я бы, честно, m квадрат
[51:22.720 --> 51:29.520]  какой-нибудь отщепил сразу, тогда лучше тильду нарисовать, поскольку мы знаем,
[51:29.520 --> 51:35.920]  что оно должно отщепиться. Вот. Ну, откуда я его отщепил? Например, отсюда. То есть,
[51:35.960 --> 51:43.520]  здесь осталось m минус 2 факториал. Так, чего у меня получается после того, как я сокращаю m минус
[51:43.520 --> 51:51.360]  k факториал и m минус 2 факториал? Ну, в знаменателе остается m минус 2, m минус 3 и так до m минус k
[51:51.360 --> 52:04.040]  плюс 1. Противно. m минус 2, m минус 3, m минус k плюс 1. Следите, следите, могу где-то наврать,
[52:04.040 --> 52:09.520]  но я стараюсь так рассказывать, чтобы за этим уследить было можно. То есть, я отщепил m в квадрате
[52:09.520 --> 52:15.280]  так, чтобы получилось ровно то, чего хочется. У меня тут осталось m минус 2 факториал. Ну, вообще
[52:15.280 --> 52:20.840]  не важно, какой факториал, пусть будет m минус 2 факториал. А тут m минус k факториал. Ну, вот, значит,
[52:20.840 --> 52:28.640]  выживают m минус 2, m минус 3, m минус k плюс 1. Это я вот этих сократил. m минус k плюс 1. А этих вот
[52:28.640 --> 52:42.160]  прям так, как они написаны, сокращаем. Тут сверху остается m минус k, m минус k минус 1, m минус 2 k
[52:42.160 --> 52:54.120]  плюс 3 что ли? Ну да, конечно. Согласны? Так, в ютубе вроде вопросов тоже нет. Так, давайте
[52:54.120 --> 53:00.920]  посчитаем, сколько совмножителей внизу и сколько сверху. Значит, здесь m минус k минус 0, потом m
[53:00.920 --> 53:09.080]  минус k минус 1. То есть, от нуля считаем 1. А это m минус k минус сколько? Минус k вычесть 3, да? То
[53:09.080 --> 53:23.040]  есть, k минус 2. Вот здесь k минус 2 совмножителя. А здесь сколько? Здесь вычитается 2, 3, потом k минус
[53:23.040 --> 53:35.440]  1 тоже k минус 2. Согласны? И тут сверху k минус 2 совмножителя, и тут снизу k минус 2
[53:35.440 --> 53:47.000]  совмножителя. Давайте сделаем гениальный ход. Вытащим просто и сверху m в k минус 2, и снизу m в
[53:47.000 --> 54:02.960]  k минус 2. Сверху останется 1 минус k на m, 1 минус k плюс 1 на m и так далее. 1 минус 2 k минус 3 на m.
[54:02.960 --> 54:15.400]  Ну и снизу такая же фигня. 1 минус 2 на m, 1 минус 3 на m. Я очень подробно объясняю, вы в принципе должны
[54:15.400 --> 54:22.000]  были научиться этому в начале курса, но наверняка же забыли. 1 минус k минус 1 на m. Согласны?
[54:22.000 --> 54:36.440]  Просто из каждой скобки вытащил m. Естественно, вот это вот долой. И вот тут важно, что k очень
[54:36.440 --> 54:43.640]  маленькое. Если б k, например, уже было порядка корня из m или больше, не дай бог, то ничего бы не
[54:43.640 --> 54:50.520]  получилось. Мы с вами, в принципе, в асимптотической части курса проходили с самого начала. У нас такое уже
[54:50.520 --> 54:57.040]  бывало. Ну, я не знаю, поприятнее смотреть на нижнюю часть, а верхнюю сделаете аналогично. То есть нижняя
[54:57.040 --> 55:09.880]  часть, я переписываю 1 минус 2 на m, 1 минус 3 на m, так далее. 1 минус k минус 1 на m. Это e в степени
[55:09.880 --> 55:18.040]  логарифм от 1 минус 2 на m. Абсолютно стандартная вещь. Мы с такой неоднократно сталкивались. Плюс,
[55:18.040 --> 55:23.160]  и так далее. То есть тут набор очень несложных на самом деле инструментов, которым достаточно
[55:23.160 --> 55:30.000]  привыкнуть разок. И все. 1 минус k минус 1 на m. И вы это уже будете в уме как бы считать. Ничего
[55:30.000 --> 55:44.680]  сложного. И дальше по Тейлору раскрываем. E в степени минус 2 на m, минус 3 на m, минус k минус 1 на m.
[55:44.680 --> 55:51.920]  Плюс, ну, господи, можно написать о малой от той же самой суммы и вообще не заморачиваться здесь.
[55:51.920 --> 55:58.880]  Многоточие это вот эта же самая сумма. Просто о малой. Не о большой. Квадраты ненужные.
[55:58.880 --> 56:12.640]  Просто о малой. Вот получится E в степени минус тут m. А тут чего? Ну, сумма не от 1 до k минус 1,
[56:12.640 --> 56:21.480]  а от 2. Но это уже смешно. Я лучше так напишу k квадрат на 2m на 1 плюс о малой от 1. И все.
[56:22.880 --> 56:29.560]  Как квадрат пополам, это сумма числителей с точностью до асимптотики. Ну, и вот эта асимптотика
[56:29.560 --> 56:35.120]  тоже, конечно, отправляется вот сюда, вот этого маленькой от единицы. Ну, k это логарифмическая
[56:35.120 --> 56:41.200]  величина. То есть все, что здесь стоит с огромным запасом, стремится к нулю, а вся экспонента
[56:41.200 --> 56:47.360]  стремится к единице. И точно так же абсолютно устроен числитель. Но только здесь суммирование
[56:47.360 --> 56:55.800]  начнется не от двойки и закончится не k минус 1, а от k и до 2k минус 3. Ну, какая разница?
[56:55.800 --> 57:03.400]  Оно все равно будет порядка k квадрат. А k квадрат это фигня. Все. То есть и числитель стремится к
[57:03.400 --> 57:08.480]  единице, и знаменатель стремится к единице. Значит асимптотика вот ровно такая, как нам
[57:08.480 --> 57:22.520]  хотелось. Все сократилось, и это действительно единица. Ну так вот. Доказал в общем. Ну что,
[57:22.520 --> 57:28.040]  я готов переходить к новой теме, если вопросов по этой истории нету. Я готов переходить к
[57:28.040 --> 57:34.040]  гиперграфам, рассказывать какие-то экстремальные характеристики их, всякие пересечения множества
[57:34.040 --> 57:53.840]  и так далее. Это уже другая тема. Я собираюсь стирать. Да. Но там последнее, что было про двойку
[57:53.840 --> 58:01.400]  перед суммой. Все вроде мы обсудили. Все хорошо. Время еще есть где-то 20 минут. Я сейчас порассуждаю
[58:01.400 --> 58:09.400]  про новую тему. Скорее всего ничего доказывать не буду, просто ввиду объекты, какие-то экстремальные
[58:09.400 --> 58:26.680]  характеристики, которые нам будут интересны. Давайте сразу центральную туску тоже сотру.
[58:39.400 --> 59:04.840]  Так, давайте новое. Значит, гиперграфы и некоторые их тоже экстремальные характеристики. Ну просто
[59:04.840 --> 59:10.400]  характеристики на самом деле экстремальные в том смысле, что как и для графов, нас в основном будут
[59:10.400 --> 59:18.480]  интересовать максимумы или минимумы чего-то. Как число независимости, это максимум множества какого-то.
[59:18.480 --> 59:26.920]  Но гиперграф, что это такое? Мне подарили тут магнитную наклейку на холодильник, на котором
[59:26.920 --> 59:34.960]  изображен я как ведущий этой самой, как называется, игры. Господи, Якубович, как она называется?
[59:34.960 --> 59:42.720]  Стол они крутят. Поле чудес, да, как ведущий поле чудес. Написано, значит, там в клеточках,
[59:42.720 --> 59:50.380]  как положено. Почти все слово только пропущено там, где гиперграф. А я стою с таким, знаете,
[59:50.380 --> 59:57.740]  кабачком цукини в руке. Почему цукини? Потому что гиперграф это практически то же самое,
[59:57.740 --> 01:00:05.180]  что граф. Странно, что, кстати, не с сарделькой, а именно с кабачком. Это то же самое, что граф,
[01:00:05.180 --> 01:00:10.940]  но ребра у него это не двухвершинное множество, не двухэлементное множество, а сколько-то,
[01:00:10.940 --> 01:00:16.580]  неважно сколько, двух, трех, пяти. То есть это такой же объект, есть множество вершин,
[01:00:16.580 --> 01:00:25.860]  есть множество ребер, но если вершины это просто какое-то множество, обычно конечное,
[01:00:25.860 --> 01:00:36.500]  то ребра, можно вот так написать, это подмножество в множестве всех подмножеств. 2 в степени В это
[01:00:36.500 --> 01:00:43.320]  множество всех подмножеств, а если я пишу именно такое включение, это значит, что E является
[01:00:43.320 --> 01:00:52.460]  набором каких-то объектов из вот этого множества, то есть совокупностью подмножеств. Иными словами,
[01:00:52.460 --> 01:01:01.940]  для любого A из E, A это какое-то подмножество, может быть, совпадающее даже с В. Все множество
[01:01:01.940 --> 01:01:09.460]  вершин тоже может быть ребром гиперграфа. Давайте только считать для дальнейшего, что мощность A никогда
[01:01:09.460 --> 01:01:18.660]  не принадлежать у нас будет 0 и единица. То есть мы не будем с вами рассматривать гиперграфы,
[01:01:18.660 --> 01:01:25.140]  у которых бывают пустые ребра или ребра, состоящие только из одной вершины. Каждое
[01:01:25.140 --> 01:01:31.220]  ребро в нашем понимании будет состоять хотя бы из двух вершин. Это не значит, что гиперграф
[01:01:31.220 --> 01:01:36.940]  с такими условиями, без перечеркиваний, нельзя определять, можно, но мы такие не будем рассматривать.
[01:01:36.940 --> 01:01:45.180]  Мы такие рассматривать не будем. Мы будем считать, что мощность A не принадлежит 0 и 1. Больше того,
[01:01:45.180 --> 01:02:00.100]  мы будем говорить с вами всюду только про K однородные гиперграфы. Ну то есть для любого A из E
[01:02:00.100 --> 01:02:11.940]  мощность A будет равняться K. Все ребра будут одной и той же мощности и, конечно, два однородные
[01:02:11.940 --> 01:02:20.100]  гиперграфы это просто обычный граф. Мы будем рассматривать гиперграфы всегда в таком же
[01:02:20.100 --> 01:02:27.100]  сугубо обыкновенном смысле, как и графы. То есть мы будем считать, что ребра неупорядочены. Это
[01:02:27.100 --> 01:02:39.220]  просто сочетание каких-то объектов из V. Рёбра неупорядочены, вершины в них, конечно,
[01:02:39.220 --> 01:02:45.580]  не повторяются. Кратных ребер нет, это тоже видно прямо отсюда. Вершины не повторяются,
[01:02:45.580 --> 01:02:51.340]  значит как бы аналогов петель не бывает. Ну вот такой совсем обыкновенный будет гиперграф.
[01:02:51.340 --> 01:02:59.980]  Понятно, что если графы было удобно изображать на плоскости, рисуя какие-то отрезочки или дуги,
[01:02:59.980 --> 01:03:08.060]  соединяющие вершины точки, то с гиперграфом все не так здорово. Но серьезно, либо кабачок цукини,
[01:03:08.060 --> 01:03:15.900]  либо сарделька. Мне кажется, сарделька это прекрасный способ изобразить ребро.
[01:03:15.900 --> 01:03:25.900]  Три однородных гиперграфа. Какая сарделька. Вот эти три вершины, например, тоже образуют вот
[01:03:25.900 --> 01:03:42.300]  такой вот. Как же это нарисовать? Вот так. Вот тоже сарделька. Я согласен, что рисовать такую штуку
[01:03:42.300 --> 01:03:47.500]  на самом деле, конечно, совершенно невозможно. Поэтому сардельки, это скорее для меня лично
[01:03:47.500 --> 01:03:53.580]  приятный, просто чисто на вкус приятный, наверное, способ представить себе, что происходит. Я
[01:03:53.580 --> 01:03:58.500]  действительно представляю себе такое множество объектов и там какие-то подмножества, как сардельки
[01:03:58.500 --> 01:04:04.900]  такие вот, извиваются и все. Вы можете представлять так, как вам это удобнее. Понятно, что люди,
[01:04:04.900 --> 01:04:11.660]  которые среди вас, например, изучают топологию, они наверняка или почти наверняка слышали некий
[01:04:11.660 --> 01:04:17.340]  вариант истории про гиперграф, который называется симплециальный комплекс, но может и не слышали.
[01:04:17.340 --> 01:04:23.100]  Не было симплециальных комплексов, ну услышите когда-нибудь, точно будут. Вот это тоже гиперграф,
[01:04:23.100 --> 01:04:27.780]  но с таким он не однородный, он со свойством наследования. То есть если там есть какое-то
[01:04:27.780 --> 01:04:32.900]  ребро мощности 3, то и все его подмножества тоже являются ребрами. Ладно, это проехали,
[01:04:32.900 --> 01:04:39.180]  это никто не знает, поэтому я замолчал. Никаких гиперграфов, симплециальных комплексов не будет.
[01:04:39.180 --> 01:04:54.460]  Гиперграф и гиперграф. Такая вот набор сарделек в кастрюке. Вот, давайте, наверное, сразу ведем
[01:04:54.460 --> 01:05:00.380]  три величины, изучением которых мы будем заниматься для гиперграфов, потому что они важные. Они
[01:05:00.380 --> 01:05:07.060]  связаны с теорией кодирования, они связаны с разными задачами чистой математики и ее приложений,
[01:05:07.060 --> 01:05:14.060]  поэтому я считаю крайне важным все их три обсудить в том или ином виде. Давайте начнем
[01:05:14.060 --> 01:05:25.340]  с величины, ну давайте f от nrs. Я здесь буду писать буквы rs, ну сейчас вы увидите, что это такое,
[01:05:25.340 --> 01:05:30.620]  три параметра. Параметры очень простые. Значит, на самом деле это вот что такое,
[01:05:30.620 --> 01:05:40.220]  сейчас я напишу формально, потом аккуратно прокомментирую. Это максимальное такое k,
[01:05:40.220 --> 01:05:56.540]  что существует r однородный гиперграф на n вершинах.
[01:06:10.220 --> 01:06:26.780]  Сейчас я просто думаю, ну ладно, давайте так. Такое, что мощность e равняется k и для любых ab,
[01:06:26.780 --> 01:06:39.500]  принадлежащих e, мощность a пересеченного с b не меньше, чем с. Букв много, сейчас поясню,
[01:06:39.500 --> 01:06:45.580]  все на самом деле правда несложно. Значит, что говорится? Говорится, давайте считать,
[01:06:45.580 --> 01:07:02.540]  что у гиперграфа сейчас n вершин, вершин n штук. Ребра каждая состоят из r вершин,
[01:07:02.540 --> 01:07:08.900]  r однородный, значит, каждое ребро это просто под множеством мощности r. Ну вот какое-то под
[01:07:08.900 --> 01:07:18.700]  множество условной мощности r, вот там какое-то, ой, не попал, под множество условной мощности r,
[01:07:18.700 --> 01:07:24.380]  ну тут тройки для примера нарисованы, ну и так далее. Вот такие вот r-элементные сардели. Но
[01:07:24.380 --> 01:07:30.820]  от них дополнительно требуется, чтобы они попарно имели не менее, чем s общих элементов,
[01:07:30.820 --> 01:07:37.700]  пересекались достаточно сильно, каждые два множества. Но вот на этом рисунке s равно единице,
[01:07:37.860 --> 01:07:44.980]  или нулю, как хотите, но нулю бессмысленные ограничения, а единицу уже какой-то смысл имеет.
[01:07:44.980 --> 01:07:54.060]  Можно вот такую еще сардельку добавить, такую, будет последовательный такой цикл из трех сарделек,
[01:07:54.060 --> 01:07:59.900]  которые попарно пересекаются ровно по одному элементу. Ничего не напоминает? Только что было,
[01:07:59.900 --> 01:08:08.500]  ук. Вот это про то самое. Но в ук требовалось, чтобы они попарно пересекались не больше,
[01:08:08.500 --> 01:08:13.940]  чем по единице, а здесь мы говорим наоборот не меньше, чем, ну и уже не по единице,
[01:08:13.940 --> 01:08:18.100]  а там по какому-то абстрактному s, которое зафиксировано заранее. Ну на самом деле,
[01:08:18.100 --> 01:08:24.380]  чтобы совсем было действительно хорошо, давайте я сразу скажу h от nrs, то же самое,
[01:08:24.380 --> 01:08:30.220]  только вот здесь меньше либо равно s. А вот это уже в точности ук, прям вот один в один.
[01:08:30.220 --> 01:08:40.500]  Ну просто ук, там вот это r, оно же k, было каким-то конкретным, а здесь або каким является.
[01:08:40.500 --> 01:08:47.340]  Опять же s там равнялась единице, а у нас оно або какое теперь? То есть вот это задача теории
[01:08:47.340 --> 01:08:53.460]  кодирования, прямо в чистом виде. Я могу не повторять эту историю, потому что это в прошлый раз
[01:08:53.460 --> 01:09:02.460]  обсуждалось. И там и там максимумы, то есть нас интересует, насколько много можно построить ребер,
[01:09:02.460 --> 01:09:08.020]  ну мало-то понятно, вообще ни одного ребра не возьмете, условия все формально будут выполнены,
[01:09:08.020 --> 01:09:12.780]  одно ребро возьмете, все условия формально будут выполнены. То есть хочется построить как можно
[01:09:12.780 --> 01:09:20.580]  больше ребер данной мощности, на множестве вершин тоже данной мощности, чтобы эти ребра попарно
[01:09:20.580 --> 01:09:27.300]  как-то вот друг относительно друга располагались. В этом случае сильно попарно друг друга цепляли,
[01:09:27.300 --> 01:09:33.980]  а в этом случае наоборот слабо. И еще есть величина m от nrs, которая нас очень будет интересовать,
[01:09:33.980 --> 01:09:45.620]  это вот такая. Ну они там как-то еще между собой хитро связаны, конечно. Так, первый вопрос,
[01:09:45.620 --> 01:09:51.020]  все-таки нужно ли, наверное, пояснить? Я вот сейчас подумал, я про кодирование-то может не говорил,
[01:09:51.020 --> 01:09:57.380]  почему вот эта задача про h, она задача теории кодирования, или того, что было когда-то на
[01:09:57.380 --> 01:10:05.300]  окотече, а я это рассказывал, вам достаточно? Наверное, нет, да, забыли уже все. Почему это,
[01:10:05.300 --> 01:10:12.260]  да, типичная задача теории кодирования? Наверное, это все, что я успею сегодня сделать за оставшееся
[01:10:12.260 --> 01:10:19.580]  время. Ну и хорошо, что перегружать-то информации? Почему h от nrs это про кодирование?
[01:10:29.940 --> 01:10:35.820]  Потому что фактически, я объясняю, почему h от nrs это задача теории кодирования. Писать на
[01:10:35.820 --> 01:10:41.500]  доске я не буду, слова мне несколько раз повторю. Так, потому что фактически можно
[01:10:41.500 --> 01:10:54.580]  интерпретировать всю эту историю следующим образом. У вас ребро а это не что иное, как вектор
[01:10:54.580 --> 01:11:10.900]  из единиц и нулей, вектора из единиц и нулей, у которого r единиц, ну и все, и n-r нулей. Можно
[01:11:10.900 --> 01:11:16.660]  так интерпретировать, но понятно, что имеется в виду. Если у нас а состоял из элементов 1, 2 и
[01:11:16.660 --> 01:11:26.540]  так далее r из вершин 1, 2, давайте так, v равняется 1, 2 и так далее n, это множество вершин.
[01:11:26.540 --> 01:11:33.940]  Если а у вас состоял из первых, например, r элементов множества v, то здесь будет первые r
[01:11:33.940 --> 01:11:41.140]  единицы, а дальше n-r нулей. Стандартная история, так у меня появился вопрос. Да-да-да, все,
[01:11:41.140 --> 01:11:44.500]  рассказываю, как это связано с теорией кодирования, просто чертова задержка в этом
[01:11:44.500 --> 01:11:50.180]  ютубе. Вопросы приходят позже, чем я начинаю на них отвечать. Я уже рассказываю, как это
[01:11:50.180 --> 01:11:56.100]  связано с теорией кодирования. Все поняли, да, как векторы строятся, но мы такое делали уже на
[01:11:56.100 --> 01:12:02.500]  этих лекциях. Строятся векторы, такие характеристические, что ли, соответствующие этим множеству. Векторы из
[01:12:02.500 --> 01:12:07.540]  нулей единиц. Ну что такое теория кодирования? Надо передавать каждое конкретное слово по
[01:12:07.540 --> 01:12:20.740]  каналу связи, кодируя его чем-то вот таким. Вы берете слово мама и ставите ему в соответствие,
[01:12:20.740 --> 01:12:33.540]  например, вот такой вектор r единиц, а дальше n-r нулей. Вам берете слово папа, а ему ставите
[01:12:33.540 --> 01:12:41.780]  в соответствие n-r нулей, а дальше единица. Ну, например, то есть как-то. И вот у вас есть передатчик,
[01:12:41.780 --> 01:12:48.540]  вот у вас есть приемник, вот есть канал связи. Вы по каналу связи последовательно передаете вот
[01:12:48.540 --> 01:12:59.540]  эти единички и нолики. Так? Передали единичку, а что получит приемник? Единичку? Ну на самом деле
[01:12:59.540 --> 01:13:06.380]  черт его знает. Может тут есть какие-то помехи на канале связи? Могут быть помехи. И мы можем знать,
[01:13:06.380 --> 01:13:12.820]  ну я точно рассказывал задачу теории кодирования в окотече вам, потому что я же рассказывал границу
[01:13:12.820 --> 01:13:21.380]  Плоткина. Вот это вот как раз про вот это. Мы можем знать, сколько ошибок допускается на одно слово,
[01:13:21.380 --> 01:13:27.740]  то есть сколько единиц и нулей суммарно могут преобразиться во что-то противоположное. Мы знаем,
[01:13:27.740 --> 01:13:34.420]  что есть какие-то там D-искажения. Искажения это превращение единицы в ноль не более чем D-искажения.
[01:13:34.420 --> 01:13:38.820]  Единица может превратиться в ноль, ноль может превратиться в единицу. Вот когда вы передаете
[01:13:38.820 --> 01:13:44.420]  очередное слово как последовательность из единицы нулей, вы точно знаете, что не более чем D
[01:13:44.420 --> 01:13:51.900]  ошибок будет допущено на этом канале. Вопрос состоит в том, как построить максимально большое
[01:13:51.900 --> 01:13:58.500]  количество вот таких вот кодовых слов последовательности из нулей единиц, чтобы на выходе можно
[01:13:58.500 --> 01:14:05.580]  было однозначно восстановить информацию. Ну что это значит? Вы рассматриваете какое-то кодовое
[01:14:05.580 --> 01:14:19.380]  слово, например вот такое. Рассматриваете какое-то другое кодовое слово, например вот такое. А дальше
[01:14:19.380 --> 01:14:29.100]  смотрите все слова, которые могут получиться из этого слова за счет не более чем D-искажений.
[01:14:29.100 --> 01:14:34.500]  Ну одно искажение, это значит какое-то здесь может быть нолик появился. Короче,
[01:14:34.500 --> 01:14:39.420]  вы рассматриваете такой круг. Помните, я говорил хемминговое расстояние, это все было в АКТЧ.
[01:14:39.420 --> 01:14:48.140]  Рассматриваете такой круг, у которого хеммингов радиус, это как раз вот это вот D. Хеммингов
[01:14:48.140 --> 01:14:53.220]  радиус, это как раз вот это D. Если два таких круга пересекаются, это значит,
[01:14:53.220 --> 01:14:58.980]  вы плохие слова взяли в свой словарь. Потому что представим себе, что это слово превратилось
[01:14:58.980 --> 01:15:04.060]  в какое-то вот такое, и это тоже может, в неможе превратиться. Все, вы не восстановите информацию
[01:15:04.060 --> 01:15:11.220]  никак. Но здесь как раз взят пример таких двух слов, которыми это, наверное, не случится,
[01:15:11.220 --> 01:15:17.140]  когда R фиксировано, а N большое. То есть я плохую картину нарисовал. Тут будет скорее вот так как раз.
[01:15:17.240 --> 01:15:24.340]  Они не будут эти круги пересекаться, а раз они не будут пересекаться, то мы всегда точно можем
[01:15:24.340 --> 01:15:30.660]  сказать, какое бы ни было здесь слово, оно точно не из этого слова получено. Какое бы ни было здесь
[01:15:30.660 --> 01:15:35.840]  слово, оно точно не из этого слова получено. Все заранее знают словарь. Все все заранее знают.
[01:15:35.840 --> 01:15:42.820]  Это не криптография, это все я говорил в АКТЧ. Это способ исправлять ошибки при передаче по
[01:15:42.820 --> 01:15:49.060]  зашумленному каналу связи, когда известно, какое сверху ограничение на
[01:15:49.060 --> 01:15:55.820]  количество искажений на каждое слово. Так, ну хорошо, а при чем здесь все-таки задачи
[01:15:55.820 --> 01:16:02.260]  про h от nrs еще раз? Это мы не договорились. Это мы вроде как не договорились.
[01:16:02.260 --> 01:16:09.060]  Но вот что отвечает за хеминговое расстояние между вот такими двумя словами?
[01:16:09.060 --> 01:16:14.260]  Фактически мы же хотим чего сделать? Мы хотим сделать так, чтобы каждые два
[01:16:14.260 --> 01:16:21.980]  слова в нашем коде отстояли друг от друга на расстоянии строго больше, чем 2d.
[01:16:21.980 --> 01:16:35.220]  Это понятно? Вот у нас есть какое-то слово, вот есть какое-то другое слово, у них
[01:16:35.220 --> 01:16:42.060]  сколько-то общих единиц, сколько-то общих нулей. Вот есть какие-то два слова.
[01:16:42.060 --> 01:16:47.180]  Когда расстояние по хемингу, то есть количество отличающихся координат,
[01:16:47.180 --> 01:16:53.980]  между ними больше какого-то 2d, например. Вот когда оно больше, чем 2d? При каких
[01:16:53.980 --> 01:17:00.820]  условиях? Что это значит? Это значит, что и таких много, и таких много. Но у нас r
[01:17:00.820 --> 01:17:10.980]  единиц тут, и r единиц тут. Значит, вот это равно вот этому, очевидно. Ну то есть сумма вот этих
[01:17:10.980 --> 01:17:18.620]  величин больше, чем 2d. Если каждая из них больше, чем d, а это в точности то же самое,
[01:17:18.620 --> 01:17:28.980]  как посмотреть на пересечение и сказать, что оно меньше, чем r-d. Вот это пересечение должно
[01:17:28.980 --> 01:17:36.860]  быть меньше, чем r-d. Вот обозначите это за s, наверное, минус 1, и вы придете в точности к
[01:17:36.860 --> 01:17:44.620]  задаче отыскания максимального количества таких кодовых слов. Максимальный размер словаря,
[01:17:44.620 --> 01:17:49.980]  который можно составить для передачи информации по этому каналу связи, если мы хотим с гарантией
[01:17:49.980 --> 01:18:00.380]  восстанавливать, какое слово передавали. Не знаю, я понятно объяснил? Ну с учетом того,
[01:18:00.380 --> 01:18:05.620]  что это было в прошлом году, я надеюсь, что тем более понятно. Границу-то плотки, наверное,
[01:18:05.620 --> 01:18:11.180]  действительно давал с помощью матрицы Адамара. Так что кое-что у нас про это было. Но вот кроме
[01:18:11.180 --> 01:18:17.060]  матрицы Адамара, оказывается, есть еще такая гиперграфовская история, которая легко транслируется
[01:18:17.060 --> 01:18:24.300]  в историю с теорией кодирования. Я еще одну вещь успею сказать про вот это. Это у нас тоже было.
[01:18:24.300 --> 01:18:34.460]  У нас с вами было m от n3,1, товарищи, в некотором контексте. А именно у нас был, если помните,
[01:18:34.460 --> 01:18:41.540]  такой замечательный граф. Он дважды был. Сначала он был как иллюстрация к Гамильтоновости по
[01:18:41.540 --> 01:18:48.180]  Эрдешу и Хайнеллу, а потом он был как пример, если кто помнит, рамсейского графа. Хотя слово
[01:18:48.180 --> 01:18:55.340]  рамсей там скользко произносилось. В общем, как пример ситуации, когда альфы и омега одновременно
[01:18:55.340 --> 01:19:02.540]  маленькие. Помните такую историю? В общем, это был граф, у которого вершины это всевозможные
[01:19:02.540 --> 01:19:14.340]  тройки элементов вот отсюда. Ну или что то же самое, векторы как раз из нулей единиц,
[01:19:14.340 --> 01:19:21.060]  с тремя единиц, мен минус тремя нулями. Мы его и так и так интерпретировали. А ребрами мы называли
[01:19:21.060 --> 01:19:30.340]  любую пару вершин, мощность пересечения которых в точности равнялась единице. Помните такой граф?
[01:19:30.340 --> 01:19:40.540]  Дважды использовался. Так вот, альфа от этого графа, а ровно его мы как раз читали, причем мы даже
[01:19:40.540 --> 01:19:45.660]  использовали, если кто-то еще помнит, красивый линейно-алгебрайический метод. В общем, альфа от этого
[01:19:45.660 --> 01:19:55.220]  графа это в точности м от n3-1. Потому что нас как раз интересует самое большое количество троек,
[01:19:55.220 --> 01:20:04.660]  никакие две из которых не пересекаются ровно по единице. Альфа от этого графа. И вообще,
[01:20:04.660 --> 01:20:15.620]  можно ввести граф именно g от nrs, у которого вот здесь будет вместо тройки r, а здесь вместо
[01:20:15.620 --> 01:20:27.100]  единицы будет s. И альфа уже вот от этого графа это в точности м от nrs. Вот такая связь.
[01:20:27.100 --> 01:20:38.180]  Понятно? Ну все на сегодня. Если вопросы какие-то остались, спрашивайте, а так все.
[01:20:38.180 --> 01:20:44.060]  Ну в следующий раз займемся уже изучением этих величин, то есть как-то оценивать будем и так далее.
[01:20:44.060 --> 01:21:07.100]  Ну не очень понятно, что вы имеете в виду под этим. То есть, видите, мы сопоставили
[01:21:07.100 --> 01:21:13.980]  гиперграфовской задачи, теоретикографовую, но информация другая. То есть, здесь нам нужно
[01:21:13.980 --> 01:21:19.700]  просто альфа найти, но это то же самое, что найти на гиперграфе такую характеристику. Конечно, да,
[01:21:19.700 --> 01:21:24.940]  вот именно эту характеристику гиперграфа экстремальную можно выразить вот в таких
[01:21:24.940 --> 01:21:30.220]  графовских терминах. Но вообще говоря, информация, если с практической точки зрения смотреть,
[01:21:30.220 --> 01:21:35.940]  которая содержится в гиперграфе, она больше в любом случае, чем та, которая в графе. Например,
[01:21:35.940 --> 01:21:41.980]  ну какая практическая точка зрения. Знаете, вот любят люди изучать, и это бывает очень полезно,
[01:21:41.980 --> 01:21:48.900]  графы каких-нибудь соавторств. То есть, есть большое количество математиков, например, и
[01:21:48.900 --> 01:21:54.780]  некоторые из них пишут совместные статьи. Ну обычно какой граф строят? Вершины это математики,
[01:21:54.780 --> 01:22:01.740]  а ребрами соединяют двух математиков, если у них есть общая совместная работа. Ну можно мультиграф
[01:22:01.740 --> 01:22:07.420]  делать, если совместных работ много. Но, конечно, в этом графе, мультиграф, то есть с кратными
[01:22:07.420 --> 01:22:15.340]  ребрами, в этом графе информация о структуре взаимодействия между авторами точно беднее,
[01:22:15.340 --> 01:22:22.380]  чем если вы составите гиперграф, в котором вот эти гиперрёбра, это будет прямо множество соавторов
[01:22:22.380 --> 01:22:28.740]  конкретной статьи. Потому что в конкретной статье может быть не два автора, а десять, там,
[01:22:28.740 --> 01:22:33.780]  пятнадцать. Бывают мегаколлаборации по тысячи соавторов, по пять тысяч даже. Там всякие
[01:22:33.780 --> 01:22:39.180]  экспериментальные работы по физике так пишутся. В ЦЕРНе сидит огромное количество экспериментаторов,
[01:22:39.180 --> 01:22:47.660]  в МФТИ тоже, и вот получается гигантское количество соавторов, гиперрёбро колоссальное. Так,
[01:22:47.660 --> 01:22:55.340]  Александр задал прекрасный вопрос тем временем в Ютубе. GATNRS действительно называется графом
[01:22:55.340 --> 01:23:02.960]  Джонсона, и это терминология, которая как раз идет из теории кодирования. Разумеется,
[01:23:02.960 --> 01:23:10.660]  для теории кодирования эти графы тоже нужны. Тут мы запрещаем все маленькие расстояния,
[01:23:10.660 --> 01:23:15.580]  а тут мы запрещаем конкретное расстояние Хемминга. Понятно, что величины в итоге окажутся так или
[01:23:15.580 --> 01:23:22.180]  иначе друг на друга влияющими, поэтому специалисты по теории кодирования GATNRS тоже изучают. И вот в
[01:23:22.180 --> 01:23:27.860]  теории кодирования такие графы называются графами Джонсона. Есть и другие науки типа
[01:23:27.860 --> 01:23:32.700]  комбинаторной геометрии, где эта терминология не так прижилась, но тем не менее это правильное
[01:23:32.700 --> 01:23:37.860]  действительно название, а у нас звенит звонок. Так что, наверное, на этом всё.

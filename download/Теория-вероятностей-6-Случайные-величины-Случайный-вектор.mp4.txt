[00:00.000 --> 00:11.420]  величины вели с вами дискретные непрерывные случайные величины и собственно дальше нам
[00:11.420 --> 00:21.700]  надо бы рассмотреть примеры этих случайных величин но чуть-чуть притормозим поскольку я забыл
[00:21.700 --> 00:28.700]  вести еще одно понятие когда мы вводили функцию распределения случайной величины напомню да
[00:28.700 --> 00:38.860]  функция распределения случайной величины кси в точке x это вероятностная мера омега таких
[00:38.860 --> 00:48.100]  что кси отомина меньше х ну или более краткой вероятность того что кси меньше х вот значит
[00:48.100 --> 00:58.100]  ввели функцию распределения случайной величины и совершенно аналогично мы можем ввести ну не
[00:58.100 --> 01:09.180]  аналогично а следует теории можем ввести условную функцию распределения и в кси от x при условии
[01:09.180 --> 01:18.740]  что омега принадлежит некоторому множеству сигма алгебры вот так ну множеству а которая
[01:18.740 --> 01:26.740]  в свою очередь принадлежит сигма алгебре значит что это опять же по определению это вероятностная
[01:26.740 --> 01:39.660]  мера омега таких что кси отоминга меньше х при условии что омега принадлежит а ну и как полагается
[01:39.660 --> 01:49.020]  для там условных вероятностей мы знаем чему это равно это равно вероятности того точнее говоря
[01:49.020 --> 01:57.420]  вероятности пересечения таких множеств кси отоминга меньше х одно множество в исходном
[01:57.420 --> 02:07.620]  пространстве пересечение с множеством омега принадлежит а и поделить это на вероятность того
[02:07.620 --> 02:17.340]  что омега принадлежит а то есть ну такое естественное обобщение если может так выразиться
[02:17.340 --> 02:24.740]  поскольку условная вероятность это в общем обычная вероятность со всеми свойствами вероятности то
[02:24.740 --> 02:31.220]  и условная функция распределения это тоже как бы обычная функция распределения тоже со всеми
[02:31.220 --> 02:40.260]  свойствами но в качестве во первых примера а во вторых движение так сказать к нашей цели давайте
[02:40.260 --> 02:48.980]  рассмотрим условную функцию распределения случайной величины кси при условии что случайная
[02:48.980 --> 02:55.620]  величина это принадлежит некоторому множеству b это уже из барельевской сигма алгебры то есть
[02:55.620 --> 03:01.900]  из пространства где значение у нас случайно влечено в принципе разумеется это все можно
[03:01.900 --> 03:08.220]  переписать и в терминах исходного вероятностного пространства но я этот случай выделяю отдельно
[03:08.380 --> 03:17.340]  но как бы одна из основных идей введения случайных величин состоит в том что мы
[03:17.340 --> 03:24.660]  хотим отказаться от специфики исходного пространства мы хотим работать с числами или как бы
[03:24.660 --> 03:31.740]  с числами со случайными величинами поэтому чем быстрее мы сможем отвязаться от исходного
[03:31.740 --> 03:36.620]  вероятностного пространства тем ну по большому счету лучше для нашего дальнейшего изложения patterns
[03:36.620 --> 03:45.500]  это вероятность того, что кси меньше х при условии, что другая случайная величина
[03:45.500 --> 03:53.020]  принадлежит некому барелийскому множеству. Ну и как бы понятно, что это такое
[03:53.020 --> 04:04.300]  значит дальше. Ну, например, еще один пример, еще более частный случай, да, кси х при
[04:04.300 --> 04:13.420]  условии, что это меньше у, то есть вот это множество b, это вот от минуса бесконечности до у.
[04:13.420 --> 04:22.540]  Ну тогда понятно, что это такое, это f, точнее говоря, вероятность того, что кси меньше х и
[04:22.540 --> 04:32.780]  одновременно с этим это меньше у, делит на вероятность того, что это это меньше у.
[04:32.780 --> 04:43.940]  Если, например, у нас это дискретная случайная величина, то, а ну да, понятное дело, что условная
[04:43.940 --> 04:48.980]  функция распределения определяна только тогда, когда вероятность номера вот этого множества не равна
[04:48.980 --> 04:56.260]  нулю, ну как и условная вероятность. Вот если, например, это дискретная случайная величина и
[04:56.260 --> 05:03.820]  вероятность того, что это строго равно у, не равна нулю, то можно получить условную функцию
[05:03.820 --> 05:12.020]  распределения, случайно кси в точке х при условии, что это в точности равна у. Тоже будет
[05:12.020 --> 05:23.140]  корректная запись. Ну, конечно, посещает мысль рассмотреть ситуацию, когда это, то есть условие,
[05:23.140 --> 05:31.780]  равно какому-то числу у как некий предельный переход. Но здесь единственно надо его корректно
[05:31.780 --> 05:38.900]  описать, этот переход, потому что как бы ни получилось, что он будет зависеть от типа, от как бы
[05:38.900 --> 05:46.140]  траектории стремления. Там, где это возможно сделать, мы воспользуемся чуть попозже, а пока вот просто как
[05:46.140 --> 05:54.060]  понятие условная функция распределения, условная функция распределения. Вот, ну теперь давайте вернемся
[05:54.060 --> 06:01.460]  к случайным величинам. Наконец, какие-то из них просто, что называется, посмотрим на них. Итак,
[06:01.460 --> 06:07.820]  мы с вами ввели два класса случайных величин, непрерывные и дискретные. Ну, давайте начнем с
[06:07.820 --> 06:17.780]  дискретной случайной величины, которая называется уассоновская случайная величина, зависит от одного
[06:17.780 --> 06:26.900]  параметра лямбда. Эта случайная величина задается вероятностью конкретных значений своих и выглядит
[06:26.900 --> 06:41.460]  так. Вероятность того, что, вот так вот сокращённо по, от лямбда будет равно к, есть лямбда в степени к
[06:41.460 --> 06:48.740]  делит на к факториал на е в степени минус лямбда. Почему я начал с этой случайной величины? Потому,
[06:48.740 --> 06:57.580]  что она вам знакома, правда? Это количество событий за единицу времени в поассоновском потоке,
[06:57.580 --> 07:06.140]  в том специальной, в той специальной вероятностной мере, которую мы с вами вводили. Вот,
[07:06.140 --> 07:16.100]  поассоновская случайная величина. Ну, строго говоря, я как бы постулировал, что случайная
[07:16.100 --> 07:22.700]  величина определяется своей функцией распределения. А, ну, ещё-то здесь надо указать, чему к равно. К
[07:22.700 --> 07:34.340]  равно 0, 1, 2 и так далее. Вот, ну, конечно, если имеете дело вот с такими вероятностями, то функцию
[07:34.340 --> 07:46.220]  распределения в точке x нетрудно записать. Это сумма вероятностей того, что поассоновская
[07:46.220 --> 08:04.220]  случайная величина равна k, когда k такое, что k строго меньше x. Правильно, да? Вот так
[08:04.220 --> 08:15.300]  перейти от вероятностей каждого конкретного значения к функции распределения. Следующая
[08:15.300 --> 08:23.060]  случайная величина, с которой, ну, может быть, надо начинать, но мы начали с поассоновской,
[08:23.060 --> 08:28.420]  потому что мы с ней уже столкнулись. Это так называемая Бернулевская случайная величина,
[08:28.420 --> 08:45.300]  B от P. Это дискретная случайная величина, принимающая два значения. Она принимает значение 1 с вероятностью
[08:45.300 --> 08:55.500]  P малое и 0 с вероятностью q равна единица минус P. Но на самом деле мы с этой случайной величиной
[08:55.500 --> 09:09.940]  тоже сталкивались. Какая случайная величина является по сути дела Бернулевской? Монетка. Да,
[09:09.940 --> 09:17.460]  монетка. Но монетка это всё-таки частный случай, потому что P равно 1 и 2. У нас она была прям такая,
[09:17.460 --> 09:24.100]  мы что-то с ней пробовали делать, умножали, складывали. Что за случайная величина?
[09:24.100 --> 09:37.740]  Характеристическая функция множества. Напомню, это случайная величина, которая равна единице,
[09:37.740 --> 09:44.940]  когда Омега принадлежит некому множеству А и нулю, если не принадлежит множеству А. Понятное дело,
[09:44.940 --> 09:52.740]  что все характеристические функции множества принадлежат классу Бернулевских случайных величин.
[09:52.740 --> 10:08.940]  Ну для Бернульской случайной величины, наверное, будет не лишнее нарисовать функцию распределения.
[10:08.940 --> 10:20.580]  Мы это или делали, или почти делали, но тем не менее, как сказать, повторим. Это дискретная случайная
[10:20.580 --> 10:36.620]  величина. Она принимает два значения 0 и 1. И здесь 1, 0 и 1. И для дискретной случайной величины,
[10:36.620 --> 10:43.300]  которая имеет скачки в тех точках, значения которых она принимает, чему величина скачка равна?
[10:43.300 --> 10:51.380]  Вероятности принятия значения. Вероятность принятия значения 0 равна Q. Значит, здесь будет скачок Q.
[10:51.460 --> 11:05.380]  Она у нас непрерывно слева. И второй скачок P в точке 1.
[11:05.380 --> 11:30.180]  Сверху? Не пойму вопроса. А, вот здесь, да-да-да, совершенно верно.
[11:30.180 --> 11:42.460]  Так, значит, вот функции распределения Бернульской случайной величины. Ну, кстати,
[11:42.460 --> 11:50.580]  еще одно свойство функции распределения. Мы знаем, что плюс бесконечности она равна единице,
[11:50.580 --> 11:56.220]  а это значит, что сумма всех вероятностей тоже должна равняться единице. Но обращаю ваше внимание,
[11:56.700 --> 12:03.860]  что если вот эту величину просуммировать про всем k, от 0 до бесконечности получится единица. Здесь,
[12:03.860 --> 12:11.180]  собственно, два значения нужно просуммировать. P и Q тоже единица. И третья случайная величина,
[12:11.180 --> 12:18.660]  которую мы с вами введем, дискретная, мы сейчас пока, как видите, по дискретным случайным величинам,
[12:18.660 --> 12:25.980]  это так называемая биномиальная случайная величина. Биномиальная случайная величина,
[12:25.980 --> 12:39.700]  только, извините, у нее уже два параметра, n и p. И задается она тоже своими вероятностями.
[12:40.580 --> 12:52.140]  Вероятность того, что биномиальная случайная величина будет равна k, равно c из n по k,
[12:52.140 --> 13:05.300]  p в степени k, u в степени n минус k. k принимает значение 0, 1, так далее n. То есть n плюс одно
[13:05.300 --> 13:13.500]  значение. n плюс одно значение. Биномиальная случайная величина. Можете предложить какой-нибудь
[13:13.500 --> 13:22.260]  мысленный эксперимент, результатом которого будет биномиальная случайная величина? Ну что вы к
[13:22.260 --> 13:30.140]  монетке все. Монетка p равно 1 и 2, в частный случай. Ну пусть даже с монеткой. Ну хорошо,
[13:52.260 --> 14:05.660]  откровенно скажу, не уловил вашу мысль. Значит, ну что тут можно заметить прежде всего, что если
[14:05.660 --> 14:16.260]  по всем k вот эти вероятности просуммировать, то это получится p плюс q в степени n. Понятно,
[14:16.260 --> 14:22.420]  что это должна равняться единице, ну значит соответственно q равно единица минус p, чего как бы
[14:22.420 --> 14:33.900]  и требовалось. Значит, вот эта случайная величина, биномиальная случайная величина, называется схемой
[14:33.900 --> 14:43.660]  Бернули независимых испытаний, n независимых испытаний. Значит, какой мысленный эксперимент,
[14:43.660 --> 14:51.460]  так сказать, приводит к возникновению этой случайной величины? Мы проводим независимые
[14:51.460 --> 15:00.660]  эксперименты с неким, определим некое событие как успех. Там в конкретной задаче успех, это может
[15:00.660 --> 15:07.100]  быть совсем не успех с точки зрения здравого смысла, но это название. Есть некий эксперимент,
[15:07.100 --> 15:12.500]  который с вероятностью p приводит к успеху и с вероятностью q приводит к неудаче. Ну,
[15:12.500 --> 15:20.980]  например, бросаем кубик, успехом называем выпадение шестёрки, тогда p равно 1 шестой и q равно
[15:20.980 --> 15:31.420]  5 шестых. Биномиальная случайная величина, это количество успешных опытов в серии из n,
[15:31.420 --> 15:41.980]  из вот этого n, который параметр испытаний. Понятно? То есть мы проводим некий эксперимент,
[15:41.980 --> 15:48.580]  можно даже бросать монеты, просто если мы бросаем монету, p равно q равно 1 и 2. Значит,
[15:48.580 --> 15:58.580]  мы проводим некий эксперимент n раз, успехом мы считаем выпадение решки. И вот биномиальная
[15:58.580 --> 16:06.220]  случайная величина, это такая, сколько решек у нас выпадет в серии из n испытаний. Понятно,
[16:06.220 --> 16:15.500]  почему это вот так описывается? Откуда это берется? Ну, каждый успех с вероятностью p,
[16:15.500 --> 16:22.740]  каждый неудач с вероятностью q, поэтому в серии испытаний, где k успехов и n-k неудач,
[16:22.740 --> 16:31.180]  вероятность такой серии p в степени k на q в степени n-k и еще на разных местах этот может
[16:31.180 --> 16:42.180]  произойти успех. То есть мы должны еще выбрать места для успеха. Вот такая вероятность соответствует
[16:42.180 --> 16:49.540]  эксперименту проведения независимых испытаний с вероятностью успеха p в единичном испытании и
[16:49.540 --> 16:57.140]  проводим n экспериментов. И случайной величиной является количество успехов в серии из n испытаний.
[16:57.140 --> 17:05.260]  Вот это биномиальная случайная величина. Скажите, пожалуйста, как вам кажется,
[17:05.260 --> 17:25.300]  биномиальная с биномиальной как-нибудь связана? То есть, видимо, вы хотите сказать,
[17:25.300 --> 17:32.380]  что биномиальная с параметрами np это сумма биномиальных в количестве n штук. Правильно
[17:32.380 --> 17:42.940]  я понял ваше мнение. Да, выглядит вполне здраво и соответствует логике. То есть что такое
[17:42.940 --> 17:49.500]  Бернульские случайными величины? Мы проводим эксперимент с вероятностью успеха p, получаем
[17:49.500 --> 17:55.700]  единицу, если успех. Потом мы проводим второй эксперимент, опять получаем единицу или ноль.
[17:55.700 --> 18:01.300]  Если мы их проведем n штук и сложим эти случайные величины, то мы получим, естественно,
[18:01.300 --> 18:09.420]  количество удачных экспериментов в серии из n испытаний. И в этом смысле можно сказать,
[18:09.420 --> 18:23.140]  что биномиальная случайная величина с параметрами np равна сумме бернульских индекс g,
[18:23.140 --> 18:40.060]  g равно от единицы до n. Вот так. Но это разные эксперименты. То есть если считать, что это
[18:40.060 --> 18:47.180]  подбрасывание монеты, то каждое подбрасывание монеты это бернульское случайная величина. Но
[18:47.180 --> 18:56.100]  когда мы ее подбрасываем n раз, мы получаем n бернульские случайные величины. Но вопрос не лишен
[18:56.100 --> 19:07.500]  смысла так глобально, потому что вот это равенство можно понимать в разных смыслах. И мы пока
[19:07.500 --> 19:15.220]  остановимся на том, что будем считать это равенство по распределению. То есть распределение суммы
[19:15.380 --> 19:26.820]  бернульских случайных величин совпадает с биномиальным распределением. Вот по такой формуле.
[19:26.820 --> 19:36.140]  Строго говоря, это равенство можно понимать по-разному. Например, давайте возьмем две
[19:36.140 --> 19:42.100]  бернульские случайные величины, пусть бросание монеты, и первое, определимое в ратностном
[19:42.100 --> 19:48.380]  пространстве орел-решка. Первая случайная величина равна единице, когда выпал орел, и ноль,
[19:48.380 --> 19:55.940]  когда решка. А вторая случайная величина равна единице, когда выпала решка, и ноль, если выпал орел.
[19:55.940 --> 20:05.020]  Эти две величины и та и другая бернульские. Но будет ли их сумма биномиальна? Нет, потому что их
[20:05.020 --> 20:13.940]  сумма всегда равна единице. А биномиальная с двумя испытаниями, это значит, может значение равняться
[20:13.940 --> 20:23.660]  0, 1 или 2. Поэтому это равенство нужно понимать, как бы обходиться с ним осторожно, но для первого
[20:23.660 --> 20:30.060]  знакомства да. Вот в этом смысле оно вполне корректно. То есть функция распределения суммы
[20:30.060 --> 20:37.060]  бернульских случайных величин, которые явились результатом независимых экспериментов, действительно
[20:37.060 --> 20:48.620]  имеет то же распределение, что и биномиальная случайная величина. Следующая дискретная
[20:48.620 --> 20:56.180]  случайная величина, с которой мы сегодня познакомимся, это так называемая геометрическая.
[20:56.180 --> 21:07.740]  Гео p тоже зависит от одного параметра. Тоже задается вероятностью своего значения. Вероятность того,
[21:07.740 --> 21:20.220]  что гео от p равно k, равно q в степени k умножить на p. k принимает значение 0, 1, 2 и так далее.
[21:20.220 --> 21:31.660]  Но то, что формально это такие вероятности задают функцию распределения, понятно,
[21:31.660 --> 21:37.740]  потому что сумма q в степени k на p по всем k от 0 до бесконечности даст нам единицу.
[21:37.740 --> 21:48.460]  Скажите, пожалуйста, какому эксперименту соответствует вот эта случайная величина?
[21:48.460 --> 21:54.660]  Как мне в эксперименте в мысленном получить случайную величину вот с таким распределением?
[21:54.660 --> 22:06.780]  Какие-нибудь идеи есть? Ну хоть с монеткой. Монетка, кубик, как угодно.
[22:06.780 --> 22:19.780]  Но если несколько раз подряд выпало, это значит, если условно говоря успех,
[22:19.780 --> 22:33.540]  то это p в степени k. Если не удача, q в степени k. Ну да, это как бы формальная
[22:33.540 --> 22:38.940]  интерпретация вот этой формулы, но уже из этого можно построить, так сказать, мысленный
[22:38.940 --> 22:47.340]  эксперимент. Мы проводим эксперимент до первого успеха. И вот количество неудачных опытов,
[22:47.340 --> 22:53.940]  которые мы сделаем до первого успеха, это и есть случайная величина, имеющая геометрическое
[22:53.940 --> 23:02.500]  распределение. Понятно? Случайная величина, имеющая геометрическое распределение,
[23:02.500 --> 23:10.860]  это количество неудачных опытов до первого успеха. Просто для, так сказать, информации
[23:10.860 --> 23:21.860]  иногда вот эту случайную величину вводят чуть-чуть по-другому. q в степени k минус 1p и k равно
[23:21.860 --> 23:28.300]  тогда, естественно, 1, 2 и так далее. Тогда это не количество неудачных опытов до первого успеха,
[23:28.300 --> 23:38.020]  а количество экспериментов, включая первый успех. Это ничего не... просто можно и так.
[23:38.020 --> 23:48.580]  Можно ввести и так. Но смысл именно в этом. Такой базовый эксперимент — это проведение каких-то
[23:48.580 --> 23:54.900]  экспериментов до первого успеха. Количество вот этих неудачных шагов или количество плюс один
[23:54.900 --> 24:00.780]  этих неудачных шагов — это вот и есть случайная величина, имеющая геометрическое распределение.
[24:00.780 --> 24:20.460]  Следующая случайная величина — н-бене, но на самом деле это не н-бене, это отрицательно
[24:20.460 --> 24:35.260]  бинемиальное распределение с двумя параметрами n и p. Задается тоже своими вероятностями. Вероятность
[24:35.260 --> 24:49.220]  того, что отрицательно бинемиальная случайная величина будет равна k, значит это есть c из n
[24:49.220 --> 25:11.660]  плюс k минус 1 по n минус 1, q в степени k, p в степени n. Значит k равно 0, 1, 2 и так далее.
[25:11.660 --> 25:30.500]  Ну, я был бы рад, если бы вы мне сказали схему эксперимента, который приводит к такой случайной
[25:30.500 --> 25:51.260]  величине. Хорошая идея, хорошая идея. Ну, значит прежде всего отметим, что сумма вот этих вот c из n
[25:51.260 --> 26:02.500]  плюс k минус 1, n минус 1, q в степени k, p в степени n, сумма от нуля до бесконечности,
[26:02.500 --> 26:10.700]  нетрудно убедиться, что это p делить на единица минус q в степени n, ну то есть единица.
[26:10.700 --> 26:23.220]  Теперь интерпретация. Значит отрицательно бинемиальная случайная величина — это
[26:23.220 --> 26:34.380]  количество неудачных опытов до n успеха. Количество неудачных опытов до n успеха. Вот,
[26:34.380 --> 26:41.380]  так сказать, мысленный эксперимент, который приводит к этой случайной величине. Ну,
[26:41.380 --> 26:51.300]  как здесь как бы схема-то строится. Значит нас интересуют такие последствия успехов неудач,
[26:51.300 --> 27:03.620]  чтобы среди n плюс k минус 1 эксперимента был n минус 1 успех, а остальные неудачи. И потом
[27:03.660 --> 27:10.820]  мы к этой последовательности на n плюс k это место приписываем p, то есть n успех, и наш
[27:10.820 --> 27:20.180]  эксперимент прекращается мыслями. Ну вот, n плюс k минус 1, n минус 1 место для p, тогда q в
[27:20.180 --> 27:29.860]  степени k, p в степени n минус 1 — это, собственно, вероятность появления цепочек, у которых n минус
[27:29.860 --> 27:38.260]  1 успех и k неудач. И потом на последнем шаге делаем успешный эксперимент с вероятностью p,
[27:38.260 --> 27:44.460]  поэтому это p в степени n минус 1 превращается p в степени n, и получается вот такая формула.
[27:44.460 --> 27:54.700]  Понятно я объяснил? Ну и, собственно, аналогичное утверждение можно сделать. Отрицательно
[27:54.700 --> 28:02.900]  биномеральная случайная величина с параметрами n и p в смысле распределения есть сумма геометрических
[28:02.900 --> 28:15.820]  с параметром p в количестве n штук. Но это естественно. Если я беру там первую серию
[28:15.820 --> 28:22.180]  испытаний до первого успеха, количество неудач — это геометрическая случайная величина. Потом
[28:22.180 --> 28:28.460]  опять провожу эксперименты до второго успеха. Количество неудач от первого до второго успеха — это
[28:28.460 --> 28:35.220]  тоже геометрическая случайная величина. Ну вот складываем их и получаем в конце концов количество
[28:35.220 --> 28:48.660]  неудачных опытов до n успеха. Вот такие вот случайные величины. Так, сейчас соображу,
[28:48.740 --> 28:59.060]  не забыл ли какую-нибудь важную из дискретных? Вроде нет. Здесь я хотел немножко пофилософствовать
[28:59.060 --> 29:08.860]  и порассуждать, что такое случайная величина, потому что это важное понятие и не такое уж простое,
[29:08.860 --> 29:16.420]  как кажется. С одной стороны, мы со случайными величинами ведёмся как с числами. Складываем,
[29:16.460 --> 29:22.220]  умножаем их. Мы получили соответствующие результаты, что функция от случайной величины — случайная
[29:22.220 --> 29:29.860]  величина, то есть измеримое отображение и так далее. Но в то же время это не числа. А что же
[29:29.860 --> 29:36.300]  это такое? Что такое случайная величина? То есть мы с ними ведёмся как с числами, но на самом деле,
[29:36.300 --> 29:44.940]  но очевидно, что это не числа. Это нечто другое. Как-то надо по-другому это описать. Вот то, что это
[29:44.940 --> 29:52.300]  такая нетривиальная штука. Можно в качестве примера привести этот известный, как это назвать,
[29:52.300 --> 30:01.340]  парадокс или байку про кота Шреггенгера. Знаете, да, вы в форситетической физике? Знаете про такой
[30:01.340 --> 30:09.460]  эксперимент? Ну, на самом деле, никакого там парадокса нет. Это всё раздули журналисты. Значит,
[30:09.460 --> 30:17.540]  так описали, да, суть эксперимента и придумали там какие-то страшных котов, которые наполовину мёртв,
[30:17.540 --> 30:24.620]  наполовину жив. На самом деле это как бы... Все знают эту схему эксперимента мысленного,
[30:24.620 --> 30:32.660]  да, не надо рассказывать. Вот, значит, на самом деле вот как раз вот этот псевдопарадокс возник от того,
[30:32.660 --> 30:38.700]  что люди, которые это описывали, журналисты, не понимали, что такое случайная величина.
[30:38.700 --> 30:47.900]  Значит, пока случайная величина... Вот пока мы не измерили случайную величину или не получили её
[30:47.900 --> 30:55.420]  реализации, её нет ни в каком виде. То есть ни мёртвого кота нет, ни живого нет, его просто нет.
[30:55.420 --> 31:02.140]  У нас есть случайная величина, такой абстрактный объект, а как только мы, опять же, так сказать,
[31:02.140 --> 31:09.420]  наш мысленный эксперимент, заглянем в этот ящик, мы поймём, проведём наблюдение, наша случайная
[31:09.420 --> 31:16.740]  величина получит какую-то реализацию. А в этом, в эксперименте с котов Шреднегера, их всего две.
[31:16.740 --> 31:26.220]  Кот жив, кот мёртв. Это во-первых. Ну и во-вторых, что так, ну, отмечу, что никакого отношения к квантовым
[31:26.220 --> 31:36.140]  эффектам этот эксперимент не имел, потому что вместо, значит, распада частиц, которые запускали
[31:36.140 --> 31:41.700]  механизм умерщеления этого кота, можно было подбросить монету в микромирии, так сказать,
[31:41.700 --> 31:52.380]  в квантовые эффекты. Решка, грубо говоря, кота умертвили, прошу прощения, за такие
[31:52.380 --> 32:00.100]  живодёргские посылы. Если нет, коту повезло, он остался жив. Но ещё раз скажу, что мы с вами
[32:00.100 --> 32:05.940]  рассуждаем в перименах мысленных экспериментов, поэтому, конечно, никакие животные у нас, значит,
[32:05.940 --> 32:14.540]  не пострадали. Но, тем не менее, от непонимания, что такое случайная величина, возникает большое
[32:14.540 --> 32:22.900]  количество недоразумений в быту. И основная здесь ошибка, ещё раз повторюсь, это сродник квантовым
[32:22.900 --> 32:33.020]  эффектам, где находится электрон, описаный псифункцией, где угодно. И как только мы там,
[32:33.020 --> 32:38.980]  ну, тем или иным способом его зафиксируем, вот тогда мы сможем сказать, где находится электрон. Правда,
[32:38.980 --> 32:44.220]  с точностью до принципа неопределённости, тем не менее. Вот, значит, ну, тем не менее, так сказать,
[32:44.220 --> 32:51.900]  более-менее можем сказать, где находится электрон. Пока мы этого не сделали, он везде и нигде. Это там,
[32:51.900 --> 32:59.140]  ну, в терминах физики, волна. Вот, то есть, вот такая вот случайность, ну, как бы некое проявление
[32:59.140 --> 33:07.820]  случайности, это случайная величина. Случайная величина ничему не равна. Возвращаясь к эту
[33:07.820 --> 33:15.100]  Шрёдингера, он не... это не те термины, в которых можно рассуждать, жив он или мёртв. Случайная
[33:15.100 --> 33:22.980]  величина может проявляться в своих реализациях, но она ничему не равна конкретно, потому что
[33:22.980 --> 33:32.260]  случайная величина, ну, как бы одна из интерпретаций, это механизм генерации реализации,
[33:32.260 --> 33:37.700]  механизм. Вот какой механизм такая случайная величина? Грубо говоря, случайная величина
[33:37.700 --> 33:45.460]  определяется своим механизмом, но сама по себе она не является ни числом, ни там приводит систему
[33:45.460 --> 33:52.500]  в какое-то состояние и так далее. Вот, я ещё раз скажу, что это очень непростая вещь, потому что,
[33:52.500 --> 34:00.260]  например, даже Эйнштейн так, вообще говоря, до конца и не принял вот эту как бы идею,
[34:00.260 --> 34:09.300]  такое некого вероятностного устройства мира. Ну, вы знаете его знаменитое выражение. Старик
[34:09.300 --> 34:18.540]  не играет в кости, да, то есть там господь-бог, ну, как бы, там это всё задаёт детерминированно.
[34:18.540 --> 34:24.700]  Если мы что-то не можем точно измерить, то это не следствие устройства мира, а просто там
[34:24.700 --> 34:32.300]  недостаток нашего как бы эксперимента. И, собственно, человек, который там, ну, разумеется там,
[34:32.300 --> 34:40.620]  как это, безусловный гений, ну, вот с этим, с этими вещами так и не смог до конца смириться. Вот,
[34:40.620 --> 34:49.140]  поэтому это такое очень-очень непростое понятие случайная величина. Вот, если так уж думаться. Ну,
[34:49.140 --> 34:58.260]  ещё одну, ещё приведу один пример тоже как бы с понятием случайной величины или со случайной
[34:58.260 --> 35:06.300]  величиной. Значит, как вы знаете, если электроны пропускать через маленькую дырочку, то возникает
[35:06.300 --> 35:16.140]  интерфюкционная картина, круги такие возникают. Ну, физики это говорят или там объясняют, или уже
[35:16.140 --> 35:23.140]  может и не так объясняют. Ну, как бы такое для школьников старших классов в нынешнее время,
[35:23.140 --> 35:27.980]  то есть когда-то это были нобелевские премии, а теперь это вот школьники старших классов, им как
[35:27.980 --> 35:34.260]  бы можно объяснить, что электрон, когда он проходит через это отверстие, он проявляет
[35:34.260 --> 35:42.220]  свойство волны. Вот как волны дают интерфюкционную картину, так и электрон дает, потому что он вот в
[35:42.220 --> 35:47.900]  этом случае проявляет себя как волна. Ну и как бы так можно рассуждать, не то что можно, это там,
[35:47.900 --> 35:57.220]  ну один из принципов основных так сказать, это дуализм, да, волна, частица. Ну, то есть это такая
[35:57.220 --> 36:05.220]  ну фундаментальная концепция. Но давайте мы на это взглянем с точки зрения нашего предмета,
[36:05.220 --> 36:15.220]  наших понятий, случайных величин. Вот, значит, где-то там, не знаю, в середине прошлого века
[36:15.220 --> 36:25.140]  группой советских физиков, там лабораторий, там этот фабрикант, такой физик был у нас, я в школе
[36:25.140 --> 36:31.020]  учебники физики, так сказать, его авторстве, так сказать, там, ну там использовал. Вот, значит,
[36:31.020 --> 36:40.100]  был поставлен такой эксперимент. Значит, эксперимент как бы понятный, опять же это дырочка там
[36:40.100 --> 36:48.540]  в свинцовой пластине пучок электронов и получается интерфюкционная картина. Но, значит, на этот раз
[36:48.540 --> 36:55.700]  чуть-чуть изменили схему эксперимента, не пучок электронов, а по одному, по одному электрону пускали.
[36:55.700 --> 37:02.220]  Причем время между, ну там от первого до второго и так далее, там в десятки раз,
[37:02.220 --> 37:11.620]  превышало время, пока этот электрон фиксировался на фотопластинке. То есть каждый отдельный
[37:11.620 --> 37:19.420]  электрон оставлял на этой фотопластинке точку, никакой интерфюкционной картины, просто точка,
[37:19.420 --> 37:26.180]  как из краника вода капает, одна точка, вторая точка, третья точка. Ну и когда так с этих достаточно
[37:26.180 --> 37:37.580]  много там пустили, получилась та же самая круговыми кольцами картина. С точки зрения нашей науки,
[37:37.580 --> 37:43.260]  нашего подхода, нашего предмета, который мы изучаем, что мы можем сказать? Мы можем сказать,
[37:43.260 --> 37:55.780]  что функция плотности, ну вот так вот, если вот это взять там у нас там, получается какая-то вот такая
[37:55.780 --> 38:05.620]  картинка светлая-темная. Ну если взять радиус и вдоль радиуса посмотреть, то можно сказать,
[38:05.620 --> 38:12.700]  что мы имеем дело со случайной влечиной, ну где-то вот с такой функцией плотности примерно.
[38:12.700 --> 38:24.660]  То есть это немыслимый эксперимент, это реальный эксперимент. Каждый электрон реализовал какое-то
[38:24.660 --> 38:31.860]  значение. Один попадал сюда, другой сюда, и вот в эту окрестность их попадало гораздо больше,
[38:31.860 --> 38:41.460]  чем вот в эту окрестность. Мы не будем задаться вопросом почему, а просто скажем, что до того,
[38:41.460 --> 38:48.700]  пока мы эти электроны не выпустили, они не обладали никакими свойствами и никакой не давали
[38:48.700 --> 38:54.140]  картина. Но как только мы этот эксперимент провели, мы получили большое количество, как говорят,
[38:54.140 --> 39:03.540]  реализации. То есть случайная влечина из функции, из механизма генерации, превратилась в число,
[39:03.540 --> 39:10.740]  то есть реализация наступила. И вот эта вот реализация, вот она ведет себя вот таким образом.
[39:10.740 --> 39:21.580]  А почему говорят об интерференции? Потому что, конечно, это очень похоже по картинке на интерференцию
[39:21.580 --> 39:34.540]  волн световых. Заканчиваю философскую часть и как бы сделаю там вывод или подчеркну. Понятие
[39:34.540 --> 39:45.460]  случайной величины совсем непростое. Это сложный объект, который в общем с трудом даже там давался
[39:45.460 --> 39:55.060]  очень умным людям. Ну, со временем, время проходит как бы, то, что было когда-то там на передовом
[39:55.060 --> 40:04.740]  краю становится бытовыми вещами, но я вам призываю вас не упрощать это понятие, понятие случайной
[40:04.740 --> 40:11.500]  величины. Это не такой уж простой объект, как может быть, так сказать, может показаться вот на этих
[40:11.500 --> 40:21.900]  примерах. Ну, теперь давайте рассмотрим примеры непрерывных случайных величин. Ну, и такая, в общем-то,
[40:21.900 --> 40:31.620]  нам в целом знакомая, точнее говоря, сейчас вы ее узнаете, надеюсь. Это равномерная на АВ случайная
[40:31.620 --> 41:00.460]  величина. Ее давайте для разнообразия зададим функцией распределения. АВ единица. Вот так выглядит
[41:00.460 --> 41:07.260]  эта функция распределения этой случайной величины. И что мы отсюда видим, и какой вывод сразу
[41:07.260 --> 41:13.980]  можем сделать. Вероятность того, что равномерно распределенная на АВ случайная величина попадет
[41:13.980 --> 41:20.980]  в какой-то интервальщик, ну не знаю там, фиксированной длины, это вероятность одна и та же,
[41:20.980 --> 41:28.940]  если этот интервальщик в АВ укладывается и не зависит от его местоположения. Это понятное
[41:28.940 --> 41:35.940]  утверждение. Вероятность того, что случайная величина вот здесь попадет в небольшой
[41:35.940 --> 41:43.500]  интервальщик, это вот она. И в любом другом месте, если такой же ширины интервальщик,
[41:43.500 --> 41:51.820]  то будет та же самая вероятность. Что это нам напоминает? Какой мысленный эксперимент?
[41:51.820 --> 42:04.220]  Мы с вами пользовались ну так, ну несколько раз, не один, может не многократно. Бросание точки
[42:04.220 --> 42:12.220]  на удачу. То есть у нас был с вами такой как бы абстрактный эксперимент бросание точки на удачу.
[42:12.220 --> 42:18.860]  То есть точка может попасть в любое место, никаких предпочтений нет, вероятность попадания в
[42:18.860 --> 42:24.380]  какой-то интервал пропорциональна его длине. Ну собственно формализация вот этого мысленного
[42:24.380 --> 42:36.220]  эксперимента, это случайная величина равномерно распределенная на АВ. Ну и собственно нетрудно
[42:36.220 --> 42:43.300]  нарисовать плотность, поскольку это дискретная случайная величина, то непрерывная случайная
[42:43.300 --> 43:06.620]  величина, то у нее плотность существует. Плотность выглядит вот так. Причем обращаю ваше внимание,
[43:07.220 --> 43:14.100]  что чему эта функция равна в точках Х или В несущественно. Как бы мы ее не определили,
[43:14.100 --> 43:22.820]  это будет множество нулевой меры, поэтому можно рассматривать случайную величину равномерно
[43:22.820 --> 43:30.620]  на отрезке АВ, можно на интервале АВ, на полуинтервале АВ с точностью до множества нулевой меры. Это все одно
[43:30.620 --> 43:40.940]  и то же. Если есть какая-то задача специфика, можно до определить, чему там в точках Х или В
[43:40.940 --> 43:50.220]  равно, то ли нулю, то ли единица делить на В-А. Но с точки зрения функции плотности и там вероятности
[43:50.220 --> 43:55.900]  событий это никакого значения не имеет. И это вообще характерное свойство непрерывных случайных
[43:55.900 --> 44:08.100]  величин. Изменение на множестве меры ноль не меняет вероятности. Добавление множества меры ноль,
[44:08.100 --> 44:21.300]  вычитание множества меры ноль и так далее. Вот первая случайная величина имеющая непрерывное
[44:21.300 --> 44:28.060]  распределение равномерное. Вторая случайная величина, вам тоже в общем-то знакома, она называется
[44:28.060 --> 44:36.540]  нормальной случайной величиной и зависит от двух параметров, m и σ квадрат. И задается своей,
[44:36.540 --> 44:47.940]  традиционно задается своей функцией плотности. fnm σ квадрат от х это единица делить на σ корень
[44:47.940 --> 45:04.620]  из 2p е в степени минус х минус m в квадрате делить на 2 σ квадрат. Знакомая функция? Ну,
[45:04.620 --> 45:11.580]  я думаю, что совсем вы ее узнаете, если мы рассмотрим так называемое стандартное нормальное
[45:11.580 --> 45:23.380]  распределение с параметрами 0,1, тогда плотность примет вид единицы делить на корень из 2p е в степени
[45:23.380 --> 45:31.380]  минус х квадрат пополам. Так называемый интеграл ошибок, да, гауссова кривая. Да, действительно,
[45:31.380 --> 45:39.460]  гауссова кривая это частный случай нормального распределения с параметрами 0,1. Ну, наверное,
[45:39.460 --> 45:46.260]  возникает вопрос, какой содержательный смысл вот этих параметров m и σ квадрат.
[45:46.260 --> 45:56.780]  Чтобы на него ответить, давайте рассмотрим случайную величину это, которая равна некому
[45:56.780 --> 46:06.420]  a умножить на n 0,1 и плюс b. Ну, единственное необременительное требование a больше 0,
[46:06.420 --> 46:14.980]  а больше 0, b произвольное. Ну и давайте попробуем функцию распределения, функцию плотности найти,
[46:14.980 --> 46:23.060]  вот такой случайный, новый случайный величины. Значит, вероятность того, что это меньше х,
[46:23.060 --> 46:36.420]  это есть вероятность того, что, ну, я сразу действие произведу, n 0,1 меньше, чем х минус b делить на a.
[46:36.420 --> 46:54.340]  Правильно, да? Простейшие действия произвел. Вот это по определению, напоминаю, функция распределения
[46:54.340 --> 47:00.740]  случайной величины это в точке х. И чтобы получить плотность этой случайной величины, нужно
[47:00.740 --> 47:07.180]  продиференцировать функцию распределения. Если мы продиференцируем, мы получим f малая,
[47:07.180 --> 47:14.180]  то есть плотность случайной величины это в точке х будет равна. А вот это, как мы видим,
[47:14.180 --> 47:20.540]  функция распределения стандартной нормальной случайной величины только взятой в точке х
[47:20.540 --> 47:34.140]  минус b делить на a. Поэтому это будет, во-первых, f n 0,1 в точке х минус b делить на a,
[47:34.140 --> 47:40.420]  но еще надо продиференцировать вот это выражение, это единица на а.
[47:40.420 --> 47:52.380]  Ну и давайте подставим вот сюда значение этой функции. Возьмем в точке х минус b делить
[47:52.380 --> 47:59.620]  на а, ну и не забудем про единицу на а. Получим единица делить на а корень из 2p,
[47:59.620 --> 48:16.740]  е в степени минус х минус b в квадрате делить на 2a квадрат. Правильно, да? То есть функция распределения,
[48:16.740 --> 48:24.340]  вот такой вот линейной функции от стандартного нормального распределения, на самом деле это
[48:24.340 --> 48:29.580]  тоже нормальное распределение, то есть вот при линейном преобразовании нормальность,
[48:29.580 --> 48:37.180]  так сказать, сохраняется, и нам становится понятен смысл параметров m и sigma. m,
[48:37.180 --> 48:44.820]  который у нас здесь буквой b обозначен, это, ну грубо говоря, параметр сдвига,
[48:44.820 --> 48:55.100]  а корень sigma квадрат, тубиш sigma, это параметр масштаба a. Вот содержательный смысл этих
[48:55.100 --> 49:02.700]  параметров. Как только мы a умножили на стандарт нормальную учину, у нас это a попало вот сюда и
[49:02.700 --> 49:11.460]  сюда, то есть в точности как вот здесь sigma. То есть, значит, во-первых, при линейном преобразовании
[49:12.140 --> 49:20.020]  нормальная величина остается нормальной и случайной величиной, и во-вторых, вот пока мы как бы потом
[49:20.020 --> 49:27.940]  более точный смысл придадим, но тем не менее, пока так содержательно, параметр m это параметр сдвига,
[49:27.940 --> 49:37.340]  а sigma это параметр масштаба относительно гауссовой этой вот случайной величины стандартной нормальной.
[49:37.340 --> 49:58.220]  Да.
[50:07.340 --> 50:35.220]  Даже не скажу вам. Я немецкий не знаю, но может быть это какой-нибудь рэндом,
[50:35.220 --> 50:46.340]  ну могу только гипотезу высказывать, не знаю. Вот, значит, равномерная и нормальная случайная
[50:46.340 --> 50:53.460]  величина. Ну и давайте еще введем тоже такую важную непрерывную случайную величину, даже
[50:53.460 --> 51:01.420]  можно сказать целый класс. Это так называемая гамма-распределение. Гамма-распределение зависит
[51:01.420 --> 51:16.220]  от двух параметров альфа и лямда и задается своей плотностью. f гамма от x равно лямда в степени
[51:16.220 --> 51:28.100]  альфа делить на гамма функции от альфа x в степени альфа минус 1, e в степени минус лямда x. Значит,
[51:28.100 --> 51:38.460]  x альфа и лямда все больше нуля. Но x, правда, можно больше или равно, несущественно, но вот так вот.
[51:38.460 --> 51:49.760]  Ну, видимо, с такой функцией вы не сталкивались, да? Нигде, вроде, не могли столкнуться. Но что
[51:49.760 --> 52:04.280]  такое гамма-функция, знаете, да? Вот, значит, вот таким образом получается у нас распределение. Если вы
[52:04.280 --> 52:11.680]  проинтегрируете это от нуля до бесконечности, ну гамма альфа вынесите, ну, собственно, у вас
[52:11.680 --> 52:20.720]  как раз и получится гамма-функция от альфа и интеграл от плотности равен единице, как это должно быть.
[52:20.720 --> 52:32.240]  Значит, у вот этого распределения, семейства распределений есть там пару важных частных
[52:32.240 --> 52:45.560]  случаев. Значит, первый частный случай это когда альфа равно единице, альфа равно единице. Тогда
[52:45.560 --> 53:02.560]  функция плотности принимает вид лямбда на е в степени минус лямбда х. Ничего не напоминает вам?
[53:02.560 --> 53:22.440]  Какое? Экспоненциальное. А вы этот термин откуда взяли? А, в семинарских занятиях было. Понятно.
[53:22.440 --> 53:31.160]  Значит, ну давайте вот что сделаем, чтобы как узнать эту случайную величину. Давайте
[53:31.160 --> 53:37.640]  напишем ее функцию распределения. Чтобы написать ее функцию распределения, мы должны ее проинтегрировать.
[53:37.640 --> 53:44.440]  Ну, если мы это с учетом граничных условий сделаем, то мы получим, что функция распределения равна
[53:44.440 --> 54:05.920]  единице минус е в степени лямбда х. Правильно, да? Напоминаю, что функция распределения это равно
[54:05.920 --> 54:13.080]  вероятность того, что наша случайная величина, так ее обозначу Т-большое случайную величину,
[54:13.080 --> 54:19.560]  меньше х. И вот эта вероятность равна вот такой величине. А тогда, соответственно,
[54:19.560 --> 54:25.280]  вероятность того, что Т больше х, ну равно здесь не существенно, вот ввиду того,
[54:25.280 --> 54:33.720]  что плотность существует. Естественно, есть е в степени минус лямбда х. Опять не узнаете?
[54:33.720 --> 54:51.720]  Ну нет, Пуассон... или совсем не Пуассон даже сказал. Но тем не менее, когда мы с вами получали
[54:51.720 --> 54:58.320]  вероятностную меру для Пуассоновского, для простейшего потока, мы с вами там получили
[54:58.320 --> 55:05.680]  вероятность того, что за время Т не произошло ни одного события. Или в терминах случайных
[55:05.680 --> 55:14.000]  величин, если Т это случайная величина соответствующая времени до события, то
[55:14.000 --> 55:22.480]  событие Т больше х как раз означает, что за время от 0 до х не произошло ни одного события. И эта
[55:22.480 --> 55:30.520]  вероятность как раз была равна, если вы помните, е в степени минус лямбда х. Помните, да? То есть это,
[55:30.520 --> 55:37.080]  в принципе, тоже знакомая нам случайная величина, просто мы ее сразу не узнали, потому что надо было
[55:37.080 --> 55:45.280]  ее так привести к какому-то виду. Но тем не менее, этот частный случай называется или задает
[55:45.280 --> 55:52.080]  действительно показательное или экспоненциальное распределение. То есть случайная величина вот
[55:52.080 --> 55:59.120]  с такой функцией распределения и вот с такой плотностью называется показательной или экспоненциальной.
[55:59.120 --> 56:11.560]  Ну, обозначают, например, вот так, эксп от лямбда. Экспоненциальная случайная величина с параметром
[56:11.560 --> 56:21.680]  лямбда. Видим, что она зависит от одного параметра. Это значит первый частный случай и второй важный
[56:21.680 --> 56:38.960]  частный случай. Это когда α равно n пополам, а лямбда равно 1 вторая. Случайная величина с
[56:38.960 --> 56:50.120]  такими параметрами, плотность которой есть 1 вторая в степени n пополам, делить на гамма от n пополам
[56:50.120 --> 57:03.680]  на х, х в степени n пополам минус 1, e в степени минус х пополам. Вот с такой довольно сложной функцией
[57:03.680 --> 57:14.360]  плотности. Но это всего лишь на всего частный случай гамма распределения. Называется х квадрат
[57:14.360 --> 57:25.880]  случайной величиной с n степенями свободы или n еще просто индексом пишут иногда. То есть случайная
[57:25.880 --> 57:33.280]  величина х квадрат с n степенями свободы, это гамма распределенная случайная величина с параметрами
[57:33.280 --> 57:48.760]  α равно n пополам и лямбда равно 1 вторая. Эта случайная величина очень важное значение имеет
[57:48.760 --> 58:01.440]  статистике, поэтому это случай хоть и частный, но очень важный. Итак, помимо самого семейства гамма
[58:01.440 --> 58:10.920]  распределенных случайных величин мы выделили два частных случая. Показательное распределение и х квадрат
[58:10.920 --> 58:15.080]  с n степенями свободы распределения.
[58:15.080 --> 58:41.800]  Ну, анонсирую.
[58:45.080 --> 58:56.200]  Не будем времени терять. Стандартная нормальная случайная величина в квадрате, оказывается, равна
[58:56.200 --> 59:09.400]  по распределению х квадрат с одной степенью свободы. Ну, мы на семинаре это покажем. Ну, это одна из
[59:09.400 --> 59:23.560]  связей, так сказать. Ну, и еще одна случайная величина непрерывная, это вот такая случайная
[59:23.560 --> 59:39.200]  величина E в степени nm sigma квадрат. То есть, экспонента в степени нормальная случайная величина.
[59:39.200 --> 59:50.440]  Эта случайная величина называется лог нормальный. Ну, естественно, зависит от двух параметров m и sigma
[59:50.440 --> 01:00:03.160]  квадрат. Ну, и в данном случае мы ее задали, как вы видите, не функцией распределения или функции
[01:00:03.160 --> 01:00:10.680]  плотности, а функциональным видом. То есть, мы знаем, что E в степени случайная величина, это тоже
[01:00:10.680 --> 01:00:16.800]  случайная величина. Ну, теперь, значит, нам надо функцию плотности или функцию распределения
[01:00:16.800 --> 01:00:30.680]  получить. Давайте это сделаем. Значит, f лог n в точке х это вероятность того, что E в степени nm
[01:00:30.680 --> 01:00:41.440]  sigma квадрат будет меньше х. Значит, х больше нуля по понятным причинам, поэтому эта вероятность равна
[01:00:41.440 --> 01:00:57.480]  вероятности nm sigma квадрат меньше логарифм х. Ну, а это не что иное, как функция распределения nm sigma
[01:00:57.480 --> 01:01:12.200]  квадрат, взятая в точке логарифм х. Ну, и, соответственно, плотность f лог n в точке х,
[01:01:12.200 --> 01:01:30.360]  это будет единица на х sigma, корень из двух π, E в степени минус логарифм х минус m квадрате
[01:01:30.360 --> 01:01:43.160]  делить на 2 sigma квадрата. Вот функция плотности лог нормальной случайной величины. Ну,
[01:01:43.160 --> 01:01:48.200]  помимо того, что вам просто нужно знать эту случайную величину, она там некоторыми свойствами
[01:01:48.200 --> 01:01:55.720]  обладает полезными. Ну, например, произведение лог нормальной случайных величин лог нормальная
[01:01:55.720 --> 01:02:05.960]  случайная величина, но еще здесь обратите внимание на как бы методику способ, каким я ее ввел. Я ее
[01:02:05.960 --> 01:02:15.240]  описал как функцию от известной случайной величины и получил вот такой нехитрой техникой, получил ее
[01:02:15.240 --> 01:02:23.400]  функцию распределения через нормальную и явно выписал функцию плотности этой случайной величины.
[01:02:23.400 --> 01:02:34.560]  Вот так. Значит, это не все случайные величины, с которыми мы с вами познакомимся, но пока давайте
[01:02:34.560 --> 01:02:39.640]  на этом остановимся и двинемся чуть-чуть дальше содержательно.
[01:02:53.400 --> 01:03:22.640]  Значит, напомню, что мы с вами
[01:03:22.640 --> 01:03:35.520]  ввели такой объект, случайная величина, да? Xi это некая f от ω, которая элементу из ω ставит
[01:03:35.520 --> 01:03:43.040]  соответствие числу. Это мы назвали случайной величиной. Ну и давайте по аналогии введем
[01:03:43.040 --> 01:04:03.840]  случайный вектор. Xi вектор от ω равный Xi1 от ω, так далее Xin от ω. И это тоже отображение из ω в Rn.
[01:04:03.840 --> 01:04:16.000]  У нас еще будет один объект, это из омега в последовательности, ну пока значит мы остановимся
[01:04:16.000 --> 01:04:23.480]  на векторе. Итак, первый это случайная величина, второй это случайный вектор. На что обращаем
[01:04:23.480 --> 01:04:29.400]  внимание в определении случайного вектора? Вот это омега одна и та же для всех, так сказать,
[01:04:29.400 --> 01:04:38.120]  компонент. То есть каждому омега, малому, каждому исходу вот отсюда ставится в соответствие весь
[01:04:38.120 --> 01:04:46.960]  вектор сразу, а не так, что одному омега 1 ставится в соответствие Xi1 от омега 1 и так далее. Нет,
[01:04:46.960 --> 01:04:54.400]  определение именно такое. Каждому элементарному исходу ставится в соответствие вектор. Вот такой
[01:04:54.400 --> 01:05:10.480]  получился у нас объект. Ну и по аналогии мы введем функцию распределения случайного вектора.
[01:05:10.480 --> 01:05:25.120]  Это есть вероятность того, что одновременно омега такое, что ксикатая от омега меньше
[01:05:25.120 --> 01:05:51.000]  хкт, к от единицы до n. Ну такая очевидная аналогия. Значит, на какой вопрос надо ответить?
[01:05:51.000 --> 01:05:58.200]  Эта вероятность определена ли? Ну здесь ответ довольно очевиден. Поскольку каждый ксикатый это
[01:05:58.200 --> 01:06:04.760]  измеримое отображение, то вот это множество принадлежит сигма-алгебре. Ну и естественно,
[01:06:04.760 --> 01:06:09.920]  что их конечное пересечение тоже принадлежит сигма-алгебре. То есть вот это вот измеримый
[01:06:09.920 --> 01:06:15.080]  элемент. То есть вероятность его есть. То есть функция распределения определена корректно.
[01:06:15.080 --> 01:06:24.600]  Ну и давайте свойства этой функции. Первое свойство, ну пройдемся по свойствам,
[01:06:24.600 --> 01:06:29.600]  которые были у функции распределения случайно влечены, и просто покажем, что есть аналогичные.
[01:06:29.600 --> 01:06:39.840]  Значит, первая функция распределения, ну я так для краткости иногда буду и тут вектор тоже
[01:06:39.840 --> 01:06:48.200]  писать x-аргумент. Меньше или равна единице, больше или равна нуля. Ну как и полагается любой
[01:06:48.200 --> 01:06:58.120]  там вероятности. Следующее свойство или там у нас промежуточное свойство, из которого потом
[01:06:58.120 --> 01:07:10.720]  выведем остальные. Значит, функция распределения x1, вот этого вектора нашего, xk-1,
[01:07:10.720 --> 01:07:40.440]  b, xk, плюс 1 и так далее, xn равна f, x, x1, xk-1, a, xk, плюс 1, xn, плюс вероятность
[01:07:40.440 --> 01:08:04.520]  того, что xg меньше xg при g не равном k, и одновременно с этим xk принадлежит полуинтервалу a, b.
[01:08:10.440 --> 01:08:20.520]  Вот это понятно? Ну аналогичную формулу мы с вами получали для функции распределения
[01:08:20.520 --> 01:08:32.280]  случайной величины. Понятно, нет? То есть значение функции распределения это значит,
[01:08:32.280 --> 01:08:38.640]  что xk меньше b, мы разбиваем это на два непересекающихся интервала, xk меньше a,
[01:08:38.640 --> 01:08:49.840]  xk принадлежит от a до b вот такому полуинтервалу. Берем вероятности, эти вероятности это функция
[01:08:49.840 --> 01:08:56.120]  распределения. Значит, понятное свойство. Ну и что отсюда следует? Отсюда следует,
[01:08:56.120 --> 01:09:04.960]  так, свойство аналогичное функции распределения, это свойство монотонности нестрогой, то есть f,
[01:09:04.960 --> 01:09:34.840]  xi, x1, xk-1, xk', меньше или равно f, xi, x1, xk-1, xk', как только x1', меньше xk',
[01:09:35.120 --> 01:09:46.760]  меньше xk', понятно, да? И второе, что из этой формулы следует, это покомпонентная непрерывность
[01:09:46.760 --> 01:09:56.840]  слева. Понятно, да, как ее доказывают? А n устремляем b, к b, которая возрастает, так сказать,
[01:09:56.840 --> 01:10:07.440]  там стремится к b, тогда пересечение всех таких множеств дает пустое множество, и мы получаем то,
[01:10:07.440 --> 01:10:17.120]  что предел вот этой функции, когда a, n возрастает, сходится к b, равно, собственно, вот этой точке,
[01:10:17.120 --> 01:10:25.540]  что и означает непрерывность слева. Пока все аналогично, функции распределения случайно
[01:10:25.540 --> 01:10:53.060]  влечены. xk-1, xk-0, xn равно f, xi, x1, и так далее, xk, и так далее, xn.
[01:10:53.060 --> 01:11:05.540]  Что у нас там еще было в функциях распределения случайно влечены? Свойство было такое, что когда
[01:11:05.540 --> 01:11:11.180]  x стремится к плюс бесконечности, функция распределения стремится к единице, и когда
[01:11:11.180 --> 01:11:21.860]  стремится к минус бесконечности, стремится к нулю, да? Ну, давайте для примера исследуем ситуацию,
[01:11:21.860 --> 01:11:39.580]  когда функция распределения, у нас есть некая xk-1, xn, и посмотрим, куда это стремится,
[01:11:39.580 --> 01:11:48.460]  когда xk-1 стремится к минус бесконечности. xk-1 стремится к минус бесконечности. Ну, в
[01:11:48.460 --> 01:11:58.660]  эта функция распределения по определению это вероятность того, что xi gt меньше xg, g не равно k,
[01:11:58.660 --> 01:12:20.900]  и xi gt меньше xg. Вот эта вероятность меньше или равна вероятности того, что xi gt меньше xg,
[01:12:20.900 --> 01:12:27.940]  поэтому если мы xg стремим к минус бесконечности, то не будет ни одного элементарного исхода,
[01:12:27.940 --> 01:12:36.300]  который будет принадлежать тому множеству, поэтому я тут как бы понятно, что не говорю,
[01:12:36.300 --> 01:12:48.940]  но пользуюсь теоремы непрерывности вероятностей. Это п пустое множество равно нулю. Ну и можно
[01:12:48.940 --> 01:12:57.900]  доказать аналогично, а можно там разными способами. Второе свойство аналогичная функция распределения
[01:12:57.900 --> 01:13:11.660]  случайной величины, что f xi x1 и так далее xn стремится к единице, если для любого g и gt стремится
[01:13:11.660 --> 01:13:20.660]  к плюс бесконечности. То есть для случайной величины там у нас была одна переменная,
[01:13:20.660 --> 01:13:25.860]  она стремилась к плюс бесконечности и функция распределения стремилась к единице, а здесь надо,
[01:13:25.860 --> 01:13:30.260]  чтобы все стремились. Если все стремятся к единице, то есть аналогичное свойство,
[01:13:30.260 --> 01:13:35.620]  но там чуть-чуть отличается. Все стремятся к единице, то и функция распределения стремится к единице.
[01:13:35.620 --> 01:13:45.060]  Ну собственно все, что по аналогии у нас есть, но у случайных векторов есть еще одно или у функции
[01:13:45.060 --> 01:13:59.100]  распределения случайной векторы. Есть еще одно свойство, которое выглядит так. f xi x1 и так далее xk-1 xk
[01:13:59.100 --> 01:14:11.860]  и так далее xn стремится, когда xkt стремится к единице, то есть не все, а одно. Ой, бесконечности,
[01:14:12.500 --> 01:14:27.460]  когда xkt стремится к бесконечности, это стремится к некой функции f xk-1 xk-1 xn, то есть у которой,
[01:14:27.460 --> 01:14:38.060]  во-первых, на единицу меньше переменных. Такое свойство имеет место, как иногда, так сказать,
[01:14:38.060 --> 01:14:45.500]  его пишут в учебниках. Но, с моей точки зрения, это в такой формулировке ни о чем, потому что,
[01:14:45.500 --> 01:14:51.460]  если мы возьмем функцию n переменных и какую-нибудь переменную стремление бесконечности, то понятно,
[01:14:51.460 --> 01:15:01.900]  что мы получим функцию с n-1 переменной. И что? И в чем фокус? Но здесь надо вот на что обратить
[01:15:01.900 --> 01:15:08.980]  внимание. Вот это свойство, во-первых, называется свойством согласованности, а во-вторых,
[01:15:08.980 --> 01:15:15.860]  означает, что когда вы какую-то компоненту, например, xk устремляете плюс бесконечности,
[01:15:15.860 --> 01:15:24.620]  то функция n-1 переменной, которую вы получаете, это в точности функция распределения вектора
[01:15:24.620 --> 01:15:39.420]  x' который имеет вид x1, xk-1, xk-1, xn. То есть это не произвольная функция от n-1 переменной,
[01:15:39.420 --> 01:15:47.660]  это функция распределения случайного вектора, у которого нету катой компонента. Понятно, да?
[01:15:54.620 --> 01:16:07.180]  Если у вас есть функция двух переменных, вы одну переменную устремляете к чему-нибудь,
[01:16:07.180 --> 01:16:13.500]  к бесконечности. Если этот предел существует, у вас получается функция одной переменной.
[01:16:13.500 --> 01:16:23.780]  Правильно, да? То есть здесь как раз ничего нет, это естественно. Особенно специфика,
[01:16:23.780 --> 01:16:28.900]  почему это выделено в отдельное свойство. Не потому же, что при переходе к пределу по одной
[01:16:28.900 --> 01:16:36.020]  переменной получается функция с одной переменной меньше, а потому, что та функция, которая
[01:16:36.020 --> 01:16:43.460]  получается, это тоже функция распределения какого вектора. Практически вектор x, только у него
[01:16:43.460 --> 01:16:54.580]  отсутствует катой компонента. Вот такое новое свойство, которого естественно нету просто для
[01:16:54.580 --> 01:17:05.580]  функции распределения случайных величин. Так сейчас прибегут наши коллеги,
[01:17:05.580 --> 01:17:11.740]  которые будут нас выгонять. Ну ладно, давайте, в принципе, докажем в следующий раз.

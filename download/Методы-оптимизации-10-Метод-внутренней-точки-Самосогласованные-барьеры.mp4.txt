[00:00.000 --> 00:13.280]  Сегодня продолжаем говорить про задачи с ограничениями, ну и соответственно смотрим
[00:13.280 --> 00:27.080]  на новый метод, новый подход к этим задачам. Целевая функция минимизируем на всем пространстве,
[00:27.080 --> 00:33.560]  но теперь у нас есть соответственно ограничение типа неравенства. Ограничение типа неравенства.
[00:33.560 --> 00:43.920]  Что хочется сделать? Напоминаю, что у нас происходило в прошлый раз. Брали штрафную функцию и к
[00:43.920 --> 00:51.080]  нашей целевой функции просто добавляли вот такого штрафа, вот такого вида. Ну и соответственно да,
[00:51.080 --> 00:58.520]  что с ним происходило? Когда у нас функция g была меньше нуля, в некотором смысле он активировался и
[00:58.520 --> 01:05.160]  вносил какую-то поправку в зависимости от размера параметра ro. Ну и соответственно если когда у
[01:05.160 --> 01:11.200]  нас функция была больше нуля, штраф не активировался в связи с того, что мы используем вот такую вот
[01:11.200 --> 01:18.500]  срезку. Один из подходов, мы с вами уже поняли в прошлый раз, что в принципе неплохой подход,
[01:18.500 --> 01:24.860]  единственное, что нужно увеличивать параметр ro, в связи с этим растет число обусловленности задачи,
[01:24.860 --> 01:30.620]  растет константа липшица, но при этом какие-то свойства в духе сильной выпуклости не меняются,
[01:30.620 --> 01:35.780]  поэтому сложность задачи от этого увеличивается. Плюс соответственно проблема здесь в том,
[01:35.780 --> 01:44.580]  что решение, которое вы получаете, оно может не удовлетворять ограничениям и чаще всего так и есть.
[01:44.580 --> 01:54.180]  Так, ну и у меня к вам вопрос, а как нам тогда ввести штраф, чтобы обязательно удовлетворять
[01:54.180 --> 02:00.580]  ограничениям и вот лежать в пределах вот этого множества, которое и этими функциями и вводится.
[02:00.580 --> 02:07.740]  Как можно ввести так, чтобы вроде как уводить что-то на бесконечность, но в нужный момент оставлять
[02:07.740 --> 02:18.740]  это в нуле? Какие-нибудь идеи? Как можно это сделать? Ну, топорный вариант сделать вот так. Мы
[02:18.740 --> 02:25.300]  вводим просто индикаторную функцию, которая и задает наше множество. Смысл этой функции в том,
[02:25.300 --> 02:31.700]  что она равна нулю на наше множество, которое задано ограничениями, и равна плюс бесконечности
[02:31.700 --> 02:38.060]  вне этого множества. Получаем что ли вот такую вот стену, которая за пределами множества говорит о том,
[02:38.060 --> 02:42.860]  что нужно просто возвращаться обратно, потому что значение целевой функции становится равным
[02:42.860 --> 02:49.140]  плюс бесконечности. Ну а в пределах нашего множества эта функция просто ноль, и она не вносит никакого
[02:49.140 --> 02:56.820]  вклада именно со саму минимизацию. Ну вот такое вот ограничение. С точки зрения каких-то формальных
[02:56.820 --> 03:04.820]  вещей, это ограничение на самом деле очень хорошее, потому что задача не меняется, и кажется,
[03:04.820 --> 03:14.540]  что более чем нормально, но какие вы видите у такого ограничения проблемы? Такого вида штрафа. Да,
[03:14.540 --> 03:19.860]  правда, он особо не помогает на практике, и в теории вы ничего про него хорошего этой тоже не скажете,
[03:19.860 --> 03:26.540]  потому что он не дифференцируем на границе, как раз в самой интересной области, у него просто
[03:26.540 --> 03:32.500]  не существует производный. Ну и как с ним жить, непонятно в теории. Ну в теории с ним можно только
[03:32.500 --> 03:37.340]  жить, когда у вас множество уже просто, и вы можете делать проекцию на него. Ну с этим мы с вами уже
[03:37.340 --> 03:43.660]  разбирались. Сейчас у нас хочется окунуться в варианты, когда каких-то проекций или простых
[03:43.660 --> 03:51.380]  решений, линиейных задач, вы их сделать не можете. Поэтому да, вот такого вида штрафа, ну он конечно
[03:51.380 --> 04:00.020]  имеет место, но скорее как какая-то теоретическая игрушка и для простых множеств g. Хорошо. Так, ну и
[04:00.020 --> 04:08.620]  возникает идея, отталкиваясь от этого штрафа, вот такого вида, воспроизвести его, переняв от
[04:08.620 --> 04:15.100]  него хорошие свойства, но как-то поборясь с теми проблемами, которые мы с вами озвучили. То есть
[04:15.100 --> 04:22.780]  хочется, чтобы вне множества g штраф уходил на бесконечность, и соответственно в пределах множества
[04:22.780 --> 04:33.860]  происходило что-то хотя бы около нулевое. Ну и пожалуйста, можно попробовать... так, это что?
[04:33.860 --> 04:43.340]  Это то же самое. Так, ну и перед тем, как вообще начать что-то описывать, ведем дополнительные
[04:43.340 --> 04:51.820]  предположения. Пусть у нас внутренняя часть множества, внутренние точки множества g это
[04:51.820 --> 04:58.860]  некоторые непустые множества. Также предполагаем, что у нас для любой точки x из множества g
[04:58.860 --> 05:04.460]  существует некоторая последовательность из внутренности, такая, что эта последовательность
[05:04.460 --> 05:13.500]  сходится к этой точке x. Плюс дополнительно говорим, что у нас множество g ограничено. Ну и просим,
[05:13.500 --> 05:22.540]  чтобы во внутренности множества g у нас все функции вот эти из ограничений были строго отрицательны.
[05:22.540 --> 05:30.940]  Ну и также предполагаем, что у нас функция f непрерывно дифференцируемая. Введя вот такие вещи,
[05:30.940 --> 05:39.060]  давайте посмотрим на вот такую функцию. Функцию f, которая у нас является непрерывно дифференцируемой
[05:39.060 --> 05:47.660]  на нашей внутренности множества, при этом для любой последовательности точек из внутренности,
[05:47.660 --> 05:53.980]  которая стремится к границе множества, эта функция просто улетает в бесконечность. Это как раз та самая
[05:53.980 --> 05:59.940]  конструкция, с которой хочется работать, то есть вести какую-то функцию, которая на внутренности множества
[05:59.940 --> 06:07.740]  ведет себя хорошо, а приближаясь к границе, начинает в некотором смысле повторять поведение индикатора,
[06:07.740 --> 06:15.460]  то есть улетать на бесконечность. Окей, ну и вот примеры таких функций можно посмотреть здесь.
[06:15.460 --> 06:20.620]  Примеры довольно очень простые. 1 делить на x. Обратная функция, все знают, что она просто уходит
[06:20.620 --> 06:28.100]  на бесконечность, когда соответственно аргумент этой функции стремится к нулю. Ну и пожалуйста,
[06:28.100 --> 06:36.300]  хотим вести наш нашу функцию, которая повторяет индикатор на бесконечность. Вот используем
[06:36.300 --> 06:41.460]  пожалуйста вот такую функцию f. Аналогично можно поиграться с логарифмом. Про него мы тоже знаем,
[06:41.460 --> 06:46.340]  что у него там происходит в нуле. Уходит в бесконечность, но хочется в плюс бесконечность
[06:46.340 --> 06:53.340]  уходить, поэтому берем со знака минус. Все, очень простые идеи. То есть получается, что вы вместо
[06:53.340 --> 06:58.500]  индикатора, который по факту ставит вот такую вот стену, за которую вам просто нельзя выходить,
[06:58.500 --> 07:04.780]  эту стену в некотором смысле опроксимируете, ну или не стену, а в данном случае это все называется
[07:04.780 --> 07:11.020]  барьером. Еще что в этом случае хорошо, то что действительно из-за того, что вот эти функции
[07:11.020 --> 07:17.460]  улетают на бесконечность, когда вы приближаетесь к границе, вы интуитивно кажется, что вы за пределы
[07:17.460 --> 07:27.140]  множества не выйдете. Не выйдете, а значит это что-то хорошее, и тут появляется свойство,
[07:27.140 --> 07:32.860]  которое у нас не было до этого. То есть множество, что решение, которое мы найдем,
[07:32.860 --> 07:38.740]  во-первых, мы пока не ввели задачу, но да ладно. Давайте пока скажем, что мы действительно
[07:38.740 --> 07:44.660]  воспроизвели индикатор. Воспроизвели индикатор, но более гладким и дифференцируем образом.
[07:44.660 --> 07:49.500]  Вот, единственный вопрос вот к тем функциям, которые у меня определены здесь, у индикатора
[07:49.500 --> 07:56.340]  значение на самом множестве было ноль. Вот, понятно, что для этих функций это не так. Это не так,
[07:56.340 --> 08:01.500]  и в зависимости от того, что у вас там происходит в функции g, но они могут принимать явно не нулевые
[08:01.500 --> 08:09.980]  значения. Как можно с этим побороться, чтобы все еще бесконечность осталась, но вот здесь была ноль на самом множестве.
[08:09.980 --> 08:18.500]  Ну, минимум, минимум, минимум просто сделает вас этот. Он срезку сделает,
[08:18.500 --> 08:23.980]  это будет не дифференцируемость. Вот, чем можно сделать? Что можно сделать? Ну, вот предлагается
[08:23.980 --> 08:29.460]  сделать следующее. А давайте я введу параметры ro. Как и в штрафах, я введу параметры ro, которые
[08:29.460 --> 08:34.860]  у меня будут выполнять вот такую функцию. Я его добавлю вот так, вот 1 делить на ro к функции f.
[08:34.860 --> 08:43.860]  Соответственно, если у меня ro фиксировано, функция f все еще ведет себя хорошо в плане ухода на
[08:43.860 --> 08:50.660]  бесконечность. Ну, понятно, уменьшая ro, я могу контролировать поведение этой функции на необходимом
[08:50.660 --> 08:55.780]  мне множестве. При достаточно больших ro f, вот это значение функции, вот которая вот здесь,
[08:55.980 --> 09:03.060]  оно будет близко к нулю. Вот, и таким вот образом я буду контролировать поведение вот этой моей
[09:03.060 --> 09:07.740]  новой штрафной функции. На самом деле уже и барьерной функции, потому что это уже не штраф, а барьер,
[09:07.740 --> 09:16.140]  потому что он просто запрещает нам выходить за пределы множества. Свойства. Ну, и соответственно,
[09:16.140 --> 09:22.740]  рассматриваем вот такую вот задачку. f от ro, f большое от ro, где у нас есть целевая функция, плюс
[09:22.740 --> 09:35.300]  барьер соответствующим множителям. Один делить на ro. Так, ну давайте быстренько пробежимся по каким-то
[09:35.300 --> 09:42.020]  простеньким свойствам, что мы теперь знаем про нашу новую целевую функцию f от ro. Она непрерывно
[09:42.020 --> 09:48.500]  дифференцируема на внутренности множества g. Простой факт, потому что мы предположили, что у нас
[09:49.140 --> 09:56.140]  функция f непрерывно дифференцируема не просто на внутренности g, а на всем множестве g. Про барьер,
[09:56.140 --> 10:00.100]  когда мы его водили, барьерную функцию f когда мы водили, мы предположили, что она непрерывно
[10:00.100 --> 10:05.040]  дифференцируема на внутренности g. Ну, значит, и сумма будет непрерывно дифференцируема на
[10:05.040 --> 10:13.380]  внутренности. А также можно заметить, то что функция f от ro при стремлении к бесконечности,
[10:13.380 --> 10:18.340]  при стремлении к границе будут стремиться к бесконечности. Откуда это следует?
[10:18.340 --> 10:24.640]  исследуют ровно из свойств функции f потому что она стремится к бесконечности
[10:24.640 --> 10:31.040]  а из непрерывности функции f маленькая на все множестве g мы можем сказать что
[10:31.040 --> 10:34.640]  она принимает какие-то там конечные значения ограниченные значения
[10:34.640 --> 10:41.520]  соответственно они не влияют на функцию f' и по факту поведение на
[10:41.520 --> 10:44.880]  границе определяется вот барьерной функцией вот стремление к бесконечности
[10:44.880 --> 10:52.900]  бесконечности хорошее свойство так такой у меня к вам вопрос на самом деле
[10:52.900 --> 10:57.780]  кажется что мы задачу переписали в виде задачи уже безусловной то есть у нас
[10:57.780 --> 11:03.300]  есть множество x мы по всему rd просто минимизируем нашу функцию f' но формально
[11:03.300 --> 11:07.460]  это задача все же с ограничениями почему
[11:08.160 --> 11:17.120]  потому что на ноль делить нельзя как это можно более формально сказать у вас
[11:17.120 --> 11:24.080]  функция f определена только в пределах внутренности множества g за пределами
[11:24.080 --> 11:30.080]  этого множества ваш штраф не существует как не существует там условно логарифм и
[11:30.080 --> 11:35.600]  соответственно по-хорошему вы должны решать в некотором смысле эту задачу на
[11:35.600 --> 11:42.380]  множестве определения но с этим проблем нету с этим проблем нету потому что на
[11:42.380 --> 11:47.660]  самом деле если у вас есть некоторая стартовая точка которую вы взяли из
[11:47.660 --> 11:54.020]  внутренности а дальше вы запускаете какой-то метод спуска который приближается
[11:54.020 --> 11:58.300]  к решению вот вашей функции к минимуме и к минимуму функции f
[11:58.300 --> 12:04.060]  рот икс то есть вы можете гарантировать что каждую эту рацию вы становитесь в
[12:04.060 --> 12:09.520]  некотором смысле только лучше хотя бы не хуже чем стартовая точка с точки
[12:09.520 --> 12:16.120]  зрения значения по значения функции от текущего текущего икс на текущей
[12:16.120 --> 12:23.040]  итерации ну и получается так что вы становитесь только ниже с точки зрения
[12:23.040 --> 12:28.640]  функции но вы знаете что у вас функция f рост стремится бесконечности когда вы
[12:28.640 --> 12:36.700]  приближаетесь к границе когда вы приближаетесь границе ну и получается что
[12:36.700 --> 12:42.860]  в какой-то окрестности границы у вас значение функции будет больше чем значение
[12:42.860 --> 12:50.900]  функции в точке x 0 получаем что тогда у нас экската будет лежать точно не в
[12:50.900 --> 12:55.940]  этой окрестности границы то есть все еще будет лежать во внутренности множество
[12:55.940 --> 13:00.620]  вот то есть получается что-то хорошее в духе того что вы действительно за
[13:00.620 --> 13:07.280]  пределы множества не выходите и это такой приятный факт надавать его можно
[13:07.280 --> 13:12.500]  более формально доказать более формально доказать следующим образом вот плюс нам
[13:12.500 --> 13:17.660]  тут понадобится некоторое вспомогательное утверждение пусть у нас да дан некоторые
[13:17.660 --> 13:22.760]  параметры больше нуля соответственно заданная вот эта функция f с барьером и мы
[13:22.760 --> 13:27.720]  хотим доказать что эта функция принимает свое минимальное значение на
[13:27.720 --> 13:34.040]  внутренности множество g вот более того мы хотим доказать что вот множество
[13:34.040 --> 13:39.880]  такого вида для любого параметра а это туда попадают те в точки из
[13:39.880 --> 13:46.800]  внутренности где у вас функция с барьером меньше значения а вот является
[13:47.360 --> 13:58.880]  доказательства довольно простое я не знаю как вам водилось замкнутость множества на
[13:58.880 --> 14:06.320]  лекциях по мотонализу первом семестре как водилось отлично супер через предельную
[14:06.320 --> 14:10.680]  точку здесь просто в книге иванова я посмотрел там по-другому ну на самом деле очень
[14:10.680 --> 14:17.000]  похоже вот рассматривая некоторую последовательность из множества у хочу
[14:17.000 --> 14:22.400]  доказать что множество у меня является замкнутым рассматриваем некоторую
[14:22.400 --> 14:27.520]  последовательность сходящиеся в точке x чтобы доказать замкнутость мне нужно
[14:27.520 --> 14:34.760]  показать что тогда та вот предельная точка x тоже будет лежать в у вот так как
[14:35.560 --> 14:41.600]  стремиться к teachers для x и при этом иксы то у меня лежат во внутренности множество
[14:41.600 --> 14:49.760]  g то у меня два варианта есть для точки x либо она лежит на границе либо оно
[14:49.760 --> 14:58.100]  лежит во внутренности множества жлежища мнеKI что я при этом знаю про
[14:58.100 --> 15:06.460]  про функцию f от точки x и t. x и t, здесь аргумент у меня лежит в пределах g, я знаю, что он
[15:06.460 --> 15:13.620]  всегда меньше a, просто по определению множества u. По определению множества u
[15:13.620 --> 15:18.760]  у меня вот выполнено вот это. Тогда когда я перехожу к пределу под
[15:18.760 --> 15:24.340]  знаком неравенства, перехожу к пределу под знаком неравенства, что я получаю? То,
[15:24.340 --> 15:38.460]  что у меня всегда, всегда получается, что вот этот предел, так, сейчас как бы тут лучше
[15:38.460 --> 15:44.660]  сделать, если мы на границе лежим, непрерывность этого функции f.
[15:44.660 --> 16:02.780]  Ну тогда да, можно, наверное, сделать что-то в духе того, что... просто у нас f не определено в x,
[16:02.780 --> 16:10.420]  так как x на границе, там нет, точнее, определено, но нет непрерывности. Нет непрерывности, тогда что
[16:10.420 --> 16:20.260]  можно сделать? Ну, если у нас x лежит где-то на границе, лежит где-то на границе, то в некоторый
[16:20.260 --> 16:27.500]  момент у меня x начнут всегда попадать в некоторую эпсилонокрестность этой границы. В эпсилонокрестность,
[16:27.500 --> 16:34.380]  эпсилонокрестность границы. Ну и, соответственно, в силу того, что я знаю, что на границе функция
[16:34.380 --> 16:39.740]  ведет себя, функция уходит в плюс бесконечность, я могу подобрать эпсилонокрестность такую,
[16:39.740 --> 16:50.260]  что у меня значение функции fхt в этой эпсилонокрестности будет больше, чем а. Так? Ну,
[16:50.260 --> 16:56.540]  я получаю противоречие, потому что у меня, мы точно предположили, что в эту последовательность
[16:56.540 --> 17:02.740]  хт входят только такие точки, где у меня значение функции меньше а. Вот, получается,
[17:02.740 --> 17:10.220]  что, тут надо переписать, получается, переход, тогда у меня х принадлежит внутренности, другого
[17:10.220 --> 17:17.500]  варианта не дано. Ну, а здесь тогда вообще все хорошо, потому что функция f непрерывно у меня на
[17:17.500 --> 17:24.180]  внутренности множество g. Откуда тогда следует предельный переход? У меня есть f ро х и t,
[17:24.180 --> 17:31.820]  и когда я делаю, и оно меня меньше, чем а, соответственно, ставлю пределы с обеих сторон,
[17:31.820 --> 17:37.620]  этот предел у меня существует в силу непрерывности f ро, на внутренности у меня здесь
[17:37.620 --> 17:42.180]  просто получается f ро в точке х, ну и здесь получается предел от константа. Это просто
[17:42.180 --> 17:47.560]  константа. Все, конец. Получается, что действительно х у меня лежит во внутренности, и значение в точке
[17:47.560 --> 17:55.740]  х у меня меньше а, а значит, что у меня х принадлежит множеству у. Так? Получили, что у
[17:55.740 --> 18:00.300]  меня предельная точка тоже лежит в этом множестве. Отсюда следует то, что множество у у меня
[18:00.300 --> 18:08.300]  замкнута. Ограниченность множества u следует из того, что мы во множество u берем только точки из
[18:08.300 --> 18:14.180]  внутренности множества g, а множество g у меня ограничено. Значит его внутренность ограничена,
[18:14.180 --> 18:19.940]  значит ограниченные множество u. Получается, что у меня это компакт, u это компакт.
[18:19.940 --> 18:28.540]  Ну и как когда-то в школе зашел в университет, дедушка там так на лекции сказал, как говорил
[18:28.540 --> 18:34.980]  Веерштраз, если у меня функция непрерывно на компакте, то она достигает своего минимума на
[18:34.980 --> 18:51.300]  этом компакте. Поэтому я могу сказать, что на множестве u f' достигает минимума на u. Ну и в силу
[18:51.300 --> 19:00.860]  того, как я определял u, это как получается, туда входят иксы, для которых там значение f' от x меньше
[19:00.860 --> 19:08.420]  чем некоторое значение. А понятно, что минимум на u есть и будет минимумом этой функции на
[19:08.420 --> 19:23.380]  все множестве int g. Хорошо, тут подоказывали про уровни, поняли. Теперь похожая теорема на ту,
[19:23.380 --> 19:29.960]  которую мы с вами рассматривали для барьерных функций. То есть хочется показать, что с увеличением
[19:29.960 --> 19:35.460]  ρ, а мы действительно хотим увеличивать ρ, чтобы как раз у нас свойство бесконечности оставалось,
[19:35.460 --> 19:43.500]  но при этом мы прижимали функцию на наше множество к нулю. С увеличением ρ мы будем попадать в
[19:43.500 --> 19:50.300]  епсилонокрестность, все меньше и меньше епсилонокрестность множество решений x звездой.
[19:50.300 --> 19:56.300]  Но здесь множество решений x звездой уже определяется как не просто некоторые, не просто x из rd,
[19:56.300 --> 20:07.940]  а x из g. Я его немного раздуваю на епсилон. Хорошо, что тут дополнительно я предполагаю? Еще то,
[20:07.940 --> 20:17.260]  что у меня замыкание внутренности моего множества равно самому множеству. В принципе,
[20:17.260 --> 20:22.660]  нормальное предположение, никаких страшностей нет, просто чтобы там полегче доказывалось.
[20:22.660 --> 20:31.300]  Хорошо. Пару понятных фактов, которые следуют сразу из утверждения. То, что у меня x звездой
[20:31.300 --> 20:36.180]  это не пустое множество, то есть множество решений у меня не пустое. Почему? Потому что в силу того,
[20:36.180 --> 20:43.300]  что я уже предполагал, что у меня g это ограниченное множество, теперь я еще дополнительно предположил,
[20:43.300 --> 20:48.140]  что оно замкнуто, потому что представляет собой замыкание некоторого открытого множества.
[20:48.140 --> 20:56.260]  Тогда получается, что мы решаем задачу оптимизации функции f на некотором компакте. Непрерывная
[20:56.260 --> 21:01.700]  функция на компакте принимает опять же по теореме Бирштрасса свое минимальное значение. Значит,
[21:01.700 --> 21:12.740]  множество решений в данном случае не пусто. Исходные задачи. Ну а то, что у меня множество
[21:12.740 --> 21:19.380]  решений для какого-то фиксированного РО моей новой задачи с барьером не пустой, а мы доказывали
[21:19.380 --> 21:25.420]  в предыдущем свойстве, показывали, что там действительно решение существует, а оно лежит во
[21:25.420 --> 21:35.340]  внутренности множества g. Хорошо. Ну а теперь поехали доказывать от противного то, что у меня
[21:35.340 --> 21:44.060]  действительно с увеличением РО я буду попадать в епсилонокрестность все меньше и меньше множества x.
[21:44.060 --> 21:51.940]  Похожая техника, как мы с вами делали на предыдущем занятии. Говорим, что существует
[21:51.940 --> 21:59.300]  епсилон больше нуля и соответственно какая-то последовательная стероида, которая стремится к
[21:59.300 --> 22:06.780]  бесконечности. Такая, что соответственно, что там происходит? У нас существует x и t со звездой из
[22:06.780 --> 22:17.860]  множества x со звездой и РО и. То есть некоторые x из решения конкретной задачи с фиксированным РО и там,
[22:17.860 --> 22:31.380]  для которых выполнено, что x и t со звездой лежит вне множества x епсилон x епсилон епсилон
[22:31.380 --> 22:41.060]  со звездой. То есть не лежит в этой епсилонокрестности. Да, не лежит, не лежит. Вот. Окей. То же самое,
[22:41.060 --> 22:47.940]  что сделали в прошлый раз. То же самое предположение, которое было. Вот. Оно же здесь записано.
[22:47.940 --> 22:57.580]  Вот. Ну давайте рассуждать. Давайте рассуждать. В силу того, что у меня теперь мы знаем, что у меня
[22:57.580 --> 23:06.700]  вот это множество множество решений принадлежит лежит внутри int g, а int g у меня в свою очередь это
[23:06.700 --> 23:13.060]  ограниченное множество, ограниченное множество. Я опять же говорю, что последовательность x и t со
[23:13.060 --> 23:23.740]  звездой это у меня ограниченная последовательность. И, ровно как в прошлый раз, теорема, кого? Бальсана
[23:23.740 --> 23:30.300]  Вирштраса, ограниченная последовательность существует сходящейся под последовательностью. Рассматриваем
[23:30.300 --> 23:37.420]  под последовательность x и t, которая сходится к точке x с волной, например. x с волной со звездой.
[23:37.420 --> 23:46.740]  x с волной со звездой. Хорошо. Рассмотрели вот такую точку. Это понятно. То есть существует такая
[23:46.740 --> 23:55.420]  последовательность, которая сходится со звездой. Так. Написали. Дальше что предполагаем? Почему у меня
[23:55.420 --> 24:03.900]  точка x со звездой лежит в g? Почему она лежит в g? Кто понимает? Кто понимает, почему она лежит в g?
[24:03.900 --> 24:19.980]  Да. Да, да, да, да. В силу того, что у меня все x и t лежат в g, то у меня предельная точка x тильдой
[24:19.980 --> 24:32.780]  g лежит либо во внутренности, либо на границе. Так. Но я знаю, что у меня множество g замкнуто,
[24:32.780 --> 24:40.180]  поэтому эта вещь, она включает в себя то, что x по факту у меня лежит в g. Так. Ну и как раз
[24:40.180 --> 24:53.020]  предположение о замкнутости здесь помогает. Вот. Хорошо. Так. Это мы с вами обсудили. Дальше. Что мы
[24:53.020 --> 25:00.380]  еще говорим? Почему у меня x тильдой не лежит в x со звездой? Не лежит в x со звездой. Почему это так?
[25:00.380 --> 25:13.020]  Что будет, если оно лежит в x со звездой? Да. То есть если у меня x тильдой со звездой будут
[25:13.020 --> 25:23.300]  лежать в x со звездой, то с какого-то номера i у меня x и t будут лежать в эпсилонокрестности x со
[25:23.300 --> 25:29.300]  звездой. Для любого эпсилона это будет справедливо, потому что предел лежит в x со звездой, тогда и точки
[25:29.300 --> 25:35.180]  начнут попадать в какую-то эпсилонокрестность этого множества. Но мы с вами предположили, что x и t
[25:35.180 --> 25:44.660]  у нас создаются ровно так, что они не лежат в x со звездой эпсилон. Вот. Ну получается противоречие и в силу
[25:44.660 --> 25:50.260]  того, что мы предположили, x со звездой не лежит в пределах x. То есть получается x тильдой со звездой
[25:50.260 --> 25:59.140]  лежит в g, но не лежит в x со звездой. Не лежит в x со звездой. Тогда что я могу сказать о значении в
[25:59.140 --> 26:06.780]  точке x тильдой со звездой? Так как она, вот эта x тильдой со звездой лежит вне моего множества
[26:06.780 --> 26:13.500]  решений, вне моего множества решений, то тогда существует некоторое значение дельта, положительное,
[26:13.500 --> 26:22.100]  положительное значение дельта. Такое, что вот выполнено следующее соотношение. Значение
[26:22.100 --> 26:32.740]  минимума, вот это минимум на g функции f будет меньше либо равно, чем f тильдой со звездой, причем
[26:32.740 --> 26:38.300]  еще дельта эту разность контролирует. Ну понятно, в силу того, что это у меня не решение, между ними
[26:38.300 --> 26:45.100]  есть зазор. Вот, и этот зазор, как раз вот я в этот зазор помещаю дельту. Так, какую-то константу положительную.
[26:45.100 --> 26:55.660]  Хорошо, хорошо, поместили эту дельту сюда. Бум-бум-бум. Так, здесь это поместили. Дальше,
[26:55.660 --> 27:06.060]  соответственно, следующее рассуждение. В силу того, что у меня множество, в силу того, что у меня множество g
[27:06.060 --> 27:15.180]  замкнуто, а функция f непрерывно, то я могу найти некоторую точку x тильдой из внутренности,
[27:15.180 --> 27:25.380]  которая будет близка в некотором смысле к точке к x звездой. x тильдой близка к x звездой. При этом
[27:25.380 --> 27:32.660]  вот это из внутренности g. Вот, x ну это просто решение, которое мы с вами уже рассматривали. Вот,
[27:32.660 --> 27:38.340]  ну и в силу того, что у меня f непрерывно, непрерывно, я эту близость могу контролировать следующим образом.
[27:38.340 --> 27:46.580]  То есть, я могу подобрать x тильдой настолько близко, что у меня значение в этой точке x тильдой будет
[27:46.580 --> 27:51.740]  меньше либо равно, чем f от x звездой, плюс вот этот параметр дельта, но теперь уже пополам,
[27:51.740 --> 27:59.740]  который мы уже с вами определили, как вот то значение. Окей? Вот, хорошо. Вот так вот наопределяли
[27:59.740 --> 28:07.540]  это всё безобразие. x тильдой f от x звездой плюс дельта. Дальше что хочется сделать? Дальше
[28:07.540 --> 28:14.340]  хочется попробовать пописать, что у меня будет происходить с функцией row, f row, например,
[28:14.340 --> 28:26.140]  в точке x тильдой со звездой it, ну и соответствующее row it я здесь беру. Вот. Что это? Будьте здоровы.
[28:26.140 --> 28:35.500]  Что это такое? Это у меня f от x стильдой со звездой, f от x тильдой со звездой, плюс 1 делить на row.
[28:35.500 --> 28:52.980]  Моя штрафная функция row it, здесь row it с тильдой, а здесь будет просто f x с тильдой. Так вот. Окей.
[28:52.980 --> 29:00.980]  Это оптимальное значение для вот этого текущего параметра row. Получается, что это значение будет
[29:00.980 --> 29:09.100]  меньше либо равно, чем значение функции row it с тильдой в точке x тильдой. Так? Ну, просто потому что это
[29:09.100 --> 29:15.020]  решение, а это какая-то произвольная точка. Что хорошо, что она из внутренности. Что хорошо,
[29:15.020 --> 29:20.100]  что она из внутренности, поэтому подставлять ее функцию можно. Подставлять ее функцию можно,
[29:20.100 --> 29:29.460]  она определена будет. Вот. Так. Ну и тогда здесь это тоже расписываю. Получаю, что у меня здесь f с тильдой,
[29:29.460 --> 29:44.700]  плюс 1 делить на row it тильда, f x тильда просто, f x тильда. Вот. Хорошо. Хорошо. Давайте я переношу это
[29:44.700 --> 29:53.820]  аккуратненько в две стороны. Давайте вот так вот. f x и t со звездой меньше либо равно, чем f от x со звездой,
[29:53.820 --> 30:09.180]  плюс f row и тильда f от x с волной, минус f row и t f от x с волной. Так. Только что с вами доказывали то,
[30:09.180 --> 30:20.540]  что у меня функция f row x и t в некотором смысле ограничена снизу на int g, на int g, так,
[30:20.540 --> 30:27.060]  через линии уровня. То есть существует какая-то константа c, которая строго больше,
[30:27.060 --> 30:31.820]  чем минус бесконечность, которая ограничивает у меня f row. Предыдущее свойство про это было,
[30:31.820 --> 30:36.660]  про уровни, что у меня вот это множество u, про множество u, где мы это доказывали с вами. Вот.
[30:36.660 --> 30:43.740]  Аналогично я могу доказать, что у меня и уровни будут ограничены какой-то константой там s тильдой,
[30:43.740 --> 30:48.500]  которая меньше минус бесконечности, потому что там я что, пользовался только тем, что у меня
[30:48.500 --> 30:54.940]  есть функция f, но я сейчас уберу просто непрерывные на всем же функциям. Ну, не страшно. Тоже минимально
[30:54.940 --> 30:59.780]  максимальное значение, соответственно, но она не повлияет на вот это то, что у меня есть там
[30:59.780 --> 31:06.740]  ограничивающее снизу у барьера функция на множестве int g. Вот. А это хорошо,
[31:06.740 --> 31:14.660]  то что на int g у меня функция ограничена, потому что точки x тильдой у меня лежат в int g. Вот. Поэтому
[31:14.660 --> 31:20.060]  вот это выражение у меня тоже в некотором смысле ограничено, потому что ограничено,
[31:20.060 --> 31:26.460]  если функция ограничена снизу, то минус функция ограничена сверху. И поэтому здесь я могу вот
[31:26.460 --> 31:38.020]  оценить это вот так. f от x тильдой минус rho и t тильда c. Ну, с тильдой пусть будет. Вот.
[31:38.020 --> 31:44.060]  Получается что? Получается, что у меня вот это некоторая константа, некоторая константа,
[31:44.060 --> 31:51.900]  это тоже некоторая константа. Вот. Что будет, когда я перейду к пределу в этом неравенстве? Что будет,
[31:51.900 --> 32:03.220]  если я возьму здесь предельчики? Справа и слева. Куда будет стремиться правый кусочек в пределе,
[32:03.220 --> 32:10.340]  если я возьму предел по и? Просто в f от x тильдой. Правильно. Потому что rho у меня будет стремляться
[32:10.340 --> 32:16.940]  в бесконечность, мы ее так определяем. Вот. Соответственно, все вот эти константы делить
[32:16.940 --> 32:22.580]  на что-то, которое стремится к бесконечности, это будет просто стремиться к нулю, останется поэтому f от x тильдой.
[32:22.580 --> 32:31.380]  Вот. А про f от x тильдой мы еще знаем, что оно у меня f от x звездой плюс дельта пополам. Вот. Вот оно. То,
[32:31.380 --> 32:38.780]  как мы определили f от x тильдой. Так. А правый кусочек куда будет стремиться? Правый кусочек куда
[32:38.780 --> 32:51.420]  будет стремиться? Кто понимает, куда будет стремиться? Ой, да, левый, левый, левый кусочек. Вот этот левый
[32:51.420 --> 33:01.620]  кусочек, куда он будет стремиться? f от x тильдой со звездой. f от x тильдой со звездой. Просто потому
[33:01.620 --> 33:15.860]  вот мы это с вами уже находили. То, что у меня f от x тильдой со звездой лежит в g, а у меня f непрерывно
[33:15.860 --> 33:24.980]  на g, поэтому предел валиден. Придел валиден. Так. Это все стремится к f от x тильдой со звездой. Но,
[33:24.980 --> 33:32.020]  с другой стороны, про f от x тильдой со звездой я знаю то, что у меня он меньше, строго меньше,
[33:32.020 --> 33:42.620]  чем f от x плюс дельта. f от x со звездой плюс дельта. Но теперь смотрим, что получилось. Раз, строго меньше.
[33:42.620 --> 33:54.540]  Раз. Так. Строго меньше. Получается, что у меня вот здесь вот это выражение меньше, чем вот это
[33:54.540 --> 34:00.180]  выражение. Ну и по факту оно означает, что положительная дельта меньше, чем дельта пополам.
[34:00.180 --> 34:08.380]  Противоречие? Противоречие. То есть, изначально предположение о том, что у меня существует такая
[34:08.380 --> 34:15.020]  последовательность, которая будет выбиваться из того предположения, что у меня прибольший кро будет
[34:15.020 --> 34:22.940]  все больше-больше вмещаться в этого апсилонокреста, смудует множество, оно было неверным. Согласны? Все,
[34:22.940 --> 34:32.540]  отлично. Получается, что мы доказали похожий факт, который у нас был на прошлый раз. Здесь
[34:32.540 --> 34:44.740]  все расписано. Итог по вот этой половине лекции. Поняли, что, опять же, условные задачи превратили
[34:44.740 --> 34:51.820]  в безусловную. Формально нет, но по факту да. Поняли, что при увеличении row у нас в некотором смысле
[34:51.820 --> 34:58.780]  апроксимация барьера, который мы строим, становится лучше. Это свойство нуля появляется. Хорошая,
[34:58.780 --> 35:09.660]  опять же, новость в том, что мы удовлетворяем ограничением. Ну и почему, например, тот сегодняшний
[35:09.660 --> 35:16.780]  разговор вообще озаглавлен как метод внутренней точки? Потому что, как раз удовлетворяя ограничением,
[35:16.780 --> 35:23.980]  мы никогда не выходим за пределы нашего множества. Мы доказали, что для каждой функции f-row
[35:23.980 --> 35:33.460]  минимум его лежит в пределах множества. Inge, мы никогда не выходим за пределы. Барьеры нас
[35:33.460 --> 35:42.100]  от этого удерживают. Ну и на самом деле, поэтому это все называется внутренняя точка. Ну и в общем-то,
[35:42.100 --> 35:48.580]  случай и все. Больше ничего сказать тут хорошего нельзя, ровно как было и со штрафами. Как-то можно
[35:48.580 --> 35:54.780]  попробовать. Что нужно делать? Просто взять какое-то row, попробовать решить задачу безусловной оптимизации,
[35:54.780 --> 36:00.900]  ну, например, градиентным спуском метод, ньютона, чем угодно. Понравилось решение по качеству,
[36:00.900 --> 36:08.180]  с точки зрения сходимости, ну тогда можно оставить. Не понравилось, нужно попробовать увеличить row,
[36:08.180 --> 36:13.420]  тогда у вас в некотором смысле ограничение на себя потянут сильнее, вот, и решение станет
[36:13.420 --> 36:19.700]  качественнее. Вот, в принципе, на этом все. То есть, что можно в общем случае сказать. Но это далеко
[36:19.700 --> 36:24.980]  не все, что можно сказать в частных случаях. И вот как раз вторая часть лекции будет про то,
[36:24.980 --> 36:31.460]  что у нас в некотором смысле патриотичная, потому что как раз в частном случае выпуклой оптимизации,
[36:31.460 --> 36:39.020]  эту задачу хорошо так исследовали Аркадий Семенович Немировский и Юрий Евгеньевич Нестеров. И поведение
[36:39.020 --> 36:46.020]  метода внутренней точки для задачи выпуклой оптимизации, это их заслуга. Вот, мы постараемся
[36:46.020 --> 36:51.060]  просто чуть-чуть прикоснуться к их результатам. Так-то там уже все результаты, которые есть по
[36:51.060 --> 36:56.580]  моменту внутренней точки, даже классические, занимают книгу тысячи страниц. Понятно,
[36:56.580 --> 37:02.340]  всю книжку мы с вами прочитать не можем. Но какую-то такую основную теорему на нее посмотрим,
[37:02.340 --> 37:07.460]  и увидим, что действительно результат классный. И вот то, что мы доказали сейчас, это вот по
[37:07.460 --> 37:14.540]  сравнению с ним ничего. Мы получим прям хорошие оценки, гарантии сходимости, вплоть до того,
[37:14.540 --> 37:20.540]  сколько... сколько... как сильно нужно уменьшать РО. Вот, и как быстро мы будем сходиться к решению.
[37:20.540 --> 37:29.860]  Ну что, продолжаем? Продолжаем разговор. Успокаиваемся, продолжаем разговор. Так,
[37:29.860 --> 37:39.980]  окей. Чтобы вести на самом деле что-то хорошее про барьерный метод, нужно дополнительное понятие.
[37:39.980 --> 37:49.260]  В данном случае это самосогласованные функции и самосогласованные барьеры. Определение не
[37:49.260 --> 37:58.060]  очень приятное, вот, и не очень понятное. Вот. Но на самом деле тут в него вникать особо не
[37:58.060 --> 38:03.900]  нужно. Потому что по факту пользуются просто хорошими примерами, которые удовлетворяют
[38:03.900 --> 38:10.300]  этому определению. Вот. А смысл на самом деле у этого определения, ну там даже мне на самом
[38:10.300 --> 38:15.460]  деле не до конца понятно, откуда оно пришло. Потому что хорошая мотивация у этого всего безобразия,
[38:15.460 --> 38:24.420]  то что хочется определить какой-то класс функций, который будет неплохо работать в методе Ньютона.
[38:24.420 --> 38:31.060]  Потому что мы знаем, что у вас метод Ньютона сходится локально. За счет подбора шага, за счет
[38:31.060 --> 38:38.080]  демфонирования, демфрирования, демфрирования можно добиться того, что будет сходиться и
[38:38.080 --> 38:45.580]  глобально, но линейно. Никакой супер линейной скорости у вас там не будет. Вот. Поэтому пытаются
[38:45.580 --> 38:53.120]  определить какие-то более хитрые классы. Какие-то более хитрые классы. Ну, в том числе определяют
[38:53.120 --> 39:01.400]  вот такой. Потому что в некоторых анализах метода Ньютона, в том числе с демфрированием,
[39:01.400 --> 39:10.200]  важно, как ведет себя третья производная. Вот. Ну, хорошо. Определяется вот такой класс. Еще раз
[39:10.200 --> 39:15.280]  я говорю, тут в него вникать не нужно. Опять же, про это написана книга 1000 страниц. Почему вот это
[39:15.280 --> 39:21.160]  хорошо? В том числе анализ метода Ньютона, я этими фактами просто буду пользоваться. Примерчики.
[39:21.160 --> 39:28.000]  Примерчики, которые нам нужны. Квадратичная функция, логарифм и комбинация логарифма и
[39:28.000 --> 39:33.240]  квадратичной функции. Вот. По факту последний пример, это то, что нам надо. То, что у нас
[39:33.240 --> 39:41.120]  логарифм, это запиханная туда линейная функция квадратичная, будет являться самосогласованной
[39:41.120 --> 39:46.080]  функцией. Вот. Потому что мы ровно идем к тому, чтобы сделать барьер самосогласованным. То есть,
[39:46.080 --> 39:51.440]  каким-то хорошим довольно-таки. Почему это хорошо? Ну, вот сейчас будем эти свойства
[39:51.440 --> 39:58.600]  определяться и с помощью них там красиво все докажется. Также операции, которые сохраняют
[39:58.600 --> 40:05.120]  самосогласованность, это комбинация сумма двух самосогласованных функций будет самосогласованной,
[40:05.120 --> 40:10.920]  если соответственно, коэффициенты, с которыми вы суммируете, больше единицы. Вот. Ну, а также,
[40:10.920 --> 40:18.240]  если вы меняете аргумент самосогласованной функции на какую-то афинную комбинацию, то тогда тоже
[40:18.240 --> 40:24.120]  новая функция будет самосогласованной. Понятное тоже свойство. Скорее вот они нам и понадобятся.
[40:24.120 --> 40:30.240]  Вот. Определение, оно у нас страшное. Вот. И да, из него много чего хорошего следует, но это технические
[40:30.240 --> 40:38.240]  факты, которые, ну, сегодня мы пропустим. Вот. Самосогласованный барьер. Как раз ровно к нему мы идем.
[40:38.240 --> 40:46.800]  Барьер у нас самосогласованный. В каком случае? Причем вводится параметр ню, параметр
[40:46.800 --> 40:53.880]  самосогласованности барьера. Что, соответственно, ню всегда у нас больше единицы. Барьер самосогласованный,
[40:53.880 --> 40:59.240]  когда функция, которая его порождает, самосогласована. Это раз. Плюс выполнено вот такое вот соотношение.
[40:59.240 --> 41:06.200]  Вот такое вот соотношение. Опять же, какое-то странное соотношение, оно нам понадобится. Вот.
[41:06.200 --> 41:12.800]  Даже не в таком виде, которым у нас записано у меня сейчас, но вот такое вот определение формальное дано.
[41:12.800 --> 41:18.960]  Будем работать. В частности, из этого определения сразу же следует, так как у вас вот эта часть
[41:18.960 --> 41:32.400]  не отрицательная, то у вас получается, что гисиан функции f большое будет положительно полуопределен
[41:32.400 --> 41:39.200]  по определению. Ровно это написано для любого вектора h. У вас эта квадратичная форма больше
[41:39.200 --> 41:45.320]  либо равна нулю. На самом деле, если технически покопаться, можно показать, что тут не просто не строгий
[41:45.320 --> 41:52.160]  знак, тут можно поставить строгий знак. Вот. То есть, гисиан будет просто положительно определен для
[41:52.160 --> 41:59.000]  самосогласованного барьера. Вот. Пример, опять же, барьера ровно с теми барьерами, с которыми часто и
[41:59.000 --> 42:07.120]  приходится иметь дело на практике. Логорифм от линейных ограничений. Единственное, что нужно
[42:07.120 --> 42:12.200]  дополнительно предположить, что эти линейные ограничения удовлетворяют условия слейтера. То есть,
[42:12.200 --> 42:20.160]  есть такая точка, где знак в ограничениях становится строгим. Помните условия слейтера, соответственно,
[42:20.160 --> 42:27.320]  что равенства выполняются, а неравенства, соответственно, должны в какой-то точке, в этой же,
[42:27.320 --> 42:33.880]  где выполняются равенства, принимать значение. Знак должен быть строгим. Понятно, это как раз вам
[42:33.880 --> 42:40.880]  гарантировать существование сильной двойственности и, соответственно, то, что у вас решение двойственной задачи
[42:40.880 --> 42:48.480]  там совпадает с решением исходной. Вот. Ну и здесь она тоже требуется, чтобы барьер, соответственно,
[42:48.480 --> 42:55.640]  был самосогласованным. Вроде бы как естественное свойство, которое, кажется, понятно, откуда вытекает.
[42:55.640 --> 43:03.640]  Окей. Давайте поехали рассматривать, что там придумали Нестеров с Немеровским. Во-первых,
[43:03.640 --> 43:08.280]  сразу же оговоримся, что это все работа для выпуклой оптимизации. Только для выпуклой
[43:08.280 --> 43:14.520]  оптимизации, когда у нас целевая функция f выпукла и все ограничения тоже выпуклы. Ну, в принципе,
[43:14.520 --> 43:22.560]  в таком сетинге мы с вами обычные работаем. Окей. А теперь я эту задачку чуть-чуть перепишу. Чуть-чуть
[43:22.560 --> 43:33.600]  перепишу. Вот в таком вот виде. Это так называемый вид эпиграф. Ну, короче, это эпиграф. Вот с этим
[43:33.600 --> 43:41.560]  множеством вы, скорее всего, знакомы, познакомились на семинарах. Это множество называется эпиграфом,
[43:41.560 --> 43:47.880]  для которых выполнено вот это ограничение. И что мы знаем про эпиграф, как он связан с выпуклостью
[43:47.880 --> 43:56.120]  функции? Да, эти вещи эквивалентны. Выпуклость эпиграфа и выпуклость исходной целевой функции,
[43:56.120 --> 44:02.520]  эти вещи эквивалентны. То есть, функция f выпукла только тогда, когда вот это множество задамаемое
[44:02.520 --> 44:10.560]  вот этим ограничением тоже будет выпукло. Ну и смотрите, что я делаю. Я выношу как целевую функцию
[44:10.560 --> 44:18.560]  просто переменную t. Ну а дальше что? Чтобы это ограничение у меня срабатывало, в лучшем случае t
[44:18.560 --> 44:25.680]  у меня должно быть равно f от x. Ну и понятно, что мне нужно уменьшать f от x. И у меня вместе с этим
[44:25.680 --> 44:34.640]  будет уменьшаться t. Вот такая вот задачка. Переформулировка исходной задачи выпуклооптимизации
[44:34.640 --> 44:40.120]  в виде эпиграфа. Чем она хороша? Почему мы говорим только про выпуклооптимизацию? То, что если бы целевая
[44:40.120 --> 44:46.480]  функция f у нас бы была не выпукла, то понятно, что с эпиграфом так бы не прокатило. И вот это ограничение
[44:46.480 --> 44:53.760]  у вас бы стало уже не выпуклым. Вот. И новая задача стала бы тоже не выпуклой. А так у вас выпуклая
[44:53.760 --> 44:59.920]  задача с выпуклыми ограничениями. Причем целевая функция является очень простой. Это просто, ну,
[44:59.920 --> 45:04.840]  линейная функция. Вы можете сюда добавить там единичку транспонированную. Размер это уже 1 там.
[45:04.840 --> 45:13.080]  1 умножить на транспонированную. Получается линейная функция. Вот. Поэтому в оставшейся части лекции мы
[45:13.080 --> 45:21.840]  будем с вами рассматривать задачи линейные только. Вот такого вот вида. Вот. Ну, в этом и соль как раз в том,
[45:21.840 --> 45:28.400]  что, во-первых, а выпуклые задачи к ним сводятся. Б, ну, сами по себе они тоже более чем популярны.
[45:28.400 --> 45:33.680]  Более чем популярны. Вот. Опять же на семинаре, когда вы разговаривали про разные классы задач,
[45:33.680 --> 45:43.640]  они возникали. Ограничения выпуклые. Ну, работаем с такой задачей. В общем виде метод внутренней точки,
[45:43.640 --> 45:50.440]  про который мы с вами говорили, как решать задачу с барьером. Это вы просто берете некоторые параметры
[45:50.440 --> 45:57.720]  row, отрешиваете вот эту задачу целевой функции f от row. Ну, и получаете какой-то выход. Дальше вы можете
[45:57.720 --> 46:06.600]  прекратить попытки по туге делать что дальше, что-то еще. А можете увеличить параметры row. Взять ту старую точку,
[46:06.600 --> 46:12.040]  которую вы получили до этого в качестве стартовой. Ну, и продолжить процесс. Снова оптимизировать вот эту
[46:12.040 --> 46:20.400]  функцию f от row. Теперь row уже будет новая. Будет больше. Ну, и соответственно получить новый выход. И уже решить,
[46:20.400 --> 46:27.160]  останавливается или нет. Это общая схема. Как это обычно работает. Ну, вот если у нас задача произвольная.
[46:27.160 --> 46:34.320]  Произвольная. Но в чем главная красота? Главная красота так это в том, что в линейном случае,
[46:34.320 --> 46:39.840]  в линейном случае, когда у нас еще и барьеры, с которыми мы работаем, new,
[46:39.840 --> 46:48.800]  самосогласованы, будет все довольно значительно приятнее. Метод будет прям точный, будет сказано,
[46:48.800 --> 46:55.000]  чем отрешивать, как увеличивать row и соответственно будут даже даны гарантии исходимости. Ну, и как мы
[46:55.000 --> 47:00.520]  видим дальше, чем меньше new параметр самосогласованности барьера, тем лучше, тем сходимость
[47:00.520 --> 47:06.040]  будет быстрее. Вот. Поэтому, когда определяется барьер, нужно в том числе на это обращать внимание.
[47:06.040 --> 47:14.920]  Вот. А метод будет довольно хитрый. Метод будет довольно хитрый. Во-первых, что нужно сделать? Нужно
[47:14.920 --> 47:23.760]  сделать, мы определяем расстояние до решения вот таким вот образом. Через вспомогательную функцию f.
[47:23.760 --> 47:29.600]  В принципе, вспомогательная функция f, она не особо сложная. Это наша функция f-row умноженная на row.
[47:29.600 --> 47:38.200]  То есть, просто row из барьера перенесена в линейную часть. А дальше определяется расстояние.
[47:38.200 --> 47:47.920]  Расстояние до... Как мы можем измерять в некотором смысле расстояние до решения? То есть, в принципе,
[47:47.920 --> 47:53.040]  то есть, если бы я вот эту убрал матрицу отсюда, что бы это просто было? Чего бы равнилась лямбда?
[47:53.040 --> 48:03.040]  Норма градиента вот этой функции, которую я по факту переделал чуть-чуть. Вот. Просто норма градиента.
[48:03.040 --> 48:10.320]  Но с матрицей... А что хорошо в этой матрице, сейчас мы поймем, чему равен гисиан функции f от row?
[48:10.320 --> 48:15.720]  Кто понимает? Если мы линейную функцию два раза продеференцируем, что-нибудь останется от нее?
[48:15.720 --> 48:21.840]  Ничего не останется. Получается, что гисиан просто равен гисиану f от x. А мы с вами только что
[48:21.840 --> 48:28.880]  обсудили, что самосогласованность барьера влечет то, что гисиан положительный. И по факту вот это
[48:28.880 --> 48:36.120]  порождает в некотором смысле ту норму, которую мы видели. Норма, которая порождается матрицей. То есть,
[48:36.120 --> 48:44.360]  вместо евклидовой нормы у вас будет норма вот такая вот. Норма, порожденная матрицей. Здесь то же
[48:44.360 --> 48:50.480]  самое. Как бы сходимость не по норме градиента, а по вот такой вот хитрой норме градиента, которая еще
[48:50.480 --> 48:56.960]  вот с матрицей. Матрица гисиану. Еще и обратного. Ну хорошо, сейчас увидим зачем это надо. На самом
[48:56.960 --> 49:06.400]  деле это все следует из сходимости метода Ньютона, демпфированного метода Ньютона для самосогласованной
[49:06.400 --> 49:13.440]  целевой функции. Вот что делаем в методе. Во-первых, первое, что нужно сделать, это правильно выбрать
[49:13.440 --> 49:24.440]  стартовую точку. Стартовая точка должна быть довольно близка к решению задачи. Можно реально
[49:24.440 --> 49:29.400]  заметить, что у нас вот этот критерий равняется нулю, когда соответственно градиент равен нулю,
[49:29.400 --> 49:33.800]  потому что матрица положительная определенная, а градиент равен нулю в решении. Получается,
[49:33.800 --> 49:45.760]  что мы должны выбрать точку x0 близко к решению исходной задачи с rho-1. С rho-1 у нас есть какая-то
[49:45.760 --> 49:52.480]  задача с барьером, ну или вот функция f, которая эту задачу с барьером отображает, где у нас есть
[49:52.480 --> 50:00.080]  исходное значение rho-1. Я говорю то, что я выбираю x0 так, что у меня решение этой задачи с барьером,
[50:00.080 --> 50:08.400]  и x0 близки точностью до epsilon, где epsilon меньше единицы. Как такую точку подобрать? Ну,
[50:08.400 --> 50:18.440]  например, запустить какой-то метод оптимизации для функции либо вот phi, либо функции f, и найти
[50:18.440 --> 50:28.440]  соответственно довольно хорошее приближение f-rho. Дальше что делаем? Взяли хорошую точку. Каждый раз,
[50:28.440 --> 50:35.120]  как я описывал, увеличиваем rho. Здесь оно увеличивается линейно, то есть есть единица
[50:35.120 --> 50:42.600]  плюс какой-то коэффициент e2, который положительный меньше единицы. Ну и вот тут все будет еще завязано
[50:42.600 --> 50:52.800]  на constant nu самосогласованности барьера. Вот rho увеличили. Получается наша задача f-rho поменялась,
[50:52.800 --> 51:01.960]  и f-rho поменялась. И оказывается, достаточно сделать всего один шаг демпфированного метода Ньютона,
[51:01.960 --> 51:14.200]  чтобы мы снова вернулись вот к этому. Только теперь уже для точки, для функции f-rho текущей,
[51:14.200 --> 51:22.080]  и точки текущие, то есть x1, rho1, ну и так далее. rho k, xk. Понятная идея, да? То есть что мы делаем?
[51:22.080 --> 51:27.880]  Мы увеличиваем rho, задача меняется. Вроде бы как та предыдущая точка x, которая у меня была,
[51:27.880 --> 51:33.920]  она, возможно, перестала быть хорошим решением новой задачи. Но один шаг вот такого метода Ньютона,
[51:33.920 --> 51:41.080]  а по факту это он и есть, вот гисиан, вот ингредиент, а это шаг. Демпфонирование это же есть просто шаг.
[51:41.080 --> 51:50.840]  Ой, да ладно, короче, вот это слово. Вот, по-английски просто проще там дамп называется,
[51:50.840 --> 51:58.440]  его проще произносить, а по-русски. Вот шаг гисиан обратный, норма градиента. Делая шаг вот
[51:58.440 --> 52:04.160]  такого метода Ньютона, одного шага может быть достаточно при правильном подборе e1 и e2,
[52:04.160 --> 52:11.760]  чтобы вернуться вот к этому условию, что у меня текущая точка, в которой я работаю, будет хорошо,
[52:11.760 --> 52:22.680]  ну с точки зрения именно близости к решению, к реальному решению задачи с текущим rho. Понятная
[52:22.680 --> 52:28.520]  идея, да? Сейчас мы это докажем. Вот, что вот этот шаг валиден и что за один шаг метода Ньютона,
[52:28.520 --> 52:34.840]  вот, мы действительно возвращаемся к правильному, находим правильную точку xk t f плюс 11,
[52:34.840 --> 52:42.600]  что условие все еще выполняется. Вот, ну это то, что я описал. Берем довольно близко точку x
[52:42.600 --> 52:50.640]  стартовую и, соответственно, которая близка к x звездой rho, к решению задачи с барьером. Дальше,
[52:50.640 --> 52:58.200]  соответственно, критерии мы это тоже обсудили, что это близко к норме градиента, бла-бла-бла. Вот,
[52:58.200 --> 53:06.480]  далее мы увеличиваем rho и за счет шага однимфонированного метода Ньютона мы снова
[53:06.480 --> 53:14.040]  гарантируем, что новый x будет близок к решению уже новой задачи с барьером. Вот, окей, давайте это
[53:14.040 --> 53:23.080]  доказывать. Ведем сначала обозначение матрица H. Матрица H, это, соответственно, у нас просто
[53:23.080 --> 53:30.040]  гессиан нашей функции, вот этой f rho. Но, как мы поняли, это просто гессиан нашего штрафа,
[53:30.040 --> 53:34.960]  потому что линейная часть на него не влияет. Плюс, соответственно, тут аннотация, которую мы опять же
[53:34.960 --> 53:42.440]  обсудили, это x транспонированное ax. Ну и здесь тоже тот факт, который я вам сказал, что из
[53:42.440 --> 53:48.240]  определения самосогласованного барьера можно вытащить то, что он будет, вот, гессиан будет
[53:48.240 --> 53:54.600]  положительно определён. Положительно определён, поэтому те нормы, которые мы определяем, эти там
[53:54.600 --> 54:02.960]  обратные матрицы, действия, которые мы будем делать, они будут все валидными. Так, ну что нам
[54:02.960 --> 54:08.240]  нужно сделать? Нам, по факту, нужно оценить, а вот в новых обозначениях мы ведем, как выглядит
[54:08.240 --> 54:18.040]  функция λ. Вот, ну просто по тому, как мы ее ввели, это что это? Градиент умножить на, соответственно,
[54:18.040 --> 54:26.640]  градиент-градиент, а тут гессиан обратный. Вот, ну вот ровно это и есть, соответственно,
[54:26.640 --> 54:33.320]  норма, а еще там корень был. Вот, ну и вот здесь это и записано. Гессиан, вот он обратный гессиан,
[54:33.320 --> 54:39.920]  через вот такое вот обозначение. Гессиан встает сюда обратный, здесь встают нормы градиента, вот,
[54:39.920 --> 54:49.600]  и берем из них корень. Все, просто переобозначили. Дальше я выписываю градиент функции f. Функции f
[54:49.600 --> 54:58.320]  это ρс плюс градиент моего барьера, градиент моего барьера. Ну что мне нужно сделать? Мне нужно
[54:58.320 --> 55:08.160]  сделать, во-первых, хочется понять, как меняется... Изначально, когда я захожу в цикл, у меня
[55:08.160 --> 55:19.600]  гарантировано то, что у меня f ρ к-1 xk меньше, чем ε1. Так я инициализирую, и так я хочу по индукции
[55:19.600 --> 55:24.160]  показать, что у меня вот это всегда будет выполняться. Предполагаю, что вот это выполнено изначально
[55:24.160 --> 55:30.640]  в цикле по инициализации, как минимум. А дальше я хочу понять, как увеличение ρ мне все это испортит.
[55:30.640 --> 55:39.560]  То есть я меняю ρ, а точку не меняю. Вот, вопрос, как вот это я могу оценить? Ну вот сейчас этим мы
[55:39.560 --> 55:47.080]  будем заниматься. Сейчас вот этим мы будем заниматься. Да, вот оно расстояние, я его выписал.
[55:47.080 --> 55:53.760]  Дальше что происходит? Дальше в силу того, что это у меня некоторая норма, а для нормы я
[55:53.760 --> 56:01.920]  могу использовать неравенство треугольника. Поэтому сюда я закидываю умный ноль, вычитаю ρ и
[56:01.920 --> 56:12.920]  добавляю ρ к-1 на c, на c плюс градиент. Дальше неравенство треугольника, здесь оставляю ρ к-1,
[56:12.920 --> 56:19.440]  здесь оставляю разницу рошек, здесь оставляю разницу рошек xc. Что мы можем сказать про вот это
[56:19.440 --> 56:27.160]  выражение, чему оно эквивалентно? С точки зрения обозначений, ну-ка смотрим на обозначение.
[56:27.160 --> 56:40.680]  Все правильно, лямбда и ρ к-1 xc. Вот про это, вот про это я узнал, что это меньше, чем epsilon 1.
[56:40.680 --> 56:46.680]  Так мы зашли в цикл, в текущую эту рацию. Но видите, у меня появилась какая-то добавочка,
[56:46.680 --> 56:56.480]  неприятная добавка, с которой нужно будет поработать. Так, ладно, поработаем здесь. Что я
[56:56.480 --> 57:06.320]  сделаю? Я сделаю следующее, ρ к-ρ к-1 вынесу за пределы нормы, разделю еще на ρ к. Здесь,
[57:06.320 --> 57:14.840]  соответственно, ρ к я внесу в норму и на c умножу, h-1 xc. Что я знаю про вот это выражение,
[57:14.840 --> 57:29.960]  если я подставлю ρ к сюда, что я про него знаю, как это меняется. Там же разница всего лишь
[57:29.960 --> 57:40.680]  между ними. Эпсилон 2 я вводил в корень из этого. ρ к-1 есть ρ к. Поэтому если я возьму просто
[57:40.680 --> 57:52.600]  разность, у меня останется что-то в духе epsilon 2 ню, делить на ρ к-1. Ро к-1. Вот получается,
[57:52.600 --> 58:02.040]  что вот это равно просто вот этому мнению. Согласны? Что-то тут чуть-чуть... Да-да-да-да. Ну давайте
[58:02.040 --> 58:12.440]  сделаем вот так. Я, может, тогда подпортачил, чтобы... Вот так. Сейчас как бы... Вот так вот.
[58:12.440 --> 58:23.960]  Сейчас. Рока тогда будет у меня... А, вот тут минус первое. То есть ρ к-1 делить на вот эту скобку.
[58:23.960 --> 58:36.440]  Так. Это равно рука. Вот. И тогда, когда я вычту, что у меня будет? Тут останется 1 минус
[58:36.440 --> 58:45.160]  отношение рука минус 1. Делить на ρ? А? Не-не-не. Я чуть-чуть поменял просто. Чуть-чуть поменял,
[58:45.160 --> 59:00.080]  чтобы у меня красивше вылезло. Почему? Сейчас. Сейчас. Сейчас. Рука. Тогда нет,
[59:00.080 --> 59:03.840]  рука минус 1, тогда будет больше. Вот, короче, вот такое вот выражение, кажется, нужно взять.
[59:03.840 --> 59:10.280]  Рока минус 1 делить на... А, нет, делить, тогда она станет... Господи, сейчас.
[59:10.280 --> 59:26.640]  Вот. Рок... А, с минусом, что ли, тогда нужно взять сейчас. Рока 1 минус эпсилон 2 корень. Вот так.
[59:26.640 --> 59:37.080]  Вот. Тогда у меня ρ к-1 будет меньше. Меньше, потому что я беру что-то положительное. Рука умножена
[59:37.080 --> 59:45.120]  что-то меньше единицы, тогда ρ к-1 будет меньше. Ну, это пойдет, это пойдет. То есть там мы увидим,
[59:45.120 --> 59:53.720]  что главное, что просто ρ увеличивалась линейно. Вот. Сейчас главное подсобрать тут вот это. Так.
[59:53.720 --> 01:00:09.520]  Тогда что здесь будет? ρ к-1 делить на ρ к, и это будет 1, 1, минус, это 2, ню, ню, 1,
[01:00:09.520 --> 01:00:18.400]  минус вот так вот. И тогда это будет е2 корень из ню. Вот. В алгоритме этот тогда чуть-чуть поменяем.
[01:00:18.400 --> 01:00:28.280]  Давайте вот вернемся на алгоритм, и я вот здесь поставлю минус. Минус, и здесь минус 1, окей? Разницы
[01:00:28.280 --> 01:00:33.920]  не будут. Сейчас увидите, что там главное, что просто линейно его менять. Вот. Потому что это вещи,
[01:00:33.920 --> 01:00:40.720]  на самом деле, близкие. Вот. В это выражение я ровно и получил. А, ну, здесь, кстати, я
[01:00:40.720 --> 01:00:46.840]  ρ к-1 вытащил. Окей. Ладно, не страшно. Что-то я даже перемудрил. То есть можно было оставить,
[01:00:46.840 --> 01:00:57.580]  как было. Вот. Так. Нужно оценить ρ к-1 умножить на С по матрице H. По матрице H вот этой нормой. Ну,
[01:00:57.580 --> 01:01:03.880]  давайте хорошо оценим. Оно, благо, у меня вот здесь вот вылезает где-то. Оно вылезает обычно в лямде,
[01:01:03.880 --> 01:01:18.180]  вылезает. Давайте глянем, чему равно вот эта лямда ХК. Это как раз ρ к-1 С плюс F от ХК и на
[01:01:18.180 --> 01:01:27.680]  матрицу H-1 ХК. Так. Что я могу про это сказать? Я могу сказать, что на входе алгоритм это у меня
[01:01:27.680 --> 01:01:36.320]  было меньше, чем ε1. Так. А теперь я стираю левую часть. Я про нее нужно записал, что это меньше,
[01:01:36.320 --> 01:01:42.800]  чем ε1. И хочу воспользоваться неравенством треугольника. Опять же, у меня норма. Сумма
[01:01:42.800 --> 01:01:51.320]  норм. Как ее снизу оценить с помощью неравенства треугольника? Что тогда там будет? Модуль разности,
[01:01:51.320 --> 01:01:58.000]  все правильно. Но я даже тут не с модулем возьму, я модуль еще и раскрою. Потому что число меньше,
[01:01:58.000 --> 01:02:04.200]  под модульное выражение просто меньше либо равно, чем модуль. Я вот так вот его раскрою. Так,
[01:02:04.200 --> 01:02:25.640]  здесь у меня будет H-1 ХК. Вот. Здесь, соответственно, минус градиент, градиент H-1. Вот. Окей. Ну и
[01:02:25.640 --> 01:02:44.440]  тогда я получу, что РО К-1 С H-1 меньше либо равно, чем Е1 плюс градиент F от Х H-1. Вот. То есть,
[01:02:44.440 --> 01:02:50.920]  чтобы оценить теперь вот это выражение, вот это у меня уже есть, осталось только оценить норму
[01:02:50.920 --> 01:02:59.520]  градиента по норме H-1. Вот. И для вот этого всего безобразия мне как раз понадобится свойство
[01:02:59.520 --> 01:03:08.000]  барьера, которое я записал. H транспонированная, норма градиента, норма градиента по модуле
[01:03:08.000 --> 01:03:20.440]  меньше либо равно, чем ню, ню корень, H транспонированная, набло F Х H. Вот. Это у меня
[01:03:20.440 --> 01:03:28.840]  выполнено для любого H. В частности, будет выполнено для вектора H, который равен H-1,
[01:03:28.840 --> 01:03:34.680]  градиент F от Х. Так. Раз выполнен для любого H, я могу подставить вот такой вот H,
[01:03:34.680 --> 01:03:44.160]  какой я захотел. Ну и тогда, что у меня будет получаться? H транспонированная. Так.
[01:03:44.160 --> 01:03:51.400]  Когда H транспонированная, подставляю, у меня что будет? Набло F Х транспонированная здесь,
[01:03:51.400 --> 01:04:00.040]  H минус транспонированная, градиент Х по модулю меньше либо равно, чем ню. И когда здесь,
[01:04:00.040 --> 01:04:08.040]  начну сейчас H подставлять, у меня снова вылезет градиент транспонированной. H минус
[01:04:08.040 --> 01:04:14.480]  транспонированная. Одна из H сократится с гессианом, потому что это тоже гессиан.
[01:04:14.480 --> 01:04:23.360]  H-1 сократится с гессианом, у меня останется одна матрица, и здесь будет еще один градиент. Так.
[01:04:23.360 --> 01:04:28.800]  Дальше, что я сделаю? В силу того, что у меня гессиан, это вещь симметричная,
[01:04:28.800 --> 01:04:38.160]  я транспонирование здесь сниму. Окей? Вот. Ну и тогда, что получается? Вот здесь у меня
[01:04:38.160 --> 01:04:48.480]  записано одно выражение, а здесь записан его корень. Согласны? Да? Всё. Тогда у меня,
[01:04:48.480 --> 01:04:56.800]  я делю на этот корень, и получается, что корень вот этот, градиента, H-1 градиент, будет меньше,
[01:04:56.800 --> 01:05:07.440]  чем корень из ню. Вот. А это же и есть вот это выражение. Норма по матрице. Корень, то есть берем
[01:05:07.440 --> 01:05:13.600]  матрицу, ставим, домножаем на векторы, транспонирован, только забыл, и берем из этого корень. Получается,
[01:05:13.600 --> 01:05:23.280]  что вот это у меня будет меньше, чем корень из ню. Окей? Ну вот такое вот странное свойство,
[01:05:23.280 --> 01:05:30.000]  которое мы изначально ввели, оно вот здесь выстреливает. Вот. Чтобы оценить просто вот такую норму
[01:05:30.000 --> 01:05:39.000]  матрицы. Норма градиента по матрице. Вот. Необычное свойство. Понятно, что его нужно отдельно
[01:05:39.000 --> 01:05:45.600]  проверять для каждого из барьеров, но вот оно немного искусственное. Ну и вот оно здесь вот выстреливает.
[01:05:45.600 --> 01:05:54.400]  Выстреливает. Поэтому мы можем оценить рожки, сюда это подставить, и здесь будет что-то в духе E1
[01:05:54.400 --> 01:06:04.800]  плюс E2 делить на ню. Здесь ещё будет E1 плюс корень из ню. Окей? Всё. Оценили вроде как у меня
[01:06:04.800 --> 01:06:13.480]  поменялось расстояние. Вот. Когда я поменял ро. С ро минус первого до ро катого. Вот. Оценил,
[01:06:13.480 --> 01:06:22.920]  оценил. Тан-тан-тан. Тан-тан. Вот. Хорошо. Хорошо. А теперь нужно применить факт,
[01:06:22.920 --> 01:06:29.680]  который мы доказывать не будем. Это просто сходимость демпфонированного метода ньютона для
[01:06:29.680 --> 01:06:38.080]  самосогласованной функции. Оказывается справедливо вот такая вот теорема. Ну вот,
[01:06:38.080 --> 01:06:46.560]  что вот так вот будет сходиться метод ньютона, когда у нас функция f самосогласована. Согласована.
[01:06:46.560 --> 01:06:50.440]  Единственное, что нужно отметить, что у нас действительно функция f является самосогласована,
[01:06:50.440 --> 01:06:57.880]  потому что барьер самосогласованный, так? И у нас ещё к барьеру добавляется линейная функция. А мы
[01:06:57.880 --> 01:07:03.640]  поняли, что там квадратичные, а значит и линейная функция. У нас будут самосогласованными. Ну и сумма
[01:07:03.640 --> 01:07:11.360]  двух самосогласованных функций, это вещь самосогласованная. Вот. Если коэффициенты там больше единицы. Вот.
[01:07:11.360 --> 01:07:18.160]  Ну всё, значит у меня f самосогласована функция, если я ро изначально задал больше единицы. Вот.
[01:07:18.160 --> 01:07:24.120]  Получается, что я могу применять результаты сходимости метода ньютона для этой задачки. Вот. И они
[01:07:24.120 --> 01:07:30.480]  будут вот такими. Вот такими. Хотелось бы, то есть мы же что хотели, после применения метода ньютона,
[01:07:30.480 --> 01:07:36.720]  хотим добиться, чтобы вот это было меньше, чем epsilon1. Один шаг сделали и сразу же вернулись в нужную
[01:07:36.720 --> 01:07:44.360]  окрестность. Только уже решение новой функции. И оказывается, так и будет, если правильно
[01:07:44.360 --> 01:07:52.040]  подобрать e1 и e2. Потому что для альфа, для лямбда мы уже выражение знаем. Понятно, что если взять
[01:07:52.040 --> 01:07:58.280]  e1 и e2 довольно маленькими, у вас этот квадрат будет кушать всё, а здесь 1 минус что-то маленькое,
[01:07:58.280 --> 01:08:04.520]  оно будет там что-то хорошее. Вот. А квадрат там всё съест. В частности, если взять e1 там 0,5,
[01:08:04.520 --> 01:08:14.420]  e2 0,8, получится, что новое расстояние до решения новой функции будет меньше, чем e1. Вот. Это просто
[01:08:14.420 --> 01:08:21.000]  технический подбор вот этого безобразия, учитывая ещё то, что nu больше ещё единицы. Вот. Тут не особо
[01:08:21.000 --> 01:08:27.160]  что-то такое секретное. Вот. Просто нужно аккуратненько это всё подобрать. Понятная идея, почему так
[01:08:27.160 --> 01:08:31.680]  получается. То есть по индукции можно доказать, что изначально мы задали расстояние до решения,
[01:08:31.680 --> 01:08:37.720]  потом поменяли row, поменяли задачу, сделали всего один шаг метода и сразу же получили хорошее
[01:08:37.720 --> 01:08:44.440]  решение для новой задачи за один шаг метода ньютона. Классная идея, красивая идея. Но это
[01:08:44.440 --> 01:08:49.640]  ещё не всё. Это ещё не всё, потому что мы вроде как поняли, что да, у нас row растёт,
[01:08:49.640 --> 01:08:56.920]  row растёт, и это вроде как хорошо. И за одну итерацию метода ньютона мы сразу же получаем решение.
[01:08:56.920 --> 01:09:03.040]  Окей, но всё равно непонятно, а как мы в итоге приближаемся к решению исходной задачи. Сколько
[01:09:03.040 --> 01:09:08.160]  итерации такого алгоритма нужно сделать? То, что итерация рабочая, мы поняли. Вот. А то,
[01:09:08.160 --> 01:09:12.760]  что сколько их нужно сделать, ну давайте это поймём сейчас. Опять же, пак довольно
[01:09:12.760 --> 01:09:21.060]  сложный, поэтому я его покажу, как у Аркадия Семёныча Немировского в его лекциях. Он
[01:09:21.060 --> 01:09:26.800]  показывает только на примерчике алгоритмических барьеров, что это всё работает, а для всех
[01:09:26.800 --> 01:09:35.800]  остальных, ну там смотрите, книжку тысяча страниц. Вот. Упрощаю задачу, говорю, что пусть у меня метод
[01:09:35.800 --> 01:09:42.640]  не просто находит какое-то близкое решение вот этой задачи с барьером. Пусть у меня вот на каждой
[01:09:42.640 --> 01:09:48.400]  итерации мне доступно чёткое решение. Я могу находить каждый раз прям чёткое решение задачи с
[01:09:48.400 --> 01:09:57.160]  барьером. Вот. Ну и, соответственно, барьеры у меня не простые, а алгоритмические. Вот. Теперь,
[01:09:57.160 --> 01:10:01.680]  соответственно, возникает вопрос, как быстро я приближаюсь к решению. Тут всё довольно будет
[01:10:01.680 --> 01:10:08.400]  простенько. Так как у меня x это решение, то что я могу сказать про градиент вот такой вот функции?
[01:10:08.400 --> 01:10:19.800]  Точки x. Почему он равен? Нулю. Просто потому что это же как раз x у меня решение вот этой
[01:10:19.800 --> 01:10:30.280]  задачи. Он равен нулю. Давайте я выпишу этот градиент. Чё там у нас было? Ро, С и, соответственно,
[01:10:30.280 --> 01:10:38.760]  от логарифма. Старый совсем стал. Давайте я представлю просто. Вот. Ро, С плюс вот так вот. Градиент
[01:10:38.760 --> 01:10:47.280]  равен нулю. Вот. Так. Ну вынесли вектор А, а дальше логариф продиференцировали. Так. Получилось что-то
[01:10:47.280 --> 01:10:52.880]  вот такое. Дальше я вот это всё безобразие скалярно домножаю на вектор. Ну это же у меня вектор
[01:10:52.880 --> 01:11:01.560]  получился. Который равен нулевому вектору. Я домножаю на x минус x со звездой. Вот. Когда я
[01:11:01.560 --> 01:11:11.280]  домножу, у меня получится Ро, С транспонированное x минус Ро, С транспонированное x со звездой равно
[01:11:11.280 --> 01:11:23.640]  равно a it транспонированная сумма тут. А здесь x со звездой минус x делить на b it a it транспонированная
[01:11:23.640 --> 01:11:31.240]  x. Вот. Что я могу сказать про a it транспонированная на x со звездой? x со звездой это решение исходной
[01:11:31.240 --> 01:11:40.680]  задачи. Что я могу сказать про a it транспонированная x со звездой? b it. Потому что это же ограничение.
[01:11:40.680 --> 01:11:49.000]  Вот у меня было ограничение b it равно a it x. Так. Вот. Поэтому в решении оно должно быть как бы удовлетворять
[01:11:49.000 --> 01:11:56.720]  ограничением. Вот это будет просто b it. Поэтому вот здесь вот я запишу следующее. b it минус a it
[01:11:56.720 --> 01:12:06.360]  транспонированная x. b it минус a it транспонированная x. Итого это равно просто m. Ну, drop
[01:12:06.360 --> 01:12:13.160]  равна 1. Суммирую от 1 до m, получаю просто m. Вот. Оказывается, для логарифмического барьера вот
[01:12:13.160 --> 01:12:23.080]  такого вида m еще и равна i µ. Вот. Константия вот этой самосогласованности. Вот. Откуда я
[01:12:23.080 --> 01:12:33.960]  получаю, что c и транспонированная x минус c транспонированная x со звездой равно µ делить на
[01:12:33.960 --> 01:12:42.040]  ρ. µ делить на ρ. Вот. А что такое транспонированная x? Это же просто у меня f от x. Так. Потому что у меня
[01:12:42.040 --> 01:12:49.800]  целевая функция линейная. А это f от x со звездой. Вот. И получается, что у меня расстояние между f от
[01:12:49.800 --> 01:12:57.720]  x и f от x со звездой на текущей итерации. Это просто µ делить на ρ. µ делить на ρ. Вот. Но это при условии,
[01:12:57.720 --> 01:13:03.760]  что я нахожу точное решение каждый раз. Что у меня x это точное решение. Опять же обрабатывать
[01:13:03.760 --> 01:13:10.440]  случаи, когда решение не точное, как у нас, что оно там с какой-то точностью близко. Ну, технические
[01:13:10.440 --> 01:13:17.320]  вещи, которые, ну, пропускаем. Просто вы достоверились, что у меня вот так вот меняется. Но здесь что видно?
[01:13:17.320 --> 01:13:23.800]  Я же ρ теперь начинаю уменьшать с геометрической прогрессией. Я начинаю少 reign делать меньше,
[01:13:23.800 --> 01:13:28.380]  меньше, меньше, меньше, меньше. Вот. Оно у меня падает со скор Control... увеличивается со
[01:13:28.380 --> 01:13:33.760]  скоростью геометрической прогрессии, а значит расстояние до решения уменьшается со
[01:13:33.760 --> 01:13:42.760]  скоростью геометрической прогрессии. Так. Всё. Это то, что нужно было. Вот. Получается, что.. это мы выразили..
[01:13:42.760 --> 01:13:51.160]  получается итоговое решение так как у меня коэффициент геометрической прогрессии это что-то
[01:13:51.160 --> 01:14:00.280]  в порядке там 0.8 делить на корень изню вот то мне нужно просто прикинуть за сколько там
[01:14:00.280 --> 01:14:08.040]  итераций вот это у меня там к-1 будет равно епсилон ну и беру логарифмирую и получаю
[01:14:08.720 --> 01:14:17.140]  вот он влезает параметр самосогласованности барьеров вот 0.8 понятно я просто вот ку
[01:14:17.140 --> 01:14:22.840]  засуду ну и логариф мы быть то есть получается линейная сходимость к решению вот
[01:14:22.840 --> 01:14:29.040]  и видно что параметр самосогласованности писался вот параметр самосогласованности
[01:14:29.040 --> 01:14:36.060]  играет ключевую роль чем больше параметр самосогласованности тем ус Canyon метод сходится
[01:14:36.060 --> 01:14:39.900]  медленнее, но скорость при этом все равно линейная. Единственное, конечно, расплата
[01:14:39.900 --> 01:14:44.820]  за это — это использование метода Ньютона на каждой итерации.
[01:14:44.820 --> 01:14:48.900]  Но при этом все равно классная скорость по сравнению с тем, что мы
[01:14:48.900 --> 01:14:52.660]  получили на первой части лекции. Тут прям классные гарантии для любой задачи
[01:14:52.660 --> 01:14:58.020]  выпуклой оптимизации, линейная скорость сходимости, но за счет того, что вы когда-то
[01:14:58.020 --> 01:15:03.260]  считаете десианы. Все, спасибо большое. Я надеюсь, что вот этот классный результат
[01:15:03.260 --> 01:15:08.140]  понравился, потому что, ну, вот это, наверное, такой труд, на котором Аркадий
[01:15:08.140 --> 01:15:15.500]  Семенович и Юрий Евгеньевич сделали себе имя и карьеру, в том числе, потому что, ну,
[01:15:15.500 --> 01:15:22.620]  нестеровский метод, тот же, который про ускорение, ну, он долгое время был непонят
[01:15:22.620 --> 01:15:26.980]  комьюнити, просто потому что, ну, как-то популярностью не пользовался, а потом,
[01:15:26.980 --> 01:15:31.420]  когда это выстрелило в нейронных сетях, сразу стал популярен. А вот это, соответственно,
[01:15:31.540 --> 01:15:36.340]  применяется в куче разных бизнесовых задач от экономики до распределения ресурсов и так далее,
[01:15:36.340 --> 01:15:40.820]  просто потому что задача с ограничениями — это же ровно ресурсы. Ограничение на ресурсы,
[01:15:40.820 --> 01:15:47.660]  какая-то целевая функция, это ваш, это прибыль, вот. И сразу же в селверах этот метод стал суперклассно
[01:15:47.660 --> 01:15:51.420]  работать, просто потому что, чтобы решать эти бизнесовые задачи, как распределить ресурсы,
[01:15:51.420 --> 01:15:56.620]  как распределить деньги, как распределить время, чтобы, соответственно, заработать как можно больше.
[01:15:56.620 --> 01:16:02.420]  Вот. Ну и поэтому вот этот метод, наверное, такая вот прям одна из коронных фишечек Аркадий
[01:16:02.420 --> 01:16:08.180]  Семеновича и Юрия Евгеньевича. Вот. И этот результат прям, наверное, ключевой. Вот. Всё. Спасибо большое.

[00:00.000 --> 00:07.760]  Я еще раз повторю, что мы сейчас поговорим про методы
[00:07.760 --> 00:10.680]  первого порядка, то есть про методы, которые используют
[00:10.680 --> 00:16.080]  только информацию о градиенте, вот, и поймем, что будет
[00:16.080 --> 00:18.640]  происходить, когда этот градиент доступен только
[00:18.640 --> 00:19.640]  приближенным.
[00:19.640 --> 00:20.640]  Что значит приближенным?
[00:20.640 --> 00:22.880]  То есть мы будем посмотреть, что называется, заглянем
[00:22.880 --> 00:27.120]  в черный ящик, и вот таких вот задач перейдем к задачам,
[00:27.120 --> 00:29.440]  где у нас целевая функция известной структуры, то
[00:29.600 --> 00:33.280]  есть х у нас будет лежать по-прежнему в рэн, где n маленькая,
[00:33.280 --> 00:35.720]  вот, а целевая функция будет представляться в виде
[00:35.720 --> 00:42.400]  суммы большого числа функций, n большое, вот, и, значит,
[00:42.400 --> 00:44.720]  из этого, не знаю, вот эту вот структуру замечательную
[00:44.720 --> 00:49.880]  мы сможем получить методы, которые будут работать
[00:49.880 --> 00:52.880]  быстрее по времени, хотя с практической оценкой
[00:52.880 --> 00:55.840]  сходимости будут некоторые соответствующие проблемы,
[00:55.840 --> 00:58.760]  за более быструю сходимость по времени надо будет что-то
[00:58.840 --> 01:02.640]  заплатить, вот, значит, вот эта штука, давайте f большой
[01:02.640 --> 01:06.240]  обозначим что-ли, вот, можно ее понятным образом, я надеюсь,
[01:06.240 --> 01:08.600]  представить, что это на самом деле мы от ожидания
[01:08.600 --> 01:15.760]  от некоторые функции там fхxy, xxy, где xxy это там равномерная
[01:15.760 --> 01:22.000]  юниформ, вот, да, равномерная, ну, в общем, сейчас, мегтры
[01:22.000 --> 01:27.960]  p и p там, k равно i единиц на n, вот, ну, то есть мы сэмплируем,
[01:27.960 --> 01:33.400]  равномерное распределение и оцениваем от ожидания как среднее по нашим эффектам.
[01:33.400 --> 01:37.600]  Вот. Понятно ли переход к отожаданию? Хорошо. Если будет что-то непонятно,
[01:37.600 --> 01:43.200]  ставьте минус в чате или говорите в микрофон, что что-то непонятно. Вот. Смотрите, значит,
[01:43.200 --> 01:48.360]  что это нам дает. Это нам дает возможность, собственно, попытаться вместо градиента
[01:48.360 --> 01:54.400]  точного, который будет равен, ну понятно чему. Вот. Используй градиент неточный. Вот. А именно
[01:54.400 --> 02:03.840]  градиент, который будет равен равен fх просто какой-то, например, ик от x. То есть ик лежит у
[02:03.840 --> 02:09.040]  нас в множестве от 1 до n. Вот. И мы можем, например, в качестве оценки на градиент взять просто
[02:09.040 --> 02:16.560]  какую-то слагаемую. Вот. Ну и важная штуковина и важным свойством этой оценки будет являться
[02:16.560 --> 02:22.160]  то, что им от ожидания по… Ну давайте я вот так перенапишу сейчас, что все-таки эта штука
[02:22.160 --> 02:28.040]  немножко случайная, потому что мы ик атэ сэмплируем, ик атэ из… Ну, в общем, из этого
[02:28.040 --> 02:33.120]  распределения, но вероятность того, что ик атэ равняется и, соответственно, единиц на n. Вот. И мы
[02:33.120 --> 02:39.320]  случайным образом просэмплировали некоторый индекс, потом взяли нужное нам слагаемое вот отсюда. Вот.
[02:39.320 --> 02:44.080]  И потом сказали, что окей, теперь это наш градиент. И дальше мы будем его использовать вместо градиента,
[02:44.080 --> 02:53.040]  который у нас был всегда, и получим так знаменитый… А что у нас за нижний индекс? Ну, кси обозначает то,
[02:53.040 --> 02:59.080]  что у нас… Пам-пам-пам. Наверное, правильно так записать. Сейчас, как всегда,
[02:59.080 --> 03:06.240]  какая-то путаница в обозначениях. Ну, это ничего страшного. Вот так. Ну, то есть вот этот вот нижний
[03:06.240 --> 03:10.360]  индекс, он нужен для того, чтобы обосновать, откуда здесь ик атэ берется. Ик атэ берет сэмплированием
[03:10.360 --> 03:15.520]  случайной величины кси с равной вероятностями. А лучше ли сейчас? А, окей, гуд. Ну, вот.
[03:15.520 --> 03:19.240]  Соответственно, мы получаем классический градиентный метод, который обновляет нам
[03:19.240 --> 03:26.800]  наш иксы по некоторой случайной оценке на градиент. Вот важно, что у нас не смещенные. Вот. К сожалению,
[03:26.800 --> 03:34.800]  ну, понятно, что это все так-не так. Вот это быстро посчитать. Вот. Ну, то есть мы считаем градиенты
[03:34.800 --> 03:39.400]  от одной функции вместо того, чтобы считать их от N. Вот. То есть мы в N раз быстрее по просто
[03:39.400 --> 03:45.360]  количеству операций. И давайте теперь посмотрим на то, каким последствием это приводит. Вот. А именно,
[03:45.360 --> 03:51.240]  то изменится, как вы думаете, в траектории сходимости по сравнению с тем, что у нас было просто, то есть,
[03:51.240 --> 03:55.600]  вот давайте сравним с хостичкой основой градиента и точной основой градиента, что изменится,
[03:55.600 --> 04:00.120]  какие будут наблюдения. Что можно наблюдать и будет ли это хорошо или плохо, и как с этим бороться,
[04:00.120 --> 04:05.400]  если это будет плохо. Пожалуйста, напишите ваши идеи в чате. И монотонно. Да, это правда. Еще. Вот
[04:05.400 --> 04:09.360]  эта не монотонность, она с чем связана? Да, вот это на самом деле интересный вопрос, что делаем
[04:09.360 --> 04:15.080]  больше итерации. Вот. Ну, вот. Он в части связан с тем, что с моим предыдущим вопросом, что вот с чем
[04:15.080 --> 04:21.160]  связана не монотонность, что типа делаем больше итерации, а что изменится ли то те результаты
[04:21.160 --> 04:26.400]  сходимости, которые мы до этого получали. Если изменится, то в какую сторону и в чем, собственно,
[04:26.400 --> 04:33.360]  называется первая причина таких изменений? Ну, вижу, что пока идеи особо нет, но первая причина изменений в том,
[04:33.360 --> 04:39.800]  что вот здесь появляется дисперсия, которая у нас не была в случае градиентного метода. Вот. И вот
[04:39.800 --> 04:45.200]  с этой дисперсией она будет нам очень много проблем создавать при честном анализе сходимости. Вот.
[04:45.200 --> 04:53.120]  То есть, здесь у нас была скорость единицы на к для L-гладкой выпуклой функции. Вот. И это была,
[04:53.120 --> 04:59.640]  это сходимость, она была, если я правильно помню, типа fkt минус f со звездочкой. Можно вот так писать,
[04:59.640 --> 05:06.120]  просто писать. Вот. То есть, там был учет меньше, прошлый раз, вроде, доказывали, учет 2L на альфа к,
[05:06.120 --> 05:11.580]  и тут еще x со звездочкой минус x0. Может быть, помните. Если не помните, посмотрите скрипт прошлой
[05:11.580 --> 05:16.480]  лекции, там это было более подробно. Теперь важный вопрос. Что будет здесь для этого же класса
[05:16.480 --> 05:21.320]  функции? Ну, давайте сейчас вот, как вы думаете, что будет? Какая-то интуиция. Проверим вашу интуицию.
[05:21.320 --> 05:26.760]  Что мы пронаблюдаем? Может быть, сублинейную сходимость? Ну, она и так сублинейная и в градиентном
[05:26.760 --> 05:30.720]  спуске. А какой порядок-то будет? И будет ли вообще сходимость, что более важно? Ну, не раз
[05:30.720 --> 05:35.720]  вы так спрашиваете, то, видимо, не будет. Ну, а как бы. Действительно, будут некоторые проблемы.
[05:35.720 --> 05:39.960]  Сейчас мы, собственно, теоремы, которые я попытаюсь сейчас быстренько, ну, вот, может быть,
[05:39.960 --> 05:46.720]  относительно быстренько доказать. Вот. Будет нам говорить о том, что, если у нас есть fL-гладкая
[05:46.720 --> 05:52.080]  выпуклая, то есть, довольно сильные условия, по сравнению с тем, как этот метод применяется более
[05:52.080 --> 06:00.680]  или менее в реальной жизни. fL-гладкая выпуклая, значит, направление сэмплируется из, ну, понятно,
[06:00.680 --> 06:06.360]  какого распределения. Вот. И... Чего? И будем обозначать вот то, что у нас сэмплируется за
[06:06.360 --> 06:16.120]  VIT. Вот. Так. Сейчас. Что-то я всё не метон. А, всегда. Пом-пом-пом-пом. Сейчас. То есть,
[06:16.120 --> 06:28.400]  обозначим его за VIT. Вот. И мы будем знать, что, то есть, VIT это направление, ценка f' от x-итова.
[06:28.400 --> 06:33.320]  Вот так правильно сказать. Что ещё известно? Известно, что дисперсия у этого VIT-а,
[06:33.320 --> 06:40.120]  она ограничена для всех и сигму квадрат. Ну, то есть, понятно, шум конечный. Вот. И также шаг у нас
[06:40.120 --> 06:45.280]  меньше, либо 1 на L, как это тоже самое нерайс, который был в датаминированном случае. То есть,
[06:45.280 --> 06:49.560]  пытаемся максимально... Вот я сейчас подчеркну то, что у нас было в датаминированном случае. То есть,
[06:49.560 --> 06:55.480]  у нас было вот это и у нас было вот это. Всё остальное, это что-то новенькое. Сейчас поймём... Да. И,
[06:55.480 --> 07:02.120]  собственно, результат-то какой? Результат-то кое-что... Тут сейчас будет хитро. Дани f от x-ка среднее,
[07:02.120 --> 07:09.720]  минус f от x-а звёздочкой будет меньше либо равен, чем x0 минус x-а звёздочкой. Всё как и было.
[07:09.720 --> 07:16.760]  Делить на 2 альфа к, вот. И внезапно плюс альфа сигма квадрат пополам. То есть, вот это вот
[07:16.760 --> 07:25.000]  результат. Ну, xkt с чертой это единица на к сумма x-итых и там от 0 до k-1. Ну, где-то так. Ну, либо
[07:25.000 --> 07:29.160]  от 1 до k. Понятно, что это усреднение за первой к айтерацией. Сейчас, Ниндия, а можете ещё раз на
[07:29.160 --> 07:36.120]  v-i-t, что тирает оценка f? Это направление, по которому будем двигаться. То есть, у нас будет выражение
[07:36.120 --> 07:40.920]  x-i плюс 1 равняется x-i плюс... Там будет плюс или минус. Мы сейчас ещё посмотрим. Там альфа,
[07:40.920 --> 07:45.720]  i-t, v-i-t. И это как бы v-i-t, это вот та самая оценка градиента, которую мы до этого обозначали вот так.
[07:45.720 --> 07:52.000]  Видите, много-много символов v-i-t. Побыстрее просто оценивать. Ну, понятно. Вот. И здесь для шума этого,
[07:52.000 --> 07:57.720]  собственно, для шума этой оценки задна граница вверху. То есть, смотрите, что произошло. У нас
[07:57.720 --> 08:02.880]  появилось, помимо того, ну, то есть понятно, что с ростом k у нас фразуется что-то, что стремится к
[08:02.880 --> 08:07.280]  нулю и что-то, что константа и константном альфе. Вот. То есть, если мы возьмём постоянный шаг,
[08:07.280 --> 08:13.080]  то эта штука перестанет сходиться. Вот. Что есть хорошо. Вот. Поэтому следующим шагом после доказательств
[08:13.080 --> 08:17.880]  этого факта мы поймём, какой альфа стоит взять. Ну, это более-менее очевидно. Я думаю, давайте.
[08:17.880 --> 08:21.440]  Какую альфу надо брать, чтобы сходимость появилась? Киньте в чат выражение, пожалуйста.
[08:21.440 --> 08:26.640]  Чтобы правая часть полюсходилась при к стремящемся бесконечности. 2 в степени минус k. Да, агрессивно.
[08:26.640 --> 08:33.320]  Ну, тогда будет какая-то грустная сходимость, кажется. Ой-ой. Чё-то вы неправы. Типа, давайте
[08:33.320 --> 08:39.080]  подставим. Пусть у нас альфа k-ты, да, видимо, равняется 2 в степени минус k, и мы эту штуку
[08:39.080 --> 08:44.920]  сюда подставляем. И мы внезапно получаем 2 в степени k на x 0 минус x со звёздочкой. Делить
[08:44.920 --> 08:49.480]  всего лишь на 2k, и эта штука стремится к плюс бесконечности, что вообще-то очевидно. Ну, я думаю,
[08:49.480 --> 08:56.640]  очевидно. Это не ряд, нет. Это прям вот оценка, как она есть. То есть это не подходит. Да, вот 1 на
[08:56.640 --> 09:03.760]  корень из k. Из k более правдоподобная гипотеза. Вот. И если, ок, если сейчас это вот сюда подставить,
[09:03.760 --> 09:10.400]  то мы получим сходимость вида 1 на корень из k. Ну, я думаю, что подставить для вас не составит
[09:10.400 --> 09:15.280]  большого труда. Вот. Просто вот в эту оценку подставляется альфа равная 1 на корень из k,
[09:15.280 --> 09:21.200]  и вот в знаменателе здесь получается просто корень из k, а альфа единица на корень из k,
[09:21.200 --> 09:28.800]  корень из k переходит сюда. Вот. И получаем вот такую оценку, что существенно хуже от 1 на k,
[09:28.800 --> 09:36.160]  которое было в градиентном спуске. То есть если мы делаем уменьшающиеся последовательность шагов,
[09:36.160 --> 09:40.920]  ходимость ухудшается в отличие от постоянного шага в градиентном спуске детерминирован. Вот.
[09:40.920 --> 09:46.960]  Понятно ли разница и то, насколько, ну, то есть степень ухудшения. Ставьте плюс, если понятно,
[09:46.960 --> 09:52.800]  и минус, если нужно еще подробнее уточнить. Никита, у вас там дела? А, все, вижу, спасибо. Вот. Ну,
[09:52.800 --> 09:57.360]  давайте докажем, чтобы как-то все более-менее обосновать. Вот. Ну, доказательство начинается
[09:57.360 --> 10:03.120]  с уже классических шагов, которые были проделаны в прошлый раз. Сначала мы используем квадратичную
[10:03.120 --> 10:06.760]  оценку сверху в силу L-гладкости. Вот. А вот дальше уже будут некоторые отличия,
[10:06.760 --> 10:13.160]  которые в себя вберут то, что у нас теряется терминированность в направлении. Вот. То есть
[10:13.160 --> 10:18.080]  вот тут было вот так, и L-стрик здесь, это точный градиент. Ровно лемма о спуске,
[10:18.080 --> 10:24.400]  который мы использовали ранее. Вот. Теперь мы замечаем, что у нас, да, здесь все-таки минус,
[10:24.400 --> 10:30.480]  вот так. Вот. Замечаем, что вот эта штука, это минус альфа и т в и т. Ну, и это понятно то же
[10:30.480 --> 10:34.640]  самое. Где вы, это некоторая случайная штука, случайный вектор. Теперь, если мы это все подставим,
[10:34.640 --> 10:41.240]  то получится что? Получится, что х и плюс один меньше либо равно, чем f от х и минус,
[10:41.240 --> 10:51.800]  ну, минус, да, альфа, f-стрих х и v и и, и плюс l альфа квадрат пополам, норма v и в квадрате.
[10:51.800 --> 10:59.800]  Вот. Вроде пока что довольно простые преобразования. Вот. Которые превратятся в следующую... Так,
[10:59.800 --> 11:04.360]  давайте так. Что теперь хочется сделать? Глядя на выражение, которое стоит справа. Ладно,
[11:04.360 --> 11:09.920]  я думал, что у вас довольно часто такие приемы были в других курсах. Ну, в общем, взять мотождание
[11:09.920 --> 11:13.640]  хочется, потому что здесь какие-то случайные величины, у нас случайные векторы появились.
[11:13.640 --> 11:18.280]  Давайте мотождание возьмем. Вот. Ну, если так записать через мотождание по, собственно,
[11:18.280 --> 11:24.840]  сэмплированному индексу. Вот. У нас образуется тут f от х и т, тут минус альфа. У нас же не смещенный
[11:24.840 --> 11:29.680]  оценок, поэтому тут появится квадрат. Тут будет l альфа квадрат пополам, а тут будет мотождание,
[11:29.680 --> 11:37.800]  правильно ли я пишу? Да, мотождание от v и t в квадрате. Так. То теперь мешается. Какая
[11:37.800 --> 11:42.560]  часть этого выражения, которое стоит справа, кажется, еще не докрученной до конца? Кажется,
[11:42.560 --> 11:47.080]  последнее с мотожданием вы... Ну, последнее, конечно. Давайте поймем, что это такое. Ну,
[11:47.080 --> 11:53.080]  вот у нас было прекрасное выражение про дисперсию. Что такое дисперсия от... Так.
[11:53.080 --> 12:00.440]  Дисперсию от v и t можно, как ее определить, как мотождание нормы квадрата минус квадрат
[12:00.440 --> 12:06.800]  мотождания. Квадрат, это же квадрат мотождания. Вот так. Ну, собственно, вот эта штука,
[12:06.800 --> 12:11.960]  которая у нас стоит в третьем слагаемом, раз, два, три, да. Вот. Отсюда как бы напрямую следует,
[12:11.960 --> 12:18.160]  что это там равно, да. Видимо, можно точную равенство поставить. f от x и t минус альфа норма
[12:18.160 --> 12:24.200]  градиента. Все прекрасно. Плюс l альфа квадрат пополам. А тут появляется внезапная дисперсия
[12:24.200 --> 12:29.720]  v и t в плюс... Ну, плюс понятно что, потому что у нас не смещенная оценка. Вот. То же самое выражение,
[12:29.720 --> 12:37.480]  которое у нас было и предыдущим слагаемом. Вот. Ну и теперь, поскольку мы знаем, что дисперсия
[12:37.480 --> 12:44.120]  оценивается сверху сигму квадрат, то это все меньше либо равно чем f от x и t. Вот. А дальше,
[12:44.120 --> 12:51.240]  ну понятно, что вот это группируется с вот этим. Следующее выражение минус альфа. А здесь остается
[12:51.240 --> 12:58.960]  единица минус l альфа пополам на норму градиента. Вот. И плюс, соответственно, l альфа квадрат
[12:58.960 --> 13:03.600]  пополам, умножаясь на сигму квадрата. Так. Понятно ли, что произошло? Поставьте плюс,
[13:03.600 --> 13:08.800]  если понятно и минус, если нужны более детальные пояснения. Так. Здорово. Вижу плюсы. Замечательно.
[13:08.800 --> 13:13.120]  Вот. Ну а дальше, Иван, пользуемся просто тем же самым фактом, что у нас l альфа меньше либо равно
[13:13.120 --> 13:20.840]  единицы. Вот. Поэтому это все меньше либо равно чем f от x и минус альфа пополам. Вот. На норму
[13:20.840 --> 13:27.680]  градиента и плюс, что там получается, альф пополам, да, на сигму квадрат. То есть,
[13:27.680 --> 13:33.360]  вот это вот все, оно уехало вот сюда и оно же уехало вот сюда. Вот. Получили такую оценку.
[13:33.360 --> 13:40.200]  Теперь. Что теперь? Теперь мы блестящим образом учитываем наше прекрасное неравенство для f от
[13:40.200 --> 13:44.680]  x этого. Вот. У нас же теперь и включается. То есть, вот это все, что было до этого, это была исключительная
[13:44.680 --> 13:49.440]  гладкость. Вот. И мы выпуклость никак не использовали. Ну вот. Теперь мы как бы используем явным образом
[13:49.440 --> 13:56.400]  выпуклость. Вот. Тем же самым образом, каким мы это использовали в, доказывается, сходимости для
[13:56.400 --> 14:00.480]  простого градиентного метода. Вот. И там у нас появилось такое неравенство, что f от x
[14:00.480 --> 14:05.040]  со звездочкой меньше либо равняется, чем f от x it. Ну, собственно, критерии первого порядка выпуклости
[14:05.040 --> 14:11.520]  задействуем. Плюс f штрих от x i. Колярное произведение x i минус x со звездочкой. Вот. Это,
[14:11.520 --> 14:17.040]  собственно, выпуклость. Теперь. Ну, собственно, тут частный градиент. Вот. Поэтому мы вот. Ой. Я
[14:17.040 --> 14:23.960]  немножко тут наврал. Прошу прощения. Тут, на самом деле, наоборот. Вот тут x i, тут x со звездочкой. А,
[14:23.960 --> 14:29.280]  нет. Наврал только в одном месте. Сейчас. А, нет. Все, да. Да-да-да. Все правильно теперь. И вот эта штука
[14:29.280 --> 14:34.960]  теперь мы подставляем вот сюда. Получаем еще раз, что в нашем от ожидания f x и плюс один меньше
[14:34.960 --> 14:39.160]  либо равно. Ну, то есть нам же надо с f от x со звездочкой как-то сравниваться. Вот мы сейчас так и
[14:39.160 --> 14:47.400]  сравниваемся. f штрих x it x it минус x со звездочкой. А дальше. Ой. Дальше минус альфа пополам f штрих
[14:47.400 --> 14:53.760]  от x it. Плюс альфа пополам сигн квадрат. Вот. Ну, то есть просто подставили, чтобы у нас получилось. Ой.
[14:53.760 --> 15:00.720]  Чтобы у нас с одной стороны получилось x и плюс один. А тут появился x со звездочкой. А дальше нам
[15:00.720 --> 15:05.480]  нужно что-то сделать с вот этой вот непонятной штуковиной. Вот. Сейчас будем думать, что с ней
[15:05.480 --> 15:10.120]  сделать. Ну и вот этот шрум, он как был, так и у нас останется. Поэтому мы как-то пока его тянем за
[15:10.120 --> 15:17.400]  собой. Вот. Теперь сделаем следующий, казалось бы, странный трюк. Вот. Суть которого заключается в том,
[15:17.400 --> 15:25.600]  чтобы внести под мат ожидания справа как можно большее количество выражений, чтобы получить опять
[15:25.600 --> 15:29.720]  же ту самую телескопическую сумму, которая у нас фигурировала в прошлый раз. Вот. Для этого мы
[15:29.720 --> 15:36.360]  подставим, как бы странным это не казалось, то, что до этого мы вынесли. А именно у нас f штрих
[15:36.360 --> 15:45.800]  от x it в этом от ожидания v it. Вот. А норма f от x it в квадрате, это что такое? Ну, то есть это
[15:45.800 --> 15:54.320]  мат ожидания, понятно. Это мат ожидания от нормы v it в квадрате, в квадрате. Мила дисперсии. Вот. То есть
[15:54.320 --> 16:00.840]  все немножко наоборот. То есть то, от чего и где берется норма, и это, соответственно, меньше
[16:00.840 --> 16:07.680]  либо равно, чем, чем что, чем что, чем что, чем норм, мат ожидания опять же. Ну и минус сигма квадрата,
[16:07.680 --> 16:11.840]  потому что меньше либо равно. Вот. Вот получили такое замечательное неравенство, а поскольку здесь
[16:11.840 --> 16:16.760]  стоит знак минус, сейчас я уточню, где он стоит. Вот. Знак минус стоит. То раз мы оценили это сверху,
[16:16.760 --> 16:30.480]  то когда мы будем это вычитать, ну вычитать, то... А я понял вопрос. Да, слушайте, давайте сейчас мы тогда
[16:30.480 --> 16:34.920]  более более аккуратно это все провернем. Да, тут давайте пока ограничимся вот этим. Сейчас нам
[16:34.920 --> 16:40.120]  просто плюс же появится. Вот смотрите, вот у нас есть вот это, да. Вот. И мы давайте просто подставим.
[16:40.120 --> 16:46.200]  То есть получается, что от ожидания слева f от x и плюс один меньше либо равняется f от
[16:46.200 --> 16:53.200]  x со звездочкой плюс, плюс скалярное произведение от ожидания v и того на x и то минус x со звездочкой,
[16:53.200 --> 17:00.640]  минус. Это вот важный минус, который нам сейчас все починит. И мы это умножаем на мат ожидания от v
[17:00.640 --> 17:06.400]  и t в квадрате, минус дисперсию v и то, плюс альфа к полам сигма квадрата. Вот. Здесь просто
[17:06.400 --> 17:11.480]  определение. Теперь если мы скобки раскроем, f со звездочки я буду так еще списать. Немножко подскорчу
[17:11.480 --> 17:16.160]  тут скалярное произведение, которое как было так осталось. А дальше остается минус альфа к полам
[17:16.160 --> 17:23.200]  от ожидания нормы v и t в квадрате и плюс альфа к полам на дисперсию и плюс еще альфа к полам
[17:23.200 --> 17:27.800]  на сигму квадрата. А это уже меньше либо равно. Вот здесь как бы вот появился плюс, который позволит
[17:27.800 --> 17:34.120]  нам сейчас сверху оценить. Плюс снова дисперсия, я вот так вот напишу. Вот. Минус снова альфа к полам
[17:34.120 --> 17:40.840]  от ожидания v и t в квадрате и плюс альфа сигму квадрата. Вот. Понятно, что произошло, да? Ну,
[17:40.840 --> 17:48.480]  а теперь давайте вот напишем аккуратно равенство. Плюс от ожиданий от чего? От ожидания от, большая
[17:48.480 --> 17:56.600]  скобка, скалярного произведения v и t на x и t минус x со звездочкой и минус альфа к пополам
[17:56.600 --> 18:02.560]  норма v и t в квадрате. Плюс альфа сигму квадрата. Вот. То есть у нас какие основные ингредиенты
[18:02.560 --> 18:08.080]  получившегося равенства? У нас есть вот этот кусочек от нашего от ожидания и вот этот кусочек. Это
[18:08.080 --> 18:13.160]  как бы то, что уже хорошо. У нас есть довольно приятный кусочек вот этот. Есть не очень понятный,
[18:13.160 --> 18:18.440]  понятный по содержанию, но хорошо выверенный по структуре кусочек с от ожиданием от некой величины,
[18:18.440 --> 18:23.880]  которую вы сейчас скажете, на что очень похоже и чего надо с ней теперь сделать. Какие будут
[18:23.880 --> 18:30.800]  варианты? Чего похоже-то? Третий курс, наверное, должны такое видеть с первого взгляда. Кажется,
[18:30.800 --> 18:37.520]  не очень. Смотрите, ну, есть v и t и его норма в квадрате. Есть v и t, который на что-то умножается.
[18:37.520 --> 18:41.840]  Чего дальше надо сделать? В плане, может норму раскрыть в скалярном произведении с самим
[18:41.840 --> 18:49.440]  собой и свернуть в общее скалярное произведение v и t на… Нет. Мы с этого начинали. Мы с этого
[18:49.440 --> 18:54.360]  начинали. Нет, нет, нет. Не совсем. Что еще можно сделать? Нужно добавить и вычесть норму вот этой
[18:54.360 --> 18:59.000]  величины, чтобы полный квадрат выделить. То есть это почти что полный квадрат у нас тут. Отсюда,
[18:59.000 --> 19:06.120]  собственно, довольно простая гипотеза, что… Ну, в общем, так, давайте это как-то обозначить. Ну,
[19:06.120 --> 19:09.440]  слушайте, да, у меня уже есть о прекрасном значении. Вот то, что выделено красненьким,
[19:09.440 --> 19:16.120]  вот оно будет у меня тут равно. Я сейчас вынесу что-нибудь отсюда типа 1 на 2 альфа, как будто бы.
[19:16.120 --> 19:25.400]  Вот. Дальше тут будет мотождание от вот такой штуки. Будет 2 альфа v и t x и t минус x… Ой,
[19:25.400 --> 19:34.800]  минус x со звездочкой. Минус… Минус альф… Минус альфа v и t в квадрате. Во, прекрасно. Так,
[19:34.800 --> 19:41.320]  ну давайте думать. Дальше минус вынести на всякий случай. Здесь останется плюс, здесь будет минус.
[19:41.320 --> 19:50.320]  Сюда появится плюс… Так, ну давайте, плюс x и t минус x со звездочкой в квадрате и минус x и t
[19:50.320 --> 19:54.720]  минус x со звездочкой в квадрате. Поставьте плюс, если вы… О, кто-то еще хочет присоединиться,
[19:54.720 --> 20:00.920]  приятно. Поставьте плюс, если вы увидели, что произошло, и проверьте, что все корректно заодно.
[20:00.920 --> 20:06.000]  Лидия, приветствую. Мы сейчас занимаемся тем, что доказываем прекрасный результат о том,
[20:06.000 --> 20:13.400]  что стахастический градиентный спуск с постоянным шагом для сильно… для выпуклой функции или гладкой
[20:13.400 --> 20:19.160]  не сходится по функции, с вот такой вот оценкой. Вот. Мы почти закончили, на самом деле. Сейчас,
[20:19.160 --> 20:23.800]  я думаю, ваши коллеги быстро подскажут, проверят и подскажут, что все правильно. Тогда мы пойдем
[20:23.800 --> 20:29.080]  дальше, здесь будет понятнее. О, я вижу плюс, это прекрасно. Значит, все в порядке. Вот, смотрите,
[20:29.080 --> 20:35.840]  а дальше что происходит? Дальше мы это записываем как минус 1 на 2 альфа. Мотор ожидания. Дальше
[20:35.840 --> 20:39.880]  будет некоторый выклад, который я сейчас напишу. В общем, вроде бы получилось что-то как-то сложить
[20:39.880 --> 20:45.640]  вместе, и минус я, соответственно, внес внутрь. Вот. То есть, смотрите, теперь я вот итоговое
[20:45.640 --> 20:51.520]  выражение постараюсь записать. То есть, у нас есть мотор ожидания от f, x и плюс 1. Меньше или
[20:51.520 --> 20:58.640]  бы равняется f со звездочкой. Плюс мотор ожидания от 1 на 2 альфа. Вот такой вот штуки. x и
[20:58.640 --> 21:07.320]  t минус x со звездочкой. Минус x и плюс 1 минус x со звездочкой. Вот. И плюс альфа сигма квадратов. Вот.
[21:07.320 --> 21:13.520]  И тут мы снова увидим прекрасное выражение, которое уже было встречено прошлый раз. А именно,
[21:13.520 --> 21:18.920]  вот такая вот сумма, которая при суммировании благополучно телескопически уничтожится. То есть,
[21:18.920 --> 21:23.120]  вот это вот сократится с вот этим на двух соседних слагаемых. Поставьте плюс,
[21:23.120 --> 21:29.280]  если вы это видите и понимаете, о чем я говорю сейчас. А? Вот. Ну и отсюда следует как бы прямое
[21:29.280 --> 21:33.680]  понятное следствие. То есть, мы просто берем и суммируем теперь вот все оба оба. Суммируем
[21:33.680 --> 21:44.000]  оба обе части по и от нуля до к минус 1. Смотрим, что получается. Тут мотор ожидания f, x и плюс 1 минус f
[21:44.000 --> 21:52.000]  со звездочкой. Вот. А здесь остается что? Здесь остается k альфа сигма квадрат. Плюс. Плюс что?
[21:52.000 --> 22:02.640]  Плюс 1 на 2 альфа x0 минус x со звездочкой в квадрате. И минус. Минус мотор ожидания от нормы xk минус
[22:02.640 --> 22:07.600]  x со звездочков в квадратике. Вот. Ну это понятно, что раз мы тут из чего-то вычитаем от ожидания
[22:07.600 --> 22:14.400]  отрицательного числа, то получаем здесь k альфа сигма квадрат плюс 1 на 2 альфа нормы x0 минус x со
[22:14.400 --> 22:21.400]  звездочкой в квадрате. Смотрите-ка, мы почти что у цели. Вот. Теперь осталось осознать, что делать с
[22:21.400 --> 22:29.000]  левой частью. То есть, что вот это за сумма от ожидания f от x и т плюс 1? Понятно ли, что осталось с этим
[22:29.000 --> 22:33.200]  побороться? Или про правую часть тоже что-то хотите спросить? Поставьте плюс, если все понятно, идем
[22:33.200 --> 22:41.800]  дальше. И минус, если нужно еще про правую часть что-то сказать. Вижу, вижу, вижу, что вроде нормально.
[22:41.800 --> 22:52.880]  Да, вы правы, но это в общем-то неважно, потому что... Да, тут надо бы дописать, спасибо. Потому что все
[22:52.880 --> 22:58.000]  равно вычитаем и убираем в оценке. Да, хорошо, для корректности, конечно, нужно добавить множество.
[22:58.000 --> 23:02.480]  Ну, давайте теперь, собственно, вернемся к тому, с чего начали, а именно к самой формулировке.
[23:02.480 --> 23:06.800]  В формулировке у нас фигурирует вот такая вот странная величина. Вот. Давайте поймем теперь,
[23:06.800 --> 23:12.400]  что это вообще такое. Вот. И окажется, что неожиданным образом, вот, что мы вспомним не Райси
[23:12.400 --> 23:16.920]  Янсона. Надеюсь, вы помните, что это такое. Это вот для выпуклой функции появилась такая вот оценка,
[23:16.920 --> 23:24.040]  что выпуклая функция от выпуклой комбинации аргументов типа x и t и от 1 до k меньше либо равно,
[23:24.040 --> 23:34.120]  чем 1 на k сумма f от x и t. Вот. И, значит, что это значит? Вот. Это значит, что если мы теперь
[23:34.120 --> 23:41.880]  множим обе части на k, то у нас получается k на функцию от x, k, ну, типа средняя, да, и это меньше
[23:41.880 --> 23:50.560]  либо равно, чем сумма f от x и t. Вот. Далее. Что это как бы... Чем нам это помогает? Это нам помогает тем,
[23:50.560 --> 23:56.640]  что у нас смотожидание линейно. Потому, когда мы пишем, что у нас тут сумма мотожидания f от x и
[23:56.640 --> 24:03.840]  плюс один, минус f со звездочкой, да, и от нуля до k минус один, то это равняется мотожидании от
[24:03.840 --> 24:09.760]  внимания сумма. Ну, а дальше сейчас увидите, что получится. Вот. Получили такое выражение. А вот
[24:09.760 --> 24:15.240]  это мотожидание снизу подпирается, ну, в общем, понятной какой оценкой, да. То есть вот эта штука
[24:15.240 --> 24:23.600]  это больше либо равно, чем k на f от x, k среднее. Ну, k вносится, то есть, как бы это правильно
[24:23.600 --> 24:34.680]  сформулировать? А, да, давайте вот так. k на f от k нам от ожидания f от x, k среднее, вот,
[24:34.680 --> 24:42.280]  минус f со звездочкой. Вот. Эта штука меньше либо равна получается, чем наша вот эта вот замечательная
[24:42.280 --> 24:48.480]  оценка. Вот. Ну и значит, мы при делении на k получим, что нашим от ожидания от f от x, k среднее,
[24:48.480 --> 24:55.400]  минус f со звездочкой будет меньше либо равно, чем вот вся вот эта вот замечательная история. Так,
[24:55.400 --> 25:02.320]  копировать. Вот. И вот здесь все вставляется. И деленный на k. Это ровно то, что у нас было в правой
[25:02.320 --> 25:07.480]  части. Видно ли, что это одно и то же? Поставьте плюс, если все понятно и видно, и минус, если
[25:07.480 --> 25:13.520]  нужно подробнее что-то пояснить. Вижу пока два плюса. Так, Дмитрий. Все ли понятно? Ага, да,
[25:13.520 --> 25:18.920]  спасибо. Ну все, короче, это мы доказали. Все классно. Так, отлично. Время еще есть. Вот.
[25:18.920 --> 25:24.400]  Собственно, что... Да, ну и в общем, понятно, что при постановке выражения с 1 на корень
[25:24.400 --> 25:32.400]  s, k становится понятно, что вот здесь вместо вопросика появляется о от 1 на корень s, k для
[25:33.280 --> 25:37.440]  обывающего шага, что существенно хуже, чем то, что у нас было для дедаминированного случая как
[25:37.440 --> 25:42.040]  раз таки еще дисперсии. Вот. Значит, ну понятно, что первое желание, возможно, которое возникает для
[25:42.040 --> 25:48.040]  того, чтобы пытаться все-таки победить и получить хотя бы такую же скорой исходимости, это сказать,
[25:48.040 --> 25:53.240]  что ну давайте мы типа ускорим это все дело. У нас же были ускоренные методы. Вот. Давайте ускорим и
[25:53.240 --> 25:57.920]  будем надеяться на то, что после ускорения мы получим хотя бы сопоставимый с градиентным
[25:57.920 --> 26:02.200]  методом скорой исходимости. Вот. Оказывается, что ускорение здесь не работает. Вот. То есть
[26:02.200 --> 26:08.600]  конструкции, которые были в ускоренных методах при постановке неточного градиента, коренные
[26:08.600 --> 26:20.040]  методы не как бы сформулировать, не приводят к более быстрой асимпатической исходимости при
[26:20.040 --> 26:25.240]  неточном градиенте. То есть оказывается, что вот те прекрасные последовательности мы строили,
[26:25.240 --> 26:30.360]  еще там что-то было в прошлый раз. Вот они тут не помогают в точке зрения именно асимпатической
[26:30.360 --> 26:35.440]  расходимости для лопковых функций. Помогает немножко другая история. Вот. Ну, которая, я думаю,
[26:35.440 --> 26:39.600]  более-менее очевидна. Вот. Называется она усреднение. Потому что раз у нас большая дисперсия,
[26:39.600 --> 26:43.840]  ну раз у нас проблемы с дисперсией, то давайте мы ее уменьшим. Вот. Ну, какой самый простой
[26:43.840 --> 26:48.200]  способ уменьшения дисперсии приходит в голову, вы думаете? Вы уже сказали усреднение, действительно.
[26:48.200 --> 26:54.600]  Нет, а что это, как это работает? Ну, посчитать несколько независимых одинаково распределенных
[26:54.600 --> 27:01.160]  оценок. То есть с разными, но... Давайте-давайте, формулируйте, да. Все правильно. Говорите,
[27:01.160 --> 27:07.200]  формулируйте, это важно. Посчитать f с разными сидами рандома для w? Да, то есть смотрите,
[27:07.200 --> 27:13.960]  я пытаюсь кратко как-то, кратко повторить то, что вы сформулировали. Самый простой способ
[27:13.960 --> 27:18.520]  побороться с этой штукой, это сделать следующее, следующее преобразование. Сказать, что, окей, вот мы
[27:18.520 --> 27:25.080]  берем нашу точку x, которая здесь, она так... Видно лазерную указку или нет? Да, видно. Видно,
[27:25.080 --> 27:32.160]  прекрасно. Смотрите, взять x текущий и взять несколько слагаемых из этой суммы и для них
[27:32.160 --> 27:35.720]  посчитать гриденты, просуммировать. Ну, в общем, получите ровно то же самое, что в ней ромка
[27:35.720 --> 27:42.360]  называется там типа batch, вот, сложно переводимая, к сожалению, на русский слово. Вот. То есть типа
[27:42.360 --> 27:52.520]  наша оценка f' от x, это типа формально вот что-то такое. И из вот этого множества, вот. И, собственно,
[27:52.520 --> 27:57.960]  чем можно сказать, ну, довольно логично, это формально можно показать, то чем больше вы берете
[27:57.960 --> 28:05.080]  эту множество, больше мощность этого множества, тем меньше дисперсия. Вот. Это как бы... Но доказывается,
[28:05.080 --> 28:11.520]  более высокие скорости сходимости немного для другого способа учета этого самого средней. Вот.
[28:11.520 --> 28:17.120]  Ну, пока давайте тем способом и общей стратегии закончим. А вот про вот эту схему я хочу
[28:17.120 --> 28:22.840]  примерно показать. Вот. Чтобы как бы не быть голословным, для более как бы какого-то
[28:22.840 --> 28:27.720]  теоретически обоснованного, но абсолютно на практике неприменимого способа я приведу,
[28:27.720 --> 28:34.280]  ну, пока покажу теорию. Вот. А для вот этого наивного и довольно неплохо, ну, относительно неплохо
[28:34.280 --> 28:40.080]  работающего на практике способа, давайте покажу какую-нибудь картинку, и мы это все дело увидим
[28:40.080 --> 28:44.360]  просто собственными глазами. Так. Мне нужно для этого прекратить тут демонстрацию. Да, что ж такое,
[28:44.360 --> 28:48.920]  это все такое ощущение вылезает. Так. И включить вот эти демонстрации. Вот. Для этого возьмем какую-нибудь
[28:48.920 --> 28:56.400]  простую нейронку. Вот. Так. Runtime. Тут, по-моему, GPU стоит. Да. Вот. Взяли простую нейронку. Неважно.
[28:56.400 --> 29:03.560]  Просто суперпозиции некоторых функций. Вот. Неважно. Такой там loss. Это все вам расскажут на других
[29:03.560 --> 29:09.600]  курсах, я думаю. Вот. Ну, и возьмем датасет картинок и зададим, чтобы от часа израниться. Типа вот
[29:09.600 --> 29:16.400]  абсолютно вырожденный супер, такой называется, крайний случай. Вот. И запустим всю эту балалайку
[29:16.400 --> 29:24.440]  с ускоренным методом Momentum. Вот. И с методом, у которого там адаптивный шаг подбирается, про это
[29:24.440 --> 29:28.680]  я тоже чуть попозже скажу. Вот. Наверное, кстати, можно было бы без него обойтись. Ой, что-то как-то
[29:28.680 --> 29:36.400]  много это... Ну, да. Э-э-э. Надо было, наверное, по-другому запускать. Что-то, что-то не... Большой лог. Да.
[29:36.400 --> 29:41.920]  Давайте пока тут остановимся. И я, наверное, перепилю сейчас. Ой, кошмар какой-то. Перепилю...
[29:41.920 --> 29:47.440]  Что перепилю-то? А, там есть параметр, который называется logInterval. Вот. Его надо сделать не
[29:47.440 --> 29:54.160]  10, типа, например, вот так. Вот. И тут сейчас будет что-то... Ну, да. Тысячи тоже много. А-а-а. Давайте
[29:54.160 --> 29:59.080]  10 тысяч. Вот. Сейчас будет нормально. Вот здесь видно, как меняется loss. Был 2. Да, нет. Ну да,
[29:59.080 --> 30:03.520]  заметьте, что, типа, долго, да? Гадаете, почему долго? Хотя батч один. Ну, какие будут идеи?
[30:03.520 --> 30:08.200]  Может быть, оно работает так, чтобы, ну, эпоха заканчивается, когда у нас loss уменьшился?
[30:08.200 --> 30:11.720]  Не-не, эпоха заканчивается, когда мы всегда... Ну, одна эпоха, это, типа, мы прошли по всей сумме.
[30:11.720 --> 30:17.800]  То есть мы, типа, отсмотрели все сэмплы, все слагаемые, но, ну, как бы, для... У соседних батчей у нас
[30:17.800 --> 30:23.000]  разные параметры. Наверное, это не очень понятно. Понятно. Короче, 43 процента точность. Классифицируем
[30:23.000 --> 30:28.680]  картинки, если... Короче, классифицируем картинки 10 классов. Вот. Это супер-классика. Вот. Поэтому
[30:28.680 --> 30:36.040]  я ее сейчас показываю. Вот. Ну, видно, что loss пляшет. То есть, типа, 79, ну, 26, вот сейчас вот. Вот.
[30:36.040 --> 30:40.920]  А потом я хочу показать картину, как он изменяется на каждой... На каждой этой рации. Там он будет
[30:40.920 --> 30:45.240]  посчитан только под выборки. Только по батчу, как бы он будет посчитан. Там будет, конечно,
[30:45.240 --> 30:49.160]  большая осцилляция. Сейчас это будет видно. Так, меня так немножко смущает, что это все длится
[30:49.160 --> 30:55.400]  over бесконечность, все пять эпох. Ой, печально как-то. Так, короче, вы оценили, насколько это долго в
[30:55.400 --> 31:02.000]  таком варианте. Да? Ставьте плюс, если вы считаете, что это долго. Оценили. Окей. Ладно, давайте чуть
[31:02.000 --> 31:09.080]  похитрее сделаем. Как это? Чуть похитрее сделаем. Скажем, что типа, батч 100. Да, все хорошо. Так. И должно
[31:09.080 --> 31:15.480]  стать немножко побыстрее. Да, это точно на GPU. И я догадываюсь, что это так медленно, потому что
[31:15.480 --> 31:22.760]  может есть какая-нибудь безумная размерность. Ну, да. 20 каналов, это немало. А тут я еще плохой лог
[31:22.760 --> 31:27.560]  поставил. То есть, он сейчас будет только в конце каждой эпохи делать принты. Ну, короче, смотрите,
[31:27.560 --> 31:34.400]  батч size 1 — это грустно, потому что он, эти данные, они типа последовательные. Ну, да. Я думаю, уже видно,
[31:34.400 --> 31:40.160]  что быстрее немножко. Они обрабатываются типа последовательно. Вот. И при этом, типа, каждый батч
[31:40.160 --> 31:44.760]  загружается на GPU. И там на этой GPU кучу места оставится свободного, потому что он не может
[31:44.760 --> 31:49.040]  обработать следующий батч, пока текущий не закончился. Вот. И поэтому получается так, что,
[31:49.040 --> 31:55.520]  типа, называется utility — производительность, загрузка. Ну, короче, степень того, как активно
[31:55.520 --> 31:59.560]  используется память GPU для обработки этих данных, она, типа, супер низкая, потому что мы всего лишь
[31:59.560 --> 32:05.160]  одну картинку загружаем в память каждый момент времени. Вот. Поэтому, типа, один, батч 1 — это,
[32:05.160 --> 32:11.360]  типа, в таких случаях не очень хорошая стратегия. Вот. Так. Ну, уже почти конец. Я думаю, видно,
[32:11.360 --> 32:15.120]  что стало, вроде, побыстрее немножко. Или нет? То есть, не особо. Наверное, было лучше бы таймер
[32:15.120 --> 32:20.480]  поставить. Ну, да. Визуально, наверное, не очень. Это все хорошо. Ну, runtime — вот есть,
[32:20.480 --> 32:24.920]  типа, GPU стоит. Потому что там какая-нибудь, типа, подобный GPU по дефолту выделивался,
[32:24.920 --> 32:29.840]  поэтому так медленно. Ну ладно, SGD сейчас докрутится. До 42 он, вроде, долетел. Конечно,
[32:29.840 --> 32:38.640]  на тесте такой ноль. А, это, типа, сколько обработан. Да, неважно. Да, ну, короче, 46. Вот.
[32:38.640 --> 32:46.000]  46 доехал. Вот. И сейчас Adam, наверное, тоже за такое же, примерно, время доедет. Ну, делайте в
[32:46.000 --> 32:50.920]  аштарке, что называется. Будет больше 46 или нет. Начали уже с 35. То есть, сейчас, вот, типа,
[32:50.920 --> 32:56.640]  на уровне третьей эпохи SGD — одна эпоха, я думаю, типа. Вот. Сейчас я скажу, что за всеми этими словами
[32:56.640 --> 33:01.920]  скроется по поводу там SGD. Ну, про SGD, вроде, уже сказали. Вот. И там, типа, momentum — это,
[33:01.920 --> 33:07.600]  вот, собственно, ускоренная техника, которая симпатически лучше не делает, но на практике
[33:07.600 --> 33:12.400]  все-таки позволяет немного ускориться. Вот. Так. 41%. Видите, немножко замедлилось.
[33:12.400 --> 33:19.440]  Медлился темп. Кстати, хорошая идея по поводу посчитать, сколько стоит один батч. Сейчас,
[33:19.440 --> 33:23.560]  наверное, это проделаю. Интересно. Ну, наверное, 45%. С другой стороны, типа, не очень понятно,
[33:23.560 --> 33:27.480]  зачем это мерить, потому что понятно уже, что в работу несколько картинок будет надо
[33:27.480 --> 33:32.120]  в время одной эпохи мерить, наверное, тогда. Что лучше сразу обработать с той или десять раз
[33:32.120 --> 33:36.680]  по одной. Да. Ну, короче, наверное, сейчас я не буду это все делать, только запускать, потому что
[33:36.680 --> 33:41.680]  не очень очевидно, как именно, что и с чем нужно будет сравнивать. 47%. Смотрите-ка, немножко побольше.
[33:41.680 --> 33:47.640]  И сейчас еще одна эпоха осталась. Да, 48%. Ну, в общем, немножко он повыше. Смотрим на то. Да. Ну,
[33:47.640 --> 33:53.800]  короче, вот график ходимости. Вот. Здесь, типа, каждая точка — это loss на соответствующем куске
[33:53.800 --> 33:58.920]  до куске части слагаемых. Видно, что он, ну, типа, прыгает, ну, где-то 0,2, да, у него,
[33:58.920 --> 34:03.840]  так, в среднем амплитуде. Понятно, что на картинке понятно, почему 0,2 или не очень. Если не очень,
[34:03.840 --> 34:08.600]  поставьте минус. Ну, окей, вроде вижу плюсы. Хороший знак. Вот это было 100. Ну, давайте поставим,
[34:08.600 --> 34:14.200]  типа, 500. И все то же самое. Да, есть еще подозрение, что тут, типа, в тесте он 16,
[34:14.200 --> 34:18.800]  поэтому пока он пройдет по всем этим 16, все 100 лет может пройти. Тут тоже не дело не особо
[34:18.800 --> 34:23.240]  преувеличивает скорости. Так, ну, 11%. Ура. Не помню, сколько там было в самом начале. Да,
[34:23.240 --> 34:28.840]  бать был по меньшим. Сейчас заодно обсудим, если вдруг он сойдет. Ну, там был, типа, 47, да, 48 в
[34:28.840 --> 34:32.600]  конце. Сейчас сойдется к чему-то, что меньше. Тоже обсудим, почему так произошло. Ну, видно,
[34:32.600 --> 34:38.480]  что СГД почему-то стухает, да, и никаких там 40% уже, как бы, и нет. Ваши варианты,
[34:38.480 --> 34:45.080]  почему так произошло, можно писать в чате. Ну, слушайте, ну, детально может буить. Нет,
[34:45.080 --> 34:49.800]  погодите. Направление не точное. Мы вроде бы увеличили размер бачей, и они стали более точными.
[34:49.800 --> 34:54.600]  Как будто бы ожидается как раз-таки увидеть что-то противоположенную динамику. Это они были не
[34:54.600 --> 34:58.880]  точные в самом начале, но в самом начале получилась вышеточность. Немного контраиндуитивно получается.
[34:58.880 --> 35:03.960]  Ну, видите, тут как бы Adam более устойчив в этой штуке. Сейчас обсудим, что внутри сидит и почему
[35:03.960 --> 35:11.120]  так-то получается. Немножко осталось подождать. Сейчас мы оцениваем вообще что? Насколько мы близки
[35:11.120 --> 35:17.880]  к инфинному по вообще возможному множеству функций или насколько мы близки к реальному? Я не понял
[35:17.880 --> 35:22.680]  вопрос. У нас есть некоторые функции, которые есть параметры. Вот они тут внутри зашиты. Это функция,
[35:22.680 --> 35:27.400]  ее как бы отображение реализовано. Не важно, что там внутри, там какие-то есть параметры,
[35:27.400 --> 35:33.960]  которые оптимизируются. Мы хотим, чтобы нам по данному входу были нужны и нам параметры настроились
[35:33.960 --> 35:38.960]  так, чтобы минимизировать некоторый лос, который адекватно оценивает вот эти проценты. Видно,
[35:38.960 --> 35:45.320]  что он тут немножко падает. Причем падает сильнее, чем числа, которые были здесь. Вот. Ну да,
[35:45.320 --> 35:50.680]  возможно даже на тест надо смотреть вот сюда. Тут вот видно, что это 0.40013, 0.009, что гораздо меньше.
[35:50.680 --> 35:55.920]  Вот. Досчиталось, да? Ну, в общем, до 47 он все-таки долетел. Давайте на график посмотрим. Видно,
[35:55.920 --> 36:01.920]  что тут не 0.2. Сброс в среднем, то тверждает гипотезу о том, что действительно мы увеличиваем
[36:01.920 --> 36:07.680]  бачку, уменьшаем дисперсию в агредиенте. Вопрос. Почему эта штука сломалась, вы думаете? Ну,
[36:07.680 --> 36:20.000]  смотрите, поскольку… Ну да. Не функции агредиентов. Почему нет? Ну, там же было ближе. Когда бач был
[36:20.000 --> 36:25.040]  меньше, то было точнее. Лос был меньше. Задача не поменялась, поменялась только гиперпараметры.
[36:25.040 --> 36:29.280]  Смотрите, да. Я надеюсь, запомните картинку. Я постараюсь сейчас что-то изобразить. Заранее,
[36:29.280 --> 36:35.640]  прошу прощения, если будет кривовато. То есть ключевой момент, про который надо понимать,
[36:35.640 --> 36:40.520]  про то, что когда мы смотрим на подобного рода функции, во-первых, это все дело не выпукло
[36:40.520 --> 36:46.760]  катастрофически. Вот. Поэтому у нас три уровня, они какие-то вот такие могут быть. И если мы,
[36:46.760 --> 36:53.440]  грубо говоря, начинаем идти вот отсюда, куда-нибудь, да, то наличишь, и то мы как бы идем,
[36:53.440 --> 36:58.600]  идем, идем, куда-нибудь, тык-тык-тык, пришли. Может быть, еще сюда пришли. А тут, оказывается,
[36:58.600 --> 37:04.200]  слишком такая большая ямка, вот, которая с увеличением точности наших направлений говорит нам,
[37:04.200 --> 37:09.000]  да мы уже, типа, пришли в победе. Нам и тут хорошо. Не нужно никуда дальше двигаться. Вот. А если шума
[37:09.000 --> 37:13.600]  больше, то он, типа, смотрит в разные стороны, типа, такой, окей, спрыгну-ка я сюда. Он такой,
[37:13.600 --> 37:19.440]  о, типа, тут классное значение функции, поменьше и агредиент получше. Я, типа, могу сюда спуститься
[37:19.440 --> 37:24.120]  запросто. Поэтому шум как бы позволяет вам, ну, так называемый, может быть, в литературе видели,
[37:24.120 --> 37:32.560]  два свойства методов exploration и exploitation. Вот. Поставьте плюс, если видели эти слова когда-то
[37:32.560 --> 37:38.320]  раньше в соответствующем контексте. Ну, не видели еще, да? Ну, у тебя впереди еще, значит. Кто-нибудь
[37:38.320 --> 37:44.600]  видел? Не видели. В общем, exploration это то, насколько вы как бы исследуете ваше допустимое множество,
[37:44.600 --> 37:50.600]  вот. Exploitation это то, насколько вы в рамках заданного множества хорошо решаете задачу. Вот. В общем,
[37:50.600 --> 37:55.880]  ну, по-моему, это в reinforcement learning типичная, типичная терминология. Тут примерно похожа
[37:55.880 --> 38:02.160]  ситуация, когда вы, вам нужно одновременно и получше локальный минимум найти, и в то же время
[38:02.160 --> 38:07.200]  не застрять в том локальный минимум, который вы только что нашли. Вот. Поэтому получается такая
[38:07.200 --> 38:11.800]  история. Теперь так, понятно ли обоснование, ну, некоторой интуиции, почему такое может
[38:11.800 --> 38:16.320]  происходить? То есть вы как бы уменьшаете шум и получается хуже. Окей. Вот. Теперь, собственно,
[38:16.320 --> 38:22.440]  про адаптивные методы, которых великое множество. Давайте я даже покажу, насколько великое множество,
[38:22.440 --> 38:28.760]  чтобы вы примерно понимали, что это не фигура речи. Вот. Я, значит, для, если кому будет интересно
[38:28.760 --> 38:34.880]  что-то с этим поэкспериментировать, я рекомендую библиотеку Optax, вот, которая сделана для джакса,
[38:34.880 --> 38:40.920]  такого вот довольно гибкого и легковесного инструментария. Вот. Ну и вот common optimizer,
[38:40.920 --> 38:45.760]  которые, собственно, адаптивные методы, основанные на хаосическом ингридиенте, они вот. То есть,
[38:45.760 --> 38:51.480]  видите, их тут, типа, ну, много. Мы сегодня кратко, абсолютно, посмотрели на вот это. Сейчас немножко
[38:51.480 --> 39:00.800]  посмотрим на, блам-блам-блам, на вот это. Вот это. Адаград. И вот это. Ну, типа, немножко устарело,
[39:00.800 --> 39:05.640]  но кое-где это до сих пор используется. Вот. Все остальное, это некоторые надстройки над этими
[39:05.640 --> 39:11.800]  базовыми методами, вот, заточенным под решение тех или иных задач из соответствующих предных
[39:11.800 --> 39:19.120]  областей. Ну, типа, ламп, это для больших языковых моделей делается. Вот. Large batch optimizer. Вот. Ну,
[39:19.120 --> 39:24.280]  а да, это же смешная статья. Короче, large batch optimizer. То есть, берем большой batch, то, что
[39:24.280 --> 39:28.480]  мы сейчас пытались сделать, для текстового моделирования трансформеров, там, и всех вот этих
[39:28.480 --> 39:34.840]  вот штуковин, да, типа, обучаем берц 7,6 минут. Вот. Когда в 7,6 минут получить супер, супер прекрасную
[39:34.840 --> 39:40.840]  точную языковую модель. Внимание. Это все прекрасно. Бла-бла-бла. Идем в эксперименты. Вот. И
[39:40.840 --> 39:48.920]  смотрим внимательно на то, что это типа 1024 TPU. То есть, это типа супер-супер топовое железо от
[39:48.920 --> 39:54.280]  Google, которое, доступ к которому есть только у него. Вот. И вот на таком вот железе они это
[39:54.280 --> 40:00.280]  параллельно, параллельно минимизировали некоторую функцию на вот таком вот, типа, тысячи, тысячи
[40:00.280 --> 40:06.760]  размер бача, и получили 7,6 минут. Вот. Ну, то есть, как бы large batch для, скажем так, не бедных, не
[40:06.760 --> 40:12.560]  бедных собственников топового железа. Вот. Короче, это я к тому, что надо внимательно, ой, надо
[40:12.560 --> 40:19.720]  внимательно читать, что пишут в соответствующих статьях, какими бы рекламными названиями они
[40:19.720 --> 40:28.400]  не оперировали. Вот. Короче, да, это вот LAMP, LARS примерно то же самое. Later Inspired. Ну да, в общем,
[40:28.400 --> 40:33.880]  неважно. Вот. Сейчас я хочу, собственно, про этом поговорить. В основе, собственно, такие вот страшные
[40:33.880 --> 40:39.480]  формулы. Вот. Идеи которых довольно простая. Смотрите. Значит, что мы знаем? У нас есть оценка на
[40:39.480 --> 40:44.680]  градиент, и мы знаем, что она шумная. Вот. Первое, что было предложено, давайте мы будем ее сглаживать
[40:44.680 --> 40:49.440]  экспоненциально. Вот. С помощью вот такой формулы. То есть, уже t это наша оценка. Вот. А nt-1,
[40:49.440 --> 40:57.240]  ну, nt-1, собственно, вектор, который сохранит сглаженное направление градиентов. Сглаженное
[40:57.240 --> 41:04.600]  по нескольким итерациям. То есть, мы как бы вносим вклад 1-β1, при этом β1 типа 0,9. То есть,
[41:04.600 --> 41:10.920]  мы довольно сильно демпфируем новую информацию и предельно актуализируем и используем по максимуму
[41:10.920 --> 41:16.400]  то, что у нас было на первых итерациях. Вот. То же самое делаем с поэлементными квадратами. То есть,
[41:16.400 --> 41:20.600]  вот эта вот штука, это типа вектор по элементный квадрат. Вот. Дальше мы это все делаем,
[41:20.600 --> 41:25.200]  масштабируем, делим на соответствующую историю. И наше направление, собственно,
[41:25.200 --> 41:30.160]  по которому мы сдвигаемся, это наш вот этот вот learning grade, который шаг, который мы сюда писали,
[41:30.160 --> 41:34.800]  альфа. Он умножается на вот этот вот вектор, который сглаженный, сглаженный, отшкалированный.
[41:34.800 --> 41:40.760]  Потом еще поэлементно делится на корень из элементных квадратов. Вот. То есть, вот m нужно
[41:40.760 --> 41:46.240]  для того, чтобы, собственно, усреднить и сглатить, а v нужно для того, чтобы, ну, вот если у вас,
[41:46.240 --> 41:53.840]  грубо говоря, есть числа типа 100 и 10 минус второй, вот чтобы вы для каждого числа двигались
[41:53.840 --> 41:57.600]  в соответствующем масштабе. И, ну, эти числа более-менее стабильны в вашем векторе. То есть,
[41:57.600 --> 42:02.920]  это некоторое выравнивание траекторий и борьба с плохой обусловленностью локальный. Понятно ли,
[42:02.920 --> 42:08.160]  из чего сделан метод? Оставьте плюс, если понятно. Окей, вижу. Вот. Ну и смотрите,
[42:08.160 --> 42:11.960]  значит, наблюдение, которое из этого сразу расследует, это то, что, в общем-то, здесь
[42:11.960 --> 42:15.960]  довольно много некоторых базовых простых конструкций. То есть, ну, типа, взвесить,
[42:15.960 --> 42:20.720]  экспоненциально сгладить, кумулировать первые моменты, вторые моменты, там, умножить туда-сюда.
[42:20.720 --> 42:25.920]  Вот. Что делает Obstax? Почему я, собственно, на его примере все это показываю? Он предлагает вам,
[42:25.920 --> 42:31.240]  так, давайте я перейду на главную страницу, он предлагает вам некоторый набор базовых
[42:31.240 --> 42:37.000]  преобразований, которые Obstax Transformations. Вот. И вот, собственно, есть тут тоже просто маленькое
[42:37.000 --> 42:42.680]  тележко, которое вы можете спокойно применять в любом порядке, к любому оптимайзеру, который,
[42:42.680 --> 42:47.320]  ну, к любому методу, который вы хотите придумать. Вы можете его строить просто комбинируя вот эти
[42:47.320 --> 42:54.040]  вот там клипы, сделать Centralize, там что-нибудь, тут еще есть Scale. Вот. Можно сделать шкалирование одним,
[42:54.040 --> 42:58.680]  шкалирование другим и как-то скомбинировать. То есть, вам выданы некоторые кирпичики, из которых
[42:58.680 --> 43:02.960]  вы можете свои какие-то методы спокойно комбинировать буквально в несколько строчек. И не надо
[43:02.960 --> 43:09.480]  заново писать все эти сложные формулы, и переживать о том, что вы запутаетесь, умножите или поделите
[43:09.480 --> 43:13.080]  не на то. Вот. Здесь уже за вас все написано. Ну, главное как бы разобраться, что это значит,
[43:13.080 --> 43:17.320]  чтобы как бы грамотно использовать. Вот. А в остальном, как бы, вот все базовые, как бы,
[43:17.320 --> 43:21.960]  все базовые ингредиенты построения устойчивых, достаточно эффективных методов тут как бы
[43:21.960 --> 43:27.480]  представлены. То есть, достаточно много работы уже сделано, сделано за вас. Вот. И если вам
[43:27.480 --> 43:32.440]  нужно будет что-то новое сочинить, новое сочинить, то вы можете взять простые блоки отсюда и получить
[43:32.440 --> 43:37.920]  соответствующие какие-то новые результаты. Вот. Поэтому это довольно, кажется, полезно и интересно.
[43:37.920 --> 43:43.000]  Нет ли, о чем я сейчас попытался, какую мысль попытался занести? Вот. Лиги понятно. Как у
[43:43.000 --> 43:49.240]  остальных дела? Так. Вроде, вроде нормально. Окей. Так. 10.13. Я что-то еще хотел рассказать. А,
[43:49.240 --> 43:54.000]  у меня же было по плану как дисперсию снижать в общем случае. Да, короче, смотрите. Общий метод,
[43:54.000 --> 43:58.720]  который как бы приводит к теоретическим некоторым полезным результатам, вот. Отключается в
[43:58.720 --> 44:03.720]  следующем. Как раз, я думаю, за 7 минут я расскажу. Мне нужно расшарить доску. Секундочку. Немножко я
[44:03.720 --> 44:09.240]  не успеваю одновременно прекращать трансляцию и начинать трансляцию. Короче, идея очень простая,
[44:09.240 --> 44:16.080]  как всегда. Если у нас есть некоторые, некоторые х-омега, которые оценивают х. То есть,
[44:16.080 --> 44:21.760]  мотор ожидания х-омеги равно х. Вот. И мы хотим уменьшить дисперсию этой оценки. Что надо
[44:21.760 --> 44:27.400]  делать? Общая некоторая методология. Вводим новую величину у, которая равняется х-омега минус у-омега.
[44:27.400 --> 44:32.720]  То есть, у-омега некоторая новая величина. Для которой справедливо, что ее мотор ожидания примерно,
[44:32.720 --> 44:38.120]  то есть, мотор ожидания от у-омега примерно х. Ну, то есть, получили ту же самую оценку,
[44:38.120 --> 44:42.720]  той же самой величины. Но если мы посмотрим на то, из чего сделана дисперсия, то мы увидим,
[44:42.720 --> 44:50.080]  что это дисперсия от х минус удвоенная кавриация и плюс дисперсия от у. Вот. Теперь, внимание.
[44:50.080 --> 44:56.520]  Если мы добьемся того, что вот эта штука будет большой, то суммарно эта вся история станет
[44:56.520 --> 45:03.000]  много меньше, чем дисперсия от ликса. Понятно ли, как это работает? То есть, вы берете некоторую
[45:03.000 --> 45:08.520]  другую случайную величину с нулевой от ожидания, которая коррелирует с случайной величиной,
[45:08.520 --> 45:13.640]  которая опроксимирует х. И вычитая ее, получаете новую случайную величину, которая также опроксимирует х,
[45:13.640 --> 45:18.160]  но имеет меньше и меньше дисперсии. Вот. Ну и, соответственно, метод, который эксплуатирует эту
[45:18.160 --> 45:23.560]  идею и который сходится также, как градиентный спустодетерминированный случай, называется САК. Это
[45:23.560 --> 45:30.280]  типа тринадцатый год. И авторы, я, конечно же, не помню. А, помню. Шмидтс и Толк, обычно. Их там
[45:30.280 --> 45:36.080]  три штуки. Вот. И идея этого метода в следующем. Вы сейчас поразитесь, насколько можно сильно,
[45:36.080 --> 45:42.600]  что называется, усреднять. Так, это буква Г. Вот. Значит, что происходит? Сначала посчитали первый
[45:42.600 --> 45:53.600]  шаг. Ну, нулевой шаг. Есть х0. Посчитали g1 равное f1 штриху х0. g2 равняется f2 штриху х0.
[45:53.600 --> 46:02.560]  И так далее. gn равное fn штриху х0. Далее. На ката-итерации. Ката-итерация. Просэмплировали
[46:02.560 --> 46:10.040]  замечательный наш индекс. Какой? Новый индекс. Вот. Да, и давайте тут я типа поставлю нолики еще
[46:10.040 --> 46:14.480]  такие вот. Которые будут обозначать номер итерации. Новый индекс просэмплировали. Посчитали.
[46:14.480 --> 46:23.920]  Получается gk ik. f штрих ik от xk. Вот. А дальше сделали следующий трюк. Казали, что xk плюс 1 это
[46:23.920 --> 46:36.360]  xk минус альфа от, внимание, значит, 1 на n g ik на ката-итерации минус 1 на n g ik на k минус первой
[46:36.360 --> 46:46.000]  итерации и плюс 1 на n сумма g it ката-итерации. Так, ну давайте разбираться в индексах. У нас
[46:46.000 --> 46:50.960]  изначально был набор векторов, каждый из которых равен соответствующему градиенту по каждому из
[46:50.960 --> 46:57.040]  слагаемых. На каждой итерации мы вычисляем какой-то один новый градиент в новой точке и используем в
[46:57.040 --> 47:04.120]  качестве нашего направления вектор, который получен усреднением вот этих вот величин, правленные
[47:04.120 --> 47:08.760]  на то, что мы посчитали новый градиент. То есть мы вычислили старое значение, прибавили новое.
[47:08.760 --> 47:13.280]  При этом все остальные значения мы не изменили. А стало ли сейчас понятнее, как устроен метод?
[47:13.280 --> 47:19.320]  Цель немножко другая. В бейдинге-то мы хотим ансамблировать, и там, по-моему, если я правильно
[47:19.320 --> 47:27.580]  помню, то там веса как-то подбираются так, чтобы мы правильную функцию заставляли более
[47:27.580 --> 47:33.920]  правильно разделять под выборки, что ли. Справьте меня, я давно это, может быть, уже плохо помню,
[47:33.920 --> 47:41.680]  давно не занимался. Здесь как бы задача в том, что мы управляем усредненный градиент в информации
[47:41.680 --> 47:48.080]  о новой точке, но эта информация передается только через одно слагаемое. А все остальное как бы
[47:48.080 --> 47:55.480]  оставить как было. И вот он как раз таки сходится, сейчас я запишу формулу, сходится, ой, жуткая
[47:55.480 --> 48:02.080]  формула на самом деле. Ну, в общем, сходится какого-то единица на к, по тому же самому
[48:02.080 --> 48:07.440]  функционалу, как мы от ожидания f, x-кат и средний, минус f со звездочкой. Вот, там, конечно, жуткие
[48:07.440 --> 48:11.840]  всякие предположения на размер шага и прочее, но не важно. В общем, при некоторых предположениях
[48:11.840 --> 48:17.280]  сходимость такая есть. Вот, теперь если соотносить то, что так, осталось пару минут, как раз я сейчас
[48:17.280 --> 48:23.000]  наверное получится показать то, что у нас тут есть x, а что у нас тут y. Вот, ну, то есть понятно,
[48:23.000 --> 48:33.480]  что вектора g, kt и kt, вот это вот, это собственно x. Может, ну, их мы от ожидания по x, который там
[48:33.480 --> 48:41.920]  типа сэмплируется от вот этой штуки. Вот так, наверное, да. И это будет градиент. Ну, надеюсь,
[48:41.920 --> 48:47.480]  что плюс-минус понятно. Если не очевидно сходу, то запишите определение того,
[48:47.480 --> 48:53.120]  что такое мы от ожидания для кси равномерного 1 на n. Ну и, в общем, вы получите ровный градиент.
[48:53.120 --> 49:01.480]  Так, сумма 1 на n, f и соответствующий f штрих и t. Вот, это x. В качестве y мы берем, получается,
[49:01.480 --> 49:10.280]  выражение вида g, kt и kt минус вот эта самая сумма. Вот, проблема в том, что мы от ожидания этой
[49:10.280 --> 49:18.080]  штуки уже не ноль. Вот, вы можете проверить почему. Но, что можно сказать про x и y по норме?
[49:18.080 --> 49:30.560]  Норме это штука g получается kt и kt минус g минус 1 и k плюс, да, и плюс, ну, там 1 на n ушло.
[49:30.560 --> 49:36.040]  Это самая сумма. Вот, и тут, значит, мы объединяем вот это в одно и это в одно и говорим, что вот
[49:36.040 --> 49:40.680]  это стремится к нулю, потому что, ну, у нас градиент стабилизируется более-менее. То есть,
[49:40.680 --> 49:46.440]  это градиент одной и той же функции, f и t, f и kt, только на разных итерациях. И приказ стремящегося к
[49:46.440 --> 49:51.760]  бесконечности, мы верим, что эти градиенты начинают, ну, у нас аргумент меняется меньше и меньше,
[49:51.760 --> 49:56.920]  поэтому у нас градиент начинается быть все ближе и ближе друг к другу. А вот эта штука стремится к нулю,
[49:56.920 --> 50:01.560]  ну, потому что это просто градиент. Просто f штрих от x. Ну, градиент, в точке минимум, стремится к нулю,
[50:01.560 --> 50:05.320]  все в порядке. Ну, или, точнее, поэтому это все вместе стремится к нулю, приказ стремится к бесконечности.
[50:05.320 --> 50:09.840]  Вот. А раз у нас, типа, норма разности стремится к нулю, то и дисперсия тоже будет к нулю стремиться.
[50:09.840 --> 50:16.920]  Вот. Что, в общем-то, мы и хотели. Понятно ли пояснение про то, как вот эта формула замечательная
[50:16.920 --> 50:23.240]  соотносится с вот тем, что было вот здесь рассказано? Ставьте, пожалуйста, плюс, если понятно,
[50:23.240 --> 50:28.040]  и минус, если не очень понятно. Ну, окей, вроде понятно. Это прекрасно. Ну, и, наверное,
[50:28.040 --> 50:36.040]  наверное, последний, совсем последний кусочек. Вот. То есть там вот у САГа есть еще куча разных,
[50:36.040 --> 50:41.520]  там, типа, это тринадцатый год. Там вот, хоть до последних, наверное, пары лет, там всякие САГ,
[50:41.520 --> 50:47.440]  СВРГ, методы МИСО, может такие аббревиатуры быть услышать где-нибудь. Это все вот основано на
[50:47.440 --> 50:53.120]  этой штуке. Вот. И я надеюсь, сразу отсюда понятно, почему эта штука на практике никогда не работает.
[50:53.120 --> 50:57.640]  Кто скажет, почему? Ну, почему, например, вот мы там видели кучу адаптивных методов,
[50:57.640 --> 51:03.440]  которые оперируют с разными шагами и прочим, а вот САГ почему-то там нигде не реализован. В чем его
[51:03.440 --> 51:09.240]  основной как бы недостаток? Вся вот теория сходится прекрасно. Что-то как-то вариантов особо не видно.
[51:09.240 --> 51:14.640]  Ну, хорошо, понятно. Смотрите, проблема в чем? Проблема в том, что вот тут вот надо хранить,
[51:14.640 --> 51:23.120]  сколько n умножить на n векторов всегда. Простите, n векторов размерности n надо хранить. Это очень много.
[51:23.120 --> 51:29.480]  Никогда никуда не поместится. Потому что вот эту сумму без хранения каждого вектора и вот эту
[51:29.480 --> 51:33.440]  вот подмену вы никогда не посчитаете. То есть агрегированно хранить нельзя. Иначе непонятно,
[51:33.440 --> 51:39.800]  что вы читаете. И вы не знаете заранее, какой индекс у вас будет просамплирован. Понятно,
[51:39.800 --> 51:47.280]  в чем минусы? Вижу два плюса. Интересно. Тот же самый. То есть мы просамплировали индекс и мы из общей
[51:47.280 --> 51:52.600]  суммы вычитаем предыдущий градиент, который для этого индекса был посчитан, добавляем новый
[51:52.600 --> 51:57.760]  посчитанный на каты, ну на текущие террации. То есть это типа старый. Тут не k-1 на самом деле,
[51:57.760 --> 52:03.080]  правильно написать. Типа тот, который был. Вот. А индексы у них, видите, и k одни и те же. То есть тут
[52:03.080 --> 52:09.760]  как бы правильно позначит. То есть это посчитанный к k-1 террации. То есть как бы то, что там в нашем банке
[52:09.760 --> 52:14.840]  векторов осталось, вот то мы используем. И как бы обмен происходит. Вот это меняется потом на вот
[52:14.840 --> 52:19.560]  это. Привычение этой суммы. Вот. Да, хороший вопрос. Спасибо за уточнение. Окей. Так, ладно,
[52:19.560 --> 52:23.440]  я хотел еще про то, как стахосичный свет оценивать, рассказать. Но, наверное, уже не успеваем. Да,
[52:23.440 --> 52:27.680]  уже 26 минут. В общем, сегодня у нас была довольно насыщенная программа. Мы поговорили про СГД.
[52:27.680 --> 52:32.840]  Показали, как он сходится для локального случая. Посмотрели на то, как дисперсия зависит от
[52:32.840 --> 52:38.640]  размера бача. Посмотрели на адаптивные методы адаптивного поиска шага. И в конце на общую
[52:38.640 --> 52:44.160]  стратегию мишени дисперсии. Вот. Надеюсь, было интересно и достаточно понятно. Вот. Следующий раз
[52:44.160 --> 52:49.520]  у нас по плану метод Ньютон и квазинтонские методы. Вот. Более того, я немножко расписал даже
[52:49.520 --> 52:56.800]  план на следующие занятия. Вот. В репетитории он лежит. Вот. Вкратце осталось не так много. И по
[52:56.800 --> 53:04.560]  плану. Следующий раз квазинтонские методы. Через раз будут методы проекции градиента и там
[53:04.560 --> 53:09.320]  максимальные методы. Потом полупределенной оптимизации. В конце закончим занятием про то,
[53:09.320 --> 53:14.920]  как устроены пакеты решения задачи по оптимизации и что вообще там возможно и как они работают. Вот.
[53:14.920 --> 53:20.080]  Это, короче говоря, еще где-то четыре занятия. И я думаю, как раз к середине декабря мы
[53:20.080 --> 53:26.440]  благополучно закончим. У вас там уже сессия начнется. Вот. И я думаю, все успешно все сдадут. Вот такой
[53:26.440 --> 53:34.080]  план. Вроде еще четыре лекции осталось. Я надеюсь, ничего не помешает их провести в таком же... Надеюсь,
[53:34.080 --> 53:38.680]  спроса людей будет больше. Может быть. Вот. И будет поинтереснее. Ладно. Всем большой
[53:38.680 --> 53:43.400]  спасибо за внимание. Если какие-то вопросы появляются, пишите в чат. Я буду на них отвечать.
[53:43.400 --> 53:49.600]  Вот. Все записи я буду выкладывать, как обычно. Слайды. Я не уверен, что буду выкладывать.
[53:49.600 --> 53:54.560]  По этой лекции там надо их сильно переделывать. В общем, я буду сейчас смотреть.

[00:00.000 --> 00:08.000]  Итак, давайте надеяться, что все будет хорошо.
[00:08.000 --> 00:14.000]  И я еще раз повторю для записи, что мы собираемся поговорить про курс в целом,
[00:14.000 --> 00:20.000]  резюмировать, что в нем происходило и как-то вычленить какую-то полезную интуицию,
[00:20.000 --> 00:24.000]  какие-то полезные общие идеи, которые с вами останутся даже после того,
[00:24.000 --> 00:30.000]  что вы видите, все конкретные детали рафтов и мультипаксов, которые вы писали.
[00:30.000 --> 00:34.000]  И еще раз скажу, что если у вас есть какие-то свои впечатления,
[00:34.000 --> 00:38.000]  делитесь ими, и мы будем вот в эту сторону двигаться.
[00:38.000 --> 00:42.000]  Напомню, что начинали мы курс с того, что вообще задавались вопросом,
[00:42.000 --> 00:46.000]  а почему мы говорим про распределенные системы?
[00:46.000 --> 00:51.000]  И ответов у нас было, кажется, два. Мы говорили про распределенность,
[00:51.000 --> 00:55.000]  потому что нас волновала отказоустойчивость, потому что отдельные машины могут отказывать,
[00:55.000 --> 01:00.000]  и потому что нас волновала масштабируемость, когда даже в отказоустойчивую,
[01:00.000 --> 01:06.000]  в надежную машину, в группу машин, все данные не помещаются.
[01:06.000 --> 01:10.000]  Ну и давайте и про то, и про другое подробно поговорим.
[01:10.000 --> 01:14.000]  С чем мы начнем? Наверное, с отказоустойчивости.
[01:14.000 --> 01:20.000]  Вот у нас были протоколы консенсуса. Мы начинали с такого наивного алгоритма,
[01:20.000 --> 01:27.000]  который реплицировал ячеек в памяти, и увидели, что даже наивный алгоритм,
[01:27.000 --> 01:34.000]  пишем на 2 из 3, читаем с 2 из 3, сам по себе он не давал нужной нам гарантии линеризуемости.
[01:34.000 --> 01:40.000]  Про линеризуемость чуть позже. Вот даже в такой модели мы увидели, что...
[01:40.000 --> 01:46.000]  Нет, нельзя позже, конечно. Линеризуемость сейчас...
[01:46.000 --> 01:50.000]  Нет, не хочу про линеризуемость. Хочу лишь сказать, что у нас был протокол
[01:50.000 --> 01:56.000]  двухфазной АБД, который позволял нам работать с ячейкой памяти,
[01:56.000 --> 01:59.000]  как с отказоустойчивой сущностью на трех машинах.
[01:59.000 --> 02:03.000]  Но при этом мы столкнулись с проблемой, что этот протокол не обобщается
[02:03.000 --> 02:09.000]  на более сложные операции. Мы не смогли сделать операцию CAS.
[02:09.000 --> 02:15.000]  И проблема была в том, что разные реплики получали, апдейты все в эту ячейку
[02:15.000 --> 02:19.000]  были версионируемые, разные реплики получали их в разном порядке.
[02:19.000 --> 02:25.000]  И мы выдумали идею атомных бродкаста, выдумали такой примитив,
[02:25.000 --> 02:29.000]  а дальше заметили, что если мы умеем решать задачу консенсуса,
[02:29.000 --> 02:32.000]  если мы можем договориться об общем выборе, то мы можем с помощью консенсуса
[02:32.000 --> 02:36.000]  договориться про порядок доставки сообщений и таким образом
[02:36.000 --> 02:41.000]  линеризуемо реплицировать произвольное состояние, произвольный автомат.
[02:41.000 --> 02:45.000]  И можно было бы думать, что задача консенсуса про упорядочивание.
[02:45.000 --> 02:49.000]  Давайте сегодня так думать не будем и в будущем так думать не будем,
[02:49.000 --> 02:54.000]  потому что смысл, конечно, не в этом. У нас когда-то была статья
[02:54.000 --> 03:01.000]  про GFS, Google File System. И там тоже возникала задача упорядочивать что-то.
[03:01.000 --> 03:06.000]  Файловая система имела довольно странный по нынешним меркам,
[03:06.000 --> 03:10.000]  распределённая файловая система умела перезаписывать фрагменты файлов.
[03:10.000 --> 03:13.000]  Я надеюсь, вы понимаете, перезаписывать фрагменты файлов не нужно.
[03:13.000 --> 03:16.000]  Нужно уметь делать аппендо, потому что поверх аппендов
[03:16.000 --> 03:21.000]  через райтах от логи, через LSM мы можем сделать более-менее произвольное состояние.
[03:21.000 --> 03:26.000]  Так что сейчас бы мы такую файловую систему делать не стали.
[03:26.000 --> 03:29.000]  И когда мы говорили про колосус, я обращал внимание, что в колосусе
[03:29.000 --> 03:33.000]  в современном GFS операции перезаписи нет, но тогда она была.
[03:33.000 --> 03:38.000]  И для того, чтобы упорядочивать записи в одни и те же фрагменты файлов,
[03:38.000 --> 03:41.000]  в одни и те же чанки, на которые эти файлы дробились,
[03:41.000 --> 03:45.000]  среди реплик каждого чанка выбиралась реплика Primary.
[03:45.000 --> 03:51.000]  Но вот эта Primary, она брала у мастера Лизу, то есть блокировку временную.
[03:51.000 --> 03:56.000]  Про это отдельная история, почему блокировки называют Лизами в распределённых системах.
[03:56.000 --> 04:01.000]  Брала такую блокировку на время, арендовала её, чтобы было корректнее сказать,
[04:01.000 --> 04:06.000]  и получая записи от клиентов, выстраивала их в некотором порядке
[04:06.000 --> 04:10.000]  и применяла, расславала другим репликом.
[04:10.000 --> 04:14.000]  Задача упорядочивания прекрасно решается одним узлом.
[04:14.000 --> 04:18.000]  Но в конце концов, кто-то один выстраивает порядок.
[04:18.000 --> 04:22.000]  И в GFS это был один узел.
[04:22.000 --> 04:27.000]  И, конечно же, мы знаем, что такой дизайн не работает.
[04:27.000 --> 04:31.000]  Но это уже немного другая история, скорее про RAFT и про Multi-Paxis,
[04:31.000 --> 04:36.000]  про то, что вроде бы мы долго-долго с вами делали какие-то протоколы,
[04:36.000 --> 04:40.000]  долго-долго выдумывали в сети Atomic Broadcast, наивный мультипакс,
[04:40.000 --> 04:43.000]  потом строили Multi-Paxis, потом его оптимизировали,
[04:43.000 --> 04:46.000]  и в конце концов получали очень наивный протокол,
[04:46.000 --> 04:49.000]  где лидер получал команду от клиента, рассылал её на большинство,
[04:49.000 --> 04:53.000]  получал подтверждение, и всё, и на этом всё заканчивалось.
[04:53.000 --> 04:58.000]  Сейчас я две истории про то, что консенсус,
[04:58.000 --> 05:02.000]  про упорядочивание и про то, что RAFT очень наивный.
[05:02.000 --> 05:06.000]  Я хочу объединить примерно в одну историю и сказать, что, конечно же,
[05:06.000 --> 05:12.000]  и то, и другое не так, потому что упорядочивает в конце концов только один узел.
[05:12.000 --> 05:17.000]  Но, конечно же, вот такого наивного дизайна недостаточно.
[05:17.000 --> 05:22.000]  То есть RAFT выглядит довольно наивно на быстром пути, когда всё стабильно.
[05:22.000 --> 05:27.000]  Но RAFT и Multi-Paxis, и Singularity-Paxis, и все вот эти протоколы,
[05:27.000 --> 05:33.000]  и задачи консенсуса нужны не для того, чтобы упорядочивать что-то,
[05:33.000 --> 05:37.000]  а для того, чтобы пережить случаи, когда вот такой Primary умирает.
[05:37.000 --> 05:43.000]  Сложность настоящая в этом. Сложность в том, когда даже не совсем может быть точно.
[05:43.000 --> 05:47.000]  Самая большая сложность не в том, что Primary умирает,
[05:47.000 --> 05:50.000]  а в том, что Primary на самом деле не умирает.
[05:50.000 --> 05:53.000]  Но его лиза отбирается и даётся другому узлу.
[05:53.000 --> 05:57.000]  Иначе говоря, у нас с системой появляются несколько лидеров.
[05:57.000 --> 06:04.000]  Так что смысл консенсуса, это, конечно же, не фиксация порядка,
[06:04.000 --> 06:07.000]  это на самом деле конкуренция лидеров.
[06:07.000 --> 06:13.000]  И Algorithm-Singularity-Paxis – это как раз такой изолированный случай конкуренции этих лидеров.
[06:13.000 --> 06:18.000]  И вот дуэль пропоузеров, которые там возникают, когда они перебивают друг друга,
[06:18.000 --> 06:25.000]  реализация FLP-тиаремы – это конкуренция, которая не может завершиться никогда.
[06:25.000 --> 06:29.000]  Суть консенсуса в этом. Да, он может принимать разные формы,
[06:29.000 --> 06:33.000]  но фундаментальная сложность не в том, чтобы отказ пережить,
[06:33.000 --> 06:37.000]  а в том, чтобы пережить конкуренцию, когда отказа нет.
[06:37.000 --> 06:41.000]  Разумеется, мы не можем отличить медленный узел от отказавшего,
[06:41.000 --> 06:44.000]  но про это FLP-тиарема и была, так и доказывалось.
[06:44.000 --> 06:47.000]  И поэтому мы вынуждены с этой проблемой бороться.
[06:47.000 --> 06:50.000]  Вот если мы говорим про распределённые блокировки,
[06:50.000 --> 06:55.000]  то там проблема же точно такая же.
[06:55.000 --> 07:02.000]  Мы не можем гарантировать, что распределённая блокировка гарантирует нам взаимные исключения.
[07:02.000 --> 07:07.000]  Если мы делаем lock-сервис распределённый, если клиент может к нему прийти,
[07:07.000 --> 07:14.000]  а потом мы клиентом можем отказать, то сервис блокировок должен нашу блокировку изъять.
[07:14.000 --> 07:17.000]  Но опять же, он нас изъял, не потому что мы отказали,
[07:17.000 --> 07:21.000]  а потому что у нас случилась пауза из-за сборки мусора.
[07:21.000 --> 07:23.000]  И снова получилась конкуренция.
[07:23.000 --> 07:27.000]  Снова мы клиенты оба думаем, что мы владеем блокировкой,
[07:27.000 --> 07:32.000]  потому что этот клиент получил подтверждение от сервиса блокировок,
[07:32.000 --> 07:37.000]  а этот клиент всё ещё думает, что он владеет блокировкой, хотя сервис уже думает иначе.
[07:37.000 --> 07:42.000]  То есть точка, где всё понятно есть, но отдельные клиенты запутались.
[07:42.000 --> 07:45.000]  И снова конкуренция.
[07:45.000 --> 07:47.000]  Сложность всегда в этом.
[07:47.000 --> 07:52.000]  Сложность в том, что нельзя гарантировать взаимные исключения, нельзя гарантировать в системе...
[07:52.000 --> 07:58.000]  Немного аккуратнее, конечно, можно гарантировать, что одновременно лидера одного нет с помощью часов,
[07:58.000 --> 08:03.000]  но, кажется, мы доказывали с вами на первой лекции ещё, что часы синхронизировать нельзя надёжно,
[08:03.000 --> 08:07.000]  поэтому полагаться на них в общем случае не безопасно,
[08:07.000 --> 08:11.000]  а просто на основе коммуникации гарантировать, что лидеры не пересекаются во времени, уже нельзя.
[08:11.000 --> 08:15.000]  Поэтому мы весь этот курс живём в предположении, что лидеры конкурируют,
[08:15.000 --> 08:18.000]  и они конкурируют внутри паксоса, они конкурируют внутри...
[08:18.000 --> 08:22.000]  Они просто бай-дизайн конкурируют внутри мульти паксоса наивного,
[08:22.000 --> 08:25.000]  они могут конкурировать внутри рафта,
[08:25.000 --> 08:32.000]  а сервис распределённых блокировок, в конце концов, используется пользователем для того, чтобы выбирать лидеров в своих системах.
[08:32.000 --> 08:35.000]  Я в параллельном курсе рассказывал немного про кафку,
[08:35.000 --> 08:40.000]  когда в кафке лидер выбирается с помощью Зукипера.
[08:40.000 --> 08:44.000]  В общем, это всё неизбежная проблема, и она должна как-то решаться,
[08:44.000 --> 08:48.000]  и мы видели, как она решается в синхронизации крипаксоса.
[08:48.000 --> 08:52.000]  Это был довольно странный, непонятный протокол, но когда мы начали его разворачивать,
[08:52.000 --> 09:01.000]  оптимизировать и строить поверх него лог, то мы увидели, что все странные N, NP облетают некоторый смысл.
[09:01.000 --> 09:07.000]  И мы увидели, что лидер всегда связан с некоторой эпохой.
[09:07.000 --> 09:12.000]  Синхронизация крипаксоса – это был просто N, какой-то ballot number непонятный.
[09:12.000 --> 09:16.000]  Но когда мы сделали мультипаксос, мы увидели, что этот N – это просто эпоха этого лидера.
[09:16.000 --> 09:23.000]  И даже если лидеры конкурируют, чтобы блокировать старых лидеров,
[09:23.000 --> 09:29.000]  нужно привязывать их к эпохам и просто запоминать, какую эпоху старшую мы слышали.
[09:29.000 --> 09:34.000]  Так было и в RAF, так было и в сервисе блокировок.
[09:34.000 --> 09:43.000]  В любом сервисе блокировок требуется, чтобы этот сервис вместе с блокировкой давал нам
[09:43.000 --> 09:47.000]  еще и некоторый токен, с помощью которого какая-то третья система,
[09:47.000 --> 09:52.000]  которая получает команды от двух наших конкурирующих клиентов,
[09:52.000 --> 09:58.000]  понимала бы, что этот клиент новый, а этот клиент уже старый, и блокировку передали другому.
[09:58.000 --> 10:03.000]  То есть принцип один и тот же. Мы связываем эпохи лидеров, и с помощью этих эпох
[10:03.000 --> 10:05.000]  можем блокировать старых.
[10:05.000 --> 10:12.000]  Но чтобы все аккуратно исключало друг друга, мы используем еще и Quora.
[10:12.000 --> 10:19.000]  С мультипаксосом, вообще с консенсусом, была связана еще одна история,
[10:19.000 --> 10:24.000]  а именно, как мы эти протоколы разрабатывали.
[10:24.000 --> 10:28.000]  Но вот в курсе был рассказан некоторый неоптимальный,
[10:28.000 --> 10:32.000]  выбран намеренный неоптимальный путь. Мы сначала говорили про синглы декрипаксос,
[10:32.000 --> 10:37.000]  потом про мультипаксос, потом про RAF. В принципе, ничто не мешало бы начать именно с RAF.
[10:37.000 --> 10:43.000]  Ну и скажем, в MIT так и делают. То есть рассказывают просто про RAF, про мультипаксос перестали.
[10:43.000 --> 10:52.000]  Но мне кажется, что с точки зрения разработки, может быть,
[10:52.000 --> 10:57.000]  эффективнее было бы только про RAF и сразу про RAF.
[10:57.000 --> 11:02.000]  Но есть два аргумента против такого. Во-первых, авторы RAF утверждают,
[11:02.000 --> 11:06.000]  что алгоритм проще, чем паксос, и про паксос знать в принципе не нужно.
[11:06.000 --> 11:11.000]  Я же пытался до вас донести мысль, что он не проще, чем мультипаксос.
[11:11.000 --> 11:16.000]  Это просто такое логическое продолжение. Мы с вами говорим про задачу консенсуса изолированную,
[11:16.000 --> 11:20.000]  потом мы говорим про наивный мультипаксос, где мы просто выстраиваем параллельные
[11:20.000 --> 11:24.000]  эти паксосы отдельные, а потом мы выдумываем некоторые оптимизации,
[11:24.000 --> 11:30.000]  а именно выбор лидера и конвейер. Но просто их выдумываем, понимаем,
[11:30.000 --> 11:34.000]  что их хорошо бы встроить сюда. Примерно понимаем, как их можно было бы встроить,
[11:34.000 --> 11:40.000]  а потом уже говорим про RAF, как про мультипаксос, в котором все это строено, подогнано
[11:40.000 --> 11:46.000]  и работает корректно. То есть это просто скорее еще одна ступень эволюции.
[11:46.000 --> 11:52.000]  В мультипаксосе и в RAF-те слишком много похожих идей. Опять лидеры, привязанные к эпохам,
[11:52.000 --> 11:59.000]  блокировка старых лидеров по этим самым эпохам.
[11:59.000 --> 12:04.000]  Теоремы доказываются более-менее так же. И вообще все теоремы в курсе
[12:04.000 --> 12:10.000]  доказываются примерно похожим образом, и видимо просто алгоритм паксос слишком
[12:10.000 --> 12:16.000]  фундаментарен для этой области, и по-другому сделать нельзя.
[12:16.000 --> 12:22.000]  Ну и еще одна причина, по которой в курсе есть паксос наравне с RAF-том,
[12:22.000 --> 12:29.000]  потому что мне кажется, что реализуя мультипаксос, вы увидели, что проще не проще
[12:29.000 --> 12:34.000]  можно сравнивать в разных смыслах. Можно бы оптимизировать метрику, насколько
[12:34.000 --> 12:39.000]  просто его написать, а можно оптимизировать другую метрику, насколько, скажем,
[12:39.000 --> 12:45.000]  код получается простой и модульный. Ну и если вы, скажем, писали паксос
[12:45.000 --> 12:48.000]  и там писали выборы лидера в нем, ну что-то более-менее нетривиальное,
[12:48.000 --> 12:53.000]  то вы должны были увидеть, что сам протокол просто хорошо декомпозируется.
[12:53.000 --> 12:57.000]  Вы можете писать логику репликации логой, особо не думая про то,
[12:57.000 --> 13:03.000]  как устроен концентр, это может быть почти что черный ящик.
[13:03.000 --> 13:06.000]  В RAF-те, если вы пишете код, то вы в конце концов остаетесь таким вот классом
[13:06.000 --> 13:10.000]  на 600 строк, и в нем очень плохо отделяются какие-то части,
[13:10.000 --> 13:16.000]  и это, наверное, сложнее отлаживать. Ну не знаю, отлаживать проще сложнее не уверен,
[13:16.000 --> 13:21.000]  но думать об этом коде большом монолитном сложнее. То есть, с одной стороны,
[13:21.000 --> 13:26.000]  в RAF-те сплавливаются все евристики, но они сплавливаются в такой вот монолит,
[13:26.000 --> 13:34.000]  и это некоторая проблема. И можно было бы подумать, что это так мир устроен,
[13:34.000 --> 13:38.000]  что если вы хотите сделать что-то оптимальное, то нужно делать что-то такое
[13:38.000 --> 13:43.000]  монолитное и сложное. Но вот оказывается, что нет. Я один раз про это рассказывал,
[13:43.000 --> 13:51.000]  у нас был небольшой семинар про это, я коротко рассказал, что это пошло не так.
[13:51.000 --> 14:03.000]  Очень обидно. Я рассказывал про эту идею, что в Facebook делали свой сервис координации,
[14:03.000 --> 14:12.000]  свой Зукипер, и они как раз заботились о том, как сделать систему максимально модульной.
[14:12.000 --> 14:16.000]  То есть, она должна быть эффективной, разумеется, и должна быть модульной.
[14:16.000 --> 14:24.000]  И они все-таки придумали альтернативную декомпозицию, которая позволяла делать консенсус,
[14:24.000 --> 14:29.000]  то есть реплицировать лог так же эффективно, как RAF, но при этом написать код модульно.
[14:29.000 --> 14:35.000]  Обстракции были выдуманы другие, то есть декомпозиция не по слотам, а по сегментам лога
[14:35.000 --> 14:40.000]  в другом измерении, но декомпозиция все же другая. Там выделялось понятие лог,
[14:40.000 --> 14:46.000]  это такой фрагмент лога, и это был компонент, который отвечал за этот кусочек лога,
[14:46.000 --> 14:53.000]  он был отказоустойчивый, но не сильно. Это три реплики, среди них есть лидер,
[14:53.000 --> 14:58.000]  но лидер просто прибит гвоздями, он не может переехать на другую машину.
[14:58.000 --> 15:04.000]  И вот этот логвит был очень простой, там лидер получал команду, писал ее в свой лог,
[15:04.000 --> 15:09.000]  реплицировал на кворум, получал подтверждение, то есть вот такой наивный протокол.
[15:09.000 --> 15:14.000]  Но у наивного протокола, как мы обсудили уже раньше, проблема, он не отказоустойчивый,
[15:14.000 --> 15:20.000]  он не может справиться со сбоем лидера. Но вот этот протокол, этот логвит, этот модуль,
[15:20.000 --> 15:24.000]  он не решал такую задачу, он отвечал именно за упорядочивание.
[15:24.000 --> 15:28.000]  Поэтому я говорю, что консенсус для упорядочивания не нужен.
[15:28.000 --> 15:34.000]  Вот этого компонента требовалось просто уметь запечататься, остановиться и не принимать новые команды.
[15:34.000 --> 15:40.000]  Вот если он это делать умел, то поверх этого логвита можно было сделать вот непосредственно
[15:40.000 --> 15:50.000]  уже распределенный лог, реплицированный, и его можно сделать было с помощью как раз мультипаксиса.
[15:50.000 --> 15:57.000]  Вот на этом уровне, то есть на уровне упорядочивания, консенсус не нужен.
[15:57.000 --> 16:02.000]  Консенсус нужен только для того, чтобы склеивать вот эти фрагменты друг с другом.
[16:02.000 --> 16:12.000]  И оказалось, что для того, чтобы склеивать вот эти логвиты, склеивать эти сегменты достаточно,
[16:13.000 --> 16:23.000]  достаточно написать вот просто обычный неоптимизированный мультипаксус наивный,
[16:23.000 --> 16:29.000]  где в каждом слоте свой независимый двухфазный паксус даже без конвейера.
[16:29.000 --> 16:35.000]  И на этом можно построить эффективный продакшен.
[16:35.000 --> 16:40.000]  Так что, может быть, рафт это и не самая оптимальная идея.
[16:40.000 --> 16:51.000]  Кроме того, рафт, помимо того, что монолитный, он еще и мешает некой оптимизации в пакса встраивать.
[16:51.000 --> 16:56.000]  Например, я на одной из лекций рассказывал вам про лог-девайс,
[16:56.000 --> 17:02.000]  про систему в Фейсбуке, которая реплицирует логи, то есть там не то чтобы автомат какой-то, там просто лог.
[17:02.000 --> 17:05.000]  Сам лог – это есть данные.
[17:05.000 --> 17:10.000]  И там была идея такая, что можно использовать более гибкую систему кворумов,
[17:10.000 --> 17:16.000]  но вообще оказывается, что кворумы разных фаз должны пересекаться в паксусе, а кворумы одной фазы могут не пересекаться.
[17:16.000 --> 17:21.000]  Поэтому, скажем, можно реплицировать одну запись в первом слоте лога на один набор аксепторов,
[17:21.000 --> 17:30.000]  другую запись на другой набор аксепторов, потому что так можно увеличить пропускную способность.
[17:30.000 --> 17:34.000]  Ну и получится что-то более эффективное на запись.
[17:34.000 --> 17:42.000]  Но при этом в RAFT такая ауристика же не пройдет, потому что RAFT требует, чтобы когда вы добавляете что-то в лог,
[17:42.000 --> 17:50.000]  если вы лидер в RAFT добавляете в лог реплики какие-то записи,
[17:50.000 --> 17:56.000]  то реплика проверяет, что префикс лога сходится у вас и у лидера.
[17:56.000 --> 18:00.000]  Для этого нужно иметь весь префикс, а здесь префикса никакого нет.
[18:00.000 --> 18:06.000]  Здесь разные реплики, разные аксепторы хранят разные наборы данных, и просто мы их в merger склеиваем.
[18:06.000 --> 18:14.000]  Можно придумывать разные ауристики, и оказывается, что в мультипаксусе фреймворк эти ауристики встраиваются,
[18:14.000 --> 18:19.000]  а в RAFT все подогнано, и там что-то тюнить становится сложнее.
[18:19.000 --> 18:25.000]  Ну или, скажем, у вас завтра на зачете будет вопрос, можно ли переконфигурации из паксуса применять в RAFT?
[18:25.000 --> 18:29.000]  Вот он протокол, когда мы коммисили переконфигурацию просто в лог.
[18:29.000 --> 18:37.000]  Вот в RAFT это тоже нельзя делать почему-то, хотя казалось бы, примерно похожая идея должна быть.
[18:37.000 --> 18:46.000]  Ну в общем, RAFT гораздо менее гибкий, он быстрый, но можно сделать также быстрый, при этом модульный и более кастомизируемый.
[18:46.000 --> 18:54.000]  Так что мне кажется, что и то, и другое знать полезно.
[18:54.000 --> 18:59.000]  Ну и в конце концов много продакшн на свете написано именно через мультипаксус.
[18:59.000 --> 19:06.000]  Ну по крайней мере его использует Google, потому что исторически его использует Amazon, потому что, видимо, он им больше нравится.
[19:06.000 --> 19:15.000]  Ну а Open Source системы чаще всего используют RAFT, потому что есть Open Source реализация, мы ее копируем к себе, и мы счастливы.
[19:15.000 --> 19:20.000]  У нас вопрос про мультипаксус на таблите в этом случае.
[19:20.000 --> 19:29.000]  А таблит в этом случае, когда мы говорим о мультипаксусе, про усиление, про прочную способность, что несколько лидеров, если у нас очень мало клиентов, мы используем таблит?
[19:29.000 --> 19:35.000]  Подожди, таблит, у нас слово появлялось в контексте масштабирования, про которое мы сейчас еще не говорим, не успели.
[19:35.000 --> 19:44.000]  Это было шардирование, мы брали кивалию хранилища, брали таблицы, делили их по строкам на какие-то части.
[19:44.000 --> 19:52.000]  И сказали, что за каждую часть таблицы отвечает свой набор реплик. В биктейбле это называлось таблит.
[19:52.000 --> 19:56.000]  Да, про другое, про что тогда?
[19:56.000 --> 20:06.000]  Я говорил, что в мультипаксусе у нас есть одна маленькая проблема с лидером, то, что мы добавляемся к конкретности, но в то же время у нас огромное количество таблет прямо от нас.
[20:06.000 --> 20:16.000]  Но это про масштабируемость разговора, мы сейчас про другой уровень говорим, мы говорим про консенсус.
[20:16.000 --> 20:31.000]  И последнее, я раз уж упомянул про MIT, что они рафт перестали учить, но с другой стороны у них есть проблемы, потому что рафт, хоть в рафте заявляют, что дизайн проще и понятнее,
[20:31.000 --> 20:40.000]  но если вы читали статью и писали код, то вы знаете, что там есть некоторый обман, что вроде бы говорят, что есть выборы лидера независимые, есть репликация независимая, потом они друг о другу начинают зависеть.
[20:40.000 --> 20:50.000]  И в мультипаксусе можно написать отдельный модуль выбора лидера абсолютно произвольным образом, а в рафте так сделать нельзя, потому что там нужно выбирать лидера очень аккуратно с полным логом.
[20:50.000 --> 20:59.000]  В общем, некоторый обман есть, но это не то чтобы обман, и вообще сравнивать не нужно, нужно вынести мысль, что рафт это оптимизированный мультипаксус,
[20:59.000 --> 21:10.000]  а мультипаксус это скорее фреймворк, где есть такой базовый ядросинг, и его уже можно по-разному разворачивать, по-разному декомпозировать и добавлять разные наборы лидер.
[21:10.000 --> 21:22.000]  И в контексте этого GFS, с чего я начинал свой пример, что консенсус про конкуренцию лидеров, не про упорядочивание.
[21:23.000 --> 21:31.000]  Примеры с Facebook, это как раз пример, где упорядочивание есть, а консенсуса нет, то есть они на разных уровнях.
[21:31.000 --> 21:43.000]  Сложность в конкуренции лидеров, и поэтому в любом протоколе консенсуса ядро связано с тем, чтобы мы привязывали лидеров к эпохам и аккуратно эти эпохи обновляли.
[21:43.000 --> 21:49.000]  В общем, два неразрывных понятия, в любом протоколе они будут.
[21:49.000 --> 21:54.000]  Что еще можно про репликацию сказать?
[21:54.000 --> 22:07.000]  Наверное, тот подход, который мы к репликации выбрали, логи и упорядочивание всего, он был связан с тем, чего мы ожидали от распределенной системы.
[22:07.000 --> 22:13.000]  Мы в смысле клиенты. Я говорил вам в первом занятии, что про клиента вообще важно думать.
[22:13.000 --> 22:20.000]  Клиенты тоже часть системы. Почему клиенты часть системы? Потому что они тоже отказывают, как и узлы.
[22:20.000 --> 22:28.000]  Если вы вспомните блокировки распределенные, почему у нас везде не локи, а лизы?
[22:28.000 --> 22:33.000]  Потому что клиенты отказывают, и мы не можем им отдать блокировку во владение.
[22:33.000 --> 22:39.000]  И ровно из-за этого получается конкуренция неизбежная, которую мы героически преодолеваем.
[22:39.000 --> 22:46.000]  Какие еще примеры отказов клиентов? Если вы вспомните лекцию про спандера, про распределенные транзакции,
[22:46.000 --> 22:51.000]  то там же был двухфазный комит, который должен кто-то координировать.
[22:51.000 --> 22:57.000]  И клиент, если умирал в середине двухфазного комита, то протокол блокировался.
[22:57.000 --> 23:02.000]  Это сложная интуиция совсем.
[23:02.000 --> 23:08.000]  Этим была посвящена лекция про детерминированные транзакции.
[23:08.000 --> 23:14.000]  Там был разговор о том, что двухфазный комит вообще транзакциям не нужен, он менее фундаментальный, чем ПАКСС.
[23:14.000 --> 23:19.000]  И от него можно избавиться, если сделать транзакции не интерактивными.
[23:19.000 --> 23:22.000]  Потому что интерактивная транзакция – это транзакция, в которой пользователь может умереть,
[23:22.000 --> 23:27.000]  которая ее выполняет. И транзакция может в середине откатиться из-за смерти пользователя.
[23:27.000 --> 23:33.000]  Если мы избавимся от интерактивности, если мы потребуем, чтобы клиент отправил транзакцию целиком в системе,
[23:33.000 --> 23:37.000]  то можно сделать что-то более эффективное.
[23:37.000 --> 23:42.000]  В общем, про это тоже нужно думать. Нужно думать, потому что у клиентов есть ретраи.
[23:42.000 --> 23:47.000]  И дисциплина не помогает, потому что соединение разрывается, гарантии пропадают внутри дисциплины соединения.
[23:47.000 --> 23:51.000]  И причем ретраи – это, конечно, могут быть на одну машину, могут быть на разные машины.
[23:51.000 --> 23:55.000]  Поэтому если мы хотим в нашем РСМе, в нашем РАФте, успешить экзоклюанс,
[23:55.000 --> 24:01.000]  то мы должны попросить клиента помочь нам. Пусть он выдумывает идентификаторы.
[24:01.000 --> 24:06.000]  В сервисе блокировок клиент еще пинги отправлял.
[24:06.000 --> 24:12.000]  Ну, короче, клиент – полноценный участник, а еще клиент наблюдает поведение нашей системы.
[24:12.000 --> 24:18.000]  И вот это был разговор. Мы заявили его в самом начале курса, он был нам важен.
[24:18.000 --> 24:22.000]  И ровно поэтому мы дальше говорили именно про упорядочивание всего подряд.
[24:22.000 --> 24:28.000]  Потому что клиенты могли работать с нашей системой конкурентно
[24:28.000 --> 24:33.000]  и не хотели в свою очередь думать, что эта система распределенная.
[24:33.000 --> 24:38.000]  Было бы удобно, если бы клиенты думали про систему, где там тысяча узлов,
[24:38.000 --> 24:44.000]  как просто про один бесконечно большой, бесконечно емкий, бесконечно надежный компьютер.
[24:44.000 --> 24:49.000]  И мы эти ожидания сформулировали в виде модели согласованности,
[24:49.000 --> 24:54.000]  которая называлась линейализуемость. Мы говорили, что клиенты работают с системой конкурентно,
[24:54.000 --> 25:00.000]  отправляют туда записи и чтения и ожидают, что эти записи и чтения исполнятся
[25:00.000 --> 25:05.000]  как будто бы в некотором глобальном порядке. Причем этот порядок будет уважать
[25:05.000 --> 25:10.000]  предшествование операции в реальном времени. Если запись завершилась до начала другой записи,
[25:10.000 --> 25:15.000]  то в системе вторая запись все-таки останется в конце концов, первая перетрется.
[25:15.000 --> 25:22.000]  И вот эту гарантию мы обеспечивали как? Ну, просто выстраивали внутри системы
[25:22.000 --> 25:27.000]  все команды пользователей в порядке с помощью консенсуса,
[25:27.000 --> 25:31.000]  с помощью мультипакса, сарафта, всех протоколов, которые у нас были.
[25:31.000 --> 25:37.000]  Помните ли вы, почему мы требуем, почему мы вообще ожидаем, мы клиенты от системы,
[25:37.000 --> 25:42.000]  что она будет учитывать предшествование операции в реальном времени?
[25:42.000 --> 25:47.000]  Потому что это ведь странно, ведь мы не можем наблюдать это время реальное.
[25:47.000 --> 25:51.000]  У нас есть часы, а часы неточные, синхронизировать их нельзя.
[25:51.000 --> 25:56.000]  А при этом мы от системы требуем.
[25:56.000 --> 26:02.000]  Мы на самом деле от системы хотим, чтобы она не время уважала, а heavens before наша.
[26:02.000 --> 26:07.000]  То есть если мы сделали запись, получили подтверждение от системы,
[26:07.000 --> 26:10.000]  сказали другому клиенту, что он сделал чтение, то он ожидает, конечно, что он увидит запись,
[26:10.000 --> 26:16.000]  потому что он знает, что запись предшествовала его чтению, потому что ему об этом сказал другой клиент.
[26:16.000 --> 26:20.000]  У нас причина есть между чтением и записью, между двумя операциями.
[26:20.000 --> 26:24.000]  Но эта причина реализуется не внутри системы, она реализуется снаружи,
[26:24.000 --> 26:27.000]  и система про эту причину ничего не знает.
[26:27.000 --> 26:31.000]  Поэтому мы на месте системы делаем так. Мы консервативно предполагаем,
[26:31.000 --> 26:38.000]  что если две операции у порядочного времени, то между ними могло быть heavens before,
[26:38.000 --> 26:43.000]  и значит клиент мог чего-то ожидать. Поэтому мы вот пытаемся этой гарантии достичь.
[26:48.000 --> 26:52.000]  Что еще про клиентов? Не знаю, про клиентов, наверное, все.
[26:52.000 --> 26:57.000]  Впрочем, то, что мы внутри системы выстраиваем все в одном порядке,
[26:57.000 --> 27:01.000]  все операции, чтобы реплики у нас не расходились, это же не единственный способ,
[27:01.000 --> 27:05.000]  как можно задачу решать, задачу репликации в смысле.
[27:05.000 --> 27:09.000]  Мы про это, правда, не поговорили совсем, но есть альтернативный подход,
[27:09.000 --> 27:15.000]  который состоит в том, что мы просто реплицируем не произвольное состояние,
[27:15.000 --> 27:22.000]  а некоторое специальное состояние, которое более устойчиво к реодолингу операций.
[27:22.000 --> 27:26.000]  Но вот если две операции коммутируют, то не нужно их особо упорядочивать,
[27:26.000 --> 27:30.000]  потому что их можно слить в конце концов и получить одно и то же состояние на разных репликах.
[27:30.000 --> 27:37.000]  Вот это называется CRDT. Конечно, там гарантии более слабые, но принцип совершенно другой.
[27:37.000 --> 27:41.000]  То есть мы на уровне структуры данных справляемся с тем,
[27:41.000 --> 27:46.000]  что операции на разных репликах приходят в разном порядке, просто разные под наборы операций.
[27:46.000 --> 27:52.000]  Нам важно, мы хотим достичь не линеризуемости, мы хотим достичь eventual consistency,
[27:52.000 --> 27:56.000]  то есть чтобы в конце концов, когда все апдейты становятся,
[27:56.000 --> 28:00.000]  реплики сошлись к одному и тому же состоянию.
[28:00.000 --> 28:05.000]  Вот если у нас структура данных, это, например, множество растущее или счетчик,
[28:05.000 --> 28:10.000]  то это можно сделать гораздо проще без рафта, без мультипаксов, без консенс.
[28:10.000 --> 28:20.000]  Смотри, он используется... Google Doc это такой подход.
[28:20.000 --> 28:24.000]  То есть это совсем другая задача. Не то чтобы у нас гигантская система, много клиентов,
[28:24.000 --> 28:28.000]  но смысл примерно такой же. Вот у нас есть консенсус,
[28:28.000 --> 28:33.000]  и консенсус говорит нам, что требует от нас, что в случае partition
[28:33.000 --> 28:37.000]  наша система должна в одной из половин partition блокироваться.
[28:37.000 --> 28:41.000]  То есть если мы оказывались в меньшей части partition, то апдейты останавливаются.
[28:41.000 --> 28:45.000]  А теперь другая задача. У тебя мобильное приложение, у тебя много клиентов,
[28:45.000 --> 28:50.000]  и клиент заходит в метро, и при этом у него связи нет с остальными,
[28:50.000 --> 28:55.000]  но при этом он хочет свое состояние менять, просто локально в Google Doc что-то трогать.
[28:55.000 --> 29:01.000]  Google Doc это, строго говоря, не CRDT, но пример подходящий к лаборативной редактировании текста.
[29:01.000 --> 29:05.000]  Так вот, ты остался в изоляции, ты отдельная такая реплика,
[29:05.000 --> 29:09.000]  и при этом ты в себе можешь аккумулировать апдейты, которые другие не видят.
[29:09.000 --> 29:13.000]  Потом ты вернешься в сеть, раздашь эти апдейты, и они как-то смогут
[29:13.000 --> 29:17.000]  с другими локальными апдейтами других пользователей смерзаться.
[29:17.000 --> 29:23.000]  То есть здесь скорее оппозиция такая. Изабел, твой вопрос, прости, я в свою сторону ушел.
[29:23.000 --> 29:29.000]  Что?
[29:29.000 --> 29:32.000]  Ну, конфликты могут возникать, но они как-то разрешаются.
[29:32.000 --> 29:37.000]  Вот если у тебя счетчик, то какие тут конфликты? Там плюс, тут плюс, и вот они слились.
[29:37.000 --> 29:42.000]  Если обзац вставил, а в другой пользователь удалил,
[29:42.000 --> 29:46.000]  то если можно аккуратно скомбинировать разумно, то это так будет сделано.
[29:46.000 --> 29:50.000]  Если нельзя, то получится, возможно, какая-то ерунда.
[29:50.000 --> 29:54.000]  Ну, собственно, CRDT отличается тем, то есть люди в этой области пытаются
[29:54.000 --> 30:00.000]  брать более сложные структуры данных и при этом заставлять их вести себя разумно при таких конфликтах.
[30:00.000 --> 30:05.000]  Не то чтобы эти цели всегда совместны, но какие-то усилия предполагаются.
[30:10.000 --> 30:15.000]  Ну вот, это такой вот подход опять связанный с репликацией.
[30:15.000 --> 30:19.000]  С репликацией у нас был еще один, мне кажется, важный сюжет.
[30:19.000 --> 30:24.000]  Он уже не про алгоритмы, а про дизайн.
[30:25.000 --> 30:31.000]  Если мы хотим сделать что-то отказоустойчивое, что-то маленькое, но отказоустойчивое,
[30:31.000 --> 30:37.000]  мы берем три реплики, кладем на три диска этих реплик копии и состояния
[30:37.000 --> 30:42.000]  и реплицируем их с помощью мультипаксиса, рафта, что вам больше нравится.
[30:42.000 --> 30:48.000]  Ну вот, мы с вами видели, когда мы говорили про Bigtable, когда мы говорили про масштабируемость,
[30:48.000 --> 30:52.000]  мы наблюдали другой подход.
[31:03.000 --> 31:11.000]  Мы использовали другой подход, а именно вместо того, чтобы работать поверх
[31:11.000 --> 31:17.000]  трех ненадежных файловых систем с ненадежными дисками
[31:17.000 --> 31:25.000]  и реплицировать состояние через рафт, мы могли работать поверх одной файловой системы,
[31:25.000 --> 31:29.000]  зато ненадежной.
[31:29.000 --> 31:33.000]  И в обоих случаях, что в рафте, что в RSM, что в таком дизайне,
[31:33.000 --> 31:37.000]  вот этот был дизайн таблета Bigtable, то есть кусочка таблицы,
[31:37.000 --> 31:43.000]  который позволял по произвольному ключу писать и читать,
[31:43.000 --> 31:48.000]  ну и реализовывал LSM поверх распределенной файловой системы.
[31:48.000 --> 31:53.000]  В общем, в чем параллель? У нас есть три levelDB на трех дисках,
[31:53.000 --> 32:03.000]  которые реплицируются с помощью мультипаксиса, или у нас есть один экземпляр этого LSM-дерева,
[32:03.000 --> 32:09.000]  один levelDB, поверх распределенной файловой системы, отказоустойчивой.
[32:09.000 --> 32:13.000]  Немного другой дизайн, и чем он отличается?
[32:13.000 --> 32:19.000]  Во-первых, тем, что такому дизайну не нужны три точки обслуживания.
[32:19.000 --> 32:24.000]  У нас в рафте или мультипаксисе три реплики или пять реплик, какое-то количество,
[32:24.000 --> 32:32.000]  и они в памяти хранят все состояние, потому что они готовы переключиться стать лидером.
[32:32.000 --> 32:37.000]  Вот здесь, в таком дизайне, у нас есть слой хранилища,
[32:37.000 --> 32:41.000]  а точка обслуживания, то есть машина, которая хранит memtable,
[32:41.000 --> 32:47.000]  которая принимает пут и геты и обслуживает пользователей, она одна для таблета.
[32:47.000 --> 32:54.000]  Если эта машина отказывает, то просто точка обслуживания переезжает на другую физическую машину,
[32:54.000 --> 32:59.000]  но данные не теряет, потому что они в подсистеме хранения лежат.
[32:59.000 --> 33:03.000]  В случае, если машина умирает в рафте, то она умирает с дисками,
[33:03.000 --> 33:07.000]  но, к счастью, у других дисков есть кое-какое состояние.
[33:07.000 --> 33:14.000]  Здесь мы хранение абстрагируем от обслуживания и уделяем в отдельные подсистемы.
[33:14.000 --> 33:23.000]  Это очень разумный дизайн, потому что он позволяет смотреть на машины физически иначе.
[33:23.000 --> 33:28.000]  В наивном подходе у вас отдельные машины – это буквально узлы системы.
[33:28.000 --> 33:36.000]  Узл системы тождественны некоторой машине. В таком дизайне узел системы – это набор ресурсов.
[33:36.000 --> 33:41.000]  Это дисковые емкости и это процессор.
[33:41.000 --> 33:47.000]  И одна и та же машина может быть частью системы хранения и частью системы, которая обслуживает пользователей.
[33:47.000 --> 33:53.000]  Но скажем, у вас одна и та же машина может быть частью колосуса и хранить там чанки файлов,
[33:53.000 --> 33:58.000]  и может быть частью спаннера, который этот колосус использует для записи в конце концов.
[33:58.000 --> 34:08.000]  И что очень любопытно, в таком дизайне вы упрощаете себе репликацию.
[34:08.000 --> 34:15.000]  Потому что если вы реплицируете буквально сложные мутабельные состояния, то вам нужен рафт.
[34:15.000 --> 34:26.000]  Но в таком дизайне, где у вас репликация скрыта внутри GFS, вот здесь у вас эсэстейблы и мутабельные.
[34:26.000 --> 34:35.000]  И у вас здесь таблет-лог, но если приложение бачит записи в него, он растет такими большими порциями, большими кусками.
[34:35.000 --> 34:40.000]  И в итоге у вас здесь такие имутабельные чанки добавляются к этому файлу.
[34:40.000 --> 34:45.000]  Здесь вот просто большие статичные файлы из статичных имутабельных чанков.
[34:45.000 --> 34:49.000]  И имутабельные данные реплицировать гораздо проще.
[34:49.000 --> 34:56.000]  Для этого не нужен рафт, и для этого можно добиться гораздо более эффективного хранения.
[34:56.000 --> 35:01.000]  В рафте мы используем X3-репликацию, то есть три копии.
[35:01.000 --> 35:07.000]  Состояние сложное. Здесь данные имутабельные, поэтому мы можем взять чанк эсэстейбла,
[35:07.000 --> 35:14.000]  или даже чанк лога вот этого, побить его на кусочки, на шесть кусочков,
[35:14.000 --> 35:20.000]  посчитать на них для этих шести кусков чексуммы с помощью кода Фридита Соломона.
[35:20.000 --> 35:26.000]  И дальше вот 9 фрагментов, 9 блоков рассыпать по кластеру.
[35:26.000 --> 35:32.000]  И теперь мы готовы в такой схеме пережить, во-первых, мы готовы пережить три отказа диска,
[35:32.000 --> 35:35.000]  а раньше были только два готовы пережить.
[35:35.000 --> 35:41.000]  А во-вторых, мы вместо тройной репликации используем полуторную.
[35:41.000 --> 35:44.000]  То есть мы сэкономили себе половину дисков.
[35:44.000 --> 35:51.000]  Если у нас данные хранятся на миллионах машин, то мы сэкономили себе миллионы дисков.
[35:51.000 --> 35:57.000]  Это прям очень-очень большие деньги.
[35:57.000 --> 36:02.000]  И вот в таком дизайне есть ещё одна интересная особенность.
[36:02.000 --> 36:06.000]  Есть ли у меня готовая ссылка? Да, есть.
[36:06.000 --> 36:11.000]  Вот такой дизайн был использован, когда мы говорили про распилённые транзакции,
[36:11.000 --> 36:16.000]  у нас были детерминированные транзакции, где как раз мы избавлялись от двухфазного комита.
[36:16.000 --> 36:23.000]  И я рассказывал про детерминированные транзакции на примере системы Яндекс.ДБ.
[36:23.000 --> 36:34.000]  И в Яндекс.ДБ система выстроена из сущности, которая называется таблет.
[36:34.000 --> 36:39.000]  Это не тот же таблет, что был Bigtable, чтобы мы ещё больше не запутались.
[36:39.000 --> 36:45.000]  Суть такая. Система состоит из акторов, которые что-то делают, общаются друг с другом.
[36:45.000 --> 36:50.000]  Есть акторы, которые отвечают за таблицы пользователей, есть служебные и системные акторы.
[36:50.000 --> 37:00.000]  Но эти акторы отказоустойчивые. Отказоустойчивый актор в этой системе называется таблеткой.
[37:00.000 --> 37:06.000]  Это актор, который хранит своё состояние в подсистеме хранения данных.
[37:06.000 --> 37:09.000]  В отдельном blob storage он называется.
[37:09.000 --> 37:16.000]  И смотрите, вот здесь это физическое воплощение этого актора, это некоторая одна машина.
[37:16.000 --> 37:27.000]  Вот он там запускается и работает. Если он отказывает, то обслуживание данных этого актора возрождается на другой машине.
[37:27.000 --> 37:38.000]  Так вот, что есть ещё любопытного? Можно заметить, что нам не нужно в таком дизайне...
[37:38.000 --> 37:48.000]  Сейчас я найду нужный слайд. Нам не нужно в таком дизайне очень-очень много паксусов, очень много консенсусов.
[37:48.000 --> 37:59.000]  Если вы вспомните, кто слушал про кавку, то там был такой дизайн, что был один зукипер, который делал консенсус.
[37:59.000 --> 38:03.000]  И он был на всю кавку разом.
[38:03.000 --> 38:07.000]  Вот здесь та же самая идея.
[38:07.000 --> 38:16.000]  Для того, чтобы заводить очень-очень много отказоустойчивых акторов, нам не нужно для каждого из них делать независимый консенсус.
[38:16.000 --> 38:21.000]  Нам нужен консенсус только в одном месте, при конкуренции.
[38:21.000 --> 38:27.000]  Вот у нас старый экземпляр машины умирает, мы возрождаем актора на другой машине.
[38:27.000 --> 38:31.000]  Но ещё раз, он там возрождается, а старый на самом деле не умер, просто он залип.
[38:31.000 --> 38:34.000]  И в итоге у нас два инстанса одного и того же актора.
[38:34.000 --> 38:38.000]  И два этих инстанса меняют одно и то же состояние.
[38:38.000 --> 38:42.000]  И снова мы должны связать их с эпохами.
[38:42.000 --> 38:50.000]  И под система хранения данных она, с одной стороны, отвечает за хранение логов этих акторов, чтобы обеспечить персистентное состояние.
[38:50.000 --> 38:56.000]  А с другой стороны, она с помощью операции BLOCK решает проблему конкуренции.
[38:56.000 --> 39:00.000]  Каждый актор возрождается в некотором поколении.
[39:00.000 --> 39:05.000]  И мы можем просто заблокировать все предшествующие поколения.
[39:05.000 --> 39:12.000]  И нам нужен консенсус только на этом уровне, под капотом этой системы, вот здесь.
[39:12.000 --> 39:20.000]  А сверху у нас уже модель другая, где отдельные узлы, которые отказывают, мы их переселяем на другие машины.
[39:20.000 --> 39:29.000]  В конце концов, консенсус нужен и снова для конкуренции, снова не для упорядочивания.
[39:29.000 --> 39:33.000]  Это важный дизайн, сейчас он используется много где.
[39:33.000 --> 39:42.000]  Какие-то системы его не используют, потому что этот дизайн требует больше аккуратности, требует большего количества абстракции под систем.
[39:42.000 --> 39:46.000]  Но я бы сказал, что он, кажется, более современен.
[39:46.000 --> 39:52.000]  И когда вы просто арендуете виртуалку в облаке, то в конце концов вы же тоже не заботитесь о том, что она откажет.
[39:52.000 --> 39:57.000]  Конечно же, она откажет, потому что она на каком-то физическом узле живет.
[39:57.000 --> 40:02.000]  Но диск в вашей виртуалке, он, конечно же, не локальный.
[40:02.000 --> 40:06.000]  Этот диск это, на самом деле, некоторая абстракция.
[40:06.000 --> 40:11.000]  И когда вы пишете туда блоки какие-то, то они отправляются в условный колоссус, и там хранятся.
[40:11.000 --> 40:16.000]  Там как раз сырые же коды и, скорее всего, не потеряются.
[40:20.000 --> 40:22.000]  Что еще про репликацию нужно сказать?
[40:22.000 --> 40:27.000]  Мы использовали репликацию для отказа устойчивости,
[40:27.000 --> 40:32.000]  и основной рабочий инструмент у нас при реализации отказа устойчивости был кворумы.
[40:32.000 --> 40:34.000]  Мы их придумали на вторую лекцию.
[40:34.000 --> 40:36.000]  Для этого вторая лекция была нужна.
[40:36.000 --> 40:38.000]  Про ABD, про регистр.
[40:38.000 --> 40:42.000]  Не потому, что это полезно, а потому, что там возникло упорядочивание, временные метки,
[40:42.000 --> 40:44.000]  и потому, что там возникли кворумы.
[40:44.000 --> 40:46.000]  И поначалу кворумы были очень простые.
[40:46.000 --> 40:50.000]  Это были два узла из трех, ну или просто большинство узлов.
[40:50.000 --> 40:55.000]  А дальше мы в течение курса эти кворумы усложняли.
[40:55.000 --> 41:04.000]  В какой-то момент мы заметили, что можно использовать разные кворумы для разных фаз паксосе,
[41:04.000 --> 41:08.000]  и что некоторые фазы частые, репликация на вторая фаза,
[41:08.000 --> 41:10.000]  а некоторые фазы редкие выбора лидера.
[41:10.000 --> 41:15.000]  Поэтому мы можем организовать реплики в такой прямоугольник
[41:15.000 --> 41:20.000]  и сказать, что у нас кворум для одной фазы это строчка для другой столбец.
[41:20.000 --> 41:27.000]  Можно здесь немного все тюнить, чтобы получить большую скорость.
[41:27.000 --> 41:34.000]  Мы заметили, что кворумы могли бы учитывать разные домены отказов.
[41:34.000 --> 41:42.000]  У нас с одной стороны кворум большинства защищает отказы меньшинства узлов,
[41:42.000 --> 41:47.000]  но при этом, когда вы строите георазпределённую систему, то у вас больше проблем,
[41:47.000 --> 41:52.000]  потому что вы должны думать не только про отказы узлов, а про отказы дата-центров.
[41:52.000 --> 41:56.000]  И вы строите систему кворумов, которая бы, скажем, учитывала,
[41:56.000 --> 42:02.000]  что у вас могут выходить из строя машины или целиком дата-центры,
[42:02.000 --> 42:08.000]  или целиком даже группы регионов, вы можете потерять с ними связанность.
[42:08.000 --> 42:11.000]  То есть кворумы здесь усложнялись.
[42:11.000 --> 42:15.000]  Они ещё больше усложнились, когда мы перешли византийскую модель.
[42:15.000 --> 42:18.000]  И там появилось два новых типа кворумов.
[42:18.000 --> 42:21.000]  Кворумы, которые назывались маскирующие и которые назывались...
[42:21.000 --> 42:24.000]  Где-то у меня здесь статья была.
[42:24.000 --> 42:29.000]  Маскирующие кворумы и диссеминатинг. Такое странное название, диссеминатинг кворума.
[42:31.000 --> 42:34.000]  Ладно, я потерял.
[42:42.000 --> 42:46.000]  Вот, маскирующие кворумы.
[42:46.000 --> 42:50.000]  Идея их состояла в том, что наши кворумы теперь пересекаются не по одному узлу,
[42:50.000 --> 42:55.000]  потому что он может быть нечестным, а по большому количеству, а именно 2f+,1.
[42:55.000 --> 43:02.000]  Почему 2f+,1? Потому что в пересечении есть точно узлы, которые знают свежую версию чего-нибудь.
[43:02.000 --> 43:07.000]  Но непонятно, как их отделить от нечестных, которые просто выдумывают что-то.
[43:07.000 --> 43:12.000]  И мы брали толстые пересечения, потому что в этом пересечении
[43:12.000 --> 43:17.000]  обязательно честных узлов было больше, чем нечестных, f+,1 против f.
[43:17.000 --> 43:23.000]  И таким образом мы сначала могли отсечь ответы просто по количеству,
[43:23.000 --> 43:29.000]  потребовать, чтобы одинаково было f+,1, а потом уже отсечь по версии.
[43:29.000 --> 43:39.000]  И вторая идея была в том, что система кворумов пересекалась по f+,1 узлу, то есть по одному честному.
[43:39.000 --> 43:43.000]  Это работало, когда данные были подписаны.
[43:43.000 --> 43:47.000]  То есть византийская реплика не могла их подделать, могла только скрыть какие-то.
[43:47.000 --> 43:54.000]  Ну вот в PBFT это пример. Мы пересекаем кворумы на выбор лидера смены эпохи и репликацию.
[43:54.000 --> 43:59.000]  Какие-то византийские узлы могут соврать, что у них транзакций нет,
[43:59.000 --> 44:05.000]  а честный узел обязательно предложит сертификат и докажет новым правилам, что именно его стоит послушать.
[44:05.000 --> 44:11.000]  А когда мы перешли к permissionless-системам, то стало еще сложнее,
[44:11.000 --> 44:17.000]  потому что раньше у нас кворумы измерялись в голосах 3 из 5, 4 из 7.
[44:17.000 --> 44:21.000]  Когда мы перешли в permissionless-модель, то число реплик больше неизвестно,
[44:21.000 --> 44:25.000]  византийские узлы могут порождать свои византийские копии,
[44:25.000 --> 44:31.000]  поэтому мы кворумы стали измерять не в голосах, не в участниках, а в процессорах,
[44:31.000 --> 44:39.000]  ну или другой популярный способ в деньгах, то есть в ресурсах, которые сложно подделать.
[44:39.000 --> 44:47.000]  Ну можно, не знаю, в дисковой емкости измерять кворумы, то есть такие подходы тоже есть, их много разных.
[44:47.000 --> 44:54.000]  Ну что ж, про репликацию я не знаю, мне кажется, что это все самое важное, что в голову приходит.
[44:54.000 --> 45:00.000]  Наверное, на этом уровне можно еще сказать, что мы должны заботиться не просто про...
[45:00.000 --> 45:06.000]  Мы отказаустойчивости добиваемся не только тем, что мы используем там эти кворумы, алгоритмы,
[45:06.000 --> 45:10.000]  мы должны подумать еще про отказаустойчивость, но просто на отдельных узлах,
[45:10.000 --> 45:14.000]  потому что если, скажем, ваша реплика записала на диск снапшот,
[45:14.000 --> 45:17.000]  потом диск немножко покорраптил этот снапшот, испортил какие-то байтики,
[45:17.000 --> 45:26.000]  а потом вы скопировали эти байтики по сети на другую машину, то, кажется, у вас теперь три неправильные реплики разломанные.
[45:26.000 --> 45:32.000]  Поэтому даже если вы используете репликацию, нужно быть очень аккуратным, что вы локализуете какие-то локальные...
[45:32.000 --> 45:35.000]  Вы не распространяете какие-то локальные избои.
[45:35.000 --> 45:41.000]  Вот проблема с дисками, проблема с процессором, проблема с проводами,
[45:41.000 --> 45:48.000]  вот вы о всем этом с памятью должны думать. Вы должны рассчитывать, что в памяти перерачиваются биты,
[45:48.000 --> 45:57.000]  вы должны думать, что диск портит данные, вы должны считать, что доверять чексумам TCP, IP и Ethernet всем трём разом нельзя.
[45:57.000 --> 46:01.000]  Если вы им доверяете, то в большом масштабе, ну, может быть, если у вас три реплики,
[46:01.000 --> 46:04.000]  то, скорее всего, вы, может быть, с этим не столкнетесь.
[46:04.000 --> 46:13.000]  Если у вас 100 тысяч РСМов, 100 тысяч ПАКСов на десятках, сотнях тысяч узлов,
[46:13.000 --> 46:22.000]  то в таком масштабе, в масштабе больших кустров любые редкие избои случаются более-менее регулярно.
[46:22.000 --> 46:27.000]  Поэтому вы должны и на них внимание тоже обращать.
[46:27.000 --> 46:32.000]  Ну, про это у нас тоже был отдельный семинар, но это, не знаю, как с файловой системой аккуратно работать.
[46:32.000 --> 46:39.000]  Вот все ваши обещания, которые вы даёте в ПАКСовстве, в РАФте, в любом протоколе, вы должны хранить их вечно, вы не должны забывать их.
[46:39.000 --> 46:45.000]  Для этого вы пишете их на диск, и нужно аккуратно заботиться, что вы сделали F-Sync, что вы сделали F-Sync в директории,
[46:45.000 --> 46:50.000]  что вы сделали Write a headlock, который может потранкетиться, потому что апент неатомарный.
[46:50.000 --> 46:54.000]  Ну, короче, миллион проблем инженерных уже, которые могут влиять.
[46:54.000 --> 46:57.000]  Когда вы пишете однопоточное приложение, то у вас всё просто.
[46:57.000 --> 47:02.000]  Если машина сломалась, то значит приложение больше не работает, думать о чём не нужно.
[47:02.000 --> 47:11.000]  Вот сейчас мы должны думать буквально о том, что в любой момент между любыми двумя инструкциями машина может перезагрузиться,
[47:11.000 --> 47:14.000]  что-то потерять, и мы должны восстановиться из-за того состояния.
[47:14.000 --> 47:21.000]  Это требует большого усердия. Я бы сказал, есть такое мнение, Максим Багенко его озвучивал в каком-то интервью,
[47:21.000 --> 47:24.000]  что хорошо брать на работу в инфраструктуру перфекционистов,
[47:24.000 --> 47:31.000]  потому что перфекционист понимает, что всё может пойти не так и к этому готовиться.
[47:31.000 --> 47:35.000]  Тут не работает подход, ну как-нибудь всё там принесёт. Нет.
[47:35.000 --> 47:43.000]  Если что-то может случиться, то в большом масштабе оно обязательно случится. Нужно к этому готовиться.
[47:43.000 --> 47:47.000]  Бог с ней с репликации надоело, давайте поговорим про дизайн.
[47:47.000 --> 47:53.000]  Это была вторая причина, по которой мы используем распределённость, потому что данные не вмещаются всё в одну машину.
[47:53.000 --> 48:02.000]  Ну и дальше мы говорим, что не беда, разрежем данные на части, и тогда заведём много отдельных.
[48:02.000 --> 48:07.000]  Возьмём много отказоустойчивых акторов, отдельных машин, которые на самом деле РСМ,
[48:07.000 --> 48:11.000]  и раздадим им кусочки данных, и тогда всё поместится.
[48:11.000 --> 48:16.000]  Иногда это не работает, потому что непонятно, как полезны кусочки.
[48:16.000 --> 48:25.000]  Мы говорили про файловые системы, и там был уровень данных, то есть дерево, iNode, дерево плохо режется на кусочки.
[48:25.000 --> 48:32.000]  А даже если хорошо режется, потому что это не дерево, а таблица просто, то даже в таком дизайне у нас возникали проблемы.
[48:32.000 --> 48:40.000]  У нас было занятие, где мы обсуждали, как масштабировать кейвалию хранилища и масштабировать файловую систему.
[48:40.000 --> 48:46.000]  И та и другая задача сводилась к тому, чтобы найти узкое место в дизайне.
[48:46.000 --> 48:50.000]  То есть то место, которое не может горизонтально масштабироваться, то место, в которое мы упираемся.
[48:50.000 --> 48:53.000]  И оба раза это были метаданные.
[48:53.000 --> 49:01.000]  Ну вот пусть у нас есть сотни тысяч машин, пусть мы разрезали наши данные на...
[49:05.000 --> 49:08.000]  Открою картинку.
[49:10.000 --> 49:14.000]  Пусть у нас есть тысячи машин, пусть у нас есть огромное количество таблиц.
[49:14.000 --> 49:20.000]  Мы нарезали эти таблицы по ключам на диапазоны, раздали их разным машинам,
[49:20.000 --> 49:26.000]  и теперь эти машины реплицируют эти диапазоны с помощью мультипаксиса.
[49:26.000 --> 49:33.000]  Этого было мало, потому что когда к вам приходит клиент, вы должны же его направить в одну из машин,
[49:33.000 --> 49:35.000]  которая является лидером для соответствующего рейнджа.
[49:35.000 --> 49:43.000]  Для этого нужно просто хранить метаданные, отображение из диапазонов ключей в набор реплик.
[49:44.000 --> 49:50.000]  Несложно сделать это отказоустойчиво, то есть выбрать группу машин, и пусть она хранит.
[49:50.000 --> 49:57.000]  Беда в том, что эта группа реплик, этот RSM, который хранит отображение, может переполниться,
[49:57.000 --> 50:02.000]  если у вас машин слишком много, диапазонов слишком много.
[50:03.000 --> 50:09.000]  Это узкое место. В файловой системе это мастер, который хранит чанки iNode.
[50:09.000 --> 50:15.000]  iNode здесь это мастер, который хранит карту кластера, где что лежит.
[50:15.000 --> 50:20.000]  Проблемы одинаковые, и вроде бы машины можно добавлять, данных можно хранить больше,
[50:20.000 --> 50:24.000]  а вот метаданные упираются в емкость одного диска.
[50:24.000 --> 50:28.000]  И мы решали эти проблемы довольно занятным способом.
[50:28.000 --> 50:34.000]  Мне кажется, что он занятный. В Bigtable как мы это делали?
[50:34.000 --> 50:38.000]  Мы сказали, что вот это отображение, сам бекты будут кивали у хранилища,
[50:38.000 --> 50:41.000]  и наши метаданные это тоже отображение исключили значение.
[50:41.000 --> 50:45.000]  Но почему бы не использовать самих себя для того, чтобы хранить эти данные?
[50:45.000 --> 50:54.000]  И мы сделали таблицу метаданных, где хранится для каждого таблета пользователя,
[50:54.000 --> 50:59.000]  для каждого диапазона ключей, какими машинами он обслуживается.
[50:59.000 --> 51:07.000]  Но вот беда. Чтобы эту таблицу читать, нужно найти таблет, который обслуживает таблет метадаты.
[51:07.000 --> 51:13.000]  Для этого сделали ещё один уровень иерархии, второй уровень метаданных.
[51:13.000 --> 51:18.000]  То есть в этом таблете было написано для каждого служебного таблета,
[51:18.000 --> 51:22.000]  какие машины его обслуживают, и узнав эти машины, можно было пойти в эту таблицу,
[51:22.000 --> 51:26.000]  прочитать из неё и пойти уже наконец обслужить пользователя.
[51:26.000 --> 51:32.000]  Но опять та же самая проблема. Как же найти узлы, которые обслуживают этот корневой таблет?
[51:32.000 --> 51:39.000]  Идея тут в том, что мы с каждым этим уровнем косвенности уменьшали объем метаданных.
[51:39.000 --> 51:43.000]  И в какой-то момент, но после двух опов, их становится настолько мало,
[51:43.000 --> 51:46.000]  что они просто умещаются в один RSM.
[51:46.000 --> 51:52.000]  Причём этот RSM, этот Chabi назывался системой, можно было использовать для разных систем.
[51:52.000 --> 51:55.000]  Для разных инсталляций Bigtable.
[51:55.000 --> 52:01.000]  Или использовать его и для Bigtable, и для Colossus, который у нас позже в лекции возникал.
[52:01.000 --> 52:06.000]  То есть ZooKeeper — это же система, которая нужна не для какой-то конкретной системы,
[52:06.000 --> 52:12.000]  а ZooKeeper может использоваться разными системами, разными инсталляциями одной системы,
[52:12.000 --> 52:16.000]  просто в принципе разными системами. Вы там можете Kafka использовать в ClickHouse,
[52:16.000 --> 52:19.000]  они будут использовать один и тот же ZooKeeper.
[52:19.000 --> 52:23.000]  Вот здесь, в конце концов, мы сошлись к такому Chabi, это Chabi 1,
[52:23.000 --> 52:27.000]  и мы здесь снова переиспользуем консенсус.
[52:27.000 --> 52:31.000]  Как я раньше говорил, что мы переиспользуем только на уровне хранения данных,
[52:31.000 --> 52:34.000]  а отдельные акторы не используют консенсус.
[52:34.000 --> 52:38.000]  Но вот здесь та же самая идея.
[52:38.000 --> 52:44.000]  У нас здесь консенсуса меньше, потому что он есть в Chabi, и он есть...
[52:44.000 --> 52:48.000]  Простите.
[52:48.000 --> 52:52.000]  Где-то вот под капотом этого GFS тоже что-то есть.
[53:08.000 --> 53:12.000]  У нас есть таблица, мы хотим в конце концов прочесть из этой таблицы
[53:12.000 --> 53:16.000]  какой-то ключ, но мы не знаем, какие машины его обслуживают.
[53:16.000 --> 53:22.000]  Для этого у нас есть методанные. В этой таблице написано, кто обслуживает твой ключ.
[53:22.000 --> 53:27.000]  Но чтобы обратиться к этой таблице, ты должен знать, кто обслуживает чтение этой таблицы.
[53:27.000 --> 53:31.000]  Поэтому ты отступаешь еще на шаг назад.
[53:31.000 --> 53:37.000]  К чему я этот пример показываю? К тому, что система может хранить свои собственные методанные
[53:37.000 --> 53:41.000]  и масштабировать. Если система может масштабированно хранить таблицы,
[53:41.000 --> 53:45.000]  то если вы завели свои методанные в виде таблицы, то почему бы ее в себя не поместить?
[53:45.000 --> 53:51.000]  И вот эта идея ценно как эти идеи придумывать.
[53:51.000 --> 53:59.000]  Этот пример иллюстрирует нам такую общую технику, что в дизайне систем распределенных
[53:59.000 --> 54:06.000]  не то чтобы какие-то новые и альтернативные всему алгоритмы,
[54:06.000 --> 54:12.000]  вот консенсус — это вот про конкарнси, а систем дизайн — он про какие-то рецепты,
[54:12.000 --> 54:16.000]  которые на самом деле уже известны людям.
[54:16.000 --> 54:22.000]  И конкретно в Bigtable мы видим, что принципы, которые уже были придуманы гораздо раньше,
[54:22.000 --> 54:27.000]  чем Bigtable, и в контексте просто одной машины, можно переиспользовать большим масштабе
[54:27.000 --> 54:31.000]  в распределенных системах. Но просто одни и те же инженерные идеи.
[54:31.000 --> 54:36.000]  Вот в самом деле эта же конструкция — это таблица страниц. У нас есть виртуальная память,
[54:36.000 --> 54:43.000]  в ней есть страницы. Страницы есть данными пользователя, а есть служебные страницы,
[54:43.000 --> 54:47.000]  которые хранят адреса других страниц. Это, собственно, page table.
[54:47.000 --> 54:52.000]  И вот этот page table — это вот некоторые бор фиксированной глубины.
[54:52.000 --> 54:55.000]  И вот этот бор фиксированной глубины.
[54:55.000 --> 54:59.000]  И также, если этот бор переполнится, в смысле слишком много листьев станет,
[54:59.000 --> 55:04.000]  то мы сможем достроить еще один уровень иерархии, и все снова станет масштабироваться.
[55:04.000 --> 55:09.000]  Так же, как и в виртуальной памяти. У вас есть поинтеры, там 64 бита,
[55:09.000 --> 55:14.000]  используется только 48, потому что таблица страниц имеет четыре уровня.
[55:14.000 --> 55:19.000]  Если памяти станет слишком много, мы сделаем пятый уровень и будем использовать еще 8 бит.
[55:19.000 --> 55:24.000]  Ну и точно так же, как в процессоре, который ищет таблицу страниц через регистр,
[55:24.000 --> 55:27.000]  у нас есть чаби в виде такого отдельного регистра.
[55:27.000 --> 55:33.000]  И так же, как в виртуальной памяти, процессор каждый раз по этой таблице страниц не ходит.
[55:33.000 --> 55:38.000]  Если бы он делал вместо одного чтения логического 4 физических, все бы тормозило.
[55:38.000 --> 55:42.000]  Он просто кэширует то, к чему он обращается часто.
[55:42.000 --> 55:48.000]  Поэтому клиент в такой системе также точно кэширует адреса таблитов со своими данными
[55:48.000 --> 55:51.000]  и просто скипывает поход по этой иерархии.
[55:51.000 --> 55:57.000]  Если его отображение устарело, ну просто он перечитает это все один раз, обновит свой кэш.
[55:57.000 --> 56:02.000]  То есть идея хорошо известная, мы чувствуем, что здесь та же самая проблема,
[56:02.000 --> 56:06.000]  поэтому переиспользуем ее. Или скажем, вот LSM-дерево, оно же известно,
[56:06.000 --> 56:09.000]  его придумали гораздо раньше, чем Bigtable.
[56:09.000 --> 56:15.000]  А потом просто решили, а почему бы не построить этот LSM не поверх локальной файловой системы,
[56:15.000 --> 56:17.000]  а поверх распределенной файловой системы?
[56:17.000 --> 56:25.000]  Просто декрутив консенсус, который нужен для того, чтобы с конкуренцией точек обслуживания справляться.
[56:27.000 --> 56:32.000]  То есть вот такие идеи, кажется, полезно знать, как устроен компьютер,
[56:32.000 --> 56:37.000]  потому что потом эти идеи можно переиспользовать в гораздо большем масштабе.
[56:37.000 --> 56:41.000]  Но в конце концов atomic broadcast это же идея опять из кэшей.
[56:41.000 --> 56:45.000]  То есть у нас есть ядра, там есть своя память, и эти ядра с памятью связаны
[56:45.000 --> 56:50.000]  с помощью шины данных, которые упорядочивают все транзакции, которые выполняются этими кэшами.
[56:50.000 --> 56:53.000]  То есть в большом масштабе все то же самое, что и в маленьком.
[56:53.000 --> 56:56.000]  В принципе ничего нового, в смысле дизайна.
[56:56.000 --> 56:59.000]  Ну ладно, иногда все-таки новое придумывают.
[56:59.000 --> 57:02.000]  Детерминированные транзакции – это довольно неожиданное изобретение,
[57:02.000 --> 57:09.000]  которое придумали в 2013 году, и не то чтобы раньше так умели.
[57:09.000 --> 57:13.000]  Долгое время ничего, кроме двухфазного комита, не было.
[57:15.000 --> 57:23.000]  И еще, мне кажется, очень классный пример на дизайн был про колосус.
[57:23.000 --> 57:26.000]  Как бы нам найти эти слайды сейчас?
[57:33.000 --> 57:40.000]  Очень жаль, что про колосус мало что известно, есть только эти маленькие короткие слайды неподробные.
[57:40.000 --> 57:44.000]  Но мне кажется, что я в лекции достаточно детально рассказал
[57:44.000 --> 57:48.000]  о каком уровне, на котором это возможно, про то, как масштабируется колосус.
[57:48.000 --> 57:56.000]  Потому что с ним возникла дополнительная трудность, а именно в колосусе нельзя...
[57:56.000 --> 57:58.000]  Да, простите, я перепрыгнул.
[57:58.000 --> 58:01.000]  Мы научились масштабировать key value, потом мы масштабировали файловую систему,
[58:01.000 --> 58:05.000]  переложив данные в key value.
[58:05.000 --> 58:10.000]  А потом мы столкнулись с проблемой, что мы хотим масштабировать файловую систему через key value,
[58:10.000 --> 58:13.000]  но если мы гугл, то наша key value строится поверх файловой системы,
[58:13.000 --> 58:15.000]  и у нас получается такой цикл.
[58:15.000 --> 58:20.000]  Но мы этот цикл заменили рекурсией, потому что мы брали большую файловую систему
[58:20.000 --> 58:26.000]  и хранили данные в метаданной этой системе Bigtable,
[58:26.000 --> 58:30.000]  который работал с чуть меньшей файловой системой,
[58:30.000 --> 58:34.000]  которая хранила метаданные в Bigtable, который работал с ещё более маленькой файловой системой.
[58:34.000 --> 58:41.000]  Ну и, как гугл говорят, объём метаданных у них к данным примерно один в десяти тысячам.
[58:41.000 --> 58:48.000]  Поэтому, сделав несколько таких итераций, мы сможем уменьшить объём метаданных
[58:48.000 --> 58:55.000]  с петабайтов, с экзобайтов уже, наверное, до килобайт
[58:55.000 --> 58:59.000]  и поместить эти килобайты опять в чаби, опять на некоторая точка входа.
[58:59.000 --> 59:03.000]  Ну и сделать аккуратно, чтобы запись файл не приводила к тому,
[59:03.000 --> 59:07.000]  что мы проваливаемся через этот бешенный каскад до самого основания, до чаби.
[59:07.000 --> 59:09.000]  Чаби сошёл бы с ума.
[59:09.000 --> 59:12.000]  Если мы всё аккуратно сделаем, то мы получаем такой bootstrapping,
[59:12.000 --> 59:18.000]  и снова система масштабируется через саму себя, только уже более сложным образом.
[59:18.000 --> 59:22.000]  Ну, как компилятор устроен, примерно так же.
[59:22.000 --> 59:26.000]  В общем, такие очень неочевидные уроки дизайна.
[59:26.000 --> 59:31.000]  Конечно, конкретные идеи привязаны к конкретным системам,
[59:31.000 --> 59:37.000]  а общая идея такая, что просто переиспользуйте своё знание про устройство компьютера.
[59:37.000 --> 59:41.000]  Вот любая деталь может пригодиться в совершенно другом масштабе.
[59:44.000 --> 59:49.000]  Что ещё нам может прийти в голову? О чём мы ещё могли бы поговорить?
[59:49.000 --> 01:00:09.000]  Не то чтобы было какое-то сложное наблюдение, но мне кажется, что всё уже полезное.
[01:00:09.000 --> 01:00:19.000]  Про то, что при переходе к византийской модели мы в свой набор инструментов, достаточно незатейливый,
[01:00:19.000 --> 01:00:23.000]  добавили ещё одно измерение, а именно криптографию.
[01:00:23.000 --> 01:00:29.000]  Мы про это подробно не говорим. Надеюсь, у вас будет курс криптографии.
[01:00:29.000 --> 01:00:32.000]  Где-нибудь, когда-нибудь вы про это всё узнаете.
[01:00:32.000 --> 01:00:36.000]  Но какие инструменты мы использовали, когда мы перешли к блокчейнам?
[01:00:36.000 --> 01:00:42.000]  Мы использовали цифровые подписи, огромное количество способов криптографических хеш-функций,
[01:00:42.000 --> 01:00:53.000]  и мы совсем не поговорили про такую технику, с помощью которой мы можем доказывать что-то другим узлам,
[01:00:53.000 --> 01:00:55.000]  не раскрывая содержание.
[01:00:55.000 --> 01:01:00.000]  Эта идея, с помощью которой реализуется анонимность в современных блокчейнах,
[01:01:00.000 --> 01:01:05.000]  очень красивая, очень эстроумная.
[01:01:05.000 --> 01:01:13.000]  Переходя от блокчейн, от репликации не византийской к византийской, мы добавляем себе все эти проблемы,
[01:01:13.000 --> 01:01:18.000]  и кажется, что их очень увлекательно решать, и люди до сих пор прямо сейчас этим занимаются.
[01:01:18.000 --> 01:01:25.000]  Не так давно эти доказательства с нулевым расположением не интерактивные придумали
[01:01:25.000 --> 01:01:30.000]  и стали их применять в криптовалютах, чтобы обеспечить анонимность, вместо псевдонимности.
[01:01:30.000 --> 01:01:39.000]  Что еще? Если говорить про инженерные вещи, то мне кажется...
[01:01:39.000 --> 01:01:48.000]  Кое-что еще. Наверное, полезно было бы заметить, как мы иногда обходили некоторые физические ограничения
[01:01:48.000 --> 01:01:54.000]  в наших системах, а именно физические ограничения на коммуникацию.
[01:01:54.000 --> 01:01:58.000]  У нас было два довольно странных примера, довольно радикальных.
[01:01:58.000 --> 01:02:05.000]  Не то чтобы вы их будете часто использовать в своей жизни, но интересно на них обратить внимание.
[01:02:05.000 --> 01:02:12.000]  У нас в биткоине была задача выбора лидера, а в распределённых системах, в базах данных, в спанре
[01:02:12.000 --> 01:02:18.000]  была задача упроточения транзакций. И та и другая задача решается через коммуникацию.
[01:02:18.000 --> 01:02:25.000]  Естественным образом решается через коммуникацию, потому что узлы должны всё-таки о чём-то договориться.
[01:02:25.000 --> 01:02:33.000]  И мы в двух этих частных случаях смогли выдумать совершенно альтернативные решения, которые коммуникация не требовала.
[01:02:33.000 --> 01:02:37.000]  В одном случае это был True Time, в другом случае это был Proof of Work.
[01:02:37.000 --> 01:02:46.000]  И удивительно, что так иногда получается сделать. Обойти накладные расходы, связанные с тем, что узлы системы находятся далеко.
[01:02:46.000 --> 01:02:53.000]  И коммуникация дорогая. В случае True Time мы использовали асинхронизацию часов через спутники, через GPS.
[01:02:53.000 --> 01:03:01.000]  В случае биткоина мы использовали прожигание процессора и криптографичность функций.
[01:03:01.000 --> 01:03:10.000]  Любопытно, как такие совершенно альтернативные инструменты помогают решать распределённые задачи, довольно типичные.
[01:03:10.000 --> 01:03:18.000]  True Time – это, наверное, повод сказать про ещё одну деталь.
[01:03:18.000 --> 01:03:29.000]  А именно, что если мы хотим строить отказоустойчивую систему, то ещё один организующий принцип – разумно как можно больше всего виртуализировать.
[01:03:29.000 --> 01:03:34.000]  True Time – это надёжные часы. Мы не думаем, про то, как они под капотом устроены.
[01:03:34.000 --> 01:03:41.000]  Это целый сервис, и он реализован даже не программно. Это железный сервис.
[01:03:41.000 --> 01:03:50.000]  То есть это спутники, это GPS. А ещё, если мы хотим поддерживать достаточно узкое окно неопределённости на отдельных узлах,
[01:03:50.000 --> 01:03:56.000]  то мы должны ещё и в нашу всю сетевую инфраструктуру тоже встроить протокол синхронизации.
[01:03:56.000 --> 01:04:01.000]  Google научилась, кажется, делать, но, по крайней мере, они написали статью такую.
[01:04:01.000 --> 01:04:04.000]  Она про research, а всё-таки не про production.
[01:04:04.000 --> 01:04:10.000]  Но, тем не менее, они пишут, что если аккуратно покладываться в коробках с проводами
[01:04:10.000 --> 01:04:15.000]  и настроить протокол на этом уровне, на уровне сетевых, на уровне сетей,
[01:04:15.000 --> 01:04:18.000]  то можно добиться синхронизации часов до нанесекунд.
[01:04:18.000 --> 01:04:24.000]  Что в масштабах большого кластера выглядит вообще как безумие, просто нанесекунды.
[01:04:24.000 --> 01:04:33.000]  Мы виртуализируем время, мы, конечно, виртуализируем диски.
[01:04:33.000 --> 01:04:37.000]  И если мы в Google находимся, то мы не думаем просто про диски, они не отказывают,
[01:04:37.000 --> 01:04:43.000]  потому что диски все виртуальные. Мы виртуализируем, в конце концов, сами машины,
[01:04:43.000 --> 01:04:48.000]  а вот актор в Яндекс.ДБ это такая отказоустойчивая единица.
[01:04:48.000 --> 01:04:55.000]  Она последовательная, она надёжная, она последовательная и при этом надёжная.
[01:04:55.000 --> 01:05:01.000]  Она может перезапускаться на разных машинах, и мы не думаем про то, что под ней откажет диск.
[01:05:02.000 --> 01:05:06.000]  Я не знаю.
[01:05:06.000 --> 01:05:17.000]  Если ты контролируешь время передачи данных,
[01:05:17.000 --> 01:05:20.000]  если ты контролируешь все задержки, которые у тебя возникают в буферах,
[01:05:20.000 --> 01:05:25.000]  ты можешь как-то это учитывать. Если ты контролируешь, что у тебя есть разные маршруты,
[01:05:25.000 --> 01:05:32.000]  то проблема с синхронизацией часов была не в том, что долгоданные бегают,
[01:05:32.000 --> 01:05:35.000]  а в том, что они бегают немного по разным путям.
[01:05:35.000 --> 01:05:39.000]  Если у тебя есть симметрия, то ты измеришь время раунд трипа,
[01:05:39.000 --> 01:05:43.000]  но ты не можешь измерить время в одну сторону и в другую сторону.
[01:05:43.000 --> 01:05:49.000]  Там целая статья сложная, не буду ее пересказывать, я сам не понимаю просто,
[01:05:49.000 --> 01:05:55.000]  но там инфраструктура на уровне коробок с проводами рассчитывает,
[01:05:55.000 --> 01:05:59.000]  какие маршруты, какие поддеревья в сети есть, и понимает, что если кто-то отказал,
[01:05:59.000 --> 01:06:03.000]  то нужно что-то делать по-другому.
[01:06:03.000 --> 01:06:10.000]  Не нужно это делать нам, это нужно делать примерно в одном месте,
[01:06:10.000 --> 01:06:14.000]  поскольку все машины находятся либо в Амазоне, либо в Google,
[01:06:14.000 --> 01:06:17.000]  в конце концов никто больше не покупает особо.
[01:06:17.000 --> 01:06:22.000]  По крайней мере, если вы стартап какого-то разумного размера, не слишком большой.
[01:06:22.000 --> 01:06:25.000]  Если вы готовы строить свой ДЦ, строить свою инфраструктуру,
[01:06:25.000 --> 01:06:31.000]  то это отдельная история, но мир централизован довольно сейчас.
[01:06:31.000 --> 01:06:38.000]  Так что некоторые люди умеют, и скорее важно понимать,
[01:06:38.000 --> 01:06:42.000]  на каких абстракциях мы можем строить свои алгоритмы.
[01:06:42.000 --> 01:06:48.000]  Строить в XXI веке алгоритмы на основе дисков сбоенных и сбоенных машин,
[01:06:48.000 --> 01:06:51.000]  это довольно непрактично.
[01:06:51.000 --> 01:07:00.000]  Люди уже научились больше абстракций наворачивать, чтобы было проще в конце концов.
[01:07:00.000 --> 01:07:05.000]  И лишние абстракции, мы видим, иногда приносят некоторое дополнительное удобство.
[01:07:05.000 --> 01:07:10.000]  Абстракция хранения, если мы отделим процессоры от дисков,
[01:07:10.000 --> 01:07:20.000]  то мы можем сделать репликацию более эффективной, используя R&J коды.
[01:07:26.000 --> 01:07:33.000]  Актор — это в смысле модель акторов, где у тебя есть такие последовательные сущности,
[01:07:33.000 --> 01:07:37.000]  которые обмениваются сообщениями асинхронными.
[01:07:37.000 --> 01:07:43.000]  На слайдах янда вздеби написана именно в модели акторов.
[01:07:43.000 --> 01:07:46.000]  Все сущности там являются такими.
[01:07:46.000 --> 01:07:51.000]  Некоторые акторы рождаются, машина умерла, они погибают вместе с ней.
[01:07:51.000 --> 01:07:55.000]  А есть акторы, которые могут возрождаться на другой машине
[01:07:55.000 --> 01:07:59.000]  и убежать с того же места, где они остановились.
[01:07:59.000 --> 01:08:04.000]  Это про модель программирования скорее, не про распределённые системы.
[01:08:04.000 --> 01:08:12.000]  Это не как файбер, это не файбер, это актор.
[01:08:12.000 --> 01:08:19.000]  Это та же область, это конкарнси, это модель конкурентного программирования.
[01:08:19.000 --> 01:08:22.000]  Как ты организуешь свои конкурентные активности?
[01:08:22.000 --> 01:08:26.000]  У каждого актора есть своё собственное состояние, оно не разделяется никогда,
[01:08:26.000 --> 01:08:30.000]  оно только к одному актору привязано, и актор общается с сообщениями с другими акторами.
[01:08:30.000 --> 01:08:34.000]  Это такой подход к программированию распределённых конкурентных систем,
[01:08:34.000 --> 01:08:40.000]  который не то чтобы оптимальный, но у него есть много достоинств.
[01:08:40.000 --> 01:08:43.000]  Как и у любого другого подхода, есть много достоинств своих.
[01:08:43.000 --> 01:08:47.000]  В общем, ты выбираешь какой-то.
[01:08:47.000 --> 01:08:57.000]  Раз уж ты про код спросил, давай скажем про код.
[01:08:57.000 --> 01:09:06.000]  В нашем курсе мы программируем всё не на акторах, мы программируем всё на файберах.
[01:09:06.000 --> 01:09:11.000]  И я надеюсь, что из тех домашних, которые вы написали,
[01:09:11.000 --> 01:09:14.000]  понятно, что можно сделать лучше, но по крайней мере скажу свою задумку,
[01:09:14.000 --> 01:09:18.000]  она однажды воплотится в реальность до конца.
[01:09:18.000 --> 01:09:25.000]  Мы должны были увидеть, что, во-первых, разумно строить весь код на абстракциях,
[01:09:25.000 --> 01:09:29.000]  потому что тогда он декомпозируется лучше.
[01:09:29.000 --> 01:09:37.000]  Если вы работаете через STD file system, запускаете треды и пишете какой-то гигантский монорит,
[01:09:37.000 --> 01:09:42.000]  то у вас есть разные проблемы, потому что всё сложно тестировать.
[01:09:42.000 --> 01:09:50.000]  Если вы пишете код на абстракциях, то в смысле тестирования, вы можете позволить себе гораздо больше.
[01:09:50.000 --> 01:09:56.000]  Вы можете тестировать конкурентность. Вы можете тестировать работу с файловой системой.
[01:09:56.000 --> 01:10:01.000]  Вы можете тестировать ваш РПС протокол.
[01:10:01.000 --> 01:10:09.000]  А если вы тестируете вот дет你說, то вы можете проверить гораздо больше частных случаев.
[01:10:09.000 --> 01:10:17.000]  Если у вас совсем много желания, то вы можете в конце концов построить для всей вашей системы симулятор
[01:10:17.000 --> 01:10:24.000]  и детерминированно тестировать уже не просто отдельные компоненты, а просто весь ваш алгоритм, все ваши узлы.
[01:10:24.000 --> 01:10:33.000]  Это очень удобно, потому что представьте, что вы живёте в мире, где у вас нет этого симулятора,
[01:10:33.000 --> 01:10:38.000]  оно так почти все и живут, и вы отложиваете какой-то баг.
[01:10:38.000 --> 01:10:45.000]  Сейчас у нас есть один общий лог, это довольно удобно прочитал, то, что происходило раньше в логе,
[01:10:45.000 --> 01:10:51.000]  предшествует другим событиям, и всё можно в одном текстовом файле посмотреть.
[01:10:51.000 --> 01:10:58.000]  Если вы работаете в системе распределённой и недетерминированной, разумеется, мир недетерминирован,
[01:10:58.000 --> 01:11:05.000]  то какие у вас проблемы? У вас есть два события, они произошли на разных узлах, вы знаете про один порядок этих событий,
[01:11:05.000 --> 01:11:11.000]  а в логе они по часам порядочно иначе, потому что часы не синхронизированы.
[01:11:11.000 --> 01:11:19.000]  Вам нужно сливать много логов, там таймстемпы разные, и не совсем понятно, как одно с другим сопоставлять.
[01:11:19.000 --> 01:11:24.000]  Кроме того, вы не можете просто воспроизвести ошибку, потому что если случится раз в день,
[01:11:24.000 --> 01:11:29.000]  то как вы собираетесь её воспроизводить? Не знаю, терпеливо ждать?
[01:11:29.000 --> 01:11:33.000]  Или систему ломать снаружи, в смысле, задерживать сообщение, что-то ещё делать?
[01:11:33.000 --> 01:11:43.000]  Но это сложно. Кроме того, если вы работаете таким подходом, где вы детерминированно всё можете воспроизводить,
[01:11:43.000 --> 01:11:53.000]  вы можете ещё и... Мысль потеряла.
[01:11:53.000 --> 01:12:07.000]  Ну ладно, потом найду. Так бывает. Другая причина, закруглим эту мысль, понятно, что детерминизм для тестирования очень приятен.
[01:12:07.000 --> 01:12:17.000]  Другая польза от абстракций состоит в том, что вы просто очень аккуратно можете сфумулировать гарантии той или иной абстракции.
[01:12:17.000 --> 01:12:22.000]  Вот вы работаете с RPC-протоколом, и у него какие-то гарантии.
[01:12:22.000 --> 01:12:31.000]  Но вот эти гарантии строятся на основе гарантии протокола более низкого уровня, где просто шина сообщений.
[01:12:31.000 --> 01:12:36.000]  У вас асинхронная отправка сообщения, получение сообщений без всякой семантики.
[01:12:36.000 --> 01:12:44.000]  Имея абстракцию для такого уровня, вы можете для неё сфумулировать, какие же у вас гарантии в сети.
[01:12:44.000 --> 01:12:52.000]  Вы ожидаете, что сообщение доставится или не ожидаете? Оно доставится однажды или может несколько раз доставиться?
[01:12:52.000 --> 01:12:55.000]  Что если соединение порвётся? Вообще есть ли у вас понятие соединения?
[01:12:55.000 --> 01:13:03.000]  Вот такой дизайн разумен ещё и потому, что вы можете подобрать максимально точные и максимально разумные гарантии
[01:13:03.000 --> 01:13:08.000]  для этих отдельных компонентов, для отдельных границ с внешним миром.
[01:13:08.000 --> 01:13:14.000]  Мы не думаем про TCP, когда мы пишем код. Мы думаем про сетевой транспорт с понятными гарантиями.
[01:13:14.000 --> 01:13:20.000]  Мы отправляем сообщение, в смысле мы в нашем курсе, и либо с другой стороны вызовется handle message,
[01:13:20.000 --> 01:13:23.000]  либо с нашей стороны вызовется handle disconnect.
[01:13:23.000 --> 01:13:26.000]  Либо то, либо другое, либо, по крайней мере, что-то одно случится.
[01:13:26.000 --> 01:13:29.000]  И вот всё, мы понимаем, чего мы ожидаем от сети.
[01:13:29.000 --> 01:13:34.000]  Нам больше не нужно думать про гарантии TCP, которые могут быть очень сложными.
[01:13:34.000 --> 01:13:39.000]  Это так делать разумно, так жить потом проще.
[01:13:39.000 --> 01:13:45.000]  Я надеюсь, что мы увидели ещё такую важную деталь, что в распределённых системах важно иметь инструменты для того,
[01:13:45.000 --> 01:13:53.000]  чтобы анализировать их поведение. Речь тут не про симулятор, я уже про это сказал,
[01:13:53.000 --> 01:14:01.000]  а про то, чтобы в принципе понимать, как система себя ведёт, даже если она запущена в продакшене и всё недетерминировано.
[01:14:01.000 --> 01:14:04.000]  Это речь про observability.
[01:14:04.000 --> 01:14:08.000]  Observability – это, как правило, три разных измерения.
[01:14:08.000 --> 01:14:16.000]  Это логирование, это tracing и это метрики.
[01:14:16.000 --> 01:14:21.000]  Понятно, зачем нужны метрики для того, чтобы считать, сколько у вас переключений контекста, сколько у вас локаций,
[01:14:21.000 --> 01:14:28.000]  сколько у вас запросов в секунду. Зачем нужен логирование? Зачем нужен tracing?
[01:14:28.000 --> 01:14:32.000]  Для того, чтобы просто связывать разные машины друг с другом.
[01:14:32.000 --> 01:14:40.000]  У вас много машин, если у вас система сложная, там есть какие-то подсистемы, какие-то микросервисы условно,
[01:14:40.000 --> 01:14:46.000]  и клиент приходит и начинает блуждать по этим машинам, а внутри машин по файберам,
[01:14:46.000 --> 01:14:51.000]  и какой-то ещё работы выполняет, конвейеры ассинхронные запускаются.
[01:14:51.000 --> 01:14:59.000]  Хорошо бы понимать, как запрос выполнялся, на каких машинах вообще, и сколько времени он где проводил.
[01:14:59.000 --> 01:15:05.000]  По логам эта задача не решается, потому что лог – это проекция на одну машину системы,
[01:15:05.000 --> 01:15:10.000]  а trace – это проекция на один запрос.
[01:15:10.000 --> 01:15:15.000]  И для трейсинга вам хочется рисовать... Давайте сейчас найдём какую-нибудь картинку.
[01:15:18.000 --> 01:15:23.000]  Вот какие-то подобные конструкции.
[01:15:26.000 --> 01:15:30.000]  Но не судите строго, случайная картинка из интернета.
[01:15:30.000 --> 01:15:38.000]  Короче, вам хочется рисовать такие разноцветные колбаски, которые отвечают за какие-то стадии на каких-то машинах.
[01:15:38.000 --> 01:15:45.000]  И хорошо бы вы могли выбрать ваш ID-запрос пользователя и нарисовать картинку,
[01:15:45.000 --> 01:15:49.000]  как же ваш запрос по разным машинам путешествовал.
[01:15:49.000 --> 01:15:59.000]  У нас с этим было довольно плохо, потому что из инструментов у нас было только trace ID и контексты.
[01:16:00.000 --> 01:16:10.000]  В реальности есть помимо trace ID, который красит все действия, все события, связанные с одним запросом в один цвет,
[01:16:10.000 --> 01:16:13.000]  есть ещё, как правило, и scope ID.
[01:16:13.000 --> 01:16:15.000]  То есть вы можете...
[01:16:20.000 --> 01:16:29.000]  Есть scoping, и вы можете не просто цепочки отслеживать действия,
[01:16:29.000 --> 01:16:33.000]  вы можете ещё вкладывать одни действия в другие в пределах одной машины.
[01:16:33.000 --> 01:16:35.000]  Ну или в пределах разных машин.
[01:16:35.000 --> 01:16:46.000]  В общем, спаны и trace ID – это вся картинка, а спаны – это отдельная горизонтальная колбаска.
[01:16:46.000 --> 01:16:52.000]  И вам нужно и то, и другое для того, чтобы такие картинки в итоге из системы забирать.
[01:16:52.000 --> 01:16:56.000]  Вот без этого понимательная система работает довольно сложно.
[01:16:56.000 --> 01:17:06.000]  У нас был трейсинг такой простой, мы рисовали асмысле иерархию именно.
[01:17:12.000 --> 01:17:14.000]  Для этого тоже нужны инструменты.
[01:17:14.000 --> 01:17:21.000]  Понятно, что по логу можно информацию взять при желании, но просто это другое измерение, на которое ты проецируешь другая ось.
[01:17:21.000 --> 01:17:24.000]  Это ось машины, а тут ось запроса.
[01:17:24.000 --> 01:17:28.000]  И для этого у тебя должны быть отдельные инструменты.
[01:17:28.000 --> 01:17:34.000]  Они появятся в библиотеке с логингом Timber, который назывался у нас.
[01:17:34.000 --> 01:17:37.000]  Она как раз про это и должна в конце концов быть.
[01:17:37.000 --> 01:17:41.000]  То есть там должны быть инструменты для трейсинга.
[01:17:41.000 --> 01:17:46.000]  В целом, я надеюсь, принцип дизайна нашего кода должен быть понятен.
[01:17:46.000 --> 01:17:50.000]  У нас есть отдельная конкарнсия, у нас есть отдельная сеть IRPC,
[01:17:50.000 --> 01:17:57.000]  у нас есть отдельная персистентность, то есть файловая система, write-head-log, логи RAF-to-Paxos,
[01:17:57.000 --> 01:18:05.000]  и у нас есть обсервабилити отдельно, и у нас есть еще один компонент – это сериализация.
[01:18:05.000 --> 01:18:10.000]  Мы использовали библиотеку, которая под C++ заточена.
[01:18:10.000 --> 01:18:14.000]  Мне кажется, что ее нужно закопать, в смысле не библиотеку,
[01:18:14.000 --> 01:18:20.000]  а то, что мы ее используем. Перейти на протобуф – это будет с одной стороны больнее немножко,
[01:18:20.000 --> 01:18:26.000]  а с другой стороны будет менее больно, потому что меньше ошибок можно на границе допустить.
[01:18:26.000 --> 01:18:31.000]  То есть код станет тупее, больше, но проще, надежнее.
[01:18:31.000 --> 01:18:37.000]  В этом месте вы могли бы вынести, что сериализация устроена…
[01:18:37.000 --> 01:18:42.000]  Во-первых, она бывает в бинарных текстах для разных целей, для отладки и для продакшена.
[01:18:42.000 --> 01:18:46.000]  Что мы, наверное, в меньшей степени вынесли?
[01:18:46.000 --> 01:18:49.000]  Это то, что есть, во-первых, разные подходы к сериализации.
[01:18:49.000 --> 01:18:54.000]  Есть сериализация, где мы данные переупаковываем для того, чтобы отправить их в провод,
[01:18:54.000 --> 01:19:01.000]  а есть сериализация Zerocopy, когда мы представляем данные в машине, в памяти и в проводе одним и тем же образом,
[01:19:01.000 --> 01:19:04.000]  и можно делать намного эффективнее.
[01:19:04.000 --> 01:19:10.000]  А еще, что сериализация тесно связана с фиксированием схемы протокола.
[01:19:10.000 --> 01:19:14.000]  То есть у нас есть сообщение Pentantris, и оно описывается не в коде,
[01:19:14.000 --> 01:19:20.000]  оно описывается в специальном протофайле, где написано, какие поля, каких типов.
[01:19:20.000 --> 01:19:28.000]  А дальше Protobuf старается сделать так, чтобы эти схемы можно было аккуратно эволюционировать,
[01:19:28.000 --> 01:19:34.000]  добавлять там и ударять в какие-то поля, и делать это так, чтобы другие узлы,
[01:19:34.000 --> 01:19:39.000]  которые живут со старыми схемами, не поломались еще, потому что от АМАР нам все везде нельзя.
[01:19:39.000 --> 01:19:49.000]  В общем, есть такой набор библиотек под задач, то есть декомпозиция уже не системы, а кода системы.
[01:19:49.000 --> 01:20:01.000]  Еще раз перечислю, конкарнсия, сеть, конкарнсия, сеть, персистентность, сериализация, обсервабилити.
[01:20:01.000 --> 01:20:08.000]  Все это по возможности друг от друга отрезано.
[01:20:08.000 --> 01:20:12.000]  Там где-то зависит, но в целом почти перпендикулярно.
[01:20:12.000 --> 01:20:21.000]  Тут уместно вспомнить, что язык Go, который мы не использовали, потому что он фиксирует runtime,
[01:20:21.000 --> 01:20:26.000]  это runtime не кастомизируется, но который мы могли бы использовать, если мы пишем распределенную систему,
[01:20:26.000 --> 01:20:30.000]  был задизайнен в принципе ровно для таких задач.
[01:20:30.000 --> 01:20:36.000]  Вот этот язык, он про конкарнсия, про трейсинг, потому что есть все эти контексты,
[01:20:36.000 --> 01:20:41.000]  про сериализацию, про RPC и так далее.
[01:20:41.000 --> 01:20:46.000]  Вот язык создан именно для того, чтобы программировать вот такой вот код,
[01:20:46.000 --> 01:20:53.000]  и по возможности делать это более надежно, чем C++, потому что сборка мусора,
[01:20:53.000 --> 01:20:59.000]  потому что больше интеграции в сам язык, ну и много всего другого.
[01:21:11.000 --> 01:21:18.000]  Я не знаю. Я думал, ты скажешь на раз, потому что когда перепишешь себе вы, то говорят обычно на раз.
[01:21:19.000 --> 01:21:26.000]  Нет, я пока не планирую этого делать, потому что я не вижу большого пользы, кроме того, что тебя будет...
[01:21:26.000 --> 01:21:29.000]  Вообще QAO это такая штука, которую можно, не знаю, забыть написать.
[01:21:29.000 --> 01:21:32.000]  Ты пишешь там mutex log, а перед ним QAO должен стоять.
[01:21:32.000 --> 01:21:34.000]  Неприятная ситуация.
[01:21:34.000 --> 01:21:40.000]  Короче, я не вижу большого пользы, потому что в корутинах C++, к сожалению,
[01:21:40.000 --> 01:21:45.000]  есть некоторый недетерминизм, связанный с локациями.
[01:21:45.000 --> 01:21:48.000]  И непонятно просто, что он здесь даст.
[01:21:48.000 --> 01:21:53.000]  Он может дать то, что там, не знаю, можно библиотеку Unifex использовать,
[01:21:53.000 --> 01:21:59.000]  Unified Executors, которые пишут сейчас в Facebook, ну или кто сейчас пишет ее, даже непонятно.
[01:22:01.000 --> 01:22:07.000]  Но прям прямой такой очевидный пользы от корутин я не вижу, кроме того, что будут корутины.
[01:22:08.000 --> 01:22:14.000]  Ну да, немножко станет проще, потому что сейчас в рантайме узла есть такой компонент,
[01:22:14.000 --> 01:22:18.000]  там, локатор стеков, грубо говоря, там ресурс менеджер он называется,
[01:22:18.000 --> 01:22:21.000]  ну или как это называется, вот его бы не было.
[01:22:21.000 --> 01:22:28.000]  Но это не самая устрая проблема, то есть это не то, чего не хватает в первую очередь.
[01:22:28.000 --> 01:22:30.000]  Так что не знаю.
[01:22:37.000 --> 01:22:42.000]  Да, я этим займусь после того, как я съезжу в Молмаск.
[01:22:42.000 --> 01:22:47.000]  Я, к сожалению, погряз в реакциях сейчас, в зачетах во всем этом,
[01:22:47.000 --> 01:22:51.000]  но вот я недельку проваляюсь на печи, а потом буду дописывать.
[01:22:51.000 --> 01:22:54.000]  Ну это первое, что мне хочется сделать сейчас.
[01:22:54.000 --> 01:22:57.000]  Ещё раз — код будет написан так, что это могут быть разные машины.
[01:22:57.000 --> 01:23:01.000]  Если ты хочешь запустить это на одной машине, ну запусти три виртуалки.
[01:23:01.000 --> 01:23:04.000]  В конце концов, у Джевсон так и работает.
[01:23:04.000 --> 01:23:07.000]  Ты запускаешь три виртуалки, descрепляешь на Turk,
[01:23:07.000 --> 01:23:12.000]  нуton и идем же через один месяц если даже Whilst pancakes.
[01:23:12.000 --> 01:23:16.000]  В tinted, это только полетел х Climate night.
[01:23:16.000 --> 01:23:19.000]  И код простоitute.
[01:23:19.000 --> 01:23:25.000]  В конце концов, в Jefferson так и работает. Ты запускаешь три виртуальные машины,
[01:23:25.000 --> 01:23:30.000]  но какая разница, это разные физические машины или разные виртуальные машины
[01:23:30.000 --> 01:23:38.000]  на разных физических или на одной? Ладно, разница, конечно, есть.
[01:23:38.000 --> 01:23:45.000]  Но еще раз, сам код никак этого зависеть не будет. Сам код, который мы писали,
[01:23:45.000 --> 01:23:49.000]  не должен этого зависеть никак, то есть полагаться на то, что это один поток.
[01:23:49.000 --> 01:23:54.000]  Если полагается, то это плохой код. Если он написан так, что он не полагается,
[01:23:54.000 --> 01:24:00.000]  а все бюллетей, которые поверх runtime не полагаются, то этот код можно запускать
[01:24:00.000 --> 01:24:04.000]  как отдельные процессы на одной машине, как отдельные процессы на разных машинах.
[01:24:04.000 --> 01:24:11.000]  Следующий шаг он такой. В принципе, ничего не должно мешать
[01:24:11.000 --> 01:24:16.000]  напрямую переехать из симулятора в определенный мир.
[01:24:16.000 --> 01:24:21.000]  Собственно, в этом и задумка всей этой конструкции с симулятором, с runtime и со всем.
[01:24:21.000 --> 01:24:28.000]  Потому что код, в котором полностью кастомизируется runtime, это очень сложный код.
[01:24:28.000 --> 01:24:31.000]  Библиотеку с конкарнсом можно было во многих местах написать гораздо проще,
[01:24:31.000 --> 01:24:37.000]  если предполагать, что у нас не будет определенной симуляции,
[01:24:37.000 --> 01:24:43.000]  что все-таки у нас в конце концов будут потоки. Почти нет статиков,
[01:24:43.000 --> 01:24:51.000]  потому что статики ломают что-то. С другой стороны, код статики это довольно альтернативно,
[01:24:51.000 --> 01:24:56.000]  яфи часе плюс-плюс. Кажется, что можно и без нее жить, и возможно даже лучше всем будет.
[01:24:56.000 --> 01:25:04.000]  Поэтому тут дело вкуса, мне кажется. Я запутал тебя, наверное. Ответ – да.
[01:25:26.000 --> 01:25:32.000]  Что это значит? Какую задачу ты хочешь решать?
[01:25:56.000 --> 01:26:06.000]  Разумеется, полезные вещи в блокчейнах пишут. Задача это другие.
[01:26:06.000 --> 01:26:11.000]  Наши системы, которые были в первой половине курса, они про пропускную способность,
[01:26:11.000 --> 01:26:16.000]  про много данных, про много запросов. Блокчейн не про много запросов,
[01:26:16.000 --> 01:26:27.000]  но биткоин обрабатывает 7 транзакций в секунду. В Google внутри происходит 10-10 RPC вызовов в секунду.
[01:26:27.000 --> 01:26:33.000]  Это немного разный масштаб. 10-10 и 7. Поэтому задача разумеется разная.
[01:26:33.000 --> 01:26:37.000]  Задача какие-то – аукционы, выборы, что-то подобное.
[01:26:37.000 --> 01:26:45.000]  Есть контракт, я давно про него читал, называется «Король эфира»,
[01:26:45.000 --> 01:26:49.000]  где есть такой трон, можно на него залезть и заблокировать большую сумму денег.
[01:26:49.000 --> 01:26:55.000]  Если ты хочешь залезть сам, ты должен поставить большую сумму.
[01:26:55.000 --> 01:27:02.000]  И если тебя переплюну, то деньги тебе вернутся. Ну или нет.
[01:27:02.000 --> 01:27:05.000]  Или, может быть, ты так останешься самым главным.
[01:27:05.000 --> 01:27:11.000]  Кто в итоге больше поставит? Можно делать бессмысленные вещи, можно делать что-то более полезное.
[01:27:11.000 --> 01:27:17.000]  Но это совсем другие задачи. Сам инструкцион-сет криптографический.
[01:27:17.000 --> 01:27:23.000]  Он не про то, чтобы данные обрабатывать, а про то, чтобы решить подобные задачи.
[01:27:23.000 --> 01:27:28.000]  Конечно же, сам эфириум – это распределенная система.
[01:27:28.000 --> 01:27:36.000]  Что ты имеешь в виду? Состояние эфириума – это набор отображений,
[01:27:36.000 --> 01:27:45.000]  дерево, бор, склеенный из меркла три.
[01:27:45.000 --> 01:27:53.000]  И у эфириума цель – экономить это состояние и, по возможности, его не слишком часто обновлять.
[01:27:53.000 --> 01:27:58.000]  В эфириуме не то, чтобы задачи похожи. Задачи не похожие, а проблемы похожие,
[01:27:58.000 --> 01:28:03.000]  потому что блокчейнам пока масштабироваться не особо нужно.
[01:28:03.000 --> 01:28:08.000]  Есть биткоин, он порождает блок раз в 10 минут, этот блок размером 1 мегабайт.
[01:28:08.000 --> 01:28:15.000]  И мы знаем, что сейчас блокчейн биткоина весит 300 гигабайт, через год он будет весить 400,
[01:28:15.000 --> 01:28:20.000]  через два года – 500, потому что просто фиксированная скорость публикации блоков.
[01:28:20.000 --> 01:28:23.000]  Дисков, скорее всего, хватит на это, на такой рост.
[01:28:23.000 --> 01:28:29.000]  Но если мы вдруг научимся делать блокчейны без пухфорка, которые будут хорошо масштабироваться,
[01:28:29.000 --> 01:28:34.000]  станут популярны, то в них транзакций станет больше, блоков станет больше,
[01:28:34.000 --> 01:28:37.000]  на диске они перестанут помещаться, и тогда нужно будет шардировать.
[01:28:37.000 --> 01:28:41.000]  Эфириум как раз занимается сейчас следующей своей большой итерацией,
[01:28:41.000 --> 01:28:46.000]  и там должна быть финализация блоков, и там есть планы про масштабирование,
[01:28:46.000 --> 01:28:52.000]  про то, чтобы шардировать блокчейн. В этом смысле задачи похожие решаются.
[01:28:52.000 --> 01:28:56.000]  Блокчейны, скорее, к ним с некоторым лагом приходят,
[01:28:56.000 --> 01:29:02.000]  потому что сейчас рейт слишком низкий для того, чтобы эти проблемы возникали.
[01:29:02.000 --> 01:29:06.000]  Но вот это воспроизводится.
[01:29:06.000 --> 01:29:11.000]  Про транзакции мы тоже в блокчейнах говорили, что там какие-то похожие протоколы,
[01:29:11.000 --> 01:29:15.000]  не похожие, но, опять, не похожие по конкретным механикам,
[01:29:15.000 --> 01:29:18.000]  но похожие по структуре, по фазам, по лагике своей.
[01:29:28.000 --> 01:29:30.000]  Хороший вопрос. Я про это ничего не знаю.
[01:29:34.000 --> 01:29:38.000]  Можно придумать какие-то наивные способы.
[01:29:41.000 --> 01:29:43.000]  В конце концов, кто тебе мешает?
[01:29:43.000 --> 01:29:46.000]  Я просто в контексте этого курса думал, что можно было бы сделать.
[01:29:46.000 --> 01:29:49.000]  Можно написать узел, который перехватывает сообщения по сети,
[01:29:49.000 --> 01:29:52.000]  знает, что у них какая-то структура, то есть схема фиксирована,
[01:29:52.000 --> 01:29:55.000]  и ты берешь, не знаю, сохраняешь старые сообщения,
[01:29:55.000 --> 01:29:59.000]  передправляешь их другим узлам, делаешь какие-то вещи.
[01:29:59.000 --> 01:30:04.000]  А так, чтобы тестировать в смысле координированная атака злоумышленников...
[01:30:11.000 --> 01:30:18.000]  Либра паста, которую Facebook делала, а какая-то тестирующая система у них.
[01:30:18.000 --> 01:30:23.000]  Как инспортируется это? Очень-очень сложно.
[01:30:23.000 --> 01:30:28.000]  Ну, не то, чтобы... сейчас.
[01:30:34.000 --> 01:30:36.000]  Нет, ну, понятно, что многие такие вещи можно делать.
[01:30:36.000 --> 01:30:41.000]  То есть это просто такая проблема в таких системах не в том,
[01:30:41.000 --> 01:30:46.000]  что где-то диск пропадет, а в том, что большая часть, значимая часть сети
[01:30:46.000 --> 01:30:50.000]  будет вести себя как-то координированно и против нас.
[01:30:50.000 --> 01:30:53.000]  То есть это какая-то сложная стратегия должна быть.
[01:30:53.000 --> 01:30:56.000]  Она вот таким точечным fault injection не решается.
[01:30:56.000 --> 01:30:59.000]  То есть что-то можно поймать, разумеется, оно...
[01:30:59.000 --> 01:31:02.000]  Ну, не знаю, статьи пишут, доказывают что-то.
[01:31:02.000 --> 01:31:05.000]  Прямо чтобы код тестировать так автоматом, я, честно говоря, не слышал.
[01:31:05.000 --> 01:31:09.000]  А вот этих истих можно придумать, конечно, миллион...
[01:31:09.000 --> 01:31:12.000]  Это скорее просто текущие подходы.
[01:31:19.000 --> 01:31:21.000]  Что мне кажется, что если мне в голову ничего не приходит,
[01:31:21.000 --> 01:31:23.000]  то самое важное я сказал.
[01:31:23.000 --> 01:31:26.000]  То, что придет позже, было не очень важно.
[01:31:32.000 --> 01:31:34.000]  Это, наверное, и есть.
[01:31:34.000 --> 01:31:40.000]  Если у нас вопросов не осталось, мыслей, каких-то идей, чего-нибудь,
[01:31:40.000 --> 01:31:43.000]  то спасибо вам большое, что были с нами.
[01:31:43.000 --> 01:31:45.000]  Приходите еще завтра.
[01:31:45.000 --> 01:31:49.000]  Мы проверим, что вы усвоили.

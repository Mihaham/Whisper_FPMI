[00:00.000 --> 00:19.000]  Сегодня у нас сложная лекция про методы, которые работают лучше, чем градиентный спуск, но и на практике в честь и тоже.
[00:19.000 --> 00:25.000]  Начнем мы с того, чем был запрос в прошлый раз. Начнем мы с напоминания о том, что было в прошлый раз.
[00:25.000 --> 00:28.000]  В прошлый раз мы рассмотрели градиентный спуск и то, как он работает.
[00:28.000 --> 00:33.000]  То есть показали, какая у него будет скорость сходимости в случае выпуклой функции и сильной выпуклой.
[00:33.000 --> 00:44.000]  А также я привел как просто некоторый факт то, какие существуют нижние оценки точности на этом классе функций.
[00:44.000 --> 00:49.000]  Был вопрос, откуда они взялись, и сейчас некоторое время хочется посвятить.
[00:49.000 --> 00:53.000]  То есть откуда они берутся?
[00:53.000 --> 01:01.000]  Чтобы показать, что выполнила некоторая нижняя оценка на скорость сходимости, нам нужно предъявить такую выпуклую функцию.
[01:01.000 --> 01:05.000]  В случае выпуклой просто выпуклой функции, не сильной выпуклой.
[01:05.000 --> 01:09.000]  Что будет происходить в случае сильной выпуклой, я в конце скажу.
[01:09.000 --> 01:22.000]  Надо предъявить такую функцию, что если мы итерируемся с помощью такого метода, то заранее известно, что наша отличия сходимости
[01:22.000 --> 01:32.000]  между значением функции в текущей точке xк плюс один и оптимальным значением будет не меньше, чем вот некоторые выражения, которые известны с номер итерации.
[01:32.000 --> 01:48.000]  И вот сейчас в ближайших двух-три слайдах покажем, что действительно будет этот самый как квадрат, который не совпадает с тем порядком, с которым сходится обычный грядетный спуск.
[01:48.000 --> 01:55.000]  И вот как раз таки за счет наличия этого зазора и возникает необходимость получения более быстрых метод.
[01:55.000 --> 01:58.000]  Вот такая вот красивая иллюстрация.
[01:58.000 --> 02:04.000]  Для того, чтобы построить такую функцию замечательную, точнее не очень, рассмотрим вот такую матрицу.
[02:04.000 --> 02:09.000]  Она трехтиагональная, и ее размерность равна 2k плюс один.
[02:10.000 --> 02:14.000]  То есть 0k это число итераций, которые мы будем делать.
[02:14.000 --> 02:23.000]  И мы хотим построить такую функцию, для которой при заданном числе итераций сходимость будет точно не лучше, чем.
[02:23.000 --> 02:32.000]  В общем, вот такая вот матрица трехтиагональная, имеется денечки над и под диагоналями, двойки на диагонали стоят.
[02:32.000 --> 02:46.000]  Заметим, что для нее выполнена, если мы возьмем квадратичную форму с этой матрицей, то у нас образуется вот такое вот выражение.
[02:46.000 --> 02:57.000]  Это легко проверить, просто если вы это распишете, то вы увидите, что у вас x и t будет всегда, и квадрат x и t будет встречаться два раза.
[02:57.000 --> 03:00.000]  Это, собственно, двойки на диагонали.
[03:03.000 --> 03:18.000]  А попарные произведения, которые тут фигурируют как минус 2 на x и икс икс плюс один, они возникнут за счет того, что у вас вот эти минус единички над и под диагоналями, они в сумме дадут минус 2.
[03:18.000 --> 03:23.000]  Поэтому тут будет минус 2, все в порядке, последние иксы.
[03:23.000 --> 03:29.000]  Понятно ли, откуда это взялось? Вообще слышно ли меня?
[03:29.000 --> 03:37.000]  Так, кто-нибудь есть тут, кто готов что-то сказать? Слышно, хорошо, спасибо.
[03:37.000 --> 03:45.000]  Окей, значит, поскольку это сумма квадратов, то это значит, что матрица положительно полуопределена.
[03:45.000 --> 03:52.000]  Поскольку сумма квадратов не может быть отрицательной, поэтому у нас есть определение положительной определенности.
[03:52.000 --> 04:03.000]  Более того, мы можем заметить, что если мы вычтем из диагонали этой матрицы четверку, то тут образуется минус двойки.
[04:03.000 --> 04:14.000]  И если мы посмотрим на квадратичную форму, образованную вот этой матрицей, то она будет ровно такой же за вычетом того, что она будет практически точно такой же.
[04:14.000 --> 04:19.000]  Единственная разница будет в том, что перед всеми квадратами будут стоять минусы.
[04:19.000 --> 04:24.000]  А вот здесь минус поменяется на плюс.
[04:24.000 --> 04:33.000]  То есть увидим, что функция стала принимать только отрицательно неположительные значения.
[04:33.000 --> 04:36.000]  Ну, значит, сверху оценка вот такая.
[04:36.000 --> 04:43.000]  Таким образом, мы можем определить L гладкую функцию с помощью задания этой матрицы.
[04:43.000 --> 04:49.000]  И задав некоторую константу L, мы получим, что это L на 8, вот это X транспонирует на X минус L на 4.
[04:49.000 --> 04:55.000]  И вот здесь вот единичный ОРД будет принимать участие, который нам сильно поможет в дальнейшем.
[04:55.000 --> 05:04.000]  Значит, градиент можем посчитать. Матрица симметрична, поэтому градиент это L на 4, а X минус L на 4 ОРД единичный.
[05:04.000 --> 05:21.000]  И поскольку мы хотим найти ее минимум, чтобы получить вот эту величину, то нам нужно найти X азвездочка такое что, а X азвездочка равно E1.
[05:21.000 --> 05:33.000]  Рекурсивно можно расписать, что для такой матрицы решения системы с правой частью, где единица стоит только в первом индексе, а все остальные нули,
[05:33.000 --> 05:42.000]  записывается как вот такое вот выражение. Вот это буквально надо там пару строчек написать, чтобы понять, что на самом деле,
[05:42.000 --> 05:51.000]  идя снизу вверх, из последней строчки вы выражаете Xn-1, ну и в дальнейшем Xn-2, Xn-3 и так далее.
[05:51.000 --> 06:03.000]  И в конце образуется, что вот, собственно, 2X1-X2 равно единице. Вот, собственно, X1 и X2 выражается через Xn.
[06:03.000 --> 06:10.000]  Вот получается, чему равно Xn, и потом вы снова поднимаетесь и получаете вот такую вот рекурсию.
[06:10.000 --> 06:19.000]  Вот это, ну можно считать довольно несложным упражнением. Не хочу сейчас это расписывать, может быть слишком долго будет.
[06:20.000 --> 06:25.000]  Так, понятно, да, что здесь происходит? И зачем нам это надо?
[06:25.000 --> 06:29.000]  Так, понятно, окей. Едем дальше.
[06:29.000 --> 06:37.000]  Тогда мы можем просто подставить это самое значение в нашу функцию, получить, что f со звездочкой равняется минус L на 8, вот на эту штуку.
[06:37.000 --> 06:43.000]  Вот, это, то есть тут просто арифметики. Я надеюсь, что тут у своего проблемы не должно возникнуть понимание того, откуда что училось.
[06:43.000 --> 06:49.000]  Вот это, это линейный член, вот этот квадратичный. Раскрыли скобки, сократили, получили.
[06:49.000 --> 06:52.000]  Короче говоря, здесь простые, простые выпутки.
[06:52.000 --> 06:55.000]  Вот теперь пусть у нас X0 равно нулю.
[06:55.000 --> 07:00.000]  Вот, то есть начинаем мы с нулевого вектора. Если начнем мы с нулевого, то там будет всего лишь движка, как сдвижка на этот самый вектор.
[07:00.000 --> 07:02.000]  Ничего особо практикально не поменяется.
[07:02.000 --> 07:10.000]  Вот, ну то есть вот здесь вот в семействе методов можно считать, что X0 это 0, потому что все равно здесь сдвигаемся.
[07:10.000 --> 07:15.000]  Вот, тогда у нас X1 будет равен альфа на E1.
[07:15.000 --> 07:18.000]  Вот, а X2 будет равен вот такому выражению.
[07:18.000 --> 07:30.000]  Теперь если мы внимательно посмотрим на то, что это за вектор X1, X2, то станет понятно, что в векторе X1 только одна компонента, не 0, это первая.
[07:30.000 --> 07:36.000]  Векторе X2 уже две компоненты, не 0, потому что вот у этого вектора два первых.
[07:37.000 --> 07:41.000]  Собственно, когда вы будете, ну да, два первых элемента не 0.
[07:41.000 --> 07:48.000]  И в силу структуры матрица, дальнейшее умножение на X2, X3 и так далее будет просто давать плюс одну не нулевую компоненту, начиная с первой.
[07:48.000 --> 07:59.000]  Вот, поэтому после K-итерации такого метода у нас все элементы с индексами больше K будут нулями.
[07:59.000 --> 08:03.000]  То есть после первой итерации у нас все индексы начинают со второго нуля.
[08:03.000 --> 08:07.000]  После второй итерации у нас все индексы начинают с третьего нуля и так далее.
[08:07.000 --> 08:12.000]  Вот, вот такой вот получается вектор, который потихонечку накапливает не нулевые значения.
[08:12.000 --> 08:24.000]  Теперь, если мы рассмотрим задачу, что мы хотим найти арг-миниум нашей функции при условии, что все х после K-той позиции нули,
[08:24.000 --> 08:32.000]  то это, по сути дела, означает, что мы просто взяли и сузили нашу задачу до размерности K.
[08:32.000 --> 08:35.000]  Изначально она у нас размерности 2K плюс 1.
[08:35.000 --> 08:40.000]  А мы берем только K на K, вот эту под матрицу первую, из первых строк и столбцов.
[08:40.000 --> 08:44.000]  Поэтому по аналогии можно получить решение для вот этой задачи.
[08:44.000 --> 08:51.000]  И это будет вот такое вот, ну то есть все то же самое, только здесь было N у нас вот здесь вот.
[08:51.000 --> 08:58.000]  Тут станет K. Все остальное нули в силу того, что мы тут написали.
[08:58.000 --> 09:03.000]  Теперь, собственно, самое главное. Подставляем это решение, получаем значение в этой точке.
[09:03.000 --> 09:09.000]  Соответственно, справедливо следующая оценка, что f от xK минус f от x со звездочкой.
[09:09.000 --> 09:18.000]  Это точно больше, чем f от xK со звездочкой, то есть оптимальное значение такое, что K компонент решений не нули первые.
[09:19.000 --> 09:23.000]  Вот, минус f от x со звездочкой. А вот это все мы посчитали уже.
[09:23.000 --> 09:27.000]  И если это все дело подставить, то мы получим вот ровно вот это выражение.
[09:27.000 --> 09:36.000]  Но сейчас у нас тут фигурирует K, но не фигурирует нигде норма разности между x0 и x со звездочкой.
[09:36.000 --> 09:38.000]  Давайте сейчас это добавим.
[09:38.000 --> 09:46.000]  Понятно, поскольку у нас x0 это 0, то норма, квадрат нормы x со звездочкой есть не что иное, как сумма квадратов координат.
[09:46.000 --> 09:54.000]  Вспоминаем классические формулы. Тут расписываем, тут у нас архиметическая прогрессия, тут сумма квадратов.
[09:54.000 --> 09:58.000]  И одна сумма и другая сумма имеет аналитическое выражение.
[09:58.000 --> 10:02.000]  Вот они тут приведены. Я надеюсь, что все понимают, как это выводится.
[10:02.000 --> 10:05.000]  Достаточно классическая техника.
[10:05.000 --> 10:17.000]  Вот тогда, подставляя вот эти выражения вот сюда, мы получаем, что наша норма x со звездочкой ограничена сверху выражением вида 2K плюс 1 на 3.
[10:17.000 --> 10:27.000]  То есть это значит, что K плюс 1 больше, чем 3 вторых норма, квадрат нормы разности x со звездочкой x0.
[10:27.000 --> 10:32.000]  Теперь все это дело вместе, если соединим. Здесь мы умножим на K плюс 1.
[10:32.000 --> 10:41.000]  И K плюс 1 у нас сверху ограничено. Поэтому в итоге получаем вот такое замечательное выражение, которое и совпадает с нашей нижней оценкой.
[10:41.000 --> 10:55.000]  Вот такая вот буквально 3-4 слайда на то, чтобы получить то, что для метода, который линьирует следующую точку, как некоторые начальные точки,
[10:55.000 --> 11:01.000]  плюс линейная комбинация градиентов, выполняется вот такое вот выражение.
[11:01.000 --> 11:06.000]  Теперь что делать с сильным опуклым случаем?
[11:06.000 --> 11:14.000]  Для сильного опуклого случая функция, на которой достигается, для которой выполнена тоже нижняя оценка, она примерно такая же.
[11:14.000 --> 11:20.000]  Только тут некоторая нормировка происходит на константу липшицы, на числобусловленности.
[11:20.000 --> 11:26.000]  И константа сильной опуклости мют добавляется поправочками в полам на квадраты второй нормы.
[11:26.000 --> 11:34.000]  И для нее можно проделать все то же самое и получит оценку через корень из числобусловленности.
[11:34.000 --> 11:41.000]  То есть вот то, что у нас есть. У нас есть оценка снизу на выпуклую функцию с лучшим градиентом.
[11:41.000 --> 11:47.000]  Есть оценка снизу на сильную опуклую функцию с лучшим градиентом. Вот это вот корень из капы здесь ниспялен.
[11:47.000 --> 11:51.000]  В то же время сходимость градиентного спуска всего лишь вот такая.
[11:52.000 --> 12:02.000]  То есть у него сходится разность между значение функции в точке cap plus 1 и оптимального значения функции меньше, чем cap plus 4.
[12:02.000 --> 12:10.000]  1 делить на cap plus 4. А здесь сходимость линейная, но коэффициент линейной сходимости cap minus 1 на cap plus 1.
[12:10.000 --> 12:12.000]  А здесь корень.
[12:12.000 --> 12:20.000]  Этот зазор, который тут есть, вроде как быстрее, чем вот так нельзя, но градиентный спуск сходится всего лишь вот так.
[12:20.000 --> 12:25.000]  Значит, наверное, можно предъявить какой-то метод, который будет сходиться в точности вот так.
[12:30.000 --> 12:31.000]  Сейчас секунду.
[12:32.000 --> 12:43.000]  Так, собственно отсюда мы и берем мотивацию и некоторую интуицию того, откуда берется, собственно,
[12:43.000 --> 12:51.000]  ускоренный градиентный метод, который, собственно, сейчас везде по большей части в больших задачах используется.
[12:51.000 --> 12:56.000]  Значит, для того чтобы подвести, откуда это все берется, рассмотрим сначала простую задачу,
[12:56.000 --> 13:02.000]  а именно квадратичная целевая функция, симметричную строго положить определенной матрицей,
[13:02.000 --> 13:05.000]  и для нее построим ответственный градиент.
[13:05.000 --> 13:13.000]  Из необходимых условий следует напрямую, что ax равно b, и нам надо всего лишь решить линейную систему.
[13:13.000 --> 13:20.000]  Дальше нам будет нужно обозначение, что градиент будет равен нас просто невязким.
[13:21.000 --> 13:29.000]  И мы, по сути дела, свели задачу оптимизации к задаче решения систем линейных уровней.
[13:29.000 --> 13:33.000]  Вот, это понятная, хорошо изученная задача.
[13:33.000 --> 13:41.000]  И сейчас посмотрим, каким образом метод скруженного градиента будет соотноситься с тем, что мы хотим получить, а именно с вот этим.
[13:43.000 --> 13:47.000]  Так, давайте историю я, наверное, пропущу, чтобы время сэкономить немножко.
[13:47.000 --> 14:03.000]  В случае, когда мы пытаемся решить задачу квадратичную и получить решение градиентным методом каким-то,
[14:03.000 --> 14:12.000]  то мы понимаем, что, в принципе, метод может сходиться безумно долго, если условия большие.
[14:12.000 --> 14:22.000]  Но поскольку у нас задача квадратичная, то это значит, что мы можем, грубо говоря, разложить решение по базису и за одну итерацию найти просто компоненту,
[14:22.000 --> 14:27.000]  ну, скаляр, который состоит перед соответствующим базисным вектором.
[14:27.000 --> 14:34.000]  Вот этот скаляр, то есть по большому счету, мы можем сойтись за энтерацией, в точной арифметике никаких проблем здесь не будет.
[14:34.000 --> 14:41.000]  Это не работает в случае для градиентного спуска, потому что он сходится, может бесконечно долго сходиться, если условия большие.
[14:41.000 --> 14:47.000]  И, собственно, хочется сделать как раз-таки такой метод, который для квадратичной задачи точно за энтерацией сойдется.
[14:47.000 --> 14:52.000]  Потому что решение, по сути, разложение по базису системы не уравнивает.
[14:52.000 --> 15:04.000]  Понятно ли вот эта мотивация? То есть, какую проблему хочется решить, которая бы наличествует в градиентном спуске в случае квадратичной задачи?
[15:04.000 --> 15:18.000]  То есть перейти от сходимости, которая может быть сколько угодно долгой, до, ну, там понятно будет какая-то линейная, но тем не менее,
[15:18.000 --> 15:26.000]  в принципе, при плохом, при достаточно вытянутых линейных уровнях, это может быть сильно больше, чем размерность пространства.
[15:26.000 --> 15:37.000]  Что не есть хорошо и выглядит как некоторые неестественные ограничения. Ясно, отлично. Едем дальше.
[15:37.000 --> 15:45.000]  Значит, как его делать? Для этого нам потребуется определение сопряженных относительно матрицы положительно определенных векторов,
[15:45.000 --> 15:54.000]  которые, собственно, их сопряженность означает, что они попарно, в случае различных индексов, зануляют соответствующий квадратичный упор.
[15:54.000 --> 16:01.000]  Если и не равно же, то это ноль. Ну, свойства, понятно, линейно-независимые.
[16:01.000 --> 16:07.000]  И если мы возьмем сопряженное направление шаг по наискорейшему спуску, то получим изначально тратство, которое будет сходиться.
[16:07.000 --> 16:12.000]  То есть мы как бы будем строить байлис относительно скалярного произведения порожденного матрицы A.
[16:12.000 --> 16:17.000]  Вот такая вот замечательная история. Соответственно, шаг будет определяться таким вот образом.
[16:17.000 --> 16:27.000]  Если мы вычтем из обеих частей по... Ну, в общем, переведем это все на язык невязок, то получим ровно вот это.
[16:27.000 --> 16:36.000]  Тут у нас будет стоять AX минус B. Получаем RK плюс 1 равно RK плюс альфа КПК.
[16:37.000 --> 16:48.000]  Ну, собственно, вопрос, как получать сопряженное направление, он довольно кажется очевидным в плане своей постановки.
[16:48.000 --> 16:57.000]  То есть понятно, что мы можем сгенерировать случайные вектора, но как сгенерировать сопряженные направления, чтобы было вот так?
[16:57.000 --> 17:04.000]  Есть у нас какие-то идеи, что в этом случае делать можно?
[17:04.000 --> 17:11.000]  Если мы знаем, что сопряженность это по сути дела артегональность относительно матрицы системы,
[17:11.000 --> 17:14.000]  которую положить на пределе нас строгом.
[17:14.000 --> 17:23.000]  На самом деле делать можно разное. Например, можно взять просто байлис собственных векторов для матрицы, и для него это будет работать.
[17:23.000 --> 17:31.000]  Можно запустить артегонализацию грамма Шмита только с скалярным произведением, которое этой матрице порождено, и все будет работать тоже.
[17:31.000 --> 17:36.000]  То есть путь не один, можно по-разному делать.
[17:36.000 --> 17:44.000]  Теория на сходимости, что если последовательно XK генерируется методом сопряженных направлений, то у нас есть артегональность направления к невязке,
[17:44.000 --> 17:57.000]  и XK есть не что иное, как армин функции на пространстве, которое совпадает с пространством X0 плюс линейной комбинацией всех наших направлений.
[17:57.000 --> 18:03.000]  Докажем сначала, что эти два условия эквиваленты.
[18:03.000 --> 18:11.000]  Если мы рассмотрим функцию phi-gamma, которая есть не что иное, как значение функции в точке, которая просто вот это.
[18:11.000 --> 18:22.000]  Рассмотрим и параметризуем эту задачу через вектор гамма, который есть вектор-компонент, с которыми эти вектора образуют X.
[18:23.000 --> 18:30.000]  Тогда она будет строго выпукла, потому что у нас функция положить на определенные матрицы.
[18:30.000 --> 18:36.000]  Следовательно, существует гамма-созвездочка, в которой достигается решение.
[18:36.000 --> 18:39.000]  Берем производную критерию первого порядка.
[18:39.000 --> 18:50.000]  Производная от гаммы есть не что иное, как гамма – это у нас вектор.
[18:50.000 --> 18:59.000]  Поэтому градиент берем по гаммам, получаем, что каждый раз,
[19:00.000 --> 19:10.000]  поскольку это у нас градиент, это его ИТ-компонента, эта ИТ-компонента равна вот такому скалярному произведению.
[19:10.000 --> 19:12.000]  И она должна равна.
[19:12.000 --> 19:15.000]  Но вот эта штука есть не что иное, как невязка.
[19:15.000 --> 19:20.000]  Поэтому это в точности соответствует тому, что невязка у нас артегональна направлению.
[19:20.000 --> 19:23.000]  Поэтому эти два условия эквивалентны.
[19:23.000 --> 19:26.000]  Теперь по индукции докажем первое.
[19:27.000 --> 19:31.000]  База индукции у нас просто ноль по построению.
[19:31.000 --> 19:42.000]  У нас R1 – это направление, которое…
[19:42.000 --> 19:51.000]  Это AX1-B.
[19:51.000 --> 19:59.000]  Если мы поставим в качестве, что такое у нас X1, то в силу того, как определялся шаг, мы получим тут ноль.
[19:59.000 --> 20:04.000]  Шаг определяется по правилам низкорейшего спуска, поэтому это будет работать.
[20:04.000 --> 20:12.000]  Гиппози, за что для Rk-1 и всех по ИТ-ых соответствующие скалярные произведения также зановляются.
[20:12.000 --> 20:15.000]  Давайте распишем, что такое Rk.
[20:15.000 --> 20:19.000]  Rk – это просто Rk-1 плюс вот это выражение.
[20:19.000 --> 20:25.000]  Смотрим скалярные произведения Pk-1, фиксированные по направлению, и Rk.
[20:25.000 --> 20:27.000]  Вставляем.
[20:27.000 --> 20:30.000]  И опять получаем ноль, потому что у нас Rk-1 так строится.
[20:30.000 --> 20:35.000]  В случае скорейшего спуска для квадратичной формы это просто ноль.
[20:35.000 --> 20:41.000]  Теперь посмотрим, что происходит для всех предыдущих по ИТ-ых.
[20:41.000 --> 20:44.000]  От 1 до k-2 теперь.
[20:44.000 --> 20:47.000]  Отставляем снова.
[20:47.000 --> 20:50.000]  И у нас что образуется?
[20:50.000 --> 20:53.000]  Вот эта штука сидит в точности вот здесь.
[20:53.000 --> 20:58.000]  А вот эта штука есть не что иное, как ноль, потому что у нас направление острижено.
[20:58.000 --> 21:03.000]  И никогда не равно, всегда меньше k-1.
[21:03.000 --> 21:10.000]  Поскольку направления являются спряженными, то эта штука равна ноль.
[21:10.000 --> 21:13.000]  Все ли понятно в выводе?
[21:13.000 --> 21:16.000]  Еще раз.
[21:16.000 --> 21:21.000]  Мы показали, что нет спряженных направлений генерирует нам последовательность,
[21:21.000 --> 21:25.000]  которая на каждой итерации, во-первых, минимизирует функцию на таком пространстве,
[21:25.000 --> 21:31.000]  во-вторых, дает артагональность, невязки и соответствующих направлений.
[21:31.000 --> 21:35.000]  Есть ли какие-то вопросы по этой части?
[21:35.000 --> 21:37.000]  Нет вопросов.
[21:37.000 --> 21:39.000]  Хорошо.
[21:39.000 --> 21:41.000]  Давайте дальше пойдем.
[21:41.000 --> 21:43.000]  Это мы показали.
[21:43.000 --> 21:46.000]  Теперь стопстандхуд берут спряженные градиенты.
[21:46.000 --> 21:48.000]  И что это такое?
[21:48.000 --> 21:52.000]  Мы будем генерировать спряженные направления хитрым образом.
[21:52.000 --> 21:57.000]  Ранее мы обсуждали, как их можно генерировать.
[21:57.000 --> 22:01.000]  Первое – это либо байсовство векторов, либо артагонализация специальная.
[22:01.000 --> 22:06.000]  Но и то, и то довольно затратно, потому что требует артагонализации
[22:06.000 --> 22:10.000]  ко всему предыдущему набору векторов, что долго и дорого.
[22:10.000 --> 22:14.000]  Метод спряженных градиентов позволяет генерировать
[22:14.000 --> 22:19.000]  спряженные направления только используя предыдущие направления.
[22:19.000 --> 22:24.000]  Это такая хитрая процедура, которая для устроения полного набора
[22:24.000 --> 22:29.000]  спряженных направлений не требует знания в текущий момент
[22:29.000 --> 22:33.000]  всех предыдущих направлений, только одного предыдущего.
[22:33.000 --> 22:36.000]  Это существенно экономит и память, и время.
[22:36.000 --> 22:38.000]  Как это работает?
[22:38.000 --> 22:43.000]  Сначала задаем направление по 0 равный минусу 0.
[22:43.000 --> 22:49.000]  Следующий направлений генерируется как предыдущий направлений
[22:49.000 --> 22:54.000]  с некоторым коэффициентом, и плюс, точнее, минус градиент.
[22:54.000 --> 22:57.000]  РК плюс 1.
[22:57.000 --> 23:03.000]  Бета К плюс 1 выбирается так, чтобы направления были спряжены.
[23:03.000 --> 23:09.000]  Удивительно образом, но то, что ПК спряжено ПК плюс 1,
[23:09.000 --> 23:13.000]  автоматически будет соответствовать тому, что ПК плюс 1
[23:13.000 --> 23:17.000]  будет спряжено и всему остальному набору направлений.
[23:17.000 --> 23:22.000]  Легко показать, как бы Бета К плюс 1.
[23:22.000 --> 23:25.000]  Просто условие спряженности, вот оно.
[23:25.000 --> 23:29.000]  Вот оно, вставляем, получаем 0.
[23:29.000 --> 23:34.000]  Отсюда Бета К плюс 1 равно замечательному выражению вот такому отношению,
[23:34.000 --> 23:38.000]  которое есть не что иное, как значение затратичной формы для разных векторов.
[23:38.000 --> 23:41.000]  То есть тут по сути два раза на одном матрицу умножить
[23:41.000 --> 23:44.000]  и посчитать скалярное произведение.
[23:44.000 --> 23:47.000]  Вседокод довольно прозрачный.
[23:47.000 --> 23:50.000]  Считаем невязку. Вначале П, это минус R.
[23:50.000 --> 23:54.000]  Пока у нас торм больше эпсил, мы считаем альфа по правилам нескорейшего спуска.
[23:54.000 --> 23:59.000]  Потом у нас Х обновляется в соответствии с этим самым направлением.
[23:59.000 --> 24:03.000]  Считаем невязку, считаем Бета по формуле вот этой вот.
[24:03.000 --> 24:06.000]  И обновляем П.
[24:06.000 --> 24:12.000]  Вот, собственно, буквально пять строчек получили мета спряженно-градиеновый для квадратичной формы.
[24:12.000 --> 24:21.000]  Можно его немножко ускорить, если переписать правила вычисления альфа через норму.
[24:21.000 --> 24:25.000]  И через... Да, тут знаменательный поменялся.
[24:25.000 --> 24:29.000]  То есть альфа и бета можно переписать так, чтобы было...
[24:29.000 --> 24:33.000]  Тут для бета было, то есть не было умножения матрицы на эктора,
[24:33.000 --> 24:37.000]  было просто вычисление норм для двух последовательных невязок.
[24:37.000 --> 24:42.000]  Тут следствие напрямую того, что у нас направление ортогональное,
[24:42.000 --> 24:47.000]  и невязки ортогональные, направления спряжены, а невязки ортогональные направления.
[24:47.000 --> 24:53.000]  То есть это просто пересчет формул, более быстрая версия, которая состоит в реализации.
[24:53.000 --> 25:00.000]  Теперь, почему спряженный градиент оказывается спряженными направлениями?
[25:00.000 --> 25:03.000]  Оказывается, что вот эту теорему я доказывать сейчас не буду,
[25:03.000 --> 25:07.000]  но просто она довольно... Четыре пункта, и они все равносильны.
[25:07.000 --> 25:12.000]  Это довольно длительная процедура с доказательством этого факта.
[25:12.000 --> 25:17.000]  Но, значит, идея в том, что после коэтерации, если мы не сошлись,
[25:17.000 --> 25:21.000]  то у нас ортогональные наши невязки будут,
[25:21.000 --> 25:25.000]  и пространство, которое натянута на невязки и на направления,
[25:25.000 --> 25:29.000]  есть не что иное, как пространство, которое натянута на вот такие вот векторы.
[25:29.000 --> 25:31.000]  То есть одно и то же пространство.
[25:31.000 --> 25:37.000]  И более того, эти самые направления оказываются действительно спряженными относительно этой матрицы.
[25:37.000 --> 25:42.000]  То есть на самом деле все сводится к тому, что вот это за пространство такое.
[25:42.000 --> 25:45.000]  И это пространство имеет специальное название.
[25:45.000 --> 25:48.000]  Это Крыловское пространство порядка K.
[25:48.000 --> 25:50.000]  И оно... То есть это определение.
[25:50.000 --> 25:53.000]  Это пространство натянута на вот такие векторы.
[25:53.000 --> 25:55.000]  Пространство Крылова.
[25:55.000 --> 25:57.000]  Вот. Да.
[25:57.000 --> 26:00.000]  Значит, основное свойство, которое...
[26:00.000 --> 26:03.000]  Ну, может быть, оно плюс-минус очевидно, тем не менее.
[26:03.000 --> 26:08.000]  То, что решение нашей системы лежит в пространстве Крылова этого порядка.
[26:08.000 --> 26:11.000]  Это несложно показать.
[26:11.000 --> 26:13.000]  У нас есть теория Магамильтона-Келли о том,
[26:13.000 --> 26:17.000]  что матрица является корнем своего характеристического многочлена.
[26:17.000 --> 26:21.000]  То есть если вы в качестве П возьмете вот этот самый дотерминат,
[26:21.000 --> 26:25.000]  в который характеристическое уравнение вставляете вместо лямбда А,
[26:25.000 --> 26:29.000]  получаете полинома от матрицы, и он занулился.
[26:29.000 --> 26:33.000]  Поэтому когда мы полинома от матрицы умножаем на B,
[26:33.000 --> 26:36.000]  мы получаем вот такую штуку.
[26:39.000 --> 26:42.000]  Да, и она равна нулю, понятной причины.
[26:42.000 --> 26:48.000]  Теперь умножим слева и справа на минус первый.
[26:48.000 --> 26:51.000]  У нас тут степень понизится,
[26:51.000 --> 26:59.000]  и останется последнее слагаемое вида AN на A-1B.
[26:59.000 --> 27:04.000]  Ну и, собственно, это A-1B мы тут благополучно можем выразить.
[27:04.000 --> 27:09.000]  Внимание, вопрос. Почему мы можем поделить на AN?
[27:09.000 --> 27:11.000]  Есть ли у кого-то понимание?
[27:11.000 --> 27:14.000]  Надо немножко вспомнить теорию многочленов.
[27:14.000 --> 27:20.000]  Как у них коэффициенты разложения связаны с корнями, например?
[27:20.000 --> 27:23.000]  Ну, коэффициенты, да, коэффициенты,
[27:23.000 --> 27:26.000]  ну не разложения только, а вот коэффициенты записи.
[27:26.000 --> 27:29.000]  А как связано A-1AN с корнем?
[27:29.000 --> 27:32.000]  С корнями многочлена, чтобы ответить на вот этот вопрос.
[27:32.000 --> 27:35.000]  Ну, нам еще один переход будет делать, но он посмелся очевиден.
[27:35.000 --> 27:41.000]  Есть ли понимание и помнит ли кто-то о том, как это работает?
[27:41.000 --> 27:48.000]  Почти, только, по-моему, след, это как раз-таки A-1, по-моему, след.
[27:48.000 --> 27:51.000]  А вот здесь это там произведение,
[27:51.000 --> 27:56.000]  произведение вроде как этих собственных значений.
[27:56.000 --> 28:04.000]  То есть последний врунутость, ремьет, короче, говорят, что у вас там C это произведение корней,
[28:04.000 --> 28:09.000]  нам делены, понятно, на правильную вещь.
[28:09.000 --> 28:11.000]  А второй там сумма.
[28:11.000 --> 28:18.000]  В случае больших размерностей у вас вроде последние множители произведения,
[28:18.000 --> 28:21.000]  поэтому если тут будет хотя бы один ноль,
[28:21.000 --> 28:25.000]  то матрица перестает быть положительно полуопределенной,
[28:25.000 --> 28:28.000]  и становится положительно определенной строго,
[28:28.000 --> 28:30.000]  становится выраженной, и все грустно.
[28:30.000 --> 28:33.000]  Поэтому очень важно делить.
[28:33.000 --> 28:40.000]  Ну и смотрим просто на выражение и понимаем, что на самом деле тут в точности записано вот это.
[28:40.000 --> 28:44.000]  То есть действительно тут есть константный вектор,
[28:44.000 --> 28:48.000]  ну есть вектор B, ну и так далее до A n-1.
[28:48.000 --> 28:55.000]  То есть тут вроде все хорошо, но понятно, что доводить это все до n нам не хочется,
[28:55.000 --> 28:57.000]  хочется чуть-чуть сократить вычисление.
[28:57.000 --> 29:02.000]  То есть на самом деле мы пытаемся найти наилучшее приближение
[29:02.000 --> 29:07.000]  на пространство крылого катового порядка, что ли.
[29:07.000 --> 29:15.000]  При этом направления, которые мы используем, они не равны тем направлениям, которые генерируют это пространство.
[29:15.000 --> 29:18.000]  Как вы думаете, почему?
[29:18.000 --> 29:22.000]  Почему нельзя использовать вот эти векторы, например,
[29:22.000 --> 29:26.000]  для того чтобы в медленность пространных направлений просто по ним идти?
[29:27.000 --> 29:36.000]  Что не так может случиться, что может сломаться вот с этими вот направлениями, как вы думаете?
[29:36.000 --> 29:43.000]  Вот на самом деле несложно увидеть, что если вот эта штука вы будете ее генерировать,
[29:43.000 --> 29:48.000]  то векторы, которые получаются умножением одного и того же векторного матрица много раз,
[29:48.000 --> 29:51.000]  они получаются все более и более коллинеарными.
[29:51.000 --> 29:55.000]  И в конце концов они, кстати, если их еще немножко нормировать,
[29:55.000 --> 29:59.000]  будут сходиться к собственному вектору, соответствующему максимальному по модулю собственному значению.
[29:59.000 --> 30:01.000]  Вот такой замечательный факт.
[30:01.000 --> 30:05.000]  Поэтому, когда мы будем пытаться строить на их основе решения,
[30:05.000 --> 30:08.000]  то у нас будут коэффициенты очень большими, потому что они будут...
[30:08.000 --> 30:14.000]  Короче, мы будем пытаться разложить решения по байсу из почти коллинеарных векторов.
[30:14.000 --> 30:18.000]  Это не очень хорошая идея с точки зрения вычислений.
[30:18.000 --> 30:21.000]  Поэтому направления делаются по-другому.
[30:21.000 --> 30:26.000]  А именно вот этот байс, он в процессе итерации, он артагонализуется.
[30:26.000 --> 30:32.000]  То есть мы генерируем наше крыловское пространство с помощью артагонального базисы.
[30:32.000 --> 30:34.000]  Вот это.
[30:34.000 --> 30:40.000]  И по сути, весь мед. сп. градиентов мы ищем решение в артагональном, в артонормированном крыловском базисе.
[30:40.000 --> 30:42.000]  Теперь посмотрим, как он будет сходиться.
[30:43.000 --> 30:47.000]  Понятен ли посыл вот здесь? Это важное место для понимания того,
[30:47.000 --> 30:50.000]  как мед. сп. градиентов классически работает.
[30:50.000 --> 30:54.000]  Что на самом деле мы просто пытаемся найти решение линии системы,
[30:54.000 --> 30:58.000]  но это самое решение мы ищем в специальном пространстве крыловского.
[30:58.000 --> 31:04.000]  Оно хорошо тем, что для его артагонализации нужно знать всего лишь...
[31:04.000 --> 31:09.000]  Для получения следующего вектора надо знать всего лишь текущую точку и предыдущий вектор.
[31:09.000 --> 31:13.000]  И все остальное будет получаться по явным формам.
[31:13.000 --> 31:18.000]  Это, собственно, основная идея, которая сейчас будет активно использоваться
[31:18.000 --> 31:21.000]  для получения утверждения сходимости этого меда.
[31:21.000 --> 31:24.000]  Есть ли тут вопросы какие-то?
[31:33.000 --> 31:37.000]  Пауза, чтобы немножко выдохнуть и осмыслить произошедшее.
[31:39.000 --> 31:41.000]  Вроде в чате вопросов нет.
[31:41.000 --> 31:44.000]  Ну хорошо, давайте тогда пойдем дальше.
[31:44.000 --> 31:48.000]  Сейчас будет немного несколько таких не очень легких слайдов,
[31:48.000 --> 31:50.000]  но я думаю, мы справимся.
[31:50.000 --> 31:55.000]  Значит, смотрите, решение понятно как записывается.
[31:55.000 --> 32:00.000]  При этом минимум функции, если мы просто это дело подставим,
[32:00.000 --> 32:07.000]  то мы получим, что тут все сократится,
[32:07.000 --> 32:09.000]  останется вот это выражение.
[32:09.000 --> 32:13.000]  А это выражение, если что иное, как их созвелось в а-норме.
[32:13.000 --> 32:15.000]  При этом а-норма, это вот так.
[32:15.000 --> 32:17.000]  Надо расширить другой экран.
[32:25.000 --> 32:27.000]  Что такое а-норма?
[32:27.000 --> 32:29.000]  Полезно знать.
[32:33.000 --> 32:37.000]  А-норма, это такая норма, которая порождена матрицей А.
[32:40.000 --> 32:46.000]  Соответственно, наше решение, которое мы можем записать в а-норме,
[32:46.000 --> 32:48.000]  это что такое?
[32:52.000 --> 32:57.000]  И к соответствии, у нас а-1 на b транспонированное,
[32:57.000 --> 33:00.000]  а на а-1b.
[33:00.000 --> 33:07.000]  Ну и мы в точности получим, что b транспонированное а-1b.
[33:07.000 --> 33:11.000]  Поскольку матрица симметрична, то транспонированный тоже будет симметричный.
[33:11.000 --> 33:16.000]  И там справедливо, что так, что так.
[33:16.000 --> 33:18.000]  В общем одно и то же.
[33:18.000 --> 33:20.000]  Распонирование.
[33:20.000 --> 33:22.000]  Поэтому иногда это еще записывать вот таким вот знаком,
[33:22.000 --> 33:24.000]  типа минус транспонирование.
[33:27.000 --> 33:28.000]  Так.
[33:28.000 --> 33:30.000]  Распонирование.
[33:30.000 --> 33:31.000]  Так.
[33:31.000 --> 33:34.000]  Возвращаемся к слайду.
[33:34.000 --> 33:38.000]  Вот ровно оно тут и получилось.
[33:38.000 --> 33:39.000]  Далее.
[33:39.000 --> 33:41.000]  По функции сходимость, естественно, будет такой.
[33:41.000 --> 33:44.000]  Мы из f от x вычитаем их с созвездочкой.
[33:44.000 --> 33:48.000]  А дальше понимаем, что вот здесь у нас стоит а-норма x.
[33:48.000 --> 33:52.000]  Вот здесь стоит на самом деле x транспонированный на ах-созвездочке,
[33:52.000 --> 33:54.000]  потому что это просто b.
[33:54.000 --> 33:56.000]  Ах-созвездочка это b.
[33:56.000 --> 33:59.000]  Следовательно, это все можно как полный квадрат выделить
[33:59.000 --> 34:02.000]  и получить, что сходимость по функции это то же самое,
[34:02.000 --> 34:05.000]  что и сходимость по аргументу, только в анорм.
[34:05.000 --> 34:07.000]  Вот.
[34:07.000 --> 34:12.000]  Вот такая вот связь между этими двумя типами сходимости существует.
[34:12.000 --> 34:15.000]  Соответственно, нам далее достаточно определить,
[34:15.000 --> 34:20.000]  что будет происходить вот с этим вот выражением в процессе итерирования.
[34:20.000 --> 34:21.000]  Вот.
[34:21.000 --> 34:24.000]  И это же нам сразу же даст сходимость по функции.
[34:24.000 --> 34:25.000]  Вот.
[34:25.000 --> 34:27.000]  Ну и теперь, собственно, давайте сделаем там 5 минут перерыв
[34:27.000 --> 34:31.000]  на то, чтобы, перед тем как приступить к анализу сходимости метод
[34:31.000 --> 34:34.000]  сформированных градиентов, он здесь не очень простым будет.
[34:34.000 --> 34:36.000]  Все.
[34:36.000 --> 34:39.000]  5 минут перерыв продолжаем в 11.36.
[34:45.000 --> 34:46.000]  Так.
[34:46.000 --> 34:48.000]  Ну, давай продолжим.
[34:48.000 --> 34:51.000]  Собственно, аналии сходимости сформированных градиентов,
[34:51.000 --> 34:55.000]  не очень простая штука, но, в принципе, довольно понятно,
[34:55.000 --> 34:57.000]  если немножко подумать.
[34:57.000 --> 35:01.000]  Значит, поскольку у нас экската лежит в каосском пространстве,
[35:01.000 --> 35:05.000]  то мы можем его представить в виде вот такой вот линейной комбинации векторов,
[35:05.000 --> 35:07.000]  которые в процессе итерировали.
[35:07.000 --> 35:08.000]  Вот.
[35:08.000 --> 35:11.000]  Это есть не что иное, как матричный полином, умноженный на вектор.
[35:11.000 --> 35:13.000]  Вот.
[35:13.000 --> 35:16.000]  При этом, ну, то есть это некоторый полином в степени не вышедший,
[35:16.000 --> 35:19.000]  при этом, ну, то есть это некоторый полином в степени не выше k-1,
[35:19.000 --> 35:22.000]  потому что у нас, понятно, есть ограничение на размер,
[35:22.000 --> 35:24.000]  ну, на количество итераций.
[35:24.000 --> 35:29.000]  При этом мы знаем, что xk минимизирует нашу функцию в каосском пространстве,
[35:29.000 --> 35:33.000]  то есть, учитывая вот это вот соотношение о том,
[35:33.000 --> 35:38.000]  что 2 на fx-f1 равняется второй норме, а-норме.
[35:38.000 --> 35:42.000]  Это значит, что вот происходит вот это вот, выполнена вот эта равенство.
[35:42.000 --> 35:47.000]  То есть мы минимизируем нашу а-норму на Крыловском пространстве.
[35:47.000 --> 35:52.000]  Это значит, что мы ищем такой полином в степени не выше k.
[35:52.000 --> 35:55.000]  Ну, тут вот не выше k-1, вот.
[35:55.000 --> 36:00.000]  Ну, понятно, что степень меньше k, то есть k-1 меньше.
[36:00.000 --> 36:01.000]  Вот.
[36:01.000 --> 36:03.000]  Вот, вот такого вот выражения.
[36:03.000 --> 36:06.000]  То есть мы свели задачу поиска x,
[36:06.000 --> 36:10.000]  задачу поиска полинома, которым будем действовать на правую часть.
[36:10.000 --> 36:11.000]  Вот.
[36:11.000 --> 36:14.000]  Ну, дальше немножко математики.
[36:14.000 --> 36:15.000]  Что называется?
[36:15.000 --> 36:19.000]  Поскольку у нас матрица симметричная, положительно строго определенная,
[36:19.000 --> 36:21.000]  то, да, тут вот на самом деле транспонирование,
[36:21.000 --> 36:23.000]  а не звездочку, потому что у нас действительно случай.
[36:23.000 --> 36:24.000]  Ну, неважно.
[36:24.000 --> 36:28.000]  Короче говоря, тут транспонирование, потому что это ортогональная матрица.
[36:28.000 --> 36:30.000]  Вот, никаких комплексных чисел у нас тут нет,
[36:30.000 --> 36:34.000]  поэтому избыточная степень общности, скажем так.
[36:34.000 --> 36:35.000]  Вот.
[36:35.000 --> 36:38.000]  Получаем, что вот эта штука есть не что иное, как...
[36:38.000 --> 36:41.000]  На самом деле, если расписать, что такое а норма,
[36:41.000 --> 36:44.000]  и а у нас это вот такое вот имеет разложение,
[36:44.000 --> 36:46.000]  то это будет полинома диагональной матрицы,
[36:46.000 --> 36:48.000]  минус обратной к диагональной,
[36:48.000 --> 36:50.000]  и еще и норма становится диагональной.
[36:50.000 --> 36:51.000]  Вот.
[36:51.000 --> 36:54.000]  То есть полная диагонализация вот этого вот выражения.
[36:54.000 --> 36:55.000]  Вот.
[36:55.000 --> 36:59.000]  Если его расписать, то окажется, что это на самом деле...
[36:59.000 --> 37:02.000]  Тут еще квадрат стоит вот у этой нормы.
[37:02.000 --> 37:05.000]  Поэтому это будет не что иное, как сумма.
[37:05.000 --> 37:10.000]  Будет сумма отношений, где вот это деление на лямбдаиты
[37:10.000 --> 37:13.000]  вот происходит ровно потому, что здесь один делить на лямбдаиты
[37:13.000 --> 37:16.000]  элементы обратной матрицы диагональной.
[37:16.000 --> 37:17.000]  Вот.
[37:17.000 --> 37:19.000]  Тепень полинома по-прежнему сохранился.
[37:19.000 --> 37:21.000]  Теперь, если немножко...
[37:21.000 --> 37:23.000]  Ну, понятно, вот этот новый некоторый полином
[37:23.000 --> 37:26.000]  в степени уже может потенциально равный к,
[37:26.000 --> 37:28.000]  потому что мы тут на лямбда еще домножили.
[37:28.000 --> 37:29.000]  Вот.
[37:29.000 --> 37:31.000]  И в нуле эта штука единичка.
[37:31.000 --> 37:33.000]  Вот мы получили.
[37:33.000 --> 37:35.000]  Новое наше выражение.
[37:35.000 --> 37:36.000]  Вот.
[37:36.000 --> 37:38.000]  Для некоторого полинома.
[37:38.000 --> 37:39.000]  Вот.
[37:39.000 --> 37:41.000]  Понятно ли, может быть, не совсем подробно
[37:41.000 --> 37:43.000]  детальный переход отсюда сюда,
[37:43.000 --> 37:45.000]  но вот если по модуле этого перехода,
[37:45.000 --> 37:48.000]  понятно ли все, что происходило далее?
[38:04.000 --> 38:08.000]  Или есть какие-то вопросы?
[38:08.000 --> 38:10.000]  Что откуда взялось?
[38:10.000 --> 38:11.000]  Слойка следующая.
[38:11.000 --> 38:15.000]  Мы ищем наш х в крыловском пространстве.
[38:15.000 --> 38:17.000]  В крыловском пространстве выражается через
[38:17.000 --> 38:19.000]  минимум 100 х.
[38:19.000 --> 38:21.000]  Это полинома от матрицы.
[38:21.000 --> 38:22.000]  Вот.
[38:22.000 --> 38:24.000]  И поиск ха сведен к поиску полинома.
[38:24.000 --> 38:25.000]  Вот.
[38:25.000 --> 38:27.000]  Дальше происходит некая манипуляция для того,
[38:27.000 --> 38:30.000]  чтобы упростить выражение, которое нам надо минимизировать.
[38:30.000 --> 38:31.000]  Вот.
[38:31.000 --> 38:34.000]  И уточнить требования на полином.
[38:34.000 --> 38:37.000]  Вот все, что здесь происходит.
[38:39.000 --> 38:41.000]  В чате по выражению нет вопросов.
[38:41.000 --> 38:42.000]  Ладно.
[38:42.000 --> 38:43.000]  Тогда давайте дальше.
[38:43.000 --> 38:45.000]  Надеюсь, все понятно.
[38:45.000 --> 38:48.000]  Теперь можно это дело оценить.
[38:48.000 --> 38:49.000]  Каким образом?
[38:49.000 --> 38:51.000]  Вот здесь у нас есть что?
[38:51.000 --> 38:57.000]  Есть значение q от лямбитого.
[38:57.000 --> 38:59.000]  Что здесь зависит от полинома?
[38:59.000 --> 39:00.000]  Давайте поймем.
[39:00.000 --> 39:02.000]  По полиному здесь зависит только q.
[39:02.000 --> 39:06.000]  А вот выражение dt на лямбдт от q не зависит.
[39:06.000 --> 39:07.000]  Вот.
[39:07.000 --> 39:09.000]  Поэтому давайте заменим.
[39:09.000 --> 39:11.000]  Вынесем эту сумму за скобки.
[39:11.000 --> 39:13.000]  А q от лямбдитого, сумма pi,
[39:13.000 --> 39:16.000]  заменим просто на максимум по всем лямбдитам.
[39:16.000 --> 39:19.000]  Таким образом, наша неравенство...
[39:19.000 --> 39:21.000]  Ну, мы получим неравенство,
[39:21.000 --> 39:23.000]  где некоторая константа умножается на инфиво.
[39:23.000 --> 39:24.000]  Вот.
[39:24.000 --> 39:26.000]  Вот эта штука, если внимательно ее расписать,
[39:26.000 --> 39:28.000]  есть не что иное, как
[39:28.000 --> 39:30.000]  квадрата нормы для х со звездочкой.
[39:30.000 --> 39:31.000]  Вот.
[39:31.000 --> 39:33.000]  А здесь у нас этот самый инфимум остался.
[39:33.000 --> 39:34.000]  Теперь.
[39:34.000 --> 39:36.000]  То есть что это значит?
[39:36.000 --> 39:37.000]  Если мы...
[39:37.000 --> 39:39.000]  Нам надо найти инфимум
[39:39.000 --> 39:42.000]  по всем полиномам.
[39:42.000 --> 39:44.000]  Вот так, чтобы...
[39:44.000 --> 39:49.000]  А, инфимум по всем полиномам от максимума q от лямбды.
[39:49.000 --> 39:51.000]  От лямбдитов в квадрате.
[39:51.000 --> 39:52.000]  Теперь.
[39:52.000 --> 39:56.000]  Пусть у нас есть у матрицы А m различные собственные значения.
[39:56.000 --> 39:58.000]  Рассмотрим вот такой полином.
[39:58.000 --> 40:01.000]  То есть ионно зависит от y.
[40:01.000 --> 40:04.000]  Здесь стоит произведение y минус лямбда и t.
[40:04.000 --> 40:06.000]  Делить на произведение всех лямбд.
[40:06.000 --> 40:09.000]  И умножим еще на минус единичку в степени m.
[40:09.000 --> 40:10.000]  Вот.
[40:10.000 --> 40:12.000]  Что мы при нем можем сказать?
[40:12.000 --> 40:14.000]  Во-первых, в нуле он единица.
[40:16.000 --> 40:19.000]  Собственно, вот эта вот минус единичка в степени m
[40:19.000 --> 40:23.000]  это поправка на знак вот этого выражения в нуле.
[40:23.000 --> 40:25.000]  И степень равна m...
[40:25.000 --> 40:29.000]  И степень равна m, потому что их всего m разных штук.
[40:29.000 --> 40:32.000]  И вот здесь вот фигурирует y в степени m.
[40:32.000 --> 40:34.000]  Вот.
[40:34.000 --> 40:36.000]  Что же мы при нем можем сказать?
[40:36.000 --> 40:40.000]  Что, поскольку здесь мы берем инфимум по всем полиномам,
[40:40.000 --> 40:42.000]  но мы предъявили конкретный полином,
[40:42.000 --> 40:45.000]  который удовлетворит всем нашим ограничениям.
[40:45.000 --> 40:47.000]  Поэтому если мы подставим его сюда,
[40:47.000 --> 40:49.000]  то мы получим еще просто оценку сверху.
[40:49.000 --> 40:53.000]  В итоге, наша оценка на f, она больше ли оранули,
[40:53.000 --> 40:56.000]  по понятным причинам, просто по определению.
[40:56.000 --> 41:00.000]  И сверху ограничено 1 и 2 на норму,
[41:00.000 --> 41:02.000]  а-норму х звездочек в квадрате,
[41:02.000 --> 41:05.000]  на максимум по r от λi.
[41:05.000 --> 41:07.000]  Но λi это спектр,
[41:07.000 --> 41:10.000]  а в каждом из λi наши значения равны 0.
[41:10.000 --> 41:13.000]  Поэтому здесь мы тоже получаем 0.
[41:13.000 --> 41:16.000]  В итоге, что выяснилось?
[41:16.000 --> 41:21.000]  Выяснилось, что если у матрицы A есть m различных собственных значений,
[41:21.000 --> 41:27.000]  то наше отличие между fk и fm,
[41:27.000 --> 41:29.000]  тут на самом деле не k,
[41:29.000 --> 41:31.000]  надо написать ам,
[41:31.000 --> 41:35.000]  степень m,
[41:35.000 --> 41:37.000]  тут степень k,
[41:37.000 --> 41:39.000]  поэтому тут fm.
[41:39.000 --> 41:42.000]  Минус f со звездочкой в точности совпадает.
[41:42.000 --> 41:45.000]  То есть мы сошли за m-итерацией.
[41:45.000 --> 41:48.000]  Таким образом, какой бы огромной ни была наша задача,
[41:48.000 --> 41:51.000]  сколько бы миллионов там не было переменных,
[41:51.000 --> 41:53.000]  если спектр матрицы такой,
[41:53.000 --> 41:57.000]  что там всего лишь m различных собственных значений,
[41:57.000 --> 42:00.000]  то мы с помощью метода сопряженных градиентов
[42:00.000 --> 42:04.000]  сойдемся ровно за m-итерацией в точной ритуции.
[42:04.000 --> 42:07.000]  Понятен ли этот вывод?
[42:07.000 --> 42:10.000]  Он тут довольно существенный.
[42:10.000 --> 42:14.000]  Окей, идем дальше.
[42:14.000 --> 42:17.000]  Значит да, за м-итерацией нашлись.
[42:17.000 --> 42:20.000]  Например, n равно 100,
[42:20.000 --> 42:24.000]  и спектр сойдет из четырех значений.
[42:24.000 --> 42:26.000]  1, 10, 100 тысяч.
[42:26.000 --> 42:29.000]  Число обусловленности, обращая ваше внимание, равно 1000.
[42:29.000 --> 42:31.000]  Это довольно много.
[42:31.000 --> 42:33.000]  Ну и собственно спектр вот так, естественно.
[42:33.000 --> 42:37.000]  Значит как будет сходиться метод сопряженных градиентов и градиентный спуск?
[42:37.000 --> 42:40.000]  Ну вот метод сопряженных градиентов, эта синяя линия,
[42:40.000 --> 42:42.000]  сходит за 4 итерации.
[42:43.000 --> 42:46.000]  По норме градиента к 10-8 и ниже.
[42:46.000 --> 42:50.000]  А метод градиентного спуска сходится медленно, медленно,
[42:50.000 --> 42:53.000]  застой ты рад с значением минус 6 по функции.
[42:53.000 --> 42:56.000]  И, ну, про норму градиента я вообще молчу.
[42:56.000 --> 42:59.000]  Тут даже несколько единиц, я так понимаю, значений.
[42:59.000 --> 43:02.000]  То есть даже меньше единиц он не стал.
[43:02.000 --> 43:06.000]  И это все в случае использования правил на искрежа спуска.
[43:06.000 --> 43:09.000]  То есть это лучшее, что можно взять.
[43:09.000 --> 43:13.000]  То есть шаг подбирается наилучшим образом локально,
[43:13.000 --> 43:16.000]  как это может быть в методе градиентного спуска.
[43:16.000 --> 43:23.000]  Вот разительная, существенная разница в поведении этих двух методов.
[43:23.000 --> 43:26.000]  Вот, так, тут номер итерации.
[43:26.000 --> 43:31.000]  Да, я надеюсь все графики легенды довольно хорошо видны и понятно.
[43:31.000 --> 43:34.000]  Вот, значит что дальше?
[43:34.000 --> 43:37.000]  Дальше можно, если брать другие полиномы,
[43:37.000 --> 43:40.000]  типа Чебушовского, то можно получить, собственно,
[43:40.000 --> 43:43.000]  наши оценки через корень искапы.
[43:43.000 --> 43:45.000]  Вот, который и...
[43:45.000 --> 43:48.000]  К чему все это, собственно, было рассказано?
[43:48.000 --> 43:52.000]  К тому, что в случае, если у нас квадратичная целевая функция,
[43:52.000 --> 43:55.000]  то медленно-градиентом дает оценку линейной сходимости,
[43:55.000 --> 43:59.000]  ровно такую, которая соответствует оценке снизу.
[43:59.000 --> 44:03.000]  Вот, но это только для квадратичной целевой функции пока что.
[44:03.000 --> 44:06.000]  Вот корень искапы, тут вот, да, принципиально
[44:06.000 --> 44:10.000]  неулучшаемая оценка для методов, которые генерируют решение
[44:10.000 --> 44:15.000]  как в пространстве натянутым на градиент.
[44:15.000 --> 44:18.000]  Значит, вот, это важный момент здесь.
[44:18.000 --> 44:23.000]  Теперь, значит, понятно, что можно общить этот метод
[44:23.000 --> 44:25.000]  на некую дратичную функцию.
[44:25.000 --> 44:29.000]  Для этого вместо скорейшего спуска по альфу надо подбирать его
[44:29.000 --> 44:32.000]  просто адаптивно на основе тех правил, которые мы ранее обсуждали,
[44:32.000 --> 44:36.000]  там типа правила армии и так далее, убывания, вот эта вся история.
[44:36.000 --> 44:39.000]  Коэффициент мы ищем с помощью градиентов
[44:39.000 --> 44:43.000]  на основе либо методов Летчер-Ривса отношения,
[44:43.000 --> 44:46.000]  либо Полак-Рибьер, либо Хестен-Счтифель,
[44:46.000 --> 44:49.000]  вот такие вот различные выражения для бета подбираются.
[44:49.000 --> 44:52.000]  Вот, а невязка, все остальное остается прежним.
[44:52.000 --> 44:55.000]  То есть невязка заменяется на градиент,
[44:55.000 --> 44:58.000]  бета считается через градиенты, альф подбирается адаптивно.
[44:58.000 --> 45:02.000]  Все, получили зоопарк методов спяженных градиентов
[45:02.000 --> 45:05.000]  для неквадратичной целевой функции.
[45:05.000 --> 45:09.000]  Откуда это все берется, я как бы в детали вдаваться сейчас не буду,
[45:09.000 --> 45:12.000]  вот это берется просто из того, что мы когда записывали
[45:12.000 --> 45:15.000]  наше решение для квадратичной целевой функции,
[45:15.000 --> 45:18.000]  то у нас был вид вот такой вот.
[45:18.000 --> 45:21.000]  Но это что такое? Это просто градиенты.
[45:21.000 --> 45:24.000]  Градиенты на ка плюс первые итерации на ка.
[45:24.000 --> 45:27.000]  Все, вот как бы заменили просто по аналогии,
[45:27.000 --> 45:31.000]  по формулы и результаты для которых потом дополнительно выводится.
[45:31.000 --> 45:34.000]  Вот это берется из, наверное, из каких-то других соображений.
[45:34.000 --> 45:37.000]  Но в общем сейчас без подробностей, просто что вот есть такие
[45:37.000 --> 45:40.000]  три разных метода, они реализованы в большинстве пакетов,
[45:40.000 --> 45:43.000]  их можно спокойно вызвать и использовать.
[45:43.000 --> 45:46.000]  Проблема неквадратичного случая в том, что с ростом
[45:46.000 --> 45:49.000]  числа итерации направления могут становиться уже коллинеарными.
[45:49.000 --> 45:52.000]  То есть в квадратичном случае у нас что было?
[45:52.000 --> 45:55.000]  У нас была там артагональность, все хорошо.
[45:55.000 --> 45:58.000]  Вот как только мы переходим к неквадратичному случаю,
[45:58.000 --> 46:01.000]  у нас все эти свойства благополучно пропадают.
[46:01.000 --> 46:04.000]  И все может пойти не так, как хотелось бы.
[46:04.000 --> 46:07.000]  Поэтому надо делать restart.
[46:07.000 --> 46:10.000]  Мы интервируемся.
[46:10.000 --> 46:13.000]  Поняли, что наши направления стали артагональными.
[46:13.000 --> 46:16.000]  Забываем всю историю и начинаем двигаться с нуля.
[46:16.000 --> 46:19.000]  Это помогает немного как бы освежить процесс
[46:19.000 --> 46:22.000]  и позволит ему не тащить за собой хвост
[46:22.000 --> 46:25.000]  слишком старых итераций, которые уже совсем не релевантны
[46:25.000 --> 46:28.000]  в данной локальной точке.
[46:28.000 --> 46:31.000]  Это к тому, если возвращаться к обсуждению общей схемы работы методов,
[46:31.000 --> 46:34.000]  что какая общая информация задачи у нас есть.
[46:34.000 --> 46:37.000]  Вот здесь у нас информация задачи, мы ее генерируем
[46:37.000 --> 46:40.000]  это направление градиентов и сопряженные направления,
[46:40.000 --> 46:43.000]  которые используют знание градиента с предыдущей точки.
[46:43.000 --> 46:46.000]  И актуализация этой информации
[46:46.000 --> 46:49.000]  выражается в том, что
[46:49.000 --> 46:52.000]  мы не можем делать restart.
[46:52.000 --> 46:55.000]  Это довольно перспективная техника,
[46:55.000 --> 46:58.000]  которая много где может вам встретиться.
[46:58.000 --> 47:01.000]  Получается направление убывания.
[47:01.000 --> 47:04.000]  Не всегда при адаптивном поиске у нас будет направление убывания.
[47:04.000 --> 47:07.000]  Там есть некоторые специальные условия.
[47:07.000 --> 47:10.000]  Это вы можете довольно легко получить,
[47:10.000 --> 47:13.000]  что должно выполняться для направления убывания
[47:13.000 --> 47:16.000]  для P и для Альфа,
[47:16.000 --> 47:19.000]  чтобы их произведение было направлением убывания,
[47:19.000 --> 47:22.000]  и вы не ушли слишком далеко
[47:22.000 --> 47:25.000]  от текущей точки,
[47:25.000 --> 47:28.000]  чтобы аппроксимация вашей функции
[47:28.000 --> 47:31.000]  все еще работала.
[47:31.000 --> 47:34.000]  Это будет через две недели.
[47:34.000 --> 47:37.000]  Возможно, через две недели, наверное,
[47:37.000 --> 47:40.000]  или на следующей неделе.
[47:40.000 --> 47:43.000]  Тоже довольно интересно.
[47:43.000 --> 47:46.000]  Еще раз вспомним про этот метод,
[47:46.000 --> 47:49.000]  когда будем клоизинцовский метод обсуждать.
[47:49.000 --> 47:52.000]  У них есть интересная интерпретация тоже через спрешенный градиент.
[47:52.000 --> 47:55.000]  Это все, что я хотел сказать про метод спрешенного градиента.
[47:55.000 --> 47:58.000]  У нас осталось 20 минут ровно на два метода,
[47:58.000 --> 48:01.000]  которые самые интересные, общие,
[48:01.000 --> 48:04.000]  которые будут отчасти закрывать вопрос
[48:04.000 --> 48:07.000]  об оптимальности градиентных методов,
[48:07.000 --> 48:10.000]  которые лежат в том классе,
[48:10.000 --> 48:13.000]  который я несколько раз показывал.
[48:13.000 --> 48:16.000]  То есть когда у нас х плюс 1, это х0 плюс
[48:16.000 --> 48:19.000]  линейное пространство, натянутое на градиенты во всех предыдущих точках.
[48:19.000 --> 48:22.000]  Итак, метод тяжелого шарика.
[48:22.000 --> 48:25.000]  Борис Тодорович Полик, 1964 год.
[48:25.000 --> 48:28.000]  Идея в следующем.
[48:28.000 --> 48:31.000]  Давайте мы возьмем произвольную функцию здесь,
[48:31.000 --> 48:34.000]  и будем подправлять наш шаг по антиградиенту
[48:34.000 --> 48:37.000]  вот такой поправочкой.
[48:37.000 --> 48:40.000]  Будем учитывать еще и направление,
[48:40.000 --> 48:43.000]  по которому мы пришли в текущую точку.
[48:43.000 --> 48:46.000]  Из х к минус 5.
[48:46.000 --> 48:49.000]  Тогда, если у нас был
[48:49.000 --> 48:52.000]  градиентный спуск вот такими вот осцилляциями,
[48:52.000 --> 48:55.000]  то метод тяжелого шарика эти осцилляции сглаживает.
[48:55.000 --> 48:58.000]  То есть он вместо того, чтобы брать
[48:58.000 --> 49:01.000]  вот это направление,
[49:01.000 --> 49:04.000]  берет их неотрицательную комбинацию,
[49:04.000 --> 49:07.000]  и получает вот такой вот более сглаженный.
[49:07.000 --> 49:10.000]  Это метод двухшаговый.
[49:10.000 --> 49:13.000]  Он использует и х к, и х к минус 1.
[49:13.000 --> 49:16.000]  Почему тяжелый шарик?
[49:16.000 --> 49:19.000]  Потому что связь этого метода
[49:19.000 --> 49:22.000]  с дифференциальными уравнениями в том,
[49:22.000 --> 49:25.000]  что если вы дискретизуете дифференциальное уравнение с трением,
[49:25.000 --> 49:28.000]  то у шарика появляется некоторая инерция у точки.
[49:28.000 --> 49:31.000]  И учет этой инерции как раз приводит к тому,
[49:31.000 --> 49:34.000]  что мы делаем.
[49:34.000 --> 49:37.000]  Но запоставив просто формулу,
[49:37.000 --> 49:40.000]  можно показать, что метод сопряженных градиентов
[49:40.000 --> 49:43.000]  это просто частный случай этого метода для квадратичной телевой функции,
[49:43.000 --> 49:46.000]  когда у бета есть специальное выражение,
[49:46.000 --> 49:49.000]  и вот эта штука это просто предыдущее направление.
[49:49.000 --> 49:52.000]  То есть тут все довольно близко и похоже.
[49:52.000 --> 49:55.000]  Поскольку мы хотим получить некоторые оптимальные оценки,
[49:55.000 --> 49:58.000]  то для этого метда есть вот такая
[49:58.000 --> 50:01.000]  теория исходимости.
[50:01.000 --> 50:04.000]  Тут некоторый вывод.
[50:04.000 --> 50:07.000]  Теория о том, что если функция сильно выпукла,
[50:07.000 --> 50:10.000]  тогда если взять шаги альфы вот такие,
[50:10.000 --> 50:13.000]  а бета вот такие,
[50:13.000 --> 50:16.000]  то мы получим оптимальную оценку скорости исходимости
[50:16.000 --> 50:19.000]  для такого класса функций.
[50:19.000 --> 50:22.000]  Проблема этой оценки в том,
[50:22.000 --> 50:25.000]  что, во-первых, она зависит от констанции,
[50:25.000 --> 50:28.000]  от таких зависимостей,
[50:28.000 --> 50:31.000]  о которых я говорил ранее,
[50:31.000 --> 50:34.000]  когда мы обсуждали,
[50:34.000 --> 50:37.000]  что нам дает и что не дает теория исходимости.
[50:37.000 --> 50:40.000]  Точно получаем, что это быстрее градиентного спуска,
[50:40.000 --> 50:43.000]  и в каком-то смысле является аналогом метода
[50:43.000 --> 50:46.000]  сопряженных градиентов для выпуклой квадратичной функции.
[50:46.000 --> 50:49.000]  Сильно выпуклой квадратичной функции.
[50:56.000 --> 50:59.000]  Вот так.
[50:59.000 --> 51:02.000]  Что хорошего?
[51:02.000 --> 51:05.000]  Хорошо, что есть оценка.
[51:05.000 --> 51:08.000]  Хорошо, что она быстрее, чем градиентный спуск.
[51:08.000 --> 51:11.000]  Что плохо?
[51:11.000 --> 51:14.000]  Плохо, что у параметров есть зависимость от L и mu.
[51:14.000 --> 51:17.000]  Теорияма не говорит о том, что будет,
[51:17.000 --> 51:20.000]  если альфа и бета выбирать не по этим формам.
[51:20.000 --> 51:23.000]  Но на практике бета обычно выбирает порядка единицы.
[51:23.000 --> 51:26.000]  Я имею в виду, что где-то 0,7-0,9.
[51:26.000 --> 51:29.000]  Это дает вполне себе хорошие результаты.
[51:29.000 --> 51:32.000]  Альфу можно выбирать как единица L, например,
[51:32.000 --> 51:35.000]  или как адаптивно тоже.
[51:35.000 --> 51:38.000]  Это вполне себе рабочая схема.
[51:38.000 --> 51:41.000]  Типичный пример работы для квадратичной задачи.
[51:41.000 --> 51:44.000]  Спряженный градиент сходится очень быстро.
[51:44.000 --> 51:47.000]  Тут есть некоторая не монотонность,
[51:47.000 --> 51:50.000]  но это, видимо, задача не очень...
[51:50.000 --> 51:53.000]  Получайная задача.
[51:53.000 --> 51:56.000]  Спектр не очень понятен, как распределенный.
[51:56.000 --> 51:59.000]  Возможно, с той итерацией как раз таки и произошло.
[51:59.000 --> 52:02.000]  Градиентный спуск с фиксированным шагом сходится очень медленно.
[52:02.000 --> 52:05.000]  Но тяжелый шарик с оптимальными параметрами,
[52:05.000 --> 52:08.000]  поскольку их можно получить для квадратичной задачи,
[52:08.000 --> 52:11.000]  он сходится вот так.
[52:11.000 --> 52:14.000]  Хуже, конечно, чем спряженные градиенты,
[52:14.000 --> 52:17.000]  потому что для них это все было придумано.
[52:17.000 --> 52:20.000]  Но это лучше, чем метод градиентного спуску.
[52:20.000 --> 52:23.000]  И понятно, что обобщаемость у него
[52:23.000 --> 52:26.000]  будет более высокая,
[52:26.000 --> 52:29.000]  чем у спряженных градиентов.
[52:29.000 --> 52:32.000]  Тут хотя бы есть какие-то теории.
[52:32.000 --> 52:35.000]  Есть какие-то вопросы про метод тяжелого шарика?
[52:38.000 --> 52:41.000]  Нет вопросов. Прекрасно.
[52:41.000 --> 52:44.000]  Тогда давайте пойдем дальше.
[52:44.000 --> 52:47.000]  Вот этот шарик, который сегодня хочется охватить,
[52:47.000 --> 52:50.000]  это, собственно, ускоренный градиентный метод
[52:50.000 --> 52:53.000]  Юрия Ивановича Нейстера в 1983 год.
[52:53.000 --> 52:56.000]  Один из основных и главных прорывов
[52:56.000 --> 52:59.000]  в конце XX века.
[52:59.000 --> 53:02.000]  Один из вариантов его постановки он вот такой вот.
[53:02.000 --> 53:05.000]  У нас есть теперь две последовательности,
[53:05.000 --> 53:08.000]  х и у.
[53:08.000 --> 53:11.000]  При этом х делает по-прежнему шаг
[53:11.000 --> 53:14.000]  градиентного спуска.
[53:14.000 --> 53:17.000]  Но хитрый заключается в том, что в отличие от...
[53:17.000 --> 53:20.000]  Тут я обычно об этом спрашиваю.
[53:20.000 --> 53:23.000]  Нет, тут не спрашиваю.
[53:23.000 --> 53:26.000]  Отличие от тяжелого шарика в том,
[53:26.000 --> 53:29.000]  что тут мы для того, чтобы посчитать
[53:29.000 --> 53:32.000]  Xк плюс первое, считаем градиент в Xкатом
[53:32.000 --> 53:35.000]  и шагаем по антиградиенту относительно точки Xк.
[53:35.000 --> 53:38.000]  А тут чтобы получить Xкат,
[53:38.000 --> 53:41.000]  мы берем некоторую точку Yкат и другую,
[53:41.000 --> 53:44.000]  не равную Xк.
[53:44.000 --> 53:47.000]  И более того, градиент считаем тоже не в этой точке.
[53:47.000 --> 53:50.000]  То есть здесь мы,
[53:50.000 --> 53:53.000]  как можно сказать, что здесь мы тоже берем
[53:53.000 --> 53:56.000]  Xк плюс бета, то есть тоже какая-то другая точка.
[53:56.000 --> 53:59.000]  Но градиент считается только в Xк.
[53:59.000 --> 54:02.000]  А здесь и точка другая относительно которой старт.
[54:02.000 --> 54:05.000]  И градиент в другой точке считается.
[54:05.000 --> 54:08.000]  И потом эта самая точка пересчитывается
[54:08.000 --> 54:11.000]  через линии некоторую комбинацию
[54:11.000 --> 54:14.000]  векторов Xк и Xк плюс 1 минус Xк.
[54:14.000 --> 54:17.000]  Вот этот коэффициент,
[54:17.000 --> 54:20.000]  он тут немного магия,
[54:20.000 --> 54:23.000]  но есть более сложные формулы,
[54:23.000 --> 54:26.000]  это просто самая простейшая, которая плюс минус работает.
[54:26.000 --> 54:29.000]  И тут важно, что тройка.
[54:29.000 --> 54:32.000]  Там есть работы, в которой доказывается,
[54:32.000 --> 54:35.000]  что здесь тройки нельзя.
[54:35.000 --> 54:38.000]  Это важно и связано с,
[54:38.000 --> 54:41.000]  как бы это удивительно ни казалось,
[54:41.000 --> 54:44.000]  дифференциальными уравнениями и устойчивостью их решений.
[54:44.000 --> 54:47.000]  Если будут кому-то интересны,
[54:47.000 --> 54:50.000]  я могу ссылки прислать в чат,
[54:50.000 --> 54:53.000]  где про это подробно расписывается.
[54:53.000 --> 54:56.000]  Он тоже не монотонный.
[54:56.000 --> 54:59.000]  Не монотонность проявляется, ее тут плохо видно.
[54:59.000 --> 55:02.000]  Вот здесь он немного акцилирует,
[55:02.000 --> 55:05.000]  и норма градиента может возрастать.
[55:05.000 --> 55:08.000]  То есть он накапливает некоторую историю,
[55:08.000 --> 55:11.000]  чтобы потом начать убывать.
[55:11.000 --> 55:14.000]  Визуализация этого меда такая вот.
[55:14.000 --> 55:17.000]  У нас есть Xк и Yк.
[55:17.000 --> 55:20.000]  Чтобы получить Xк плюс 1,
[55:20.000 --> 55:23.000]  мы переходим из точки Y к Xк плюс 1.
[55:23.000 --> 55:26.000]  Потом берем прямую,
[55:26.000 --> 55:29.000]  которая соединяет точки Xк и Xк плюс 1.
[55:29.000 --> 55:32.000]  Получаем точку Yк плюс 1.
[55:32.000 --> 55:35.000]  Из нее шагаем по градиенту,
[55:35.000 --> 55:38.000]  получаем точку Xк и так далее.
[55:38.000 --> 55:41.000]  Вот такая схема работы,
[55:41.000 --> 55:44.000]  которая приводит к замечательным результатам.
[55:44.000 --> 55:47.000]  Сходимость по функции становится равна 1 на как квадрат
[55:47.000 --> 55:50.000]  для обычной пуклы функции.
[55:50.000 --> 55:53.000]  А для сильной пуклы функции она действительно становится равна
[55:53.000 --> 55:56.000]  1 на корень Xк.
[55:56.000 --> 55:59.000]  То, как это было показано и требовалось
[55:59.000 --> 56:02.000]  нашими нижними оценками.
[56:02.000 --> 56:05.000]  То есть вот этот метод в теории
[56:05.000 --> 56:08.000]  оказывается очень выигрышным
[56:08.000 --> 56:11.000]  по сравнению с градиентным спуском.
[56:11.000 --> 56:14.000]  На практике есть некоторые проблемы.
[56:14.000 --> 56:17.000]  Вот они тут уже нарисованы.
[56:17.000 --> 56:20.000]  Это, насколько я понимаю, та же самая квадратичная задача.
[56:20.000 --> 56:23.000]  Здесь у метода Нистерова
[56:23.000 --> 56:26.000]  такие рифовые поведения.
[56:26.000 --> 56:29.000]  То есть он опять же не монотонный.
[56:29.000 --> 56:32.000]  Выбор параметров для него
[56:32.000 --> 56:35.000]  это немного отдельная история.
[56:35.000 --> 56:38.000]  Это не всегда просто и не всегда очевидно, какие нужно подбирать.
[56:38.000 --> 56:41.000]  Какие тут параметры?
[56:41.000 --> 56:44.000]  В этой схеме это шаг.
[56:44.000 --> 56:47.000]  Понятно, что если вы в качестве вот этого коэффициента возьмете
[56:47.000 --> 56:50.000]  то будет еще один параметр, который тоже надо настраивать.
[56:50.000 --> 56:53.000]  Это не всегда легко.
[56:53.000 --> 56:56.000]  Обычный шаг берут...
[56:56.000 --> 56:59.000]  Типично я бы сказал, что лучше брать шаг поменьше.
[56:59.000 --> 57:02.000]  Если взять шаг поменьше, то он будет сходить
[57:02.000 --> 57:05.000]  точно быстрее, чем градиентный спуск таким же шагом.
[57:05.000 --> 57:08.000]  За счет именно того, что история учитывается хитрым образом.
[57:08.000 --> 57:11.000]  И левочисление следующей точки
[57:11.000 --> 57:14.000]  используется не вспомогательно последовательно.
[57:14.000 --> 57:17.000]  Для начальных итераций видно, насколько здесь существенное ускорение.
[57:17.000 --> 57:20.000]  Даже по сравнению с методом
[57:20.000 --> 57:23.000]  тяжелого шага.
[57:23.000 --> 57:26.000]  Это метод, который
[57:26.000 --> 57:29.000]  сходится к не самой высокой точности, но очень быстро.
[57:29.000 --> 57:32.000]  Я бы так его описал вкратце.
[57:32.000 --> 57:35.000]  В целом, градиентные методы
[57:35.000 --> 57:38.000]  сделаны для того, чтобы сделать много тяжелых итераций
[57:38.000 --> 57:41.000]  с сайтики точности 10-12, 10-13.
[57:41.000 --> 57:44.000]  Поэтому можно начать стопориться,
[57:44.000 --> 57:47.000]  потому что у них просто порядка точности не хватает.
[57:47.000 --> 57:50.000]  Давайте некоторое общение проведем
[57:50.000 --> 57:53.000]  того, что мы знаем.
[57:53.000 --> 57:56.000]  Есть ли какие-то вопросы про метод ускоренного
[57:56.000 --> 57:59.000]  как он работает, и что для него надо
[57:59.000 --> 58:02.000]  использовать, и какие основные моменты здесь могут
[58:02.000 --> 58:05.000]  случиться?
[58:05.000 --> 58:08.000]  Вижу, что написали, что нет вопросов.
[58:08.000 --> 58:11.000]  Окей.
[58:11.000 --> 58:14.000]  Тогда небольшой обзор того, что мы только что изучили.
[58:14.000 --> 58:17.000]  Что мы знаем? Мы знаем, что сходиме с градиентом с куском
[58:17.000 --> 58:20.000]  можем улучшить. Мы знаем, что
[58:20.000 --> 58:23.000]  метод с упруженными градиентами надо использовать, если у вас
[58:23.000 --> 58:26.000]  целевая функция квадратично сильно выпукла.
[58:26.000 --> 58:29.000]  Что ускоренный метод является оптимальным для выпуклых,
[58:29.000 --> 58:32.000]  для всех тех двух классов функций, на которых мы
[58:32.000 --> 58:35.000]  все это дело рассматривали.
[58:35.000 --> 58:38.000]  Какие у нас остались вопросы?
[58:38.000 --> 58:41.000]  Которые мы в следующий раз, наверное, или в следующий,
[58:41.000 --> 58:44.000]  или в следующий раз будем пытаться решать.
[58:44.000 --> 58:47.000]  Что делать, если градиент точно не посчитать?
[58:47.000 --> 58:50.000]  Если функция настолько сложная, настолько многомерная,
[58:50.000 --> 58:53.000]  что посчитать градиент нельзя, но можно посчитать его отца.
[58:53.000 --> 58:56.000]  Это так называемые стахотические методы.
[58:56.000 --> 58:59.000]  Мы про них будем говорить, наверное, в следующий раз.
[58:59.000 --> 59:02.000]  Что делать?
[59:02.000 --> 59:05.000]  Везде зависит от константа, который непонятно как подбирать.
[59:05.000 --> 59:08.000]  Это тоже обсудим
[59:08.000 --> 59:11.000]  в контексте того, что изменится
[59:11.000 --> 59:14.000]  в наших результатах про ускорение
[59:14.000 --> 59:17.000]  и его наличие для модификации градиентного спуска
[59:17.000 --> 59:20.000]  в случае, когда у нас появляется стахастика.
[59:20.000 --> 59:23.000]  Если у нас есть стахастика,
[59:23.000 --> 59:26.000]  то насколько у нас скорость сходимости начнет портиться?
[59:26.000 --> 59:29.000]  Мы же не можем быть уверенными в том, что
[59:29.000 --> 59:32.000]  мы делаем шаг по неточному градиенту
[59:32.000 --> 59:35.000]  и при этом у нас скорость сходимости
[59:35.000 --> 59:38.000]  не ломается ни разу.
[59:38.000 --> 59:41.000]  Это было бы, наверное, слишком странно, чтобы быть правдой.
[59:41.000 --> 59:44.000]  Поэтому в следующий раз мы посмотрим
[59:44.000 --> 59:47.000]  на то, как это все будет портиться
[59:47.000 --> 59:50.000]  и насколько сильно.
[59:50.000 --> 59:53.000]  Это анонс
[59:53.000 --> 59:56.000]  на следующий раз.
[59:56.000 --> 59:59.000]  Я буду там более подробно рассказать про всю эту стахастику.
[59:59.000 --> 01:00:02.000]  Это важный кусочек,
[01:00:02.000 --> 01:00:05.000]  который, наверное,
[01:00:05.000 --> 01:00:08.000]  наиболее активно будет использоваться, мне кажется.
[01:00:08.000 --> 01:00:11.000]  Резюме сегодняшней лекции.
[01:00:11.000 --> 01:00:14.000]  Мецпежонных градиентов для квадратичной
[01:00:14.000 --> 01:00:17.000]  целевой функции сильной выпуклы. Его сходимость.
[01:00:17.000 --> 01:00:20.000]  Какова обобщается это все на неквадратичную задачу
[01:00:20.000 --> 01:00:23.000]  и какие там возможные нюансы?
[01:00:23.000 --> 01:00:26.000]  Что такое метод тяжелого шарика?
[01:00:26.000 --> 01:00:29.000]  Какие там параметры? Как он сходится?
[01:00:29.000 --> 01:00:32.000]  И ускоренный градиентный метод как некоторое завершение
[01:00:32.000 --> 01:00:35.000]  обсуждения того, какие методы первого порядка
[01:00:35.000 --> 01:00:38.000]  из класса методов, которые генерируют решение
[01:00:38.000 --> 01:00:41.000]  как линейную комбинацию градиентов в начальной точке.
[01:00:41.000 --> 01:00:44.000]  Куда это все дело пришло
[01:00:44.000 --> 01:00:47.000]  и какие дальнейшие расширения в плане появления
[01:00:47.000 --> 01:00:50.000]  стахастики, появление нелинейных
[01:00:50.000 --> 01:00:53.000]  функций для генерации следующей точки.
[01:00:53.000 --> 01:00:56.000]  Сейчас мы жили в мире,
[01:00:56.000 --> 01:00:59.000]  где у нас... Так, надо нарисовать.
[01:01:05.000 --> 01:01:08.000]  Где у нас
[01:01:08.000 --> 01:01:11.000]  xCAD плюс 1 это x0 плюс
[01:01:11.000 --> 01:01:14.000]  линейная комбинация градиентов.
[01:01:14.000 --> 01:01:17.000]  Вот, а что будет,
[01:01:17.000 --> 01:01:20.000]  если, например, здесь будет что-то нелинейное,
[01:01:20.000 --> 01:01:23.000]  типа x0 плюс какая-то нелинейная функция чего-то там.
[01:01:23.000 --> 01:01:26.000]  Вот, для этого класса все понятно,
[01:01:26.000 --> 01:01:29.000]  для этого класса непонятно, и на практике оказывается,
[01:01:29.000 --> 01:01:32.000]  что к этому классу относятся квазинью туннельские методы,
[01:01:32.000 --> 01:01:35.000]  квазинью туннельские методы,
[01:01:35.000 --> 01:01:38.000]  которые сходятся
[01:01:38.000 --> 01:01:41.000]  очень хорошо.
[01:01:41.000 --> 01:01:44.000]  Для задач, где вы можете
[01:01:44.000 --> 01:01:47.000]  посчитать полный градиент точный,
[01:01:47.000 --> 01:01:50.000]  ну, то есть это там десятки-сотни тысяч переменных,
[01:01:50.000 --> 01:01:53.000]  эта штука работает очень хорошо.
[01:01:53.000 --> 01:01:56.000]  И квазинью туннельские методы
[01:01:56.000 --> 01:01:59.000]  и сопряженные градиенты для нелинейных задач,
[01:01:59.000 --> 01:02:02.000]  не квадратичные, то есть простите,
[01:02:02.000 --> 01:02:05.000]  это основные методы, которыми надо их решать
[01:02:05.000 --> 01:02:08.000]  в случае, когда у вас большая размерность.
[01:02:08.000 --> 01:02:11.000]  Потому что они хранят только векторы,
[01:02:11.000 --> 01:02:14.000]  то есть никаких матриц ничего не надо.
[01:02:14.000 --> 01:02:17.000]  И единственный вопрос в том,
[01:02:17.000 --> 01:02:20.000]  как правильно эффективно посчитать градиент, как его хранить,
[01:02:20.000 --> 01:02:23.000]  и в каком виде с ним обращаться.
[01:02:23.000 --> 01:02:26.000]  Про это мы будем говорить про квазинью туннельские методы
[01:02:26.000 --> 01:02:29.000]  и их связь с сопряженными градиентами, которые были анонсированы,
[01:02:29.000 --> 01:02:32.000]  будет, наверное, через раз.
[01:02:32.000 --> 01:02:35.000]  И в целом, насколько я понимаю,
[01:02:35.000 --> 01:02:38.000]  сейчас все это дело, почему-нибудь, движется к концу.
[01:02:38.000 --> 01:02:41.000]  Анонс на ближайшее время такой.
[01:02:41.000 --> 01:02:44.000]  В следующий раз будет стокастика,
[01:02:44.000 --> 01:02:47.000]  через раз будут квазинью туннельские методы,
[01:02:47.000 --> 01:02:50.000]  6 декабря будет про максимальные методы
[01:02:50.000 --> 01:02:53.000]  и про решение сдачи на простых множеству.
[01:02:53.000 --> 01:02:56.000]  Будет ли что-то 13-го, не уверен, но посмотрим.
[01:02:56.000 --> 01:02:59.000]  Посмотрим, во-первых, по тому,
[01:02:59.000 --> 01:03:02.000]  сколько людей будет приходить отключаться.
[01:03:02.000 --> 01:03:05.000]  Дальше я расскажу о темах, которые я уже анонсировал.
[01:03:05.000 --> 01:03:08.000]  Потому что они являются основополагающими для всего остального.
[01:03:08.000 --> 01:03:11.000]  Поэтому, я думаю, если у вас будет понимание что-то про них,
[01:03:11.000 --> 01:03:14.000]  то дальше вам будет не так сложно
[01:03:14.000 --> 01:03:17.000]  разобрать, что происходит.
[01:03:17.000 --> 01:03:20.000]  Лейна программирования или еще чем-то,
[01:03:20.000 --> 01:03:23.000]  что немножко не успевается.
[01:03:23.000 --> 01:03:26.000]  12.08, две минуты на вопросы.
[01:03:26.000 --> 01:03:29.000]  Все ли понятно, что будет происходить, какой у нас план,
[01:03:29.000 --> 01:03:32.000]  и как все будет завершаться.
[01:03:35.000 --> 01:03:38.000]  Очень хорошо, что все ясно.
[01:03:38.000 --> 01:03:41.000]  Тогда давайте на этом сегодня закончим. Спасибо за внимание.
[01:03:41.000 --> 01:03:44.000]  До следующего раза видео я выложу так же, как обычно,
[01:03:44.000 --> 01:03:47.000]  в папочку на Google Диске.
[01:03:47.000 --> 01:03:50.000]  Надеюсь, что, возможно, те, кто не смогли подключиться,
[01:03:50.000 --> 01:03:53.000]  что-то там посмотрят и для себя полезно узнают.
[01:03:53.000 --> 01:03:56.000]  Все, спасибо и до следующей недели.

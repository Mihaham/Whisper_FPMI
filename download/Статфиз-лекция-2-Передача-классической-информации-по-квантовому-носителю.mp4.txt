[00:00.000 --> 00:09.360]  Может ли фиксированное слово преобразоваться в любое другое слово по действиям такого канала?
[00:09.360 --> 00:15.120]  Надо посмотреть, все ли вероятности не нулевые. Все не нулевые. Значит вообще любое входное слово
[00:15.120 --> 00:27.120]  может в любое выходное слово преобразоваться. Поэтому да. И тогда встает вопрос, а раз вы можете
[00:27.120 --> 00:32.680]  на выходе получить вообще любое слово, можно ли при этом передавать информацию надежно через
[00:32.680 --> 00:44.520]  такой канал? Ответ какой? Можно. Восемтотики. Привет, стремящимся к бесконечности. Ошибка будет
[00:44.520 --> 01:00.800]  стремиться к нулю. Не очень понятно. Тут так же, как я уже сто раз говорил, так же, как на английском
[01:00.800 --> 01:10.920]  языке. Нужно понять, что от вас хотят. Что от вас хотят? От вас хотят узнать такое. Поняли ли вы вот
[01:10.920 --> 01:19.920]  эту вот картинку? Пусть это множество слов на выходе. Вы какое бы фиксированное слово на входе
[01:19.920 --> 01:26.840]  не взяли, оно может в любое перейти. Вообще в любое. Вот какое не тыкнем, вот в это. Может и в него
[01:26.840 --> 01:37.120]  перейти. Вероятности перехода в эти все слова, они разные. И где-то будет скученность вот этих
[01:37.120 --> 01:43.520]  вот слов, в которые вы можете перейти, а где-то будет разреженность. Когда Ян стремится к
[01:43.520 --> 01:50.200]  бесконечности, у вас вероятность выйти за пределы этой какой-то границы будет стремиться к нулю.
[01:50.200 --> 02:01.440]  Понимаете, она будет не ноль, но будет стремиться к нулю. Это понятно?
[02:01.440 --> 02:20.960]  Если у вас на входе независимые случайно распределенные вот эти буквы, то да, всегда будет так.
[02:20.960 --> 02:36.600]  Вот в чем нетривиальность теории Шеннона. У вас шум такой, видите, шум такой, что какой бы вы
[02:36.600 --> 02:44.360]  слово на входе не взяли, на выходе все равно будет, любое слово может появиться. Но вероятность этих
[02:44.360 --> 02:52.440]  слов разная. Когда длину слова увеличиваем, здесь будут не условно-типичные слова, а вот тут,
[02:52.440 --> 02:58.840]  вот в этой области условно-типичные слова. Поэтому нам нужно разделить вот это все выходное
[02:58.840 --> 03:06.480]  множество слов на некоторые дольки нарезать. Это области принятия решений. Слайды смогли посмотреть,
[03:06.480 --> 03:15.280]  которые высылал. Там вот математические определения написаны. Все смогли или надо
[03:15.280 --> 03:27.520]  повторить? Я тогда напишу. Могу переписать. Мне несложно переписать. Ну давайте перепишу,
[03:27.520 --> 03:41.240]  тогда я открою. Там последние странички в общем про это. Кстати хорошая новость, на кафедру
[03:41.240 --> 03:49.600]  привезли эти задавальнички. Так что тот, кто хочет на бумаге их смотреть, а не в электронном
[03:49.600 --> 04:02.080]  виде, их можно взять в 508 главного корпуса. Открываем. Смотрите какие там определения. Есть
[04:02.080 --> 04:17.400]  понятие кода. Что такое код? Код использует сколько-то слов. Давайте я напишу так. Код состоит
[04:17.400 --> 04:36.600]  из N большой слов. Каждая длина N маленькая. N маленькая. Дальше вы что делаете? Вы вот это множество
[04:36.600 --> 04:59.200]  выходных слов. Множество слов на выходе. Разбиваете на области принятия решений. Разбиваем. Эти области
[04:59.200 --> 05:09.280]  принятия решения буквы V обозначены в книжке Холлива. Вот я также и напишу V, N. Там написано
[05:09.280 --> 05:18.680]  подмножество Y в степени N. Что это значит? Что вы берете слова на выходе, Y это алфавит на выходе,
[05:18.680 --> 05:26.760]  в степени N значит что длины N. Эти подмножества интерпретируются как области принятия решения.
[05:27.240 --> 05:41.960]  Вот эти вот V это области принятия решения. То есть подмножество выходных слов. Что за решение?
[05:41.960 --> 05:51.440]  Решение о том какое слово было на входе послано. Так, если вот там так написано,
[05:51.440 --> 06:01.240]  если Y маленькая N принадлежит V житому, где ж пробегает значение от 1 до N большом,
[06:01.240 --> 06:13.880]  то принимается решение, чтобы было послано слово W жита. Принимаем решение.
[06:21.440 --> 06:40.160]  Так, если получено слово там из V0, то никакого определенного решения не принимается. Это,
[06:40.160 --> 06:46.800]  грубо говоря, вы оставляете зазоры в этом разделении пирога. V0 это все,
[06:46.800 --> 06:54.840]  что не вошло в эти кусочки. Это все будет V0. Не буду про это писать. Это все равно,
[06:54.840 --> 07:06.360]  что вы ошибку допускаете. Эта ошибка связана с чем? С тем, что у вас всегда есть маленькая
[07:06.360 --> 07:15.200]  вероятность попасть в какое-то даже нетипичное слово на выходе. Поэтому, чтобы все было строго,
[07:15.200 --> 07:24.440]  вот это еще V0 пишут. Я не буду его писать. Максимальная вероятность ошибки P, E есть
[07:24.440 --> 07:35.680]  максимум среди этих слов, которые мы выбрали. От чего? От единицы минус вероятность успешного.
[07:35.680 --> 07:51.680]  Так, максимум. Давайте квадратные скобки здесь. Что здесь записано? Здесь стоит вероятность того,
[07:51.680 --> 08:00.600]  что вы получите слово Yn из области принятия решения W жита при условии, что на входе было
[08:00.600 --> 08:11.360]  слово W. Единица минус эта вероятность, это значит, что вы попадете куда-то в другую часть. Максимум по
[08:11.360 --> 08:16.800]  всем словам, которые есть. Это будет максимальная вероятность ошибки. Дальше в общей теории
[08:16.800 --> 08:23.240]  показывается, что максимальная вероятность ошибки ведет себя в асимптотике точно так же,
[08:23.240 --> 08:29.640]  как и средняя вероятность ошибки. Между ними есть линейный коэффициент. Поэтому в общей теории
[08:29.960 --> 08:35.160]  если книжку Холлива посмотрите, там будет доказываться для средней вероятности ошибки,
[08:35.160 --> 08:47.400]  но максимальная связана со средним некоторым выражением. Поэтому можно в строгом определении
[08:47.400 --> 08:52.560]  написать и про максимальную вероятность ошибки, что в принципе и сделано. Дальше вы что можете
[08:52.560 --> 08:59.920]  делать? Вы можете варьировать выборы вот этих областей, сами нарезать пирог как хотите и выбирать
[08:59.920 --> 09:05.200]  вот эти слова дубль W житы тоже можете как хотите. И теперь соответственно вам нужно
[09:05.200 --> 09:14.000]  минимизировать эту вероятность ошибки по тому, как нарезать эти области. Давайте я здесь напишу,
[09:14.000 --> 09:23.680]  минимизируем по выбору n слов и почему еще по разбиению на область принятия решения.
[09:23.680 --> 09:38.920]  Вот если мы минимизируем эту вероятность ошибки, p, e, e от английского r, то мы получим с вами величину,
[09:38.920 --> 09:51.720]  которая будет зависеть уже теперь только от n маленькая и n большое, от длины слов. В том тексте,
[09:51.720 --> 09:57.920]  который я вам высылал, здесь вот p большое, давайте я напишу тоже здесь, а здесь уже p маленькое.
[09:57.920 --> 10:11.800]  Ну вот и теперь вы хотите все измерять в битах, поэтому вот это n теперь записываете в другом
[10:11.800 --> 10:19.440]  виде, просто эквивалентном, 2 в степени nr. Если бы r равнялось единице, то это было бы количество
[10:19.440 --> 10:33.800]  битовых строк длины n маленькой, а в степени r показывает вам, что может быть там больше,
[10:33.800 --> 10:41.440]  если алфавит больше, чем из двух букв стоит или меньше, если у вас есть ошибки. Просто другая
[10:41.440 --> 10:51.080]  запись n равняется вот такому числу. Переопределили. И теперь вот эта величина r называется
[10:51.080 --> 11:16.120]  достижимой скоростью передачи данных, если вот это вероятность p, e, n, а вместо n большого
[11:16.120 --> 11:27.280]  пишем 2 в степени nr. Видите, теперь у нас одна всего буква осталась, n маленькая. Если эта величина
[11:27.280 --> 11:43.480]  в пределе при n стремящемся к бесконечности дает вам ноль, то есть вот это и есть условие
[11:43.480 --> 11:53.560]  асимпатически исчезающей ошибки. Понятно, что если вы r сделаете очень большим, то вы этого условия
[11:53.560 --> 12:05.160]  не сможете достичь. Например, ошибок никаких нет вообще, используем только нолики и единички.
[12:05.160 --> 12:12.520]  Вот ошибок нет, используем нолики и единички. Пусть длина слова n. Ну вот такое вот сообщение.
[12:12.520 --> 12:23.320]  Раз ошибок нет, я его могу надежно принять в неизменном виде. Сколько всего слов я смогу
[12:23.320 --> 12:32.360]  различных передать? Ну понятно, что начиная от всех ноликов и заканчивая всеми единичками,
[12:32.360 --> 12:41.600]  тут всего количество слов есть 2 в степени n, правильно? Если же я захочу передавать больше слов,
[12:41.600 --> 12:55.840]  то есть если r будет больше, чем единица, то тогда вот эта вероятность pen2n в степени r
[12:55.840 --> 13:08.280]  уже не будет равна нулю при энстимящемся бесконечности. Понятно? Ну например, я веду
[13:08.280 --> 13:16.960]  еще вместо нолика и единички еще символ там тройку какой-нибудь на выходе. Ну вот это понятно.
[13:16.960 --> 13:26.720]  R показывает вам, грубо говоря, коэффициент в показателе экспоненты по отношению к идеальному
[13:26.720 --> 13:46.560]  случаю. Это определение для r. Определение для вот этой вот достижимой скорости. Вот если r
[13:46.560 --> 14:01.960]  больше единицы, то здесь и тогда это r недостижимый. Непонятно? Ну по-моему,
[14:01.960 --> 14:07.520]  это просто формализация того, что мы в прошлый раз сказали. Теперь, значит, у вас, смотрите,
[14:07.520 --> 14:19.120]  делать хуже, это особо труда не занимает. Поэтому сделать r маленькой, это не проблема. Вива стремить
[14:19.120 --> 14:26.800]  ее к нулю тоже не проблема. Проблема в том, чтобы найти вот это максимальное значение у r. Вот точная
[14:26.800 --> 14:32.320]  верхняя грань для r. Почему точная грань? Потому что математики, они не знают, что будет, если r точно
[14:32.320 --> 14:38.320]  равно. Поскольку вы симптотики, то вот это вот точная верхняя грань для достижимых.
[14:38.320 --> 14:49.880]  R это называется пропускной способностью вашего классического канала связи. Так,
[14:49.880 --> 15:06.280]  записать supremum по r, где r достижима, есть c. Ну или давайте вот, ну да, по достижимым. Вот,
[15:06.280 --> 15:24.880]  это пропускная способность классического канала связи. Есть и более строгие утверждения,
[15:24.880 --> 15:34.520]  которые показывают, что происходит с ошибкой, с вот этой вот p,e. Если скорость будет больше,
[15:34.520 --> 15:41.480]  чем r. Значит мы знаем, что p,e в симптотике предел приянсты мячемся бесконечности. Давайте я
[15:41.480 --> 15:47.600]  напишу здесь, предел приянсты мячемся бесконечности. Это будет ноль, то есть вот здесь вероятность
[15:47.600 --> 15:54.920]  ошибки ноль. Если будет больше, то значит смотрите, есть оценка, которая показывает,
[15:54.920 --> 16:04.320]  что она будет расти. А есть еще такое понятие в математике, как strong converse с r.
[16:04.800 --> 16:15.920]  Говорит, что вероятность ошибки будет стремиться к единице, если у вас скорость больше достижим.
[16:15.920 --> 16:23.480]  Понятно? И обычно реализуется как раз таки вот этот случай. Но мы про саму вероятность ошибки не
[16:23.480 --> 16:33.960]  говорим. Мы говорим с вами про то, что она должна равняться нулю в пределе. Вот такое вот свойство.
[16:33.960 --> 16:48.320]  Интересное. В прошлый раз я кратко уже показал взаимную информацию. Сейчас просто напомню.
[16:48.320 --> 17:02.640]  И с вами вот эту теорему Шинона сформулируем. Значит у нас была такая ситуация, что был
[17:02.640 --> 17:08.600]  входной алфавит. Потом мы через какой-то классический канал связи передавали информацию,
[17:08.600 --> 17:16.600]  получали элементы выходного алфавита. И у нас что было задано? У нас было распределение букв на входе.
[17:16.600 --> 17:31.920]  У нас была стахастическая матрица, которая описывала вот этот классический канал. И мы
[17:31.920 --> 17:41.400]  с вами могли написать, например, вероятность совместной вот этой случайной величины,
[17:41.400 --> 17:50.720]  где x это буквы на входе, y это буквы на выходе. Это было просто px умножить на py при условии x.
[17:50.720 --> 17:57.680]  В прошлый раз, по-моему, греческие какие-то буквы использовали, но сейчас так напишу.
[17:57.680 --> 18:09.000]  Можно еще вероятность py найти, как сумма по x. Для каждого из этих распределений вы
[18:09.000 --> 18:18.000]  можете составить энтропию. Здесь будет hx, здесь у вас будет энтропия совместного распределения hy,
[18:18.000 --> 18:30.480]  здесь будет энтропия y. Эти энтропии задают количество типичных слов. В входном алфавите
[18:30.480 --> 18:38.400]  есть типичные слова, в выходном алфавите есть тоже, в множестве выходных слов есть типичные
[18:38.400 --> 18:56.640]  слова. Вот это множество типичных слов на выходе. Сколько таких слов на выходе? Множество типичных
[18:56.640 --> 19:06.240]  слов на выходе. Это 2 в степени n h y. Вот сколько таких слов. Теперь смотрите, что мы делаем. Мы
[19:06.280 --> 19:16.560]  выбираем n, слов случайным образом из множества слов на входе. Это так называемое случайное
[19:16.560 --> 19:24.880]  кодирование, когда мы случайным образом выбираем эти n. Они преобразуются в условно-типичные слова.
[19:24.880 --> 19:34.920]  Размерность вот этих условно-типичных слов. Сколько таких условно-типичных слов? Это 2 в
[19:34.920 --> 19:45.800]  степени n h y при условии x. Условная энтропия. Писали ее в прошлый раз. Теперь если мы с вами
[19:45.800 --> 20:00.720]  случайным образом выберем эти n, то они, поскольку мы их выбрали случайно, будут иметь вероятность
[20:00.720 --> 20:08.840]  пересечения для выходов вот этих условно-типичных слов, стремящихся к нулю. То есть вот эти области не
[20:08.840 --> 20:16.480]  будут пересекаться. Это основа доказательства теоремы Шеннона о том, что случайное кодирование
[20:16.480 --> 20:27.520]  работает. На практике его не используют, потому что его сложно реализовать. Случайное кодирование.
[20:27.520 --> 20:37.720]  И мы с вами понимаем, сколько n слов можно пересылать надежно. То есть так, чтобы не
[20:37.720 --> 20:53.400]  пересекались вот эти вот области принятия решений. Это 2 в степени n h y разделить на 2 в
[20:53.400 --> 21:03.720]  степени n h y при условии x. То есть 2 в степени n. Взаимная информация. И эта взаимная информация
[21:03.720 --> 21:20.760]  допускает такую запись h y минус h y при условии x. Можно написать и поменяв y и x и h x минус h x
[21:20.760 --> 21:28.560]  при условии y. То есть формула симметрична по перестановке x и y и записывается в таком
[21:28.560 --> 21:38.680]  виде h x плюс h y минус энтропия совместного распределения. Вот такая формула. Взаимная информация.
[21:38.680 --> 21:57.200]  Теперь давайте попробуем с нашим определением сопречь. Откуда мы эту оценку получили? Из того,
[21:57.200 --> 22:05.320]  что всю размерность типичных слов на выходе разделили на размер условно-типичных слов.
[22:05.320 --> 22:11.480]  Давайте я здесь напишу тогда меньше либо равно. Различимые слова.
[22:11.480 --> 22:24.440]  Вот оказывается, что эта величина, которую мы здесь с вами написали, видите, она тоже входит в
[22:24.440 --> 22:34.120]  показатель экспонент, так же как r входила скорость передачи данных. Что она как раз таки и есть вот тот
[22:34.120 --> 22:43.040]  самый supremum, то есть верхняя грань точная для достижимых скоростей r. И это и есть иаремашина,
[22:43.040 --> 22:53.800]  которая использует случайное кодирование. Формулировка ее очень простая. Пропускная
[22:53.800 --> 23:09.760]  способность классического канала связи есть просто supremum этой вот взаимной информации.
[23:09.760 --> 23:17.080]  Почему? По распределениям на входе. Потому что единственное, что вы можете делать в этой
[23:17.080 --> 23:34.440]  ситуации, это вы можете изменять веса букв на входе. Вот что вы можете делать. Потому что все
[23:34.440 --> 23:40.680]  остальное зафиксировано. Размер алфавита зафиксирован, поскольку классический канал связи
[23:40.680 --> 23:46.960]  задается стахастической матрицей. Вот сколько элементов x принимает, вот столько и такова
[23:46.960 --> 23:52.760]  размерность алфавита на входе. Y размерность алфавита на выходе тоже зафиксирована. Поскольку
[23:52.760 --> 23:59.440]  вот эта матрица зафиксирована, то единственное, что вы можете варьировать здесь, это распределение
[23:59.440 --> 24:07.600]  вероятностей, которые для этих букв используется. То есть вы можете, раз буква A, например,
[24:07.600 --> 24:15.520]  встречается, ну, например, вот смотрите какая ситуация. Пусть буквы по-разному проходят через
[24:15.520 --> 24:21.760]  канал. Ну, например, буква A теряется чаще, чем буква B. Значит, вы можете сдвинуть веса
[24:21.760 --> 24:29.280]  в вот этих вероятностях и букве B больше вес приписать. Вот что вы можете сделать.
[24:29.280 --> 24:52.560]  Да, но слова составлены из случайных независимо распределенных букв. Ну, те буквы,
[24:52.560 --> 24:59.440]  вот представьте, вот канал устроен таким образом. Ну, или вот пример такой. Вот Хокинг писал
[24:59.440 --> 25:08.200]  книжки. Может, слышали когда-нибудь про его книжки, про происхождение времени, вселенная,
[25:08.200 --> 25:17.720]  в ореховую скорлупку и еще что-то. Он такую закономерность нашел, что чем больше я туда
[25:17.720 --> 25:26.000]  включу формул, тем меньше будет продаваемость этих книжек. Поэтому, чтобы донести информацию
[25:26.000 --> 25:32.200]  до читателя, он что делал? Он сместил акцент вот в этом распределении вероятностей на что? На
[25:32.200 --> 25:39.360]  картинки и на текст, правильно? А формулы там тоже есть, но их очень мало. Им припишем маленькую
[25:39.360 --> 25:48.640]  вероятность. И тогда информация, которую мы донесем до читателя, будет больше. Ведь ту же
[25:48.640 --> 25:53.800]  самую информацию я могу донести в виде формул, правильно? Ну, тогда просто пропускная способность.
[25:53.800 --> 26:01.480]  А вот, кстати, пропускная способность, она показывает вот максимум из того,
[26:01.480 --> 26:08.440]  что можно выжить. Видите? Вы должны взять максимум по всевозможным распределениям вероятностей
[26:08.440 --> 26:19.400]  на входе. Вот что вы должны делать. Это понятно? Вот теперь, как выглядел бы тест вот этот вот?
[26:19.400 --> 26:26.760]  В следующий раз, не знаю, но, наверное, не будет такой задачки. Но как бы он выглядел? Теперь вот
[26:26.760 --> 26:34.560]  вам задана стахастическая матрица для канала. Вы ее записали, я ее тоже записал. А теперь нужно
[26:34.560 --> 26:39.120]  найти распределение букв на входе. То есть, с какой вероятностью нужно использовать букву А,
[26:39.120 --> 26:45.120]  с какой вероятностью нужно использовать букву В, чтобы вот эта взаимная информация приняла
[26:45.120 --> 26:53.000]  максимальное значение. И это и будет классическая пропускная способность этого канала связи. Понятно?
[26:53.000 --> 27:05.760]  Все, делаем перерыв. Сейчас у нас с вами новая тема. Это передача классической информации,
[27:05.760 --> 27:19.360]  закодированной в квантовые носители. Передача классической информации. То есть, текст какой-то,
[27:19.360 --> 27:30.400]  буквы и так далее. Передача классической информации при кодировании в квантовые состояния.
[27:30.400 --> 27:47.160]  В чем стоит задача? Представьте, что у вас теперь буква Х кодируется в некоторое квантовое
[27:47.160 --> 27:56.840]  состояние, описываемое матрицей плотности РОХ. А потом вы декодируете и получаете какую-то
[27:56.840 --> 28:05.680]  букву выходного алфавита, скажем Y. То есть, начало здесь классическое, конец классический,
[28:05.680 --> 28:14.480]  а промежуточное звено квантовое. То есть, например, мне нужно переслать бит 0, я
[28:14.480 --> 28:25.680]  приготовлю состояние горизонтальной поляризации, отправляю его через уже квантовый канал связи.
[28:25.680 --> 28:40.360]  Например, оптоволоконная линия для этих фотонов. Потом измеряю его. Какое измерение можно сделать?
[28:40.360 --> 28:46.120]  Например, с помощью поляризационного делителя пучка. Помните то самое, что было на первой лекции
[28:46.120 --> 28:59.360]  в прошлом семестре. Этот клик детектора верхнего или нижнего я интерпретирую как 0 или единичку.
[28:59.360 --> 29:08.440]  Значит, что у меня получается? Я могу, например, а единичку могу в вертикально поляризованную
[29:08.440 --> 29:15.800]  состоянию кодера. Получается, что я использую квантовые носители информации, то есть вот эти
[29:15.800 --> 29:23.280]  фотоны горизонтальной и вертикальной поляризации, пропускаю их через квантовый канал связи. А квантовый
[29:23.280 --> 29:30.480]  канал связи у нас задается отображением фи, которое вы все знаете. Вполне положительно,
[29:30.480 --> 29:38.440]  сохраняющийся след. И затем извлекаете классическую информацию снова путем измерения.
[29:38.440 --> 29:46.000]  Значит, измерение квантовое позволяет вам вернуться в классический мир. А вот это кодирование,
[29:46.000 --> 29:51.840]  как оно делается? Ну, вы приготовливаете эти состояния с горизонтальной и вертикальной
[29:51.840 --> 29:59.880]  поляризации в зависимости от того, что на входе у вас есть. То есть понятно, начало классическое,
[29:59.880 --> 30:05.760]  потом погружаемся в квантовую часть, а с помощью квантового измерения возвращаемся снова в классику.
[30:05.760 --> 30:16.600]  Получается, что встает задача, какие здесь вы можете получить скорости передачи данных и какова
[30:16.600 --> 30:25.880]  пропускная способность. Вот в таком сценарии. Понятно задача общая. Когда мы эту задачу формулируем,
[30:25.880 --> 30:35.000]  у нас должен возникнуть некоторый диссонанс в голове. Сейчас объясню почему. Потому что здесь,
[30:35.000 --> 30:42.960]  вот когда у вас какие-то алфавиты, булевые переменные или, например, алфавит из 33 букв,
[30:42.960 --> 30:49.040]  у вас здесь все дискретно. Как только вы погружаетесь в квантовый мир, у вас множество
[30:49.040 --> 30:58.400]  состоений для одного кубита, это уже континуум. Если вспомните шар Блоха, то что получается?
[30:58.400 --> 31:09.560]  Получается, что тут-то уже у вас в континуум состоянии для шара Блоха задается непрерывным
[31:09.560 --> 31:18.040]  вектором. Можно и в эту точку, и в эту, а можно и в бесконечно близкую к ней закодировать. РОХ
[31:18.040 --> 31:27.600]  то может быть. Получается, что множество вот этих вот всяких РОХ, у вас его мощность,
[31:27.600 --> 31:38.760]  бесконечность по сравнению с мощностью входного алфавита. Получается, что в один кубит
[31:38.760 --> 31:46.680]  можно записать сколько угодно информации. Могу войну и мир сюда записать. Это будет вот эта
[31:46.680 --> 31:58.960]  точечка. Могу конспекты всех наших лекций записать. Будет вот эта точка. Не похоже на войну и мир.
[31:58.960 --> 32:12.040]  А могу другое произведение Толстого в воскресенье написать. Паспортные данные все ваши.
[32:12.040 --> 32:21.200]  Это понятно, что здесь в одном кубите может содержаться бесконечное количество информации.
[32:21.200 --> 32:35.920]  Виктория задала правильный вопрос. Записать в кубит мы можем сколько угодно информации.
[32:35.920 --> 32:44.760]  То есть сделать соответствие такое. Эта книжка, эта точка, эти данные. А можем ли мы извлечь
[32:44.760 --> 32:54.320]  информацию из этого кубита? То есть как нам узнать, что это именно вот эта точка? Этот парадокс
[32:54.320 --> 33:00.920]  разрешается таким образом, что записать в кубит вы можете всё. А сколько информации вы можете
[33:00.920 --> 33:09.680]  извлечь из кубита при проведении измерений? Давайте рассуждать так. Вот если у меня измерение с
[33:09.680 --> 33:16.600]  двумя исходами. То есть могу измерить и получить там либо ноль, либо единицу. Получается, что за один
[33:16.600 --> 33:26.320]  акт измерения я могу извлечь только один бит информации. Но либо ноль, либо единичку получить.
[33:26.320 --> 33:38.680]  И поэтому кажется, что вот парадокс разрешился. Но у меня контр-аргумент к вам. Измерение в общем
[33:38.680 --> 33:46.760]  случае описывается положительно операторно-значной мерой. И количество исходов измерения ничем не
[33:46.760 --> 33:59.600]  ограничено. Хотите 10 исходов? Сделаю вам 10 исходов измерения. Сделаю прибор, у которого будет
[33:59.600 --> 34:22.240]  10 лампочек и одна из них будет загораться. Пивовия. Общее описание измерения. Что это такое? В
[34:22.240 --> 34:31.320]  самом простом случае. У вас есть отображение из множества исходов. Мы буквой у обозначали. Давайте
[34:31.320 --> 34:46.680]  у в еу. Что такое еу? Еу это эрмитовые неотрицательно определенные операторы, которые суммируются в единичный
[34:46.680 --> 35:01.480]  оператор. Так вспомнили? Теперь я говорю. Мой контр-аргумент к вашему. Возьму POVM с m исходами измерений.
[35:01.480 --> 35:12.240]  И буду m увеличивать. Могу сделать тысячу исходов измерений. Могу миллион. И тогда кажется,
[35:12.240 --> 35:19.640]  что раз у меня миллион, значит двоичный алгорифм от этого миллиона это количество битов,
[35:19.640 --> 35:26.560]  которые я могу извлечь. А могу сделать миллиард. Получается, что тоже не ограничено. Понимаете?
[35:26.560 --> 35:35.320]  Так, у меня к вам вопрос. Вы понимаете, что число исходов никак не ограничено. Внутренность
[35:35.320 --> 35:42.240]  этой коробочки может устроено быть таким образом. У вас есть состояние на входе. Вы потом используете
[35:42.240 --> 35:51.440]  вспомогательные какие-то кубиты или еще что-то, кутриты, что хотите. Разрешаете им взаимодействовать
[35:51.440 --> 35:58.880]  с вашей системой. То есть перепутываете их. А потом делаете измерение уже того, что на выходе. А
[35:58.880 --> 36:04.320]  когда измеряете, понятно, что количество исходов здесь никак не ограничено.
[36:04.320 --> 36:16.720]  Теперь у нас встает с вами задачка, которую логично обсуждать в курсе кафедры теор-физики.
[36:16.720 --> 36:29.480]  Так сколько же информации можно извлечь? Да, а тут получается хуже. Счетная, конечная даже,
[36:29.480 --> 36:36.760]  но ничем не ограниченная. То есть если вы хотите, если так рассуждать, как вот здесь вот сейчас у нас,
[36:36.760 --> 36:44.880]  то есть вы задаете мне наперед какую-то границу, ну например, извлекать не меньше 50 битов.
[36:44.880 --> 36:59.480]  Информация. Я рисую два в пятидесятые там чего-нибудь, исходов и все. То есть какое число вы мне не задали,
[36:59.480 --> 37:11.800]  я могу его реализовать. Но на самом деле задачка-то вот в чем состоит. У вас есть взаимная информация
[37:12.480 --> 37:19.120]  и икс и у. И согласно теореме Шэннона, она показывает вам
[37:19.120 --> 37:30.760]  достижимую скорость. И ее supremo по иксам, ой, по распределению на входе, показывает вам максимум
[37:30.760 --> 37:39.760]  из того, что вы можете извлечь. Понятно? Вот эта величина показывает, сколько информации можно
[37:39.760 --> 38:04.080]  извлечь. Показывает количество в кавычках извлекаемой информации. И теперь у нас с вами будет некоторое утверждение.
[38:09.760 --> 38:22.960]  Ну давайте, чтобы сформулировать утверждение, надо еще ввести некоторые понятия. Давайте вернусь
[38:22.960 --> 38:29.760]  сюда на эту картинку. Буквы икс у вас встречаются с вероятностями px. Здесь будьте внимательны,
[38:29.760 --> 38:36.240]  потому что РО, поэтому отрицеплотность, а p распределение вероятностей. Пишется похожим
[38:36.240 --> 38:45.440]  образом, но это разные выражения. Иногда в книжках пишут px. p теперь это не число p, а буква,
[38:45.440 --> 38:52.160]  которая показывает распределение. Ну ладно, я не буду вас мучить такими обозначениями, поэтому давайте
[38:52.160 --> 38:57.880]  латинские использовать. Так, значит, смотрите, получается, что вот эти состояния РО и икс будут с
[38:57.880 --> 39:05.640]  вероятностью px реализовываться. Есть такое понятие среднее состояние ансамбла. Есть понятие
[39:05.640 --> 39:22.040]  ансамбля. Что такое ансамбль? Ансамбль состояний. Ансамбль состояний это множество вот таких вот пар,
[39:22.040 --> 39:32.160]  вероятность, с которой реализуется матрица плотности РО икс. Понятно? Множество таких пар.
[39:32.160 --> 39:43.080]  Икс принадлежит вот этому входному алфавиту. Вот что такое ансамбль. Теперь есть понятие
[39:43.080 --> 39:56.280]  среднего состояния ансамбля. Ну это очевидно. Среднее состояние ансамбля. РО с чертой сверху
[39:56.280 --> 40:08.920]  напишем. Это есть сумма по иксам px РО икс. Среднее состояние ансамбля. И теперь я готов сформулировать утверждение.
[40:08.920 --> 40:27.640]  Утверждение такое. Вот эта взаимная информация и икс-ы в этой схеме не превышает некоторого значения,
[40:27.640 --> 40:34.560]  которое называется границей Холли. А вычисляется оно как? Как энтропия фон Эймана от
[40:34.560 --> 40:52.800]  энтропии среднего состояния ансамбля минус средняя энтропия. Эта граница называется граница Холли.
[40:53.000 --> 41:07.680]  Того самого, который написал книжку, которую я вам рекомендую. Само утверждение понятно?
[41:07.680 --> 41:20.600]  Что такое С, помните? С от РО, а это есть минус след РО логарифм РО. Энтропия фон Эймана.
[41:20.600 --> 41:32.400]  Давайте сначала проанализируем, чтобы для кубита уже понять. Энтропия может быть отрицательной?
[41:32.400 --> 41:45.000]  Нет. Последний член только может уменьшить величину. Вот эта взаимная информация, следствие.
[41:45.000 --> 42:00.400]  Мы еще не доказали, но уже обсуждаем. Следствие и икс-ы не превосходит средней энтропии среднего
[42:00.400 --> 42:07.600]  состояния ансамбля. Теперь, если кодируете в кубитные состояния, вот здесь кубиты,
[42:07.600 --> 42:24.520]  то РО среднее это матрица плотности какая-то кубита. Энтропия матрицы плотности 2 на 2 не
[42:24.600 --> 42:47.320]  превышает в свою очередь 1 бита. А если бы у нас была матрица не кубитная, а например кудитная,
[42:47.320 --> 42:53.760]  то есть если бы мы, например, использовали D-уровневые квантовые системы D на D, то тогда
[42:53.760 --> 43:06.120]  у нас бы получилась с вами логарифм D бит. Таким образом, если мы с вами докажем это утверждение,
[43:06.120 --> 43:11.920]  то это означает, что хотя в кубиты можно записать бесконечное количество информации,
[43:11.920 --> 43:19.440]  пытаясь ее извлечь вне зависимости от того, сколько исходов измерения у меня будет,
[43:19.440 --> 43:28.360]  то есть мой контраргумент не прокатит. Могу использовать хоть 10 измерений с 10 исходами,
[43:28.360 --> 43:35.240]  хоть с миллионом, все равно больше чем один бит информации из одного кубита я извлечь не смогу.
[43:35.240 --> 43:47.840]  А, ломается он вот в каком месте, потому что вероятности исходов, когда я делаю измерения
[43:47.840 --> 43:56.160]  с многими исходами, они задаются некоторой формулой. Эта формула, это след РОСЕ и получается,
[43:56.160 --> 44:06.480]  что у вас стехосидическая матрица имеет определенный вид, не произвольный, а определенный,
[44:06.480 --> 44:17.040]  конкретный вид, который задается правилами квантовой механики. Сейчас запишу. И в этом месте как раз и ломается.
[44:17.040 --> 44:30.480]  Все. Когда вы задали POVM, давайте теперь набросок доказательства сделаем. Наверное, все не успеем даже.
[44:30.480 --> 44:51.200]  Вот вероятность получить исход у при измерении при условии, что вы хотели переслать букву х,
[44:51.200 --> 45:03.400]  вот эта вероятность, как у вас запишется, это след того самого РОХ с ЕY. Вот это вот понятно.
[45:03.400 --> 45:16.080]  Но если POVM помните, то вот эта формула должна сразу всплыть. И поскольку у вас вот это выполняется
[45:16.080 --> 45:25.720]  свойство, вот поэтому благодаря ему, этому свойству у вас стехосидческая матрица имеет определенный вид,
[45:25.720 --> 45:33.040]  вот задаваемый этой формуле. Именно из-за этого вы не можете извлечь больше информации, чем один бит, из одного кубит.
[45:33.040 --> 45:45.360]  Так, теперь смотрите, что нам нужно сделать. Нам нужно эту формулу доказать, вот с этим неравенством.
[45:45.360 --> 45:53.760]  А в доказательстве используются еще вспомогательные некоторые конструкции, поэтому нам нужно еще будут некоторые леммы.
[45:53.760 --> 46:10.240]  Помните, что такое относительная энтропия квантовая? Вот я теперь в элементы этой относительной энтропии
[46:10.240 --> 46:24.080]  вставлю действие канала на некоторое РО и пишу, что это будет не превышать РОС для любого
[46:24.080 --> 46:42.800]  вполне положительного и сохраняющего след отображения ФИ. То есть для квантового канала ФИ, что происходит?
[46:42.800 --> 46:49.680]  Что квантовая относительная энтропия обладает свойством монотонности. Физически как его нужно понимать?
[46:50.160 --> 47:03.760]  Вот пусть у вас есть множество квантовых состояний. Вот у вас РО и СИГМА. Квантовая относительная энтропия своего рода расстояние,
[47:03.760 --> 47:09.600]  но несимметричная по перестановке РО и СИГМА. То есть оно показывает, насколько отличаются РО и СИГМА.
[47:09.600 --> 47:14.000]  Напомню, что равна нулю это увеличена тогда и только тогда, когда РО совпадает с СИГМА.
[47:14.000 --> 47:25.760]  Помните? Так, им нужна какая-то реакция этого. Помните? Своего рода расстояние. Как действует отображение?
[47:25.760 --> 47:31.680]  Вспоминайте задачки из прошлого семестра. Нарисуйте образ шара, блог под действием какого-то отображения.
[47:31.680 --> 47:40.320]  Все сжимается, правильно? Либо к оси, либо к точке, либо еще куда-то. Образ всегда меньше, чем исходная множество.
[47:40.320 --> 47:48.720]  Значит, ФИ РО будет лежать где-то здесь, ФИ СИГМА будет лежать где-то здесь.
[47:48.720 --> 47:57.440]  Состояние что сделается под действием сжимающего отображения? Уменьшится. Вот эта величина меньше, чем исходная величина.
[47:57.520 --> 48:00.800]  Вот геометрический смысл этой формы. Понятно?
[48:04.320 --> 48:09.120]  Не доказываем. Если такая потребность возникнет, то будем доказывать.
[48:11.120 --> 48:17.120]  Пока используем вот эту интуицию, которая здесь хорошо работает.
[48:17.440 --> 48:39.600]  Кстати, недавно люди, совсем недавно, несколько лет назад года, в каком году? В 17-м, доказали, что это верно не только для вполне положительных отображений, но и просто положительных, что было некоторой сенсацией в нашей области науки.
[48:40.080 --> 48:46.080]  Так, теперь другой пример. Давайте лему другую еще сделаем. Лемма 2.
[48:48.640 --> 49:01.680]  Взятие частичного следа – это тоже канал. Мы про это раньше не думали, а сейчас обсудим.
[49:01.760 --> 49:11.760]  Поэтому, это как бы даже утверждение сразу, следствие из этой леммы.
[49:11.760 --> 49:15.760]  Поэтому относительно энтропия может только уменьшаться.
[49:15.760 --> 49:27.760]  То есть, если вы возьмете составную систему РОАБСААВ, возьмете частичный след, то энтропия может только уменьшиться.
[49:31.760 --> 49:41.760]  Так, сейчас объясню, что имеется в виду. Здесь имеется в виду конкретный вид ФИ.
[49:43.760 --> 49:45.760]  Сейчас, сейчас напишу.
[49:49.760 --> 49:59.760]  Сумма по ж и ж. Напоминаю вам формулы из предыдущего семестра.
[50:01.760 --> 50:09.760]  И единичный оператор под системе А, ж в B.
[50:11.760 --> 50:19.760]  Вы видите, что вот то, что здесь написано, это есть представление Крауса с вот такими операторами Крауса.
[50:21.760 --> 50:27.760]  А поэтому, это квантовый канал. Но в чем особенности этого квантового канала?
[50:27.840 --> 50:31.840]  Размерность на выходе не совпадает с размерностью на входе.
[50:33.840 --> 50:39.840]  Так же, как при изучении языка сначала какие-то простые моменты проходят, потом обобщается, усложняется.
[50:39.840 --> 50:43.840]  Точно так же и здесь. Мы с вами рассматривали каналы на какие,
[50:43.840 --> 50:49.840]  которые на вход брали матрицу D на D, на выходе выдавали ту же самую матрицу D на D.
[50:49.840 --> 50:53.840]  А здесь на выходе матрица меньшего размера получилась.
[50:53.920 --> 50:58.920]  Но это тоже вполне положительное и сохраняющий след отображения.
[50:58.920 --> 51:03.920]  Ну, след сохраняется, почему? Потому что у частичного следа,
[51:05.920 --> 51:11.920]  как вы помните, есть такое свойство, что след сохраняется.
[51:16.920 --> 51:20.920]  А вполне положительное следует из представления Крауса.
[51:21.000 --> 51:23.000]  Так, вот здесь все понятно?
[51:24.000 --> 51:30.000]  Взятие частичного следа, это тоже канал, поэтому к нему применима вот эта формула.
[51:32.000 --> 51:38.000]  Понятно? Все. Теперь две леммы у нас есть, теперь будет вспомогательная конструкция.
[51:41.000 --> 51:45.000]  Видите, какое непростое утверждение у нас с вами получается.
[51:45.080 --> 51:52.080]  Вспомогательная конструкция такая. Будет у нас с вами классическая, как бы, часть.
[51:52.080 --> 51:56.080]  Буквой P ее обозначим от английского preparation.
[52:00.080 --> 52:03.080]  Будет квантовая часть. Это наша система.
[52:07.080 --> 52:10.080]  И будет часть, связанная с измерением measurement.
[52:15.080 --> 52:19.080]  И давайте теперь посмотрим вот на такой объект.
[52:20.080 --> 52:31.080]  Ро PQM. Это будет эффективная трехсоставная система с такой матрицей плотности.
[52:33.080 --> 52:35.080]  Сумма по х.
[52:38.080 --> 52:44.080]  Дальше у нас будет стоять Px, вероятность появления х.
[52:45.080 --> 52:48.080]  Потом будет стоять проектор на x.
[52:50.080 --> 52:54.080]  Дальше сами ro x через тендерные произведения.
[52:55.080 --> 52:59.080]  И 0, то, что стоит в измерении.
[53:01.080 --> 53:04.080]  Так, я стираю левую доску.
[53:04.160 --> 53:07.160]  Это вспомогательная конструкция.
[53:07.160 --> 53:12.160]  То есть абстрагируйтесь пока от того, что было.
[53:13.160 --> 53:16.160]  Посмотрите на этот объект по-новому.
[53:16.160 --> 53:24.160]  Что мы здесь ввели? Мы ввели вспомогательное Гильбертово пространство с вот этими x классическими.
[53:24.160 --> 53:27.160]  То есть вот эти x и x-штрих.
[53:27.240 --> 53:37.240]  Если вы возьмете пространство, натянутое на такие векторы, то есть с Px возьмете линейную оболочку,
[53:37.240 --> 53:42.240]  то это будет что? Это будет Гильбертово пространство, связанное с приготовлением P.
[53:43.240 --> 53:47.240]  Это вспомогательное Гильбертово пространство.
[53:47.320 --> 53:50.320]  Сами руками его сделали.
[53:53.320 --> 53:58.320]  Значит, у вас еще есть пространство, связанное с вашей системой,
[53:58.320 --> 54:03.320]  там, где матрица плотности ro x, это ваша обычный Гильбертово пространство,
[54:03.320 --> 54:06.320]  в которое вы кодируете.
[54:06.320 --> 54:10.320]  То есть у вас еще есть пространство, связанное с вашей системой,
[54:10.320 --> 54:12.320]  там, где матрица плотности ro x.
[54:12.400 --> 54:16.400]  Матрица плотности ro x, это ваша обычный Гильбертово пространство,
[54:16.400 --> 54:20.400]  в которое вы кодируете ваш классический буквы.
[54:20.400 --> 54:23.400]  А m – это результаты измерений.
[54:23.400 --> 54:30.400]  Размерность этого h m – это есть как раз то количество исходов измерения.
[54:30.400 --> 54:35.400]  Количество исходов измерений.
[54:35.400 --> 54:38.400]  И оно ничем не ограничено.
[54:38.480 --> 54:43.480]  То есть может быть миллион, может быть десять, может быть два.
[54:46.480 --> 54:49.480]  Какое хотите – такое сделаем.
[54:49.480 --> 54:54.480]  Теперь давайте посмотрим, что здесь есть.
[54:58.480 --> 55:03.480]  Давайте посмотрим под систему.
[55:03.560 --> 55:11.560]  Под систему вы видите, что сумму по x можно приписать только к первым двум выражениям.
[55:14.560 --> 55:21.560]  Поэтому давайте посмотрим на вот эти первые два выражения.
[55:21.560 --> 55:26.560]  Ro pq – редуцированная матрица плотности, в этом случае тривиальная.
[55:26.640 --> 55:33.640]  Это будет сумма по x, px, x, x, ro x.
[55:33.640 --> 55:40.640]  Ну и давайте найдем энтропию этого состояния.
[55:40.640 --> 55:43.640]  Значит как мы будем искать энтропию?
[55:43.640 --> 55:46.640]  Мы с вами поднимаем, как записать эту матрицу.
[55:46.640 --> 55:54.640]  Эта матрица на самом деле в стандартном базисе из x и базисы для ro
[55:54.720 --> 55:58.720]  будет иметь блочно диагональный вид.
[55:58.720 --> 56:07.720]  На диагонали будут стоять p1, ro1, p2, ro2 и так далее.
[56:07.720 --> 56:10.720]  Все остальные нулевые элементы.
[56:10.720 --> 56:13.720]  Отхожу, смотрите, почему это так?
[56:13.720 --> 56:18.720]  Потому что x – ортонормированный базис.
[56:18.800 --> 56:24.800]  Значит возьму первый x, это будет 1 с нулями, 1 с нулями,
[56:24.800 --> 56:28.800]  умножается на ro x тензорно, даст мне вот этот элемент.
[56:30.800 --> 56:33.800]  Блочно диагональная структура, понятно?
[56:33.800 --> 56:40.800]  Значит энтропия, как вы помните, связана с собственными значениями.
[56:40.880 --> 56:47.880]  В данном случае, когда у вас блочно диагональная матрица,
[56:47.880 --> 56:55.880]  это будет сумма px, энтропии вот этих блоков px, rox.
[56:58.880 --> 57:01.880]  Так, правую доску сотру.
[57:01.880 --> 57:04.880]  Или здесь, давайте здесь верхнюю часть.
[57:04.960 --> 57:13.960]  Теперь вспоминаем, что такое энтропия.
[57:13.960 --> 57:30.960]  s px, rox, это есть минус, след, дальше стоит px, rox и логарифм от этого px, rox.
[57:31.040 --> 57:38.040]  Значит, логарифм произведения в школе – это была сумма логарифмов.
[57:38.040 --> 57:44.040]  С матрицами почти то же самое, px – это у вас число, а rox – матрица.
[57:44.040 --> 57:51.040]  Значит, у вас будет логарифм px умножить на единичную матрицу,
[57:51.040 --> 57:57.040]  прибавить логарифм rox.
[58:01.040 --> 58:04.040]  Понятно?
[58:05.040 --> 58:08.040]  Что тогда получается?
[58:08.040 --> 58:13.040]  p, ro, p и берем след.
[58:13.040 --> 58:18.040]  Значит, от первого множителя просто возьмет след от матрицы плотности, будет единичка.
[58:18.040 --> 58:24.040]  Значит, будет минус px, логарифм px.
[58:24.040 --> 58:27.040]  От второго выражения что будет?
[58:27.120 --> 58:33.120]  px число выношу, а будет след rox, логарифм rox.
[58:33.120 --> 58:35.120]  Это просто энтропия.
[58:35.120 --> 58:40.120]  Плюс px, s, rox.
[58:40.120 --> 58:43.120]  Таким образом у нас что получается?
[58:43.200 --> 58:48.200]  Что вот это s, p, q, s, ro, p, q.
[58:50.200 --> 58:53.200]  Это есть...
[58:57.200 --> 58:59.200]  Что такое?
[58:59.200 --> 59:04.200]  Энтропия распределения px.
[59:06.200 --> 59:10.200]  Энтропии для классических величины буквы h обычно обозначаются.
[59:10.280 --> 59:17.280]  Прибавить сумма по x, px, s, rox.
[59:17.280 --> 59:20.280]  Вот это h, px.
[59:20.280 --> 59:27.280]  Это есть энтропия для классических величин, то есть минус px, лог px.
[59:31.280 --> 59:35.280]  Теперь с этой энтропией разобрались.
[59:35.360 --> 59:48.360]  Теперь нам нужно какую-то конструкцию сделать, включающую в себя относительную энтропию.
[59:48.360 --> 59:53.360]  Что мы с вами здесь тогда посмотрим?
[59:53.360 --> 01:00:00.360]  Отдельно можем еще найти rope из этой формулы.
[01:00:00.360 --> 01:00:03.360]  Давайте отдельно найдем rope.
[01:00:03.440 --> 01:00:08.440]  Редуцированная матрица плотности под системы p.
[01:00:08.440 --> 01:00:13.440]  Это будет просто сумма px, px, xx.
[01:00:13.440 --> 01:00:17.440]  А x-ы образуют ортонормированный базис.
[01:00:17.440 --> 01:00:26.440]  Поэтому энтропия вот этого rope, p, это есть просто вот та h, px.
[01:00:26.520 --> 01:00:31.520]  А что еще у нас есть?
[01:00:31.520 --> 01:00:36.520]  У нас есть еще roe, q.
[01:00:36.520 --> 01:00:45.520]  Roe, q – это взятие частичного следа, значит по этой первой части x-ы след единичный.
[01:00:45.520 --> 01:00:50.520]  Поэтому px, roe, x будет просто сумма px, px, roe, x.
[01:00:50.520 --> 01:00:54.520]  А это есть среднее состояние ансамбля, roe среднее.
[01:00:54.600 --> 01:00:57.600]  И теперь мы с вами можем записать относительную энтропию.
[01:00:57.600 --> 01:01:00.600]  Относительная энтропия чего?
[01:01:00.600 --> 01:01:08.600]  Roe, p, q с одной стороны, а с другой стороны тензорное произведение.
[01:01:08.600 --> 01:01:13.600]  Roe, p и roe, q.
[01:01:13.600 --> 01:01:16.600]  И что это такое?
[01:01:16.680 --> 01:01:26.680]  Так, это задачка из прошлого семестра.
[01:01:26.680 --> 01:01:41.680]  Это есть энтропия roe одной подсистемы, плюс энтропия другой подсистемы, минус энтропия всего в целом.
[01:01:46.680 --> 01:01:57.680]  Подставляем s, roe, p – это h, энтропия вот этого распределения классического.
[01:01:57.680 --> 01:02:04.680]  Прибавить s, roe, q – это энтропия среднего состояния ансамбля.
[01:02:04.680 --> 01:02:10.680]  И отнять вот эту величину, которую мы считали s, roe, p, q.
[01:02:10.760 --> 01:02:23.760]  Значит, это будет минус h, этого px и минус средняя энтропия по ансамблю.
[01:02:23.760 --> 01:02:33.760]  Видите, что h уходит, и в итоге получается та самая граница Холева, которая у нас есть.
[01:02:33.840 --> 01:02:43.840]  s, roe средняя минус сумма по х, px, s, roe, x.
[01:02:45.840 --> 01:02:54.840]  Значит, мы с вами правую часть в утверждении для границы Холева записали в виде относительной энтропии.
[01:02:54.840 --> 01:02:56.840]  Дальше какой будет наш трюк?
[01:02:56.920 --> 01:03:02.920]  Мы с вами придумаем сейчас некоторое вполне положительное отображение phi,
[01:03:02.920 --> 01:03:07.920]  которое сведет вот эту запись к другой записи,
[01:03:07.920 --> 01:03:14.920]  а эта другая запись будет как раз-таки равна взаимной информации и x, y.
[01:03:14.920 --> 01:03:16.920]  Вот в чем трюк заключается. Понятно?
[01:03:16.920 --> 01:03:20.920]  Тогда, используя лему про монотонность относительной энтропии,
[01:03:20.920 --> 01:03:24.920]  мы получим с вами результат, который нужен.
[01:03:25.000 --> 01:03:31.000]  Если успеем. Сколько у нас осталось там, минуточек?
[01:03:33.000 --> 01:03:35.000]  Четыре.
[01:03:35.000 --> 01:03:37.000]  Тогда, наверное, не успеем.
[01:03:39.000 --> 01:03:43.000]  Значит, идея дальнейшая, смотрите, идея.
[01:03:43.000 --> 01:03:54.000]  Найти, ну да, это не найти, построить, phi, построить phi.
[01:03:54.080 --> 01:04:00.080]  Значит, смотрите, кстати, что мы можем с вами делать?
[01:04:00.080 --> 01:04:02.080]  Что мы можем с вами делать?
[01:04:02.080 --> 01:04:07.080]  Здесь только две было под системой из трех.
[01:04:07.080 --> 01:04:14.080]  Почему? Потому что третья была вот в таком вот тривиальном состоянии для измерения изначально.
[01:04:14.080 --> 01:04:19.080]  Но поскольку она в факторизованном виде, я могу ее приписать сюда,
[01:04:19.160 --> 01:04:21.160]  ничего не потеряв.
[01:04:21.160 --> 01:04:26.160]  То есть, перед этой идеей еще мы с вами вот что сделаем.
[01:04:28.160 --> 01:04:32.160]  Я могу дописать P Q M,
[01:04:32.240 --> 01:04:46.240]  Ро, так, П, М, так, правильно или нет?
[01:04:46.240 --> 01:04:50.240]  Нет, Ро П и Q M, вот так вот.
[01:04:53.240 --> 01:04:57.240]  Равно в точности вот тому выражению?
[01:04:57.320 --> 01:05:00.320]  П
[01:05:04.320 --> 01:05:08.320]  Поскольку, почему?
[01:05:08.320 --> 01:05:11.320]  Поскольку мы знаем с вами формулу,
[01:05:11.320 --> 01:05:18.320]  что энтропия тензорного произведения Ро на любой там другую матрицу Омега,
[01:05:18.320 --> 01:05:22.320]  это будет S Ро плюс S Омега.
[01:05:22.400 --> 01:05:26.400]  А если Омега это 0,0, то энтропия равна 0,
[01:05:26.400 --> 01:05:29.400]  то получается просто S Ро.
[01:05:32.400 --> 01:05:35.400]  Понятно?
[01:05:35.400 --> 01:05:37.400]  Нет?
[01:05:42.400 --> 01:05:45.400]  P Q, а да, перепутан.
[01:05:45.400 --> 01:05:50.400]  Да, все перепутал, и тут должно быть тензорное произведение.
[01:05:50.480 --> 01:05:55.480]  Ро П, ну то самое выражение, которое там Ро П.
[01:05:59.480 --> 01:06:04.480]  Смотрите, теперь у нас здесь тройная часть, правильно?
[01:06:04.480 --> 01:06:09.480]  И это тройное выражение для трех подсистем.
[01:06:09.480 --> 01:06:13.480]  Это есть энтропия среднего минус средняя энтропия.
[01:06:17.480 --> 01:06:19.480]  Значит, в чем стоит дальнейшая идея?
[01:06:19.560 --> 01:06:23.560]  Построить ФИ, который нам,
[01:06:23.560 --> 01:06:30.560]  подействовав на вот это Ро П Qm, даст некоторый объект,
[01:06:30.560 --> 01:06:37.560]  подействовав на вот эту часть Ро П тензорно Ро Qm,
[01:06:37.560 --> 01:06:41.560]  даст некоторый объект, который с одной стороны
[01:06:41.560 --> 01:06:46.560]  будет равняться взаимной информации между X и Y.
[01:06:46.640 --> 01:06:49.640]  И мы даже знаем, как его сделать.
[01:06:49.640 --> 01:06:53.640]  Вам нужно сначала описать процесс измерения,
[01:06:53.640 --> 01:06:56.640]  а потом выкинуть квантовую часть, правильно?
[01:06:56.640 --> 01:06:59.640]  Тогда получится только P и M.
[01:07:03.640 --> 01:07:06.640]  А с другой стороны, в силу того,
[01:07:06.640 --> 01:07:10.640]  что имеет место монотонность относительно энтропии,
[01:07:10.640 --> 01:07:13.640]  у вас это будет не превосходить исходной величины.
[01:07:16.640 --> 01:07:19.640]  А исходная величина, это и есть граница Холева.
[01:07:19.640 --> 01:07:21.640]  Вот в чем состоит идея.
[01:07:21.640 --> 01:07:23.640]  То есть, как только мы этот ФИ предъявим,
[01:07:23.640 --> 01:07:26.640]  который равняется вот этому ИХY,
[01:07:26.640 --> 01:07:29.640]  то сразу все и получится.
[01:07:29.640 --> 01:07:32.640]  И вот этот ФИ, он выглядит как
[01:07:32.640 --> 01:07:36.640]  последовательно применение двух операций.
[01:07:36.640 --> 01:07:39.640]  Сначала квантовый процесс измерения,
[01:07:39.640 --> 01:07:42.640]  а потом вот этот ФИ,
[01:07:42.720 --> 01:07:45.720]  частичный след по квантовой подсистеме.
[01:07:45.720 --> 01:07:48.720]  Это измерение тоже могу написать,
[01:07:48.720 --> 01:07:51.720]  но, наверное, уже в следующий раз,
[01:07:51.720 --> 01:07:54.720]  чтобы не загромождать.
[01:07:54.720 --> 01:07:57.720]  Логика какая?
[01:07:57.720 --> 01:08:00.720]  Граница Холева устанавливает верхнюю
[01:08:00.720 --> 01:08:03.720]  границу того, сколько энтропии есть,
[01:08:03.720 --> 01:08:06.720]  а потом выкинуть квантовую часть,
[01:08:06.720 --> 01:08:09.720]  а потом выкинуть квантовую часть,
[01:08:09.800 --> 01:08:12.800]  то есть выкинуть границу того,
[01:08:12.800 --> 01:08:15.800]  сколько информации можно выжать
[01:08:15.800 --> 01:08:18.800]  из квантового объекта.
[01:08:18.800 --> 01:08:21.800]  На семинарах тогда пойдете вперед
[01:08:21.800 --> 01:08:24.800]  и посмотрите на конкретных задачах,
[01:08:24.800 --> 01:08:27.800]  сколько информации можно получить
[01:08:27.800 --> 01:08:30.800]  в конкретных задачах.
[01:08:30.800 --> 01:08:33.800]  На сегодня все.

[00:00.000 --> 00:14.500]  Тема сегодняшней лекции – это многопоточность, вообще на самом деле все, что связано с потоками,
[00:14.500 --> 00:25.340]  синхронизацией, мьютексы, блокировки, файберы. Все это вас расскажется в отдельном курсе,
[00:25.340 --> 00:33.340]  поэтому в окосе про это упоминается совсем немного, только те вещи, которые связаны
[00:33.340 --> 00:38.300]  непосредственно с низкого уровня взаимодействия на уровне операционных систем. То есть никакой
[00:38.300 --> 00:45.380]  большой теории про разные синхронизации у вас не будет, будет именно использование инструментов
[00:45.380 --> 00:54.260]  POSIX. Итак, потоки. На самом деле слово «потоки» – оно очень плохое и правильнее называть преды.
[00:54.260 --> 00:59.940]  Ангалицизм, конечно, но хорошего перевода на русский язык до сих пор не придумано,
[00:59.940 --> 01:07.460]  потому что слово «поток» уже занято это слово «стрим», «потоки» бывают очень разные. В принципе,
[01:07.460 --> 01:14.020]  еще где-то вот у мгушников используется терминология нити, достаточно странная. Еще
[01:14.020 --> 01:21.260]  используется терминология «легковесные процессы», но тоже не очень так хорошо. В общем,
[01:21.260 --> 01:29.420]  просто thread – это thread. Давайте их называть так. Что такое thread? Трэды – это просто какие-то
[01:29.420 --> 01:40.060]  задачи, которые выполняются процессорами, либо несколькими процессорами. На самом деле любой
[01:40.060 --> 01:47.540]  многоядерный процессор с точки зрения логической организации и с точки зрения операционной системы –
[01:47.540 --> 01:53.540]  это просто несколько процессоров одинаковых, даже несмотря на то, что они находятся все на одном
[01:53.540 --> 02:02.180]  кристалле. И что есть у каждого трэда? У каждого трэда есть свое состояние процессора, т.е. текущая
[02:02.180 --> 02:09.220]  выполняемая инструкция, набор регистров, состояние процессора, находится ли он в привилегированном
[02:09.220 --> 02:16.260]  режиме или нет, набор флагов. Но еще плюс у каждого трэда есть своя небольшая область
[02:16.260 --> 02:24.540]  памяти, которая называется stack. Каждый трэд – это отдельная задача, они выполняются независимо
[02:24.540 --> 02:30.420]  друг от друга, могут выполнять какие-то функции. Соответственно, для выполнения функций нам нужно
[02:30.420 --> 02:36.300]  где-то хранить адрес возврата, и для этого как раз нужен stack, но опять же функции могут быть
[02:36.300 --> 02:47.460]  локальные переменные. И вот в чем принципиальное отличие трэдов от процессов? Когда мы говорили
[02:47.460 --> 02:53.260]  про процессы, то я постоянно упоминал о том, что это что-то самодостаточное, изолированное,
[02:53.260 --> 03:01.500]  и между собой процессы могут общаться только посредством игра. Вот с потоками это немножко не
[03:01.500 --> 03:09.340]  так, потому что все потоки в рамках одного процесса, они разделяют одно и то же адресное
[03:09.340 --> 03:17.700]  пространство, у них могут быть общие файловые дискрипторы, ну и все атрибуты процесса распространяются,
[03:17.700 --> 03:28.020]  в том числе и на потоке. Вот когда мы запускаем какой-то процесс, то мы уже запускаем одну задачу,
[03:28.020 --> 03:34.220]  это самый первый поток, который у нас выполняется. Он начинает выполнять функцию подчеркивания start,
[03:34.220 --> 03:40.420]  которая в свою очередь начинает выполнять сишную функцию main, либо плюсовую функцию main,
[03:40.420 --> 03:46.420]  либо ни то ни другое, если у вас программа на паскали. Вот есть некая точка входа подчеркивания
[03:46.420 --> 03:57.460]  start, которая начинает выполнять именно самый первый поток. При этом этот самый первый поток может
[03:57.460 --> 04:04.740]  создавать новые потоки программы. В свою очередь каждый вновь созданный поток тоже может создавать
[04:04.740 --> 04:12.940]  еще какие-то потоки. И получается, что вроде как что-то похоже на организацию нескольких процессов,
[04:12.940 --> 04:18.260]  но тут есть одно важное принципиальное отличие. Когда мы говорим про процессы, мы можем посмотреть
[04:18.260 --> 04:26.740]  на дерево процессов, кто для кого является родителем. В случае смерти какого-то узла в дереве процессов,
[04:26.740 --> 04:35.220]  у нас его дочерние процессы переподвешиваются на корневой процесс, а вот с трэдами это все не так.
[04:35.220 --> 04:41.620]  У нас конечно запускается какой-то трэд при запуске процесса, но этот трэд совершенно
[04:41.620 --> 04:48.060]  равноправен со всеми остальными трэдами, которые он запустил. И в принципе возможность ситуации,
[04:48.060 --> 04:54.540]  когда главный тред завершился, но программа еще не завершилась. И между трэдами в отличие от
[04:54.540 --> 05:02.060]  процессов нет такого понятия, как один тред является родителем другого. Они все являются
[05:02.060 --> 05:10.300]  равнозначными и вытворять могут все что угодно. Когда вы завершаете работу программы с помощью
[05:10.300 --> 05:16.860]  системного вызова exit либо функции exit, есть еще системный вызов под названием exit group,
[05:16.860 --> 05:22.780]  если вы посмотрите вывод команды strace на какую-нибудь программу, которая завершает
[05:22.780 --> 05:28.420]  корректную свою работу, то иногда вместо системного вызова используя экзит group,
[05:28.420 --> 05:37.780]  который завершает работу всех трэдов, то у вас завершается работа сразу всех поток. Есть
[05:37.780 --> 05:43.540]  аналогичные функции, которые называются pit-thread и exit, которые завершают работу только
[05:43.540 --> 05:51.260]  текущего потока. Давайте посмотрим, что будет, если мы запустим какие-то трэды,
[05:51.260 --> 06:01.420]  заодно посмотрим на API для их создания и потом завершим какую-то их работу. Итак,
[06:01.420 --> 06:10.380]  как у нас вообще выглядит создание трэдов? Трэд должен выполнять какую-то функцию. Стандартная
[06:10.380 --> 06:18.580]  сигнатура для функций в POSIX это функция, которая принимает единственный аргумент по размеру с
[06:18.580 --> 06:25.380]  машиной слова void-звездочка. Это может быть либо указатель на какой-то контекст, с которым должен
[06:25.380 --> 06:31.780]  работать трэд, либо просто какое-то произвольное число по размеру, не превышающее размер
[06:31.780 --> 06:38.980]  машинного слова. Ну и функция трэд возвращает опять же что-то по размеру, не превышающий,
[06:38.980 --> 06:44.780]  чем размер машинного слова. Просто функция, которая принимает ровно один вход и имеет ровно один
[06:44.780 --> 06:54.340]  вих. И эту функцию нужно скормить функцией pit-thread-create, которая создает новый поток
[06:54.340 --> 07:01.940]  выполнения, к которому выполняется та функция, которую мы ему передали. В курсе липовского вы,
[07:01.940 --> 07:09.980]  наверное, на плюсах пишите. Так, у вас там std-thread используется или самописное что-то. В общем,
[07:09.980 --> 07:22.780]  на std-thread все то же самое, только это все запускается через конструкцию. POSIX-thread. В общем,
[07:22.780 --> 07:30.220]  создание это просто создание потока с какой-то функцией, ничего сверхъестественного здесь нет.
[07:30.220 --> 07:46.220]  Ну и мы можем сделать такую вещь, что мы запускаем какую-то программку очень простую, в которой каждый
[07:46.220 --> 07:55.060]  thread что делает. Он просто выводит свой номер и спит одну секунду, потом все повторяет. Просто
[07:55.060 --> 08:05.140]  демонстрация того, что thread действительно работает. Итак, компилируем это все. Так,
[08:05.140 --> 08:12.620]  я это компилирую под маком, поэтому здесь опция командной стройки немножко нестандартная. Под
[08:12.620 --> 08:21.100]  linux тут надо еще написать хорошему опцию minus p thread. Под маком и под std она просто
[08:21.100 --> 08:29.020]  игнорируется. Так, вот мы хотим запустить 10 потоков. Запускаем. Что мы получаем в выводе?
[08:29.020 --> 08:42.780]  Запускается поток 0.1.2.3. Так, и все. Запускаем еще раз. Уже лучше. 0.1.2.3.4.5.6.7.8.9. Давайте еще
[08:42.780 --> 08:55.500]  раз попробуем. 0.3. Кто-нибудь может объяснить этот эффект? Вот что у нас происходит с созданием
[08:55.500 --> 09:07.260]  thread. Здесь у нас, я не просто так закомментировал строчку, мы запускаем 10 thread. Thread начинает
[09:07.260 --> 09:12.860]  работать независимо друг от друга с разной скоростью. Все, мы уже не можем гарантировать
[09:12.860 --> 09:20.460]  строгий порядок вывода, который у нас получается. От каких условий зависит, с какой скоростью
[09:20.460 --> 09:27.100]  выполняются треды? Условий очень много. У нас программа, она, как правило, в Unix, в многозначной
[09:27.100 --> 09:34.300]  системе живет не сама по себе. Есть еще куча других процессов. Тот же Zoom сейчас делает
[09:34.300 --> 09:42.780]  видеозапись. Иногда, когда я мало говорю, он, наверное, мало ресурсов расходит. Когда много
[09:42.780 --> 09:49.540]  говорю, расходует много ресурсов. И это тоже влияет на все треды внутри программы. У нас
[09:49.540 --> 09:59.140]  есть никакой гарантии, что треды будут работать хоть с какой-то одинаковой скоростью и с каким-то
[09:59.140 --> 10:05.620]  предсказуемым поведением, если не использовать их синхронизацию. Почему у нас все завершается?
[10:05.620 --> 10:13.900]  Давайте посмотрим на функцию, которая возвращает целочисленное значение. Даже несмотря на то,
[10:13.900 --> 10:22.900]  что я не написал никакой return, есть исключение в стандартах C и C++, когда функция может не
[10:22.900 --> 10:29.980]  возвращать значение, даже если это объявлено сигнатурой. Это в том случае, если функция
[10:29.980 --> 10:36.540]  называется мы. В этом случае комператор здесь подставляет явным образом значение return 0,
[10:36.540 --> 10:41.860]  то есть вернуться с кодом возврата 0. И после того, как запущено несколько тредов, которые
[10:41.860 --> 10:50.940]  начали независимо друг от друга что-то выполнять, и мы видим результат их работы,
[10:50.940 --> 11:01.820]  тот тред, который запустил все эти треды, то есть самый первый, он доходит до конца функции main,
[11:01.820 --> 11:07.940]  потом результат работы функции main передается, обрабатывается из функции подчеркивания start и
[11:07.940 --> 11:15.220]  вызывается системный вызов exit, который в свою очередь завершает работу сразу всех тредов
[11:15.220 --> 11:27.820]  внутри нашего процесса. Если я хочу просто запустить какое-то количество потоков и завершить
[11:27.820 --> 11:36.700]  функцию main, есть легальный способ это сделать. Мы можем внутри любой функции, внутри любого треда
[11:36.700 --> 11:42.100]  завершить его работу двумя способами. Либо дойти до конца выполнения и вернуть какой-то
[11:42.100 --> 11:53.020]  результат, либо по аналогии с функцией exit мы можем просто из любого места нашей функции,
[11:53.020 --> 11:58.300]  либо из любого места любой другой функции, которая вызывается из функции обработчика треда,
[11:58.300 --> 12:05.940]  вызвать некоторые аналог функцию exit, но уже не для процесса целиком, не для программы,
[12:06.060 --> 12:13.300]  а именно для конкретного треда, который завершит только текущий поток. В том числе это применимо и
[12:13.300 --> 12:21.060]  к главному треду, который выполняет нашу функцию main. Если я раскомментирую строчку petread exit в самом конце,
[12:21.060 --> 12:35.220]  теперь у меня функция main завершит свой поток, но все остальные запущенные потоки продолжают
[12:35.220 --> 12:43.420]  работать. Если какой-нибудь из потоков вызовет теперь системный вызов exit, тогда работа программы
[12:43.420 --> 12:49.540]  завершится. Либо точно так же я могу просто послать обычный сигнал. Этот сигнал дальше
[12:49.540 --> 12:55.740]  обрабатывается случайным образом в одном из тредов и приводит к завершению работы всего процесса.
[12:55.740 --> 13:09.420]  У всех тредов есть общее адресное пространство. Напомню, что в классическом случае у нас в самой
[13:09.420 --> 13:15.620]  нижней части адресного пространства есть небольшой участок, заполненный нулями. Дальше
[13:15.620 --> 13:22.020]  располагается сама программа, которая состоит из секции кода, секции константа, секции данных.
[13:22.020 --> 13:30.100]  Дальше снизу вверх растет куча, если она нужна. Где-то в верхней части адресного пространства у
[13:30.100 --> 13:38.780]  нас располагается, если считать только пространство пользователя, а не пространство ядра, это разделяемые
[13:38.780 --> 13:45.780]  библиотеки, как минимум стандартная сибиблиотека для большинства программ. А дальше сверху вниз
[13:45.780 --> 13:51.500]  растет стек. Причем размер стека у нас обычно фиксирован и память уже считается выделенной.
[13:51.500 --> 14:00.180]  До начала работы функции подчеркиваемых стак. Тем не менее у каждого треда, который работает в
[14:00.180 --> 14:08.540]  рамках одного процесса, есть свой стек. И где эти стеки находятся? Они располагаются тоже в верхней
[14:08.540 --> 14:16.740]  части адресного пространства, но особенность в том, что мы можем запускать новые треды в произвольный
[14:16.740 --> 14:22.020]  момент времени. А еще мы в произвольный момент времени можем загружать какие-то дополнительные
[14:22.020 --> 14:28.460]  динамические библиотеки. То есть совершенно необязательно наша программа должна быть скомпонована
[14:28.460 --> 14:33.940]  с кем-то библиотеками, чтобы их использовать. Классический пример это, к слову, питон вас не
[14:33.940 --> 14:40.540]  должен смущать, у вас поединок будет. Слово Python, интерпретатор Python, который загружает какие-то
[14:40.540 --> 14:46.540]  дополнительные модули, эти модули в свою очередь могут использовать под не только написанный
[14:46.540 --> 14:52.220]  на Python, но и на языке AC, ну или вообще любая система плагинов. И соответственно они тоже
[14:52.220 --> 14:56.500]  динамически в произвольный момент времени подгружают какие-то библиотеки. И в общем случае,
[14:56.500 --> 15:00.980]  что у нас находится в верхней части адресного пространства, считается неопределенным,
[15:00.980 --> 15:06.860]  потому что стеки разных потоков могут чередоваться с разными динамическими
[15:06.860 --> 15:11.740]  подгруженными библиотеками. Да, кстати, напоминаю, что вот картинка, которую сейчас
[15:11.740 --> 15:16.860]  провожу на этом слайде, это классическое размещение процесса в виртуальном адресном
[15:16.860 --> 15:24.460]  пространстве. Потому что у нас еще бывает Ubuntu, Debian, в чем особенности дистрибутивы, кто помнит?
[15:24.460 --> 15:37.460]  Да, там всякие не скучные обои разноцветные, а еще, ну не совсем насыщенные, там по умолчанию
[15:37.460 --> 15:45.140]  работает разномизация размещения процесса в адресном пространстве. То есть, на самом деле,
[15:45.140 --> 15:52.340]  ядро Linux уже очень давно поддерживает эту технологию, когда можно процесс разместить не
[15:52.340 --> 15:59.780]  по фиксированному адресу, а выбрать случайное место в памяти при запуске и разместить
[15:59.780 --> 16:04.900]  исполняемый файл туда. То же самое можно, в принципе, делать для библиотек. Но для того,
[16:04.900 --> 16:11.300]  чтобы любая программа могла быть размещена в произвольном месте, она должна быть скомпилирована
[16:11.300 --> 16:22.500]  с поддержкой позиционно-независимого кода. И в некоторых дистрибутивах программы
[16:22.500 --> 16:28.300]  компилируются без поддержки позиционно-независимого кода. В некоторых дистрибутивах, например,
[16:28.300 --> 16:33.540]  Debian и Ubuntu, они по умолчанию собраны с поддержкой позиционно-независимого кода. То есть,
[16:33.540 --> 16:43.820]  это фактически опция компиляции и поэтому размещаются случайным образом. Так, вот давайте
[16:43.820 --> 16:50.940]  чуть более крупным планом рассмотрим, что у нас у себя представляет верхняя часть адресного
[16:50.940 --> 16:59.580]  пространства там, где стэк. У нас есть стэк самого первого трэда, совпадающий со стэком программы.
[16:59.580 --> 17:06.580]  Обычно по умолчанию его размер 8 мегабайт. На самом деле, вы можете это настроить до запуска
[17:06.580 --> 17:15.820]  программы, используя команду u-limit, и размер стэка вы можете варьировать. На самом деле,
[17:15.820 --> 17:23.940]  8 мегабайт очень много для стэка. Я не помню, показывал вам или нет. В своей группе, наверное,
[17:23.940 --> 17:30.660]  показывал точно, при каких размерах стэка у нас система остается работоспособной. На самом деле,
[17:30.660 --> 17:37.780]  если уменьшить размер стэка до 256 килобайт, то можно даже продолжать успешно запускать Firefox.
[17:37.780 --> 17:46.540]  Google Chrome уже не хочет. На самом деле, 8 мегабайт это более чем достаточно. Вообще, стэк можно
[17:46.540 --> 17:52.420]  увеличить достаточно сильно, только непонятно, зачем это нужно. Очень редкие ситуации, когда это
[17:52.420 --> 17:59.140]  может понадобиться. В отличие от кучи, стэк всегда у нас считается выделенным в начальном моменте
[17:59.140 --> 18:08.980]  времени. Когда мы запускаем новый трэд в рамках одного процесса, у нас появляется еще один новый
[18:08.980 --> 18:19.820]  стэк. По умолчанию его размер равен размеру стэка для основного трэда. Хотя это можно, конечно,
[18:19.820 --> 18:27.940]  понимать. Есть еще одна маленькая особенность, что между стэками, которые стоят рядом,
[18:27.940 --> 18:37.540]  размещаются небольшие участки памяти минимально возможного размера. Минимально возможный
[18:37.540 --> 18:44.460]  адресуемый участок памяти, который может иметь какие-то обособленные атрибуты, это одна страница.
[18:44.460 --> 18:51.380]  Размер страницы в большинстве архитектур 4 килограмма, которые называются защитными страницами. Еще
[18:51.380 --> 19:01.620]  иногда их называют канарейками. Зачем нужны канарейки? Кто вообще слышал слово канарейка?
[19:01.620 --> 19:11.620]  Применительно койти. Причем здесь птичка. Птичка это при том, что канарейки существуют очень нежные,
[19:11.620 --> 19:18.740]  и поэтому они когда-то очень активно использовались в качестве расходного материала в угольных шахтах.
[19:18.740 --> 19:27.940]  Садят клетки с канарейками, если вдруг возникает загазованность, повышенный уровень метана,
[19:27.940 --> 19:37.580]  что происходит с птичками? Они отправляются куда-то там к своим праптичкам. И это хорошо заметно,
[19:37.580 --> 19:44.060]  поэтому это индикатор. То есть до тех пор, пока не изобрели электронные газоанализаторы,
[19:44.060 --> 19:50.620]  очень активно в шахтах использовали вот эти вот птички. Какое это отношение имеет койти?
[19:50.620 --> 19:59.820]  Имеет отношение следующее. У нас могут быть ситуации, когда переполнение стека, выход за
[19:59.820 --> 20:07.700]  границу чего-нибудь, случайно потерли нету память. И что мы можем сделать? Мы можем в определенной
[20:07.700 --> 20:15.260]  участке памяти неиспользуемый прописывать некоторые целочисленные значения. Например,
[20:15.260 --> 20:21.860]  заранее известные случайные значения, либо посчитанный хэш для определенной структуры.
[20:21.860 --> 20:27.660]  В общем, какие-то осмысленные данные небольшого размера. Для того, чтобы периодически проверять
[20:27.660 --> 20:36.500]  целостные данные. И в случае, если данные перестали быть целостными, считать, что что-то у нас пошло не так,
[20:36.500 --> 20:47.820]  и целостность программы нарушена. Так вот, в ядре реализована поддержка механизма,
[20:47.820 --> 20:55.980]  похожего немножко на канарейки, только немножко более жесткой ситуации. Между стеками выделяется
[20:55.980 --> 21:01.900]  небольшая область памяти размером с одной страницей, которая недоступна вообще. То есть,
[21:01.900 --> 21:08.380]  область памяти для стека имеет атрибуты страниц доступ на чтение и доступ на запись,
[21:08.380 --> 21:16.220]  а защитные страницы не имеют вообще никаких прав. То есть, любая попытка прочитать данные,
[21:16.220 --> 21:21.380]  записать данные, либо пытаться их выполнить, приведет к ошибке нарушение сегментации.
[21:21.380 --> 21:28.180]  Для чего это нужно? После ситуации, что у вас какая-то функция, запущенная в одном из потоков,
[21:28.180 --> 21:35.860]  начинает рекурсивно сама себя вызывать. Что у вас произойдет? Stack Overflow – известная ошибка.
[21:35.860 --> 21:43.100]  Если у вас программа однопоточная, ну, казалось бы, Stack Overflow, да, вы просто добрались до границы
[21:43.100 --> 21:49.220]  стека, все, вас прибили. В случае, если у вас программа многопоточная, то ситуация переполнения
[21:49.220 --> 21:56.740]  стека может привести к неопределенному поведению, потому что вы можете добраться не до того,
[21:56.740 --> 22:03.900]  что стек закончился, а задеть память другого потока. И если система на это никак не отреагирует,
[22:03.900 --> 22:11.220]  то что будет происходить с другим потоком, память, которую вы попортили – это загадка природы.
[22:11.220 --> 22:20.660]  И защитная страница – это некоторый простейший и фактически бесплатный механизм для того,
[22:20.660 --> 22:26.340]  чтобы преотвращать ситуацию с переполнением стека. Понятно, что это все-таки не полноценная такая
[22:26.340 --> 22:34.780]  панацея защита. Можно придумать ситуацию, когда вы просто объявляете какой-то огромный массив на
[22:34.780 --> 22:42.580]  стеке, дальше обращаетесь к очень большим индексам этого массива, который просто задевает
[22:42.580 --> 22:47.860]  память другого потока, но не попадает на защитную страницу, и такой механизм, такая канарейка,
[22:47.860 --> 22:57.220]  вас не спасет. Вот размеры стеков, какие у вас возможны. Стек не бывает до него размер. Максимальный
[22:57.220 --> 23:05.100]  его размер – 8 мегабайт. Не максимальный, а стандартный – 8 мегабайт. Можно делать 32. Граница
[23:05.100 --> 23:11.860]  снизу – она достаточно четко обозначена. В линуксе 116 384 байта. Никого не смущает,
[23:11.860 --> 23:24.940]  что такое странное число. Это почти 16 килобайт. То есть примерно 4 страницы памяти за вычетом 128
[23:24.940 --> 23:36.100]  мегабайт, которые для архитектуры x8664 считаются гарантированной зоной, куда не будет попадать
[23:36.100 --> 23:45.780]  стек обработчика 7. В отличие от стека, защитную страницу можно все-таки полностью убрать,
[23:45.780 --> 23:56.420]  сделать их на его размер. Давайте подытожим. У нас есть процесс. В рамках процесса вы запускаете
[23:56.420 --> 24:05.860]  какое-то количество трэдов. Они же потоки, они же нити, они же просто трэды. И все это у нас работает
[24:05.860 --> 24:13.260]  либо на одном процессоре, либо на нескольких процессорах. И между разными задачами планировщик
[24:13.260 --> 24:19.660]  заданий периодически переключается. То есть у нас есть приоритеты как у процессов, так и у всех
[24:19.660 --> 24:31.460]  трэдов, которые в рамках этого процесса выполняются. И если вам не нужно делать осмысленных действий,
[24:31.460 --> 24:37.220]  например, вы ждете, пока появится что-нибудь в другом трейде. Вы можете принудительно передать
[24:37.220 --> 24:43.460]  управление другому трэду с помощью системы вызова Shared Yield. Вы уже, наверное, сталкивались с ним раньше,
[24:43.460 --> 24:50.460]  когда писали что-то для процессов, для того чтобы передать управление другому процессу. Так вот,
[24:50.460 --> 24:55.980]  на самом деле, Shared Yield работает не только между процессами, а между задачами в целом. А это могут
[24:55.980 --> 25:04.380]  быть как процессы, так и трэды. Естественно, трэдов может стать очень много. Планировщику задача
[25:04.380 --> 25:14.100]  поплохеет, система будет сильно нагружена, и надо как-то уметь это лимитировать. Если количество
[25:14.100 --> 25:21.500]  процессов мы худобедно умеем лимитировать с помощью команды Ulimit, то с трэдами, на самом
[25:21.500 --> 25:29.100]  деле, примерно все то же самое. Но тут есть одна очень важная особенность. Есть некоторая путаница,
[25:29.100 --> 25:34.540]  которая нигде не прописана. Не в мане на Ulimit, хотя в мане может, конечно, и прописана,
[25:34.540 --> 25:44.340]  но в Help точно нет. Что именно он ограничен? Ulimit позволяет установить ограничение на максимальное
[25:44.340 --> 25:50.540]  количество процессов для одного пользователя в текущий момент времени. По крайней мере,
[25:50.540 --> 25:56.660]  так написано во всех классических UNIX учетах. На самом деле, это не количество процессов,
[25:56.660 --> 26:04.660]  а суммарное количество трэдов во всех процессах. Для однопоточных программ у нас есть строгое
[26:04.660 --> 26:09.980]  соответствие, что один процесс – это один трэд, но, опять же, внутри процесса у некоторых программ
[26:09.980 --> 26:16.260]  могут быть несколько трэдов, их может быть много. Более современный способ как-то ограничивать
[26:16.260 --> 26:22.820]  количество трэдов – это использовать контрольные группы. И если первая версия, если группа была
[26:23.580 --> 26:30.700]  достаточно замороченная, то вторая достаточно приятная уже. Там есть ограничения немножко разные,
[26:30.700 --> 26:40.460]  они могут быть как на количество процесс ID, так можно и прописать отдельное максимальное
[26:40.460 --> 26:49.140]  количество задач. И с C-групп, опять же, вы можете более тонко настраивать поведение вашей программы,
[26:49.140 --> 26:54.620]  какие у вас лимиты могут быть, по умолчанию. Так, вообще кто-нибудь помнит, что это контрольная
[26:54.620 --> 27:01.940]  группа C-групп? Это такая древовидная файловая система, где процессы объединяются в группы,
[27:01.940 --> 27:10.260]  внутри каждого процесса есть в файловой системе куча файликов, куда можно прописывать разные
[27:10.260 --> 27:18.580]  ограничения. Когда вы создаете дочерние процессы, они находятся в той же самой группе и могут при
[27:18.580 --> 27:24.140]  этом создавать подгруппы, объединяться. В общем, появляется такая иерархическая система ресурсов.
[27:24.140 --> 27:33.340]  Так вот, по умолчанию трэды находятся все в одной группе того процесса, в котором они работают,
[27:33.340 --> 27:39.780]  но это можно перенастроить файлик, C-групп type, прописать трэвис вместо думаем, и так вы можете
[27:39.780 --> 27:46.780]  на каждый трэд создавать отдельное поддерево файловой системе и для каждого трэда ID прописывать
[27:46.780 --> 27:51.620]  свои отдельные ограничения по памяти, по использованию процессов времени и так далее.
[27:51.620 --> 28:04.100]  Процессы это что-то, что полностью изолировано друг от друга, в то же время трэды работают в
[28:04.100 --> 28:10.620]  едином адресном пространстве. Вот и процессы, и трэды часто используют как механизм для того,
[28:10.620 --> 28:16.660]  чтобы распараллелить выполнение какой-то задачи. Например, у вас есть огромный кусок данных,
[28:16.780 --> 28:22.020]  зачем нам может наладиться несколько трэдов или несколько процессов? Просто для того,
[28:22.020 --> 28:27.340]  чтобы использовать полноценно ваш процессор, в котором меньше двух ядер,
[28:27.340 --> 28:34.100]  но вряд ли вы уже где-то встретите, даже в мобильных телефонов. И для того, чтобы задействовать все ядра
[28:34.100 --> 28:39.460]  или все логические процессоры, вам нужно программу раскладать либо на процессы,
[28:39.460 --> 28:45.660]  либо на трэды, в общем, на разные задачи. Вот с процессами их назначение основное это понятно,
[28:45.660 --> 28:49.740]  это запуск отдельных программ, которые не мешают, но с точки зрения распараллеливания
[28:49.740 --> 28:57.660]  они уже становятся неудобными. Потому что если у вас под задачи не являются полностью независимыми
[28:57.660 --> 29:03.340]  друг от друга, то между ними нужно как-то организовывать взаимодействие. И есть разные
[29:03.340 --> 29:09.500]  способы межпроцессного взаимодействия, разделяемая память, пайпы и все остальное. Но опять же,
[29:09.500 --> 29:15.460]  это лишние затраты, как с точки зрения труда-пограммиста, особенно по части отладки,
[29:15.460 --> 29:21.260]  потому что отлаживать и бажить межпроцессное взаимодействие – это удовольствие неисприятных.
[29:21.260 --> 29:30.500]  Во-вторых, все это взаимодействие обычно еще требует участия ядра, и это уже дополнительно
[29:30.500 --> 29:36.780]  к вам не расходит. В то же время трэды работают, поскольку в едином адресном пространстве вы можете
[29:36.780 --> 29:44.060]  творить все что угодно. Это происходит достаточно дешево, но, опять же, это не дается совсем
[29:44.060 --> 29:51.300]  бесплатно. И если какой-нибудь хотя бы один тред в рамках процесса сделает что-нибудь нехорошее,
[29:51.300 --> 29:58.740]  попытается разыменовать, например, нулевой указатель, что у нас произойдет? У нас грохнется
[29:58.740 --> 30:05.220]  просто весь процесс телекон. Как этот механизм работает? Что значит процесс грохнется,
[30:05.220 --> 30:11.100]  когда попытается разыменовать нулевой указатель? Знаете, что ядро пошлет ему сигнал нарушению
[30:11.100 --> 30:17.380]  сегментации. Этот сигнал придет не конкретному процессу, не конкретному потоку, он придет
[30:17.380 --> 30:29.380]  всему процессу, ну и результат предсказуем. Так, что есть общего у всех трэдов? Поскольку они
[30:29.380 --> 30:36.860]  работают в рамках одного процесса, у трэдов общий process ID, parent ID, у всех трэдов общий
[30:36.860 --> 30:43.300]  текущий каталог. То есть если вы напишите две функции, которые работают в разных потоках,
[30:43.300 --> 30:52.460]  и какая одна из функций делает change directory, а другая функция пытается найти файл по относительному
[30:52.460 --> 30:58.460]  имени, а не по абсолютному, то ничего хорошего из этого очевидно не выйдет. Файлы в дескрипторе
[30:58.460 --> 31:03.740]  тоже общие, то есть вы можете открыть какой-то файл в дескриптор в одном файле, в одном трэде,
[31:03.740 --> 31:11.420]  использовать в другом. Этот эффект, например, может быть использован для того, чтобы создать либо
[31:11.420 --> 31:19.500]  pipe, либо socket pair, для того чтобы организовать какое-то последовательное общение данными
[31:19.500 --> 31:27.260]  между разными потоками. В то же время у каждого трэда есть по аналогии с process ID свой уникальный
[31:27.260 --> 31:34.340]  thread ID, это целое число, свой стэк и, как ни странно, есть еще свой отдельный стэк для обработки сигналов.
[31:34.340 --> 31:44.180]  Зачем это нужно? Ну потому что к нам, когда прилетает сигнал, он обрабатывается все-таки не в каком-то
[31:44.180 --> 31:51.980]  там абстрактном месте, он обрабатывается одним из трэдов. И тут уже возникают некоторые проблемы,
[31:51.980 --> 32:00.860]  связанные с тем, что весь EPA UNIX-систем проектировался еще тогда, когда не было
[32:00.860 --> 32:07.100]  никаких многоядерных процессоров. В частности, одна из проблем, это глобальная переменная верну,
[32:07.100 --> 32:15.180]  которая хранит код ошибки последней неуспешной выполненной операции. Большинство системных
[32:15.180 --> 32:19.780]  вызов позикс возвращает просто значение минус один как признак ошибки, а что именно за ошибка,
[32:19.780 --> 32:25.940]  это как раз верну. Так вот, на самом деле, это никакая не глобальная переменная. Если посмотреть на
[32:25.940 --> 32:38.340]  реализацию из Gleap C, что такое верно, это просто оформленный в виде переменной, чтобы без скобочек
[32:38.340 --> 32:43.260]  можно было вызывать макрос, который просто вызывает некоторую внутреннюю функцию,
[32:43.260 --> 32:51.660]  которая, в свою очередь, для каждого из трэдов возвращает свое уникальное значение кода ошибки.
[32:51.660 --> 32:57.580]  Потому что трэды выполняются независимо друг от друга, могут дополнять какие-то системные вызовы,
[32:57.580 --> 33:07.820]  и ошибки могут быть разными в разных трэдах. Как создаются потоки? Одним из двух способов. Есть
[33:07.820 --> 33:19.540]  белая сторона, это позикс. Причем неважно, мак, линукс, vzd. Там API для работы с трэдами одинаковый.
[33:19.540 --> 33:25.860]  Называется позикс трэдс. Поэтому называется функция, вот вы спрашивали, чтобы p означает,
[33:25.860 --> 33:32.740]  это как раз слово позикс. Есть еще и темная сторона, это кое-где позикс трэдс не поддерживается,
[33:32.740 --> 33:40.660]  там нужно использовать всякие функции вида createthread, подключать файл заголовочной окошки.h,
[33:40.660 --> 33:52.980]  в общем, не будем это обсуждать. Винопи это вообще столько всего интересного. С одной стороны,
[33:52.980 --> 34:00.460]  казалось бы, оно было логичным в 90-е годы, и можно понять логику, но сейчас можно почитать,
[34:00.460 --> 34:15.540]  зайти на docs.microsoft.org и посмеяться. Про это я сейчас чуть позже скажу, сигналами там отдельнее
[34:15.540 --> 34:24.260]  заморышка. Насчет позикс. Когда я показывал вам, как скомпилировать код, я сделал уговорку,
[34:24.260 --> 34:32.100]  что я это компилирую на маке, и по-честному под линукс надо писать еще одну опцию. Тут есть одна
[34:32.100 --> 34:38.780]  особенность, что под линукс есть API для работы с позикс, он реализован в отдельной библиотеке,
[34:38.780 --> 34:48.140]  которая называется list-thread.iso, валяется где-нибудь там в slashlib, либо slashlib, еще куча там префиксов,
[34:48.140 --> 34:54.980]  типа i386 там подкатал. В общем, ищется поиском в разных дистрибутивах по-разному. Это библиотека,
[34:54.980 --> 35:03.060]  которая содержит все функции для работы с потоками, но и не только с потоками, а также с разными
[35:03.060 --> 35:11.340]  примитивами для синхронизации, например, CYMOFOG и MUTUX. И list-thread.create это на самом деле никакой
[35:11.340 --> 35:18.540]  не системный вызов, это всего лишь функция оболочка, которая вызывает системный вызов
[35:18.540 --> 35:25.940]  clone с кучей разных аргументов. Вот слово clone вы уже, наверное, стучали, например, в контрольной,
[35:25.940 --> 35:35.580]  в конце пришлось и местный. Тогда система вызов clone отображался, использовался как
[35:35.580 --> 35:40.020]  системный вызов для создания новых процессов. На самом деле он универсальный, у него куча
[35:40.020 --> 35:48.820]  параметров, можно создавать не только новые процессы, но и новые thread. Поэтому, если вы,
[35:48.820 --> 35:56.740]  например, исследуете поведение вашей программы с помощью команды estrace, которая отображает список
[35:56.740 --> 36:00.900]  системных вызовов, которые выполняются в вашей программе, не удивляйтесь, если вы часто будете
[36:00.900 --> 36:08.940]  встречать это слово, потому что clone используется не только аналог форка, но и если у вас создаются
[36:08.940 --> 36:17.980]  треды, то косвенно вызывается из функции piss.redKey. Еще одна особенность Linux, связанная с тем,
[36:17.980 --> 36:27.900]  что функциональность работы с потоками реализована в user space, а не в ядре, это сигналы
[36:27.900 --> 36:37.180]  реального времени. А именно, я как-то уже говорил, что в Linux константа Sigr2bin начинается с 34,
[36:37.180 --> 36:47.580]  а не с 32, как в других UNIX-системах. Хотя сигналы с номерами 32 и 33, они также являются полноценными
[36:47.580 --> 36:54.940]  сигналами реального времени. Из-за чего такое происходит? Потому что два сигнала реального времени
[36:54.940 --> 37:01.700]  используются как раз библиотекой piss.red для внутренней координации потоков между собой,
[37:01.700 --> 37:12.740]  например, для отправки запроса на стану. У нас есть библиотека в Linux, и в macOS либо в FreeBSD
[37:12.740 --> 37:18.620]  эту библиотеку не нужно подключать, хотя насчет FreeBSD я уже не помню, в Mac точно не нужно.
[37:18.620 --> 37:26.020]  Казалось бы под Linux, как у нас линкуется все с библиотекой. Minus L маленькая, и название библиотеки
[37:26.020 --> 37:34.660]  без префиксолит, без уфикса.iso. При этом чаще используется немножко другая запись, просто
[37:34.660 --> 37:40.020]  минус piss.red, это отдельный флаг специальный для компиляторов RSC и SILAN, который означает,
[37:40.020 --> 37:46.780]  что в некоторых архитектурах, например, Linux, в некоторых операционных системах Linux нужно
[37:46.780 --> 37:51.820]  отдельно линковать библиотеку Pozix threads, в некоторых других операционных системах, например,
[37:51.820 --> 38:02.500]  macOS. Этого делать не нужно. Это некоторые опциональные параметры. Если абстрагироваться от
[38:02.500 --> 38:09.900]  Pozix API и все-таки думать о том, что у нас бывает система под названием Windows, не только
[38:09.900 --> 38:17.060]  Pozix системы, и под них как-то тоже хочется писать многопроточные приложения, то тут у нас есть
[38:17.060 --> 38:27.620]  стандарты языков C и C++, и, например, в C++, начиная с стандарта 11 года, появился такой стандартный
[38:27.620 --> 38:33.300]  класс SDS thread, который вы все знаете, там в конструктор можно передать функцию плюс параметры,
[38:33.300 --> 38:40.980]  и именно в момент вызова конструктора, странная логика, конечно, но тем не менее, у вас начинает
[38:40.980 --> 38:47.340]  выполняться новый thread. И эта штука работает везде одинаково на всех платформах, где гарантируется
[38:47.340 --> 38:55.380]  поддержка стандарта C++ 11 и выше. Почему вы не используете всегда просто эту функциональность?
[38:55.380 --> 39:03.780]  Зачем вам нужно знать какие-то там Pozix threads и все остальное? Потому что язык C++ у нас изобилует
[39:03.780 --> 39:11.700]  всякими абстракциями. И что такое thread на языке C++? Это просто некоторая абстракция, чтобы удобно,
[39:11.700 --> 39:18.580]  легко и просто запускать новые задачи, но при этом запускать одинаково хорошо на разных
[39:18.580 --> 39:24.220]  операционных системах, и поэтому функционально это содержит только самый минимум, который
[39:24.220 --> 39:30.460]  поддерживается всеми операционными системами. То есть, если используя Pozix threads, вы можете
[39:30.460 --> 39:41.780]  тонко настроить, например, размер стека, размер guard page, то на плюсах вы это сделать в принципе не
[39:41.780 --> 39:49.020]  можете. У вас есть только дефолтный std thread, и все, ничего тут вы не поделаете. Есть еще язык C,
[39:49.020 --> 39:56.860]  и в стандарте языка C 11 года тоже появился класс, ну не класс, точнее, а модуль для работы
[39:56.860 --> 40:06.740]  и стредами. И что удивительно, сейчас уже 2022 год наступил, стандарты 19 года. До сих пор в компиляторах
[40:06.740 --> 40:12.580]  эта функциональность не реализована. Одной простой причине, она никому не нужна. Когда у вас есть
[40:12.580 --> 40:22.780]  Pozix threads, зачем нам еще придумать какой-то новый язык? Так, и в потоках вы можете делать все
[40:22.780 --> 40:30.340]  что угодно, они равнозначные, но когда у вас программа многопоточная, иногда надо думать не
[40:30.340 --> 40:35.980]  только о синхронизации, но и о том, что нужно аккуратно пользоваться какими-то другими
[40:35.980 --> 40:42.700]  системными вызовами, которые тоже что-то порождают. В частности, есть известная проблема, связанная
[40:42.700 --> 40:52.420]  Форком. А именно, что делает Форк? Он создает копию процесса, но что именно считать копией?
[40:52.420 --> 41:01.340]  Он создает копию адресного пространства, и вот когда проектировали систему Unix в 70-е годы,
[41:01.340 --> 41:06.620]  опять же, не задумывались о том, что когда-нибудь появится поток. Поэтому Форк, после того как он
[41:06.620 --> 41:15.260]  скопирует виртуальное адресное пространство, начинает выполнение новой задачи, одной задачи,
[41:15.260 --> 41:23.340]  даже если в исходном процессе у вас было много тредов. Что при этом происходит? Мы скопировали
[41:23.340 --> 41:29.740]  память всех тредов, процесс как бы находится в том же замороженном состоянии, и текущий тред,
[41:29.740 --> 41:39.020]  который вызвал Форк, продолжает выполняться, но тут могут возникнуть какие проблемы. Например,
[41:39.020 --> 41:47.460]  у вас могут быть какие-то мьютексы, либо семафоры, которые уже были залочены в предыдущем
[41:47.460 --> 41:56.220]  процессе каким-то другим потоком, и в новом процессе они никогда не будут разблокированы,
[41:56.220 --> 42:02.180]  потому что других потоков просто нет. Осталась их память и всё, но не более того.
[42:02.180 --> 42:14.300]  Да, после Форка работает только один тред, который его вызвал. Какая тут ещё возможна проблема,
[42:14.300 --> 42:19.220]  которая приводит к deadlock? Например, у вас могут быть какие-то пайпы, socket payers,
[42:19.220 --> 42:27.340]  каналы, если говорить про язык Go, из которых вы тоже можете попытаться что-то прочитать после
[42:27.340 --> 42:38.260]  Форка, но опять же никто вам не запишет, вы получите deadlock. Например, вы могли сделать пайп,
[42:38.260 --> 42:45.940]  использовать пайп между двумя тредами, и соответственно после Форка, то есть в нормальной
[42:45.940 --> 42:53.300]  истории использования пайпов, есть два треда, один читает, другой пишет. Вы из того треда,
[42:53.300 --> 42:58.940]  который занимается чтением, сделали Форк, потом пытаетесь прочитать что-то из пайпа,
[42:58.940 --> 43:06.140]  никто вам уже ничего не пишет, и вы получите deadlock. Такие ситуации могут проводить к deadlock,
[43:06.140 --> 43:13.220]  и на самом деле к этим deadlock могут проводить в весьма неочевидных ситуациях, особенно когда
[43:13.220 --> 43:20.140]  кодовая база у вас большая, особенно если это плюсы с кучей абстракций. Например, вы можете
[43:20.140 --> 43:26.900]  вызывать какой-то метод банальный getter какого-то класса, ни о чем не подозревая, что внутри этого
[43:26.900 --> 43:32.540]  класса у вас может быть попытка заблокировать какой-нибудь mutex, который был уже заблокирован.
[43:32.540 --> 43:38.820]  Поэтому что можно безопасно делать после Форка? Вот самое безопасное после Форка сразу же сделать
[43:38.820 --> 43:46.260]  exit. Тогда мы просто грохнем все адресное пространство, которое у нас было, забудем про все
[43:46.260 --> 43:53.700]  блокировки и заменим наш процесс на какую-то другую программу. Вот, кстати, по этой причине,
[43:53.700 --> 44:04.100]  да, есть разные языки программирования хорошие, Python, Golang, Dart, в конце концов, и у многих
[44:04.100 --> 44:08.140]  современных языков программирования есть какой-нибудь модуль в стандартной библиотеке,
[44:08.140 --> 44:15.820]  который содержит функции для работы с операционной системой почти в полном составе,
[44:15.820 --> 44:23.620]  как это присмотрен стандартом POSIX. Так вот, нигде вы не найдете аналогов в ускоренных языках
[44:23.620 --> 44:28.780]  программирования. Просто по одной причине, что если у вас есть какой-то runtime какого-то языка
[44:28.780 --> 44:34.860]  программирования, то, скорее всего, там будет как-то задействована многопоточность под что-нибудь,
[44:34.860 --> 44:43.300]  и использование Форка это опасно и чревато всякими хорошими Zlock. И вместо Форка обычно есть
[44:43.300 --> 44:50.740]  что-нибудь типа запустить новую программу с кучей параметров по Windows. И, кстати, не только
[44:50.740 --> 44:56.300]  другим языкам программирования, но и к разным фреймворкам на C и на plus. Теперь немножко про
[44:56.300 --> 45:05.100]  сигналы. Вот, как я уже говорил, у каждого трэда есть свой стэк для обработки сигналов. И что у нас
[45:05.100 --> 45:14.860]  происходит, когда прилетает сигнал? У каждого процесса есть маска сигналов, ожидающих доставки,
[45:14.860 --> 45:21.620]  у каждого процесса, но не у трэда. В то же время есть еще такое понятие, как маска сигналов,
[45:21.620 --> 45:26.860]  то есть, фактически, это некоторый фильтр, который блокирует доставку определенных сигналов,
[45:26.860 --> 45:33.220]  которые нам могут послать. Так вот, в отличие от маски сигналов, ожидающих доставки, маски
[45:33.220 --> 45:41.300]  сигналов, которые выполняют роль фильтрации, они могут быть разными у разных трэда. И что происходит,
[45:41.300 --> 45:49.740]  когда к нам прилетает какой-то сигнал, который никакой маской не перехвачен? Этот сигнал
[45:49.740 --> 45:58.220]  начинает обрабатываться в этом процессе и в каком-то из трэдах. А вот кому повезло,
[45:58.220 --> 46:04.220]  кто-то начал его выполнять. То есть, как у нас работает обработка сигналов на низком уровне?
[46:04.220 --> 46:10.620]  Планирующий к заданию выбирает очередной процесс. Внутри процесса есть какой-нибудь текущий поток,
[46:10.620 --> 46:18.260]  который нужно выбрать. Он его выбирает, дальше смотрит на маску сигналов, ожидающие доставки.
[46:18.260 --> 46:25.380]  Если у нас есть сигналы не обработанные, то в том же трэде, который планирующий к заданию выбрал,
[46:25.380 --> 46:31.580]  начинается обработка доработчика сигналов. В общем случае, в каком из трэдов у вас будет
[46:31.580 --> 46:39.980]  выполняться, это не предсказуемо. Тем не менее, вы на каждый трэд можете навешивать свою маску
[46:39.980 --> 46:47.740]  сигналов, и это бывает полезно для того, чтобы, например, в своем приложении с огромной логикой
[46:47.740 --> 46:54.540]  разрушить, выделить просто отдельный трэд, который будет заниматься только обработкой сигналов и
[46:54.540 --> 46:59.500]  больше ничем. Как это сделать? На все трэды навешивать маски сигналов, которые все блокируют,
[46:59.500 --> 47:09.900]  на один из трэдов навесить маску, которая разрешает и ставку сигналов. До этого трэда
[47:09.900 --> 47:13.500]  все равно будет доходить исполнение, и поскольку он единственный, кто может принимать сигналы,
[47:13.500 --> 47:19.260]  то он и будет обрабатывать все сигналы. Не будет мешать при этом другим трэдам.
[47:19.260 --> 47:27.260]  Вот трэды у нас можно запустить. В какой-то момент трэды заканчиваются, могут выполнять
[47:27.260 --> 47:34.740]  какие-то побочные действия, хулиганить, мешать друг другу. И по-хорошему вообще любой запущенный
[47:34.740 --> 47:39.740]  трэд самостоятельно должен завершиться. То есть нужно предусмотреть в своей многопоточенной
[47:39.740 --> 47:48.380]  программе какой-то легальный способ, чтобы дать трэду это возможное. Что будет, если все-таки нам
[47:48.380 --> 47:54.860]  нужно принудительно прибить какой-то трэд, который уже выполняется. Например, сделать аналог кил.
[47:54.860 --> 48:02.660]  Системный вызов кил или команда кил, она прибивает процесс целиком. И процесс, например, который стал
[48:02.660 --> 48:10.980]  всем мешать, он больше никому не мешает. Что-то похожее есть и у трэдов. Вы можете не всегда,
[48:10.980 --> 48:18.180]  конечно, достаточно часто остановить трэд, хотя это плохо. Но представьте себе, что у вас есть
[48:18.180 --> 48:24.700]  многоядерный процессор. И одно ядро процессора начинает тупо выполнять инструкции, джамп,
[48:24.820 --> 48:33.420]  что-нибудь. Все. Как вы можете его остановить? Можно ли это сделать?
[48:36.700 --> 48:43.740]  Но только выполнить какое-то прерывание. Поэтому ситуация с остановкой какой-то произвольного
[48:43.740 --> 48:49.060]  трэда, она вообще говоря не гарантирует, что как только вы нажавали на красную кнопку,
[48:49.060 --> 48:57.260]  трэд тут же завершится. Трэды могут завершаться в одном из двух случаях. Я имею ввиду, что завершаться
[48:57.260 --> 49:04.740]  не добровольно. Либо они наступили на какую-то точку под названием точка останова. Вот если
[49:04.740 --> 49:16.300]  открыть седьмой раздел Man в Linux, в Mac его кажется нет, то который описывает всю технологию связанную
[49:16.300 --> 49:26.860]  с Pozix Threads. Тут есть интересный список функций. Фактически это почти все системные вызовы,
[49:26.860 --> 49:35.140]  которые называются cancellation points. Как у нас вообще работает cancellation points? Фактически у нас какой-то
[49:35.140 --> 49:42.780]  поток начинает выполнять какой-нибудь системный вызов. И первым делом, что происходит, проверяется,
[49:42.780 --> 49:51.180]  нужно ли вообще дальше этот поток выполнять или нет. И если не нужно выполнять, поток тут завершает
[49:51.180 --> 49:57.420]  свою работу. Может быть ситуация, когда поток просто что-то делает и никогда не вызывает
[49:57.420 --> 50:02.700]  системный вызов. Это более сложная ситуация. Тут поток остановить уже сложнее. То есть мы не
[50:02.700 --> 50:10.060]  можем другому ядру процессора просто приказать перестать выполнять этот поток. Здесь можно
[50:10.060 --> 50:18.900]  прервать только сигналом, поскольку у нас есть аппаратное прервание по таймеру, которое периодически
[50:18.900 --> 50:25.420]  получает процессор. И соответственно перед тем, как продолжить выполнение, нужно проверить,
[50:25.420 --> 50:32.220]  а действительно нужно выполнять. Фактически у нас механизм сигналов так работает. И второй
[50:32.220 --> 50:38.580]  способ как раз позволяет, хотя опять же не мгновенно, все-таки поток остановить. То есть,
[50:38.580 --> 50:48.540]  если вы хотите остановить поток, есть два способа. Способ по умолчанию. Это дать возможность потоку
[50:48.540 --> 50:54.340]  остановиться на некоторый cancellation point. Либо, если поток разрешил, да, есть два системных вызова.
[50:54.340 --> 51:00.740]  Один указывает, можно ли поток завершать или нельзя. Второй, каким образом это работает либо
[51:00.740 --> 51:06.140]  через точки останова, либо принудительно по умолчанию работать через точки останова. Все-таки это
[51:06.140 --> 51:10.220]  можно сделать для того, чтобы иметь возможность, например, отменить какую-то длительную операцию.
[51:10.220 --> 51:16.780]  Открывайте фотошоп и файлы на 4 гигабайта. Что у вас происходит? Открытие. Вы можете
[51:16.780 --> 51:22.260]  комментировать cancel, если вам надоело ждать. Хотя лучше это сделать совершенно по-другому.
[51:22.260 --> 51:33.260]  Да, cancel это принудительная остановка. Лучше это делать все-таки непринудительно. Например,
[51:33.380 --> 51:39.740]  иметь какой-то boolean flag, который из вашего потока отдельного, этот флаг, например, проверяется.
[51:39.740 --> 51:48.380]  Почему очень плохо принудительно завершать работу какого-то потока? Потому что у вас могут
[51:48.380 --> 51:53.380]  остаться какие-то не освобожденные ресурсы. И это может быть не только утечки памяти.
[51:53.380 --> 52:01.940]  Более страшно, что при многопоточном программировании вы наверняка будете
[52:01.940 --> 52:06.020]  активно использовать какие-то объекты синхронизации, мьютексы и симмофоры.
[52:06.020 --> 52:12.860]  И если утечка памяти хорошо имеет свойство накапливаться, хорошо расходовать память,
[52:12.860 --> 52:20.660]  тем более что курс доллара у нас резко подскочил и оперативка подорожала. С мьютексами и
[52:20.660 --> 52:27.460]  симмофорами тут еще сложнее. Потому что если вы не освободите ресурс, связанный с блокировкой
[52:27.460 --> 52:35.900]  какого-то участка памяти, то вы можете застопорить все остальные треки. И первое правило, надо забыть
[52:35.900 --> 52:41.620]  про принудительную остановку. Это крайне аварийный случай. Но все-таки, если вам приходится к нему
[52:41.620 --> 52:49.700]  прибегать, можно зарегистрировать в каждом треде обработчик, функция обработчик, который будет
[52:49.700 --> 52:55.460]  выполняться при остановке треда и, например, принудительно освобождать все ресурсы,
[52:56.660 --> 53:00.900]  разблокировать все мьютексы, закрывать ненужные файловые дескрипторы, ну и так далее.
[53:00.900 --> 53:08.620]  Так, что еще надо знать про многопоточные программиеры? У нас бывают всякие разные
[53:08.620 --> 53:16.340]  функции, которые могут быть thread-safe, то есть потока безопасными, могут быть функции async
[53:16.340 --> 53:21.860]  signal-safe, это функции, которые можно использовать в обработчиках сигналов. И вообще говоря,
[53:21.860 --> 53:31.260]  эти два класса функций у нас не совпадают. Классический пример. Функция print, put, scan и так
[53:31.260 --> 53:37.460]  далее. То есть функции, которые взаимодействуют с какими-то глобальными объектами, например,
[53:37.460 --> 53:47.300]  буфер водо-вывода, ну стандартно водо-вывода, sdout, sdv и sdv. Какие здесь возможные проблемы?
[53:47.300 --> 53:54.860]  Функции форматного ввода, вообще стандартные функции водо-вывода в CE, они являются потока
[53:54.860 --> 54:00.780]  безопасными. Почему они сделаны в thread-safe без всякой реализации дополнительной? Ну потому,
[54:00.780 --> 54:07.020]  что было очень много кода написано на старом Lego CC, и когда начали портировать на многопоточное
[54:07.020 --> 54:15.740]  окружение, то в самый простой способ. Почему в thread-safe функции не являются функциями,
[54:15.740 --> 54:19.820]  которые можно использовать в обработчиках сигналов? Потому, что они активно используют
[54:19.820 --> 54:27.020]  объекты блокировки для того, чтобы не было гонки данных за разделяемые ресурсы. А все-таки обработка
[54:27.020 --> 54:33.540]  сигналов у нас происходит не в отдельном потоке. И это чревато тем, что вы можете просто налететь
[54:33.540 --> 54:40.100]  на какой-нибудь deadlock из обработчика сигнала, если будете использовать thread-safe функции. Хотя
[54:40.100 --> 54:44.940]  чисто некоторые thread-safe функции, которые, например, просто не используют по глобальным
[54:44.940 --> 54:50.020]  состояниям и не приводят к проблеме гонки данных, их использовать из обработчиков сигнала можно
[54:50.020 --> 55:01.780]  вполне спокойно. Если сигнала безопасная, то тоже не факт. Хотя в большинстве случаев,
[55:02.260 --> 55:11.180]  асинхроны сигнала безопасные, они обычно являются потока безопасными, но я навряд ли сейчас
[55:11.180 --> 55:14.860]  смогу это доказать. Скорее просто я не смогу вспомнить примеров, когда бы это было не так.
[55:14.860 --> 55:23.620]  То есть системные вызовы, они как правило являются не только асинхронно безопасными,
[55:23.620 --> 55:32.100]  но и потоком безопасными. И вот есть еще некоторый способ локализовать какие-то глобальные
[55:32.100 --> 55:40.780]  переменные. Это ключевые слова thread-local C++ и C, просто некоторые дополнительные
[55:40.780 --> 55:47.740]  модификаторы перед именем переменной, которые фактически превращают какие-то глобальные
[55:47.740 --> 55:55.020]  переменные в локальные, но которые локальные с точки зрения потока. То есть они существуют
[55:55.020 --> 56:01.700]  по одному экземпляру на каждый thread, хотя обращаться к ним можно из любого места программы. Просто
[56:01.700 --> 56:07.260]  некоторые другие удобные способы обращения. Но где реальной жизни в проде такое используется,
[56:07.260 --> 56:15.660]  опять же, я практически не встречал. Так, и вот тут мы уже начинаем вспоминать эту проблему,
[56:15.660 --> 56:21.340]  что у нас есть огромное количество старого нетрадсейв кода, который нужно как-то портировать
[56:21.340 --> 56:30.620]  хотя бы левой ногой для возможности выполнения на многопоточных системах. И почему вообще
[56:30.620 --> 56:36.700]  эта проблема возникла? Потому что, вообще говоря, самый первый процессор, который начал массово
[56:36.700 --> 56:42.700]  распространяться и при этом содержал больше одного ядра, появился только в 2006 году. На самом деле,
[56:42.700 --> 56:50.140]  до этого были многопроцессорные системы. Они появились давно, если говорить про
[56:50.140 --> 56:54.820]  микропроцессорные системы, но это еще в 80-е годы. Только они стоили жутко дорого,
[56:54.820 --> 57:00.660]  были очень узкоспециализированными и мало распространенными. И поэтому даже
[57:00.660 --> 57:06.820]  многозадачные системы, которые существовали в 90-е годы, в начале бытовых, они фактически
[57:06.820 --> 57:14.420]  использовали только один процессор, только одно ядро. И жесткой проблемы того, что у нас могут
[57:14.420 --> 57:20.020]  возникать какие-то блокировки, ее просто нет. Даже если они были, они были настолько маловероятными,
[57:20.020 --> 57:26.980]  просто потому что у вас некому конкурировать за какие-то ресурсы. Процессор один, у вас, да,
[57:26.980 --> 57:31.860]  много процессов. Внутри процессов можно создавать многопотоков, но все они все-таки выполнялись
[57:31.860 --> 57:38.900]  последним. Более того, даже когда появились двухпроцессорные, ну, двухядерные процессоры,
[57:38.900 --> 57:45.860]  то некоторый софт просто начал работать глючно, потому что он никогда до этого не тестировался в
[57:45.860 --> 57:53.380]  реальной поточной среде. Так вот, в 2006 году появляется массовый процессор Intel Core Duo. Все,
[57:53.380 --> 57:59.100]  везде все стало многоядерным, многопроцессорным, а в то же время код, который нормально
[57:59.100 --> 58:05.940]  проектировался под параллельное выполнение и реально был протестирован, его фактически отсутствовал,
[58:05.940 --> 58:13.220]  и много где ставили заплатки. Какую заплатку вы можете поставить, чтобы программа, которая была
[58:13.220 --> 58:18.980]  написана без поддержки многопоточности, вдруг стала безопасной для многопоточного исполнения.
[58:18.980 --> 58:24.220]  Ну, поднаставить везде мьютоксов и все. Вот так много где сделали, в том числе в стандартном
[58:24.220 --> 58:35.740]  себе библиотеке. Так, ну и к чему это привело? Ну, классическая проблема — язык Python, в котором
[58:35.740 --> 58:44.060]  есть такое понятие, как Global Interpreter Log. Откуда эта проблема вообще растет? Проблема корнями растет
[58:44.060 --> 58:52.500]  оттуда, что интерпретатор Python изначально проектировался не для многопроцессованного
[58:52.500 --> 58:59.260]  выполнения. Потом уже в нем появилась функциональность для поддержки многопоточности,
[58:59.260 --> 59:07.060]  но тем не менее у нас, да, что такое многопоточность в Python? Она там очень честная, то есть,
[59:07.060 --> 59:12.820]  действительно, когда вы создаете новый thread в Python, у вас честно создается новый stack,
[59:12.820 --> 59:19.820]  у вас честно создается новый thread ID, в общем, обычный перестраит create, который действительно
[59:19.820 --> 59:26.380]  отдельно выполняется на отдельном виде процессора, как отдельная задача, как все легально. В чем проблема?
[59:26.380 --> 59:32.540]  В том, что у нас есть ровно один экземпляр интерпретатора, который интерпретирует bytecode
[59:32.540 --> 59:38.020]  Python. То есть Python у нас сначала текст преобразует bytecode, хранит его в памяти, кэширует,
[59:38.020 --> 59:41.860]  потом вот bytecode выполняет. Так вот, интерпретатор существует ровно в одном экземпляре, у него
[59:41.860 --> 59:48.940]  единое глобальное состояние. И сколько у нас одновременно может выполняться тогда потоков?
[59:48.940 --> 59:55.380]  Естественно, одно. Один поток выполняется, выполняется только одна программа, даже несмотря на то,
[59:55.380 --> 01:00:01.100]  что порождено много остальных трэдов. Все остальные трэды просто тупо висят на Utex и ждут,
[01:00:01.100 --> 01:00:07.340]  пока Utex разблокируется, они смогут выполняться. И фактически многопоточная программа на Python
[01:00:07.340 --> 01:00:15.420]  превращается в однопоточный. На самом деле, здесь есть некоторые исключения, когда можно
[01:00:15.420 --> 01:00:25.020]  освободить Global Interpreter Log. Как правило, это что-то, что реализовано внутри сишных модулей на Python,
[01:00:25.020 --> 01:00:32.020]  либо на длительных операциях ввода-вывода. То есть есть небольшие оптимизации, что, например,
[01:00:32.020 --> 01:00:37.900]  когда вы читаете какие-то данные из файла и это длительный блокирующий ввод-вывод, то временно
[01:00:37.900 --> 01:00:43.260]  освобождается блокировка интерпретатора, но опять же это единичный случай. В целом, Python нельзя
[01:00:43.260 --> 01:00:49.820]  назвать многопоточным языком программируемого. Модуль Multi-Processing там существует скорее для того,
[01:00:49.820 --> 01:00:54.740]  чтобы учить студентов и школьников многопоточного программирования, не более того. Если вам
[01:00:54.740 --> 01:01:00.260]  действительно нужна параллельная обработка данных по Python, то есть похожий по программу
[01:01:00.260 --> 01:01:07.340]  интерфейс-модуль, который называется Multi-Processing, который честно форкает кучу новых процессов,
[01:01:07.340 --> 01:01:12.340]  организует межпроцессные взаимодействия, хотя делается, конечно, на Python. Тем не менее,
[01:01:12.340 --> 01:01:21.540]  хоть как-то оно запускается параллельно. Еще один известный программный продукт, который тоже
[01:01:21.540 --> 01:01:28.140]  исходно проектировался без поддержки многопоточности. Потом его все-таки запихали
[01:01:28.140 --> 01:01:33.260]  многопоточность. Это, как ни странно, ядро операционной системы. Причем не только Linux,
[01:01:33.260 --> 01:01:38.980]  многие операционные системы тут пострадали. Зачем вообще ядру многопоточность? Во-первых,
[01:01:38.980 --> 01:01:44.060]  вы можете внутри ядра отдельные подсистемы, особенно медленные, которые работают с медленными
[01:01:44.060 --> 01:01:52.940]  устройствами, выносить в обработку отдельными потоками. Вторая задача, зачем может потребоваться
[01:01:52.940 --> 01:01:59.260]  многопоточность внутри ядра, потому что у вас выполняются разные процессы, в результате процессов
[01:01:59.260 --> 01:02:07.140]  разные пользовательские потоки, и одновременно разным процессом может потребоваться системный
[01:02:07.140 --> 01:02:16.860]  вызов. Но процессы должны ждать, пока с ядром пообщается другой процесс. Зачем? Тоже можно
[01:02:16.860 --> 01:02:24.300]  сделать многопоточную обработку. И есть такая проблема, как большая блокировка ядра.
[01:02:24.300 --> 01:02:34.420]  В Genk.rln.ru еще иногда встречается тема Object-BigKernelLog. Очень страшная штука, потому что если у вас
[01:02:34.420 --> 01:02:41.980]  процессор одноядерный, то ладно, это не страшно, если у вас хотя бы два ядра, это сильная просадка
[01:02:41.980 --> 01:02:50.580]  производительства. В опенсорсных системах FreeBSD и Linux эту проблему решили еще 10 лет назад. Откуда
[01:02:50.580 --> 01:02:57.180]  вообще корни проблемой Genk.rln.ru появляются? Корни проблемы, как правило, лежат в каком-нибудь
[01:02:57.180 --> 01:03:01.740]  не очень качестве на написанном коде, который изобиливают использованием глобальных переменных,
[01:03:01.740 --> 01:03:08.500]  статических переменных и так далее. В основном это были всякие драйверы старинных, уже может быть
[01:03:08.500 --> 01:03:13.940]  даже неподдерживаемых устройств. То есть частично эти проблемы были решены переписыванием, частично
[01:03:13.940 --> 01:03:21.420]  выкидыванием лишнего. Тем не менее в Linux и FreeBSD этой проблемы уже нет. Насчет MacOS не знаю. Этой
[01:03:21.420 --> 01:03:27.340]  информации я не нашел нигде в открытой доступе. Самое последнее упоминание про какие-то проблемы
[01:03:27.340 --> 01:03:36.740]  с глобальной блокировкой ядра относится к OS Tiger 10.4. Ну это достаточно старое уже ядро. Возможно
[01:03:36.740 --> 01:03:43.660]  просто Apple старается про это не рассказывать, если какие-то проблемы действительно есть.
[01:03:43.660 --> 01:03:54.540]  На самом деле непонятно зачем, учитывая, что если в open-source 2009-й год, казалось FreeBSD уже все
[01:03:54.540 --> 01:03:59.660]  решили, тут же проблемы даже не только в ядре операционной системы, а в разных подсистемах,
[01:03:59.660 --> 01:04:05.540]  там файловая система, еще что-нибудь. То есть если у вас есть хотя бы один модуль ядра,
[01:04:05.540 --> 01:04:11.780]  который немногопоточный, то он может стать слабым местом в производительности. Захватывайте
[01:04:11.780 --> 01:04:23.260]  не блокировки. И вообще говоря, если немножко вернуться в историю, то на десктопах многопроцессорные
[01:04:23.260 --> 01:04:31.900]  системы уже существовали. В 90-е годы был товарищ из компании Apple, который решил свалить из
[01:04:31.900 --> 01:04:39.300]  компании Apple и основал свою компанию под названием BIBOX, которая делала десктопные компьютеры с
[01:04:39.300 --> 01:04:47.340]  достаточно симпатичной операционной системой под названием BIOS. В чем особенности компов? Это
[01:04:47.340 --> 01:04:54.340]  были фактически первые доступные десктопные системы с двумя процессами, а не с одним,
[01:04:54.340 --> 01:05:00.820]  как тогда это было принято. И естественно операционная система уже by design исходно
[01:05:00.820 --> 01:05:08.740]  была спроектирована с прицелом на многопоточность. Но в какой-то момент ни компания BIBOX, ни
[01:05:08.740 --> 01:05:15.740]  операционная система BIOS уже не стала царствием небесное. Зато у нас есть open source, в котором
[01:05:15.740 --> 01:05:21.540]  много чего живет. И есть такая операционная система под названием Haiku, тоже достаточно симпатичная с точки
[01:05:21.540 --> 01:05:30.180]  зрения пользовательского интерфейса, которого является дальнейшим развитием идей BIOS и даже
[01:05:30.180 --> 01:05:34.900]  гарантирует совместимость со в том, этой самой BIOS хотя бы тому, он сейчас нужен,
[01:05:34.900 --> 01:05:41.820]  где исходной этой проблемы не было вообще с глобальной блокировкой ядра. И более того,
[01:05:41.820 --> 01:05:47.220]  если вы поставите себе виртуалку, например, операционную систему, чтобы поиграться,
[01:05:47.220 --> 01:05:55.700]  залезете в список процессов, то внутри каждого процесса, не только внутри каждого процесса,
[01:05:55.700 --> 01:06:02.860]  в самом ядре вы увидите огромное количество потоков, которые доступны отдельным процессам.
[01:06:02.860 --> 01:06:09.380]  Так, ну и на этой радостной ноте мы видим многопоточность от бота,
[01:06:09.380 --> 01:06:13.380]  бруда или от сбер. Это означает, что лекция закончилась.

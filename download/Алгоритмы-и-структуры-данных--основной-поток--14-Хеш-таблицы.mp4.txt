[00:00.000 --> 00:08.060]  У нас две лекции, мы поговорим про хэш таблицы и всякие
[00:08.060 --> 00:09.260]  такие приколы с ними.
[00:09.260 --> 00:22.780]  Вот, ну мы продолжаем жить в парадигме, что нам нужно
[00:22.780 --> 00:25.080]  придумать какую-то структуру данных, которая умеет означать
[00:25.080 --> 00:32.560]  на те же самые три запроса, insert, erase и find, но теперь
[00:32.560 --> 00:36.640]  в каком-то смысле мы отказываемся от требования на упорядоченность,
[00:36.640 --> 00:39.320]  потому что когда мы хранили дерево поиска, у нас в каком-то
[00:39.320 --> 00:43.560]  смысле легко было восстановить порядок тех элементов, которые
[00:43.560 --> 00:44.760]  в структуре лежат.
[00:44.760 --> 00:47.800]  Если мы храним дерево поиска, то мы знаем, где лежит минимум,
[00:47.800 --> 00:50.520]  это самая левая вершинка, если идти влево-влево, будет
[00:50.520 --> 00:51.520]  минимум.
[00:51.520 --> 00:54.200]  Потом там второе от него, это вторая порядка статистики
[00:54.200 --> 00:55.200]  и так далее и так далее.
[00:55.200 --> 00:58.120]  То есть на самом деле в дереве поиска всегда хранится
[00:58.120 --> 01:00.800]  порядок элементов, можно его оттуда легко извлечь
[01:00.800 --> 01:04.120]  линейным проходом, то есть за размер структуры можно
[01:04.120 --> 01:06.920]  вывести по факту отсортированную версию текущего множества.
[01:06.920 --> 01:11.000]  В hash таблицах у нас такое свойство пропадает, но
[01:11.000 --> 01:13.720]  зато появляется огромное преимущество, что все запросы
[01:13.720 --> 01:18.720]  работают, ну грубо говоря, за единицу.
[01:19.200 --> 01:26.200]  Мы хотим делать все те же инсерты, рейсы и файнды.
[01:28.280 --> 01:35.960]  Теперь все это будет амортизированная единица на запрос, и это будет
[01:35.960 --> 01:36.960]  в среднем.
[01:36.960 --> 01:41.880]  В среднем, то есть математическое ожидание времени ответа
[01:41.880 --> 01:45.760]  на запрос будет единичкой в том же смысле, как когда
[01:45.760 --> 01:52.760]  у нас это было про квиксорт, мы когда обсуждали, там,
[01:53.360 --> 01:55.920]  где n log n в среднем, что мы там что-то случайный пивот
[01:55.920 --> 01:59.240]  взяли, разделили и пошли в обе части рекурсивно,
[01:59.240 --> 02:02.080]  вот здесь то же самое, мат ожидания будет единичная.
[02:02.080 --> 02:06.960]  То есть в среднем все работает хорошо, но иногда в каких-то
[02:06.960 --> 02:09.640]  очень выраженных случаях что-нибудь вырождается
[02:09.640 --> 02:12.880]  и может работать долго, и тогда в идеале надо что-то
[02:12.880 --> 02:13.880]  перестраивать.
[02:14.400 --> 02:21.000]  Ну вот, представьте, что у нас явным образом описано
[02:21.000 --> 02:23.960]  множество из которого черпаются ключи.
[02:23.960 --> 02:30.960]  У это какое-то универсум, и все ключи, все элементы
[02:30.960 --> 02:34.640]  которые надо в это множество добавлять, которые мы храним,
[02:34.640 --> 02:40.280]  мы же множество, а С храним, все ключи, которые в С играют,
[02:40.280 --> 02:42.040]  они все черпаются из у.
[02:42.040 --> 02:45.040]  Ну мы там знаем какую-то структуру этого у, грубо говоря.
[02:45.040 --> 02:53.440]  Хочется сделать следующее, хочется в каком-то смысле
[02:53.440 --> 02:56.840]  все элементы этого у типа пронумеровать желательно
[02:56.840 --> 03:00.280]  маленькими целыми числами и просто завести массив
[03:00.280 --> 03:02.960]  такого размера, какой размер вот этого перенумерованного
[03:02.960 --> 03:03.960]  множества.
[03:03.960 --> 03:05.320]  Ну представьте, пусть у меня есть большой универсум
[03:05.320 --> 03:09.680]  у, но нам откуда-то заранее известно, скажем, что интересующий
[03:09.840 --> 03:11.480]  у меня числа, те, которые будут поступать в запросах,
[03:11.480 --> 03:14.480]  вот эти вот х и у, они будут только из какого-то маленького
[03:14.480 --> 03:15.480]  подможества.
[03:15.480 --> 03:17.240]  Ну понятно, если у вас чисел много, а запросов мало,
[03:17.240 --> 03:20.440]  то так или иначе интересующие вас числа это какое-то маленькое
[03:20.440 --> 03:21.440]  подможество.
[03:21.440 --> 03:26.120]  Ну если бы они все вам были известны заранее, если
[03:26.120 --> 03:30.120]  за просы были известны заранее, то наверное понятно,
[03:30.120 --> 03:31.120]  как поступить.
[03:31.120 --> 03:34.440]  Можно просто все прочитать, запомнить все элементы,
[03:34.440 --> 03:36.520]  которые были, как их отсортировать и пронумеровать.
[03:36.520 --> 03:38.520]  Ну то есть тогда у меня о каждого элемента будет
[03:38.520 --> 03:39.520]  номер.
[03:39.520 --> 03:43.720]  будет там от 1 до n, где n количество запросов.
[03:43.720 --> 03:46.360]  Ну и тогда можно просто массив длины n ввести,
[03:46.360 --> 03:48.840]  и про каждый элемент, по его номеру, понимать, есть
[03:48.840 --> 03:49.840]  он там или нет.
[03:49.840 --> 03:50.840]  Понятно?
[03:50.840 --> 03:56.440]  Ну, это как бы тут много проблем, во-первых, как минимум
[03:56.440 --> 03:59.120]  то, что мне нужно, чтобы все запросы были известны
[03:59.120 --> 04:00.120]  заранее.
[04:00.120 --> 04:04.360]  Но идея такая, что каждому элементу мы в каком-то смысле
[04:04.360 --> 04:10.680]  мы сопоставляем его номер, номер XA, она у нас будет
[04:10.680 --> 04:11.680]  играть.
[04:11.680 --> 04:12.680]  Вот.
[04:12.680 --> 04:16.840]  А именно мы будем вот этот номер вычислять с помощью
[04:16.840 --> 04:20.200]  какой-то функции h, х-функции.
[04:20.200 --> 04:29.240]  Вот, сегодня будет немножко вероятности.
[04:29.240 --> 04:34.240]  Представьте себе, что функция h бралась бы полностью случайно.
[04:34.320 --> 04:36.200]  То есть, ну представьте, вот у меня h, это какая-нибудь
[04:36.200 --> 04:44.240]  функция из u, скажем, в целые числа от 0 до и минус 1.
[04:44.240 --> 04:46.320]  Пусть она случайна, вот прям полностью случайная
[04:46.320 --> 04:47.320]  функция.
[04:47.320 --> 04:48.320]  Что значит случайная функция?
[04:48.320 --> 04:52.040]  Это значит, что на каждом элементе u у меня значение
[04:52.040 --> 04:55.880]  h в этой точке случайно и равномерно распределено
[04:55.880 --> 04:56.880]  по этому множеству.
[04:56.880 --> 04:58.880]  То есть вероятность попасть в каждое конкретное число
[04:58.880 --> 05:00.720]  одна и та же, один делить на m.
[05:00.800 --> 05:04.520]  Ну а значит, если h случайна, давайте напишу полностью
[05:04.520 --> 05:09.120]  случайно, прям честно, абсолютно честно, случайно, полностью
[05:09.120 --> 05:14.600]  случайно, то вероятность по, собственно, случайной
[05:14.600 --> 05:20.360]  h, того, что h от x равно какому-нибудь i, она была бы в точности
[05:20.360 --> 05:21.360]  1mt.
[05:21.360 --> 05:24.880]  Значит, давайте напишем, что для любого х из u, для
[05:24.880 --> 05:28.440]  любого i, ну вот целого числа из этого диапазона.
[05:29.360 --> 05:31.360]  0d минус 1.
[05:35.360 --> 05:38.800]  Ну, обычная такая дискретная вероятность, у меня есть
[05:38.800 --> 05:42.080]  там пространство всех функций, я вот из них выбираю одну
[05:42.080 --> 05:44.280]  случайно равную вероятность, среди всех возможных функций
[05:44.280 --> 05:46.200]  выбираю одну наугад.
[05:46.200 --> 05:49.200]  Тогда понятно, что вероятность этой случайной функции
[05:49.200 --> 05:52.000]  в конкретной точке быть конкретному числу равной,
[05:52.000 --> 05:54.160]  это в точности 1mt, потому что в этой точке она какое-то
[05:54.160 --> 05:57.560]  значение принимает, и среди всех возможных m она принимает
[05:57.560 --> 06:01.480]  ровно 1 и равновероятно, все вероятности по всем
[06:01.480 --> 06:04.880]  i должны быть равны, но раз у меня всего m образов, значит
[06:04.880 --> 06:05.880]  вероятность 1mt.
[06:05.880 --> 06:07.880]  Понятно, никого не шокирует.
[06:07.880 --> 06:10.880]  Хорошо.
[06:10.880 --> 06:14.400]  Ну вот представьте у нас такой идеальный мир, допустим,
[06:14.400 --> 06:19.800]  мы вот эту процедуру извлечения номера по x реализуем с
[06:19.800 --> 06:22.360]  помощью х-функции h, полностью случайной функции h.
[06:22.360 --> 06:29.480]  Тогда было бы довольно неплохо, потому что мы
[06:29.480 --> 06:33.240]  опять-таки реализовали вот эту вот процедуру перенумирования
[06:33.240 --> 06:35.840]  всех элементов универсума, так чтобы они все поместились
[06:35.840 --> 06:39.720]  в какое-то небольшое множество размера m, ну и тогда мы могли
[06:39.720 --> 06:42.120]  бы делать просто следующее, когда мне приходит какой-то
[06:42.120 --> 06:44.800]  элемент x, я сначала вычисляю его х-функцию, то есть его
[06:44.800 --> 06:48.280]  номер в этом массиве, и дальше, например, кладу его в ту
[06:48.520 --> 06:49.520]  ячейку, которая посчиталась.
[06:49.520 --> 06:54.520]  У нас же всегда есть функция для конкретного элемента,
[06:54.520 --> 06:57.520]  конкретная, одно и то же извлечение.
[06:57.520 --> 06:58.520]  Да.
[06:58.520 --> 07:01.640]  Я считаю, что у меня там один раз когда-то h вначале
[07:01.640 --> 07:04.480]  сфиксировалось, я ее случайно сгенерировал, но теперь функция
[07:04.480 --> 07:06.840]  она в каждой конкретной точке, сколько раз я ее не считаю
[07:06.840 --> 07:09.040]  в одной точке, она считает одно и то же.
[07:09.040 --> 07:11.760]  То есть это детерминированная функция, которая в начале
[07:11.760 --> 07:13.080]  как-то случайно сгенерировалась.
[07:13.080 --> 07:19.320]  Ну вот представьте, пусть у меня там пришел какой-то
[07:19.320 --> 07:25.880]  x, я завел массив длины m, пришел x, я посчитал его
[07:25.880 --> 07:29.120]  значение h от x, посмотрел на ячейку с этим номером
[07:29.120 --> 07:30.120]  и сюда его положил.
[07:30.120 --> 07:32.760]  Ну хорошо, понятно, вот он здесь лежит, все.
[07:32.760 --> 07:34.840]  Дальше пришел какой-то y, мне нужно опять его положить
[07:34.840 --> 07:35.840]  в мою структуру.
[07:35.840 --> 07:39.520]  Я беру, считаю, скажем, h от y, это какой-то новый элемент,
[07:39.520 --> 07:42.720]  я кладу y вот сюда, в эту ячейку массива, по номеру
[07:42.720 --> 07:45.040]  собственно h значения и так далее.
[07:45.040 --> 07:49.280]  Вот, ну какая бывает проблема?
[07:49.280 --> 07:51.800]  Ну проблема бывает в том, что иногда происходят коллизии,
[07:51.800 --> 07:54.600]  то есть когда h значение одинаковое, потому что когда я беру
[07:54.600 --> 07:57.920]  случайную функцию и считаю ее значение, скажем, в разных
[07:57.920 --> 08:01.600]  каких-то точках a и b, то у меня вполне возможно
[08:01.600 --> 08:03.600]  h от a равно h от b.
[08:03.600 --> 08:05.680]  Ну вот эта ситуация называется коллизией.
[08:05.680 --> 08:10.760]  Ну в случае, когда a и b различны, значит, называется коллизия.
[08:10.760 --> 08:15.520]  И тогда, если, скажем, у меня приходит запрос insert
[08:15.520 --> 08:20.760]  a, insert b, то что делать, сходу непонятно, потому что они
[08:20.760 --> 08:23.200]  должны лежать в какой-то одной и той же ячейке, вот
[08:23.200 --> 08:27.080]  этой вот, скажем, такой, что h от a равно h от b, вот в ячейке
[08:27.080 --> 08:29.000]  с таким номером они должны оба как бы лежать.
[08:29.000 --> 08:34.320]  Вот, ну есть ли у вас предложение, что можно в таком случае
[08:34.320 --> 08:35.320]  сделать?
[08:35.320 --> 08:39.440]  Вот если они обе должны лежать здесь, да, ровно так
[08:39.440 --> 08:40.440]  мы и сделаем.
[08:40.440 --> 08:42.920]  Просто возьмем и вместо того, чтобы хранить одно
[08:42.920 --> 08:47.520]  число в этом элементе, будем здесь хранить просто
[08:47.520 --> 08:51.400]  список тех чисел, которые ровно этим хэшом обладают.
[08:51.400 --> 08:53.760]  То есть вместо того, чтобы у меня было здесь одно число,
[08:53.760 --> 08:59.000]  я просто здесь заведу односвязанную, кажется, достаточно односвязанную
[08:59.000 --> 09:03.080]  списку, положу сюда a, положу сюда b, ну и так далее.
[09:03.080 --> 09:04.720]  Каждое новое число, которое будет поступать, я буду
[09:04.720 --> 09:06.040]  просто подвешивать в этот список.
[09:06.040 --> 09:11.960]  Вот, ну примерно так работает хэштаблица.
[09:11.960 --> 09:18.480]  Если h от x полностью случайно и верно лишь, то у меня всегда
[09:18.480 --> 09:21.960]  для конкретного значения будет одно и то же возвращаться,
[09:21.960 --> 09:22.960]  это ведь важно?
[09:22.960 --> 09:23.960]  Да.
[09:23.960 --> 09:24.960]  Да, будет.
[09:24.960 --> 09:25.960]  И да, верно.
[09:25.960 --> 09:29.720]  Ну то есть представляете себе это все следующим
[09:29.720 --> 09:30.720]  образом.
[09:30.720 --> 09:33.720]  Мы сначала до выполнения всех запросов сгенерировали
[09:33.720 --> 09:34.720]  h.
[09:34.800 --> 09:37.160]  В каком-то смысле мы обратились к каракулу, сказали, дай
[09:37.160 --> 09:38.160]  мне случайную функцию.
[09:38.160 --> 09:39.160]  Вот он дал.
[09:39.160 --> 09:41.400]  Вот какая-то функция, написан код какой-то функции h.
[09:41.400 --> 09:44.240]  Дальше, если вы в нее одно и то же подставляете, дальше
[09:44.240 --> 09:45.640]  функция уже детерминирована.
[09:45.640 --> 09:48.440]  Если вы в нее одно и то же много раз подаете, она возвращается
[09:48.440 --> 09:49.440]  одно и то же.
[09:49.440 --> 09:52.360]  В этом смысле, если вы многократно у вас х приходит,
[09:52.360 --> 09:55.440]  insert, а потом find к х, то вы будете обращаться к одной
[09:55.440 --> 09:58.040]  и той же чеке массива, потому что h уже детерминирована.
[09:58.040 --> 10:01.560]  Если вы ее заранее сгенерировали, то теперь уже h от x одно и
[10:01.560 --> 10:02.560]  то же будет.
[10:02.560 --> 10:03.560]  И тогда проблем не будет.
[10:03.560 --> 10:09.120]  Ну и, смотрите, давайте, например, посмотрим вероятность
[10:09.120 --> 10:10.120]  коллизии.
[10:10.120 --> 10:12.640]  Пусть мы фиксируем какие-то a и b, давайте зафиксируем
[10:12.640 --> 10:16.600]  какие-то два элемента a и b из u, давайте, например,
[10:16.600 --> 10:20.920]  посчитаем вероятность коллизии на этой паре элементов.
[10:20.920 --> 10:26.600]  Да, будет ровно 1 мт, абсолютно верно.
[10:26.600 --> 10:33.320]  Потому что, смотрите, у нас функция h случайна, и в каждой
[10:33.320 --> 10:35.400]  точке она принимает свое значение независимо от
[10:35.400 --> 10:36.400]  остальных.
[10:36.400 --> 10:40.920]  То есть значение h от a и h от b – это независимые
[10:40.920 --> 10:44.520]  реализации одной и той же случайной величины с равномерным
[10:44.520 --> 10:46.320]  распределением вот в этом множестве.
[10:46.320 --> 10:48.320]  То есть все значения принимаются равновероятно.
[10:48.320 --> 10:52.520]  Ну, можно это так написать, если хочется.
[10:52.520 --> 10:57.160]  Давайте просто переберем значение h от a.
[10:57.160 --> 11:01.560]  Ну да, можно так записать.
[11:01.560 --> 11:09.200]  Значение по i, вероятность того, что h от a равно i, равно
[11:09.200 --> 11:10.200]  h от b.
[11:10.200 --> 11:15.200]  То есть я вот это вот событие равенства вероятностей
[11:15.200 --> 11:19.120]  разбиваю на дезинктное объединение.
[11:19.120 --> 11:21.200]  То есть у меня понятно, если h от a равно h от b, то оно равно
[11:21.200 --> 11:23.960]  какому-то конкретному i, давайте я по всем возможным
[11:23.960 --> 11:24.960]  и это просуммирую.
[11:24.960 --> 11:31.000]  Ну а такое событие, если у меня h от a и h от b – независимые
[11:31.000 --> 11:33.960]  величины, я говорю, что в каждой конкретной точке
[11:33.960 --> 11:37.400]  у меня значение h определяется независимо от остальных.
[11:37.400 --> 11:40.000]  Тогда с какой вероятностью у меня и здесь выпала i, и
[11:40.000 --> 11:41.000]  здесь выпала i?
[11:41.000 --> 11:44.400]  Ну, значит, вот это выпадает значением i с вероятностью
[11:44.400 --> 11:47.720]  1mt, и это тоже с вероятностью 1mt.
[11:47.720 --> 11:50.120]  Поэтому эта сумма расписана следующим образом.
[11:50.120 --> 11:56.920]  Это просто 1m2, потому что чтобы это выполнилось мне
[11:56.920 --> 12:00.400]  нужно, чтобы и это было i, что происходит с вероятностью
[12:00.400 --> 12:03.200]  1mt, и это было i, что происходит с вероятностью 1mt, потому
[12:03.200 --> 12:04.680]  что все независимо и равновероятно.
[12:04.680 --> 12:08.000]  Ну а это как раз 1mt в точности.
[12:08.000 --> 12:19.280]  Ну я формально расписал.
[12:19.280 --> 12:24.680]  То, что вы говорите, на самом деле я ровно это и записал,
[12:24.680 --> 12:28.880]  просто как бы формулками скажем так.
[12:28.880 --> 12:29.880]  Вообще правда, конечно, да.
[12:29.880 --> 12:35.720]  Вот, значит, если бы мы брали случайную h, то в принципе
[12:35.720 --> 12:38.360]  у меня коллизии довольно невероятны.
[12:38.360 --> 12:41.560]  Если m достаточно большое, ну там не знаю, 10 в 5, грубо
[12:41.560 --> 12:43.880]  говоря, то это как бы не очень большая вероятность.
[12:43.880 --> 12:46.880]  Даже если она выпала на каких-то конкретных элементах,
[12:46.880 --> 12:48.720]  то, наверное, длина вот этого списка будет не очень
[12:48.720 --> 12:49.720]  большая.
[12:49.720 --> 12:51.360]  Наверное, не очень много элементов будут попадать
[12:51.360 --> 12:52.640]  в один и тот же список много раз.
[12:52.640 --> 12:55.960]  Вот, ну хорошо.
[12:56.960 --> 12:57.960]  Вопрос.
[12:57.960 --> 13:01.680]  Можем ли мы вот прям вот это закодить и этим пользоваться
[13:01.680 --> 13:02.680]  в реальной программе?
[13:02.680 --> 13:07.800]  Смотря на сколько устойчивого вы хотите программовать.
[13:07.800 --> 13:10.320]  Вот прям ровно это, вот то, что я описал, ровно это
[13:10.320 --> 13:11.920]  я и хочу записать.
[13:11.920 --> 13:13.800]  Просто можно ли это физически закодить?
[13:13.800 --> 13:16.920]  Абсолютно разобавную функцию нельзя закодить.
[13:16.920 --> 13:17.920]  Да.
[13:17.920 --> 13:20.560]  Значит, чтобы сгенерировать случайную h, то есть я здесь
[13:20.560 --> 13:24.580]  живу в предположении, что я как-то умудрился сгенерировать
[13:24.580 --> 13:27.900]  вот такую функцию, которая на каждой точке принимает
[13:27.900 --> 13:29.820]  независимый индекс, независимое значение.
[13:29.820 --> 13:33.380]  Ну, это можно, конечно, ее можно просэмплировать,
[13:33.380 --> 13:35.860]  ее можно сгенерировать на каждом элементе универсума,
[13:35.860 --> 13:38.460]  как там одно из вот этих чисел, и все это сохранить.
[13:38.460 --> 13:41.980]  То есть более-менее единственное, что здесь можно сделать,
[13:41.980 --> 13:42.980]  это изначально.
[13:42.980 --> 13:44.560]  То есть как вот эту h сгенерировать?
[13:44.560 --> 13:47.360]  Мы проходимся по всем элементам универсума, на каждом элементе
[13:47.360 --> 13:50.860]  подбрасываем нужную монетку и генерируем случайное
[13:50.860 --> 13:52.060]  число из этого диапазона.
[13:52.180 --> 13:54.820]  Тем самым мы для каждого х запоминаем значение h от
[13:54.820 --> 13:55.820]  х.
[13:55.820 --> 13:58.580]  Но это не очень хорошо, потому что у меня память
[13:58.580 --> 13:59.580]  уже будет вот такая.
[13:59.580 --> 14:02.820]  Если у меня такая память, то зачем мне вообще это сжимать?
[14:02.820 --> 14:05.980]  Я мог бы просто хранить массив вот такого вот размера
[14:05.980 --> 14:08.900]  и просто для каждого элемента универсума помечать, был
[14:08.900 --> 14:09.900]  он или нет.
[14:09.900 --> 14:13.540]  Поэтому как бы вот столько памяти нам точно не позволительно
[14:13.540 --> 14:14.540]  хранить.
[14:14.540 --> 14:18.980]  И более-менее никак по-другому случайную функцию не реализовать.
[14:18.980 --> 14:21.860]  Настоящую случайную функцию, и именно вам нужно, чтобы,
[14:21.860 --> 14:23.780]  если и много раз одну и ту же скороблю, чтобы она
[14:23.780 --> 14:26.780]  возвращала одинаковое значение, более-менее никак по-другому
[14:26.780 --> 14:29.780]  это не сделать.
[14:29.780 --> 14:32.980]  Ну поэтому вот этого идеального мира мы переходим к более-менее
[14:32.980 --> 14:36.780]  реалистичному миру, которым на самом деле всегда будем
[14:36.780 --> 14:37.780]  пользоваться.
[14:37.780 --> 14:38.780]  Ну давайте скажем определение.
[14:38.780 --> 14:47.060]  Пусть, давайте напишу h красивое, это какое-то семейство
[14:47.060 --> 14:52.660]  хэш-функций, занумерованных, ну пусть будет элементами
[14:52.660 --> 15:03.660]  s из u в… Так, ну если я вот это вот обозначу за ZMT, никто
[15:03.660 --> 15:04.660]  не испугается же.
[15:04.660 --> 15:12.740]  Ну, у меня есть какое-то семейство функций, каждая
[15:12.740 --> 15:17.140]  функция, каждая h-эстая бьет из u в целые числа от 0
[15:17.140 --> 15:20.020]  до ими минус 1, и все они индексированы какими-то
[15:20.020 --> 15:21.020]  s-ками.
[15:21.020 --> 15:23.180]  Ну вот пока не буду говорить штуку s, это просто какая-то
[15:23.180 --> 15:25.420]  характеризация, характеристика функций.
[15:25.420 --> 15:30.740]  Вот, значит, пусть эта штука семейство функций, семейство
[15:30.740 --> 15:42.380]  функций, значит, тогда h называется универсальным
[15:42.380 --> 15:59.060]  семейством хэш-функций, если… Ну, давайте я напишу
[15:59.820 --> 16:02.980]  обычное определение, значит, вероятность по… для любых
[16:02.980 --> 16:06.100]  различных элементов, опять для любых различных x и y
[16:06.100 --> 16:12.220]  из u, вероятность по выбору случайного s, вероятности
[16:12.220 --> 16:20.100]  коллизии в этих двух точках, не больше 1 mt.
[16:20.100 --> 16:26.820]  Вот, ну поскольку у меня теперь уже как бы h это не
[16:26.860 --> 16:29.780]  полностью случайная функция, а функция, характеризующаяся
[16:29.780 --> 16:32.300]  каким-то индексом s, то у меня теперь будет вероятность
[16:32.300 --> 16:33.300]  по s.
[16:33.300 --> 16:35.740]  Ну, я иногда буду так писать, иногда буду опять писать
[16:35.740 --> 16:36.740]  просто ph-то.
[16:36.740 --> 16:39.820]  То есть здесь вероятность по s, которыми проиндексированы
[16:39.820 --> 16:41.820]  все вот эти вот функции, хэш-функции.
[16:41.820 --> 16:43.940]  Ну, я требую, собственно, то же самое, что мне нужно
[16:43.940 --> 16:47.740]  было вот в этом равномерном… вот в этом идеальном случае,
[16:47.740 --> 16:50.580]  что на любых двух элементах вероятность коллизии маленькая.
[16:50.580 --> 16:54.380]  Вот ровные-то условия, ну даже усиленные, что здесь
[16:54.380 --> 16:55.380]  меньше либо равно.
[16:56.380 --> 16:59.380]  Вот, хорошо.
[16:59.380 --> 17:02.380]  Ну, это называется универсальное семейство.
[17:02.380 --> 17:04.860]  Нам на самом деле везде для приложений будет достаточно
[17:04.860 --> 17:08.380]  слабо универсальности, это когда здесь стоит не единица,
[17:08.380 --> 17:09.380]  а какая-то константа.
[17:09.380 --> 17:12.620]  Ну, понятно, от того, что у меня как бы здесь на константу
[17:12.620 --> 17:14.900]  испортится, у меня, грубо говоря, все списки вырастут
[17:14.900 --> 17:16.700]  в константу раз и ничего не сломается с точки зрения
[17:16.700 --> 17:17.700]  симпотики.
[17:17.700 --> 17:22.460]  Поэтому давайте я здесь сразу допишу слабо универсальное,
[17:22.460 --> 17:23.940]  и здесь допишу какая-то c.
[17:23.940 --> 17:27.020]  Если существует там какая-то константа c, такая, что вот
[17:27.020 --> 17:28.020]  это вот верно.
[17:28.020 --> 17:30.700]  Ну там, не знаю, 10, 15, что-то такое, не очень большая константа.
[17:32.700 --> 17:34.700]  Вот.
[17:38.700 --> 17:40.700]  Ну, хорошо.
[17:42.700 --> 17:46.100]  Значит, тогда давайте считать, что мы будем жить в этом
[17:46.100 --> 17:47.100]  мире.
[17:47.100 --> 17:50.060]  То есть по факту самое главное, что я хотел от вот этого
[17:50.060 --> 17:52.740]  идеального мира, это вот это свойство, что коллизии
[17:52.740 --> 17:53.740]  маловероятны.
[17:54.540 --> 17:55.540]  Ну, давайте это потребуем.
[17:55.540 --> 17:59.260]  Это условие более слабое, чем условие на настоящие
[17:59.260 --> 18:02.060]  равномерные функции, случайные из всего множества функций.
[18:02.060 --> 18:03.060]  Да?
[18:03.060 --> 18:04.060]  Вот.
[18:04.060 --> 18:05.060]  Мне достаточно какого-то семейства.
[18:05.060 --> 18:06.060]  Очень хорошо.
[18:06.060 --> 18:10.700]  Значит, тогда давайте построим, собственно, х-таблицу с
[18:10.700 --> 18:11.700]  цепочками.
[18:11.700 --> 18:14.140]  Х-таблица с цепочками.
[18:14.140 --> 18:19.420]  Значит, она реализуется следующим образом.
[18:20.100 --> 18:28.820]  Ну, во-первых, мы, так, да, ну там изначально мы говорим,
[18:28.820 --> 18:32.860]  что м какой-нибудь, не знаю, 8 или 16, ну какое-нибудь маленькое
[18:32.860 --> 18:33.860]  число.
[18:33.860 --> 18:36.740]  И генерируем случайное h-s-t.
[18:36.740 --> 18:41.220]  Генерируем случайное, давайте я просто буду писать h, понимая,
[18:41.220 --> 18:44.980]  что вот они из того семейства, из универсального семейства.
[18:44.980 --> 18:56.460]  Вот, значит, будем хранить переменную n, пусть n, это всегда
[18:56.460 --> 18:58.220]  количество элементов в таблице.
[18:58.220 --> 19:04.620]  Количество элементов в таблице, ну, изначально ноль.
[19:04.620 --> 19:14.380]  Вот, ну и, кажется, последнее обозначение альфа, это
[19:14.380 --> 19:19.820]  отношение n делить на m, это так называемый load-фактор,
[19:19.820 --> 19:22.060]  ну, соответственно, фактор загруженности, да, показатель
[19:22.060 --> 19:25.820]  загруженности, степень загруженности, степень загруженности.
[19:25.820 --> 19:34.340]  То есть, сколько у меня элементов множестве, деленное
[19:34.340 --> 19:37.540]  на размер х-таблицы, на то, сколько ячеек у меня в массиве.
[19:37.540 --> 19:41.300]  Вот, ну и мы хотим, чтобы эта штука была всегда не
[19:41.300 --> 19:43.500]  очень большая, ну, потому что понятно, если элементов
[19:43.540 --> 19:45.660]  сильно больше, чем размер х-таблицы, то там будут
[19:45.660 --> 19:48.980]  длинные коллизии, там просто по принципу Дерехлей обязательно
[19:48.980 --> 19:52.220]  будут, скажем, два числа, лежащие в одном списке, ну
[19:52.220 --> 19:54.380]  и там, чем больше это отношение, тем больше чисел будет в
[19:54.380 --> 19:55.380]  каждом списке.
[19:55.380 --> 19:56.660]  Вот я хочу, чтобы альфа была всегда небольшая.
[19:56.660 --> 20:00.780]  Вот, ну тогда, значит, давайте реализуем всякие наши
[20:00.780 --> 20:01.780]  штуки.
[20:01.780 --> 20:04.260]  Значит, во-первых, как делать find, find x.
[20:04.260 --> 20:08.900]  А, ну и, так, пардон, я это не проговорил, но давайте
[20:08.900 --> 20:12.940]  запишем, что у меня хранится массив односвязанных списков
[20:12.940 --> 20:14.900]  в количестве m штук.
[20:14.900 --> 20:24.500]  Значит, храним, давайте назову l, массив односвязанных
[20:24.500 --> 20:25.500]  списков.
[20:25.500 --> 20:26.500]  Ау?
[20:26.500 --> 20:31.500]  А что значит односвязанных?
[20:31.500 --> 20:34.820]  Ну изначально мне же нужно какого-то размера создать
[20:34.820 --> 20:35.820]  х-таблицу.
[20:35.820 --> 20:38.140]  Ну, давайте делать такую, это как в векторе, типа,
[20:38.140 --> 20:40.260]  когда вы заводите пустой вектор, там же какая-то
[20:40.260 --> 20:41.260]  память выделяется.
[20:41.260 --> 20:44.140]  Ну там вот, там сколько, пусть будет 16, это неважно.
[20:44.140 --> 20:46.700]  Значит, массив односвязанных списков.
[20:46.700 --> 20:51.940]  Вот, и теперь как обрабатываться операцией.
[20:51.940 --> 20:59.580]  Ну, во-первых, find, понятно, надо посчитать х же значение,
[20:59.580 --> 21:04.980]  h от x, и проверить, есть ли x в том самом списке, в котором
[21:04.980 --> 21:05.980]  он должен быть.
[21:06.820 --> 21:21.100]  Значит, пусть и, это h от x, проверяем, есть ли x в l-item.
[21:21.100 --> 21:27.940]  Вроде просто, если у него должен быть такой х, то он
[21:27.940 --> 21:30.980]  должен быть в этом списке, в l-item списке, просто проходимся
[21:30.980 --> 21:33.340]  за линию, по этому списку проверяем, он есть там или
[21:33.340 --> 21:34.340]  нет.
[21:34.340 --> 21:35.700]  Если есть, то он значит есть в структуре, если нет,
[21:36.420 --> 21:39.420]  то его больше нигде нет, можно сказать, что его нет.
[21:39.420 --> 21:51.140]  Insert, значит, то же самое, считаем х значение, ну и мне нужно
[21:51.140 --> 21:55.220]  этот элемент добавить в l-item список, в l-item.
[21:55.220 --> 22:00.420]  Вот, ну тут опять подробности, что делать в случае, когда
[22:00.420 --> 22:03.420]  x и так уже есть в структуре, ну давайте я буду считать,
[22:04.140 --> 22:05.860]  что таких запросов нет, что запросы хорошие, как бы
[22:05.860 --> 22:11.700]  если x было, то его добавлять не просят, ну так или иначе
[22:11.700 --> 22:13.740]  мы можем сначала запустить find и проверить, если он
[22:13.740 --> 22:16.060]  там был, то добавлять не надо, значит, поэтому я считаю,
[22:16.060 --> 22:20.620]  что x не было в нашем нольстве s, вот, ну тогда нужно просто
[22:20.620 --> 22:24.980]  x добавить в список l-item, но я сделаю следующее, я добавлю
[22:24.980 --> 22:34.380]  его не в конец, а в начало, добавить, добавим x в начало
[22:34.380 --> 22:41.180]  l-item, вот, почему в начало, с точки зрения теории это
[22:41.180 --> 22:44.420]  не особенно важно, потому что у меня все равно все списки
[22:44.420 --> 22:48.500]  на самом деле будут в среднем маленького размера, ну там
[22:48.500 --> 22:51.140]  константного размера на самом деле, и поэтому куда
[22:51.140 --> 22:53.500]  я добавляю этот x в список константного размера, более-менее
[22:53.500 --> 22:56.340]  не важно, можно хоть в конец, хоть в середину, куда угодно,
[22:56.340 --> 22:59.980]  но с точки зрения, во-первых, константы, потому что, ну
[22:59.980 --> 23:01.780]  если вы добавляете в начало, вам не нужно весь список
[23:01.780 --> 23:07.340]  проходить, это выгоднее, во-вторых, ну если мы, скажем, вернемся
[23:07.340 --> 23:10.620]  к чему-то похожему на сплей дерево, то у меня получится,
[23:10.620 --> 23:13.260]  что недавно пришедшие элементы как бы ближе всего находятся
[23:13.260 --> 23:17.340]  к началу списка, и если я недавно добавил x и потом
[23:17.340 --> 23:20.300]  спросил, есть ли он в множестве или нет, то он будет близко
[23:20.300 --> 23:23.060]  к началу списка, и я сэкономлю проход по всему списку, просто
[23:23.060 --> 23:27.940]  посмотрю на первый элемент, вот, это выгодно, но с точки
[23:27.940 --> 23:29.580]  зрения такой неосимпатической оптимизации.
[23:29.580 --> 23:40.900]  Так, значит, третий, ну и race, ну тоже понятно, надо просто
[23:40.900 --> 23:44.060]  в нужном списке его оттуда удалить, ну опять, пусть
[23:44.060 --> 23:50.540]  и это h от x, просто берем и удаляем x из элитого.
[23:50.540 --> 24:02.900]  Ну, понятно, вот, значит, ну, к сожалению, на этом закончить
[24:02.900 --> 24:08.060]  нельзя, потому что мне нужно поддерживать не очень большое
[24:08.060 --> 24:12.300]  значение load-фактора, потому что, как мы уже сказали, если
[24:12.420 --> 24:16.460]  альфа становится сильно большим, если у меня в таблице
[24:16.460 --> 24:19.900]  лежит слишком много элементов, чем, собственно, ее размерность,
[24:19.900 --> 24:23.020]  тогда у него будет много коллизий и будут там списки
[24:23.020 --> 24:25.700]  очень длинными, и будет все долго работать, ну, поэтому
[24:25.700 --> 24:31.080]  ведем следующую операцию rehash, значит, мы ее вызываем
[24:31.080 --> 24:33.060]  каждый раз, когда альфа превышает какой-нибудь
[24:33.060 --> 24:42.140]  небольшой порог, ну, например, скажем, альфа стала
[24:42.140 --> 24:46.180]  больше, чем 0,95, ну, что-то такое, тут, опять же, конкретная
[24:46.180 --> 24:48.580]  константа не важна, но на практике вот что-то такое
[24:48.580 --> 24:51.860]  выбирает, 0,95, 0,75, где-то вот в таких пределах она
[24:51.860 --> 24:56.220]  выбирается, вот, и когда у меня альфа перешла за
[24:56.220 --> 24:58.480]  этот порог, я просто делаю полный rehash, я полностью
[24:58.480 --> 25:01.380]  перестраиваю мою хэштаблицу, предварительно увеличив
[25:01.380 --> 25:08.240]  m в два раза, значит, я m увеличиваю в два раза и полностью
[25:08.240 --> 25:13.220]  перестраиваем всю хэштаблицу, в частности, выбираем новое
[25:13.220 --> 25:35.720]  hash, с новым hash, вот, тогда, значит, если я m увеличил
[25:35.720 --> 25:38.660]  в двое, тогда у меня альфа автоматически уменьшилась
[25:38.660 --> 25:42.060]  в двое, и я могу еще много операций делать, скажем,
[25:42.060 --> 25:44.620]  если у меня там пришло восемь элементов, то у меня
[25:44.620 --> 25:48.300]  стало альфа одна вторая, я в два раза увеличил m, альфа
[25:48.300 --> 25:50.940]  стала 0,25, я теперь могу еще восемь элементов добавить
[25:50.940 --> 26:00.940]  без перестраиваний, вот, кажется, все, значит, ну, понятно,
[26:00.940 --> 26:03.820]  поскольку у меня каждый раз m увеличивается в двое,
[26:03.820 --> 26:07.600]  то количество этих rehash и перестраиваний будет маленьким,
[26:07.600 --> 26:09.300]  потому что, еще раз, каждый раз таблица расширяется
[26:09.300 --> 26:12.040]  в два раза, даже несмотря на то, что у меня, то есть,
[26:12.040 --> 26:15.760]  формально, вот этот шаг работает за отn, потому что мне нужно
[26:15.760 --> 26:18.800]  все старые ключи из старой таблицы переложить в новую,
[26:18.800 --> 26:21.120]  это работает, ну, как минимум, за отn, мне нужно все ключи
[26:21.120 --> 26:25.800]  перебрать, переложить в новую таблицу, но поскольку
[26:25.800 --> 26:28.960]  это происходит экспоненциально редко, то есть, грубо говоря,
[26:28.960 --> 26:32.120]  сначала это было на восьмой операции, потом на шестнадцатый,
[26:32.120 --> 26:34.380]  на тридцать второй, шестнадцать четвертый и так далее,
[26:34.380 --> 26:36.780]  то, ну, там, как обычно, так же, как это в векторе было,
[26:36.780 --> 26:39.380]  когда мы перекладываем, когда мы увеличиваем размер
[26:39.380 --> 26:43.140]  вектора в двое и все перекладываем за линию, у меня это все суммируется
[26:43.140 --> 26:45.420]  и будет амортизирована единица.
[26:45.420 --> 26:50.180]  Вот теорема, да, ну, давайте без доказательства, потому
[26:50.180 --> 26:55.940]  что там нужен какой-то тервер, значит, если h реально черпается
[26:55.940 --> 27:00.980]  из универсального семейства, если h из универсального
[27:00.980 --> 27:14.280]  семейства х-функции, то каждая операция, каждый запрос
[27:14.280 --> 27:24.360]  обрабатывается за учетную единицу в среднем.
[27:24.360 --> 27:36.160]  Ну, зависит от подробностей, если вам не гарантируется,
[27:36.160 --> 27:39.340]  что x в таблице нет, тогда надо, то есть вам надо
[27:39.340 --> 27:41.920]  весь список пройти, убедиться, что его там нет.
[27:41.920 --> 27:44.920]  Если гарантируется, или если, скажем, вам нужны
[27:44.920 --> 27:47.800]  дубликаты, то есть там вы храните мультим множество,
[27:47.800 --> 27:51.520]  и если там x два раза инцертится, то вы его два раза инцертите,
[27:51.520 --> 27:53.920]  тогда не надо, зависит от реализации.
[27:53.920 --> 27:56.880]  Ну, на это все это не влияет.
[27:56.880 --> 27:59.160]  Вот, значит здесь именно амортизированная единичка
[27:59.160 --> 28:01.100]  в среднем, в среднем, потому что у меня есть элемент
[28:01.100 --> 28:03.640]  вероятности, да, я вот здесь, на каждом шаге, сначала
[28:03.640 --> 28:06.800]  выбираю случайную h, поэтому у меня все в среднем.
[28:06.800 --> 28:10.000]  У меня все, ну, как бы, в среднем�� от ожидания
[28:10.000 --> 28:12.800]  взята, то есть, мотожидание времени, ответа на запрос
[28:12.800 --> 28:13.800]  эта единичка.
[28:13.800 --> 28:15.760]  И не только в среднем, то есть, не только это
[28:15.760 --> 28:19.120]  мотожидание, но еще и амортизированная единичка, потому, что
[28:19.120 --> 28:22.360]  вот есть такие долгие записы, то есть, скажем, если
[28:22.360 --> 28:26.620]  пришло 8 инсертов, это много инсертов, то мне нужно после этого сделать rehash.
[28:26.620 --> 28:32.600]  rehash – тяжелая операция, но редкая. Ну и можно доказать, что амортизирована это будет
[28:32.600 --> 28:35.800]  единичка. То есть, несмотря на то, что, как обычно, конкретная операция работает
[28:35.800 --> 28:42.840]  долго, но учетно, если я все просуммирую, то можно считать, что как будто
[28:42.840 --> 28:45.840]  каждый работает за единичку.
[28:47.280 --> 28:51.600]  Так, ну здесь я это не прописал. Давайте я напишу чуть подробнее. Давайте напишу
[28:51.600 --> 29:00.800]  здесь 1 плюс альфа. Вот так будет. 1 плюс альфа. 1 плюс альфа. Это как бы в частности
[29:00.800 --> 29:04.000]  означает, что вам нужно следить за тем, что альфа не переполняется, что альфа всегда маленькая.
[29:04.000 --> 29:15.000]  Ну я вот, я именно ее сюда добавил, чтобы
[29:15.400 --> 29:22.560]  типа, грубо говоря, если бы мы вот здесь вот поставили альфа какую-нибудь другую,
[29:22.560 --> 29:27.800]  2, 3, 5, тогда здесь настолько бы увеличилась асимпточка, скажем так. То есть, можно
[29:27.800 --> 29:32.440]  реализовать все то же самое с другим порогом на loadfactor, тогда у вас
[29:32.440 --> 29:47.320]  отечественным образом изменится асимпточка. Вот. Так, ну кажется все. Кажется все.
[29:48.520 --> 29:57.040]  Вопросы может еще? Хорошо. Теперь давайте перейдем к другой задаче, но для этого
[29:57.040 --> 30:06.600]  мне нужно будет следующий. Еще раз? Такая что? А, ну просто имперически что-то померили,
[30:06.600 --> 30:10.040]  вот типа с ним хорошо. А, да, я забыл еще одну из вещей сделать. Собственно,
[30:10.040 --> 30:26.320]  давайте построим универсальное семейство. Ну смотрите, еще раз, у меня есть M списков
[30:26.320 --> 30:31.840]  односвязных. В них записана вся информация о том, какие ключи есть в таблице. Я просто их в тупую
[30:31.840 --> 30:36.760]  прохожу, пересчитываю значение хэш-функции кладу в новую большую таблицу, просто за линию не думая.
[30:36.760 --> 30:50.480]  Итак, утверждение. Пусть P простое и универсум каким-то волшебным образом вложен в ZP. То есть,
[30:50.480 --> 30:55.120]  все ключи, все элементы, которые надо хранить, это какие-то целые числа от 0 до P-1.
[30:55.120 --> 31:05.440]  Тогда, если я рассмотрю следующее семейство хэш-функций, параметризованных двумя числами A и B,
[31:05.440 --> 31:16.200]  определенные следующим образом. Я сначала вычисляю AX плюс B по модулю P, беру остаток отделения,
[31:16.200 --> 31:26.840]  потом от этого всего беру остаток отделения по модулю M. И здесь все A и B это произвольные
[31:26.840 --> 31:45.120]  случайные числа из ZP. Такое семейство универсальное. Мне же нужно, я здесь написал,
[31:45.120 --> 31:50.000]  пусть H из универсального семейства, мне надо построить. Вот такая штука является универсальной
[31:50.000 --> 32:08.280]  семейством хэш-функций. Какое я хочу, такое и будет. У меня M по дороге 16, 32, 64. Ну и здесь тоже,
[32:08.280 --> 32:13.760]  какое M надо, такое и будет. Для каждого M, имеется ввиду для каждого M, ну давайте подчеркнем,
[32:13.760 --> 32:23.600]  что для любого M меньше ВП. Вот это верно. А в результате, откуда мы берем хэш-функцию,
[32:23.600 --> 32:30.600]  если мы ее генерируем случайным образом, как вначале говорили, то она не факт, что она попадет?
[32:30.600 --> 32:37.160]  Не совсем случайно, смотрите, тут написано, что мне ее достаточно брать не из всех функций
[32:37.160 --> 32:41.520]  вообще в природе, а только из вот этого маленького универсального семейства. Теперь я его конкретно
[32:41.520 --> 32:45.880]  демонстрирую. Вот это семейство универсально. Если я это докажу, тогда, чтобы сгенерировать
[32:45.880 --> 32:50.840]  случайную хэш-функцию, достаточно случайно для наших целей, мне будет необходимо всего лишь
[32:50.840 --> 32:56.000]  сгенерировать два случайных числа A и B. И дальше, если у меня A и B фиксированы, то чтобы посчитать
[32:56.000 --> 33:00.600]  значение этой функции, вот этой конкретной функции в любой точке, я просто арифметически эти операции
[33:00.600 --> 33:05.520]  делаю. То есть мне уже не нужна произвольная случайная функция, мне достаточно вот такой,
[33:05.520 --> 33:09.400]  который я умею быстро, эффективно генерировать. Просто A и B сгенерирую и все получу.
[33:09.400 --> 33:23.040]  Ну, М меняется. Не обязательно. Это правда, да. Это правда. Ну, типа, почему бы не сгенерить новое?
[33:23.040 --> 33:30.400]  Вот. Доказательства. Ну, давайте посчитаем. Мне нужно всего лишь понять, с какой вероятностью
[33:30.400 --> 33:38.240]  эти две штуки совпадают. То есть я фиксирую какие-то два элемента, пусть X, Y. Ну, давайте я буду считать,
[33:38.240 --> 33:46.120]  что у меня универсум это просто ZP, не особо это важно. Значит, пусть X, Y произвольны элементы ZP,
[33:46.120 --> 33:53.800]  которые не равны. Вот. Тогда мне нужно понять, с какой вероятностью по выбору случайных A и B,
[33:53.800 --> 34:04.360]  то есть мне нужно понять, с какой вероятностью при случайных A и B, h от X равно h от Y. Мне нужно
[34:04.360 --> 34:10.360]  такую вероятность оценить, чтобы она была не больше, чем c делит на m для какой-то константы c.
[34:24.360 --> 34:31.720]  Ну, давайте думать, при каком условии вот это вот выражение при подстановке сюда X, Y будет
[34:31.720 --> 34:38.440]  одинаковым. Значит, я тут по факту делаю следующее. Я сначала вычисляю AX плюс B, потом беру по
[34:38.440 --> 34:44.080]  одному модулю, потом по другому. Вот вопрос. Могли ли они образовать коллизию при взятии по первому
[34:44.080 --> 34:50.160]  модулю? Ну, то есть понятно, что если AX плюс B процент P равно AY плюс B процент P, то есть вот без
[34:50.160 --> 34:55.360]  вот этого взятия по модулю, если бы они совпали в точках X и Y, тогда они бы и совпали по модулю m.
[34:55.360 --> 34:59.880]  Ну, потому что два одинаковых числа по одинаковому модулю одно и то же. Давайте зададимся вопросом,
[34:59.880 --> 35:12.360]  может ли быть такое, что AX плюс B процент P равно AY плюс B процент P? Бывает ли такое? Ну, не
[35:12.360 --> 35:18.720]  бывает. Потому что если бы это было, то это означало бы, что у меня просто вот такие вот величины,
[35:18.720 --> 35:29.840]  сравнимые по модулю P, B можно сократить. Здесь написано, что A на X минус Y сравнимо с нулем по
[35:29.840 --> 35:37.600]  модулю P. Значит, не то что не бывает. Иногда бывает. Например, если нам не повезло и мы изгенерили A
[35:37.600 --> 35:50.160]  равное нулю, то он точно совпадет. Вот. Можно, а можно и не палится. Ну, давайте я сделаю так. То есть
[35:50.160 --> 35:54.720]  можно было бы потребовать, что то A, которое мы генерируем, оно обязательно не нулевое. Ну, понятно,
[35:54.720 --> 35:59.280]  это не очень хорошая функция. Если вы взяли A равно нулю, тогда здесь просто написано 0, и все функции
[35:59.280 --> 36:04.080]  вас равны B. Это не очень хорошая функция. Можно как бы такую заигнодировать. Ну, давайте даже не
[36:04.080 --> 36:10.920]  делать этого. С точки зрения как бы реализации, проще просто сгенерить случайное число от нуля
[36:10.920 --> 36:16.920]  до по минус одного. Оно и так почти, наверное, не будет нулем. И можно там, можно ничего лишнего не
[36:16.920 --> 36:32.040]  фазить, скажем так. Ну, это, понятно, бывает только, бывает только если A равно нулю. Потому что X
[36:32.040 --> 36:35.960]  не равно Y, значит их разность не делится на P, но чтобы произведение делилось на P, мне нужно,
[36:35.960 --> 36:43.400]  чтобы хотя бы один из множеств делился на P, потому что P простое. Вот. Ну, с какой вероятностью это
[36:43.400 --> 36:52.760]  происходит? 1P, это ясное дело. Вероятность по выбору, по случайному выбору A и B того,
[36:52.760 --> 36:59.920]  что A равно нулю, это 1PT. Потому что я A генерирую случайно от нуля до по минус одного, B случайно
[36:59.920 --> 37:05.680]  от нуля до по минус одного. Ну, вероятность того, что выпали конкретным нулем, это как раз 1PT.
[37:05.680 --> 37:12.680]  Вот. Хорошо. Кроме вот этой маленькой поправочки, а мы понимаем, что если P больше чем M, то эта
[37:12.680 --> 37:23.360]  штука не больше чем 1MT. Да, потому что M меньше P, все правильно. Вот. Значит, кроме этого случая,
[37:23.360 --> 37:29.880]  у меня коллизии на первом шаге, вот на этом этапе, до взятия процента по модулю M, на этом этапе они
[37:29.880 --> 37:48.640]  не совпадают. А кроме этого случая, кроме этого случая, у меня вот эти штуки не равны. Хорошо.
[37:48.640 --> 38:03.920]  Значит, более того, я утверждаю, что после вот такого преобразования X и Y в AX плюс B и AY
[38:03.920 --> 38:12.160]  плюс B, вероятность получить каждую конкретную пару, скажем, UV, если это обозначу за U, это обозначу
[38:12.160 --> 38:17.920]  за V, которые различны, то я утверждаю, что вероятность получить конкретную пару UV,
[38:17.920 --> 38:28.200]  это что-то типа 1 на P квадрат. Ну вот давайте, пусть U и V какие-то конкретные фиксированные числа,
[38:28.200 --> 38:37.280]  пусть U не равно V, давайте посчитаем вероятность того, что, ну вот это вот произошло, что AX плюс B
[38:37.280 --> 38:53.800]  это U, AY плюс B это V. Значит, можно записать вот это вот условие, можно записать как систему
[38:53.800 --> 38:59.000]  уравнений и можно записать как систему уравнений в матричном виде. Я могу записать следующее,
[38:59.000 --> 39:13.160]  вот то, что написано здесь, это следующее условие. Вот матрицы же умеете перемножать? Ну хорошо,
[39:13.160 --> 39:22.200]  значит, ну вроде верно, да, X1 как раз AX плюс B должно быть равно U, AY плюс B должно быть равно V,
[39:22.200 --> 39:28.920]  ну естественно все вычления по модулю P, ZP. Вот, и поскольку, смотрите, поскольку мы живем в ZP,
[39:28.920 --> 39:38.160]  поскольку у меня X не равно Y, то у меня определитель этой матрицы будет X минус Y,
[39:38.160 --> 39:46.560]  значит, не нулевой, да, значит, существует, если определить не нулевой, существует ровно
[39:46.560 --> 39:56.680]  один столбец AB, для которого это верно. Согласны? Ну вот, значит, вероятность ровно одна P квадратная,
[39:56.680 --> 40:12.840]  да, хорошо. Нечтяными словами мы получили следующее, что когда мы первый раз навешиваем первое
[40:12.840 --> 40:24.520]  преобразование линейное, вот это вот, мы различные X и Y равновероятно перевели в различную пару UV по модулю P.
[40:24.520 --> 40:30.200]  То есть, когда у меня было X и Y, я навесил на них случайную функцию вот эту и вот эту вот,
[40:30.200 --> 40:37.160]  я равновероятно получаю любую пару U не равно V. Среди всех возможных пар U не равно V у меня все
[40:37.160 --> 40:45.000]  равновероятны с вероятностью одна P квадратная. Вот, ну, значит, я могу просто переписать
[40:45.000 --> 40:57.760]  мое условие. Я могу написать, что вероятность коллизии не больше чем, ну, смотрите, одна P-ты у меня
[40:57.760 --> 41:05.400]  остается от случая A равно нулю, а дальше я могу переписать, что это просто вероятность по всем
[41:05.400 --> 41:22.160]  различным парам UV у не равное V того, что U%M равно V%M. Вот, да, потому что еще раз, когда навешу первое
[41:22.160 --> 41:28.120]  преобразование, у меня каждая конкретная пара X и Y равновероятно отображается в любую пару UV.
[41:28.120 --> 41:31.840]  Ну, то есть, по факту у меня есть опять случайная равномерная распределенность всех вот таких парах.
[41:31.840 --> 41:38.400]  Каждая такая пара реализуется с одинаковой вероятностью. Поэтому вместо того, чтобы брать вероятность по A и B,
[41:38.400 --> 41:43.920]  я могу приспишать вероятность все равных U и V, потому что каждая пара UV однозначно соответствует паре AB.
[41:43.920 --> 42:03.880]  Это, скорее всего, правда. Ну, то есть, скорее всего, сейчас, давайте подумаем. Мне кажется,
[42:03.880 --> 42:12.320]  вот здесь должно быть, опять же, не P квадрат, а P на P-1. Ну, вот именно, да, что порядка все равно
[42:12.320 --> 42:17.520]  будет по квадрату. То есть, вот здесь, скорее всего, написано P на P-1 должно быть, потому что как раз
[42:17.520 --> 42:31.000]  я исключаю случай A равно нулю. Да, да, здесь будет как раз P на P-1. Вот, но если прям дотошничать,
[42:31.000 --> 42:36.480]  то как раз P на P-1 и здесь вот как раз то, что вы сказали, оно не нужно. Но давайте я загруглю, скажу,
[42:36.480 --> 42:44.880]  что это примерно 1P квадратное. Ну все, а теперь смотрите, вот картинка такая. Я хочу посчитать
[42:44.880 --> 42:50.600]  такую вероятность. Что для этого происходит? Давайте я рассмотрю все ZP, занумеру их там.
[42:50.600 --> 42:57.760]  Значит, я беру два случайных различных числа отсюда, хочу понять, с какой вероятностью они
[42:57.760 --> 43:04.160]  равны по модулю M. Ну понятно, что такое равенство по модулю M. Вот если я нарисую где-нибудь U,
[43:04.160 --> 43:14.360]  то какие числа с ним сравнимы по модулю M? Это U плюс M, U плюс 2M, ну и так далее. И здесь U минус M и
[43:14.360 --> 43:20.920]  так далее. Все, что не выходит за границы, вот здесь вот, через ноль не проходит. Вот, ну сколько их?
[43:20.920 --> 43:26.800]  Их примерно P делить на M. Потому что у меня есть точка, если я, скажем, U фиксирую, я могу направо
[43:26.800 --> 43:32.240]  ходить на плюс M, налево ходить на минус M. По факту я рассматриваю все числа, сравнимые с U по
[43:32.240 --> 43:48.120]  модулю M. Их примерно P делить на M. Значит, при фиксированном U подходящих V примерно P делить на M.
[43:48.120 --> 43:52.960]  Но опять же, если точно расписывать, наверное не больше, чем верхняя целая часть P делить на M.
[43:52.960 --> 44:02.600]  Я так оставлю. Вот, ну и поскольку у меня U случайно и V случайно, вот эта штука как раз будет
[44:02.600 --> 44:18.480]  одна M. Потому что если U фиксировано, и V выбирается случайно среди всех P возможных реализаций,
[44:18.480 --> 44:24.080]  то вероятность этой V попасть в подходящие – это вот это делить на P. Потому что подходящих P делить
[44:24.080 --> 44:29.840]  на M, вероятность попасть в конкретное – это 1Pt. Значит, вероятность для данного V быть подходящим –
[44:29.840 --> 44:35.040]  это вот это делить на P. То есть как раз 1Mt. Вот, значит, здесь эта вероятность просто равна 1Mt.
[44:35.040 --> 44:41.640]  Ну и все получилось. Здесь 1Pt плюс 1Mt – это все не больше, чем 2 делить на M. Значит, семейство
[44:41.640 --> 44:56.200]  универсальное, как и хотелось. Вот. Похоже? Вот здесь? Смотрите, для данного U подходящих V вот столько.
[44:56.200 --> 45:03.240]  Теперь, если вы U фиксируете, а V генерируете случайно среди всех P возможных реализаций,
[45:03.240 --> 45:08.960]  какой вероятностью вы попадете вот в одно из этих чисел? Вот это делить на P. Потому что столько
[45:08.960 --> 45:21.240]  подходящих из всего P реализаций. Это делить на P – это как раз 1Mt. Вот. Значит, получается,
[45:21.240 --> 45:26.960]  что если у нас, ну, например, задача такая, что мне нужно хранить множество чисел, скажем,
[45:26.960 --> 45:31.600]  там, помещающихся в какой-нибудь int. Тогда нужно взять достаточно большое простое число P,
[45:31.600 --> 45:36.520]  которое больше, чем все возможные потенциальные элементы, добавляемые в множество. Ну и вот
[45:36.520 --> 45:43.440]  реализовать такую хэш-функцию, вот эту H, и с ней все будет работать хорошо. Все будет
[45:43.440 --> 45:54.360]  работать за единичку в среднем амортизированно. Так. Ну вроде все. Теперь давайте перейдем к
[45:54.360 --> 46:12.680]  совершенному хэшированию. Вот. И для этого мне нужна теорема, что ли. Тоже из теорвера.
[46:12.680 --> 46:17.440]  Значит, у нас есть парадокс дня рождения. Наверняка многим знакомое.
[46:17.440 --> 46:31.640]  Значит, парадокс такой. Представьте, что у вас есть n случайных величин. Давайте я их назову
[46:31.640 --> 46:45.080]  x1 и так далее, xn. Случайные величины. Со значениями в Zm. Со значениями в Zm. Вот. Ну,
[46:45.080 --> 46:54.240]  величины там независимые, да, они генерируются независимо друг от друга. Тогда, значит,
[46:54.240 --> 47:10.440]  пункт первый. Значит, если m больше или равно 1 втраян квадрат, то вероятность того,
[47:10.440 --> 47:23.360]  что произойдет коллизия маленькая. Вероятность того, что, ну, есть коллизия, давайте так и напишу,
[47:23.360 --> 47:31.920]  есть коллизия не больше одной второй. Есть коллизия, то есть из вот этих вот n сгенерированных
[47:31.920 --> 47:36.800]  случайных величин хотя бы две одинаковые. То есть формально вот это значит, что существуют
[47:36.800 --> 47:49.240]  различные i и g такие, что x и t равно x и g. Вот. То есть, если взять m, если взять размер образа,
[47:49.240 --> 47:56.440]  куда действуют все эти случайные величины, если взять m достаточно большим, то с хорошей
[47:56.440 --> 48:00.800]  вероятностью у вас не будет коллизии. То есть, вероятность наличия коллизий будет не больше
[48:00.800 --> 48:06.720]  половины. Значит, с дополнительной вероятностью хотя бы половина коллизий не будет. Ну и второй,
[48:06.720 --> 48:15.200]  наоборот, если у вас m маленькая, давайте скажем что-нибудь типа 1 пятая n квадрата, то, наоборот,
[48:15.200 --> 48:25.720]  вероятность того, что коллизия есть, да, все правильно, вероятность того, что коллизия есть,
[48:25.720 --> 48:35.280]  будет, наоборот, хотя бы 1 вторая. Вот, и как раз вторая часть это парадокс дня рождения,
[48:35.280 --> 48:42.440]  что если вы возьмете n человек, а в году у вас всего m дней, где m не больше чем 1 пятая n квадрат,
[48:42.440 --> 48:48.160]  тогда с хорошей вероятностью у вас будет 2 человека с одинаковым днем рождения. Ну там,
[48:48.160 --> 48:54.880]  на нормальных примерах, если у вас в классе там 20 человек и в году 365 дней, и мы считаем,
[48:54.880 --> 48:59.720]  что даты рождения все независимые, случайные, распределенные во всем годе, то с вероятностью
[48:59.720 --> 49:08.720]  хотя бы 1 вторая у вас там будет 2 человека с одним днем рождения. Ну 22, ну что-то такое, порядка 20.
[49:08.720 --> 49:17.000]  То есть коллизия получается, когда m квадратично по n квадрату. Вот если m квадратично по n квадрату,
[49:17.000 --> 49:22.600]  то для маленьких коэффициентов коллизии есть, для больших уже нет, ну опять же, с достаточно хорошей вероятностью.
[49:22.600 --> 49:33.680]  Вот, опять-таки это в идеальном мире, если х были по-настоящему случайными и независимыми.
[49:33.680 --> 49:39.600]  Значит, на самом деле, если в качестве х брать значение хэш-функции какой-нибудь,
[49:39.600 --> 49:52.720]  тоже давайте, ну не знаю, замечание что ли, тоже без доказательства, что если вместо реальных х
[49:52.720 --> 50:00.800]  брать значение хэш-функции в каких-то разных точках, то по крайней мере вот этот первый пункт будет
[50:00.800 --> 50:14.760]  также верен. Давайте запишем, что если вместо x1 и так далее xn брать значение хэш-функции в каких-нибудь заранее
[50:14.760 --> 50:23.120]  заодных точках, ну скажем y1 и так далее yn, где h опять-таки берется из универсального семейства хэш-функций.
[50:30.800 --> 50:36.480]  То, по крайней мере, первый пункт тоже выполняется.
[50:50.480 --> 50:57.680]  Вот, то есть достаточно требовать не полной независимости всех вот этих величин, а того,
[50:57.760 --> 51:04.280]  что они сгенерированы как результаты применения случайной хэш-функции в данном наборе точек,
[51:04.280 --> 51:08.160]  где эта хэш-функция взята из универсального семейства. Тогда опять-таки вероятность
[51:08.640 --> 51:23.520]  наличия коллизии будет не больше 1 в 2. Понятно, что написано? Сейчас, хороший ответ.
[51:23.520 --> 51:39.400]  Вот, то есть грубо говоря, что это значит? Это значит, что если вы считаете значение хэш-функции,
[51:39.400 --> 51:43.560]  вот этой вот хэш-функции из нашего универсального семейства, если вы считаете значение хэш-функции
[51:43.560 --> 51:50.480]  в n точках и хотите, чтобы они все были различными, тогда вам достаточно брать m вот такого порядка.
[51:50.480 --> 51:55.240]  Тогда с хорошей вероятностью у вас реально все значения будут различными. То есть если ваша цель
[51:55.240 --> 52:00.600]  сгенерировать, выбрать h такую, что все вот эти значения попарно различны, все различные,
[52:00.600 --> 52:07.120]  то вам достаточно выбрать вот такое m в определении хэш-функции. И тогда с неплохой вероятностью вы
[52:07.120 --> 52:11.960]  победите. То есть будут коллизии с маленькой вероятностью, не будет с вероятностью хотя бы 1 в 2.
[52:11.960 --> 52:20.840]  В этом семействе, который вы описали, было простое число, оно должно быть порядком максимального
[52:20.840 --> 52:30.320]  из предполагаемых? Ну типа если все там до 10 в 9, то порядка 10 в 9. Если все до 10 в 18, то порядка 10 в 18.
[52:30.320 --> 52:37.440]  Вот, хорошо. Теперь на основе этого мы можем сделать совершенное хэширование. Совершенное
[52:37.440 --> 52:43.760]  хэширование, немножко обрезанная задача по сравнению с полной. А именно, задача следующая,
[52:43.760 --> 52:54.360]  у вас заранее известен набор ключей. Давайте скажем x1 и так далее, xn. Заранее известный набор ключей.
[52:54.360 --> 53:07.240]  Ну и дальше, кажется можно так обобщить, что у вас во-первых поступают запросы insert
[53:07.240 --> 53:22.560]  какой-то x и t и find произвольное y. То есть вам заранее сказали, какие потенциально ключи можно
[53:22.560 --> 53:29.960]  добавлять в вашу базу данных, в вашу таблицу. Добавлять могут только их, ничего не удаляют, хотя в
[53:29.960 --> 53:35.000]  принципе удалять тоже можно одни из них, но давайте без этого. Обычно это не реализуется. Значит
[53:35.000 --> 53:41.760]  добавлять могут только их, а спрашивать про наличие могут чего угодно, если произвольный y в вашей
[53:41.760 --> 53:46.280]  таблице. Например, откуда такое может быть? Не знаю, у вас есть какие-нибудь участники, которые
[53:46.280 --> 53:50.840]  зарегистрировались на соревнования, и вы понимаете, что только они могут совершать какие-нибудь посылки
[53:50.840 --> 54:00.280]  в контест. Тогда x это участники, вы знаете, что посылать будут только они. Ну и соответственно
[54:00.280 --> 54:05.160]  инсерты, кто-нибудь что-нибудь послал и приходит там директор спрашивает, а Вася Петров что-нибудь
[54:05.160 --> 54:12.000]  отправил или нет? Ну что-нибудь такое. То есть в принципе не совсем оторванная от реальности задача,
[54:12.000 --> 54:21.040]  такое бывает. Вот, что мы сделаем? Давайте мы сначала сделаем то же самое. Давайте сделаем х-таблицу
[54:21.040 --> 54:31.280]  типа с цепочками. Давайте возьмем m равное n просто-напросто. Построим вот такой вот массив размера m и
[54:31.280 --> 54:39.960]  возьмем какую-нибудь внешнюю хэш-функцию, первую хэш-функцию hout, которая вот эти вот все x как-нибудь
[54:39.960 --> 54:46.840]  раскидает по этому массиву. Тогда там могут быть коллизии, даже более того, скорее всего будут,
[54:46.840 --> 54:55.320]  потому что m у меня равно n. Ну и там в какую-то ячейку может быть попало много чисел. Что делать?
[54:55.320 --> 55:00.400]  Давайте мы вместо того, чтобы делать цепочки, давайте мы здесь сделаем еще одну хэш-таблицу.
[55:00.400 --> 55:10.960]  Причем такую, в которой не будет коллизий вообще. А именно, пусть вот сюда попала циитая элементов,
[55:10.960 --> 55:19.680]  попала циитая ключей. То есть, скажем, я сгенерировал случайную внешнюю хэш-функцию hout,
[55:19.680 --> 55:27.760]  посчитал ее значение во всех иксах, ну и посчитал для каждого и сколько раз мы получили такое
[55:27.760 --> 55:34.920]  значение хэша. Вот пусть для конкретного и сюда попала циитая ключей. Тогда моя цель, смотрите,
[55:34.920 --> 55:39.280]  если я знаю, что здесь всего циитая ключей, и скажем, циитая не очень большая, а мы можем ожидать,
[55:39.520 --> 55:44.120]  что циитая не очень большая, потому что мы, грубо говоря, равномерно размазали по этому массиву все
[55:44.120 --> 55:50.680]  элементы. Вот, тогда давайте сделаем просто следующее. Давайте мы заведем массив размера циитой в
[55:50.680 --> 55:59.120]  квадрате, точнее хэш-таблицу на циитой в квадрате элементов, хэш-таблица на циитой в квадрате
[55:59.120 --> 56:08.640]  элементов. Вот, и здесь внутри сгенерируем новую внутреннюю хэш-функцию hout, которая все эти
[56:08.640 --> 56:15.880]  элементы, которые сюда попали, будет рассовывать по этой хэш-таблице без коллизий. Поскольку,
[56:15.880 --> 56:22.440]  если у меня всего в эту ящику попала циитая ключей, то с хорошей вероятностью мы ссылаемся на теорему
[56:22.440 --> 56:27.840]  парадокса День рождения, что с хорошей вероятностью, если у вас всего столько элементов, а размер таблицы
[56:27.840 --> 56:33.920]  такой, то при ровномерной генерации, при случайной генерации универсальной хэш-функции у вас с хорошей
[56:33.920 --> 56:39.720]  вероятностью не будет коллизий. Вот, но ровно это мы и хотим. То есть мы будем генерировать такую h i,
[56:39.720 --> 56:46.320]  чтобы не было коллизий. Все, и дальше, если мы такого добились, то есть если мы добились, что у меня,
[56:46.320 --> 56:52.280]  ну там, все вот эти вот цшки не очень большие, что h i t внутренне распределяют без коллизий, тогда на
[56:52.280 --> 56:56.960]  все запросы отжать элементарно. Мы для каждого х сначала считаем его значение относительно внешней
[56:56.960 --> 57:03.600]  хэш-функции, понимаем, в какой внутренней хэш-таблице оно должно лежать, какой-то g. Затем
[57:03.600 --> 57:08.760]  смотрим просто на эту таблицу, ей ассоциирована какая-то хэш-функция h g, которую легко запоминать,
[57:08.760 --> 57:15.200]  там всего два числа надо хранить, a и b по моделю p. Ну и дальше в нее подставляем вот этот x и
[57:15.200 --> 57:20.880]  смотрим есть он там или нет. Если нет, добавляем. То же самое с файндом. Мы сначала считаем внешнюю
[57:20.880 --> 57:27.880]  хэш-функцию, потом внутреннюю, проверяем есть она внутри или нет. А чтобы с хорошей вероятностью не
[57:27.880 --> 57:46.080]  было коллизий и все. В смысле не понял? 2c и почему не хватит? Ну вот почему например, потому что вот
[57:46.080 --> 57:52.680]  если у вас размер менее чем квадратичен, то у вас есть коллизия с большой вероятностью и более
[57:52.680 --> 57:57.560]  того чем меньше здесь констант, тем больше здесь вероятность. То есть тут именно как бы n квадрат
[57:57.560 --> 58:02.520]  это именно прям ну грубо говоря точная точная граница. Что если больше, то коллизии нет, если
[58:02.520 --> 58:08.120]  меньше, то коллизии есть. А мы хотим прям полностью без коллизий. Поэтому нам с квадратом придется
[58:08.120 --> 58:21.120]  работать. Она оттуда же. У нас все функции из одного и того же универсального хэш-функции.
[58:21.120 --> 58:34.360]  Итак, смотрите. Я сейчас все пропишу. Смотрите, что мы делаем сначала инициализация. У меня есть
[58:34.360 --> 58:41.080]  какое-то там универсальное семейство хэш-функций, универсальное семейство хэш-функций. Пусть так будет
[58:41.080 --> 58:48.840]  написано. Первое, что я делаю, это я хочу сгенерировать внешнюю h out так, чтобы сумма
[58:48.840 --> 58:53.880]  циит и квадрат их была не очень большая. Потому что если циит и квадрат был очень большой, то мне
[58:53.880 --> 58:59.000]  нужна квадратичная память. Это прям совсем от нас. Мы такого не хотим. Давайте делать следующее.
[58:59.000 --> 59:17.080]  Генерируем случайную h out. Для нее считаем размеры всех ячеек, всех корзиночек. Вычисляем циит,
[59:17.080 --> 59:29.920]  это количество, мощность множества тех х таких, что h out равно i. Это формально. По смыслу мы для
[59:29.920 --> 59:35.080]  каждой ячейки запоминаем, сколько ключей туда попало. Для каждого и считаем, сколько х имеют
[59:35.080 --> 59:46.480]  такой хэш. Дальше, если сумма квадратов циит больше чем 4n, то перегенерируем и начинаем заново.
[59:46.480 --> 59:55.080]  Иначе мы говорим, что h out хорошая. Иначе фиксируем h out.
[59:55.080 --> 01:00:10.240]  Грубо говоря, пока не получится, давайте просто генерировать новые хэш функции. Вот мы взяли один
[01:00:10.240 --> 01:00:16.840]  раз h out, посчитали сумму квадратов циит. Если это слишком много, ну давайте много 4n. Если больше
[01:00:16.840 --> 01:00:22.280]  чем 4n, все, говорим, что плохая хэш функция, она слишком многими обладает коллизиями, давайте
[01:00:22.280 --> 01:00:28.640]  генерировать заново. Вот я утверждаю, что если мы такое условие напишем, то очень быстро мы победим.
[01:00:28.640 --> 01:00:35.360]  Можно показать, что среднее количество шагов для генерации правильной h out будет двойка. То есть
[01:00:35.360 --> 01:00:41.160]  у вас, если вы взяли случайную h out, то это событие происходит с вероятностью всего лишь 1 вторая,
[01:00:41.160 --> 01:00:46.560]  опять-таки. Ну и значит с вероятностью 1 вторая мы побеждаем на каждом шаге. В среднем будет как
[01:00:46.560 --> 01:00:54.680]  раз двойка шагов в среднем. Так или иначе мы просто генируем много раз h out, до тех пор пока не
[01:00:54.680 --> 01:01:02.160]  получится, что сумма циит квадрат меньше чем 4n. Теперь у нас получается верно вот такое, что сумма
[01:01:02.160 --> 01:01:09.400]  циит квадрат не больше чем 4n. А значит у нас как бы есть возможность для каждого и завести хэш
[01:01:09.400 --> 01:01:14.840]  таблицу размера циит квадрат, потому что суммарный размер будет линейным, и нам нас линейная память
[01:01:14.840 --> 01:01:19.720]  устраивает. То есть мы это делали, потому что мы не хотели квадратичную память, что-то большое. А линей
[01:01:19.720 --> 01:01:41.320]  нам хватит. Ну тогда значит разбиваем все х на группы по равному значению h out, по значению h out
[01:01:41.320 --> 01:01:56.400]  от x. Внутри этой группы нам нужна новая хэш таблица, которая имеет квадратичный размер.
[01:01:56.400 --> 01:02:13.040]  Генерируем хэш таблицу размера циит в квадрате. Без коллизий давайте здесь напишем, без коллизий.
[01:02:13.040 --> 01:02:25.440]  Как добиться того, чтобы она была без коллизий? Опять давайте брать просто случайную h i,
[01:02:25.440 --> 01:02:34.160]  строить явным образом хэш таблицу, проверять нет ли там коллизий. По теореме с хорошей вероятностью
[01:02:34.160 --> 01:02:38.720]  коллизий не будет, потому что у меня размер достаточно большой, размер квадратичен по
[01:02:38.720 --> 01:02:43.960]  количеству элементов. С хорошей вероятностью коллизий вообще не будет. То есть если я возьму
[01:02:43.960 --> 01:02:49.000]  случайную h it, то она с хорошей вероятностью не будет обладать коллизиями. Если нам не повезло
[01:02:49.000 --> 01:02:54.480]  и мы взяли плохую h it, давайте перегенерируем. Здесь опять то же самое, что пока у меня есть
[01:02:54.480 --> 01:03:00.560]  коллизии, то есть пока в конкретной it ячейке, в конкретной it таблице, пока есть коллизия, я беру
[01:03:00.560 --> 01:03:07.280]  новую h i. Так много-много раз, пока не найдут ту h i, на которой нет коллизий. То есть генерируем h i,
[01:03:07.280 --> 01:03:11.480]  пока не получится. h i, пока не получится.
[01:03:11.480 --> 01:03:40.480]  Это нам не даст никакой победы. То есть, грубо говоря, если мы ограничим количество какой-то
[01:03:40.480 --> 01:03:46.800]  константой, то, окей, я не уверен, но возможно тогда вместо квадрата нужно будет там типа, ну вот здесь
[01:03:46.800 --> 01:03:53.080]  какой-нибудь корень писать. Короче, это не даст выигрыша никакого. То есть так можно было бы делать,
[01:03:53.080 --> 01:04:00.320]  но не нужно. То есть это ничем не будет лучше, потому что у нас и так, смотрите, у меня и так
[01:04:00.320 --> 01:04:05.840]  линейная память суммарно, у меня память вот такая, то есть 4n максимум. У меня время построения будет
[01:04:05.840 --> 01:04:13.480]  линейное, потому что, ну что, я сначала несколько раз сделал вот это, в среднем там два раза надо
[01:04:13.480 --> 01:04:19.280]  сделать, это за линию делается. Потом внутри каждой группы, для каждого i, я опять много раз генерирую,
[01:04:19.280 --> 01:04:24.040]  пока не получится без коллизий. Ну суммарно, вот уже опять будет за линейное время. Больше оптимизировать
[01:04:24.040 --> 01:04:30.160]  некуда. Вот, значит, тогда ответ на запрос будет за чистую единицу. Ответ на каждый запрос
[01:04:30.160 --> 01:04:45.120]  за чистая у от одного, извините. Потому что как ответить на запрос? Мне нужно сначала к пришедшему
[01:04:45.120 --> 01:04:50.160]  элементу применить внешнюю hash функцию hash out, сначала понять в какую из внутренних hash таблицы она
[01:04:50.160 --> 01:04:56.880]  попала, а дальше внутри посчитать hash шитая от x и понять в каком элементе она лежит. И поскольку у меня
[01:04:56.880 --> 01:05:01.400]  в этой таблице уже нет коллизий, то там все понятно, либо элемент есть, либо элемента нет, он только
[01:05:01.400 --> 01:05:06.720]  там, да, мне не нужно ничего просматривать. Мне по факту нужно два раза посчитать hash функцию, внешнюю,
[01:05:06.720 --> 01:05:10.960]  а потом внутреннюю, и понять есть элемент или нет. То есть мне не нужно пробегаться по списку,
[01:05:10.960 --> 01:05:34.080]  у меня чистая единица. Да, значит, итог. Мы за у от н в среднем построили структуру размера у от н,
[01:05:34.080 --> 01:05:41.640]  которая отвечает на все запросы, потом уже за от единицы чистая. Которая умеет отвечать
[01:05:41.640 --> 01:05:56.880]  на запросы за от единицы. Ну, можно добавить на самом деле, то есть если рейсы тоже только с x
[01:05:56.880 --> 01:06:04.120]  происходят, то почему бы нет. Ну, повторю, обычно типа этого не происходит, но добавить можно,
[01:06:04.120 --> 01:06:14.080]  от этого ничего не поменяется. Это хороший вопрос, но вот на самом деле, если вам нужен порядок в
[01:06:14.080 --> 01:06:19.920]  каком-то смысле, то надо. Грубо говоря, если вам там надо кроме всех обычных операций узнавать,
[01:06:19.920 --> 01:06:24.560]  там если вы в контесте что-нибудь сдавали, там будет задача типа определить количество чисел
[01:06:24.560 --> 01:06:30.160]  не больше, чем x. В дереве поиска делается на халя, вы просто спускаетесь и понимаете,
[01:06:30.160 --> 01:06:34.400]  сколько элементов слева у вас было. А здесь, чтобы в хэш таблице что-то сделать, хэш таблица,
[01:06:34.400 --> 01:06:38.080]  порядок элементов на у вообще полностью перемешивает, она абсолютно, грубо говоря,
[01:06:38.080 --> 01:06:43.160]  случайно их все перемешивает. Тогда, чтобы понять, сколько элементов не больше, чем x, в хэш таблице
[01:06:43.160 --> 01:06:47.440]  это непонятно как сделать. То есть лучше, что можно это все элементы явным образом перебрать,
[01:06:47.440 --> 01:06:54.040]  а это линия. То есть если хоть что-то, кроме вот обычных операций, insert, find, erase, если хоть что-то
[01:06:54.040 --> 01:07:04.320]  есть, связанное с порядком элементов, тогда вот дерево поиска выигрывает. Так, хорошо.
[01:07:14.320 --> 01:07:20.320]  Вы имеете в виду типа завести массив для каждого х свой? Ну тогда непонятно,
[01:07:20.320 --> 01:07:27.760]  ну вот это я так хорошо сказал, find x и t, на самом деле просто insert x. Мне не говорят его номер в этом
[01:07:27.760 --> 01:07:32.800]  порядке. Это важное замечание. Мне не говорят номер элемента вот в этом его порядке. Мне говорят
[01:07:32.800 --> 01:07:38.680]  просто какой-то Вася Петров. Мне не говорят, что Вася Петров имеет номер 148 в каком-то списке.
[01:07:38.680 --> 01:07:44.120]  Мне просто говорят Вася Петров. Соответственно, этот x я не знаю, где он здесь лежит. Ну а собственно
[01:07:44.120 --> 01:07:51.080]  это вопрос локализации. Мы такого делать не умеем, если у нас нет порядка на них. Вот, хорошо,
[01:07:51.080 --> 01:07:55.720]  значит это все хорошо в теории. Что это значит? Работает за линию предпочета и за единицу на
[01:07:55.720 --> 01:08:01.600]  запросы чистые. Прям супер. Но константа тут ну такая толстенькая, потому что у меня много маленьких
[01:08:01.600 --> 01:08:07.200]  векторов. С памятью там не очень хорошо все это происходит. Ну в смысле с кэшом, с константой и
[01:08:07.200 --> 01:08:14.520]  там. То есть теоретически это хорошо, но константа тут большая на самом деле. Вот, а значит как я
[01:08:14.520 --> 01:08:20.480]  понимаю самый популярный способ реализации х-таблицы это следующий. Значит это х-таблицы с открытой
[01:08:20.480 --> 01:08:24.640]  адресацией. Х-таблицы с открытой адресацией.
[01:08:37.200 --> 01:08:53.080]  Ну давай сделаем следующее. Опять мы заведем массив, длины m. Но теперь у меня в каждой ячейке
[01:08:53.080 --> 01:08:57.920]  будет не односвязанных писок, а просто число. Ну просто элемент лежать. То есть либо элемент,
[01:08:57.920 --> 01:09:05.520]  либо ничего. Тогда, значит опять, вот пришел какой-то х, я посчитал у него h от x, положил х в эту ячейку.
[01:09:05.520 --> 01:09:14.760]  Пришел какой-то у, я посчитал h от у, положил у сюда. Пришел какой-то z, я положил его сюда.
[01:09:14.760 --> 01:09:21.200]  Да, мы помним, что проблема в том случае, когда возникают коллизии. Давайте коллизии будем
[01:09:21.200 --> 01:09:28.080]  решать следующим образом. Не цепочками, а так. Вот представьте, пришел какой-нибудь t, у которого
[01:09:28.080 --> 01:09:34.800]  h от t равно h от x. То есть, грубо говоря, t хочет встать вот сюда. Но тут занято. Значит давайте тогда вместо
[01:09:34.800 --> 01:09:40.760]  этого мы просто пойдем направо и найдем первую свободную ячейку. Вот такая ивристика. То есть,
[01:09:40.760 --> 01:09:45.240]  я просто вижу, что ага, тут занято. Ну ничего не поделаешь, давай порубим сюда. Тут занято.
[01:09:45.240 --> 01:09:53.680]  Порубим сюда. Тут занято. Здесь свободно. Сюда ставим t и заканчиваемся. Все. Весь алгоритм.
[01:09:53.680 --> 01:10:18.240]  Значит, ну там давайте скажем insert t. Сначала вычисляю, ну давайте g равно h от t. Нет,
[01:10:18.240 --> 01:10:31.620]  не хочу. Давайте просто вычисляю h от t. Затем для каждого i давайте я определю run it. Это h
[01:10:31.620 --> 01:10:42.440]  tt плюс i. Для i там 0, 1, 2 и так далее. Run it от t. То есть, run это последовательность ячейок, в которой я
[01:10:42.440 --> 01:10:49.360]  смотрю одну за другой. Run 0 от t это просто h от t, куда я хочу попасть в самом начале. Run 1 это
[01:10:49.360 --> 01:10:55.400]  следующий ячейк, run 2 здесь, run 4 здесь и так далее. Значит, я просто иду по списку вот этих значений,
[01:10:55.400 --> 01:11:00.600]  то есть грубо говоря, я посчитал h от t и добавляю по единичке каждый раз. Когда нахожу свободную
[01:11:00.600 --> 01:11:19.080]  ячейку туда и кладу t. На кладу t в первую свободную ячейку. В первую свободную ячейку. Вот. Так работает
[01:11:19.080 --> 01:11:26.120]  insert. Find абсолютно аналогично. Если приходит find какой-то t, то же самое. Я считаю h от t,
[01:11:26.120 --> 01:11:32.440]  значит мы понимаем, что если t где-то и лежит, то либо в точке h от t, либо ht плюс 1, либо и так далее,
[01:11:32.440 --> 01:11:38.040]  до первой дырки. То есть, грубо говоря, если у меня есть какой-то блок заполненных элементов,
[01:11:38.040 --> 01:11:45.280]  начиная с h от t, а вот здесь дырка, тут пусто, тогда t если где-то и присутствует, то где-то на этом
[01:11:45.280 --> 01:11:52.040]  отрезке. Вот где-то здесь оно должно быть. Потому что если я дошел до дырки и t не встретил,
[01:11:52.040 --> 01:11:57.640]  значит t не могло раньше прийти, потому что если оно пришло раньше, то оно прошло весь этот путь,
[01:11:57.640 --> 01:12:13.080]  ну и закончилось до дырки. Значит t просто нет. Аналогично идем до свободного места.
[01:12:13.080 --> 01:12:26.480]  Нет, ну это скорее описание процедуры, как мы ищем t.
[01:12:26.480 --> 01:12:39.720]  Ну на самом деле хранить его не особенно есть смысл. Я обобщил, ну то есть я написал так,
[01:12:39.720 --> 01:12:44.840]  потому что можно другие раны использовать. Чуть позже я скажу другие раны, но это просто тупой
[01:12:44.840 --> 01:12:50.760]  линейный проход. У меня рана it, это it ячейка после h от t. Это я просто формально написал. На самом
[01:12:50.760 --> 01:13:03.400]  деле можно просто считать, что иду слева направо, начиная с h от t. Ну тогда казалось бы, все идеально.
[01:13:03.400 --> 01:13:10.000]  Значит здесь очень хорошо все с кэшом, все с памятью, потому что у меня нет вот этих вот
[01:13:10.000 --> 01:13:16.560]  перескакиваний в памяти. В отличие от хэш таблицы со списками, когда я там прыгаю очень сильно по
[01:13:16.560 --> 01:13:24.320]  памяти. Я сначала посчитал хэш, прыгнул сюда, потом еще по таблице пошел по списку, и сам список
[01:13:24.320 --> 01:13:27.960]  это указатели, которые тоже бог пойми где в памяти лежат. Там с памяти все очень плохо,
[01:13:27.960 --> 01:13:33.000]  константа очень большая, потому что мне нужно много прыгать по памяти. А здесь у меня все изменения
[01:13:33.000 --> 01:13:37.760]  очень локальные. Если я один раз посчитал хэш, то дальше мне нужно пройти просто слева направо вот
[01:13:37.760 --> 01:13:43.520]  во все эти ячейки посмотреть. Но память, доступ к памяти так работает, что если вы как бы сюда
[01:13:43.520 --> 01:13:48.960]  смотрите, то следующие элементы у вас близко. Вам не нужно далеко перемещать указатель, и у вас
[01:13:48.960 --> 01:13:53.720]  все все эти данные близко, и до них дойти можно быстро. Константа сильно меньше для вот этого
[01:13:53.720 --> 01:14:05.080]  перехода. На практике здесь константа сильно лучше, работает сильно быстрее, чем цепочка.
[01:14:05.080 --> 01:14:15.360]  Что же делать с эрейзом? Тут уже не сработает такая идея, что давайте просто встанем сюда,
[01:14:15.360 --> 01:14:21.400]  найдем значение t и его отсюда удалим, просто удалим из таблицы. Потому что, например,
[01:14:21.400 --> 01:14:29.880]  представьте себе у вас такая картинка. Значит, у вас был x, y, z. Пусть у них,
[01:14:29.880 --> 01:14:37.920]  например, у всех одинаковый хэш. h от x равно h от y равно h от z. Вы их вот так вставили в
[01:14:37.920 --> 01:14:44.200]  таком порядке, потом вызвали, например, race y. Если вы просто сотрете этот y и скажете,
[01:14:44.200 --> 01:14:50.400]  что эта клетка пустая, то вы теперь z не сможете найти, у вас z как бы нету. Потому что чтобы найти z,
[01:14:50.400 --> 01:14:54.560]  вы встаетесь сюда и идете до первой дырки, а дырку у вас вот сразу же вы z не найдете.
[01:14:54.560 --> 01:15:02.960]  Поэтому мы просто так затирать нельзя. Вместо этого давайте мы оставим здесь лежать y,
[01:15:02.960 --> 01:15:10.520]  просто пометим, ну или там, не важно, короче, мы пометим, что этот элемент удален. Мы явно не
[01:15:10.520 --> 01:15:15.120]  будем его удалять из таблицы, мы просто пометим, булис и флаг какой-то на него повесим, скажем,
[01:15:15.120 --> 01:15:23.080]  что этот человек удален. Это называется tombstone, типа могильный камень. Все, он от нас ушел,
[01:15:23.080 --> 01:15:28.280]  мы пометили, что его там нет. Мы его удалили, но как бы не удалили, мы пометили, что он удален,
[01:15:28.280 --> 01:15:36.160]  скорее так. И тогда у нас такой проблемы уже не будет, что когда я пытаюсь найти z, я точно так
[01:15:36.160 --> 01:15:42.120]  же делаю, я встаю сюда, здесь не z, встаю сюда, это не дырка, tombstone не считается дыркой, я говорю,
[01:15:42.120 --> 01:15:48.920]  что окей, его тут пока нет, иду дальше. И потом до него дошел. Вот, поэтому как бы find работает
[01:15:48.920 --> 01:15:57.280]  так же корректно. Единственное, что нужно подправить, это insert. Нужно сделать следующее, нужно сделать так,
[01:15:57.280 --> 01:16:03.320]  что insert может вставлять на место tombstone. То есть если он идет слева направо и нашел не дырку,
[01:16:03.320 --> 01:16:08.760]  а элемент, который удален, тогда можно на его место что-то поставить. Потому что как раз удаленность,
[01:16:08.760 --> 01:16:15.480]  удалять элементы нельзя, потому что мы нарушим связанность вот этого блока. А на его место ставить
[01:16:15.480 --> 01:16:19.240]  что-то другое можно. То есть если у меня здесь был tombstone, а я сюда хочу что-то положить,
[01:16:19.240 --> 01:16:25.800]  какой-нибудь t, то я просто говорю, что здесь t и снимаю метку удаленности. Давайте запишем,
[01:16:25.800 --> 01:16:50.800]  можно класть во время инсерта новые элементы на место удаленных. То есть точно так же я иду
[01:16:50.800 --> 01:16:59.920]  слева направо, жду либо дырки, либо tombstone. И в том и в другом случае я на это место кладу элемент,
[01:16:59.920 --> 01:17:12.360]  который я хочу добавить. Так, ну вот теперь все. Давайте я только здесь еще формализую, что здесь
[01:17:12.360 --> 01:17:22.760]  я беру все это по модулю m. Потому что если вдруг так вышло, что у вас h от x это последний элемент,
[01:17:22.760 --> 01:17:27.400]  и вы хотите взять следующий, то вам надо чтобы следующий был. Поэтому я как бы массив зацикливаю
[01:17:27.400 --> 01:17:33.880]  и говорю, что следующий это первый. А следующий за последним это первый. Но это тонкость.
[01:17:33.880 --> 01:17:44.840]  Вот. Это реализация hash таблицы с открытой адресацией. Это называется линейное пробирование.
[01:17:44.840 --> 01:17:54.920]  Вот такие раны это линейное пробирование. Линейное пробирование. То есть когда для
[01:17:54.920 --> 01:18:01.400]  определения места положения x я просто иду слева направо, прибавляя единичку каждый раз.
[01:18:01.400 --> 01:18:12.920]  Есть другие альтернативные реализации, есть квадратичные пробирования. Это когда у вас
[01:18:12.920 --> 01:18:31.600]  ran it от t это h плюс и квадрат. Но опять-таки по модулю m. Вот. И еще более крутая реализация,
[01:18:31.600 --> 01:18:45.680]  называется двойное хэширование. Это когда у вас ran it от t это h первое плюс и наш второе.
[01:18:45.680 --> 01:18:57.640]  То есть у вас есть две хэш функции. Первая ячейка, в которую вы встаёте, это h1, а дальше вы идёте
[01:18:57.640 --> 01:19:03.400]  линейно с шагом h2. Не с шагом 1, а с шагом h2, которая тоже хэш функция, которую вы заранее
[01:19:03.400 --> 01:19:22.920]  фиксировали. Случайно. Ещё раз? Вроде хэширование. Вроде хэширование называется. По-моему это называется
[01:19:22.920 --> 01:19:31.960]  так. Пробирование это какие элементы мы пробуем на то, чтобы вставить х, а здесь двойное пробивание
[01:19:31.960 --> 01:19:39.840]  как-то странно. Мы не два раза пробили. Кажется так называется. Неважно. В чём смысл, в чём выигрыш
[01:19:39.840 --> 01:19:49.840]  квадратичного и двойного? Например, смотрите, если так вышло, что у меня образовался какой-то
[01:19:49.840 --> 01:19:59.000]  большой блок чисел, который заполнен x, y, z, t, u и так далее. Наверное, хоть хэши у них и близкие,
[01:19:59.000 --> 01:20:05.720]  но наверное различные. Например, здесь было h от x, здесь было h от t, то есть сначала попробовал сюда,
[01:20:05.720 --> 01:20:13.840]  потом добавил сюда. То есть у них хэши близкие, но различные. Если у вас начало различное и вы будете
[01:20:13.840 --> 01:20:17.320]  идти нелинейным просто проходом слева направо, тогда они обязательно склеиваются, потому что они
[01:20:17.320 --> 01:20:21.880]  близкие, вы просто вот так идёте, они склеиваются. А если вы будете прыгать как-то более нетривиально,
[01:20:21.880 --> 01:20:29.720]  скажем вот так квадратично или с шагом равным другой хэш функции от элемента, тогда у вас скорее
[01:20:29.720 --> 01:20:34.880]  всего вот эти раны будут как бы менее пересекаться. То есть у вас сейчас прикол в том, что когда вы
[01:20:34.880 --> 01:20:41.520]  начинаете здесь и идёте слева направо и вот здесь идёте слева направо, у вас раны склеились,
[01:20:41.520 --> 01:20:47.160]  у вас вот эти все элементы, и вот это одно и то же. А если бы вы начали здесь и здесь и шли бы с
[01:20:47.160 --> 01:20:53.240]  другими последовательностями, не плюс один, плюс один, плюс один, а плюс h2 зависит ещё от x. Тогда у
[01:20:53.240 --> 01:20:57.680]  вас они как бы разойдутся. Этот бы сразу перешёл сюда, а этот вот туда вот. Они были бы различны.
[01:20:57.680 --> 01:21:12.200]  Ну нет, смотрите, потому что если раны определять именно как вот там, то у нас ничего не пропустится.
[01:21:12.200 --> 01:21:16.560]  То есть там опять будет верно то же самое, что... Ну то есть представьте, что вот здесь вместо того,
[01:21:16.560 --> 01:21:22.320]  чтобы идти слева направо, я иду там ну по квадратам, например, не плюс один, плюс один, плюс один,
[01:21:22.320 --> 01:21:31.440]  а плюс один, плюс четыре, плюс девять и так далее. Тогда вот уже ничего не пропустится. Ну с инсертом
[01:21:31.440 --> 01:21:38.760]  всё будет нормально, а с рейзами, смотрите, если я когда добавил x, то есть вот если я когда-то
[01:21:38.760 --> 01:21:45.200]  куда-то вставил x, это значит я сначала посчитал h от x, потом вот по этому рану до него дошёл. Но
[01:21:45.200 --> 01:21:51.280]  если я хочу теперь сделать рейс x, то я встаю сюда и иду по тому же рану. Я его не упущу никогда.
[01:21:51.280 --> 01:22:00.240]  Поэтому как именно я реализовываю ран, это неважно. Если элемент когда-то вставил, то найти его я
[01:22:00.240 --> 01:22:04.160]  смогу по тому же рану. Просто последовательность ячеек, которые я переводил, та же. Не важно,
[01:22:04.160 --> 01:22:14.560]  как именно она устроена. Да, да. Значит вот есть теорема. Ну не знаю, наверное, в следующий раз
[01:22:14.560 --> 01:22:22.720]  формализую, но, грубо говоря, давайте напишу формулировку, что если первое при линейном пробировании
[01:22:22.720 --> 01:22:30.840]  значит вам нужно, чтобы семейство х-функций было 5 независимое, но, видимо, в следующий раз уже
[01:22:30.840 --> 01:22:35.440]  дадим определение, но, грубо говоря, это более сильное требование. Если семейство х-функций 5
[01:22:35.440 --> 01:22:45.600]  независимое, тогда у вас опять-таки будет, ну, учетная единичка на запрос. А если вы берете двойное
[01:22:45.600 --> 01:22:58.640]  хэширование, тогда вам достаточно два независимых семейств, и тоже будет единичка. Вот. То есть двойное
[01:22:58.640 --> 01:23:05.280]  идти получше. В следующий раз я доформализую, но мораль именно такая, что если вы вместо линейного
[01:23:05.280 --> 01:23:11.600]  пробирования будете использовать двойное хэширование, то будет приятнее и быстрее. А по-разумевшим,
[01:23:11.600 --> 01:23:14.400]  ничего не понятно. Там ничего не понятно. Все, спасибо.

[00:00.000 --> 00:11.500]  Как я анонсировал сначала, мы хотим сейчас рассмотреть альтернативный подход к транзакциям,
[00:11.500 --> 00:18.540]  к определенным транзакциям, который бы позволил вам избежать двухвазного комита.
[00:18.540 --> 00:26.860]  Давайте подумаем, почему двухвазный комит нас беспокоит. Вот на примере того же самого спаннера
[00:26.860 --> 00:33.860]  давайте нарисуем картинку, где у нас есть шарды, находящиеся в разных дата-центрах.
[00:33.860 --> 00:49.860]  Пусть у нас есть первый дата-центр, второй, третий, и в каждом есть по реплике одного из трех шардов.
[00:49.860 --> 00:59.860]  Вот это какой-то шард, и есть два других еще.
[01:09.860 --> 01:14.860]  А теперь мы запускаем транзакцию, которая выполняет двухвазный комит над этими шардами.
[01:14.860 --> 01:22.860]  Какова цена этого двухвазного комита? Смотрите, в нем, я говорил, было три записи.
[01:22.860 --> 01:31.860]  Сначала запись параллельная в каждый шард на фазе Prepr, где мы надежно сохраняли блокировки,
[01:31.860 --> 01:40.860]  затем запись решения координатора, и после этого параллельные записи для фиксации изменений в данных.
[01:40.860 --> 01:49.860]  Секунду, я закрою дверь.
[01:49.860 --> 01:57.860]  Но каждая запись, это же запись не просто на локальный жесткий диск, это запись прямо в шард системы,
[01:57.860 --> 02:04.860]  а запись в шард это запись в Paxos, а Paxos у нас между дата-центровый.
[02:04.860 --> 02:15.860]  И в итоге, когда мы делаем параллельную запись Prepr, мы выполняем коммуникацию через моря, океаны,
[02:15.860 --> 02:18.860]  потому что эти реплики могут находиться очень далеко друг от друга.
[02:18.860 --> 02:23.860]  То есть это не просто один раунд трип на комит команды в мультипаксосе оптимизированном,
[02:23.860 --> 02:30.860]  это то, что называется White Area Network RTT, а он дорогой.
[02:30.860 --> 02:36.860]  И вот мы делаем три таких раунд трипа. То есть мы сначала пишем в каждый шард,
[02:36.860 --> 02:41.860]  потом мы пишем в какой-то шард координатор, потом мы пишем снова в каждый шард.
[02:41.860 --> 02:48.860]  Три записи подряд.
[02:48.860 --> 02:52.860]  Гораздо приятнее комит был устроен в транзакциях, которые касались только одного шарда,
[02:52.860 --> 02:54.860]  потому что он был однофазный.
[02:54.860 --> 02:58.860]  Просто посылали комит, и если этот комит был неуспешен, потому что реплика,
[02:58.860 --> 03:03.860]  потому что шард забыл про нашу сессию или там истек тайм-аут или какой-то конфликт,
[03:03.860 --> 03:06.860]  то просто вся транзакция откатывалась, потому что задевал только его.
[03:06.860 --> 03:10.860]  В случае кроссшардовых транзакций у каждого шарда свое независимое решение,
[03:10.860 --> 03:15.860]  поэтому нужно было их согласовать, поэтому у нас были двухфазные транзакции.
[03:15.860 --> 03:21.860]  Так вот, нас сейчас интересует вопрос, насколько фундаментальным является двухфазный комит
[03:21.860 --> 03:25.860]  для распределенных транзакций? Верно ли, что он необходим?
[03:25.860 --> 03:29.860]  Верно ли, что его невозможно избежать?
[03:29.860 --> 03:37.860]  Давайте подумаем, что является причиной двухфазного комита?
[03:37.860 --> 03:40.860]  Что нас заставляет его выполнять?
[03:40.860 --> 03:47.860]  Кажется, три причины.
[03:47.860 --> 04:00.860]  Это тайм-ауты, это смена лидера
[04:00.860 --> 04:16.860]  и это конфликты блокировок в двухфазных, в 2PL.
[04:16.860 --> 04:21.860]  Вот по этой причине каждый шард мог отказать в комите транзакций,
[04:21.860 --> 04:26.860]  поэтому нам нужен был двухфазный протокол, чтобы коллективное решение было общим.
[04:26.860 --> 04:31.860]  Чтобы решение было общим на всех шардах.
[04:31.860 --> 04:41.860]  Давайте подумаем, а почему нам нужны были, точнее, можно ли всего этого избежать?
[04:41.860 --> 04:45.860]  Вот верно ли, что это все прямо необходимо, что мы без этого
[04:45.860 --> 04:51.860]  распределенной транзакции представить невозможно?
[04:51.860 --> 04:57.860]  Если мы каким-то образом сможем избавиться от тайм-аутов, которые приводили к аборту транзакций,
[04:57.860 --> 05:02.860]  если мы сможем избавиться, если мы сможем как-то переживать смену лидеров в шарде,
[05:02.860 --> 05:07.860]  если мы сможем каким-то образом справляться с конфликтами в 2PL, если, не знаю,
[05:07.860 --> 05:11.860]  как-то мы сможем вообще избежать, то, может быть, нам не потребуется двухфазный комит,
[05:11.860 --> 05:19.860]  потому что тогда, если у нас в системе не будет всех этих эффектов недетерминированных,
[05:19.860 --> 05:27.860]  тогда просто разные шарды не будут расходиться во мнении относительно успеха или отката транзакций.
[05:27.860 --> 05:34.860]  Отверждается, что проблема в недетерминизме, то есть корень всех бед наших в недетерминизме.
[05:34.860 --> 05:38.860]  Вот почему возникает тайм-аут? Потому что кто-то где-то залип.
[05:38.860 --> 05:43.860]  Почему смена лидера приводит к... Ну, смена лидера тоже, в общем,
[05:43.860 --> 05:50.860]  какой-то внешний эффект протокола транзакций. Конфликты – это тоже следствие недетерминизма.
[05:52.860 --> 05:57.860]  Идея, про которую мы сейчас поговорим, называется детерминированной транзакцией.
[06:00.860 --> 06:06.860]  Она о том, что если из систем, которые исполняют эти кросс-шардовые транзакции,
[06:06.860 --> 06:15.860]  исключить источники недетерминизма, то и сделать так, чтобы транзакция применялась или откатывалась
[06:15.860 --> 06:20.860]  только потому, что не из-за каких-то внешних эффектов, эффектов среды,
[06:20.860 --> 06:28.860]  а только потому, что в ней явно выполнился комит и реапорт, то успех транзакции
[06:28.860 --> 06:32.860]  целиком определяется ее телом и текущим состоянием хранилища.
[06:32.860 --> 06:37.860]  Вот если мы сможем этого добиться, то тогда двухфазный комит нам просто будет не нужен.
[06:39.860 --> 06:48.860]  Вот давайте подумаем, как можно все эти источники недетерминированного отката транзакции
[06:48.860 --> 06:53.860]  на отдельных шардах исключить. Начнем с тайм-аутов.
[06:54.860 --> 06:58.860]  Почему нам нужен тайм-аут? Потому что у нас, кажется, транзакции интерактивные.
[06:59.860 --> 07:05.860]  То есть, шарт слушает запись транзакции, чтение транзакции, ожидает комита,
[07:05.860 --> 07:09.860]  но в какой-то момент это всего может не случиться, потому что просто клиент отказал.
[07:09.860 --> 07:12.860]  Нужно локи снять и транзакцию откатить.
[07:14.860 --> 07:18.860]  Но если проблема вне интерактивности, то давайте, чтобы избавиться вот этих тайм-аутов,
[07:18.860 --> 07:22.860]  просто потребуем, чтобы транзакции перестали быть интерактивными.
[07:22.860 --> 07:26.860]  Давайте сейчас сузим класс транзакций, который мы готовы исполнять,
[07:26.860 --> 07:30.860]  до транзакций, которые зафиксированы заранее.
[07:30.860 --> 07:35.860]  Если клиент работает с системой, то он просто пишет всю транзакцию на некотором языке,
[07:35.860 --> 07:38.860]  но это может быть какой-то декларативный язык, может быть функциональный язык,
[07:38.860 --> 07:41.860]  но в общем, какой-то специальный язык для описания транзакций,
[07:41.860 --> 07:44.860]  и отправляет ее сразу целиком в систему.
[07:45.860 --> 07:49.860]  То есть, да, мы, конечно, класс транзакций сузили, нас, наверное, это беспокоит,
[07:49.860 --> 07:52.860]  но вот такой анонс на будущее, мы эту проблему решим.
[07:52.860 --> 07:56.860]  Но если мы будем отталкиваться от того, что все наши транзакции зафиксированы
[07:56.860 --> 08:00.860]  с самого начала целиком, в смысле, зафиксированы, плохое слово,
[08:00.860 --> 08:03.860]  она может нас запутать, если транзакция известна сразу целиком,
[08:03.860 --> 08:08.860]  если она не интерактивная, то тайм-аутов уже не нужно.
[08:09.860 --> 08:13.860]  Эту проблему, это источник двухфасного комитта, можно исключить.
[08:14.860 --> 08:18.860]  Что со сменой ридера? Здесь чуть сложнее, я бы про это разговор отложил,
[08:18.860 --> 08:21.860]  в смысле, почему, как мы можем его избежать.
[08:21.860 --> 08:24.860]  Но вот поговорим про конфликты в двухфазных блокировках.
[08:24.860 --> 08:28.860]  Вот это, наверное, главный источник недетерминизма.
[08:29.860 --> 08:33.860]  И почему, откуда там недетерминизм берется?
[08:33.860 --> 08:37.860]  Авторы подхода с детерминированными транзакциями утверждают,
[08:37.860 --> 08:44.860]  разумная мысль вообще, что в двухфазные блокировки протокол смешивает сразу две задачи.
[08:45.860 --> 08:57.860]  В двухфазных блокировках транзакции одновременно исполняются и упорядочиваются.
[09:00.860 --> 09:02.860]  И вот это две разные задачи.
[09:02.860 --> 09:06.860]  Они как-то запускаются, они управляют свои операции,
[09:06.860 --> 09:11.860]  какие-то потоки там запускаются, переключаются, все это недетерминировано,
[09:11.860 --> 09:14.860]  и возникает какой-то порядок за эти блокировок,
[09:14.860 --> 09:19.860]  и в зависимости от того или иного порядка при исполнении этих транзакций
[09:19.860 --> 09:23.860]  возникает та или иная сериализация, которая нигде даже явно не представлена.
[09:24.860 --> 09:27.860]  То есть вот недетерминизм берется здесь,
[09:28.860 --> 09:30.860]  возникает вот в этом месте,
[09:34.860 --> 09:40.860]  и он приводит к тому, что у вас возникает какой-то порядок сериализации.
[09:42.860 --> 09:46.860]  В чем наша большая идея сегодня?
[09:46.860 --> 09:51.860]  Давайте выберем порядок сериализации заранее.
[09:51.860 --> 09:54.860]  Просто мы разделим два этих шага.
[09:55.860 --> 09:59.860]  Когда нам клиент присылает транзакцию, можно уже нарисовать некоторую схему.
[10:01.860 --> 10:03.860]  У нас будут клиенты.
[10:12.860 --> 10:16.860]  Они нам посылают свои транзакции целиком.
[10:21.860 --> 10:24.860]  И мы сразу же их упорядочиваем.
[10:27.860 --> 10:30.860]  Мы построим отдельный компонент, который называется секленсер.
[10:30.860 --> 10:35.860]  Его задача – получать транзакции от клиентов и просто выстраивать из них некоторые планы.
[10:42.860 --> 10:48.860]  Этот секленсер будет фиксировать какой-то определенный порядок,
[10:48.860 --> 10:51.860]  в котором транзакции нужно исполнить.
[10:51.860 --> 10:56.860]  Мы не полагаемся на 2P, что в ходе исполнения родится какая-то сериализация.
[10:56.860 --> 11:00.860]  Мы ее выбираем с самого начала, еще до того, как мы транзакции будем исполнять.
[11:02.860 --> 11:05.860]  Этот план, конечно, динамически достраивается.
[11:05.860 --> 11:08.860]  То есть приходит новый клиент с другими транзакциями, этот план продолжается.
[11:09.860 --> 11:14.860]  Мы хотим зафиксировать его.
[11:17.860 --> 11:23.860]  А дальше уже мы этот план исполнения транзакций отправим на отдельные шарды.
[11:27.860 --> 11:33.860]  Data shard 1, data shard 2, data shard 3.
[11:39.860 --> 11:45.860]  Каждый из них будет получать транзакции целиком и в одном и том же порядке.
[11:46.860 --> 11:50.860]  И дальше его задача – эти транзакции в этом порядке исполнить.
[11:51.860 --> 11:57.860]  Например, если у нас транзакция T2 выглядит так.
[11:58.860 --> 12:03.860]  Я пишу по ключу X и пишу по ключу Y.
[12:03.860 --> 12:07.860]  А транзакция T3 выглядит так.
[12:08.860 --> 12:18.860]  Я читаю ключ X.
[12:19.860 --> 12:25.860]  И если в нем какое-то значение, не будем углубляться, то я пишу ключ Z.
[12:29.860 --> 12:36.860]  Пусть каждый shard последовательно выполнит вот этот общий план транзакций.
[12:38.860 --> 12:42.860]  Если транзакции известны заранее, то если они пройдут через секвенсер,
[12:42.860 --> 12:47.860]  если мы запомнили эти транзакции, то никакие тайм-ауты клиентов нас уже не беспокоит.
[12:48.860 --> 12:52.860]  И если мы исполняем транзакции в некотором фиксированном заранее выбранном порядке,
[12:52.860 --> 12:57.860]  пока очень неэффективно последовательно, то у нас не бывает никаких откатов из-за того,
[12:57.860 --> 13:04.860]  что как-то неудачно взялись локи.
[13:04.860 --> 13:08.860]  Мы просто пока выполняем транзакции одна за другой.
[13:08.860 --> 13:13.860]  Мы написали очень неэффективный планировщик на каждом датэшарде.
[13:13.860 --> 13:20.860]  Ну и беспокоят ли нас перевыборы лидера и рестарты отдельных датэшардов?
[13:20.860 --> 13:23.860]  Мы считаем, что они отказаустойчивы.
[13:23.860 --> 13:27.860]  Он может перезагрузиться, но страшно ли это? Нет, не страшно,
[13:27.860 --> 13:35.860]  потому что в случае, когда перезагружался RSM здесь, он забывал все свои блокировки.
[13:35.860 --> 13:42.860]  Здесь же план исполнения транзакции останется, его нужно просто снова будет повторить.
[13:42.860 --> 13:48.860]  Вот кажется, не будет никаких источников, никаких причин транзакцию на одном шарде применять,
[13:48.860 --> 13:51.860]  а на другом откатывать.
[13:51.860 --> 13:55.860]  Просто если мы все вот эти источники недотриминизма исключим,
[13:55.860 --> 14:02.860]  то значит, каждый шард исполнит одну и ту же серию транзакций в одном и том же порядке одинаково.
[14:02.860 --> 14:07.860]  И все шарды будут согласованы друг с другом, и никаких недотриминированных откатов не будет.
[14:07.860 --> 14:12.860]  Значит, двухфазный комит не нужен. Понятная идея?
[14:12.860 --> 14:18.860]  Да, разумеется. Я, собственно, для этого все и рисовал.
[14:18.860 --> 14:23.860]  Тут нужно некоторое пояснение, как именно эти шарды будут исполнять транзакции.
[14:23.860 --> 14:27.860]  В первом приближении мы считаем, что каждый шард исполняет весь план транзакций.
[14:27.860 --> 14:35.860]  Разумеется, у нас шарды возникают по той причине, что они хотят разделить множество строчек, таблиц, множество ключей.
[14:35.860 --> 14:45.860]  Поэтому пусть шард 1 хранит ключ Y, шард 2 хранит ключ Y, шард 3 хранит ключ Z.
[14:45.860 --> 14:57.860]  Когда каждый шард получает транзакцию T2 от секундсера, то он просто выполняет свою часть записей.
[14:57.860 --> 15:06.860]  Этот шард ничего не делает, этот шард пишет в Y и пропускает запись X, а этот шард пишет в X и пропускает запись Y.
[15:06.860 --> 15:11.860]  Если мы говорим про третью транзакцию, то тут уже сложнее.
[15:11.860 --> 15:21.860]  Опять, шард 2 может ничего не делать, потому что его транзакция вообще не касается.
[15:21.860 --> 15:25.860]  Она не касается ключей, которые он хранит.
[15:25.860 --> 15:31.860]  Шард 3 записи делать не должен никаких, потенциально даже.
[15:31.860 --> 15:39.860]  Но с другой стороны, шард 1 не сможет выполнить эту транзакцию, пока у него нет результата чтения X с другого шарда.
[15:39.860 --> 15:43.860]  Но в таких случаях между ними должна быть коммуникация.
[15:43.860 --> 15:47.860]  Вот мы сюда отправим результат чтения X.
[15:47.860 --> 15:51.860]  Ой, что-то пошло не так в моей картинке.
[15:51.860 --> 15:55.860]  Где же стёрка? Я её не вижу.
[15:56.860 --> 16:00.860]  Стрелка должна быть в обратную сторону.
[16:10.860 --> 16:17.860]  То есть между шардами есть прямая коммуникация, но каждый шард проходит через одну и ту же серию транзакций,
[16:17.860 --> 16:25.860]  исполнитых одинаково, и, кажется, никаких ретраев, никаких конфликтов ничего нас не беспокоит.
[16:25.860 --> 16:31.860]  Вот такая общая схема, которая, кажется, позволяет избежать двухфазного коммита,
[16:31.860 --> 16:35.860]  только нужно разобраться в деталях, как это всё работает.
[16:35.860 --> 16:40.860]  Давайте начнём с каждого дата шарда.
[16:40.860 --> 16:47.860]  Я говорил вам, что мы хотим в качестве примера взять конкретную систему.
[16:47.860 --> 16:50.860]  Enix Database.
[16:50.860 --> 16:53.860]  И давайте я немного расскажу, как она устроена.
[16:53.860 --> 16:57.860]  Точнее, не то чтобы как она устроена, а из каких компонентов она состоит
[16:57.860 --> 17:02.860]  и по каким принципам эти компоненты написаны.
[17:02.860 --> 17:07.860]  Enix Database – это геораспределённая база данных,
[17:07.860 --> 17:14.860]  в которой есть изолированные сервизуемые транзакции, в которой есть, разумеется, отказ за устойчивость.
[17:14.860 --> 17:20.860]  И из каких компонентов она состоит? Она состоит из таблетов.
[17:20.860 --> 17:27.860]  Каждый таблет – это некоторый актор, в том смысле, в котором это называется в модели акторов,
[17:27.860 --> 17:32.860]  то есть это некоторый компонент, который общается с другими отправкой сообщений.
[17:32.860 --> 17:35.860]  Но этот актор является отказа устойчивым,
[17:35.860 --> 17:42.860]  потому что он построен поверх слоя хранения, поверх слоя Distributed Storage.
[17:42.860 --> 17:46.860]  Это называется таблет, это не обязательно кусочек таблицы.
[17:46.860 --> 17:51.860]  Таблеты бывают служебными, компонент системы самой, то есть ее внутренностей,
[17:51.860 --> 17:56.860]  и таблет, который отвечает за фрагмент данных пользователей, за фрагмент его таблицы.
[17:56.860 --> 18:02.860]  Но так или иначе все эти таблеты построены поверх распределённого хранилища,
[18:02.860 --> 18:07.860]  которое хранит некоторые имутабельные блобики порции данных.
[18:07.860 --> 18:14.860]  И вот поверх этого хранилища уже строятся отказаустойчивые таблеты, отказаустойчивые акторы.
[18:14.860 --> 18:20.860]  В этой системе реализован подход, про который я вам уже немного говорил в лекции про масштабируемость.
[18:20.860 --> 18:30.860]  Когда мы обсуждали Bigtable. Сейчас я найду подходящую картинку.
[18:30.860 --> 18:36.860]  В Yandex DB слой хранения и слой точек обслуживания разделен.
[18:36.860 --> 18:39.860]  То есть каждая машина, это с одной стороны некоторые ресурсы для хранения,
[18:39.860 --> 18:44.860]  а с другой стороны это некоторые процессоры для обслуживания каких-то действий команд пользователей
[18:44.860 --> 18:48.860]  или каких-то внутренних активностей.
[18:48.860 --> 18:51.860]  Слой хранения отвечает за имутабельные блобы.
[18:51.860 --> 18:58.860]  И поверх этого хранилища строится сущность, которая называется таблет, которая по смыслу похожа на RSM,
[18:58.860 --> 19:07.860]  то есть это некоторый отказаустойчивый узел. Но на самом деле это не то чтобы несколько реплик.
[19:07.860 --> 19:11.860]  Если у нас хранилища отказаустойчивая, то есть репликация на этом уровне находится,
[19:11.860 --> 19:17.860]  то у таблета физическое представление может быть одно, это одна единственная машина.
[19:17.860 --> 19:22.860]  Но если она отказывает, то мы просто перезапускаем таблет на другой физической машине,
[19:22.860 --> 19:26.860]  и он поднимает свое состояние из слоя distributed storage.
[19:26.860 --> 19:30.860]  Если вы помните, то таблет и Bigtable строились по тем же принципам.
[19:30.860 --> 19:40.860]  У нас есть GFS, в котором хранится лог и сестейблы для RSM, из которых состоит каждый кусочек таблицы Bigtable.
[19:40.860 --> 19:43.860]  И у нас есть просто одна машина, которая обслуживает эти данные,
[19:43.860 --> 19:47.860]  которая в памяти держит MemTable и пишет записи в Write и HeadLog.
[19:47.860 --> 19:51.860]  Вот тут смысл такой же. У нас хранение и вычисления разделены,
[19:51.860 --> 19:56.860]  и хранение выделено в отдельную подсистему. Это не файловая система, все же.
[19:56.860 --> 20:03.860]  Это более примитивная конструкция. То есть API существенно другое и более простое.
[20:03.860 --> 20:08.860]  Но смысл декомпозиции такой же. Мы отделяем хранение от точек обслуживания
[20:08.860 --> 20:13.860]  и используем просто машины как некоторый пул ресурсов у тех и других.
[20:19.860 --> 20:25.860]  И каждый шард, он является... Шарды бывают служебные.
[20:25.860 --> 20:28.860]  Есть шарды, которые обслуживают фрагмент таблицы.
[20:32.860 --> 20:34.860]  Вот есть какие-то другие примеры.
[20:34.860 --> 20:39.860]  И каждый шард наших таблиц будет таким вот таблицом.
[20:39.860 --> 20:44.860]  От отказа устойчивого. Если узел, который обслуживает этот шард, откажет,
[20:44.860 --> 20:49.860]  то он перейдет на другой узел и там запустится поднимет данные из DistributedStorage.
[20:49.860 --> 20:53.860]  Поэтому, скажем, Restart нас не очень волнует.
[20:57.860 --> 21:02.860]  В принципе, этого даже достаточно для того, чтобы про транзакции дальше говорить.
[21:02.860 --> 21:07.860]  Давайте пока вернемся на доску и по необходимости будем к слайдам возвращаться.
[21:07.860 --> 21:09.860]  Пока вроде нам этого не нужно.
[21:11.860 --> 21:13.860]  Вот каждый даташард надежен.
[21:17.860 --> 21:20.860]  Сиквенсер тоже должен быть отказоустойчивым, разумеется.
[21:20.860 --> 21:24.860]  То есть, когда к нему прилетают транзакции, он должен их надежно запоминать,
[21:24.860 --> 21:26.860]  ни в коем случае не забывать.
[21:27.860 --> 21:29.860]  И если все действительно отказоустойчиво,
[21:29.860 --> 21:34.860]  и все работает вот ровно так, как нарисовано, то очевидно,
[21:34.860 --> 21:38.860]  все транзакции исполняются одинаково на каждом шарде,
[21:38.860 --> 21:42.860]  с необходимостью коммуникации, и необходимости в двухфазном комиссии не возникает.
[21:42.860 --> 21:46.860]  Нужно обсудить какие-то еще детали, а именно про реализацию сиквенсера.
[21:46.860 --> 21:48.860]  Но как бы вы делали сиквенсер?
[21:49.860 --> 21:52.860]  К нему прилетают транзакции, и он должен их упорядочивать.
[21:53.860 --> 21:56.860]  Но можно себе представить какой-то multipax, band奶,
[21:56.860 --> 21:59.860]  но что-то, что реплицирует какой-то лог.
[21:59.860 --> 22:02.860]  Реплицирует автомат, но у нас данные,
[22:02.860 --> 22:04.860]  это просто последовательность транзакций,
[22:04.860 --> 22:07.860]  поэтому мы реплицируем прямо лог.
[22:07.860 --> 22:09.860]  Но с другой стороны, сложно представить,
[22:09.860 --> 22:15.860]  что наurate эффект�ни не разделяет с тиманистами,
[22:15.860 --> 22:18.860] gy realmente в harvested input.
[22:18.860 --> 22:20.860]  Иwaiting ив const,
[22:20.860 --> 22:28.860]  Но с другой стороны, сложно представить, что даже один RSM сможет пережить все транзакции, которые есть в системе.
[22:28.860 --> 22:33.860]  Сложно представить, что в него они все поместятся, он будет успевать.
[22:33.860 --> 22:48.860]  Можно ли как-то шардировать секвенсор?
[22:48.860 --> 22:57.860]  Заведем несколько экземпляров, и когда будут приходить клиенты с какими-то транзакциями,
[22:57.860 --> 23:05.860]  то каждый секвенсор, он отказоустойчивый, но это несколько разных секвенсоров, они строят разные порядки,
[23:05.860 --> 23:08.860]  разные планы для разного набора транзакций.
[23:08.860 --> 23:20.860]  Каждая транзакция, каждый клиент выбирает себе какого-то секвенсора, но не знаю, похешу от транзакции.
[23:20.860 --> 23:22.860]  Как устроен каждый секвенсор?
[23:22.860 --> 23:26.860]  Но он, конечно, не может построить план на бесконечность, он строит его порциями.
[23:26.860 --> 23:33.860]  Он открывает опять такое пяти миллисекундное окно и копит транзакции, которые в него падают, каким-то образом их упорядочивает,
[23:33.860 --> 23:39.860]  не важно каким, и получает кусочек общего плана.
[23:39.860 --> 23:43.860]  Первый кусочек плана секвенсора 1.
[23:43.860 --> 23:58.860]  Второй кусочек плана секвенсора 1.
[23:58.860 --> 24:10.860]  Б, потому что, очевидно, батч.
[24:10.860 --> 24:17.860]  Ну это здорово, но всем шардам нужно знать про общий план исполнения транзакции.
[24:17.860 --> 24:25.860]  А у них есть много разных планов для разного набора транзакций. Как быть?
[24:25.860 --> 24:37.860]  Нужно в каком-то порядке все это обойти. Предлагается обойти, скажем, вот так.
[24:37.860 --> 24:47.860]  То есть можно себе представить, что каждый даташард, каждый такой отказоустойчивый компонент, который отвечает за кусочек пользовательской таблицы,
[24:47.860 --> 24:58.860]  слушает всех секвенсоров, раз 5 миллисекунд получает от них очередной пачку и выстраивает их в некоторый общий план, после чего применяет.
[24:58.860 --> 25:06.860]  Набор секвенсоров зафиксирован, так что все даташарды выстраивают один и тот же план.
[25:06.860 --> 25:16.860]  Будет ли это масштабироваться? Кажется, что нет, потому что даташардов у нас может быть очень много, у нас могут быть их миллионы.
[25:16.860 --> 25:27.860]  И мы не сможем дать такую нагрузку на секвенсора, чтобы он посылал постоянно на миллион машин вот такие планы.
[25:27.860 --> 25:35.860]  Нам нужно какое-то промежуточное звено внести в эту конструкцию.
[25:35.860 --> 25:49.860]  Сделано так. У нас есть набор секвенсоров, есть шарды, их очень много.
[25:49.860 --> 26:00.860]  И мы между этими секвенсорами и этими шардами ставим набор, они называются медиаторы.
[26:00.860 --> 26:12.860]  Каждый секвенсор отправляет свою часть плана медиатором, всем медиатором.
[26:12.860 --> 26:18.860]  Каждый медиатор собирает у себя вот эту последовательность, уже полный план.
[26:18.860 --> 26:28.860]  И каждый даташард потом хэшот своего кого-то идентификатора, выбирает себе того медиатора, которого он будет слушать.
[26:28.860 --> 26:37.860]  В итоге число секвенсоров выбирается просто исходя из нагрузки, из количества транзакций в секунду, чтобы они суммарно могли пережить этот поток.
[26:37.860 --> 26:47.860]  А число медиаторов выбирается исходя из количества даташардов, чтобы мы могли распределять все эти планы между всеми шардами.
[26:47.860 --> 26:49.860]  Понятно?
[26:49.860 --> 26:56.860]  Таким образом у нас получается такой лишний хоп, мы не сразу от секвенсора в даташард приходим, а через дополнительную группировку медиаторов.
[26:56.860 --> 27:02.860]  Но таким образом мы достигаем масштабируемости.
[27:02.860 --> 27:05.860]  Хорошо. Такой вопрос.
[27:05.860 --> 27:11.860]  А что такое 1, 2 и 3? Это просто монотонные числа или нет?
[27:11.860 --> 27:13.860]  Можно ли это сделать просто монотонными числами?
[27:13.860 --> 27:17.860]  Но в конце концов, каждый секвенсор, если он реплицирован, он может...
[27:17.860 --> 27:24.860]  Ну, если это RSM в таком простом понимании, хотя это не совсем технически верно в случае Яндекс.ДБ,
[27:24.860 --> 27:34.860]  может ли он буквально назначать последовательные номера этим пачкам, реплицировать их внутри себя с помощью мультипаксуса и отдавать это все медиаторам?
[27:34.860 --> 27:36.860]  Оказывается, что не может.
[27:36.860 --> 27:38.860]  И смотрите почему.
[27:38.860 --> 27:48.860]  Каждый секвенсор отказоустойчивый, но он может быть некоторое время недоступен, потому что сломалась конкретная машина и нужно перенести работу на другую машину.
[27:48.860 --> 27:54.860]  Мы ничего не потеряем, но при этом мы некоторое время работать не будем.
[27:54.860 --> 27:59.860]  И в итоге мы некоторое время не будем генерировать вот эти вот пачки.
[27:59.860 --> 28:03.860]  А без них нельзя собрать общий план.
[28:03.860 --> 28:06.860]  Проблема понятна?
[28:06.860 --> 28:13.860]  И в итоге, если какой-то секвенсор вдруг будет опаздывать сильно, в смысле будет недоступен некоторое время,
[28:13.860 --> 28:18.860]  то у нас все сильно замедлится и мы будем ждать вот именно его очень долго.
[28:18.860 --> 28:30.860]  Поэтому Яндекс.ДБ на самом деле используются здесь не такие вот абстрактные индексы, а прямо физическое время.
[28:30.860 --> 28:40.860]  Если секвенсор перезагружается, переезжает на другую машину, и он понимает, что он не работал там 10 секунд,
[28:40.860 --> 28:48.860]  то он просто посылает всем медиаторам пустые пачки за вот этот интервал мгновенно.
[28:48.860 --> 28:53.860]  Он не мог так сделать в случае переезда с такими вот индексами,
[28:53.860 --> 28:59.860]  потому что он не знает, какой сейчас верхний индекс, то есть сколько он может закрыть себя.
[28:59.860 --> 29:06.860]  Тут возникает такая задача, можно это делать с помощью фактической синхронизации часов с другими репликами,
[29:06.860 --> 29:09.860]  но это в этом смысла уже нет, потому что есть NTP и время просто.
[29:09.860 --> 29:18.860]  Поэтому мы в качестве индекса патчей используем именно физическое время и там каждые 5 миллисекунд открываем новую патчу.
[29:18.860 --> 29:30.860]  Тут от того, что часы не синхронизированы, не может произойти ничего плохого.
[29:30.860 --> 29:36.860]  То есть, может быть, какой-то секвенсор отстанет и тогда все будут немного на нем тормозить,
[29:36.860 --> 29:40.860]  но, скорее всего, NTP тут просто проблемы решит сам по себе.
[29:40.860 --> 29:46.860]  Никакой монотонности или какого-то вот эпсилона между часами здесь не требуется.
[29:46.860 --> 29:52.860]  На корректность это не влияет все.
[29:52.860 --> 29:59.860]  И того, от тайм-аутов мы избавились, потому что клиент сразу отправляет запрос в систему,
[29:59.860 --> 30:06.860]  и мы его здесь запоминаем, просто реплицируем, и тайм-аут нас здесь не беспокоит.
[30:06.860 --> 30:15.860]  От конфликтов, откатов на уровне 2PL мы избавляемся, потому что мы исполняем транзакции последовательно
[30:15.860 --> 30:21.860]  по заранее созданному плану, разделяя вот эти две подзадачи.
[30:21.860 --> 30:31.860]  И от переезда рестартов отдельных даташардов мы спасаемся тем, что он перезагрузился
[30:31.860 --> 30:36.860]  и начал с того места, где он остановился.
[30:36.860 --> 30:40.860]  То есть, все источники имидотерминизма устранены, а значит, устранена причина,
[30:40.860 --> 30:43.860]  по которой мы делали двухфазный комит.
[30:43.860 --> 30:48.860]  И в этой конструкции все кажется в этом смысле оптимальней.
[30:48.860 --> 30:54.860]  Но все же пока не до конца, потому что, с одной стороны, здесь все отказуустойчивое,
[30:54.860 --> 30:57.860]  и вот масштабируемое мы это обсудили, как сделать.
[30:57.860 --> 31:02.860]  А с другой стороны, каждый даташард в нашей схеме выполняет транзакции просто одна за другой,
[31:02.860 --> 31:07.860]  последовательно, очень непараллельно.
[31:07.860 --> 31:14.860]  Вопрос, можно ли добиться параллельности на этом уровне?
[31:14.860 --> 31:18.860]  Мы сделаем это с помощью вариации двухфазных блокировок.
[31:18.860 --> 31:23.860]  Вообще алгоритм двухфазных блокировок предназначен для того, чтобы с помощью локов
[31:23.860 --> 31:30.860]  построить некоторые, выстроить транзакции в некотором порядке.
[31:30.860 --> 31:35.860]  Авторы детерминированных транзакций предлагают некоторую вариацию 2PL,
[31:35.860 --> 31:44.860]  которая тоже берет блокировки, которая тоже позволяет транзакциям исполняться все же параллельно,
[31:44.860 --> 31:50.860]  но при этом гарантирует, что она порождает не какую-то сериализацию неизвестную,
[31:50.860 --> 31:56.860]  а вот конкретно вот эту, которая нам спущена сверху.
[31:56.860 --> 32:08.860]  Для этого используется протокол, который называется детерминированные блокировки.
[32:08.860 --> 32:17.860]  Давайте я это покажу на экране.
[32:17.860 --> 32:24.860]  То, что я рассказывал про секвенсер и про даташарды, это некоторые детали устройства Яндекс.ДБ.
[32:24.860 --> 32:30.860]  А то, про что я говорю сейчас, это задумка, которая была в оригинальной статье,
[32:30.860 --> 32:36.860]  и оригинальная статья называется Kelvin.
[32:36.860 --> 32:43.860]  Авторы этой статьи не поленились и написали такую референсную реализацию на C++,
[32:43.860 --> 32:52.860]  она абсолютно не продакшн качества, но об этом можно судить даже по описанию коммита.
[32:52.860 --> 32:58.860]  Но это не важно, есть специально обычные люди, которые могут написать промышленную систему по такому скромному описанию.
[32:58.860 --> 33:04.860]  Но, тем не менее, здесь просто есть реализация вот этого детерминированного менеджера блокировок,
[33:04.860 --> 33:10.860]  который раньше просто получал отдельные команды, брал блокировку и порождал какое-то расписание сериализуемое.
[33:10.860 --> 33:18.860]  А в Kelvin этот менеджер порождает конкретное расписание. Каким образом?
[33:18.860 --> 33:25.860]  Он разделяет внутри 2.pl взятие блокировок и исполнение транзакций.
[33:25.860 --> 33:35.860]  Взятие блокировок происходит в отдельном потоке, а исполнение происходит уже многопоточно.
[33:35.860 --> 33:42.860]  Вот прям класс Shadower, который отвечает за исполнение транзакций.
[33:42.860 --> 33:47.860]  В нем запускается много потоков, которые транзакции готовы исполнять, в смысле делать чтение и записи,
[33:47.860 --> 33:51.860]  и один поток, который будет работать с блокировками.
[33:51.860 --> 34:01.860]  Принцип такой.
[34:01.860 --> 34:09.860]  Если в плане, который нужно исполнить, есть две транзакции, одна из которых предшествует другой, разумеется,
[34:09.860 --> 34:20.860]  ИТ предшествует житой, и ИТ и житая пересекаются по набору ключей, то есть по набору блокировок, которые они должны взять.
[34:20.860 --> 34:33.860]  То мы делаем так. Мы требуем, чтобы транзакция, которая идет раньше, получила все свои блокировки до транзакции, которая идет позже.
[34:33.860 --> 34:39.860]  Делается это так. Мы просто скамливаем лог-менеджеру транзакции по порядку,
[34:39.860 --> 34:54.860]  и лог-менеджер просто обходит все блокировки для транзакции, которые должны быть для транзакции, и встает на них в очередь.
[34:54.860 --> 34:59.860]  Может быть, после этого транзакция готова сразу запуститься, потому что в очереди для всех блокировок никого не было.
[34:59.860 --> 35:07.860]  Тогда она бросается в тредпул, и там исполняется. Если же хотя бы на одной блокировке мы встали в очередь, то мы ждем.
[35:07.860 --> 35:18.860]  Из этого следует, что если есть житая транзакция, которая следует в плане позже, то она на той блокировке, которую она разделяет с ИТ транзакцией,
[35:18.860 --> 35:26.860]  обязательно окажется в очереди позже, поэтому блокировки строго позже первой.
[35:26.860 --> 35:33.860]  Не бывает такого, чтобы мы одну блокировку ИТ транзакции взяли раньше, чем житой, а другую взяли позже, чем житой.
[35:33.860 --> 35:45.860]  Это понятно? Дедлогов не бывает, не бывают конфликтов, по причине которых нужно транзакции откатывать,
[35:45.860 --> 35:59.860]  И исполнение в таком планировщике с таким лок-менеджером будет гарантировать, что это по смыслу 2PL, потому что мы берем все локи и только после этого что-то делаем.
[35:59.860 --> 36:06.860]  В 2PL классическом мы берем локи и сразу пишем и читаем, а здесь мы сначала берем все локи, а потом пишем и читаем.
[36:06.860 --> 36:13.860]  Но на свойства 2PL это не влияет. Мы получаем стерилизуемое расписание, причем ровно такое, которое мы хотели.
[36:13.860 --> 36:22.860]  Таким образом мы все-таки можем добиться параллелизма некоторого, но сохранить детерминированный порядок и избежать откатов.
[36:22.860 --> 36:28.860]  Вот в такой схеме, в таком исполнении внутри самой системы откатов нигде не возникает.
[36:28.860 --> 36:35.860]  Вот транзакция может откатиться только потому, что в ней явно пользователь написал abort transaction.
[36:35.860 --> 36:40.860]  Если этого нет, то транзакция успешно применяется везде.
[36:43.860 --> 36:52.860]  Ну а давайте теперь подумаем, в чем подвох.
[36:52.860 --> 36:57.860]  Почему так сделать не очень просто?
[36:57.860 --> 37:01.860]  Ну вот такой lock manager написать не очень просто.
[37:01.860 --> 37:09.860]  Точнее, написать ты его просто, он был маленький, умещался в 100 строчек.
[37:09.860 --> 37:16.860]  Почему нам как пользователям это все сильно усложняет жизнь?
[37:16.860 --> 37:24.860]  Вот мы сейчас используя такой подход к блокировкам, то есть сначала все блокировки взять, а потом транзакцию исполнить.
[37:24.860 --> 37:28.860]  Смотрите, мы здесь декомпозировали 2PL сначала.
[37:28.860 --> 37:33.860]  Мы выполнили ordering вообще на уровне выше, в отдельном компоненте.
[37:33.860 --> 37:38.860]  А потом еще в пределах даташарда мы декомпозировали взятие блокировок и само исполнение.
[37:38.860 --> 37:42.860]  То есть мы на 3 стадии декомпозировали этот протокол.
[37:42.860 --> 37:46.860]  Но когда мы так сделали, мы заработали пару ограничений.
[37:46.860 --> 37:55.860]  Вот первое ограничение возникло здесь, потому что мы должны заранее сконструировать целиком транзакцию, дать ее системе.
[37:55.860 --> 37:58.860]  А второе ограничение возникло здесь.
[37:58.860 --> 38:04.860]  Вот планировщик должен сначала взять локи, а потом исполнить транзакцию.
[38:04.860 --> 38:09.860]  Но давайте пример я нарисую, тогда станет понятно.
[38:12.860 --> 38:17.860]  Вот мы с этим поборолись, уже можно это забыть.
[38:20.860 --> 38:27.860]  Пусть наша транзакция выглядит так, чтобы еще удалить.
[38:34.860 --> 38:43.860]  Какая это транзакция?
[38:55.860 --> 38:59.860]  Это транзакция, которая работает с вторичным индексом.
[38:59.860 --> 39:03.860]  Вот у нас есть таблица, мы можем обращаться к ее строкам по первичному ключу.
[39:03.860 --> 39:07.860]  Но ведь мы не знаем первичного ключа, и мы должны взять какой-то вспомогательный индекс,
[39:07.860 --> 39:12.860]  по нему прочитать, узнать первичный ключ и по нему записать.
[39:12.860 --> 39:15.860]  Вот мы не можем для такой транзакции воспользоваться таким протоколом,
[39:15.860 --> 39:19.860]  потому что в этом протоколе сначала берутся блокировки, а потом транзакция исполняется.
[39:19.860 --> 39:25.860]  Но пока мы транзакцию не исполнили, мы не знаем, чему равен Y.
[39:25.860 --> 39:30.860]  И надо как-то с этим быть.
[39:30.860 --> 39:34.860]  Для этого мы решаем еще одно ограничение.
[39:34.860 --> 39:36.860]  С одной стороны, не интерактивность, которая появляется здесь,
[39:36.860 --> 39:38.860]  потому что мы хотим избавиться от сессии тайм-аутов.
[39:38.860 --> 39:44.860]  Вторая проблема в том, что мы должны знать для транзакции ее read-write-set.
[39:44.860 --> 39:48.860]  То есть заранее на уровне дата шарда мы должны знать,
[39:48.860 --> 39:52.860]  какие ключи эта транзакция собирается читать и писать.
[39:52.860 --> 39:56.860]  Но это полезно еще не только потому, что мы здесь не сможем ничего сделать,
[39:56.860 --> 40:00.860]  а еще и потому, что мы просто не хотим нагружать шарды,
[40:00.860 --> 40:04.860]  которые транзакция не нужна, знанием про эту транзакцию.
[40:04.860 --> 40:06.860]  То есть тоже можно кое-что пооптимизировать.
[40:06.860 --> 40:09.860]  Я про это чуть позже, наверное, скажу.
[40:09.860 --> 40:13.860]  Но вот в этом месте нам точно нужно знать X и Y.
[40:13.860 --> 40:15.860]  Y мы не знаем.
[40:15.860 --> 40:17.860]  Вот это, к счастью, можно обойти.
[40:17.860 --> 40:22.860]  Довольно изящно, но смотрите, если вы не догадались еще.
[40:22.860 --> 40:25.860]  Мы эту транзакцию выполняем в два шага.
[40:25.860 --> 40:29.860]  На первом шаге мы выполняем оптимистичное чтение.
[40:29.860 --> 40:32.860]  Просто идем в шарт, который содержит ключ X,
[40:32.860 --> 40:38.860]  и вот, минуя секвенсер, минуя вот этот заранее план исполнения транзакции,
[40:38.860 --> 40:42.860]  просто приходим и из шарда читаем по ключу X.
[40:42.860 --> 40:46.860]  Может быть, это значение тут же устареет, мы не знаем.
[40:46.860 --> 40:52.860]  А дальше, на втором шаге, мы выпускаем такую служебную транзакцию,
[40:52.860 --> 40:55.860]  Y', которая строена так.
[41:16.860 --> 41:19.860]  Мы читаем текущее значение по ключу X.
[41:19.860 --> 41:22.860]  Если это попадает с ожидаемым значением, то мы пишем по Y'.
[41:22.860 --> 41:26.860]  И вот X и Y' это то, что известно из тела транзакции,
[41:26.860 --> 41:31.860]  то, что система может сама узнать и взять соответствующие блокировки.
[41:31.860 --> 41:36.860]  Но теперь, правда, у нас транзакция может откатиться.
[41:36.860 --> 41:40.860]  Здесь никакой логики отката не было, здесь она появилась,
[41:40.860 --> 41:44.860]  но все же этот откат детерминированный.
[41:44.860 --> 41:48.860]  То есть он произойдет на каждом шарде, который эту транзакцию исполняет.
[41:48.860 --> 41:53.860]  Этот откат по воле самой транзакции происходит.
[41:53.860 --> 41:57.860]  Нигде внутри системы никакие детали реализации не приводят к тому,
[41:57.860 --> 42:01.860]  что транзакция откатывается против воли пользователя.
[42:01.860 --> 42:04.860]  Но при этом все-таки откаты бывают, рестарты бывают.
[42:04.860 --> 42:09.860]  Но, опять же, они детерминированные.
[42:09.860 --> 42:22.860]  Имея такую идею, можно ее обобщить и использовать для того, чтобы побороть неинтерактивность.
[42:22.860 --> 42:28.860]  Вот наши транзакции неинтерактивны, мы должны их отправить заранее в систему.
[42:28.860 --> 42:32.860]  Но что если мы хотим с транзакциями работать так?
[42:32.860 --> 42:36.860]  Прочитали, подумали, записали, подумали, что-то еще сделали.
[42:36.860 --> 42:41.860]  То есть у нас транзакция состоит из шага start transaction, как обычно,
[42:41.860 --> 42:47.860]  потом записи, чтение, потом commit и re-abort.
[42:57.860 --> 42:59.860]  Вот стартуем транзакцию.
[42:59.860 --> 43:04.860]  Когда мы что-то читаем, мы снова делаем оптимистичное чтение.
[43:04.860 --> 43:08.860]  Когда мы пишем, мы запоминаем запись.
[43:08.860 --> 43:13.860]  Когда мы доходим до точки комита, мы выпускаем служебную,
[43:13.860 --> 43:18.860]  детерминированную транзакцию, которая перечитывает все, что мы читали,
[43:18.860 --> 43:23.860]  сравнивает значение с тем, что мы прочитали оптимистично,
[43:23.860 --> 43:28.860]  и если все сошлось, то мы все записываем.
[43:28.860 --> 43:31.860]  То есть та же самая идея.
[43:31.860 --> 43:36.860]  По сути мы делаем мульти-касс.
[43:36.860 --> 43:40.860]  Вот если вы помните, как мы делали, как сделать фич-ет с помощью
[43:40.860 --> 43:42.860]  операции Comperexchange на томиках.
[43:42.860 --> 43:46.860]  Прочитать, а потом сделать касс, плюс один, если совпало.
[43:46.860 --> 43:49.860]  Это же по смыслу то же самое.
[43:49.860 --> 43:55.860]  И это просто такой мульти-касс, потому что мы трогаем теперь много ключей.
[43:55.860 --> 43:59.860]  Но если мы что-то ретраем, нужно думать про гарантии прогресса.
[43:59.860 --> 44:03.860]  В случае двухвазных блокировок, где у нас были ретраи, то мы сохраняли
[44:03.860 --> 44:06.860]  таймстемп, и все-таки каждая транзакция рано или поздно комитилась.
[44:06.860 --> 44:10.860]  Вот здесь каждая транзакция может в принципе ретравиться вечно.
[44:10.860 --> 44:15.860]  Но важно то, что в каждой интерактивной транзакции есть только одна
[44:15.860 --> 44:18.860]  такая детерминированная транзакция в конце.
[44:18.860 --> 44:24.860]  И если она проваливается, то это просто означает, что какие-то ключи
[44:24.860 --> 44:29.860]  изменились, то есть какая-то другая транзакция совершила прогресс.
[44:29.860 --> 44:34.860]  Поэтому локального прогресса нет, но глобальные все же есть.
[44:34.860 --> 44:37.860]  Этого достаточно.
[44:37.860 --> 44:41.860]  Ну вот такая конструкция.
[44:41.860 --> 44:44.860]  Да.
[44:52.860 --> 44:58.860]  Смотри, мы в этом протоколе требуем от транзакции...
[44:58.860 --> 45:01.860]  Какая-то теористика.
[45:01.860 --> 45:06.860]  Тут шарды, они как-то поделены, таблица поделена на какие-то фрагменты.
[45:06.860 --> 45:10.860]  И какую транзакцию пользователь напишет, таким шардам она и достанется.
[45:10.860 --> 45:13.860]  Не то, чтобы мы этим как-то управляем.
[45:13.860 --> 45:18.860]  Тут скорее речь про то, как именно вот эту коммуникацию выстроить
[45:18.860 --> 45:21.860]  и как именно доводить транзакции до шардов.
[45:21.860 --> 45:23.860]  Вот про это я не говорил, а про последнее скажу.
[45:23.860 --> 45:26.860]  Вот здесь как будто бы клиент посылает транзакцию прямо в Sequencer,
[45:26.860 --> 45:31.860]  Sequencer ее прогоняет через консенсус, там еще передает его медиаторам,
[45:31.860 --> 45:33.860]  медиаторам, короче, очень долго.
[45:33.860 --> 45:36.860]  А транзакция что такое? Это же работа с данными.
[45:36.860 --> 45:39.860]  И данных может быть много.
[45:39.860 --> 45:42.860]  Что на самом деле делает Yandex DB?
[45:42.860 --> 45:45.860]  Клиент пишет свою транзакцию.
[45:45.860 --> 45:48.860]  Транзакция, чтобы быть обработанной всей этой конструкцией,
[45:48.860 --> 45:52.860]  должна быть детерминированной, должна быть доступна целиком.
[45:52.860 --> 46:00.860]  И в ней должны быть написаны, из ее тела можно извлечь все ключи,
[46:00.860 --> 46:02.860]  которые она пишет и читает.
[46:02.860 --> 46:06.860]  Поэтому клиент, ну, рякаем про Окси на входе в систему,
[46:06.860 --> 46:13.860]  берет эту транзакцию, генерирует ей какой-то уникальный идентификатор,
[46:13.860 --> 46:17.860]  из нее достает все ключи, которым она обращается,
[46:17.860 --> 46:20.860]  и данные отправляет вот сюда,
[46:20.860 --> 46:24.860]  на все шарды, которые эта транзакция затрагивает.
[46:24.860 --> 46:26.860]  Это потенциально много данных.
[46:26.860 --> 46:34.860]  И каждый шарт должен надежно сохранить свою порцию данных транзакции.
[46:34.860 --> 46:37.860]  Это очень интересным эффектом приведет сейчас.
[46:37.860 --> 46:41.860]  После того, как каждый шарт, который в транзакции участвует,
[46:41.860 --> 46:46.860]  надежно сохранил свою часть транзакции, свои данные,
[46:46.860 --> 46:52.860]  транзакция, ну, просто идентификатор транзакции фиксируется в секвенсоре,
[46:52.860 --> 46:55.860]  встраивается в план и план уже рассылается.
[46:55.860 --> 46:59.860]  То есть мы данные много раз через весь этот конверт не прогоняем.
[46:59.860 --> 47:03.860]  Но по смыслу мы получаем фактически двухвазный комит здесь.
[47:03.860 --> 47:06.860]  Потому что раньше у нас была подготовка транзакции,
[47:06.860 --> 47:12.860]  а теперь мы сохраняем ее данные надежно на каждый шарт,
[47:12.860 --> 47:14.860]  перед тем, как транзакцию дальше запустить.
[47:14.860 --> 47:17.860]  Потому что если какой-то шарт ее забудет, а мы уже встроили ее в план,
[47:17.860 --> 47:21.860]  то понятно, что все развалится.
[47:21.860 --> 47:24.860]  Так что, как говорят сами разработчики Yandex DB,
[47:24.860 --> 47:27.860]  у них по эффективности, по количеству раунд-trip,
[47:27.860 --> 47:31.860]  по лутентности это примерно одно и то же, что и двухвазный комит.
[47:31.860 --> 47:36.860]  Но зато в их системе нет ретраев.
[47:36.860 --> 47:40.860]  И это повышает пропускную способность.
[47:40.860 --> 47:46.860]  То есть просто меньше бесполезных попыток делается, больше всего...
[47:46.860 --> 47:49.860]  Конечно, это зависит от того, какого типа у нас транзакции.
[47:49.860 --> 47:54.860]  Если у нас все клиенты интерактивные, то, наверное, в этом пользы не будет никакой.
[47:54.860 --> 47:59.860]  Потому что мы просто перенесли ретраи.
[47:59.860 --> 48:02.860]  Трудно сказать, это не одно и то же, конечно,
[48:02.860 --> 48:07.860]  но ретраи по другой причине происходят.
[48:07.860 --> 48:12.860]  Но смысл примерно такой же, что мы что-то пробуем делать, не получилось, пробуем заново.
[48:12.860 --> 48:18.860]  Но не все сценарии применения, баз данных и транзакций,
[48:18.860 --> 48:22.860]  требуют на самом деле интерактивности.
[48:22.860 --> 48:27.860]  Я вам про это ничего не рассказывал, потому что все успеть невозможно.
[48:27.860 --> 48:35.860]  Про еще один протокол, который был реализован в Google еще до Spanner.
[48:35.860 --> 48:39.860]  Сейчас я его найду.
[48:48.860 --> 48:52.860]  Это такая каноническая реализация и зарядцы снапшотов поверх Bigtable.
[48:52.860 --> 48:55.860]  В Bigtable есть транзакции однострочные.
[48:55.860 --> 48:57.860]  И вот прям реализован двухфазный коммит.
[48:57.860 --> 49:00.860]  Если вы хотите разобраться, как именно он написан, то он буквально в коде.
[49:00.860 --> 49:06.860]  Но тут интересно, что транзакции реализованы на стороне клиента.
[49:06.860 --> 49:08.860]  То есть клиент является координатором.
[49:08.860 --> 49:11.860]  Сам Bigtable про эти транзакции ничего не знает.
[49:11.860 --> 49:18.860]  А значит, клиент может упасть и вместе с собой упасть в середине коммита.
[49:18.860 --> 49:21.860]  И для того, чтобы пережить такого клиента, сама транзакция должна быть lock free.
[49:21.860 --> 49:24.860]  То есть если она упала по середине, то должна быть другая транзакция,
[49:24.860 --> 49:28.860]  которая способна допинать то, что она видит, незаконченное.
[49:28.860 --> 49:30.860]  Я забыл, почему я начал говорить про Bigtable.
[49:30.860 --> 49:33.860]  Про это, честно говоря, должна быть какая-то причина.
[49:33.860 --> 49:37.860]  А, про то, что не все сценарии требуют интерактивных транзакций.
[49:37.860 --> 49:43.860]  Так вот, этот протокол реализован снаружи системы, поэтому он заведомо менее эффективен.
[49:43.860 --> 49:45.860]  И авторы об этом пишут.
[49:45.860 --> 49:50.860]  Но они говорят, что у них такая задача, где на самом деле эффективность нас не очень беспокоит.
[49:50.860 --> 49:54.860]  Google пишет поискового краулера.
[49:54.860 --> 49:57.860]  Мы обходим интернет.
[49:57.860 --> 50:01.860]  У нас есть гигантская таблица, где мы храним по рулам документы интернета
[50:01.860 --> 50:08.860]  и по хэшам от документов храним рулы, чтобы склеивать какие-то дубли.
[50:08.860 --> 50:14.860]  И для того, чтобы при обходе фиксировать в какой-то большой таблице новый документ,
[50:14.860 --> 50:18.860]  мы выполняем вот такую транзакцию.
[50:18.860 --> 50:24.860]  Так вот, здесь, в этой транзакции нет никакой интерактивности.
[50:24.860 --> 50:27.860]  У нас есть документ, у него есть URL, мы можем посчитать хэш.
[50:27.860 --> 50:32.860]  А дальше нам нужно по URL что-то записать и по хэшу прочитать, записать.
[50:32.860 --> 50:35.860]  Вот эта транзакция, она вполне может быть не интерактивной.
[50:35.860 --> 50:40.860]  Мы можем бросать ее в систему, и система будет обходиться вообще без ретраев.
[50:40.860 --> 50:44.860]  И пропускная способность в этой системе будет выше.
[50:44.860 --> 50:47.860]  Она совершает просто больше полезной работы.
[50:53.860 --> 50:56.860]  Более того, мы даже можем не дожидаться результатов.
[50:56.860 --> 51:00.860]  Как только транзакция была зафиксированна на каждом шарде и в секвенсере,
[51:00.860 --> 51:05.860]  то все, мы можем уходить, потому что мы знаем, что ни что не помешает дальше ей исполниться.
[51:05.860 --> 51:09.860]  От клиента мы уже ничего не ждем, никакого там таймаута, никаких хардбитов.
[51:09.860 --> 51:13.860]  Каждый шарт может перезагрузиться, но у него остается тот же план запросов,
[51:13.860 --> 51:16.860]  который он может получить с недиатора и проиграть его заново.
[51:16.860 --> 51:23.860]  Ну и конфликтов на уровне исполнения транзакции, на уровне планировщика и лог-менеджера тоже нет.
[51:28.860 --> 51:30.860]  Ну что, вот такая история.
[51:32.860 --> 51:36.860]  Мне кажется, что у этого подхода есть, в этом подходе есть своя красота,
[51:36.860 --> 51:41.860]  потому что все-таки какой-нибудь ваундвейт, то есть вот эти откаты транзакций
[51:41.860 --> 51:44.860]  из-за конфликта блокировок выглядят как некоторые кастели.
[51:44.860 --> 51:48.860]  Вот мы их изъяли, изъяли весь недотерминизм, добавили два ограничения
[51:48.860 --> 51:53.860]  на то, чтобы знать Read-Write-Set и на то, чтобы исключить интерактивность.
[51:53.860 --> 51:57.860]  И с этими ограничениями мы решили задачу гораздо проще
[51:57.860 --> 52:04.860]  и получили решение, которое на самом деле гораздо более модульное, чем Spanner.
[52:04.860 --> 52:10.860]  Ну вот в Spanner-е там был TRSM, там лог-менеджеры, вот мы говорили, что там где-то в памяти что-то живет,
[52:10.860 --> 52:15.860]  ну короче, там много всяких деталей, которые взаимодействуют друг с другом, перемешаны друг с другом.
[52:15.860 --> 52:19.860]  Вот здесь Sequencer упорядочивает идентификаторы транзакций.
[52:19.860 --> 52:23.860]  Здесь вот планировщик берет сначала логи, потом исполняет.
[52:23.860 --> 52:27.860]  Вот все очень изолировано, все очень так слоисто.
[52:27.860 --> 52:32.860]  И кажется, что для того, чтобы все это рассказать, никаких особенных знаний это и не требуется.
[52:32.860 --> 52:35.860]  Но кроме того, что Paxos нужно уметь писать как-то.
[52:35.860 --> 52:43.860]  В YarnXDB еще раз напомню, не совсем такая схема, то есть там не RSM, а автомат поверх реплицированного лога в BlobStorage,
[52:43.860 --> 52:48.860]  но думать об этом можно примерно одинаково.
[52:48.860 --> 52:52.860]  И получается все кажется гораздо проще.
[52:52.860 --> 52:58.860]  Это не значит, что система простая, конечно, система сложная, писали ее там много лет и до сих пор пишут как и что угодно.
[52:58.860 --> 53:02.860]  Но вот подход к транзакциям тут совершенно иной.
[53:06.860 --> 53:14.860]  Ну что ж, если у вас вопросы есть, то самое время их задать, потому что я, кажется, исчерпал содержание пока.
[53:14.860 --> 53:18.860]  Не пока, но сегодня.
[53:22.860 --> 53:31.860]  Ключевые инсайты. Вот избавиться от недетерминизма, понять, что двухфазный коммит не является необходимым для кросс-шардовых транзакций.
[53:31.860 --> 53:41.860]  Этот недетерминизм исключить с помощью неинтерактивности и разделения 2PL
[53:41.860 --> 53:49.860]  на собственно недетерминированные исполнения и детерминированные блокировки.
[53:49.860 --> 53:59.860]  И таким образом избежать искусственных технических откатов по вине самой системы.
[54:01.860 --> 54:07.860]  Ну что ж, если вопросов нет, то спасибо. На сегодня все.

[00:00.000 --> 00:12.780]  Добрый день, давайте продолжать. Нам осталось после метода гаусса просто уже
[00:12.780 --> 00:17.800]  выписать решение системы, точнее доказать, что это действительно решение. Итак,
[00:17.800 --> 00:25.320]  теорема, напоминаю, у нас была такая, что если у нас есть система AX равно B,
[00:25.320 --> 00:36.560]  где матрица A имеет упрощенный вид, но пока что вот такой вот. У нас там был уже второй пункт,
[00:36.560 --> 00:50.520]  мы считаем, что матрица A имеет вот такой вот вид, и тогда мы говорим, что фундаментальная матрица
[00:50.520 --> 00:59.000]  нашей системы, то есть матрица, у которой столбцы это базис пространства решений однородной системы,
[00:59.000 --> 01:16.680]  имеет вид вот такой вот. Ну а частное решение имеет вид, конечно же, вот такой вот B, после этого 0.
[01:16.680 --> 01:26.200]  Ну и давайте вот мы это докажем. В случае, когда система не совместная, мы уже с вами разобрали в
[01:26.200 --> 01:34.600]  прошлый раз, а вот совместные оставили на этот. Так, ну давайте разбираться по порядку. Для начала
[01:34.600 --> 01:52.040]  частное решение. Ну что будет, если я матрицу A умножу на X0? Я умножу матрицу E аж 3 на B,
[01:52.040 --> 01:57.440]  а дальше 0. Что у меня произойдет, когда я буду это перемножать? У меня элементы матрицы E,
[01:57.440 --> 02:03.040]  когда я буду строчку на столбец умножать, элементы матрицы E будут умножаться на элементы вот этого
[02:03.040 --> 02:09.280]  столбца B, правильно? А элементы матрицы аж 3 будут умножаться на нижнюю часть нашего столбца,
[02:09.280 --> 02:16.360]  то есть на нули. Поэтому нам не важно, какая аж 3, нам важно только, что здесь у меня будет E умножаться
[02:16.360 --> 02:26.440]  на B, ну и естественно получится B. Вот, ну я на всякий случай напоминаю, да, вот я формальные
[02:26.440 --> 02:33.960]  доказательства привожу, но на самом деле ситуация-то у нас вот какая. Вот если мы
[02:33.960 --> 02:43.720]  посмотрим на расширенную матрицу системы, E, затем аж 3, а вот здесь вот написан B столбец
[02:43.720 --> 02:50.960]  свободных членов, то вы можете посмотреть, что получится, если нам говорят, что когда мы свободные
[02:50.960 --> 02:57.800]  значения свободных переменных фиксируем, главные выражаются однозначно. Свободные переменные здесь
[02:57.800 --> 03:05.120]  все кроме вот самых первых соответствующих матрицы E. И если вы все свободные переменные положите равными
[03:05.120 --> 03:11.400]  нулю, как мы с вами и сделали, то мы увидим, что вот из этой системы главные переменные выражаются
[03:11.400 --> 03:16.800]  очень просто. Каждая главная переменная равна своему элементу в правой части. Вот представьте
[03:16.800 --> 03:22.520]  себе, что свободные переменные все обратились в ноль в этой системе. Вот это способ, как это,
[03:22.520 --> 03:29.920]  конечно же, вычислить. Но аналогично вычисляется и фундаментальная матрица. Давайте разбираться с
[03:29.920 --> 03:43.320]  фундаментальной матрицей. Это, напоминаю, в ней в ее столбцах должен стоять базис ее столбцы.
[03:43.320 --> 03:59.640]  Должны быть базисом под пространство U, давайте я его сразу обозначу, их решений однородной системы
[03:59.640 --> 04:10.280]  AX равно 0. Вот и давайте мы будем проверять, что ее столбцы образуют этот базис. Во-первых,
[04:10.280 --> 04:17.880]  давайте мы проверим, это, конечно же, очень важно, что столбцы в этом пространстве U лежат.
[04:17.880 --> 04:29.240]  То есть столбцы это решения однородной системы уравнений, то есть они лежат в U. Естественно,
[04:29.240 --> 04:35.080]  это нужно проверить, чтобы базис проверять. Но для этого мне нужно матрицу A умножить на вот эту
[04:35.080 --> 04:43.320]  вот матрицу, правда? То есть я матрицу A умножаю на каждый столбец, у меня должен
[04:43.320 --> 04:53.120]  получиться 0. А что у меня получается? Я е а штрих умножаю на минус а штрих е. Обратите,
[04:53.120 --> 05:02.000]  пожалуйста, внимание на всякий случай, вот это е и вот это е могут быть разными матрицами. Почему?
[05:05.640 --> 05:10.360]  Потому что размер а штрих может быть каким угодно, он может быть прямоугольный. Поэтому
[05:10.360 --> 05:15.280]  здесь матрица е имеет такой же размер, сколько строчек у а штрих, здесь матрица е имеет
[05:15.280 --> 05:24.540]  такое же размер, сколько столбцов у а штрих, правильно? Как и раньше, матрица е при
[05:24.540 --> 05:30.320]  таком перемножение блощных матриц будет умножаться на минус а штрих, матрица а штрих будет
[05:30.320 --> 05:35.440]  умножаться на E, правильно, когда мы будем строчку на столбцы умножать, и значит
[05:35.440 --> 05:44.320]  получится минус E' плюс A'E, то есть нулевая матрица. Это значит, что все столбцы
[05:44.320 --> 05:49.760]  являются действительно решениями нашей системы, ну и давайте я на всякий случай
[05:49.760 --> 05:54.480]  объясню, откуда они взялись. Эти столбцы берутся из очень простого
[05:54.480 --> 06:01.200]  соображения. Опять же таким мы хотим построить решение, соответствующее хорошим
[06:01.200 --> 06:06.080]  значениям свободных переменных. Свободные переменные у нас соответствуют нижним
[06:06.080 --> 06:10.880]  координатам, и мы берем просто каждый раз одну единичку, полагаем, одну из
[06:10.880 --> 06:15.600]  свободных переменных равна единичке, а все остальные нуля. И вот вычисляем
[06:15.600 --> 06:18.840]  решение, если вы это решение непосредственно вычислите, так оно и
[06:18.840 --> 06:27.560]  получится. Итак, столбцы у нас решение нашей системы. Что нам еще нужно проверить?
[06:27.560 --> 06:36.200]  Столбцы линейно-независимы. Как мне проверить, что они линейно-независимы? Взять
[06:36.200 --> 06:40.360]  любую их линейную комбинацию и посмотреть, что будет, когда она будет
[06:40.360 --> 06:50.440]  обращаться в ноль, правильно? Линейная комбинация столбцов матрицы Фи, это
[06:50.440 --> 06:57.720]  разумеется, я должен Фи умножить на какой-то столбец. Давайте я его назову С.
[06:58.360 --> 07:05.760]  Когда я матрицу Фи умножаю на какой-то столбец подходящего размера, я как раз
[07:05.760 --> 07:10.160]  беру линейную комбинацию столбцов матрицы Фи, как раз вот с коэффициентами
[07:10.160 --> 07:15.280]  из этого самого столца. Ну а что это будет такое? У меня каждая строчка Фи по
[07:15.280 --> 07:20.000]  отдельности умножится на С, поэтому сверху у меня будет минус а штрих на
[07:20.000 --> 07:26.440]  столбец С, а снизу у меня будет Е умножить на С, то есть сам столбец С,
[07:26.440 --> 07:36.800]  правильно? Ну и что это означает? Это означает, что если ми на С оказалось
[07:36.800 --> 07:46.920]  нулём, то и С это ноль, потому что вот у нас С-столбец стоит внизу. Значит, если мы
[07:46.920 --> 07:51.840]  взяли линейную комбинацию столбцов Фи и получился ноль, то мы взяли линейную
[07:51.840 --> 07:58.200]  комбинацию с нулевыми коэффициентами. Это означает, что столбцы независимы.
[07:58.800 --> 08:06.120]  Наконец В, любой элемент, давайте я его через игрока
[08:06.120 --> 08:14.520]  обозначу, из У, то есть любое решение нашей однородной системы выражается
[08:14.520 --> 08:20.000]  через столбцы Фи.
[08:20.000 --> 08:28.160]  То есть мы проверяем, что элементы нашего вот якобы базисы лежат в
[08:28.160 --> 08:31.880]  пространстве, что они независимы и что их линейная оболочка это всё это
[08:31.880 --> 08:38.960]  пространство, правильно? Ну давайте мы рассмотрим наш столбец У и разделим
[08:38.960 --> 08:43.320]  его на две части, ну часть соответствующую главным переменным и
[08:43.800 --> 08:47.920]  часть соответствующую свободным переменным.
[08:54.400 --> 09:01.800]  Тогда я утверждаю, что через столбцы матрицы Фи можно выразить решение с теми
[09:01.800 --> 09:09.280]  же самыми значениями свободных переменных. Давайте я возьму линейную
[09:09.280 --> 09:14.240]  комбинацию столбцов Фи с такими коэффициентами, так чтобы свободные
[09:14.240 --> 09:18.840]  переменные были бы теми же самыми. Какие коэффициенты нам нужно взять?
[09:18.840 --> 09:23.200]  Нам что нам нужно умножить Фи, чтобы значения свободных переменных получились
[09:23.200 --> 09:30.120]  вот такими вот. И игрок 2, конечно, мы же сами знаем, что нижние элементы рогно у
[09:30.120 --> 09:35.240]  нас и получаются тем, на что мы умножаем, правильно? Поэтому если мы Фи умножим на
[09:35.240 --> 09:44.760]  игрок 2, то мы получим минус А штрих игрок 2, а здесь будет игрок 2.
[09:44.760 --> 09:50.800]  Значит, это будет решение. Почему? Потому что мы сказали, что столбцы Фи — это
[09:50.800 --> 09:55.160]  решение нашей однородной системы, но их линейная комбинация тоже будет
[09:55.160 --> 10:03.400]  решением, потому что решение образует подпространство. Это будет решение, и оно
[10:03.400 --> 10:13.560]  имеет те же значения свободных переменных. Но по значениям свободных
[10:13.560 --> 10:18.440]  переменных решение вычисляется однозначно. Значит, если у нас есть два решения, у
[10:18.440 --> 10:25.760]  которых это вычисляется, у которых эти значения совпадают, то это одно и то же и есть.
[10:25.760 --> 10:35.840]  И мы с вами всю нашу теорему уже доказали.
[10:39.040 --> 10:44.800]  Обратите, пожалуйста, внимание, мы с вами что сделали? Мы выписали решение
[10:44.800 --> 10:50.440]  произвольной неоднородной системы уравнений, если ее матрица имеет вот такой
[10:50.520 --> 10:59.320]  вид пока что. В частности, мы доказали, что у любого такого пространства заданного
[10:59.320 --> 11:04.320]  нашей системы есть конечный базис. Мы же изначально даже не знали, конично оно
[11:04.320 --> 11:10.360]  порождено или нет. Вдруг да нет. Но сейчас мы поняли, что базис у него, конечно же, есть.
[11:10.360 --> 11:27.680]  Так, ну надо сделать замечание. А что делать, если упрощенный вид выглядит иначе?
[11:27.680 --> 11:38.480]  Что будет, если мы, решая нашу систему, привели матрицу систему к упрощенному виду,
[11:38.480 --> 11:47.480]  и вот они, главные столбцы, идут не подряд. Если главные столбцы будут идти подряд, то они
[11:47.480 --> 11:52.880]  будут как раз образовывать матрицу Е, правильно? Но там еще правда внизу должны были быть нулевые
[11:52.880 --> 11:58.360]  строки, но они на систему понятное дело не влияют, правильно? Мы считаем, что этих нулевых строк нет,
[11:58.360 --> 12:04.760]  от них безболезненно можно уйти. Если бы главные столбцы стояли слева, то они бы как раз образовывали
[12:04.760 --> 12:12.640]  вот такую матрицу Е, но они могут стоять не подряд. И вот здесь вот у нас такая ступенчатая
[12:12.640 --> 12:21.800]  матрица нарисована. Здесь еще что угодно стоит. В такой ситуации мы можем, разумеется, переставить
[12:21.800 --> 12:30.200]  столбцы нашей матрицы А. Что означает переставить столбцы? Поменять местами переменные. У нас каждый
[12:30.200 --> 12:35.160]  столбец соответствует своей переменной, правильно? Это коэффициенты при той переменной. Ну и вот просто
[12:35.160 --> 12:41.360]  можно представить себе нашу систему линейных уравнений. Что будет, если мы х2 и х3 поменяем местами?
[12:41.360 --> 12:47.320]  Это означает, что мы столбцы нашей матрицы 2 и 3 как раз поменяем местами, правильно? Значит,
[12:47.320 --> 13:01.240]  мы можем поменять местами столбцы. Это то же самое, что поменять местами ровно так же.
[13:01.240 --> 13:20.960]  Переменные получив уже требуемую на систему х равно b. Столбец свободных шрифтов у нас останется
[13:20.960 --> 13:30.680]  неизменным. У этой системы мы сможем выписать решение. В этом решении, чтобы перейти обратно
[13:30.680 --> 13:35.160]  к исходному порядку переменных, нужно будет... Вот что нужно будет сделать в этом частном решении
[13:35.160 --> 13:42.480]  и вот в этой матрице, чтобы перейти обратно к порядку переменных? Переставить обратно кого?
[13:42.480 --> 13:51.960]  Здесь мы переставляли столбцы, правильно? А здесь у нас переменным, каждой переменной
[13:51.960 --> 13:58.000]  соответствует строчка, правильно? Х5 соответствует пятая строчка вот этой вот матрицы, ну и здесь
[13:58.000 --> 14:09.480]  х5 стоит на пятом месте. Поэтому мы должны обратно поменять местами строки в решении. Затем
[14:09.480 --> 14:23.320]  надо поменять местами строки. Давайте я один пример на всякий случай разберу для ясности.
[14:24.240 --> 14:29.320]  Пример, давайте представим себе, что матрица А у нас получилась вот какой.
[14:38.240 --> 14:42.920]  Ну а столбец В у нас должен быть из двух элементов это 7, 8.
[14:42.920 --> 14:55.560]  После перестановки столбцов, давайте я вот матрицу А со звездочкой напишу.
[14:55.560 --> 15:09.600]  2, 3, 4, 0, 5, 6. Эта матрица будет соответствовать переменным x1, x3, x4, извините x2, x4 и x5, правильно?
[15:09.600 --> 15:32.960]  Вот и поэтому решение нашей системы будет следующим. Мы с вами знаем, как выписывается
[15:32.960 --> 15:39.000]  решение, давайте вот я его выпишу. Мы с вами переставили переменные, поэтому я могу сказать,
[15:39.000 --> 15:48.520]  что столбец x1, x3, x2, x4, x5 будет выражаться как столбец x0, который мы выписываем следующим
[15:48.520 --> 15:58.400]  образом. В ставим сверху, а дальше дописываем нули. И плюс общее решение однородной системы,
[15:58.400 --> 16:06.520]  то есть линейная комбинация столбцов фундаментальной матрицы, то есть как мы
[16:06.520 --> 16:10.600]  пишем линейную комбинацию столбцов фундаментальной матрицы, умножаем ее на какой-то столбец,
[16:10.600 --> 16:19.560]  правильно? И умножаем на гамму. Ну и давайте я фи сразу запишу. Матрицу мы должны умножить на гамму.
[16:19.560 --> 16:27.400]  Матрица сейчас выписывается ровно так, как мы с вами сказали. Сверху мы выписываем минус вот
[16:27.400 --> 16:33.880]  эту вот матрицу, то есть я пишу минус два, минус три, минус четыре, ноль, минус пять, минус шесть.
[16:33.880 --> 16:46.600]  Внизу я выписываю единичную матрицу. Вот. Вот наше решение в нужном порядке, а если мы хотим его
[16:46.600 --> 16:53.400]  выписать в помененном порядке, а если мы хотим выписать решение исходной системы, то мы должны
[16:53.400 --> 16:59.760]  обратно переставить столбцы и, соответственно, получить 7, ноль строки, извините, обратно
[16:59.760 --> 17:05.320]  переставить переменные, то есть строки. Ну и, соответственно, вот мы видим, что здесь происходит.
[17:05.320 --> 17:13.000]  На места, внимание, главных переменных, которые у нас х1, х3, мы ставим элементы столбца В, 7, 8,
[17:13.000 --> 17:21.400]  правильно? Одни в результате стали. На места главных переменных у нас встает та самая матрица минус
[17:21.400 --> 17:32.760]  h', а на места свободных переменных здесь встает единичная матрица. Поэтому вот так вот у нас
[17:32.760 --> 17:44.200]  выписывается общее решение неоднородной системы, если у нас ступенчатая матрица, упрощенный вид матрицы
[17:44.200 --> 17:51.040]  получился с длинными ступеньками. Ну последнее, что я хочу отметить, это то, что столбец гамма на
[17:51.040 --> 18:00.360]  самом деле, это как раз значение свободных переменных. То есть гамма ничто иное, как х2, х4, х5.
[18:00.360 --> 18:05.400]  Если мы здесь это вычислим, на втором, четвертом, пятом месте на местах свободных переменных будут
[18:05.400 --> 18:25.680]  стоять как раз элементы столбца гамма. Ну что ж, мы с вами это дело закончили. Кроме главного
[18:25.680 --> 18:33.160]  результата этих штудий, выписывания решения произвольной системы линейных уравнений,
[18:33.160 --> 18:40.480]  мы получили следствие, которое нам будет важно в дальнейшем, прям сейчас. Следствие говорит вот о чем.
[18:40.480 --> 18:52.040]  О однородной системе линейных уравнений мы ее записываем АХ равно нулю, где А это матрица размера
[18:52.040 --> 19:02.440]  к на n. Напоминаю, у нас здесь k это количество уравнений, n это количество переменных. Давайте
[19:02.440 --> 19:07.960]  представим себе, что n больше, чем k, что количество переменных больше, чем количество уравнений.
[19:07.960 --> 19:21.160]  Всегда есть не нулевое решение, то есть решение, состоящее не из всех нулей, у кого-то такого значения
[19:21.160 --> 19:32.040]  хотя бы одной переменной, не нулевой столбец. Доказательство очень простое. Когда мы приведем к
[19:32.040 --> 19:43.680]  упрощенному виду, в упрощенном виде у нас будет не больше, чем х ступенек, ну то есть не больше,
[19:43.840 --> 19:57.520]  чем к главных переменных, ну а значит хотя бы n-k, то есть уж хотя бы одна свободная. У нас будет
[19:57.520 --> 20:05.000]  свободная переменная, ей мы можем присвоить какое-то не нулевое значение, всем свободным
[20:05.000 --> 20:09.760]  переменным присвоим какие-то значения, ну в частности вот хотя бы одну не нулевую, и получим не
[20:09.760 --> 20:22.200]  нулевое решение, правильно? Присваивая свободным переменным не нулевое значение,
[20:22.200 --> 20:39.600]  получаем не нулевое решение. Ну или по-другому сказать, просто мы можем взять нашу
[20:39.600 --> 20:48.480]  фундаментальную матрицу и сказать, что в ней это столбцы не нулевые и хотя бы один. Вот мы сказали,
[20:48.480 --> 20:57.440]  что в фундаментальной матрице столбцов будет хотя бы n-k, то есть хотя бы один. А почему я,
[20:57.440 --> 21:10.640]  кстати, там говорю, что у нас ступенек будет хотя бы n-k, а не ровно n-k? Потому что некоторые строки
[21:10.640 --> 21:15.840]  нашей системы могут обнулиться, правильно? У нас изначально было k-строчек, мы когда мы приводим
[21:15.840 --> 21:21.120]  ее к ступенечному виду, некоторые строки могли обнулиться, и тогда будет еще больше решений.
[21:21.120 --> 21:35.760]  Вот так. Ну что ж, с этим мы с вами выяснили все. Ну и давай теперь идем дальше. Наше линейическое
[21:35.760 --> 21:40.520]  отступление к системам линейных уравнений кончилось вроде как описанием решения. И
[21:40.520 --> 21:49.920]  теперь мы можем с достигнутых высот заново взглянуть на любые линейные пространства и
[21:49.920 --> 21:58.360]  понять, что такое их размерность, понять, как работать с их базисами. Я напоминаю,
[21:58.360 --> 22:05.520]  что понятие базиса мы ввели, но мы пока что еще не были уверены во всяком случае в том,
[22:05.520 --> 22:14.640]  что все базисы в одном и том же пространстве имеют одно и то же количество элементов. Ну и вот
[22:14.640 --> 22:26.800]  давайте мы с вами это сейчас быстренько выясним, доказав следующую лему. Она еще называется основной
[22:26.800 --> 22:49.720]  леммой о линейной зависимости. Итак, пусть E1 и так далее, E-каты, это какие-то
[22:49.720 --> 23:10.000]  векторы в линейном пространстве. Ну и давайте мы возьмем векторы V1 и так далее, Vn в линейной
[23:10.000 --> 23:18.440]  оболочке этих товарищей. Часто эта лему формулирует в ситуации, когда векторы E1 и так далее,
[23:18.440 --> 23:24.040]  E-каты независимые или даже базис, но в принципе можно и вот в такой ситуации это тоже сформулировать.
[23:24.040 --> 23:38.280]  Итак, пусть у нас есть n векторов в их линейной оболочке, где n больше чем k. Тогда система векторов
[23:38.280 --> 23:49.360]  V1 и так далее, Vn линейно зависима. Если n векторов выражаются через k и n больше чем k, то эти n
[23:49.360 --> 23:58.560]  векторов линейно зависимы. Вот что нам говорит лемма. Для доказательства давайте мы посмотрим,
[23:58.560 --> 24:06.200]  что нам сказали. Нам сказали, что векторы V1 и так далее, Vn выражены через векторы E1 и так далее,
[24:06.200 --> 24:15.720]  E-каты. То есть мы можем написать следующую строчку. Мы можем сказать, что V1 и так далее,
[24:15.720 --> 24:26.680]  Vn, это E1 и так далее, E-каты умножить на некоторую матрицу А, где А это матрица, у которой в столбцах
[24:26.680 --> 24:33.840]  мы напишем просто координаты выражения Я. Выйдет их через Е, правильно? То есть первый столбец
[24:33.840 --> 24:40.600]  нам говорит, что V1 выражается как линейная комбинация этих товарищей с коэффициентами
[24:40.600 --> 24:49.360]  из первого столбца и так далее. Так, матрица А у нас соответственно имеет какие размеры, скажите мне?
[24:55.360 --> 25:03.840]  Осталось понять в каком? k на n, конечно же, правильно? У нее n столбцов, поскольку у нас
[25:04.080 --> 25:12.000]  должно быть n векторов в результате. Матрица А имеет размеры k на n. Ну и как мы с вами только что выяснили,
[25:12.000 --> 25:29.600]  по предыдущему следствию, мы можем найти не нулевой столбец, а f в n. Если мы А умножаем на
[25:29.600 --> 25:36.160]  какой-то столбец, столбец должен иметь n координат такой, что А на гамма равно нулю.
[25:36.160 --> 25:43.280]  Решение вон той самой однородной системы уравнений мы знаем, что есть не нулевое такое решение.
[25:43.280 --> 25:55.120]  Ну давайте мы вот это вот равенство сейчас А на гамма равно нулю. Обратите, пожалуйста, внимание,
[25:55.640 --> 26:02.440]  что ноль это у нас столбец какой-то, правильно? Давайте мы это равенство умножим слева на строчку
[26:02.440 --> 26:15.720]  e1 и так далее и eкат. Тогда значит e1 и так далее и eкат умножить на нулевой столбец и это у нас
[26:15.720 --> 26:22.920]  получится естественно нулевой вектор, правильно? Взяли эти векторы с нулевыми коэффициентами. Есть
[26:22.920 --> 26:33.440]  e1 и так далее и eкат умножить на А на гамму, а здесь у нас написано не что иное, как v1 и так далее
[26:33.440 --> 26:41.540]  vn на гамма. Что мы с вами получили? Мы с вами получили не нулевой столбец коэффициентов,
[26:41.540 --> 26:47.000]  такой, что мы взяли линейную комбинацию наших векторов с этими коэффициентами и
[26:47.000 --> 26:51.880]  получили ноль. Это и означает, что наша система линейно-зависима. Мы предъявили
[26:51.880 --> 27:03.240]  нетривиальную линейную комбинацию равную нулю. Это и есть нетривиальная линейная комбинация наших
[27:03.240 --> 27:19.560]  векторов, вот этих вот товарищей, равная нулю. Ну здорово! Теперь мы с вами имеем право быстренько
[27:19.560 --> 27:37.800]  доказать следующую теорему. Пусть v это конечно порожденное линейное пространство над полем f,
[27:37.800 --> 27:49.080]  можно этого даже не говорить, тогда все его базисы, напоминаю, мы с вами уже давно знаем,
[27:49.080 --> 27:54.880]  что конечно порожденное пространство обладает конечным базисом, все его базисы равномочны в них
[27:54.880 --> 28:09.520]  поровну элементам. Наказательство очень простое. Ну, собственно, применяем эту лему. Пусть e1 и e2
[28:09.520 --> 28:23.240]  это два базиса. Тогда мы, скажем, знаем, что базис e2 линейно выражается через e1,
[28:23.240 --> 28:28.720]  правильно? Любой вектор в нашем пространстве выражается через базис e1, в частности,
[28:28.720 --> 28:39.880]  векторы из базиса e2 тоже выражаются. E2 выражается через e1, и более того,
[28:39.880 --> 28:50.040]  e2 линейно независим, мы с вами знаем, поскольку это базис. Из нашей леммы, ровно по нашей лемме,
[28:50.040 --> 28:58.160]  следует, что в e2 элементов не больше, чем в e1. Если бы здесь элементов было больше,
[28:58.160 --> 29:04.640]  то вот этих вот свойств следовало бы, что e2 линейно зависим. Ну и аналогично, разумеется,
[29:04.640 --> 29:21.280]  в e1 не больше, чем в e2, то есть их поровну. И мы с вами доказали весьма важную теорему и
[29:21.280 --> 29:31.880]  получили весьма важную характеристику любого линейного пространства. Определение, соответственно,
[29:31.880 --> 29:43.480]  нам нужно дать. Пусть v это линейное пространство, значит, конечно порожденное,
[29:43.480 --> 29:59.080]  его размерность обозначается она вот таким вот образом, ну, например, английского слова
[29:59.080 --> 30:14.600]  dimension. Это количество векторов в любом базисе v. Мы с вами поняли, что в любом базисе количество
[30:14.600 --> 30:24.440]  векторов одно и то же. Вот это понятие называется размерностью нашего пространства. Так, значит,
[30:24.440 --> 30:30.600]  еще раз напоминаю, мы с вами в этом семестре, во всяком случае и в следующем тоже, в основном,
[30:30.600 --> 30:36.600]  будем иметь дела только с конечно порожденными пространствами, а ниже конечномерные пространства,
[30:36.600 --> 30:47.520]  то есть пространства, обладающие конечным базисом. В принципе, эта теорема имеет смысл и верна и для
[30:47.520 --> 30:54.200]  бесконечно мерных пространств. Если вы определите базис, как линейно-независимую систему, такую,
[30:54.200 --> 30:59.480]  что любой вектор выражается через него, как конечная линейная комбинация, естественно, то,
[30:59.480 --> 31:07.120]  во-первых, такой базис всегда будет в линейном пространстве существовать в предположении
[31:07.120 --> 31:15.240]  аксиому выбора или в предположении, ну, для Эммы Цорна там для этого нужна, как обычно. Такой базис
[31:15.240 --> 31:20.400]  всегда будет существовать и, более того, теорема по-прежнему будет верна. Любые два базиса пространства
[31:20.400 --> 31:28.440]  будут равномощными и эту мощность можно будет назвать размерностью пространства.
[31:28.440 --> 31:37.800]  Ну, вот мы сейчас не будем влезать во все эти дела с применением аксиому выбора. Я хочу сказать,
[31:37.800 --> 31:42.720]  что этот факт сохраняется в бесконечно мерном пространстве, некоторые другие сохраняться не
[31:42.720 --> 31:50.360]  будут. Поэтому я сразу предупреждаю, что дальше у нас принципиально конечномерная техника будет
[31:50.360 --> 31:59.840]  присутствовать. Но, тем не менее, давайте за замечанием и скажем, теорема верна и для бесконечно
[31:59.840 --> 32:08.880]  мерных пространств, естественно, в предположении аксиому выбора. Вы слышали уже, что такое аксиома
[32:08.880 --> 32:17.160]  выбора, да? На пальцах. Ну, еще будет на самом деле. Тем более, сейчас не хочется в это углубляться.
[32:17.160 --> 32:31.680]  На первом курсе не будет, а дальше точно будет. Предположение аксиомы выбора. Вот это очень важная
[32:31.680 --> 32:38.000]  аксиома, на которой очень многие конструкции, связанные с такой суровой бесконечностью,
[32:38.000 --> 32:45.880]  опираются и, которая сейчас повсеместно принято использовать, считается, что, во всяком случае,
[32:45.880 --> 32:51.960]  получается непротиворечивая математика. Хотя некоторые занимаются и тем, что будет, если ее
[32:51.960 --> 33:05.840]  исключить. Так, значит, замечательно. Ну, теперь мы можем сформулировать следствие. Теорем у нас
[33:05.840 --> 33:18.360]  сейчас будет много, потому что у нас... Ну, просто потому что сейчас пошла очень важная часть теории.
[33:18.360 --> 33:31.960]  Следующая теорема. Пусть у нас v1 и v2 это пространство над одним и тем же полем f. Напоминаю,
[33:31.960 --> 33:41.520]  что конечнопорожденное. У нас везде конечнопорожденное. Эта теорема будет иметь
[33:41.520 --> 33:46.680]  смысл и дальше, но, тем не менее, я уже говорю, что техника в доказательстве у нас будет
[33:46.680 --> 33:54.200]  использоваться существенно, конечно, мерно. Тогда эти два пространства изоморфны. Я напоминаю,
[33:54.200 --> 34:00.880]  что изоморфность пространства у нас запритывается вот таким вот образом. То есть существует биекция
[34:00.880 --> 34:09.840]  между ними, сохраняющая операция, тогда и только тогда, когда размерность v1 равна размерности v2.
[34:09.840 --> 34:26.480]  Доказательства. Значит, если... Давайте мы сначала слева направо докажем. Если v1 изоморфно v2,
[34:26.480 --> 34:41.000]  то у нас есть изоморфизм. Биекция, сохраняющая операции. Давайте мы возьмем какой-нибудь базис v1.
[34:41.000 --> 34:52.360]  Если мы взяли какой-нибудь базис v1, то phi от e, ну, то есть там вот phi от... Мы берем систему образов
[34:52.360 --> 35:06.200]  векторов базиса. Вот это вот phi от e, естественно, будет базисом v2. Ровно потому что phi от изоморфизм и
[35:06.200 --> 35:15.680]  любое соотношение, которое можно записать с помощью наших линейных операций v1, будет
[35:15.680 --> 35:27.080]  работать и v2. Это будет базис v2. Давайте мы вкратце проговорим это, чтобы вот увидеть,
[35:27.080 --> 35:33.680]  как изоморфизм работает. Что нам говорят? Если эти товарищи будут линейно зависимыми,
[35:33.680 --> 35:40.680]  это значит, что прообразы их, то есть векторы e1 и т.д. будут зависимы здесь с теми же самыми
[35:40.680 --> 35:46.240]  коэффициентами, правильно? При изоморфизме ноль переходит в ноль, ну и только ноль переходит в ноль.
[35:46.240 --> 35:52.520]  Линейная комбинация переходит в линейную комбинацию. Если линейная комбинация вот этих товарищей
[35:52.520 --> 36:00.080]  равна нулю, это значит, что phi от линейной комбинации e тоже равно нулю. Ну и это значит,
[36:00.080 --> 36:08.200]  что исходные векторы были зависимы, что не так. Ну и дальше, поскольку любой элемент v1 выражался
[36:08.200 --> 36:17.960]  через e, применяя наши phi, получаем, что любой элемент v2 выражается через вот эти вот образы
[36:17.960 --> 36:27.880]  векторов. То есть действительно это будет базис. Ну в этих двух системах поровну векторов e и phi от e.
[36:27.880 --> 36:38.640]  Размерность v1 это количество векторов v2, это же количество векторов phi от e, а это размерность v2.
[36:38.640 --> 36:47.280]  В эту сторону мы доказали, ну а в другую сторону доказывается с использованием уже того,
[36:47.280 --> 36:57.880]  что мы с вами доказывали раньше. Нам для этого даже не была нужна инвариантность базисного числа.
[36:57.880 --> 37:07.880]  Если размерность v1 и размерность v2 это n, то есть у нас есть базис из n векторов здесь и базис из n векторов здесь,
[37:07.880 --> 37:16.080]  то мы с вами знаем, что v1 изоморфно пространству столбцов длины n. Этот изоморфизм, напоминаю,
[37:16.080 --> 37:22.080]  сопоставлял каждому вектору его координатный столбец, правильно, в нашем вот фиксированном базисе.
[37:22.080 --> 37:37.080]  Ну и а этот fn изоморфно v2. Ну коль так, то v1 изоморфно v2 нетрудно понять, что композиция двух изоморфизмов это, конечно же, тоже изоморфизм.
[37:37.080 --> 37:48.080]  И наша теорема доказана. Итого у нас пространства изоморфны тогда и только тогда, когда у них одна и та же размерность.
[37:48.080 --> 38:03.080]  Можно, но это будет то же самое. То есть вот здесь же у нас написан этот изоморфизм.
[38:03.080 --> 38:13.080]  Из v1 каждому вектору сопоставляем его координаты столбец, каждому столбцу сопоставляем линейную комбинацию базиса здесь с этими коэффициентами.
[38:13.080 --> 38:22.080]  Это и означает, что у нас получился, по сути дела, изоморфизм, переводящий произвольный базис v1 в произвольный базис v2, правильно?
[38:22.080 --> 38:28.080]  Такой изоморфизм у нас всегда есть, собственно говоря, он здесь у нас и строится.
[38:28.080 --> 38:41.080]  Так, давайте двигаться дальше. Да, конечно же, следующая теорема у нас вот какая.
[38:41.080 --> 38:52.080]  Давайте мы увидим, что базис пространства можно охарактеризовать несколько разными способами.
[38:52.080 --> 39:19.080]  Пусть размерность пространства равна n, тогда, во-первых, любая линейно независимая система
[39:19.080 --> 39:29.080]  из n векторов является базицем v.
[39:29.080 --> 39:36.080]  Ну и пункт b.
[39:36.080 --> 39:48.080]  Любая система векторов e1 и так далее, e n, порождающая все v, такая, что v это линейная оболочка этих векторов,
[39:48.080 --> 39:59.080]  говоря приземленным языком, такая, что любой вектор из v выражается через них, тоже является базисом v.
[39:59.080 --> 40:03.080]  Ну давайте мы сейчас сделаем перерыв, после перерыва это быстренько докажем.
[40:03.080 --> 40:09.080]  Смысл теоремы вот в чем. У нас есть три свойства системы векторов.
[40:09.080 --> 40:16.080]  Что она линейно независима, что она порождает все пространство и что в ней n векторов.
[40:16.080 --> 40:21.080]  Мы с вами знаем, что первые два условия дают нам базис по определению.
[40:21.080 --> 40:28.080]  На самом деле, если вы проверили любые два из этих трех условий, то получили базис.
[40:28.080 --> 40:32.080]  Перерыв, после перерыва мы это быстренько докажем.
[40:32.080 --> 40:42.080]  Так, давайте продолжим. Быстренько наше утверждение докажем.
[40:44.080 --> 40:53.080]  Ну в пункте а, что мы с вами знаем? Мы знаем, что у нас эта система уже линейно независима.
[40:53.080 --> 41:01.080]  Нам осталось доказать, что она порождает все v, что любой вектор v есть линейная комбинация этих.
[41:02.080 --> 41:14.080]  Значит, если это не так, то v не есть линейная оболочка наших векторов.
[41:14.080 --> 41:21.080]  То есть существует вектор v, который не выражается через наши векторы.
[41:21.080 --> 41:39.080]  Тогда утверждаю я, система E1 и так далее E n v линейно независима.
[41:39.080 --> 41:43.080]  Это следствие того утверждения, которое я напоминал.
[41:43.080 --> 41:52.080]  Если бы первые n векторов образовали независимую систему, а при добавлении нового вектора получилось бы линейно зависимая,
[41:52.080 --> 41:57.080]  мы с вами знали, что именно добавленный выражается через оставшийся, правильно?
[41:57.080 --> 42:03.080]  Сейчас мы знаем, что он не выражается, поэтому и получилась линейная независимая система.
[42:03.080 --> 42:14.080]  Но этого не может быть, ибо размерность пространства равна n.
[42:14.080 --> 42:22.080]  В пространстве размерности n нет n плюс одного линейно независимого вектора. Почему?
[42:22.080 --> 42:28.080]  Потому что мы знаем, что в пространстве есть базис из n элементов.
[42:28.080 --> 42:42.080]  Если v есть базис f1 и так далее fn, то и наши вектора, вот все вот эти вот товарищи, через него должны выражаться.
[42:42.080 --> 42:52.080]  То все векторы через него выражаются.
[42:52.080 --> 43:00.080]  Ну и можно применить основную лемму о линейной зависимости.
[43:00.080 --> 43:04.080]  Давайте я просто основную лемму напишу о линейной зависимости.
[43:04.080 --> 43:11.080]  И получить, что если эти векторы выражаются через эти, то они линейно зависимы.
[43:11.080 --> 43:19.080]  Мы получили противоречие, и значит у нас действительно получился базис.
[43:19.080 --> 43:23.080]  Пункт b.
[43:23.080 --> 43:37.080]  Для этого нужно проверять, что это изоморфизм.
[43:37.080 --> 43:44.080]  Пока мы не знаем, что они.
[43:44.080 --> 43:51.080]  Нам нужно понять, что это изоморфизм, что образ этого отображения будет всем v, а не чем-то меньшим.
[43:51.080 --> 43:59.080]  Вот ровно это мы сейчас и доказывали, что образ такого изоморфизма был бы всем v.
[43:59.080 --> 44:05.080]  Пункт b. У нас наоборот уже есть порождающая система.
[44:05.080 --> 44:14.080]  И чтобы проверить, что она базис, нам нужно проверить ее линейную независимость.
[44:14.080 --> 44:16.080]  Нам не хватает этого.
[44:16.080 --> 44:28.080]  Значит если это не так, то e1 и так далее, e n была бы линейно зависимой системой.
[44:28.080 --> 44:32.080]  Ну и значит один вектор выражался бы через остальные.
[44:32.080 --> 44:39.080]  Кажем, e n выражается через предыдущие вектора.
[44:39.080 --> 44:48.080]  Я это напоминаю не обязательно так, но один из этих векторов точно бы выразился через остальные.
[44:48.080 --> 44:57.080]  Тогда и наше пространство было бы линейной оболочкой только первых n минус одного вектора.
[44:57.080 --> 45:05.080]  Здесь есть e n, а значит есть и все, что выражается через e1 и так далее, e n.
[45:05.080 --> 45:12.080]  А коль скоро это так, коль скоро все векторы в нашем пространстве выражаются через n минус один вектор,
[45:12.080 --> 45:20.080]  по той же самой лемме о линейной зависимости в пространстве не может быть n независимых векторов.
[45:20.080 --> 45:32.080]  Раз так, то в v нет n линейно независимых векторов по основной лемме.
[45:32.080 --> 45:38.080]  Но мы знаем, что они есть, мы знаем, что в нашем пространстве есть базис из n элемента.
[45:38.080 --> 45:45.080]  Это противоречие доказывает уже и вторую часть.
[45:45.080 --> 45:55.080]  Итого, про систему из n векторов нам достаточно проверять только одно из свойств произвольное.
[45:55.080 --> 46:02.080]  Если мы, конечно, знаем размерность пространства, ну а как вычислить размерность пространства?
[46:02.080 --> 46:08.080]  Естественно, предъявить какой-то хороший базис, правильно?
[46:08.080 --> 46:22.080]  То есть если мы с вами хотим вычислить размерность произвольного пространства, нам достаточно предъявить в нем один базис.
[46:22.080 --> 46:27.080]  Тогда все остальные базисы будут иметь ту же самую мощность.
[46:27.080 --> 46:32.080]  Так, замечательно. С этим мы с вами разобрались.
[46:32.080 --> 46:43.080]  Двигаемся дальше. Следующее, хоть и важное, но все же таки, наверное, утверждение.
[46:43.080 --> 46:58.080]  Пусть v это линейное пространство, размерность v равна n, ну и пусть u это некоторое подпространство v.
[46:58.080 --> 47:15.080]  Тогда u тоже конечно порожденное пространство, и размерность u не превосходит размерности v, то есть не превосходит n.
[47:15.080 --> 47:31.080]  Более того, если размерность u равна n, то как вы думаете, когда это возможно, когда u это на самом деле v?
[47:31.080 --> 47:36.080]  Обратите, пожалуйста, внимание. В принципе, мы не могли так сразу сказать.
[47:36.080 --> 47:41.080]  Вот у нас есть конечно порожденное пространство, оно порождено какими-то векторами.
[47:41.080 --> 47:46.080]  Мы берем его подпространство u, непонятно, почему оно конечно порождено.
[47:46.080 --> 47:53.080]  Все его векторы выражаются через конечное количество векторов из v, но они не обязательно лежат в u, правда?
[47:53.080 --> 47:58.080]  И поэтому не очень понятно, можно ли породить u конечным числом векторов из u.
[47:58.080 --> 48:02.080]  И вот мы сейчас отвечаем на этот вопрос утвердительно.
[48:02.080 --> 48:10.080]  Доказательства очень, конечно же, не сложные.
[48:10.080 --> 48:24.080]  Значит, мы знаем, давайте я так начну, что любые n плюс 1 векторов из u линейно зависимы.
[48:24.080 --> 48:30.080]  Они выражаются через базис v, и по нашей лиме они линейно зависимы.
[48:30.080 --> 48:47.080]  Значит, у нас найдется такое наибольшее k. Пусть k это наибольший размер линейно независимой системы в u.
[48:47.080 --> 48:52.080]  Ну и пусть это система e1 и так далее ek.
[48:52.080 --> 48:59.080]  Раз у нас размеры таких систем ограничены, у нас не может быть n плюс 1 независимого вектора,
[48:59.080 --> 49:05.080]  то значит есть наибольший размер такой системы, пусть это k.
[49:05.080 --> 49:21.080]  Ну тогда для любого вектора из u система e1 и так далее ek u получилась уже линейно зависимой,
[49:21.080 --> 49:24.080]  раз k это наибольший размер.
[49:24.080 --> 49:31.080]  Ну а значит, по тому утверждению, которое мы уже вспоминали, u выражается через e1 и так далее ek.
[49:31.080 --> 49:41.080]  Что мы с вами получили? Что e1 и так далее ek, линейно независимая система, через которую все выражается,
[49:41.080 --> 49:53.080]  это значит, что e1 и так далее ek это базит в u.
[49:53.080 --> 50:03.080]  Размерность u таким образом равна k. Естественно, u порождено этими векторами, поэтому оно конечно порождено.
[50:03.080 --> 50:08.080]  Вопрос, да? Давайте я договорю фразу.
[50:08.080 --> 50:16.080]  Размерность u равна k, u в частности конечно порождено, ну и конечно же k не превосходит n,
[50:16.080 --> 50:25.080]  потому что мы сказали, что k линейно независимых векторов есть, n плюс 1 быть не может. Вопрос?
[50:25.080 --> 50:42.080]  Потому что e1 и так далее ek в u, значит все их линейные комбинации тоже в u.
[50:42.080 --> 50:51.080]  Итого k не превосходит n, ну и наконец, мне говорят, что я могу писать на этой доске, правда ведь, да?
[50:51.080 --> 50:54.080]  Тем видно.
[50:54.080 --> 51:01.080]  Наконец, нам нужно последнее утверждение доказать в нашем утверждении.
[51:01.080 --> 51:12.080]  Если k равно n, то конечно e1 и так далее en это линейно независимая система из n векторов v.
[51:12.080 --> 51:17.080]  А мы с вами только что доказывали, что она базис в v.
[51:21.080 --> 51:26.080]  Ой, ну что это я? Давайте уж я допишу.
[51:26.080 --> 51:34.080]  Значит, мы с вами знаем, что u это линейная оболочка у этих товарищей,
[51:34.080 --> 51:38.080]  ну а раз они базис в v, то это v, то эта линейная оболочка это v.
[51:38.080 --> 51:40.080]  Значит, у совпадается v.
[51:40.080 --> 51:46.080]  В n-мерном пространстве только одно n-мерное подпространство – это оно само.
[51:46.080 --> 51:54.080]  Замечательно. Ещё одна важная теорема.
[51:54.080 --> 52:00.080]  Наверное, последняя теорема, собственно, о базисах пространств.
[52:00.080 --> 52:04.080]  Вот мы с вами постепенно понимаем, какие штуки можно с ними делать.
[52:05.080 --> 52:09.080]  Последняя важная теорема.
[52:09.080 --> 52:16.080]  Пусть v – это линейное пространство, конечно, порождённое.
[52:16.080 --> 52:21.080]  Размерность v равна n.
[52:21.080 --> 52:32.080]  Ну и пусть у нас есть произвольная линейно независимая система v.
[52:32.080 --> 52:37.080]  E1, и т.д., E kt.
[52:37.080 --> 52:51.080]  Тогда её можно дополнить до базиса v.
[52:51.080 --> 52:57.080]  То есть можно, вот у нас были E1, и т.д., E kt.
[52:57.080 --> 53:01.080]  Мы можем придумать векторы E k плюс 1, и т.д., E n.
[53:01.080 --> 53:04.080]  Так что у нас получится базис v.
[53:04.080 --> 53:33.080]  Тут можно уже в силу нашей развитой теории по-разному,
[53:33.080 --> 53:36.080]  можно, например, так.
[53:36.080 --> 53:51.080]  Давайте будем добавлять векторы в нашу систему,
[53:51.080 --> 54:03.080]  сохраняя её линейно независимой, пока это возможно.
[54:03.080 --> 54:09.080]  То есть вот глядите, у нас есть векторы E1, и т.д., E kt.
[54:09.080 --> 54:12.080]  Я смотрю, можно ли в эту систему добавить ещё один вектор,
[54:12.080 --> 54:15.080]  так чтобы она осталась линейно независимой.
[54:15.080 --> 54:19.080]  Если можно, я его добавил.
[54:19.080 --> 54:22.080]  Нет, сейчас скажу почему.
[54:22.080 --> 54:24.080]  И т.д., пока это возможно.
[54:24.080 --> 54:29.080]  Вот пусть я добавил вектор E m.
[54:29.080 --> 54:34.080]  Обратите внимание, я должен закончить когда-нибудь,
[54:34.080 --> 54:38.080]  потому что размерность нашего пространства n,
[54:38.080 --> 54:42.080]  и значит в нём нет n плюс 1 линейно независимого вектора.
[54:42.080 --> 54:46.080]  Потому что размерность нашего пространства n,
[54:46.080 --> 54:50.080]  и значит в нём нет n плюс 1 линейно независимого вектора.
[54:50.080 --> 54:55.080]  То есть до n плюс 1 вектора я добраться уже не мог.
[54:55.080 --> 55:02.080]  Здесь m не больше, чем n,
[55:02.080 --> 55:09.080]  ибо в любые n плюс 1 векторы линейно зависим.
[55:09.080 --> 55:14.080]  Значит, я должен был остановиться на каком-то шаге.
[55:14.080 --> 55:23.080]  Но давайте мы сразу скажем, что если m меньше, чем n,
[55:23.080 --> 55:31.080]  то мы с вами знаем, что v это нелинейная оболочка вот этих вот построенных векторов.
[55:31.080 --> 55:36.080]  Если бы v был линейной оболочкой, то v было бы m-мерным, правильно?
[55:36.080 --> 55:39.080]  А это не так.
[55:39.080 --> 55:45.080]  Если m меньше, чем n, то v это ещё не всё.
[55:45.080 --> 55:51.080]  То есть линейная оболочка это ещё не всё v, наоборот.
[55:51.080 --> 55:58.080]  То есть существует вектор v, который не лежит в этой линейной оболочке,
[55:58.080 --> 56:03.080]  и его можно было бы к ним добавить.
[56:06.080 --> 56:12.080]  Так, чтобы система получилась линейной независимой.
[56:12.080 --> 56:17.080]  Значит, если m меньше n, то мы на самом деле не закончили,
[56:17.080 --> 56:21.080]  а когда закончили, то этого быть не может.
[56:21.080 --> 56:31.080]  Итак, m получилось равно n, а наша система получилась базисом
[56:31.080 --> 56:34.080]  к силу того утверждения, которое мы с вами доказали.
[56:34.080 --> 56:40.080]  Здесь у нас система из n-векторов, и эта система линейно-независима.
[56:40.080 --> 56:44.080]  Значит, базис.
[56:44.080 --> 56:47.080]  Обратите, пожалуйста, внимание.
[56:47.080 --> 56:54.080]  Это уже утверждение, в частности, нам говорит, что базисов в пространстве очень много.
[56:54.080 --> 56:59.080]  Я могу любой вектор взять и, начиная с него, достроить его до базиса.
[56:59.080 --> 57:06.080]  То есть любую заранее мне данную линейно-независимую систему я умею достроить до базиса.
[57:06.080 --> 57:12.080]  Причем даже на самом деле видно как, и видно, что этих способов очень много.
[57:12.080 --> 57:15.080]  Потому что, глядите, вот если я дошел до какого-то ем,
[57:15.080 --> 57:18.080]  какой вектор я могу добавить следующим шагом?
[57:23.080 --> 57:26.080]  Любой, который через них не выражается, правильно?
[57:26.080 --> 57:30.080]  Получится линейная независимая система, вот по нашему утверждению и Дахнему.
[57:30.080 --> 57:32.080]  И значит, я могу это продолжить, правильно?
[57:32.080 --> 57:39.080]  Поэтому вот видно, насколько много выборов базисов у нас может быть в любом пространстве.
[57:39.080 --> 57:42.080]  Так, ну и, да.
[57:43.080 --> 57:44.080]  Да.
[57:49.080 --> 57:54.080]  Нет, нет, мы выбираем просто, мы пользуемся тем, что у нас есть два множества,
[57:54.080 --> 57:57.080]  одно вложено в другое, да, и они не равны.
[57:57.080 --> 58:00.080]  Значит, есть элементы с дополнением.
[58:00.080 --> 58:04.080]  Аксиома выбора у нас все-таки требует, чтобы мы умели выбирать представителей
[58:04.080 --> 58:08.080]  из большого количества множества, правильно, не из одного.
[58:08.080 --> 58:12.080]  Здесь мы аксиома выбора, конечно же, не требуем, не пользуемся.
[58:12.080 --> 58:16.080]  Если множество не пусто, из него можно взять элемент.
[58:16.080 --> 58:18.080]  Это не аксиома выбора, правильно?
[58:18.080 --> 58:23.080]  Это, в общем, нечто более тривиальное.
[58:26.080 --> 58:33.080]  Так, ну, здесь я должен, конечно же, напомнить,
[58:33.080 --> 58:40.080]  коль скоро мы с вами разобрались с инвариантностью базисного числа,
[58:40.080 --> 58:44.080]  здесь я должен напомнить, что коль скоро у нас есть разные базисы,
[58:44.080 --> 58:49.080]  у нас бывает матрица перехода от одного базиса к другому,
[58:49.080 --> 58:53.080]  и опять же таки этот кусочек теории мы можем повторить дословно,
[58:53.080 --> 58:56.080]  поэтому давайте я его просто сейчас проговорю.
[58:56.080 --> 59:15.080]  Если E1, E2 это два базиса В, то существует матрица перехода от базиса E1 к E2,
[59:15.080 --> 59:21.080]  имеющая размер N на N, где N это как раз размерность пространства, правильно?
[59:21.080 --> 59:28.080]  Такая, что E2 это E1 на S.
[59:28.080 --> 59:32.080]  Ну, матрица единственна, потому что координаты векторов единственны,
[59:32.080 --> 59:40.080]  напоминаю, что в столбцах матрицы S стоят в точности координаты векторов из второго базиса в базисе первом.
[59:42.080 --> 59:50.080]  Ну и если теперь произвольный вектор из нашего В имеет координатный столбец альфа в базисе E,
[59:50.080 --> 59:56.080]  и имеет координатный столбец альфа штрих в базисе E,
[01:00:01.080 --> 01:00:09.080]  давайте альфа один в базисе E1, альфа два в базисе E2, раз уж у нас E2 и E1.
[01:00:09.080 --> 01:00:19.080]  По сути, я вторую половинку стрелочки не нарисовал.
[01:00:19.080 --> 01:00:25.080]  То, разумеется, у нас происходит выражение этих координат,
[01:00:25.080 --> 01:00:29.080]  напоминаю, что выражение происходит в обратную сторону.
[01:00:29.080 --> 01:00:34.080]  Если E2 это E1 на S, то альфа один это S на альфа два.
[01:00:34.080 --> 01:00:38.080]  Еще раз, доказательства у нас проходят дословно.
[01:00:38.080 --> 01:00:42.080]  Все, что нам нужно для этого доказательства, это, конечно же,
[01:00:46.080 --> 01:00:52.080]  единственность координатного столбца для вектора в базисе.
[01:00:52.080 --> 01:00:55.080]  Она у нас уже давным-давно есть.
[01:00:56.080 --> 01:01:04.080]  Так что все эти формулы перехода между базисами мы с вами тоже уже знаем.
[01:01:04.080 --> 01:01:19.080]  В дополнение давайте мы с вами прежде, чем перейдем к следующему разделу,
[01:01:19.080 --> 01:01:24.080]  давим еще несколько определений, связанных скорее с векторами,
[01:01:24.080 --> 01:01:27.080]  а затем уже перейдем к матрицам.
[01:01:27.080 --> 01:01:38.080]  Следующее определение, пусть S, пусть V это линейное пространство,
[01:01:38.080 --> 01:01:43.080]  а S это какое-то подмножество V.
[01:01:43.080 --> 01:01:46.080]  Конечное, бесконечное, неважно.
[01:01:46.080 --> 01:02:06.080]  Тогда рангом системы S называется наибольший размер линейной независимой
[01:02:06.080 --> 01:02:14.080]  вход системы в S.
[01:02:19.080 --> 01:02:26.080]  Ну то есть наибольшая такая K, что мы в S можем выбрать систему из K линейно-независимых векторов.
[01:02:26.080 --> 01:02:31.080]  Естественно, в конечномерном пространстве такой ранг будет существовать,
[01:02:31.080 --> 01:02:36.080]  потому что мы с вами уже неоднократно говорили, больше чем N линейно-независимых векторов
[01:02:36.080 --> 01:02:38.080]  мы отсюда выбрать не сможем.
[01:02:38.080 --> 01:02:43.080]  Значит ранг S будет не превосходить N.
[01:02:43.080 --> 01:02:50.080]  Ну и более того, давайте мы сразу накажем следующее несложное утверждение.
[01:02:50.080 --> 01:03:00.080]  Так, ранг S будет обозначаться вот таким вот образом.
[01:03:00.080 --> 01:03:12.080]  И нехитрое утверждение нам сразу говорит, что ранг S это не что иное, как размерность линейной оболочки S.
[01:03:12.080 --> 01:03:18.080]  Если мы взяли произвольную систему векторов, мы можем взять ее линейную оболочку, получить подпространство.
[01:03:18.080 --> 01:03:23.080]  Подпространство есть размерность и вот это то же самое, что ранг S.
[01:03:27.080 --> 01:03:29.080]  Доказательства.
[01:03:29.080 --> 01:03:49.080]  Ну давайте мы скажем, что пусть ранг S равен K и в S есть линейно-независимая система S1 и так далее СК.
[01:03:49.080 --> 01:04:02.080]  Тогда я утверждаю, что S1 и так далее СК это базис линейной оболочки.
[01:04:02.080 --> 01:04:12.080]  Это базис в линейной оболочке S.
[01:04:12.080 --> 01:04:17.080]  Если мы это докажем, то мы как раз докажем, что ее размерность линейной оболочки равна K.
[01:04:17.080 --> 01:04:21.080]  Почему это так?
[01:04:21.080 --> 01:04:31.080]  Если у нас есть какой-то вектор, что я должен доказать?
[01:04:31.080 --> 01:04:34.080]  Мы уже знаем, что эта система линейно-независима.
[01:04:34.080 --> 01:04:41.080]  Разумеется, она лежит в этой линейной оболочке и значит нам нужно доказать, что она порождающая.
[01:04:41.080 --> 01:04:58.080]  Если V лежит в линейной оболочке S, то V выражается через векторы из S.
[01:04:58.080 --> 01:05:02.080]  Является линейная комбинация векторов из S.
[01:05:02.080 --> 01:05:24.080]  Ну а более того, для любого вектора из S система S1 и так далее SKT, который добавлен в S, будет линейно-независимой.
[01:05:24.080 --> 01:05:30.080]  Ну а значит S выражается через векторы этой системы.
[01:05:30.080 --> 01:05:36.080]  S лежит в линейной оболочке S1 и так далее SKT.
[01:05:36.080 --> 01:05:38.080]  Итого, что мы получили?
[01:05:38.080 --> 01:05:47.080]  Любой вектор из линейной оболочки выражается через векторы из S, а каждый вектор из S выражается через векторы S1 и так далее SKT.
[01:05:47.080 --> 01:05:55.080]  Значит, и V выражается через эти векторы.
[01:05:55.080 --> 01:05:58.080]  И наше утверждение доказано.
[01:06:09.080 --> 01:06:19.080]  Так, ну что ж, про линейное пространство, про размерность базис и связанные с ним понятия мы с вами поговорили достаточно.
[01:06:19.080 --> 01:06:21.080]  Давайте переходить дальше.
[01:06:21.080 --> 01:06:27.080]  Следующая наша большая тема, сейчас мы ее начнем, сколько успеем, столько и скажем.
[01:06:27.080 --> 01:06:31.080]  Тема называется ранг матрицы.
[01:06:35.080 --> 01:06:47.080]  Мы скачем как бы от векторов к матрицам и обратно и видим, как вот эти вот общие теории отражаются на матрицах, матрицы влияют на общую теорию.
[01:06:47.080 --> 01:06:59.080]  Итак, определение, пусть A это матрица размера, скажем, N на K над полем F.
[01:06:59.080 --> 01:07:18.080]  Тогда мы можем определить ее строчный ранг, давайте я так его буду обозначать, ранг с индексом R от A, от слова row, естественно.
[01:07:18.080 --> 01:07:24.080]  Это ранг системы ее строк.
[01:07:29.080 --> 01:07:38.080]  Каждая строка нашей матрицы, это какая-то строка длины K, правильно, элемент коммерного пространства всех строк длины K.
[01:07:38.080 --> 01:07:49.080]  Мы берем эти строки и применяем к ним наше определение, то есть смотрим, какое наибольшее количество строк можно выбрать, так чтобы они образовали линейную независимую систему.
[01:07:49.080 --> 01:07:53.080]  Это и есть строчный ранг нашей матрицы.
[01:07:53.080 --> 01:08:08.080]  Это ее столбцовый ранг, ранг C от A, аналогично ранг системы столбцов.
[01:08:08.080 --> 01:08:25.080]  Очень скоро мы с вами прекратим писать эти индексы, потому что наша первая цель заключается в том, чтобы доказать, что для любой матрицы эти два ранга совпадают.
[01:08:25.080 --> 01:08:38.080]  Ну и это в принципе какое-то, на первый взгляд, весьма нетривиальное утверждение, потому что мы смотрим на строки, смотрим на столце, это какие-то совершенно разные объекты в разных пространствах, правильно?
[01:08:38.080 --> 01:08:49.080]  Для этого нам будет нужна небольшая лемма, которая сейчас будет уже очень простой.
[01:08:49.080 --> 01:09:12.080]  Значит, я утверждаю, что если A и B матрицы подходящего размера, сейчас мы увидим, что значит подходящего размера, то столбцовый ранг матрицы AB не превосходит столбцового ранга A,
[01:09:12.080 --> 01:09:22.080]  ну а строчный ранг матрицы AB не превосходит строчного ранга матрицы B.
[01:09:22.080 --> 01:09:37.080]  И с точки зрения той теории, которую мы с вами развили, это на самом деле утверждение уже должно быть очень простым.
[01:09:37.080 --> 01:09:51.080]  Давайте я докажу, например, первое утверждение, мы с вами увидим, что второе утверждение аналогично. Доказательство для столбцового ранга.
[01:09:51.080 --> 01:10:15.080]  Ну давайте я возьму систему строк матрицы A, давайте я ее обозначу через S, и давайте я обозначу через U линейную оболочку S пространство, которое им поражено.
[01:10:15.080 --> 01:10:29.080]  Столбцов, конечно же, извините меня. Раз мы говорим про столбцовый ранг, то мы смотрим на пространство столбцов, на линейную оболочку всех этих столбцов.
[01:10:29.080 --> 01:10:44.080]  И пусть у нас столбцовый ранг A, это размерность U, и она равна, скажем, K.
[01:10:44.080 --> 01:10:48.080]  Давайте мы вспомним, что мы знаем про столбцы матрицы AB.
[01:10:59.080 --> 01:11:07.080]  Именно так. Столбцы матрицы AB, мы с вами говорили об этом в самом начале, это линейная комбинация столбцов A.
[01:11:07.080 --> 01:11:34.080]  Так как столбцы AB, это линейная комбинация столбцов A, они все лежат в U. Все столбцы матрицы AB лежат в этом подпространстве, а размерность этого пространства равна K.
[01:11:34.080 --> 01:11:45.080]  Значит, из столбцов матрицы AB мы не можем выбрать больше, чем K столбцов, образующих линейную независимую систему. Они все сидят в коммерном пространстве.
[01:11:45.080 --> 01:12:01.080]  Следовательно, ранг столбцовый AB не превосходит этого самого K. Из элементов коммерного пространства мы не можем выбрать больше, чем K образующих линейную независимую систему.
[01:12:01.080 --> 01:12:06.080]  Это все еще та самая основная лемма о линейной зависимости.
[01:12:06.080 --> 01:12:22.080]  Утверждение наше доказано. Для строчного ранга все происходит точно так же, только, разумеется, мы должны использовать то, что строчки матрицы AB это линейные комбинации строчек матрицы B.
[01:12:22.080 --> 01:12:33.080]  Ну а теперь мы готовы с вами доказать теорему о ранге матрицы.
[01:12:33.080 --> 01:12:40.080]  Так она и называется. И звучит она очень просто.
[01:12:40.080 --> 01:12:48.080]  Строчный ранг матрицы A равен столбцовому рангу матрицы A.
[01:12:48.080 --> 01:13:03.080]  Ну и далее он просто называется рангом матрицы A и обозначается RKA.
[01:13:03.080 --> 01:13:07.080]  Ну далее это после того, как мы нашу теорему докажем.
[01:13:10.080 --> 01:13:30.080]  Доказательства в некотором роде хитрые, а в некотором роде и простое.
[01:13:30.080 --> 01:13:40.080]  Значит, пусть K это строчный ранг, например, матрицы A.
[01:13:40.080 --> 01:13:52.080]  Давайте мы выберем в А К строк, образующих линейную независимую систему.
[01:13:52.080 --> 01:13:58.080]  Все остальные строки через них будут выражаться, правильно? Мы с вами это уже понимаем.
[01:13:58.080 --> 01:14:13.080]  Тогда в А есть К линейно-независимых строк, и значит все строки А через них выражаются.
[01:14:13.080 --> 01:14:24.080]  Ну потому что это базис, собственно, в линейной оболочке строк матрицы А.
[01:14:25.080 --> 01:14:35.080]  Давайте мы возьмем эти самые K строк и составим из них матрицу B.
[01:14:35.080 --> 01:14:52.080]  То есть строки матрицы B это как раз вот эти вот K строк, строки которой это наши K строк.
[01:14:52.080 --> 01:15:11.080]  То это означает, каждая строчка матрицы A, давайте я скажу ИТ-я строчка матрицы А,
[01:15:11.080 --> 01:15:20.080]  это линейная комбинация строк матрицы B. Как взять линейную комбинацию строк матрицы B?
[01:15:20.080 --> 01:15:30.080]  Домножить? Нет, не справа. Если бы мы взяли с линейной комбинации столбцов, то мы бы домножали справа на столбец, правильно мы это уже делали?
[01:15:30.080 --> 01:15:40.080]  Сейчас мы, естественно, слева домножаем на строчку. То есть я беру строчку С1 и т.д. и СКТ, умножаю на матрицу B,
[01:15:40.080 --> 01:15:45.080]  это как раз линейная комбинация этих строк с коэффициентными С1 и т.д. и СКТ, правильно?
[01:15:45.080 --> 01:15:51.080]  Каждая строка матрицы A имеет вот такое вот выражение.
[01:15:51.080 --> 01:16:01.080]  Ну а это значит, то всю матрицу A я могу выразить как произведение матриц С на B,
[01:16:01.080 --> 01:16:07.080]  где С это матрица, ну вот, ровно с такими строчками, которые мы только что написали, правильно?
[01:16:07.080 --> 01:16:15.080]  Мы строчки вот эти вот из коэффициентов выражения строк A через B запишем друг под другом и получим матрицу A.
[01:16:19.080 --> 01:16:31.080]  Ну тогда, значит, мы с вами знали что-то про строчный ранг матрицы A, давайте мы поймем, что мы что-то знаем про столбцовый ранг матрицы A.
[01:16:31.080 --> 01:16:35.080]  Что нам говорят про столбцовый ранг матрицы A?
[01:16:35.080 --> 01:16:46.080]  Это столбцовый ранг матрицы C, B, и он, по нашему утверждению, не превосходит столбцового ранга матрицы C.
[01:16:46.080 --> 01:16:59.080]  Ну давайте мы посмотрим внимательно на матрицу C. В ней всего-то-навсего ка столбцов.
[01:16:59.080 --> 01:17:06.080]  В матрице B ка строк, правильно, вот в этом от совмножителе, в матрице C ка столбцов.
[01:17:06.080 --> 01:17:11.080]  В каждой ее строчке ка элементов, вот произвольная ее строчка такой вид имеет.
[01:17:12.080 --> 01:17:18.080]  Значит, раз в C ка столбцов ее столбцовый ранг не может быть больше, чем ка уж никак, правда?
[01:17:26.080 --> 01:17:34.080]  Что мы с вами доказали? Мы доказали, что столбцовый ранг матрицы A не может превосходить строчного ранга матрицы A.
[01:17:35.080 --> 01:17:38.080]  Ну, разумеется, аналогично.
[01:17:42.080 --> 01:17:45.080]  Строчный ранг не превосходит столбцового.
[01:17:46.080 --> 01:17:55.080]  Нужно проделать ту же самую операцию, просто выразить все столбцы матрицы A через какие-то вот столько ее столбцов.
[01:17:57.080 --> 01:18:01.080]  И таким образом наша теорема уже доказана.
[01:18:02.080 --> 01:18:09.080]  И мы знаем, что у нас есть такое понятие, как просто ранг матрицы.
[01:18:22.080 --> 01:18:29.080]  Ну, давайте мы сейчас немножко ранг матрицы поисследуем.
[01:18:31.080 --> 01:18:39.080]  Сформулируем несколько уже гораздо менее нетривиальных подтверждений.
[01:18:50.080 --> 01:18:58.080]  Перед этим давайте я сформулирую небольшое упражнение для всех желающих разобраться.
[01:19:02.080 --> 01:19:10.080]  Итак, внимание, пусть строки матрицы A линейно-независимы.
[01:19:10.080 --> 01:19:32.080]  Тогда я утверждаю, что вот в этом неравенстве всегда достигается равенство.
[01:19:33.080 --> 01:19:37.080]  Ранг AB равен рангу B.
[01:19:39.080 --> 01:19:45.080]  Ну, для любой матрицы B, которая естественно подходит под...
[01:19:46.080 --> 01:19:48.080]  Ну, которую можно умножить на А, правильно?
[01:19:48.080 --> 01:19:58.080]  Так, ну и еще одно утверждение.
[01:19:59.080 --> 01:20:02.080]  Да, наверное, на нем мы сегодня и остановимся.
[01:20:03.080 --> 01:20:06.080]  В следующий раз уже пойдем дальше и докажем большую теорему какую-то.
[01:20:07.080 --> 01:20:09.080]  Еще следующее утверждение.
[01:20:10.080 --> 01:20:12.080]  Значит, пусть В это линейное пространство.
[01:20:13.080 --> 01:20:17.080]  Утверждение тривиально, но стоит его сформулировать.
[01:20:18.080 --> 01:20:26.080]  E1 и так далее, En это базис В.
[01:20:27.080 --> 01:20:32.080]  V1 и так далее, Vkt это векторы В.
[01:20:33.080 --> 01:20:39.080]  Ну, а Vt имеют в базисе E координатный столбец αi.
[01:20:39.080 --> 01:20:48.080]  Тогда нас может интересовать ранг системы наших векторов V1 и так далее Vkt.
[01:20:49.080 --> 01:20:55.080]  И утверждение состоит в том, что ранг этой системы это, конечно же, ранг матрицы,
[01:20:56.080 --> 01:21:02.080]  столбцами которой являются α1, α2 и так далее αkt.
[01:21:02.080 --> 01:21:16.080]  Ну, доказательства тривиальны просто потому, что когда мы, если мы смотрим на некоторую систему векторов,
[01:21:17.080 --> 01:21:22.080]  непосредственно перед рангом матрицы,
[01:21:22.080 --> 01:21:26.080]  ранг система это максимальный набор линейно-независимых элементов ее.
[01:21:27.080 --> 01:21:33.080]  Максимальная мощность системы из ее элементов.
[01:21:34.080 --> 01:21:41.080]  Ну, на самом деле V1 и так далее Vt будут линейно-независимыми.
[01:21:42.080 --> 01:21:44.080]  Вот такая система из векторов будет линейно-независимой.
[01:21:45.080 --> 01:21:51.080]  Тогда и только тогда, когда соответствующие координатные столбцы будут линейно-независимыми.
[01:21:52.080 --> 01:22:00.080]  Из-за изоморфизма. Ну, а значит ранг V1 и так далее Vkt.
[01:22:01.080 --> 01:22:06.080]  Это будет ранг системы столбцов α1 и так далее αkt.
[01:22:07.080 --> 01:22:11.080]  Ну, а это то же самое по определению, что ранг соответствующей матрицы.
[01:22:12.080 --> 01:22:16.080]  Давайте вот я назову эту матрицу А, чтобы еще раз не писать.
[01:22:17.080 --> 01:22:18.080]  Это будет ранг А.
[01:22:18.080 --> 01:22:22.080]  То есть, если нам векторы заданы в координатах,
[01:22:23.080 --> 01:22:30.080]  для вычисления ранга системы нам достаточно посмотреть на ранг соответствующей матрицы.
[01:22:31.080 --> 01:22:36.080]  Ну, в следующий раз еще поговорим о том, как его быстренько вычислить, а на сегодня, наверное, все.

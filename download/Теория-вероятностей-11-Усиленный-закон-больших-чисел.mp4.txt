[00:00.000 --> 00:10.960]  Да, ну в общем, мы запись начали с некоторым познанием, но вот всё, что произошло, написано
[00:10.960 --> 00:18.080]  на доске. Мы пытаемся доказать, что исходимость по вероятности следует исходимость по распределению,
[00:18.080 --> 00:25.120]  и для этого будем использовать критерий исходимости по распределению, который говорит, что последний
[00:25.120 --> 00:28.320]  исходец по распределению, тогда это когда он сходится слабо, а слабая исходимость
[00:28.320 --> 00:32.960]  определение написано здесь. И вот мы будем доказывать им на это, то есть мы говорим,
[00:32.960 --> 00:37.760]  что пусть у нас есть исходящиеся по вероятности последовательность, будем доказывать,
[00:37.760 --> 00:44.800]  что какой бы мы ни взяли непрерывно ограниченную функцию, справили вот это. Начиная с некоторого
[00:44.800 --> 00:53.160]  момента модуль разости, модуль ожидания f от x и f от x меньше, чем epsilon для какого-то epsilon,
[00:53.160 --> 01:02.440]  который мы зафиксировали. Итак, давайте сделаем следующий трюк. Давайте рассмотрим последовательность
[01:02.440 --> 01:12.360]  это n и обозначим вот таких вот случайных величин x умножить на индикатор того, что модуль x меньше
[01:12.360 --> 01:24.680]  чем n. Понятно, что почти наверное такая последовательность стремится к си. То есть
[01:24.680 --> 01:29.560]  если я увеличиваю n, то начиная с некоторого омега, у меня просто в этом омега значение
[01:29.560 --> 01:35.360]  совпадет с кси. Вот из этого, конечно, следует исходимость по вероятности, это мы уже умеем доказывать.
[01:35.360 --> 01:50.440]  В частности, что у нас означает исходимость вероятности? В частности, это означает,
[01:50.440 --> 02:03.520]  что вероятность того, что модуль это n минус кси больше чем 1 вторая, стремится к нулю. А эта
[02:03.520 --> 02:09.400]  вероятность это на самом деле в точности вероятность того, что модуль кси больше чем n.
[02:09.400 --> 02:21.080]  Но что это означает? Это означает, что мы сможем найти какой-то n1, что для любого n больше
[02:21.080 --> 02:29.920]  либо равного чем n1 вероятность того, что модуль кси больше чем n меньше либо равна чем
[02:29.920 --> 02:38.840]  Эпсилон поделить на 3c.
[02:38.840 --> 02:54.440]  Вот. Хорошо. Это первая вещь, которую мы будем использовать. Теперь, собственно, равномерная непрерывность,
[02:54.440 --> 03:05.560]  о которой я говорил. Вот давайте возьмем отрезок ну скажем от минус 2n1 до 2n1.
[03:05.560 --> 03:12.840]  Значит скажем, что f равномерно непрерывно на нём. То есть, если f непрерывно на cmr, то на
[03:12.840 --> 03:26.440]  любом отрезке она будет равномерно непрерывна. Что это значит?
[03:26.440 --> 03:43.120]  Ну это значит, что найдется такое дельта. Давайте выберем его меньше чем, меньше чем n1.
[03:43.120 --> 03:50.320]  Понятно, что мы можем брать его. То есть, если мы какой-то дельта нашли, то любое меньшее дельта тоже подходит.
[03:50.320 --> 04:03.120]  Это значит, что для любых x и y, модуль разности между которыми меньше чем дельта, модуль f от x минус f от y меньше чем f сам поделить на 3.
[04:13.120 --> 04:18.520]  Так, это вторая вещь, которую мы будем использовать. И третья вещь, которую мы будем использовать, то, что нам доносится.
[04:18.720 --> 04:30.720]  Сходимость по вероятности. То есть, мы знаем, что ксиен стремится по вероятности кси. Это значит, что, что нам, сколько нам нужно?
[04:30.720 --> 04:42.720]  Так, дельта нам нужно. Вероятность того, что модуль ксиен минус кси больше чем дельта, стремится к нулю.
[04:42.920 --> 04:48.920]  Да, это вот то самое дельта, которое мы здесь взяли, вот это дельта.
[04:51.920 --> 04:54.920]  Ну, в частности, это означает, что найдется такое n2.
[04:56.920 --> 05:03.920]  Что от любого n больше нула, чем n2. Вероятность того, что модуль ксиен минус кси больше чем дельта,
[05:04.120 --> 05:14.120]  меньше равна, чем f сам поделить на 3.
[05:22.120 --> 05:26.120]  А теперь, мы давайте все это дело будем использовать для нахождения.
[05:26.320 --> 05:34.320]  Возьмем, выберем n больше чем максимум из n1 и n2, тогда мы все это смело сможем использовать.
[05:36.320 --> 05:42.320]  И, значит, для него посмотрим на модуль разности между от ожиданиям f от кси от n и от ожиданиям f от кси.
[05:42.520 --> 05:59.520]  Ну, в силу линейности, это есть, конечно, модуль от ожидания от разности f от кси от n минус f от кси.
[05:59.720 --> 06:11.720]  Что меньше не равно, чем от ожидания модуля разности.
[06:11.920 --> 06:29.920]  Вот, а дальше давайте посмотрим на отдельную ситуацию, когда модуль кси больше чем n1,
[06:30.120 --> 06:36.120]  и модуль кси меньше оно, чем n1.
[06:36.120 --> 06:46.120]  То есть, представим это как от ожидания от модуля f от кси от кси n минус f от кси
[06:46.320 --> 06:58.320]  на индикатор того, что модуль кси больше, чем n1,
[06:58.320 --> 07:06.320]  плюс от ожидания модуля f от кси n минус f от кси
[07:06.320 --> 07:12.320]  на индикатор того, что модуль кси меньше, чем n1.
[07:12.520 --> 07:18.520]  Ага, да, давайте здесь будем не на 3c делить, я прошу прощения, а на 6c.
[07:20.520 --> 07:26.520]  Здесь будем делить на 6c, да, мы тут имеем право любую константу написать, вот давайте 6c напишем.
[07:28.520 --> 07:37.520]  Тогда, так как значение нашей функции f на любом аргументе меньше 0, чем c,
[07:37.720 --> 07:41.720]  то вот этот модуль разности всегда не превосходит 2c,
[07:41.720 --> 07:45.720]  да, вообще какие бы ни были аргументы, эта штука не превосходит 2c,
[07:45.720 --> 07:51.720]  поэтому мы этим 2c этому от ожидания можем ограничить, умножить на вероятность вот этого события,
[07:51.720 --> 07:55.720]  да, потому что в ней этого события будет просто 0, потому что это индикатор,
[07:55.720 --> 07:59.720]  умножить на вероятность того, что модуль кси больше, чем n1.
[08:00.720 --> 08:04.720]  Но эта вероятность у нас вот тут вот ограничена, как кси поделить на 6c,
[08:04.920 --> 08:08.920]  поэтому, господи, f сам поделить на 6c, мы перемножим, получим f сам поделить на 3,
[08:09.920 --> 08:11.920]  да, то есть вот с этим у нас все просто.
[08:12.920 --> 08:15.920]  Что делать с вторым слагаем? Второе слагаем, давайте еще разобьем,
[08:15.920 --> 08:17.920]  у нас вот есть еще два условия,
[08:18.920 --> 08:22.920]  сейчас второй трейс, да, вот эти два, вот мы в соответствии с ними это дело разобьем.
[08:23.920 --> 08:30.920]  Я могу еще раз, а почему мы так можем ограничить вот 90 почти 2c вот на p на xn?
[08:31.120 --> 08:33.120]  Вот это или что?
[08:33.120 --> 08:37.120]  Да, да, ну смотрите, да, вот эта штука, она просто,
[08:39.120 --> 08:42.120]  эта штука, она просто не превосходит 2c,
[08:46.120 --> 08:49.120]  да, потому что значение функции f не превосходит c по модуле,
[08:51.120 --> 08:55.120]  вот, а значит вы всю эту случайную величину, вот все это произведение можете ограничить
[08:55.120 --> 08:57.120]  как 2c умножить на индикатор,
[08:57.320 --> 09:02.320]  а мод ожидания от 2c умноженного на индикатор, это в точность вот эта штука.
[09:04.320 --> 09:05.320]  Спасибо.
[09:09.320 --> 09:13.320]  Вот, значит во втором слагаемом мы индикаторы еще разобьем на две части,
[09:17.320 --> 09:24.320]  мод ожидания модуль f от xn минус f от xi умножить на индикатор того,
[09:24.520 --> 09:30.520]  что модуль xi больше меньше, меньше 0 чем n1,
[09:32.520 --> 09:35.520]  это значит первое условие,
[09:37.520 --> 09:39.520]  и второе условие вот это, да,
[09:39.520 --> 09:42.520]  которое у нас в третьем возникает в пункте,
[09:42.520 --> 09:46.520]  модуль xi минус xi n больше, чем delta,
[09:47.520 --> 09:50.520]  ну и соответственно то же самое, только меньше либо равно,
[09:50.720 --> 09:54.720]  так, меньше либо равно, да, давайте вот здесь вот напишем меньше либо равно,
[09:54.720 --> 09:59.720]  имеем право вот здесь меньше либо равно написать, чтобы все было строго, секунду.
[10:04.720 --> 10:10.720]  Значит умножить на мод ожидания модуль f от xi n минус f от xi
[10:11.720 --> 10:16.720]  на индикатор того, что модуль xi меньше 0 чем n1
[10:16.920 --> 10:22.920]  и модуль xi минус xi n меньше 0 чем delta.
[10:30.920 --> 10:34.920]  Вот, но со вторым слагаемом поступим точно так же, как с первым поступили,
[10:36.920 --> 10:42.920]  то есть опять же мы понимаем с вами, что вот эта штука меньше 0 чем 2c,
[10:46.920 --> 10:49.920]  поэтому можно написать плюс 2c на вероятность второго события,
[10:49.920 --> 10:51.920]  второе событие даже можно увеличить,
[10:52.920 --> 10:55.920]  написать просто вероятность того, что модуль xi минус xi n больше, чем delta,
[10:55.920 --> 11:00.920]  благо мы знаем, что она, ага, тоже 6c здесь,
[11:00.920 --> 11:03.920]  здесь тоже 6c мы пишем, а не 3c,
[11:05.920 --> 11:10.920]  вот, то есть точно так же получим epsilon positive на 3, как с первым слагаемом,
[11:11.920 --> 11:14.920]  а наконец с третьим слагаемом мы будем использовать второе условие,
[11:15.120 --> 11:20.120]  да, мы знаем, что если между аргументами расстояние не больше, чем delta,
[11:21.120 --> 11:23.120]  и они лежат вот в этом отрезке,
[11:27.120 --> 11:30.120]  то тогда модуль разности
[11:30.120 --> 11:34.120]  извините, вот в третьей строчке снизу вы написали модуль xi меньше n1,
[11:34.120 --> 11:38.120]  а в нижней модуль xi больше, чем n1,
[11:42.120 --> 11:44.120]  или это не к тому относится
[11:44.320 --> 11:46.320]  сейчас, вот это, это вот это
[11:50.320 --> 11:52.320]  вот второе слагаемое, вот
[11:53.320 --> 11:55.320]  и сейчас я третьим слагаемым занимаюсь
[12:00.320 --> 12:04.320]  да, значит еще раз, если у меня два аргумента лежат вот в этом отрезке
[12:04.320 --> 12:06.320]  и между ними расстояние меньше, чем delta,
[12:07.320 --> 12:09.320]  тогда я могу
[12:09.320 --> 12:12.320]  модуль разности значений
[12:12.520 --> 12:14.520]  в моей функции ограничить вот так
[12:15.520 --> 12:18.520]  у меня как раз эта ситуация, вот у меня два аргумента, xi на xi
[12:19.520 --> 12:21.520]  между ними расстояние меньше, чем delta
[12:21.520 --> 12:23.520]  и один из них меньше, чем n1
[12:23.520 --> 12:26.520]  но это значит, что второй меньше, чем n2
[12:26.520 --> 12:28.520]  так как у меня delta меньше, чем n1
[12:28.520 --> 12:31.520]  то значит оба аргумента попадают в этот отрезок
[12:33.520 --> 12:35.520]  значит я теперь могу второе условие применить
[12:35.520 --> 12:37.520]  и получить, что
[12:37.520 --> 12:40.520]  при условии выполнения вот этого индикатора
[12:40.720 --> 12:43.720]  то есть эта штука строго меньше, чем epsilon поделить на 3
[12:43.720 --> 12:46.720]  а значит в моем ожидании меньше, чем epsilon поделить на 3
[12:49.720 --> 12:51.720]  умножить там на вероятность, но
[12:52.720 --> 12:54.720]  я даже вероятность просто уберу
[12:54.720 --> 12:56.720]  понятно, что у меня от этого получится оценка сверху
[12:56.720 --> 12:58.720]  просто напишу epsilon поделить на 3
[12:58.720 --> 13:00.720]  но все это вместе даст epsilon
[13:02.720 --> 13:04.720]  все это вместе даст epsilon
[13:05.720 --> 13:07.720]  что я требуюсь
[13:07.920 --> 13:09.920]  доказали, что исходимость по вероятности
[13:09.920 --> 13:11.920]  следует исходимость по распределению
[13:12.920 --> 13:14.920]  есть какие-то вопросы
[13:24.920 --> 13:26.920]  теперь давайте еще немного поговорим
[13:26.920 --> 13:28.920]  можно вопрос, а вот как мы это получили
[13:28.920 --> 13:30.920]  диагномерно-непрерывности
[13:30.920 --> 13:32.920]  мы просто взяли какой-то epsilon
[13:32.920 --> 13:34.920]  такой вот, ну в смысле
[13:35.120 --> 13:37.120]  ну просто мы, короче
[13:37.120 --> 13:39.120]  взяли epsilon на 3, да, и все
[13:41.120 --> 13:43.120]  и мы больше ничем не пользовались
[13:48.120 --> 13:50.120]  не совсем понял вопрос
[13:50.120 --> 13:52.120]  в смысле, мы просто
[13:52.120 --> 13:54.120]  как бы записали намерно-непрерывности
[13:54.120 --> 13:56.120]  и выбрали epsilon поделить на 3
[13:56.120 --> 13:58.120]  и все, то есть мы ни с каких фактов это не уводили
[13:58.120 --> 14:00.120]  мы просто взяли epsilon
[14:00.120 --> 14:02.120]  и мы просто взяли epsilon
[14:02.120 --> 14:04.120]  и мы просто взяли epsilon
[14:04.120 --> 14:06.120]  и не уводили
[14:06.120 --> 14:07.760]  вот никаких фактов не предшествовали
[14:09.760 --> 14:11.760]  merits, никаких фактов это не предшествовал
[14:11.760 --> 14:16.760]  вот это написано просто по сути
[14:16.760 --> 14:18.760]  я определение равномерно непрерывности
[14:18.760 --> 14:20.760]  больше ничего здесь не используем
[14:20.760 --> 14:23.640]  нам доношено функции непрерывно на всем
[14:23.640 --> 14:25.640]  значит мы делаем Secco
[14:25.640 --> 14:29.640]  whatever operation we take, the last of our function will be equal
[14:29.640 --> 14:32.380]  это написано, определенная равномерно неперев올ность
[14:32.380 --> 14:44.380]  я выбрал для удобства, чтобы потом у меня все просуммировалось. я мог здесь просто эпсилу написать, окончательный вывод от этого бы не изменился.
[14:44.380 --> 15:04.380]  ну хорошо, значит мы с вами в процессе доказательства этого утверждения уже сформулировали критерии слабой сходимости, критерии сходимости по распределению, так называемой теориям Александрова.
[15:04.380 --> 15:17.380]  давайте теперь еще сформулируем еще пару полезных критериев, но для сходимости по распределению мы сформулировали, давайте еще сформулируем для сходимости почти наверное и для сходимости по вероятности.
[15:17.380 --> 15:31.380]  вот это в общем-то аналогия так называемого критерия каши. для сходимости почти наверное вообще все понятно. сходимость почти наверное по сути обычная, сходимость последовательности.
[15:31.380 --> 15:39.380]  вы говорите, что для каждого аргумента, почти для каждого аргумента есть сходимость значений функции на этом аргументе.
[15:39.380 --> 15:49.380]  ну и тем самым можно говорить про сходимость, про фундаментальность последовательности 1.
[15:49.380 --> 16:06.380]  то есть что это значит? будем говорить, что ксен фундаментально почти наверное или с вероятностью 1, если вероятность того, что ксен фундаментально она единица.
[16:06.380 --> 16:18.380]  ну и соответственно критерий каши, тогда будет как выглядеть.
[16:18.380 --> 16:43.380]  существует кси такая, что ксен стремится кси почти наверное, тогда и только тогда, когда ксен фундаментально почти наверное.
[16:43.380 --> 16:53.380]  ну это более-менее очевидно, давайте короткое доказательство напишем.
[16:53.380 --> 17:00.380]  ну пусть сперва ксен стремится кси почти наверное.
[17:00.380 --> 17:15.380]  ну в эту сторону совсем просто, тогда рассмотрим множество, давайте вы значим его а таких, что ксен от амега стремится кси от амега.
[17:15.380 --> 17:30.380]  ну у нас есть критерий каши, который говорит, что последовательность сходится тогда и только тогда, когда она фундаментальна.
[17:30.380 --> 17:40.380]  да, поэтому из этого условия следует, что ксен фундаментально.
[17:40.380 --> 17:59.380]  да, значит вероятность b равна единице, это и есть собственно определение фундаментальности последовательности случайных величин.
[17:59.380 --> 18:06.380]  по обратную сторону, пусть ксен фундаментально с вероятностью 1.
[18:06.380 --> 18:16.380]  давайте рассмотрим событие а, на котором наш последователь фундаментально.
[18:16.380 --> 18:39.380]  понятно, что для любого амега из а существует предел при стремящемся к бесконечности ксен от амега, обозначим его кси от амега.
[18:39.380 --> 18:54.380]  ну а вне, положим ну не знаю, ноль.
[18:54.380 --> 19:18.380]  ну нам только понятно, что кси это случайная величина, но это правда, поскольку мы знаем, что предел последовательности случайных величин это снова случайная величина.
[19:18.380 --> 19:32.380]  вот можно взять ксен с волной равная ксен умножить на индикатор события а, и тогда вот эта посредность будет вообще поточечна.
[19:32.380 --> 19:39.380]  то есть вообще для любого амега из амега большое ксен с волной от амега будет стремиться ксетами.
[19:39.380 --> 19:45.380]  да, то есть здесь не почти наверная сходимость, а сходимость вообще для всех амег.
[19:45.380 --> 19:52.380]  и мы знаем, что предел последовательств случайных величин является случайной величиной, след на кси с волной это случайная величина.
[19:52.380 --> 20:06.380]  ну и на множестве а, у которого вероятность равна единице, есть сходимость, а значит есть сходимость почти наверно.
[20:06.380 --> 20:11.380]  что и требовалось.
[20:23.380 --> 20:29.380]  по аналогии можно сформулировать критерий каши сходимости по вероятности, который чуть будет более сложен доказать.
[20:29.380 --> 20:37.380]  давайте сначала скажем, что такое фундаментальность по вероятности.
[20:37.380 --> 20:42.380]  будем говорить, что ксен фундаментально по вероятности.
[20:42.380 --> 21:07.380]  если для любого epsilon больше 0, предел при NAM стремящемся к бесконечности, вероятность того, что модуль ксен минус ксен больше либо равно, чем epsilon, стремится к 0.
[21:07.380 --> 21:19.380]  ну и критерий каши говорит, что есть сходимость по вероятности, когда опасность фундаментально по вероятности.
[21:19.380 --> 21:47.380]  существует такая случайная величина кси, что ксен стремится кси по вероятности, тогда и только тогда, когда ксен фундаментально по вероятности.
[21:47.380 --> 22:01.380]  так ну опять в одну сторону несложно из сходимости по вероятности фундаментальность доказать.
[22:01.380 --> 22:07.380]  пусть ксен стремится по вероятности кси.
[22:17.380 --> 22:31.380]  вот что сделаем, давайте посмотрим на модуль ксен минус ксен.
[22:31.380 --> 22:43.380]  понятно, что эта штука по неранде с треугольника меньше чем сумма модулей ксен минус кси плюс ксен минус кси.
[22:43.380 --> 23:00.380]  да, значит вероятность того, что модуль ксен минус ксен больше оно чем epsilon.
[23:00.380 --> 23:10.380]  ну если оказалось, что модуль ксен минус ксен больше оно чем epsilon, то из этого следует, что хотя бы одна из вот этих штук должна быть больше чем epsilon пополам.
[23:10.380 --> 23:22.380]  то есть вероятность это будет меньше чем вероятность того, что модуль ксен минус ксен больше繼 с逆.
[23:22.380 --> 23:32.380]  плюс вероятность того, что модуль ксен минус ксен больше да, если еще раз из того, что вот это событие влечет объединение вот этих двух событий.
[23:32.380 --> 23:36.380]  из вот этого события объединение этих двух событий следует
[23:36.380 --> 23:48.380]  а вероятность введения событий превосходит в сумму вероятностей, но так как каждый из них стремится к нулю, то эта вероятность тоже стремится к нулю, что требуется.
[23:49.380 --> 23:52.380]  Есть ли какие-то вопросы?
[23:52.380 --> 24:06.380]  Хорошо, теперь в другую сторону доказываем.
[24:07.380 --> 24:11.380]  Пусть теперь ксен фундаментально по вероятностям.
[24:15.380 --> 24:29.380]  Для того, чтобы доказать, что у нас есть сходимость по вероятности, мне потребуется одна очень важная лемма, которую мы будем в дальнейшем использовать не только здесь, но еще и несколько раз.
[24:29.380 --> 24:34.380]  Доказательств других полезных вещей.
[24:35.380 --> 24:43.380]  Ну, кстати, это тоже известная теорема из теории меры, которую Иван Гейн, наверное, рассказывал.
[24:44.380 --> 24:51.380]  О том, что если последность сходится по вероятности, из нее можно извлечь под последность, которая сходится почти на верное.
[24:52.380 --> 25:00.380]  Но это простое упражнение, простое следствие лемма-барреля кантеля. Давайте я его быстренько докажу.
[25:00.380 --> 25:17.380]  Если ксен стремится по вероятности кси, если ксен стремится по вероятности... Ой, господи, нет, не так я буду это формулировать.
[25:17.380 --> 25:31.380]  Если, значит, межфундаментальность по вероятности нужна. Если ксен фундаментально по вероятности, то из нее можно извлечь под последность сходящуюся почти на верное.
[25:32.380 --> 25:45.380]  Значит, существует под последовательность, ну, последовательность чисел НК такая, что ксен-к стремится, и какая-то случайная врачность на кси такая, что ксен-к стремится почти на верное кси.
[25:50.380 --> 25:52.380]  Приказ стремящимся бесконечности.
[25:52.380 --> 26:08.380]  Вот, ну, сперва эту лему докажем. У нее простое доказательство, которое к тому же использует лемма-барреля кантеля, которое мы с вами недавно изучили. Давайте быстренько ее с помощью леммы докажем.
[26:08.380 --> 26:28.380]  Значит, ну, прям построим эту последовательность на самом деле. Возьмем N1 равной единице от любого к, начиная с двойки, положим Nk.
[26:28.380 --> 26:38.380]  Nk это минимальное такое g больше либо равное, чем, строго больше, чем Nk-1.
[26:38.380 --> 27:05.380]  Таким, что для любых R и S больше либо равных, чем g, вероятность того, что модуль кси R минус кси S больше, чем 2 в степени минус k, это вероятность меньше, чем 2 в степени минус k.
[27:08.380 --> 27:19.380]  Ну, то есть понятно, что такое Nk просто найдется за счет того, что, на что я здесь делал, зафиксировал некоторые ексилом, ну, в каком-то странном виде 2 в степени минус k.
[27:19.380 --> 27:25.380]  Так как последовательность фундаментально по вероятности это значит, что предел при R и S стремящимся бесконечности такой вероятности равен 0.
[27:25.380 --> 27:35.380]  То есть вероятность имеется к 0 при R и S стремящимся бесконечности. Ну, значит, при достаточно больших R и S она будет меньше в частности, чем вот то, что эти штуки совпадают.
[27:35.380 --> 27:43.380]  И они совпадают, это совпадение. Я мог и разными сделать, но мне удобно их выбрать такими. То есть вот это дельта, а вот это ексилом.
[27:43.380 --> 27:52.380]  Вероятность того, что модуль кси R минус кси S больше, чем дельт стремится к 0, ну, значит, с некоторым моментом это вероятность меньше, чем такой ексил.
[27:52.380 --> 28:01.380]  Вот, поэтому такие тахлопастинцы я смогу построить. Вот, и дальше надо заметить, что...
[28:05.380 --> 28:11.380]  Что надо заметить? Надо заметить, что вероятность того, что
[28:14.380 --> 28:20.380]  модуль кси Nk-1
[28:22.380 --> 28:32.380]  больше, чем 2 в степени минус k-1
[28:33.380 --> 28:35.380]  бесконечно часто
[28:38.380 --> 28:49.380]  равна 0, так как... Ну, применяем млем барреля контель. Да, вот такая вероятность бесконечно часто равна 0, если мы знаем, что сумма
[28:49.380 --> 28:53.380]  таких вероятностей конечна.
[28:57.380 --> 29:01.380]  Она действительно конечна, так как мы так выбирали Nk-1.
[29:02.380 --> 29:05.380]  Сумма таких вот вероятностей она
[29:07.380 --> 29:11.380]  меньше, чем сумма 2 в степени минус k-1, что конечна.
[29:12.380 --> 29:20.380]  Да, вот зачем мне нужна здесь, вот зачем мне вот в этом месте нужна геометрическая прогрессия, чтобы сумма сошлась.
[29:21.380 --> 29:24.380]  Ну, я там мог 1,9k в квадрате написать, тоже бы сошлось, конечно.
[29:25.380 --> 29:27.380]  Вот.
[29:28.380 --> 29:31.380]  Значит, раз сумма ограничена, то
[29:32.380 --> 29:38.380]  вероятность того, что модуль кси Nk-1 больше, чем эта штука бесконечно часто равна 0.
[29:39.380 --> 29:43.380]  Теперь зачем мне вот здесь нужна тоже геометрическая прогрессия?
[29:44.380 --> 29:45.380]  Последующей причине.
[29:46.380 --> 29:47.380]  Значит,
[29:48.380 --> 29:50.380]  рассмотрим событие
[29:51.380 --> 29:52.380]  A
[29:53.380 --> 29:54.380]  таких Омега,
[29:55.380 --> 29:58.380]  что модуль кси Nk-1
[29:59.380 --> 30:02.380]  больше, чем 2 в степени минус k-1 бесконечно часто.
[30:03.380 --> 30:08.380]  Понятно, что вне этого события, то есть в любом Омега, который А не принадлежит,
[30:10.380 --> 30:18.380]  вот это вот дело выполнено лишь конечное число раз, там 5, 17, 28, миллион, неважно сколько, но конечное число раз.
[30:19.380 --> 30:23.380]  А значит, начиная с некоторого момента, будет меньше, чем равно.
[30:24.380 --> 30:28.380]  А значит, сумма модулей
[30:29.380 --> 30:33.380]  кси Nk-1
[30:34.380 --> 30:36.380]  по всем k, да, 2 до бесконечности?
[30:37.380 --> 30:38.380]  Конечно.
[30:39.380 --> 30:44.380]  Раз начиная с некоторого момента, мы можем ограничить нашу последовательность геометрической прогрессии,
[30:45.380 --> 30:47.380]  значит сумма конечна.
[30:48.380 --> 30:52.380]  То есть вот такой вот ряд сходится абсолютно
[30:53.380 --> 30:55.380]  а значит, если я модуль сниму,
[30:57.380 --> 30:59.380]  а именно рассмотрю кси Nt,
[31:04.380 --> 31:06.380]  кси Nt, mt скажем,
[31:07.380 --> 31:08.380]  равная сумме по всем
[31:12.380 --> 31:13.380]  равной чему?
[31:14.380 --> 31:15.380]  равной кси N1
[31:16.380 --> 31:17.380]  плюс сумма по всем k,
[31:18.380 --> 31:19.380]  от 2 до m
[31:20.380 --> 31:22.380]  кси Nk-1
[31:23.380 --> 31:27.380]  вот эта штука будет сходиться прием стремящейся бесконечности.
[31:29.380 --> 31:32.380]  Да, так как если я вот здесь вот бесконечность напишу,
[31:33.380 --> 31:37.380]  то этот ряд будет сходиться до всего того, что он сходится абсолютно.
[31:39.380 --> 31:43.380]  Значит, эта штука стремится к нектому кси
[31:45.380 --> 31:47.380]  на каждой омеге.
[31:48.380 --> 31:51.380]  Да, то есть вот я для любого омега и за счертой вот это могу утверждать.
[31:51.380 --> 31:53.380]  Что найдется некоторое кси.
[31:56.380 --> 31:59.380]  Такое, что вот эта штука стремится кси.
[32:01.380 --> 32:05.380]  Ну все, это есть в общем-то искомая случайная величина.
[32:06.380 --> 32:09.380]  То есть теперь положим для всех омегой из A
[32:11.380 --> 32:13.380]  кси от омега равна нулю
[32:14.380 --> 32:19.380]  и получим, что кси Nt стремится почти наверно кси, что требуется.
[32:21.380 --> 32:25.380]  Таким образом, если есть какие-то вопросы,
[32:26.380 --> 32:28.380]  то вероятность события равна нулю.
[32:29.380 --> 32:31.380]  Можете наполнить?
[32:32.380 --> 32:34.380]  Полеем баррель кантели.
[32:35.380 --> 32:40.380]  Да, это в точности вероятность события.
[32:41.380 --> 32:45.380]  Она равна нулю, так как полеем баррель кантели
[32:45.380 --> 32:50.380]  мы можем это выяснить на основе того, что мы знаем, что вот такой ряд сходится.
[32:51.380 --> 32:53.380]  Раз такой ряд сходится, то полеем баррель кантели
[32:54.380 --> 32:56.380]  вероятность того же события выполнена бесконечно сейчас равна нулю.
[32:57.380 --> 32:59.380]  Понятно, спасибо.
[33:01.380 --> 33:03.380]  Окей.
[33:05.380 --> 33:09.380]  Теперь мы из этой леммы должны выяснить, что это такое.
[33:10.380 --> 33:15.380]  Теперь мы из этой леммы должны выяснить то, что нам нужно,
[33:16.380 --> 33:21.380]  а именно почему из фундаментальности вероятности следует сходимость вероятности.
[33:22.380 --> 33:26.380]  Ну вот мы уже предположили фундаментальность вероятности.
[33:27.380 --> 33:35.380]  Значит, по нашей лемме мы теперь можем выделить подпосредственно сходящиеся почти наверно.
[33:35.380 --> 33:41.380]  И вот эта кси, это и будет наша интересующая нас случайная верещина,
[33:42.380 --> 33:43.380]  к которой по вероятности будет сходиться кси.
[33:44.380 --> 33:46.380]  То есть осталось доказать, что ксиан сходится по вероятности кси.
[33:48.380 --> 33:52.380]  Ну, давайте, это, наверное, уже просто.
[33:53.380 --> 33:55.380]  Значит, давайте...
[33:57.380 --> 33:59.380]  Сейчас.
[33:59.380 --> 34:03.380]  Во-первых, исходимости, почти наверно, следует сходимость вероятности.
[34:04.380 --> 34:07.380]  То есть мы знаем, что ксиан ката сходится и по вероятности кси.
[34:09.380 --> 34:11.380]  Вот, поэтому давайте сделаем две вещи.
[34:12.380 --> 34:14.380]  Во-первых, возьмем такое...
[34:17.380 --> 34:19.380]  Каноль.
[34:20.380 --> 34:22.380]  Каноль.
[34:23.380 --> 34:25.380]  Каноль.
[34:26.380 --> 34:28.380]  Каноль.
[34:31.380 --> 34:34.380]  Что для любого к больше равно, чем каноль.
[34:39.380 --> 34:43.380]  Модуль ксиан ката.
[34:44.380 --> 34:49.380]  Вероятность того, что модуль ксиан ката минус кси.
[34:56.380 --> 34:58.380]  Сейчас. Нет. Давайте знать, как сделаем.
[34:59.380 --> 35:00.380]  Еще немножко сложнее.
[35:01.380 --> 35:03.380]  Давайте напишем просто, что модуль ксиан ката и минус...
[35:04.380 --> 35:06.380]  Вероятность того, что модуль ксиан ката и минус кси.
[35:07.380 --> 35:12.380]  Больше равно, чем эпсалон пополам стремится к нулю.
[35:13.380 --> 35:14.380]  Приказ стремящейся бесконечности.
[35:15.380 --> 35:16.380]  Вот так вот напишем. Это с одной стороны.
[35:17.380 --> 35:19.380]  А с другой стороны,
[35:19.380 --> 35:21.380]  В силу...
[35:24.380 --> 35:26.380]  Фундаментальности по вероятностям.
[35:29.380 --> 35:38.380]  Мы знаем, что предел при n стремящейся бесконечности и n катом стремящейся бесконечности.
[35:39.380 --> 35:40.380]  Ну, точнее k.
[35:41.380 --> 35:42.380]  Правильно писать.
[35:43.380 --> 35:44.380]  Предел при n стремящейся бесконечности и k стремящейся бесконечности.
[35:45.380 --> 35:50.380]  Вероятность того, что модуль ксиан минус кси н ката, тоже больше Dust .
[35:51.380 --> 35:56.380]  Да, первая сила лемма, вторая сила фундаментальности, вероятность.
[35:56.380 --> 36:02.220]  вероятность и вероятность. Ну и все, а теперь возьмем и, как выше, обратим внимание, чтобы не
[36:02.220 --> 36:08.340]  равнился треугольника, вероятность того, что mod xn – xi больше значим epsilon, не превосходит
[36:08.340 --> 36:16.340]  вероятность того, что mod xn – xnkt больше значим epsilon пополам, плюс вероятность того, что mod xnkt
[36:16.340 --> 36:28.540]  – xi больше значим epsilon пополам, так как обе вероятности стремятся к нулю, то и сумма тоже стремится к нулю, что и требуется.
[36:39.220 --> 36:40.420]  Так, есть ли какие-то вопросы?
[36:46.340 --> 37:02.340]  Хорошо, мы наконец-то с вами, в общем, поговорили всякие общие факты про сходимости, и можем
[37:02.340 --> 37:06.460]  теперь непосредственно перейти к предельным теоримам. И первое, с чего мы начнем, это так
[37:06.460 --> 37:15.700]  называемые усиленные законы больших чисел, сокращенно УЗБЧ, усиленный закон больших чисел.
[37:15.700 --> 37:26.460]  Значит, у вас уже был закон больших чисел, и в нем, я напомню, фигурировала сходимость по вероятности.
[37:26.460 --> 37:37.460]  Вот, значит, чем усиленный закон больших чисел означается от обычного закона больших чисел,
[37:37.460 --> 37:46.460]  тем, что сходимость в нем почти наверно не по вероятности, тем, что сходимость в нем почти
[37:46.460 --> 37:54.460]  наверно не по вероятности. Вот, ну давайте для начала сформулируем. У нас будет два усиленных
[37:54.460 --> 37:58.460]  закон больших чисел. Усиленные закон больших чисел, они будут немного отличаться условиями.
[37:58.460 --> 38:02.460]  То есть нельзя будет сказать, что один сильнее, чем другой, в одном будут одни условия, которые в каком
[38:02.460 --> 38:06.460]  смысле будут сильнее, чем в другом, в другом будут другие условия, которые будут сильнее, чем условия
[38:06.460 --> 38:18.460]  в первом. И вот начнем, назовем его УЗБЧ-1. Так называемый усиленный закон больших чисел для случайных
[38:18.460 --> 38:26.460]  величин с ограниченной дисперсией. То есть здесь утверждается, что нужно использовать некоторую
[38:26.460 --> 38:32.460]  ограниченность дисперсии в отличие от второго усиленного закона больших чисел, в котором вообще дисперсия
[38:32.460 --> 38:46.460]  даже не обязана существовать. Итак, пусть ксиен независимые случайные величины.
[38:46.460 --> 38:58.460]  И пусть у них есть дисперсия. Ну, есть конечно второй момент.
[38:58.460 --> 39:06.460]  Они одинаково распределены? Нет, одинаковой распределенности не нужно.
[39:06.460 --> 39:20.460]  Значит, пусть, кроме того, существует некоторая последовательность Бэнте, не отрицая положительных чисел,
[39:20.460 --> 39:42.460]  которая не убывает. И в пределе равна бесконечности. Такая, что сумма по всем n дисперсии ксиен
[39:42.460 --> 39:56.460]  соответственно Бэнт в квадрате, конечно.
[39:56.460 --> 40:02.460]  Сходится вот такой ряд суммы дисперсии ксиен в квадрате.
[40:02.460 --> 40:20.460]  Тогда, Sn минус мат ожидания Sn. Значит, Sn это сумма первых n.
[40:20.460 --> 40:28.460]  Как обычно. Значит, Sn минус мат ожидания Sn поделительно Bn стремится почти, наверное, к нулю.
[40:28.460 --> 40:40.460]  Давайте соотнесем это с обычным законом больших чисел, поймем, какие вообще Бэнты можно выбирать и что вообще происходит.
[40:40.460 --> 40:50.460]  Для начала я напомню, что в обычном законе больших чисел совсем не обязательно нужна независимость.
[40:50.460 --> 40:58.460]  Там нужна попарная независимость или более того, более слабые условия попарная некоррелируемость.
[40:58.460 --> 41:04.460]  То есть, чтобы к вариации всех пар случайных причин были равны нулю. Здесь этого недостаточно, здесь нужна независимость.
[41:04.460 --> 41:11.460]  В этом смысле он слабее, но этот закон силен именно с сходимостью почти, наверное, поэтому он называется усилием.
[41:11.460 --> 41:17.460]  Теперь давайте попробуем соотнести знаменатели.
[41:17.460 --> 41:29.460]  Вы помните в предположении, что все дисперсии ограничены? Мы и так формулировали усиленный вариант, так называемый.
[41:29.460 --> 41:36.460]  Обычный закон больших чисел, там все одинаково распределены, в знаменателе n стоит.
[41:36.460 --> 41:48.460]  Но мы потом с вами выясним, что если все дисперсии просто ограничены одним и тем же цепь, то в знаменателе можно поставить все что угодно больше, чем корень из n.
[41:48.460 --> 41:58.460]  Ну вот здесь чуть хуже. Во-первых, если я в знаменателе корень из n поставлю, то здесь у меня это будет n.
[41:58.460 --> 42:06.460]  Если я дисперсии просто меняю на константу, у меня получается сумма цепь 9 и трет расходится. То есть корень из n нельзя.
[42:06.460 --> 42:14.460]  Ну и прямо чуть-чуть больше тоже нельзя. То есть если напишу корень из n log log log n, то такая штука тоже будет расходиться.
[42:14.460 --> 42:20.460]  И вот она будет расходиться вплоть до того самого повторного логарифма и закона повторного логарифма, что неудивительно.
[42:21.460 --> 42:31.460]  То есть если вы здесь напишете под корень повторный логарифм, то это будет как раз место, в котором у вас все еще будет расходиться.
[42:31.460 --> 42:37.460]  Но если вы возьмете уже чуть больше, если тут будет n log в квадрате log n, то уже будет сходиться.
[42:37.460 --> 42:49.460]  То есть как раз это log log n, это та самая граница, в которой закон больших чисел начинает или перестает переходить в усиленный закон больших чисел.
[42:49.460 --> 42:55.460]  Есть лишь небольшой зазор, в котором закон больших чисел в этом смысле сильнее.
[42:55.460 --> 43:04.460]  Сходимость по вероятности есть в знаменателе, когда есть что угодно растущее быстрее, чем корень из n, вплоть до корень из n log log n.
[43:04.460 --> 43:11.460]  То есть для такого диапазона знаменателей у нас есть сходимость по вероятности, но не сходимость почти наверно.
[43:11.460 --> 43:17.460]  А для более сильно растущих знаменателей уже будет сходимость почти наверно.
[43:20.460 --> 43:26.460]  Ну вот давайте я для того, чтобы это было записано, напишу здесь какое-нибудь простое замечание.
[43:26.460 --> 43:34.460]  Значит в частности, если все случайные величины одинаково распределены.
[43:49.460 --> 43:59.460]  То, ну, например, sn минус от ожидания sn поделить на n, стремится почти наверно к нулю.
[43:59.460 --> 44:09.460]  Ну и вообще здесь в знаменателе можно там, скажем, написать n в степени 1 вторая плюс дельта, для любого дельта больше нуля.
[44:09.460 --> 44:22.460]  Напарено же можно еще сильнее уменьшать знаменатель, не только писать степень n, но, как я уже говорил, там можно писать корень из n умножить там на логариф, например, тоже будет правдой.
[44:24.460 --> 44:28.460]  А одинаково распределенность я здесь написал просто для того, чтобы все дисперсии стали одинаковыми.
[44:28.460 --> 44:34.460]  То есть вот эти числа становятся одинаковыми, они на расходимость этого ряда перестают влиять.
[44:35.460 --> 44:38.460]  Что еще раз?
[44:38.460 --> 44:40.460]  Дисперсии должны существовать, да?
[44:40.460 --> 44:48.460]  Да, ну и когда я пишу в частности, я имею в виду, что все это выполнено. То есть мы в теории уже написали, что есть второй момент.
[44:48.460 --> 44:52.460]  Конечно, да, если дисперсии нет, то мы не сможем применить теорию.
[44:52.460 --> 45:06.460]  Да, существование дисперсии здесь по существу и все доказательства опирается именно на переход отсюда-сюда.
[45:06.460 --> 45:14.460]  То есть вот из этого нужно сделать вот этот вывод. То есть от сходимости дисперсии нужно перейти к сходимости случайных величин.
[45:14.460 --> 45:19.460]  То есть в каком смысле сходимости V2 нужно перейти к сходимости почти наверно.
[45:20.460 --> 45:28.460]  И мы сегодня не успеем этот теориям доказать, но я начну рассказывать всякие утверждения, которые будут для этой теории применяться.
[45:35.460 --> 45:42.460]  Еще раз ключевой момент. Нам нужно научиться переходить от сходимости вторых моментов к сходимости случайных величин.
[45:42.460 --> 45:45.460]  Ну вот непосредственно этот переход мы, наверное, в следующий раз осуществим.
[45:45.460 --> 45:49.460]  А сейчас я начну доказывать всякие вспомогательные утверждения для того, чтобы этот переход осуществить.
[45:49.460 --> 45:56.460]  Вот этот переход от сходимости дисперсии к сходимости почти наверно называется теориям Калмогорова и Хинччина о сходимости рядов.
[45:56.460 --> 46:03.460]  И мы его с вами докажем в следующий раз. Сегодня хотя бы надо сформулировать.
[46:03.460 --> 46:07.460]  Успеем, но может даже сформулировать не успеем, там еще перед ним нужно 2 леммы доказать.
[46:07.460 --> 46:10.460]  Значит, сперва докажем вот такую лему.
[46:16.460 --> 46:23.460]  Ну тоже такого своего рода криперисходимости почти наверно.
[46:23.460 --> 46:29.460]  Значит, ксиен фундаментально почти наверно.
[46:29.460 --> 46:41.460]  Тогда и только тогда, когда для любого эпсилум больше нуля предел при n-стримящемся бесконечности,
[46:41.460 --> 46:51.460]  вероятности того, что супремум по всем k больше чем равным единице модуль кси n плюс kt
[46:51.460 --> 46:57.460]  минус кси nt больше чем эпсилум с триммерным пределами нуля.
[46:57.460 --> 47:10.460]  Но вот это некоторый очень полезный способ записывать сходимость почти наверно,
[47:10.460 --> 47:17.460]  которую мы будем использовать, когда как раз будем переходить от сходимости дисперсии к сходимости почти наверно стученных величин.
[47:17.460 --> 47:21.460]  Так, давайте эту лему докажем.
[47:22.460 --> 47:26.460]  Значит, во-первых, переформулируем ее немного.
[47:26.460 --> 47:35.460]  Доказывать вот явно вот это не очень понятно как, но сейчас мы напишем равносильную вещь.
[47:35.460 --> 47:38.460]  Ее будет доказывать проще. Значит, смотрите.
[47:38.460 --> 47:46.460]  При делу такой вероятности равен нулю,
[47:46.460 --> 47:55.460]  тогда и только тогда, когда предел при n- и m-стримящемся бесконечности,
[47:55.460 --> 48:00.460]  то есть предел при н- и м-стримящемся бесконечности,
[48:00.460 --> 48:06.460]  то есть предел при н- и м-стримящемся бесконечности,
[48:06.460 --> 48:11.460]  то есть предел при н- и м-стримящемся бесконечности,
[48:11.460 --> 48:17.460]  то есть предел при n- и m-стримящемся бесконечности,
[48:17.460 --> 48:26.460]  нет, мы написали предел при n-стримящемся бесконечности, а m внутрь, извините.
[48:26.460 --> 48:33.460]  Значит, предел при n-стримящемся бесконечности вероятность того, что suprem по k и l
[48:34.460 --> 48:41.460]  больше равном чем n, модули того, что xk-xl больше чем ε, равна нулю.
[48:41.460 --> 48:48.460]  Почему это одно и то же? Ну понятно, что понятно.
[48:48.460 --> 48:54.460]  Понятно, что сейчас, если вот это равно нулю, то и это равно нулю.
[48:54.460 --> 48:59.460]  То есть вот справа налево понятно,
[48:59.460 --> 49:08.460]  потому что вот этот suprem он меньше либо равен, чем вот этот suprem.
[49:08.460 --> 49:13.460]  Здесь suprem вообще по всем k и l больше чем n, здесь каким-то специальным.
[49:13.460 --> 49:19.460]  То есть в роли l и n зафиксировали, а вот k уже всевозможны.
[49:19.460 --> 49:24.460]  Но на самом деле есть пределы одинаковые, просто потому что не раньше треугольник выполнен.
[49:24.460 --> 49:42.460]  Да, если вы знаете, что модуль xn плюс k1 минус xn больше чем,
[49:42.460 --> 49:53.460]  меньше либо равно, чем ε пополам, и модуль xn плюс k2 минус xn меньше либо равно, чем ε пополам,
[49:53.460 --> 50:02.460]  то из этого следует по нераде с треугольника, что модуль xn плюс k1 минус xn плюс k2 меньше равно, чем ε.
[50:02.460 --> 50:09.460]  Да, поэтому вот вы взяли какие-то k и l больше чем n.
[50:09.460 --> 50:14.460]  Вот здесь поставили кс и ка и посмотрели на такую вероятность.
[50:14.460 --> 50:20.460]  Поставили кс и l пополам, и потом кс и l и посмотрели тоже на такую вероятность.
[50:20.460 --> 50:25.460]  Обе стремятся к нулю, а значит такая вероятность будет тоже стремиться к нулю.
[50:25.460 --> 50:30.460]  Да, то есть поэтому слева направо тоже верно по нераде с треугольника.
[50:30.460 --> 50:35.460]  Вот, поэтому эта вещь равносильная.
[50:35.460 --> 50:40.460]  А значит вместо того, чтобы вот это доказывать, вместо того, чтобы первую вещь доказывать, будем доказывать вторую вещь.
[50:40.460 --> 50:44.460]  Это просто. Итак, поехали.
[50:44.460 --> 50:51.460]  Значит, кс и n фундаментально почти наверное.
[50:51.460 --> 50:56.460]  Это что значит по определению?
[50:56.460 --> 50:59.460]  Это значит, что вероятность того, что кс и n фундаментально равна единице.
[50:59.460 --> 51:05.460]  То есть вероятность того, что для любого x больше нуля.
[51:05.460 --> 51:27.460]  Существует такое n0, что для любых k и l, чтобы у него там k и l, существует такое пусть n, что для любых k и l больше равных чем n.
[51:27.460 --> 51:37.460]  Модуль кс и k минус кс и l меньше n, чем epsilon равна единице.
[51:37.460 --> 51:42.460]  Ну давайте возьмем вероятность отрицания этого события, который будет равнулю.
[51:42.460 --> 51:46.460]  То есть вероятность того, что существует epsilon больше нуля.
[51:46.460 --> 51:51.460]  Такое, что для любого n найдутся k и l больше равные чем n.
[51:51.460 --> 51:56.460]  Такие, что модуль кс и k и t минус кс и l и t больше чем epsilon равна нулю.
[51:56.460 --> 52:11.460]  Дальше мы можем перейти, так как вот эти события вложены по n, вот эти события вложены по epsilon.
[52:11.460 --> 52:16.460]  То есть чем меньше epsilon, тем меньше события.
[52:16.460 --> 52:21.460]  Чем больше событий.
[52:21.460 --> 52:28.460]  Чем меньше epsilon, тем больше событий.
[52:28.460 --> 52:30.460]  То есть они расширяются.
[52:30.460 --> 52:40.460]  То мы можем вот это вот существование заменить не на квантор по континуальному множеству, а на квантор по счетному множеству.
[52:40.460 --> 52:47.460]  И написать это как существует такое m натуральное.
[52:47.460 --> 52:51.460]  И потом вместо epsilon написать 1 m.
[52:51.460 --> 52:54.460]  Стандартный трюк.
[52:54.460 --> 53:00.460]  И это уже вероятность объединения счетного количества множеств.
[53:00.460 --> 53:07.460]  Вероятность объединения счетного количества множества равна нулю, тогда это тогда, когда вероятность каждого из них равна нулю.
[53:07.460 --> 53:09.460]  То есть это означает, что для любого n.
[53:09.460 --> 53:14.460]  Вероятность того, что для любого n существует k и l.
[53:14.460 --> 53:21.460]  Хотя бы n, такие что модуль kс и k и t минус kс и l и t больше чем 1 m, то это равна нулю.
[53:21.460 --> 53:31.460]  это мог проделывать и для изначальной задачи, то есть для такого вида супремума, проблема
[53:31.460 --> 53:37.420]  возникла на следующем шаге. значит смотрите, следующий шаг вот какой. вот теперь я смотрю
[53:37.420 --> 53:55.340]  вот на эти события, которые зависит от n, давайте это обозначим. и это события вложены. чем больше
[53:55.340 --> 54:11.180]  n, тем шире события, тем уже события вложены. чем больше n, тем меньше диапазон для поиска этих
[54:11.180 --> 54:24.940]  каэль, тем уже события. и если бы я взял изначальный вариант, я бы этого не мог убеждать,
[54:24.940 --> 54:31.860]  потому что у меня бы одно было бы любое k, а второе было бы n, и поэтому они были бы уже не вложены,
[54:31.860 --> 54:37.980]  то есть диапазон для поиска второго аргумента у меня бы каждый раз просто менялся, не уменьшался,
[54:37.980 --> 54:43.700]  а менялся. то есть у меня каждый раз n был бы фиксирован, поэтому я не мог говорить про вложенность
[54:43.700 --> 54:54.660]  событий. а эти события вложены, и поэтому я могу перейти к пределу. потеремие непрерывности
[54:54.660 --> 54:59.740]  вероятности меры это верно тогда и только тогда, когда предел при отстремляющемся бесконечности,
[54:59.740 --> 55:08.060]  вероятность того, что существует k и l больше либо равной чем n, модуль кси kt и минус кси lt
[55:08.060 --> 55:18.300]  больше чем 1mt равен нулю. но самое время вернуть epsilon. да, опять же, так как чем больше m,
[55:18.300 --> 55:26.380]  тем больше это вероятность, тем, соответственно, больше это предел, то, раз я могу это делать,
[55:26.380 --> 55:32.220]  эту величину сколь угодно малой, то я могу брать произвольно epsilon, и это в общем подтверждение
[55:32.220 --> 55:38.500]  будет разносить. тогда и только тогда, когда любого epsilon больше нуля, предел при отстремляющемся
[55:38.500 --> 55:44.620]  бесконечности, вероятность того, что найдется k и l хотя бы n такие, что модуль кси kt и минус
[55:44.620 --> 55:53.620]  кси lt больше чем epsilon равен нулю. вот, а это ровно то, что нужно доказать, ну точнее вот это.
[55:53.620 --> 55:58.220]  да, то есть вероятность того, что найдется k и l такие, что модуль кси kt и минус кси lt больше
[55:58.220 --> 56:02.420]  чем epsilon, это в точности вероятность того, что supremo больше чем epsilon. ну и давайте
[56:02.420 --> 56:07.260]  перепишем это в окончательном виде, на чем и закончим заказать стемлеммы.
[56:07.260 --> 56:14.540]  так, тогда и только когда любого epsilon больше нуля, предел вероятности того,
[56:14.540 --> 56:24.980]  что supremo пока и l больше равным чем n, модуль кси kt и минус кси lt больше чем epsilon равен нулю.
[56:24.980 --> 56:39.740]  Важный вопрос, а получается мы тем, что события вложены для того, чтобы пределить эти?
[56:39.740 --> 56:46.220]  да, да, именно так, вот для того, чтобы воспользоваться тремой непеременностью вероятности мира.
[56:46.220 --> 56:51.860]  вот у нас есть пересечение, для любого этого пересечения, у нас есть пересечение событий.
[56:51.980 --> 56:58.020]  если они не вложены, то говорить о том, что вероятность пересечений равна нулю, тогда
[56:58.020 --> 57:01.940]  только на пределы равны нули, неправильно, но возьмем, пусть одно из них в общественной
[57:01.940 --> 57:06.660]  не пересекает. вот есть куча событий пересекающихся, одна вообще не пересекать. ну и тогда, конечно,
[57:06.660 --> 57:10.340]  вероятность пересечений равна нулю, хотя у них у всех может быть какая-то большая вероятность,
[57:10.340 --> 57:18.680]  и она может с этим не сходиться. чтоб возьмем все события одинаковые, они имеют вероятность
[57:18.680 --> 57:21.760]  одна вторая, и другое тоже одна вторая, который является
[57:21.760 --> 57:24.680]  дополнением до них. Вероятность перечечения ноль, а предел
[57:24.680 --> 57:28.160]  вероятности одна вторая. Вот, поэтому такой переход
[57:28.160 --> 57:30.280]  можно сделать именно за счет теремы непрерывности,
[57:30.280 --> 57:36.920]  то есть за счет того же события вложения.
[57:36.920 --> 57:40.280]  Ну вот, значит, следующая лемма, так называемая неравенство
[57:40.280 --> 57:51.080]  Калмогорова. После того, как мы докажем эту лему,
[57:51.080 --> 57:53.640]  мы сможем наконец формулировать терему Калмогорова и Хинчина
[57:53.640 --> 57:56.120]  сходимость рядов, с которой уже споследует усиленный
[57:57.120 --> 58:23.680]  Неравенство Калмогорова. Значит, пусть есть последовательность
[58:23.680 --> 58:27.960]  независимых, не обязательно одинаково распиленных,
[58:27.960 --> 58:33.520]  просто независимых случайных величин. И для удобства мы
[58:33.520 --> 58:35.720]  всегда будем центрировать, то есть вот у нас в усиленном
[58:35.720 --> 58:42.520]  законе больших чисел вычтена сумма матожедания. То есть
[58:42.520 --> 58:45.280]  мы центрировали случайные величины, то есть это было
[58:45.280 --> 58:47.880]  бы равносильно такой формулировке, что сразу предполагали,
[58:47.880 --> 58:50.360]  что пусть у нас есть независимые случайные величины с матожеданием
[58:50.360 --> 58:53.400]  равным нулю. Да, вот мы могли это в условиях предположить,
[58:53.400 --> 58:56.400]  а здесь бы не вычитать. Это было бы равносильно, потому
[58:56.400 --> 58:58.760]  что мы любую случайную величину, у которых есть матжедание,
[58:58.760 --> 59:02.840]  вот таким вот образом можем центрировать. И для удобства
[59:02.840 --> 59:06.560]  мы давайте будем сразу везде центрировать. То есть это
[59:06.560 --> 59:11.400]  не меняет общность утверждения, но просто сокращает запись.
[59:11.400 --> 59:15.240]  Независимые случайные величины с матожеданием к ситам
[59:15.240 --> 59:22.280]  равным нулю. И пусть у них тоже конечные
[59:22.280 --> 59:26.360]  вторые моменты, как и в формулировке усиленного
[59:26.360 --> 59:38.360]  закон больших чисел, тогда, во-первых, для любого
[59:38.360 --> 59:45.360]  Эпсилона больше нуля, вероятность того, что Супрэмун, пока от
[59:45.360 --> 59:51.560]  единицы до Н, модуля скатая, вероятность того, что модуля
[59:51.560 --> 01:00:00.600]  скатая, так вероятно, Супрэмун модуля скатала
[01:00:00.600 --> 01:00:06.880]  больше Н, чем Эпсилон, меньше равна, чем матожедание
[01:00:06.880 --> 01:00:09.520]  модуля снт в квадрате 9m в квадрате.
[01:00:39.520 --> 01:00:43.640]  Но я хочу, кстати, отметить некоторую похожесть данной
[01:00:43.640 --> 01:00:48.480]  неравенства Маркова. То есть если бы вы тут убрали
[01:00:48.480 --> 01:00:51.200]  Супрэмун, то это была бы точность неравенства Маркова.
[01:00:51.200 --> 01:00:53.480]  Когда вы возводите обе части в квадрате, берете матжедание,
[01:00:53.480 --> 01:00:58.160]  у вас получается неравенство Маркова. Или неравенство
[01:00:58.160 --> 01:01:01.920]  Чебышова, что одно и то же. И именно его вы применяете
[01:01:01.920 --> 01:01:05.320]  для доказательств закона больших чисел. Для доказательств
[01:01:05.320 --> 01:01:07.800]  закона больших чисел вам достаточно этого неравенства.
[01:01:07.800 --> 01:01:10.520]  То есть вы берете в момент N и смотрите на вероятность
[01:01:10.520 --> 01:01:17.680]  того, что ваш модуль снт Эпсилон превосходит. А здесь, помните
[01:01:17.680 --> 01:01:21.760]  отличие сходимости по вероятности от сходимости почти наверно.
[01:01:21.760 --> 01:01:27.520]  Вот пример с бегающим отрезком. Мера отрезка уменьшается,
[01:01:27.520 --> 01:01:31.080]  но так он бегает, то поточной сходимости нет. Через любую
[01:01:31.080 --> 01:01:34.120]  омегу он будет пробегать бесконечно много раз. Но
[01:01:34.120 --> 01:01:37.760]  чтобы этого избежать, вам нужно смотреть не на сходимость,
[01:01:37.760 --> 01:01:40.720]  а на равномерную сходимость. То есть вам для этого этот
[01:01:40.720 --> 01:01:44.520]  Супрем здесь и нужен. То есть чтобы перейти от сходимости
[01:01:44.520 --> 01:01:46.760]  по вероятности к сходимости почти наверно, совершенно
[01:01:46.760 --> 01:01:49.640]  естественно написать вот этот Супрем. И оказывается
[01:01:49.640 --> 01:01:53.800]  все то же самое будет верно. Аналог неравенства Маркова,
[01:01:53.800 --> 01:01:56.560]  только его более сильная версия, когда мы максимум
[01:01:56.560 --> 01:02:06.560]  написали, тоже будет верно. Хорошо, значит и второе.
[01:02:06.560 --> 01:02:09.880]  Нам второе не понадобится непосредственно для доказательства
[01:02:09.880 --> 01:02:13.400]  усиленного закона больших чисел, но вот нам понадобится
[01:02:13.400 --> 01:02:17.640]  это для доказательства теоремы Колмогорова и Хинчна
[01:02:17.640 --> 01:02:21.240]  сходимости рядов в более общем виде, которая интересна
[01:02:21.240 --> 01:02:23.360]  сама по себе в общем-то. Поэтому вторую часть тоже
[01:02:23.360 --> 01:02:28.920]  сформулируем. Значит если найдется такой Эпсилон больше
[01:02:28.920 --> 01:02:33.040]  нуля, что для любого И модуль ксии та меньше, чем
[01:02:33.040 --> 01:02:34.960]  С, то есть если последность случайных величин равномерно
[01:02:34.960 --> 01:02:41.760]  ограничен некоторой константой, то тогда вероятность того,
[01:02:41.760 --> 01:02:49.280]  что этот максимум больше равна чем С, больше равна
[01:02:49.280 --> 01:02:53.680]  чем Эпсилон, больше либо равна чем 1 минус С плюс
[01:02:53.680 --> 01:03:02.160]  Эпсилон в квадрате в пределительном от ожидании модуля СН в квадрате.
[01:03:02.160 --> 01:03:05.680]  Обратите внимание, что первое неравенство, интересно
[01:03:05.680 --> 01:03:13.640]  когда вот это моджедание маленькое. И тогда можно
[01:03:13.640 --> 01:03:16.280]  утверждать, что это вероятность тоже маленькая. Если вот
[01:03:16.280 --> 01:03:19.800]  эта штука стремится к нулю, то и вот эта вероятность
[01:03:19.800 --> 01:03:23.040]  тоже стремится к нулю. Наоборот второе, интересно, когда
[01:03:23.040 --> 01:03:27.000]  моджедание большое. С одной стороны если моджедание
[01:03:27.000 --> 01:03:29.880]  маленькое, то ну и супраемом типа не может быть большим.
[01:03:29.880 --> 01:03:32.260]  С другой стороны, если моджедание большое, то
[01:03:32.260 --> 01:03:34.020]  супраемом В 것 может быть большим. То есть если
[01:03:34.020 --> 01:03:37.500]  здесь structures стремится к бесконечности, то тогда
[01:03:37.500 --> 01:03:42.240]  вся эта разность стремится к единице, и значит тогда
[01:03:42.240 --> 01:03:44.940]  вероятность это тоже стремится к единице.
[01:03:46.640 --> 01:03:50.340]  Разные утверждения, которые могут помогать доказать
[01:03:50.360 --> 01:03:53.140]  Exhale с одной стороны, что когда моджедание большое
[01:03:53.140 --> 01:03:57.360]  случайная величина большая, с другой стороны когда
[01:03:57.360 --> 01:03:58.980]  моджедание маленькое, случайное величина маленькая, то
[01:03:58.980 --> 01:04:06.020]  То есть вполне себе может быть полезно для доказательства перехода от сходимости дисперсии
[01:04:06.020 --> 01:04:11.020]  к сходимости случайных величин. Хорошо, давайте доказывать.
[01:04:11.020 --> 01:04:22.860]  Сперва первое. Ну давайте вот наше событие, которое нас интересует, обозначим за. То есть это
[01:04:22.860 --> 01:04:36.500]  множество таких омега, что супремум, то есть для всех k от единицы до n. Найдется k,
[01:04:36.500 --> 01:04:45.380]  извините. Найдется k от единицы до n, что модуль s-кат и больше, чем осьмум. Понятно,
[01:04:45.380 --> 01:04:54.340]  что это есть дизъюнкное объединение событий аккаты, где аккат единицы до n. Дизъюнкное
[01:04:54.340 --> 01:05:00.500]  значит, что это множество не пересекаются. Где аккаты это множество таких омега, что вплоть до
[01:05:00.500 --> 01:05:09.020]  катова меньше, чем эпсилум, то есть с1, модуль с1 это омега меньше, чем эпсилум, и так далее.
[01:05:09.020 --> 01:05:16.020]  Модуль sk-1 это омега меньше, чем эпсилум, а вот модуль s-кат это омега уже больше,
[01:05:16.020 --> 01:05:22.140]  чем осьмум. То есть события аккаты показывают, что k это первый момент, когда больше, чем эпсилум.
[01:05:22.140 --> 01:05:26.940]  Ну, если мы предполагаем, что такой момент существует, то тогда мы получаем что-то событие
[01:05:26.940 --> 01:05:37.020]  от в точности объединения таких событий. Вот, хорошо. Ну, кстати, эти определения будут общие
[01:05:37.020 --> 01:05:40.700]  для пунктов 1 и 2, поэтому давайте сначала их дадим, а теперь перейдем к пункту 1.
[01:05:40.700 --> 01:05:48.420]  Мат ожидания sn в квадрате, конечно, больше броно, чем мат ожидания sn в квадрате на индикатор a.
[01:05:48.420 --> 01:06:01.380]  И, значит, так как a это дизъюнкное объединение, то индикатор a это сумма индикаторов аккат.
[01:06:01.380 --> 01:06:13.140]  Дальше давайте представим sn как, во-первых, занесем его под знак суммы и вычтем и прибавим
[01:06:13.140 --> 01:06:26.300]  sk, то есть напишем sn-sk плюс sk в квадрате. Индикатора k, сумма pk. Вот, дальше квадрат суммы запишем.
[01:06:26.300 --> 01:06:39.020]  sn-sk в квадрате плюс 2 sn-sk на sk плюс sk в квадрате.
[01:06:39.020 --> 01:06:53.100]  Теперь по линейностям. Это есть сумма pk, мат ожидания sn-sk в квадрате на индикатора k,
[01:06:53.100 --> 01:07:07.660]  плюс сумма pk, 2, да, мат ожидания от sn-sk на sk на индикатора k и плюс сумма pk,
[01:07:07.660 --> 01:07:21.500]  мат ожидания sk в квадрате на индикатора k. Вот, значит, и это мы хотим ценить ограничить снизу.
[01:07:21.500 --> 01:07:30.100]  Значит, вот эта штука неотрицательна. Понятно. Ну, потому что это в квадрата, это тоже неотрицательно.
[01:07:30.100 --> 01:07:35.140]  Поэтому отжижение от неотрицательности членовеличины неотрицательны. Просто выкинем его.
[01:07:35.140 --> 01:07:50.460]  Значит, дальше в силу независимости случайных величин, вот это вот sn-sk, это же сумма ксишек с k
[01:07:50.460 --> 01:07:59.500]  плюс 1 до n. Они от первых k не зависят. Да, то есть вот эта штука и вот эта штука независимы.
[01:07:59.500 --> 01:08:09.100]  А поэтому можно представить, как произведение мат ожиданий. Мат ожидания от sn-sk умножить
[01:08:09.100 --> 01:08:20.700]  на мат ожидания от sk на индикатора k. Вот, то, что касается последнего слагаемого,
[01:08:20.700 --> 01:08:28.860]  мы знаем, что на аккатом, если выполнено событие аккаты, то sk по модулю, хотя бы эпсилон,
[01:08:28.860 --> 01:08:36.500]  а значит снизу можно в последней сумме написать вместо мат ожидания эпсилон в квадрате на
[01:08:36.500 --> 01:08:44.580]  вероятность акката. Да, так как у меня на акката эти случайные величины хотя бы эпсилон,
[01:08:44.580 --> 01:08:49.780]  то соответствующим образом я могу этому ожидания ограничить. Вот это вот место, где мы применяем
[01:08:49.780 --> 01:08:56.220]  такой же абсолютно, такую же стратегию, как предоказательство нерайства Маркова. Там мы для
[01:08:56.220 --> 01:09:03.980]  одного sn вот это сделали, а тут мы, ну похитрее, мы sn представили в виде вот таких вот сумм, sn
[01:09:03.980 --> 01:09:09.660]  представили в виде вот таких вот частей трех. И вот только для третьей части мы это сделали,
[01:09:09.660 --> 01:09:17.020]  мы sk ограничили этим эпсилоном. Можно вопрос? Да. Вот там где вы фиолетово написали во второй
[01:09:17.020 --> 01:09:24.460]  снизу строчки. Почему sn-sk независимо с sk на индикатор аккаты? Ну индикатор аккаты определяется
[01:09:24.460 --> 01:09:30.060]  первыми к случайным величинами. То есть это по определению вот такое вот событие. Это событие,
[01:09:30.060 --> 01:09:36.740]  которое зависит от первых к случайных величин, а от остальных n-k независит. Это есть функция от
[01:09:36.740 --> 01:09:48.060]  ks1 до ksk. Окей? Да, понятно. Вот. Значит, так как у меня случайно величины центрированы,
[01:09:48.060 --> 01:09:53.660]  то есть когда так у них мат ожидания ноль, вот эта полинейность, это сумма мат ожиданий. Мат ожиданий
[01:09:53.660 --> 01:09:59.260]  ksk плюс первая, плюс тогда ли мат ожиданий ksn, то они все равны нулю, значит эта штука ноль. И остается
[01:09:59.260 --> 01:10:07.380]  просто в точности epsilon в квадрате на сумму по k вероятностей аккатых. Но это вероятность a. У меня
[01:10:07.380 --> 01:10:12.460]  a представляется как дизюмбная сумма вот этих вот аккатов. Получаем epsilon в квадрате на вероятность
[01:10:12.460 --> 01:10:20.500]  a. Откуда требует, откуда следует непосредственно требуемая вероятность a к меньшему ночью мат
[01:10:20.500 --> 01:10:35.460]  ожидания sin в квадрате. Так, есть ли вопросы? Ну хорошо, есть еще четыре минуты. Успеем,
[01:10:35.460 --> 01:10:39.700]  но может быть на пару минут задержимся второй пункт доказать и на этом закончим. Теорему о
[01:10:39.700 --> 01:10:47.300]  сходимости рядов уже в следующий раз. Значит, теперь хотим мат ожидания sin в квадрате ограничивать
[01:10:47.300 --> 01:10:57.140]  снизу, сверху и сделаем это так. Значит, меньше броно, чем мат ожидания см в квадрате на индикатор
[01:10:57.140 --> 01:11:09.380]  a. И плюс, ну давайте напишем сперва вот такое точное равенство, плюс мат ожидания см в квадрате
[01:11:09.380 --> 01:11:16.740]  на индикатор a с чертой. Но на множестве a с чертой мы точно знаем, что у нас максимум меньше,
[01:11:16.740 --> 01:11:23.060]  чем эпсилон, а значит в частности см в квадрате тоже меньше, чем эпсилон. Поэтому мы можем это
[01:11:23.060 --> 01:11:29.780]  ограничить как мат ожидания см в квадрате на индикатор a и плюс эпсилон в квадрате на вероятность
[01:11:29.780 --> 01:11:41.660]  a с чертой, то есть на 1 минус вероятность a. Вот, прекрасно, теперь снова работаем с мат
[01:11:41.660 --> 01:11:52.340]  ожидания см в квадрате на индикатор a. Ну, как и выше, представляем a как сумму индикаторов
[01:11:52.340 --> 01:12:00.260]  окатых. Заносим снова см внутрь, представляем его как см минус ск плюс ск, делаем в точности то
[01:12:00.260 --> 01:12:07.780]  же самое и получаем, значит, просто перепишем, мат ожидания см минус ск в квадрате на индикатор
[01:12:07.780 --> 01:12:20.180]  окатые, сумма пока, плюс мат ожидания, два мат ожидания от суммы пока на см минус ск на ск на
[01:12:20.180 --> 01:12:31.060]  индикатор окатые и плюс сумма пока на мат ожидания ск в квадрате на индикатор окатые.
[01:12:31.060 --> 01:12:40.580]  Вот, значит, как и выше, вот эта штука просто новая.
[01:12:40.580 --> 01:12:51.540]  Значит, и у нас остается сумма пока мат ожидания от, я напомню, что см минус ск это ск
[01:12:51.540 --> 01:13:07.100]  плюс 1, плюс и так далее, плюс ск в квадрате на индикатор окатые и плюс сумма пока мат
[01:13:07.100 --> 01:13:20.260]  ожидания ск в квадрате на индикатор окатые. Значит, смотрите, дальше такая мысль,
[01:13:20.260 --> 01:13:38.340]  модуль ск меньше либо равен чем модуль ск-1 и плюс модуль ск. Вот, да, модуль суммы не превосходит
[01:13:38.340 --> 01:13:45.900]  сумму модули, а на событии окатом, я напомню по определению, все случайные величины вплоть до
[01:13:45.900 --> 01:13:50.580]  к-1 меньше чем эпсилон. То есть, если мы предполагаем, что окатая верно, то вот эта штука меньше
[01:13:50.580 --> 01:13:57.620]  чем эпсилон. Еще во втором пункте мы предполагали, что все случайные величины меньше чем с. То есть,
[01:13:57.620 --> 01:14:09.020]  вот эту штуку мы можем оценить как эпсилон, а вот эту мы можем оценить как с. Все в сумме мы
[01:14:09.020 --> 01:14:17.740]  получаем меньше либо равно чем эпсилон плюс с на окатом. Да, то есть, когда мы будем смотреть
[01:14:17.740 --> 01:14:22.340]  вот на эту штуку, мы можем от ожидания ск в квадрате ограничить как эпсилон плюс с в квадрате.
[01:14:22.340 --> 01:14:33.660]  Итак, собираем все вместе. Мат ожидания ск в квадрате меньше либо равно. Сперва вот эта
[01:14:33.660 --> 01:14:42.620]  штука. Значит, вот это слагаемое у нас еще осталось. Но здесь силу независимости получаем сумму по к.
[01:14:42.620 --> 01:14:52.300]  Мат ожидания вот этого ск плюс 1, плюс так далее, плюс ск в квадрате на вероятность окатая.
[01:14:52.300 --> 01:15:05.920]  Квадрат не туда поставил. Вероятность окатая. Дальше вот эту штуку мы оцениваем как с плюс
[01:15:05.920 --> 01:15:13.700]  эпсилон в квадрате на сумму вероятности окатая. Еще остается плюс эпсилон в квадрате на 1
[01:15:13.700 --> 01:15:29.380]  минус вероятность окатая. Ну вот еще вот это у нас есть. Ну это понятное дело, это понятное
[01:15:29.380 --> 01:15:35.660]  дело меньше 0, чем от ожидания ск в квадрате. Смотрите, мат ожидания от ск плюс 1, плюс так далее,
[01:15:35.660 --> 01:15:41.060]  плюс ск в квадрате. Если вы это возведете в квадрат, то получите там всякие удвоенные
[01:15:41.740 --> 01:15:45.020]  произведения, но они все нулю равны. Мат ожидания от удвоенных произведений равны нулю в силу
[01:15:45.020 --> 01:15:50.180]  независимости в силу того, что мат ожидания каждой случайной величины это 0. Поэтому это просто
[01:15:50.180 --> 01:15:59.540]  мат ожидания, сумма мат ожидания квадратов. А это меньше темра 0, чем сумма мат ожидания квадратов
[01:15:59.540 --> 01:16:04.900]  вообще всех случайных величин, начиная с 1. Что в точности мат ожидания см квадрат?
[01:16:04.900 --> 01:16:12.500]  да, значит окончательно мы приходим вот к такой формуле, с которой нам просто вероятность а
[01:16:12.500 --> 01:16:20.140]  останется вытащить. мат ожидания с н квадрат меньше чем равно, чем мат ожидания с н квадрат на
[01:16:20.140 --> 01:16:28.700]  вероятность а, плюс с и плюс епсимум в квадрате на вероятность а, плюс епсимум в квадрат на 1
[01:16:28.700 --> 01:16:38.380]  минус вероятность а. ну теперь отсюда выцепляем вероятность а и получаем что нам больше либо равна,
[01:16:38.380 --> 01:16:51.220]  чем мат ожидания с н в квадрате минус епсимум в квадрате и делить на мат ожидания с н в квадрате
[01:16:51.700 --> 01:17:04.480]  плюс ц плюс р в квадрате и минус епсимум в квадрате что в точности есть 1 минус ц плюс
[01:17:04.480 --> 01:17:16.160]  witnessing в квадрате под
[01:17:16.160 --> 01:17:24.160]  так как вот эта штука положительна, можем это еще чуть-чуть ослабить и получить искомую
[01:17:24.160 --> 01:17:30.240]  оценку 1-c++ в квадрате по зрителям ожидания c++ в период.
[01:17:38.000 --> 01:17:40.240]  Ну, собственно, все. Есть какие-то вопросы.
[01:17:40.240 --> 01:17:48.000]  Где-то вообще использовали, что в окатом окатый элемент больше, чем эпсилон?
[01:17:48.000 --> 01:17:55.240]  Нет, здесь мы этого не использовали, нам уже не равенство другой стороны.
[01:17:55.240 --> 01:18:02.440]  Да, мы здесь это мы используем в предыдущем, а здесь мы пользуемся вот этим.
[01:18:02.440 --> 01:18:05.760]  Предыдущем где? В пункте 1.
[01:18:05.760 --> 01:18:09.960]  Да, где? В пункте 1 мы этим пользуемся вот здесь.
[01:18:09.960 --> 01:18:12.480]  Видите, вот больше либра, но здесь эпсилон возник.
[01:18:12.480 --> 01:18:16.520]  Это из-за того, что на окатом скат в квадрате больше, чем в окатом квадрате.
[01:18:16.520 --> 01:18:17.240]  Спасибо, спасибо.
[01:18:17.240 --> 01:18:19.240]  Да.
[01:18:19.240 --> 01:18:30.680]  Так, есть еще какие-то вопросы.
[01:18:30.680 --> 01:18:45.520]  Если вопросов больше нет, то на этом все. Всем спасибо. До встречи в следующую субботу. До свидания.

[00:00.000 --> 00:15.640]  я вас тоже от себя поздравлю с началом семестра вот третьего года обучения вы на половине пути
[00:15.640 --> 00:23.400]  вот ну и дай бог вам все это успешно закончить так я представлюсь соответственно зовут без
[00:23.400 --> 00:28.400]  носиков Александр Николаевич и соответственно в этом семестре мы будем встречаться с вами в
[00:28.400 --> 00:35.440]  этой аудитории вот ну те кто доживет там до конца или до середины на курсе по методам оптимизации
[00:35.440 --> 00:45.160]  на курсе по методам оптимизации у кого-то уже были семинары у кого-то не было пока семинаров но
[00:45.160 --> 00:53.240]  в общем-то это неважно хочу в любом случае представить нашу команду тут немного про меня
[00:53.240 --> 01:01.400]  я соответственно недавно защитил кандидатскую на фистехе занимаюсь активно наукой вокруг методов
[01:01.400 --> 01:07.840]  оптимизации сейчас уже написал более 10 статей на всякие там а со звездой конференции и ко один
[01:07.840 --> 01:14.360]  журналы это самый так скажем высокий уровень среди научного сообщества где-то на стыке
[01:14.360 --> 01:20.640]  оптимизации и машинного обучения всего где-то более 30 статей соответственно работаю здесь
[01:20.840 --> 01:27.920]  заведующим лабораторией мф и яндекс еще соответственно работаю в инополисе скалтехи в
[01:27.920 --> 01:34.280]  ше и пи и что там еще а еще вот соответственно арабский университет который искусственного
[01:34.280 --> 01:41.080]  интеллекта их шейха вот сайт почта телеграмм можно писать спрашивать как у меня дела
[01:41.080 --> 01:48.440]  рассказывать как у вас дела вот что еще чей верно самое интересное что в первую очередь
[01:48.440 --> 01:58.840]  вам хочется узнать это выставление балла выставление оценки вот здесь соответственно
[01:58.840 --> 02:00.880]  обозначены все виды активности которые
[02:11.080 --> 02:33.720]  что случилось здесь соответственно обозначены виды активности которые могут принести вам
[02:33.720 --> 02:39.160]  некоторые баллы в итоговую оценку соответственно первый вид активности это соответственно тесты
[02:39.200 --> 02:45.320]  на семинарах каждый семинар кроме 1 вас будет начинаться с некоторые 10 минутки по теме
[02:45.320 --> 02:50.840]  лекций и по теме семинара то есть предыдущего семинара и предыдущей лекции вот соответственно
[02:50.840 --> 02:57.520]  три балла вот говорит соответственно домашнее задания домашние задания будут выдаваться на
[02:57.520 --> 03:05.680]  две недели по теме двух лекций и по теме двух семинаров дедлайны жесткие вот соответственно
[03:05.680 --> 03:10.160]  Тоже три балла. Фидбокс, я думаю, по ним будет довольно полный, потому что ребята-
[03:10.160 --> 03:14.680]  ассистенты будут сидеть проверять и, соответственно, выдавать вам не просто
[03:14.680 --> 03:18.680]  оценки, а какие-то еще и комментарии к тому, что вы там натворить успели.
[03:18.680 --> 03:24.360]  Также контрольная работа в середине семестра на лекции по темам,
[03:24.360 --> 03:28.680]  соответственно, которые мы прошли до этой контрольной работы. И на лекциях, и на
[03:28.680 --> 03:34.160]  семинарах. Колоквиум, что-то типа зачета экзамена в формате билетов,
[03:34.160 --> 03:38.720]  все как на устном экзамене в конце семестра. Либо на последние недели, либо на
[03:38.720 --> 03:42.200]  зачетные недели на ваш выбор. Разделим вас просто. Каждая семинария свою группу
[03:42.200 --> 03:47.240]  разделит. И получится так. Ну и последний вид активности это разбор статьи.
[03:47.240 --> 03:54.040]  Можно попросить статью на разбор про какие-то современные темы оптимизации.
[03:54.040 --> 04:00.160]  И, соответственно, тоже получить за это баллы. Тут более подробно. Первое
[04:00.160 --> 04:04.080]  важное уточнение, что если вы хотите получить оценку удовлетворительно, то
[04:04.080 --> 04:08.080]  либо за колоквиум, либо за контрольную работу нужно набрать больше, либо равно
[04:08.080 --> 04:12.160]  одного балла. Вот. Мы должны удостовериться, что вы хотя бы где-то
[04:12.160 --> 04:17.400]  приходили в сознание. Вот. Они просто там списали домашние задания или что-то
[04:17.400 --> 04:23.160]  там сделали. Вот. Если хочется получить оценку хорошо, либо отлично и по
[04:23.160 --> 04:29.000]  контрольной, по колоквиум, нужно получить оценку выше единицы. Вот. Из трех. Ну то есть
[04:29.000 --> 04:34.160]  какой-то базовый уровень показать. Ну а дальше уже все как набирается. Небольшие
[04:34.160 --> 04:38.720]  комментарии. Как вы понимаете, ни один из видов активности не является 100
[04:38.720 --> 04:43.480]  процентов обязательным. Вот. Можно, например, что-то пропустить и все равно получить
[04:43.480 --> 04:48.340]  оценку удовлетворительно. Тесты мы проводим в начале семинаров. Вот.
[04:48.340 --> 04:52.760]  ДЗ и, соответственно, обновления конспектов будут происходить на выходных.
[04:52.760 --> 04:58.120]  Как раз четверг у нас большая часть семинаров плюс лекция. Вот. И что мы сейчас
[04:58.240 --> 05:01.280]  еще, соответственно, будем делать? Ну посмотрим ваш фидбэк пятницу, что-то
[05:01.280 --> 05:06.160]  обсудим и, соответственно, будем делать новые выкаты выходные. Вот. При
[05:06.160 --> 05:10.360]  подозрении на списывание обнуляются все ДЗ, в том числе тех, кто дал списать.
[05:10.360 --> 05:14.320]  Контрольная работа в середине семестра по материалам восьми лекций и восьми
[05:14.320 --> 05:20.480]  семинаров. Колоквиум, как я сказал, по билетам, проводит его ваш семинарист и
[05:20.480 --> 05:25.000]  приглашает кого-то еще в помощь, например, лектора или других семинаристов. Вот.
[05:25.000 --> 05:28.920]  Соответственно, не ходите на семинары, приходите на колоквиум, семинарист
[05:28.920 --> 05:32.720]  удивляется, а кто вы такой? Вот. Соответственно, наверное, стоит туда все же
[05:32.720 --> 05:37.040]  иногда заглядывать. Вот. Ну и про разбор статей, соответственно, до 31 числа их
[05:37.040 --> 05:41.560]  можно попросить на разбор до 31 октября. Вот. Ну и как происходит, соответственно,
[05:41.560 --> 05:45.520]  зачет этого всего безобразия? Вы разбираете статью, разбираете доказательства,
[05:45.520 --> 05:49.840]  воспроизводите эксперименты, которые там сделаны, и потом выступаете перед
[05:49.840 --> 05:53.920]  одногруппниками и выступаете на нашем семинаре. Вот. Рассказываете, что же в
[05:53.920 --> 06:01.040]  этой статье там такого интересного происходит. Так. Вопросы? Пожелания,
[06:01.040 --> 06:11.560]  жалобы по этой части? У вас будет эта презентация? Возможно, даже у всех она
[06:11.560 --> 06:21.440]  будет. Так. Еще? Ну супер. Значит, всем все понятно, и мы можем переходить к уже
[06:21.440 --> 06:28.440]  более интересным вещам. Вот. Каким-то сутьевым вещам. Так. Окей. Давайте-ка чуть-чуть
[06:28.440 --> 06:32.240]  расшатаемся. Куар-коды вы просканировали, но давайте вы мне что-нибудь и какие-нибудь
[06:32.240 --> 06:37.600]  вопросы еще поотвечаете. Вот. Встречали ли вы когда-то задачи оптимизации уже в
[06:37.600 --> 06:40.480]  своей жизни и где?
[06:41.040 --> 06:56.320]  Где? В анализ данных. А что вы там делали? Так. Здорово. Где еще? Все только в
[06:56.320 --> 07:01.360]  машинном обучении встречали? Нет, и этого в принципе достаточно. Вот. Потому что
[07:01.360 --> 07:05.080]  машина обучения сейчас становится все более и более популярным, и на самом деле
[07:05.080 --> 07:09.720]  действительно, судя по тому, что те же статьи, которые пишем мы, мы тоже часто
[07:10.040 --> 07:14.400]  опираемся на машинное обучение, как одну из основных мотиваций. Но на самом деле
[07:14.400 --> 07:18.480]  оптимизация имеет довольно много приложений. Да. В первую очередь, на данный
[07:18.480 --> 07:23.040]  момент это машинное обучение, анализ данных, там какие-то статистические модели.
[07:23.040 --> 07:27.160]  Ну, конечно, она возникает и в финансах, и в управлении, в том числе там управлении
[07:27.160 --> 07:32.880]  там, не знаю, ракетами, дронами и так далее. Вот. Всякими вопросами логистики,
[07:32.880 --> 07:37.920]  вопросами планирования и много где еще. То есть до всяких там мелких задач в духе
[07:37.920 --> 07:42.600]  того, как нужно обработать или там отфильтровать картинку, избавить от шумов.
[07:42.600 --> 07:47.680]  Вот. То есть там какие-то есть и глобальные цели, и цели, которые там какие-то локальные,
[07:47.680 --> 07:54.600]  чтобы фотку для инстаграма улучшить. Вот. Окей. Вот. Ну, к сожалению, я не знаю, вот так
[07:54.600 --> 07:59.320]  вот повелось, что часто оптимизацию воспринимают как просто некоторый
[07:59.320 --> 08:03.200]  инструмент, который у вас там задача оптимизации возникает как вспомогательная
[08:03.200 --> 08:08.840]  вашей там исходной задачки. Вот. Ну и эту задачку оптимизации как-то решают,
[08:08.840 --> 08:15.480]  используя некоторые там известные всем пакеты, черные ящики, не особо разбираясь,
[08:15.480 --> 08:24.760]  что же там внутри этих черных ящиков происходит. Вот. Наша задача на этом курсе разобраться и в
[08:24.760 --> 08:30.160]  некотором смысле как с какими-то основными черными ящиками познакомиться с ними, но и заглянуть в них,
[08:30.160 --> 08:38.080]  что же там происходит внутри этих черных ящиков. Вот. И в том числе с точки зрения теории. Вот.
[08:38.080 --> 08:46.480]  Это цель у нас курса. Вот. Тут немного историй, как вообще развивалась оптимизация со временем.
[08:46.480 --> 08:54.280]  Все же первым основателем оптимизации считаются каши. Каши, которые ровно те теоремы, которые вы
[08:54.280 --> 09:01.520]  выучили, он и здесь отличился. Вот. Он, соответственно, использовал задачу оптимизации для решения
[09:01.520 --> 09:07.640]  линейных систем. То есть была у него задачка решить линейную систему, но он ее переписывал,
[09:07.640 --> 09:12.560]  соответственно, вот в таком вот виде и говорил, что я хочу, наверное, получить ноль, а тогда я,
[09:12.560 --> 09:18.040]  наверное, могу вот это вот так вот сделать. То есть попытаться минимизировать вот эту функцию,
[09:18.040 --> 09:23.800]  потому что как раз вам Евклидова норма даст ноль в решении системы. Вот. Один из
[09:23.800 --> 09:28.120]  альтернативных способов, как решать систему линейных уравнений, вот придумал каши и,
[09:28.120 --> 09:34.240]  соответственно, придумал он метод решения градиентный спуск. Ну, вот это, соответственно,
[09:34.240 --> 09:42.480]  точка отсчета оптимизации. Дальше, соответственно, надолгое время про нее как-то подзабыли. Но вот,
[09:42.480 --> 09:47.920]  соответственно, в середине двадцатого века в связи с развитием всяких численных методов,
[09:47.920 --> 09:56.400]  в том числе для космических исследований, для программ и всяких ядерных программ,
[09:56.400 --> 10:02.520]  вот, стали супер актуальны и появились, начали рассматривать конкретные задачи оптимизации,
[10:02.520 --> 10:06.960]  типа линейного программирования и ненедельного программирования. И в то же время примерно
[10:06.960 --> 10:12.960]  появляются первые стахистические методы, так скажем, то, что улегло в итоге в фундаментом вот
[10:12.960 --> 10:17.960]  того, что сейчас используется для нейронных сетей. Вот. В девяносто-восемидесятые годы,
[10:17.960 --> 10:24.880]  соответственно, появляется уже общая теория оптимизации с оценками сходимости. Ну, в первую
[10:24.880 --> 10:30.920]  очередь, тут, конечно, отличились наши соотечественники. Это Борис Полик, Юрий Нестеров,
[10:30.920 --> 10:37.640]  Аркадий Немирорский. Теория оптимизации, можно еще считать, что лежит на плечах вот этих трех
[10:37.640 --> 10:45.400]  гигантов советской математики. Ну и, соответственно, сейчас, что еще сейчас? Сейчас эта оптимизация,
[10:45.400 --> 10:51.720]  в первую очередь, заточена на задачи большого размера, в том числе нейронные сети, и, соответственно,
[10:51.720 --> 10:59.360]  популярность приобрели особые стахистические методы разного рода для разных постановок задач.
[10:59.360 --> 11:07.280]  Окей. Здесь, соответственно, выписана вам общая постановка задачи оптимизации. У нас есть целевая
[11:07.280 --> 11:15.000]  функция F, которую мы хотим минимизировать. При этом, в общем случае, у вас может быть задано
[11:15.000 --> 11:20.720]  некоторое ограничение в виде множества Q. Часто это множество какое-то простое, какой-то шарф в
[11:20.720 --> 11:26.760]  произвольной норме. Возможно, симплекс вероятностный. Ну, главное, что множество не очень сложно и
[11:26.760 --> 11:31.920]  легко описываемо. Плюс у вас могут быть еще дополнительные ограничения типа неравенств и
[11:31.920 --> 11:36.120]  равенств. Вот. Эти ограничения могут быть функциональными, то есть заданы в виде какой-то
[11:36.120 --> 11:42.720]  функции. Они, соответственно, добавляют, что ли, дополнительных проблем задач и дополнительного
[11:42.720 --> 11:50.200]  шарма. Вот. Как это все решать? Окей. Вот, соответственно, общая постановка. Как выглядит у вас задачка
[11:50.200 --> 11:55.840]  оптимизации? Вот. Ну и у меня, соответственно, к вам вопрос. Что вообще можно про эту задачу
[11:55.840 --> 12:03.720]  сказать? И сложно ли она вообще для решения? Какая у вас есть уже сейчас интуиция и понимание того,
[12:03.720 --> 12:15.680]  что происходит в решении задач оптимизации? Не стесняемся. Если вы не будете сами отвечать,
[12:15.680 --> 12:25.560]  я спрошу кого-нибудь самостоятельно. От конкретной задачи? Ну, это правда, да. Ну,
[12:25.560 --> 12:32.560]  вот с теми задачами, с которыми вы сталкивались, сложны они или нет? Не всегда тривиальны. Вы даже
[12:32.560 --> 12:37.240]  на первом курсе, когда нам от анализа считали вот эти все производные, искали ноль производных,
[12:37.240 --> 12:43.160]  даже там было не совсем, что ли, хорошо, как вот найти какой-то минимум вот у этой производной.
[12:43.160 --> 12:48.440]  Для каких-то базовых примеров все было хорошо, но когда функция становилась сложнее, даже в
[12:48.440 --> 12:53.960]  одномерном случае у вас уже были проблемы. Ну и, соответственно, когда мы говорим про реальные
[12:53.960 --> 13:01.160]  задачи оптимизации, но очень часто так бывает, что аналитически вы ее решить не можете. Нет
[13:01.160 --> 13:07.440]  никакого выражения в явном виде, например, как решить систему уравнения, она же не всегда имеет
[13:07.440 --> 13:13.040]  единственное решение. Возможно, численные методы в данном случае помогут вам лучше.
[13:13.040 --> 13:18.600]  Про нейронные сети тут вообще говорить не приходится, ну, то есть какое аналитическое
[13:18.600 --> 13:23.560]  решение у какой-то там очень сложной, там, не выпуклой задачи. Соответственно, да, приходится
[13:23.560 --> 13:28.520]  прибегать к каким-то дополнительным ухищрениям, чтобы эти задачи решать, на бумажке вы ответ не
[13:28.520 --> 13:35.440]  выпишете. Ну и, соответственно, да, здесь я отмечаю основные особенности, первая особенность,
[13:35.440 --> 13:39.940]  соответственно, что задача оптимизации может вообще не иметь решения даже в самых простом случае,
[13:39.940 --> 13:46.200]  минимизируем линейную функцию, понимаем, что на всем пространстве R и понимаем, что у нее есть
[13:46.200 --> 13:53.440]  минус бесконечность и по факту оптивум это нет. Вот, ну, как я сказал, аналитически мы их решать
[13:53.440 --> 14:00.480]  не можем, ну и часто действительно сложность этих задач зависит от целевой функции и от тех
[14:00.480 --> 14:05.760]  ограничений, которые у нас накладываются на задачах, в качестве множества Q и в качестве
[14:05.760 --> 14:16.880]  вот этих дополнительных функциональных ограничений G. Вот, да, вот, ну и, соответственно, да, в связи
[14:16.880 --> 14:22.520]  с тем, что аналитического решения у нас нету, аналитического решения нету, предлагается как-то
[14:22.520 --> 14:29.680]  подбираться к решению, потихонечку, итеративно, например, и находить его приближенно. Часто этого
[14:29.680 --> 14:33.640]  достаточно. То есть для большинства приложений этого достаточно. Более того, для referring dolly
[14:33.640 --> 14:39.000]  доли приложений этого достаточно. Главное-то точность, для каких-то приложений достаточно
[14:39.000 --> 14:44.240]  какой-то небольшой точности решения, приближенного, там, окрестностей до которой вы дошли. Вот. Для каких-то
[14:44.240 --> 14:52.520]  приложений нужно рассчитывать все более точно и точно. Вот, вообще, когда мы говорим про методы
[14:52.520 --> 14:58.480]  оптимизации, как искать минимум функции, мы говорим в некотором... мы пытаемся решать
[14:58.480 --> 15:02.840]  не какую-то конкретную задачку. То есть если мы там, например, хотим минимизировать просто
[15:02.840 --> 15:09.520]  функцию x в квадрате, то решение для этой задачки вы знаете. Для этой конкретной задачки вы знаете,
[15:09.520 --> 15:14.280]  что решение в нуле. Но это неинтересно. Это неинтересно. Предполагается, что методы оптимизации,
[15:14.280 --> 15:21.120]  которые вы будете разрабатывать, они будут способны решать целый класс из задач. То есть,
[15:21.120 --> 15:28.520]  например, как-то вы все же можете описать функцию, с которыми вы работаете, и в этом классе задач
[15:28.520 --> 15:35.200]  соответственно как-то жить. И для него разрабатывать уже методы. Это уже более
[15:35.200 --> 15:40.040]  интересно, потому что появляется какая-то универсальность. Соответственно, поменяли
[15:40.040 --> 15:48.960]  задачку, а метод все равно работает. В связи с тем, что у вас метод универсальный, появляется важная
[15:48.960 --> 15:56.200]  особенность. То есть, метод не знает полной информации о функции. То есть, он заточен на то,
[15:56.200 --> 16:03.600]  чтобы по факту ему на вход что-то подавали. И, соответственно, как-то с этим работает. При этом,
[16:03.600 --> 16:09.440]  как выглядит функция полностью, он не знает. Ну и не должен знать по факту, это же как раз
[16:09.440 --> 16:15.680]  исследствие универсальности, что он подстраивается под ту функцию, которую его просит минимизировать
[16:15.680 --> 16:24.640]  в данный момент. Ну и, соответственно, да. И предполагается в численных методах, что у нас метод
[16:24.640 --> 16:31.440]  может запрашивать у какой-то дополнительной программы информацию о функции. Насколько полная эта
[16:31.440 --> 16:38.240]  информация, ну уже зависит от метода. Хочется, конечно, обходиться более простыми вещами. Ну понятно,
[16:38.240 --> 16:43.200]  что, например, просто метод берется, спрашивает у какой-то дополнительной программы, где минимум
[16:43.360 --> 16:47.640]  функции выдает это как ответ, это неинтересно. Понятно, что мы хотим построить что-то более
[16:47.640 --> 16:53.840]  реальное к жизни. То есть, хочется как-то спрашивать не особо много про функцию,
[16:53.840 --> 17:01.120]  используя какие-то не особо сложные дополнительные процедуры, выяснять информацию какую-то о
[17:01.120 --> 17:08.000]  функции, чтобы ее использовать уже в минимизации этой функции, в оптимизации нахождения решений.
[17:08.000 --> 17:14.560]  Ну и, соответственно, к вам вопрос, какого рода информацию можно спрашивать у аракулов. В данном
[17:14.560 --> 17:19.240]  случае, вот то, что как раз дополнительная программа, которая нам выдает информацию о функции,
[17:19.240 --> 17:26.880]  это аракул. Что можно спрашивать? Производную. Хорошо, что еще? Сколько минимумов есть. Сколько
[17:26.880 --> 17:32.600]  минимумов есть хорошая, в принципе, информация, но уже сложно ее доставать. Сколько минимумов есть,
[17:32.600 --> 17:38.480]  это значит, я примерно знаю где минимумы. Это уже такая довольно дорогая информация. Что еще?
[17:38.480 --> 17:47.160]  Вторые производные. Все правильно. То есть, часто действительно в численных методах оптимизации мы
[17:47.160 --> 17:53.840]  запрашиваем какую-то информацию нулевого первого порядка. То есть, в данном случае могут быть
[17:53.840 --> 17:59.360]  просто значения функции. То есть, дополнительная программа просто вычисляет значения функции.
[17:59.360 --> 18:05.760]  Может вычислять какие-то градиенты. То есть, производные по каждой из переменных. У нас
[18:05.760 --> 18:10.520]  понятна задача оптимизации. Зависит от целого вектора переменных. И мы можем посчитать
[18:10.520 --> 18:15.880]  градиенты частные производные. Соответственно, можем посчитать информацию второго порядка.
[18:15.880 --> 18:21.800]  Соответственно, матрицу Гесса, он же Гессиан. Ну и можем более старшие производные, там уже в
[18:21.800 --> 18:26.400]  зависимости от специфики. Понятно, что в реальности на самом деле вот это появление аракула, оно
[18:26.400 --> 18:31.380]  конечно немного математично, что вот мы называем, что у нас есть дополнительная программа. Но в
[18:31.380 --> 18:38.720]  реальности на самом деле так и есть. Часто приходится иметь дело в жизни, что действительно
[18:38.720 --> 18:43.720]  значение функции или градиент вычисляет дополнительная процедура. Я думаю, вы когда
[18:43.720 --> 18:47.480]  даже реализовывать это будете в домашнем задании, вы подсчет градиента будете реализовать
[18:47.480 --> 18:51.520]  дополнительной функцией. Или дополнительным процедуром, или дополнительным классом. Неважно,
[18:51.520 --> 18:53.440]  как вы будете делать, но главное, что это что-то
[18:53.440 --> 18:54.440]  отдельное.
[18:54.440 --> 18:57.880]  Ну и в жизни, например, часто мы можем просто
[18:57.880 --> 19:00.520]  иметь доступ только к информации нулевого порядка,
[19:00.520 --> 19:03.480]  и даже она может быть дорогой, просто посчитать значение
[19:03.480 --> 19:04.480]  функции дорого.
[19:04.480 --> 19:09.280]  Я не знаю, кто-то знаком с всякими обучением с подкреплением,
[19:09.280 --> 19:12.880]  когда у вас есть какая-то среда, и у вас есть возможность
[19:12.880 --> 19:16.240]  действовать в этой среде, и вам нужно увеличить
[19:16.240 --> 19:21.080]  свой выигрыш, меняя действия, которые вы можете применять.
[19:21.120 --> 19:23.200]  Но при этом вы действуете вслепую, то есть вы можете
[19:23.200 --> 19:25.240]  применить какое-то действие и посмотреть, как на него
[19:25.240 --> 19:26.960]  отреагирует среда.
[19:26.960 --> 19:29.300]  Можете применить другое действие, посмотрите, как
[19:29.300 --> 19:30.680]  на него отреагирует среда.
[19:30.680 --> 19:33.760]  То есть по факту, среда вам возвращает просто значение,
[19:33.760 --> 19:36.280]  насколько она удовлетворена или не удовлетворена тем
[19:36.280 --> 19:38.920]  действиям, и какой нам выигрыш вам за него дает.
[19:38.920 --> 19:41.280]  И, соответственно, вы получаете просто значение функilities
[19:41.280 --> 19:43.660]  при этом может быть дорого, в реальных задачах motsчет
[19:43.660 --> 19:45.160]  и функция может занимать день.
[19:45.160 --> 19:48.200]  Ну и в градиенте, когда вы там обучаете нейронной
[19:48.200 --> 19:53.960]  сети, тоже подсчет этого градиента, это тоже дополнительная процедура, там forward-backward
[19:53.960 --> 19:57.800]  процедура, которая тоже занимает время, это понятно, вот такие вот аракулы могут быть.
[19:57.800 --> 20:06.080]  Окей, общая схема того, как вообще устроены методы оптимизации, как их можно описать.
[20:06.080 --> 20:12.880]  Вот здесь сяду, и мы чуть-чуть поговорим. У нас есть начальная точка, просто откуда мы
[20:12.880 --> 20:19.080]  стартуем кандидатное решение, возможно очень плохой кандидат, но почему нет. Как-то мы решили,
[20:19.080 --> 20:24.360]  что у нас точка x0, может быть как-то хорошо, может и плохо, может мы просто взяли и сказали,
[20:24.360 --> 20:34.360]  давайте стартуем из нуля. Важное уточнение, во всех местах я буду использовать номер текущего
[20:34.360 --> 20:40.960]  значения x с верхним индексом, это значит, что это текущее значение x, во время итерации мы его
[20:40.960 --> 20:45.960]  будем менять, соответственно у нас будет появляться x ката, это значит, что x на кат итерации,
[20:45.960 --> 20:55.240]  это не степень, это просто x на ката итерации. Ну я не знаю, я так привык, просто часто приходится
[20:55.240 --> 21:00.600]  работать с распределенной оптимизацией, когда у нас задача оптимизации решается на нескольких
[21:00.600 --> 21:06.000]  устройствах, и там соответственно появляется еще нижний индекс, который отвечает за номер устройства,
[21:06.000 --> 21:10.840]  который эту задачку решает, вот, и мне просто уже привычнее лепить вверх этот индекс,
[21:10.840 --> 21:18.480]  вот, потому что тогда там приходилось вот так делать, это неприятно, вот. Вот, есть, короче,
[21:18.480 --> 21:26.080]  точности epsilon, которую мы хотим достичь, вот. Что происходит? Мы задаем счетчик итерации,
[21:26.080 --> 21:34.800]  в данном случае пока он равен нулю, вот, плюс мы задаем нашу информационную модель, то, что мы
[21:34.800 --> 21:45.480]  те знания, которые мы накопили в функции на данный момент, вот. Как происходит обычно метод? На
[21:45.480 --> 21:53.280]  текущей итерации мы задаем вопрос аракулу какой-то точки x каты, текущей точки x каты, и аракул возвращает
[21:53.280 --> 22:01.440]  нам информацию о функции в этой точке, ну, например, значение функции, градиент, гисиан, вот. Что мы
[22:01.440 --> 22:06.000]  делаем? Мы добавляем соответственно в нашу информационную модель то, что мы знаем о функции,
[22:06.000 --> 22:14.280]  текущую точку и то, что нам рассказал аракул, то, что нам рассказал аракул. Ну, и соответственно,
[22:14.280 --> 22:20.320]  дальше вступает в дело наш метод оптимизации. Он что берет? Он берет нашу информационную модель,
[22:20.320 --> 22:27.960]  и как-то ту информацию, которая в этой информационной модели хранится, использует для оптимизации,
[22:27.960 --> 22:35.640]  ну, то есть для следующего шага, для пересчета текущего значения переменной x ка плюс 1. Выглядит
[22:35.640 --> 22:40.400]  немного муторно и сложно, сейчас разберемся, например, что тут происходит. Ну да, чтобы остановить эту
[22:40.400 --> 22:45.520]  процедуру, мы проверяем критерий. Останова, если критерий выполнен, тогда мы возвращаем значение,
[22:45.520 --> 22:51.040]  если нет, то переходим на новую итерацию. Давайте на примерчике посмотрим, на примерчике градиентного
[22:51.040 --> 22:57.680]  спуска. Пока мы с ним не знакомы особо, мы в следующий раз познакомимся с ним поближе, вот. Здесь
[22:57.680 --> 23:03.440]  главное просто посмотреть, как выглядит метод. Вот, у нас есть задача минимизации функции f без
[23:03.440 --> 23:09.880]  ограничений, решаем ее на rd. Мы более того, мы знаем, что у нас функция дифференцируемая. Вот, что мы
[23:09.880 --> 23:14.800]  можем делать? Можем запустить вот такую интеративную процедуру, которую как раз передумал Каши. Вот,
[23:14.800 --> 23:23.880]  мы же знаем, что производно нам о чем говорит. Локально, локально, что? О чем она говорит? О направлении
[23:23.880 --> 23:30.560]  роста. Вот. А тогда минус производно говорит о направлении спуска. Все правильно. И вся идея
[23:30.560 --> 23:36.680]  градиентного спуска говорит, заключается в том, что давайте идти по антиградиенту, раз туда функция
[23:36.680 --> 23:42.360]  убывает, вот. Ну, мы туда и пойдем. Да, делая какие-то маленькие шашки, потому чтобы далеко не улететь,
[23:42.360 --> 23:47.280]  потому что опять же, для какой-нибудь квадратичной функции вы берете, говорите, ну вот да, вот нам
[23:47.280 --> 23:51.800]  градиент говорит, что нужно идти туда, я сделаю огромный шаг и улечу вообще вот сюда, куда-нибудь,
[23:51.800 --> 23:56.400]  это далеко. Вот, делая маленькие шашки, мы соответственно по чуть-чуть приближаемся к решению.
[23:56.400 --> 24:01.480]  Вся суть градиентного спуска. Но давайте его разберем с точки зрения вот той информационной
[24:01.480 --> 24:14.400]  модели, которую мы с вами ввели. Вот. Что тут происходит? Что здесь оракул? Да, то есть какая-то
[24:14.400 --> 24:20.600]  процедура, которая умеет нам возвращать градиент. Это оракул, соответственно. Что тут информационная
[24:20.600 --> 24:31.200]  модель? Что мы в ней храним? В информационной модели мы, соответственно, храним что? То есть в
[24:31.200 --> 24:37.920]  теоретическом случае, то есть в теории, мы можем там хранить вообще все x-каты и все градиенты, да? Вот.
[24:37.920 --> 24:44.280]  А потом, соответственно, подгружать эту информационную модель в метод. Ну и метод справедливости ради
[24:44.280 --> 24:52.000]  вообще не использует все k, так? Все, что у нас хранится в информационной модели, он использует только
[24:52.000 --> 24:58.320]  последнее, да? Вот. Ну, почему нет? Есть методы, которые действительно сильно используют предысторию. Мы
[24:58.320 --> 25:04.680]  такие тоже с методами встречать. Вот. Они используют как-то предысторию. Ну, вот, градиентный спуск не
[25:04.680 --> 25:09.360]  использует, он пользуется только локальными свойствами функции. Ну, такой вот метод, довольно
[25:09.360 --> 25:16.920]  простой. Вот. В этом его как бы и плюс, и в этом его и минус. Ну и, как вы понимаете, с точки зрения теории,
[25:16.920 --> 25:22.120]  в принципе, мы в информационную модель можем просто бесконечно пихать вот эти все точки и не
[25:22.120 --> 25:27.000]  особо волноваться. С точки зрения понятной реализации методов информационной модели можно как-то
[25:27.000 --> 25:31.440]  дополнительно модифицировать. То есть зачем нам хранить эти предыдущие точки, если метод их вообще
[25:31.440 --> 25:38.200]  не использует? Вот. Поэтому можно в информационную модель там оставлять те точки, которые нужны
[25:38.200 --> 25:43.160]  конкретно для нужной итерации, и сразу как бы онлайн их пересчитывать. То есть модифицировать
[25:43.160 --> 25:47.600]  информационную модель, а не просто бездумно туда что-то пихать. Вот. Ну, это некоторые особенности
[25:47.600 --> 25:53.640]  реализации, которые в принципе становятся понятны, когда вы видите метод. Вот. Ну, здесь ответом можно
[25:53.640 --> 25:59.880]  просто из информационной модели выкидывать все, кроме текущей точки, текущего градиента. Вот. И
[25:59.880 --> 26:09.400]  не жрать память. У меня такой вопрос. До этого я вам описывал общую схему. И тут показал метод.
[26:09.400 --> 26:17.640]  Чего не хватает в схеме? Критерия останова. Здесь я использовал такой супербанальный критерий
[26:17.640 --> 26:24.920]  останова. Сделаем к большой итерации, остановимся. Вот. На самом деле можно останавливаться и по
[26:24.920 --> 26:33.120]  другому. Останавливаться и по другому. Например, когда вы достигли решения по аргументу, и вы близки
[26:33.120 --> 26:37.320]  к решению по аргументу, там до точности епсилон, вы прям приближаетесь, приближаетесь, приближаетесь,
[26:37.320 --> 26:43.800]  и можете гарантировать, что x-каты там очень близко лежит в епсилонокрестности решение. В чем проблема
[26:43.800 --> 26:52.320]  такого критерия? Да. То есть такая вещь, это такая, больше теоретическая. То есть в теории мы как раз с
[26:52.320 --> 26:56.400]  вами будем доказывать, что метод гарантирует, что вы приближаетесь к решению вот так вот.
[26:56.400 --> 27:14.320]  Отлично. Отлично. Вот. Смотрите, то есть да, вот тот критерий, который написан сверху, он не супер
[27:14.320 --> 27:17.720]  практический. В теории мы с ним будем взаимодействовать. Там теория нам будет гарантировать,
[27:17.720 --> 27:23.360]  например, градиентный спуск через какое-то число итерации будет сходить, приближаться к решению.
[27:23.360 --> 27:28.680]  Окей, но как мне это использовать на практике? Вот. А на практике можно использовать так. Я знаю,
[27:28.680 --> 27:33.600]  что у меня в теории вот это будет меньше епсилон. Вот. И вот это будет меньше, чем епсилон. Ну,
[27:33.600 --> 27:39.600]  тогда вот расстояние между соседними точками будет меньше, чем два епсилона. Здесь я просто
[27:39.600 --> 27:45.120]  неравенством треугольника воспользовался. Ничего тут только сверхъестественно нет. Я надеюсь,
[27:45.120 --> 27:51.880]  для всех это понятно. Все. Супер. Кивнули только на первой партии остальные. Молчание, знак, согласие.
[27:51.880 --> 28:02.640]  Ну, смотрите, то есть тут, видите, это следует из того, что, например, мы для алгоритма можем
[28:02.640 --> 28:08.480]  доказать, что вот он к решению приближается. По заданной точности епсилон мы знаем число
[28:08.480 --> 28:14.360]  итерации, через которое он их достигнет. Ну и тогда вот это действительно точно будет выполнено.
[28:14.360 --> 28:21.480]  Вот. Ну понятно, что не для всех этих алгоритмов это можно сделать. Вот. Но мы
[28:21.480 --> 28:25.200]  будем как раз работать с такими алгоритмами, для которых мы это можем сделать и для таких
[28:25.200 --> 28:37.120]  задач, для которых мы это можем сделать. Да. Какая? А тут мы пока вообще не говорим про скорость
[28:37.120 --> 28:52.240]  сходимости к ответу. Не-не-не. Я скорее говорю про критерия Останова. То есть мы не оцениваем
[28:52.240 --> 28:57.200]  по расстоянию до оптимума. То есть в теории мы, смотрите, можем гарантировать вот это. То есть
[28:57.200 --> 29:02.960]  теория нам говорит, что вот для этих задач вот этот метод через k итерации может гарантировать
[29:03.720 --> 29:12.880]  А теперь вопрос. Как мне на практике этот метод остановить, если я не знаю оптимум? Вот. Либо я
[29:12.880 --> 29:18.640]  могу как-то останавливать просто по теории, говорить, что я знаю число итерации. Вот. Либо,
[29:18.640 --> 29:24.920]  например, метод мне говорит о том, что там вот условно я достиг точности епсилон или там
[29:24.920 --> 29:29.880]  епсилон пополам, тогда здесь будет епсилон. Вот. И зная то, что у меня метод гарантирует,
[29:29.880 --> 29:34.880]  что мы приближаемся к решению, что xkt плюс 1 становится ближе к решению, чем xkt,
[29:34.880 --> 29:42.440]  тогда это тоже могу ограничить епсилон пополам. Ну и тогда, если у меня вот будет точность
[29:42.440 --> 29:48.600]  епсилон, вот. То метод в принципе можно остановить. Я согласен, что это не совсем эквивалентная вещь.
[29:48.600 --> 29:55.280]  То есть отсюда мы не можем делать, вот просто из этой строчки мы не можем делать четкий вывод,
[29:55.280 --> 30:01.440]  что разница между решениями нас ведет к тому, что мы в итоге сходимся к решению. Но тут еще
[30:01.440 --> 30:05.440]  дополнительно нужно принимать во внимание, что есть теория, которая говорит, что для этого класса
[30:05.440 --> 30:11.040]  задач мы к решению действительно сходимся. Вот. То есть вот из этой строчки просто без какой-то
[30:11.040 --> 30:15.040]  дополнительной информации о том, что мы сходимся, действительно сделать нельзя. Мы может просто какой-то
[30:15.040 --> 30:20.080]  дурацкой точке сходимся, ну и метод перестает как бы сходиться, а оптимум вообще в другой стороне
[30:20.080 --> 30:36.680]  валяется. Не обязательно на епсилон, но на какой-то кусочек, на какой-то кусочек. Да. Да, да, да, да, да.
[30:50.080 --> 30:56.360]  Они не стоят рядом. Ну смотрите, тут видите вопрос в том, что у вас есть теория, которая вам
[30:56.360 --> 31:02.720]  гарантирует, что условно через какое-то число итераций, через какое-то число итераций, у вас точно
[31:02.720 --> 31:10.000]  это будет выполнено. Вот. Это скорее вот так вот. Понятно, в теории мы будем работать вот с этим
[31:10.000 --> 31:17.720]  критерием. На практике можно использовать вот такой. Опять же я говорю о том, что часто, ну вот есть
[31:17.720 --> 31:23.880]  ситуации, что действительно вот отсюда вывод о том, что метод сходится как-то к решению, сделать
[31:23.880 --> 31:47.480]  нельзя. Но работать неплохо. Может так случиться, может так случиться. Может так случиться раньше
[31:47.480 --> 31:57.720]  времени, да. Окей. Есть еще также такая проблема в виде того, что х звезда не уникальна. Вот. Ну
[31:57.720 --> 32:06.080]  разные задачки соответственно у вас не обязательно для какой-то задачи есть точное решение, которое
[32:06.080 --> 32:12.080]  вот единственно. Вот. Например, для какой задачки, которая кажется довольно тривиальной, решение
[32:12.080 --> 32:23.560]  может быть не единственным. Ну, давайте что-нибудь попроще придумаем, чтобы было понятно. Давайте совсем
[32:23.560 --> 32:30.840]  просто полином напишу. Вот такое вот. Где у него оптимум, понятно, да? В нуле, то есть оптимальное
[32:30.840 --> 32:38.880]  значение у него ноль. Ну, вектор х у него состоит из двух компонентов, х1 и х2. Вот. У него два
[32:38.880 --> 32:45.200]  параметра, оптимальное значение у него ноль. Так. Вот. Но при этом, какие х дают это оптимальное
[32:45.200 --> 32:54.720]  значение, х1 равный х2. Так. И это целое семейство значений, которые по факту, все они нам подходят.
[32:54.720 --> 33:02.360]  И что из этого решения? Ну, непонятно. Поэтому вот как вот с таким работать, уже возникает вопрос
[33:02.360 --> 33:06.200]  даже в теории. То есть функция, например, самая простая функция, это вот какая-то константа,
[33:06.200 --> 33:13.800]  где каждая точка подходит как решение. Ну, тогда к чему мы там сходимся, вот непонятно. Поэтому можно
[33:13.800 --> 33:20.120]  и в теории, и на практике это модифицировать чуть-чуть вот так вот. И проверять уже сходимость по
[33:20.120 --> 33:27.920]  функции. Вот. Это решает соответственно вопрос. Ну и более того, можно использовать что-то типа
[33:27.920 --> 33:33.760]  сходимости по норме градиента. Потому что вы знаете, что в оптимуме у вас что происходит?
[33:33.760 --> 33:41.760]  Производная ноль. Производная ноль. Производная ноль. И поэтому вы можете проверять как бы как
[33:41.760 --> 33:45.240]  меняется у вас градиент. Но в каких задачах, соответственно, это можно использовать?
[33:45.240 --> 33:59.960]  Не только. Смотрите, это как бы разные варианты того, как мы можем проверять,
[33:59.960 --> 34:04.880]  вообще сходимся мы к решению или нет. Вот. То есть, смотрите, первый не подходит. Часто он
[34:04.880 --> 34:08.920]  действительно, когда мы, например, не понимаем, какой х у нас оптимум, то есть он не уникален,
[34:08.920 --> 34:14.120]  можно просто проверять сходимость по функции. Вот. Потому что функция как раз оптимальное значение
[34:14.120 --> 34:20.360]  функции. Я эту задачу, которую я описывал, она выпуклая. Вот. В принципе, там все должно сходиться,
[34:20.360 --> 34:25.280]  мы это будем доказывать. Но вот оптимум не уникален в этом проблеме. Но зато по функции все хорошо.
[34:25.280 --> 34:30.200]  Ну а такой самый вообще точно работающий критерий — это сходимость по норме градиента. Но вот вопрос,
[34:30.200 --> 34:33.520]  когда он работает? Вот вы сказали, что норма градиента равна нулю. А?
[34:33.520 --> 34:43.720]  Функция дифференцируема — окей. Пусть будет дифференцируема. Какие еще могут быть усложнения?
[34:43.720 --> 34:47.520]  Констант. Там же все хорошо будет. А?
[34:47.520 --> 34:58.760]  Могут быть проблемы, да, какие-то. Ну это какие-то такие, не особо… Так?
[34:58.760 --> 35:08.200]  Стационарная точка. Стационарная точка тоже может быть проблемой. То есть вы нашли
[35:08.200 --> 35:13.760]  стационарную точку, но не нашли в реальности решение. Вот.
[35:13.760 --> 35:25.240]  Может быть. На самом деле я тут больше говорю про тот пример, когда у вас,
[35:25.240 --> 35:31.240]  например, есть функция какая-то. Вот. Но ваше оптимизационное множество — оно ограничено.
[35:31.240 --> 35:39.040]  Вы решаете задачку вот на том, на тех воротках, которые я нарисовал. Так? Тогда у него оптимальное
[35:39.040 --> 35:47.560]  значение вот здесь вот. Так? Но градиент там не нулевой. Вот. Поэтому вот тот критерий,
[35:47.560 --> 35:53.680]  который используется, то есть его можно использовать только для безусловной оптимизации. Ну и опять же
[35:53.680 --> 36:01.200]  со специфика того, что вы не нашли стационарную точку и другими всякими частными случаями проблем.
[36:01.200 --> 36:10.920]  Окей. Когда вообще мы говорим про алгоритмы, хочется как-то их сравнивать с точки зрения того,
[36:10.920 --> 36:15.840]  что нам требуется для вычисления. То есть да, мы можем сравнить, какой оракул используется. Вот.
[36:15.840 --> 36:19.160]  Но мы сможем сравнивать алгоритмы исходя из того, что… Вот, например, алгоритмы,
[36:19.160 --> 36:23.820]  использующие одинаковый оракул, мы тоже между собой можем сравнивать. Для этого соответственно
[36:23.820 --> 36:28.400]  вводится аналитическая оракульно-аналитическая сложность и соответственно арифметическая
[36:28.400 --> 36:33.880]  сложность. Аналитическая сложность, она же оракульная сложность. Заключается в том,
[36:33.880 --> 36:40.320]  что вы просто смотрите, сколько вызовов оракула вам необходимо было сделать для достижения
[36:40.320 --> 36:46.720]  точности ε. Вот. В принципе неплохой критерий, потому что, как я и говорил, оракул может быть
[36:46.720 --> 36:51.160]  дорогим. И на самом деле, наверное, самой дорогой операцией, часто вот в численных методах
[36:51.160 --> 36:58.160]  оптимизации является вызов оракула. То есть там подсчет градиента, вызов функции. Вот. Но более
[36:58.160 --> 37:06.720]  так скажем, приближенное к времени вычисления, потому я здесь пишу временная сложность,
[37:06.720 --> 37:12.240]  это арифметическая сложность алгоритма. Там вы учитываете не только количество операций,
[37:12.240 --> 37:18.000]  то есть каких-то атомарных операций, необходимых для подсчета оракула, но еще и всякие атомарные
[37:18.000 --> 37:23.360]  операции, которые вам нужно дополнительно сделать методом. В том же градиентном спуске вы посчитали
[37:23.360 --> 37:30.320]  градиент. Вот. Но, соответственно, там еще дополнительно нужно поскладывать два вектора,
[37:30.320 --> 37:36.320]  умножить вектор на число. Это тоже дополнительное вычисление, которое необходимо сделать. Часто
[37:36.320 --> 37:44.680]  ими пренебрегают, потому что оракул, вычисление оракула значительно сложнее. Окей. Но вообще вот
[37:44.680 --> 37:50.800]  мы будем с вами строить какую-то теорию вокруг методов оптимизации, будем доказывать их
[37:50.800 --> 37:58.800]  сходимость. Вот. Хочется понять вообще какого рода результата мы сможем получать. Вот. И для этого
[37:58.800 --> 38:03.040]  давайте посмотрим вот на такой примерчик. Вот. Хочется же получать какие-то адекватные результаты,
[38:03.040 --> 38:09.600]  что вот метод сходится быстро, довольно быстро там за какое-то понятное время. Вот. Ну давайте
[38:09.600 --> 38:15.160]  посмотрим, например, ну то есть в принципе в качестве целевой функции, в задаче оптимизации
[38:15.160 --> 38:20.040]  может быть что угодно. Какая-то плохая функция, в которой, например, есть какой-то глобальным оптимум,
[38:20.040 --> 38:30.080]  ну вот пусть есть. Вот. Ну вот я предлагаю рассмотреть невыпуклые задачи, невыпуклые задачи. Вот. И функция
[38:30.080 --> 38:40.640]  f при этом является Липшицевой. Липшицевой. И мы рассматриваем эти задачки на таком вот кубике,
[38:40.640 --> 38:47.000]  на кубике. Вот. Ну в двумерном плоскости это вот просто соответственно у нас такой квадратик. Вот.
[38:47.000 --> 38:54.240]  Ну там понятно дальше будет кубик и так далее. У нас размерность d. Вот. И мы предполагаем, что наша
[38:54.240 --> 39:01.000]  функция является Липшицевой. Выполнено следующее утверждение Липшицевой в L бесконечность нормы.
[39:01.000 --> 39:10.520]  Ну то есть получается значение функции, так скажем, меняется не сильнее чем m умножить на вот эту
[39:10.520 --> 39:15.200]  разность нормы кредитов в норме бесконечность. Норма бесконечность это просто максимум. И по
[39:15.200 --> 39:24.680]  компонентный максимум из разности x и t и y и t. Вот. Мы могли вообще ничего не предположить о
[39:24.680 --> 39:29.880]  функцию, но вот предположили это. Ну и окажется, сейчас увидим, что и этого окажется не особо
[39:29.880 --> 39:35.200]  достаточно для того, чтобы получать какие-то интересные результаты, но вот это мы к концу дойдем.
[39:35.200 --> 39:43.080]  Вот. Смотрите. Во-первых, в чем мы замечаем? Множество b это у нас компакт. Вот. Из-за Липшицевой
[39:43.080 --> 39:49.320]  этих функций следует то, что она, вот если мы устремим норму бесконечность, ну вот x устремим к y,
[39:49.320 --> 39:56.120]  мы получим непрерывность нашей функции. Вот. А в силу того, что у нас теперь непрерывная функция на
[39:56.120 --> 40:05.120]  компакте, что мы про нее знаем? Есть. Она достигает и своего минимума, и своего максимума, так? Поэтому
[40:05.120 --> 40:14.920]  давайте, почему бы нет? Максимум обозначили вот так вот. Верхний индекс. Ну и соответственно. Да,
[40:14.920 --> 40:27.320]  пожалуйста. Вот. Берем методы нулевого порядка. Можем считать только значение функции. Ну и
[40:27.320 --> 40:33.920]  хотим, соответственно, найти что-то близкое к Эпсилон решению. Так. Задача понятна. Какого рода
[40:33.920 --> 40:39.680]  метода мы можем использовать тоже понятна. Вот. Давайте попробуем решать. Может быть, вы какой-то
[40:39.680 --> 40:47.800]  предложите метод, который может подойти. Как решать? Только нулевое порядок. Градиентов нет.
[40:47.800 --> 40:57.960]  Отличный вариант. Отличный вариант. Вроде бы, как бы он простой, банальный. Давайте попробуем. Вот,
[40:57.960 --> 41:04.280]  пожалуйста. Сеточка. Если мы соответственно. То есть, что делаем? Давайте я на картинку нарисую.
[41:04.280 --> 41:15.120]  На квадратике нарисую. Берем. Разбиваем наш кубик на сеточку. Вот. И смотрим значение в узлах этой
[41:15.120 --> 41:25.640]  сеточки. Так. Где самое минимальное, то соответственно и вернем в качестве решения. Согласны? Хороший
[41:25.640 --> 41:33.320]  вариант. Хороший вариант. Понятный, главное. Вот. Он здесь и описан. Строим сеточку. Вот. И,
[41:33.320 --> 41:39.240]  соответственно, среди точек просто находим с минимальным значением и ее возвращаем. Вот.
[41:39.240 --> 41:45.200]  Давайте докажем вообще какие у нас есть гарантии на то, как этот метод работает. Вот. Здесь дана теорема,
[41:45.200 --> 41:54.480]  но ее нужно бы доказать. Вот. Давайте скажем, что вот у нас есть некоторая точка X звездой,
[41:54.480 --> 42:06.120]  которая является минимум нашей функции. Миниум нашей функции. Вот. Тогда давайте,
[42:06.120 --> 42:15.360]  что сделаем? Тогда вот в этой сетке этот минимум выделим. Ну, вот он пусть будет где-то вот здесь.
[42:15.360 --> 42:27.040]  Лежит он в каком-то из квадратиков. Так. Тогда что? Тогда я могу найти в некотором смысле этот
[42:27.040 --> 42:37.680]  квадратик, где он содержится. То есть, я могу задать этот квадратик уголками. Вот. Вот можно
[42:37.680 --> 42:44.800]  даже перерисовать этот квадратик. Какой-то квадратик есть. Я его, соответственно,
[42:44.800 --> 42:55.600]  вот так вот выделил. Вот. Мы, понятно, не знаем, где он находится, но вот вычислили значения в
[42:55.600 --> 43:02.840]  уголках и, соответственно, в том числе в уголках квадратика, где находится X звездой. Вот. Дальше,
[43:02.920 --> 43:09.480]  что хочется сделать? Дальше, что хочется сделать? Мы можем, соответственно, ну мы знаем,
[43:09.480 --> 43:18.400]  что стороны этого квадратика 1 на p, 1 делить на p, где p у нас это количество, у нас наш квадратик
[43:18.400 --> 43:24.400]  кубик размера единица, количество отрезочков, на которые мы разбили p. Поэтому размер вот этого
[43:24.400 --> 43:35.440]  маленького квадратика 1 делить на p. Вот. Соответственно, расстояние у него будет 1 делить
[43:35.440 --> 43:42.480]  на p. Дальше, что я хочу сделать? Дальше, я хочу найти уголок этого квадратика, который наиболее
[43:42.480 --> 43:53.760]  близок к X звездой. Вот. Ну, в данном случае-то вот будет вот этот уголок. Понятно, что это в
[43:53.760 --> 44:05.960]  любом случае можно сделать. Вот. И теперь наша задача понять, насколько алгоритм, наш алгоритм,
[44:05.960 --> 44:13.400]  мог ошибиться, если, например, он выберет вот эту точку, вот эту точку, как решение. Пусть наш алгоритм
[44:13.400 --> 44:25.200]  выбрал эту точку как решение. Вот. И вернулся, соответственно, f с крышечкой, и x с крышечкой,
[44:25.200 --> 44:32.720]  и значение f с крышечкой. Насколько это решение отличается с точки зрения значения функции от того,
[44:32.720 --> 44:40.920]  что могло произойти в X звездой? Так? Ну, давайте посмотрим. Здесь мы можем воспользоваться тем,
[44:40.920 --> 44:50.080]  что, видите, я могу оценить расстояние между вот этим уголочком и до X звездой. В худшем случае
[44:50.080 --> 45:00.240]  оно будет 1 делить на 2p. Согласны? 1 делить на 2p. Так? Когда вот у меня, соответственно,
[45:00.240 --> 45:08.440]  X звездой находится в серединке. Это худший случай. Вот. Понятно, там оно может быть поближе. Вот. Ну,
[45:08.440 --> 45:12.680]  вот я оцениваю сверху. Соответственно, говорю, что расстояние до решения от точки,
[45:12.680 --> 45:19.920]  которые мы выдали, от точки, которые мы выдали как решение, ну, вот оно меньше, чем 1 делить на 2p.
[45:19.920 --> 45:26.920]  Ну, соответственно, бесконечную норму мы тоже можем оценить вот таким вот образом. Окей. Дальше
[45:26.920 --> 45:35.760]  пользуемся липшицовостью функции и выписываем следующее. Вот это, соответственно, у нас точка уголок.
[45:35.840 --> 45:42.520]  Уголок. И мы пользуемся, что расстояние от уголка до решения меньше, чем по бесконечной норме 1
[45:42.520 --> 45:48.400]  делить на 2p. Ну, и соответственно, функциональное значение меньше, чем m делить на 2p. Согласны?
[45:48.400 --> 45:55.560]  Вот. Здесь также учтено, что в качестве решения мы могли выдать что-то, кроме уголка. Что-то,
[45:55.560 --> 46:03.120]  кроме уголка, где у нас по факту значение было меньше. Но вот в худшем случае как бы мы вернули
[46:03.120 --> 46:09.600]  этот уголок, и тогда мы точно можем гарантировать, что у нас m делить на 2p. Расстояние до решения,
[46:09.600 --> 46:17.280]  именно функциональное оно, m делить на 2p. Вот. Вот этот момент мне кажется немного таким вот хитрым,
[46:17.280 --> 46:21.880]  потому что мы могли вернуть что-то лучшее. Вот. Но по факту могли точно вернуть уголок, и там вот
[46:21.880 --> 46:31.600]  решение будет отличаться от реального, не меньше, чем вот на это число. Вот. Этот момент мы еще чуть-чуть
[46:31.600 --> 46:38.280]  к нему потом вернемся. Вот. Окей. Тогда мы можем сказать, что вот то решение, которое мы вернули,
[46:38.280 --> 46:47.360]  вот. Оно меньше, чем вот это. И тогда мы можем оценить число точек, которые нам нужно набросать,
[46:47.360 --> 46:52.640]  то есть то число p, которое нам нужно сделать, насколько нам нужно разбить наш квадратик. Ну,
[46:52.640 --> 46:56.600]  и делается это следующим образом. Мы хотим, чтобы это было меньше либо равно, чем эпсилон. Мы хотим
[46:56.600 --> 47:08.840]  достичь точности эпсилон. Поэтому отсюда мы можем найти p. Так. Вот. А значит, мы можем найти
[47:08.840 --> 47:14.240]  количество обращений к ракулу нулевого порядка, потому что мы знаем теперь количество точек в
[47:14.240 --> 47:23.800]  сетке, мы знаем размер, у нас размер у нас задачи d. Получается, что мы сделали, мы сделали p в степени
[47:23.800 --> 47:34.000]  d в учислении. То есть растет экспоненциально с размером задачки. Так. Вот. Окей. Хороший
[47:34.000 --> 47:51.400]  результат или плохой? Средний. Сейчас поймем. Сейчас поймем. Смотрите. Хороший или плохой
[47:51.400 --> 47:59.840]  результат предлагается посмотреть на следующем примере. Пусть у нас m равно 2. Небольшая константа
[47:59.840 --> 48:07.120]  липшится. Размерность задачи равна 13. Точность решения 1 сотая. Ну, то есть кажется, что все очень
[48:07.120 --> 48:14.960]  небольшое. Вот. Размерность? Это маленькая размерность. Для задач оптимизации эта размерность
[48:14.960 --> 48:25.120]  маленькая. Ну, это количество переменных, от которых у вас зависит целевая функция. Вот. Для задач
[48:25.120 --> 48:30.840]  оптимизации, для тех же нейронных сетей, это миллиарды. Вот. Обычно. Вот. Ну, для каких-то
[48:30.840 --> 48:36.320]  поменьше задач, понятно, там это тысячи сотни тысяч. Вот. Размерность 10, это очень маленькая
[48:36.320 --> 48:42.440]  размерность. Ну, там 13 в данном случае. Вот. Смотрите. И тогда вот количество обращений к нашему
[48:42.440 --> 48:51.280]  аракулу будет 10,26. 10,26. Вот. Если при этом обращение, ну, подсчет градиентов, ну, значение функций
[48:51.280 --> 48:58.240]  в точке довольно простое, то пусть у нас какая-то арифметическая сложность компьютера нашего, там,
[48:58.240 --> 49:04.760]  10 в 11 арифметических операций в секунду. Вот. Возможно, сейчас какие-то они более мощные есть. Ну,
[49:04.760 --> 49:12.720]  получается, что для решения вот такого вот несложной задачи, небольшого размера, да, не самой
[49:12.720 --> 49:22.720]  крутой точности нужно 30 миллионов лет. Не очень оптимистично справедливости ради? Вот. Не очень
[49:22.720 --> 49:27.840]  оптимистично. Ну, давайте я вам задам вопрос. А что мы вообще получили? Что это вот то, что мы
[49:27.840 --> 49:32.560]  получили какую-то оценку на решение? Это верхняя оценка, нижняя? Что вообще такое верхняя и нижняя
[49:32.560 --> 49:51.440]  оценка? Может, вы уже сталкивались с этим? Ну, вообще, то есть мы для алгоритма какого-то,
[49:51.440 --> 49:58.960]  который наш построили, получили какую-то оценку. Что вот он найдет решение через только вот вызова
[49:58.960 --> 50:08.480]  фаракула? Вот. Эпсила решения. Это верхняя или нижняя оценка? А как вы поняли, почему это? А что такое
[50:08.480 --> 50:26.800]  верхняя и нижняя оценка? Ну, смотрите, на самом деле чуть-чуть не так. То есть вы правильно сказали,
[50:26.800 --> 50:32.720]  что верхняя оценка, это у нас условно есть какой-то алгоритм, вот нами придуманный алгоритм, есть класс
[50:32.720 --> 50:43.280]  задач. Так. Ну, например, млипчество функций. Вот. И мы хотим гарантировать, что наш алгоритм для
[50:43.280 --> 50:50.600]  любой задачи из этого класса работает не хуже чем, и вот тогда мы получаем верхнюю оценку. То, что мы
[50:50.600 --> 50:57.280]  получили, это верхняя оценка. То есть какую бы мы задачу не взяли, наш алгоритм за вот представленное
[50:57.280 --> 51:05.320]  число итераций, огромное число итераций, оракульных вычислений найдет решение с точностью
[51:05.320 --> 51:10.600]  эпсилы. Эта верхняя оценка, соответственно, она вот выполняется для любой функции с классом,
[51:10.600 --> 51:18.320]  вот мы получается бомбим класс задач алгоритмом. Если мы говорим про нижнюю оценку, это в некотором
[51:18.320 --> 51:24.080]  смысле обратная вещь. У нас есть тоже класс алгоритмов, то есть у нас есть класс алгоритмов,
[51:24.080 --> 51:28.600]  алгоритмы нулевого порядка, алгоритмы, которые используют только значение функций. Мы выбрали,
[51:28.600 --> 51:33.400]  когда брали верхнюю оценку, выбрали определенный алгоритм оттуда и бомбили им весь класс функций,
[51:33.400 --> 51:39.920]  которые мы рассматриваем. В нижней оценке наоборот. Мы говорим, что вот есть такая плохая задача,
[51:39.920 --> 51:49.680]  что любой алгоритм из алгоритмов нулевого порядка найдет решение не раньше, чем через такое
[51:49.680 --> 51:55.960]  арифметическое число оракульных вызовов. Ну и смотрите, а что происходит, когда у нас
[51:55.960 --> 52:02.480]  верхняя и нижняя оценка совпали? Что это говорит? О чем это говорит? Мы нашли оптимальный алгоритм,
[52:02.480 --> 52:09.760]  то есть вот этот алгоритм для этого класса задач не улучшаем. И в связи с этим у нас возникает
[52:09.760 --> 52:14.000]  вопрос, то, что мы вот с вами напридумывали, это вообще нормальный алгоритм или нет? Ну то есть,
[52:14.000 --> 52:18.920]  может быть, мы алгоритм плохой придумали, может, мы плохой анализ сделали, потому что я как говорил,
[52:18.920 --> 52:26.800]  то что для точки X, которая находится в углу, да, мы можем гарантировать вот это, но возможно нам
[52:26.800 --> 52:31.800]  надо как-то по-другому выбирать эту точку, и возможно вот для какой-то другой точки вот эта оценка
[52:31.800 --> 52:37.240]  будет лучше. И на этот вопрос как раз отвечают нижние оценки, причем нижние оценки можно приводить
[52:37.240 --> 52:42.880]  не только для класса алгоритмов, а для конкретного алгоритма, чтобы просто проверить его на анализ,
[52:42.880 --> 52:51.280]  то есть на то, насколько он хорошо работает. То есть привести тоже ему плохую функцию и конкретно
[52:51.280 --> 52:57.280]  для этого алгоритма показать, что вот как бы он не работал, то он будет работать плохо. Ну и
[52:57.280 --> 53:02.320]  соответственно, да, давайте поработаем над этим и покажем, что на самом деле тут все не улучшаемо.
[53:02.320 --> 53:05.880]  То есть для вот этого класса функций вот такая сеточка это лучший вариант.
[53:05.880 --> 53:23.520]  Мы не знаем, где он, я просто хочу оценить, что произойдет, если вот он находится,
[53:23.520 --> 53:29.840]  например, в этом квадратике, вот, и я же верну какое-то значение в узле сетки по алгоритму,
[53:29.840 --> 53:34.360]  алгоритм так работает. Ну вот я смотрю на ближайший квадратик и смотрю, насколько там отличается.
[53:34.360 --> 53:39.840]  Вот этой? Первый?
[53:39.840 --> 53:49.240]  А нет. Так, вот она.
[53:49.240 --> 54:01.320]  Вот, окей, теперь хочется показать то, что мы сейчас натворили, на самом деле еще и хороший
[54:01.320 --> 54:09.640]  алгоритм для вот такого класса задач. Вот, окей, смысл тут будет вот такой. Пойти
[54:09.640 --> 54:20.240]  от противного. Опять давайте рассмотрим нашу вот эту сетку. Рассмотрим эту нашу сетку. Вот,
[54:20.240 --> 54:28.640]  и скажем, что вот существует алгоритм, вот это я обозначу за n, число итерации n, там m делить
[54:28.640 --> 54:36.920]  на 2 эпсилон d, вот, пусть будет это еще pd, вот. Я говорю, что существует алгоритм, который за n,
[54:37.240 --> 54:50.880]  с волной итерации меньше, пусть будет меньше, чем n, найдет решение, найдет решение, так? Вот.
[54:50.880 --> 54:58.280]  Не итерация, только арифметических вызовов нулевой точки. Ну так, давайте подумаем,
[54:58.280 --> 55:07.480]  что мы можем сказать вообще о том, что происходит. То есть, вот, смотрите, у нас есть, опять же,
[55:07.480 --> 55:16.040]  сетка, и вот в этой сетке теперь я рассматриваю не узлы, а вот эти внутренние квадратики. Эти
[55:16.040 --> 55:20.480]  внутренние квадратики, та же самая сетка, что была в предыдущем случае. Мы взяли нашу функцию,
[55:20.480 --> 55:26.520]  разбили вот этот кубик, на p сегментов получили вот такой вот кубик, то есть разрезанный кубик,
[55:26.520 --> 55:36.400]  то есть получается pd вот таких вот квадратиков. Мы вызвали аракул в n с волной точек, так? n с
[55:36.400 --> 55:43.560]  волной точек. Вот. И предполагается следующее, давайте изначально определим нашу функцию нулем везде,
[55:43.560 --> 55:50.240]  нулем везде, и запустим тот алгоритм, мы предположили, что он существует, который работает
[55:50.240 --> 55:56.000]  быстрее, чем вот, чем вот это, чем вот это. Вот. Пусть он существует, пусть он существует,
[55:56.000 --> 56:01.760]  давайте его запустим на чисто нулевой функции. Просто функция везде равна нулю. Что он начнет
[56:01.760 --> 56:05.840]  делать? Ну, он как-то работает, мы не знаем, как он работает, но в любом случае он вызывает
[56:05.840 --> 56:17.560]  значение функции в точках, так? И эти точки как-то ложатся в нашу сетку, так? В итоге он вызывал там
[56:17.560 --> 56:24.480]  свое n с волной точек и выдал какой-то, и в итоге он везде, и мы говорят, везде ноль. Ну,
[56:24.480 --> 56:29.720]  он что-то выдаст, он еще выдаст одну точку, давайте вот здесь на всякий, для строгости напишу n минус
[56:29.720 --> 56:36.840]  один, вот. Он в итоге запросил n с волной точек, n с волной точек, как-то они раскиданы на квадратик,
[56:36.840 --> 56:41.960]  возможно они в одном и том же квадратике попадают, вот кубик наш маленький, вот. В итоге он выдал
[56:41.960 --> 56:46.240]  какое-то решение, ну вот понятно, он какой-то тоже от балды выдал, потому что везде у него ноль,
[56:46.240 --> 56:51.760]  вот. Как он будет по этому рассуждать? Ну, он тоже дал еще одну точку, которая тоже легла в квадратик.
[56:51.760 --> 56:59.600]  В итоге по факту мы просмотрели n с волной точек плюс одну точку, то есть n с волной это он
[56:59.600 --> 57:06.560]  просто популял, вот, а плюс одна это то, что он выдал в качестве решения, вот. И в силу того,
[57:06.560 --> 57:13.640]  что у нас n определено вот так вот, n большое, без волны, вот, то что у нас по принципу Дерехле, верно?
[57:13.640 --> 57:23.280]  В каком-то квадратике мы не заглянули, в какой-то квадратике мы не заглянули, вот. И получается,
[57:23.280 --> 57:30.240]  что решение, которое мы получили, которое он выдал, ну в лучшем случае лежит на границе этого квадратика,
[57:30.240 --> 57:38.160]  да? Вот. Ну идея в том, что давайте теперь чуть-чуть модернизируем функцию под этот метод конкретный и
[57:38.160 --> 57:44.760]  минимум просто провалим в этот квадратик, вот. И вот здесь вот сделаем вот такую вот ямочку в
[57:44.760 --> 57:51.220]  этом квадратике, сделаем ямочку, вот. Для каждого алгоритма это рабочая схема, то есть может быть
[57:51.220 --> 57:56.640]  другой алгоритм, он, возможно, выбирает по другому точке, по другому возвращает решение. Но для него
[57:56.640 --> 58:02.640]  я тоже найду квадратик, возможно, не этот, который здесь вот, а другой, куда я помещу ямочку. Куда я
[58:02.640 --> 58:11.480]  помещу ямочку и, соответственно, в лучшем случае я буду только на границе этой ямочки. А значит,
[58:11.480 --> 58:19.200]  могу оценить, насколько я там, вот, для этой ямочки далек от решения. Далек от решения. Ну и там,
[58:19.200 --> 58:26.240]  соответственно, получается ровно та оценка, которую получили, потому что у нас, вот, я полипшется
[58:26.560 --> 58:33.800]  в эту ямочку максимально спущу минимум, вот. Здесь у меня значение на границе ноль, вот. А вот здесь у
[58:33.800 --> 58:39.760]  меня ноль везде на границах, а здесь, соответственно, в центре вот этой ямочки будет там значение,
[58:39.760 --> 58:48.400]  сколько m делить на 2p, вот. Все. Получается, что вот так вот. Видите, в чем проблема. И, на самом деле,
[58:48.400 --> 58:54.120]  для нашего конкретного алгоритма это тоже верно, вот. Тоже можно определить вот эту плоскость,
[58:54.120 --> 58:59.800]  которая везде ноль. Мы начнем тыкать, тыкать, тыкать, тыкать. Вот. Но только мы натыкаем везде в
[58:59.800 --> 59:06.000]  нолик, так. Ну и вернем какую-то из точек. Они везде, в принципе, они между собой эквивалентны. И вот,
[59:06.000 --> 59:13.280]  когда я здесь говорил, что она может быть лучше, вот здесь вот. То есть, точка, которую мы вернем,
[59:13.280 --> 59:21.320]  от точки, которая ближайшая к решению именно по сетке, вот. Ну, в нижней нам оценке говорят о том,
[59:21.320 --> 59:27.760]  что в худшем случае эти точки между собой имеют одинаковое значение. Вот. То к чему я хотел вернуться.
[59:27.760 --> 59:32.280]  Получается, что вот для такого класса функций мы вроде как-то ограничили функции. Сказали,
[59:32.280 --> 59:37.960]  что это не совсем произвольная функция целевая. Не совсем произвольная множество, на котором мы
[59:37.960 --> 59:43.880]  решаем. Но, оказывается, мы можем как бы всегда найти такую плохую функцию, для которой вот метод
[59:43.880 --> 59:51.800]  нулевого порядка работает не лучше, чем вот этот тупой сеточный перебор. Вот. Вот плохие новости
[59:51.800 --> 59:58.520]  справедливости ради. То есть, это, как мы поняли, работает плохо. Вот. Поэтому вообще теория оптимизации
[59:58.520 --> 01:00:07.880]  строится на более, так скажем, приятных функциях. Вот. В том числе выпуклых, гладких. Вот. Но об этом мы
[01:00:07.880 --> 01:00:12.760]  подробнее, соответственно, поговорим уже в следующий раз. Почему такие функции рассматриваются?
[01:00:12.760 --> 01:00:19.360]  Ну, вот соответственно, какие, что это за функции? Какие у них есть свойства? Какие у них есть
[01:00:19.360 --> 01:00:41.400]  свойства? Вот. Да? Почему вероятностный? Он детерминистический? В среднем на классе
[01:00:41.400 --> 01:00:47.480]  функций? В среднем на классе функций довольно сложно. То есть, это интересный вопрос, но такое,
[01:00:47.480 --> 01:00:53.240]  конечно, исследователи пока не способны. Вот. Как алгоритм BCI ведет в среднем на классе функций?
[01:00:53.240 --> 01:00:59.360]  Ну, это довольно жестко. Вот. Что значит вообще среднее по всем функциям? Сложно вообще описать
[01:00:59.360 --> 01:01:09.960]  класс функций, вот именно как вот. Я понимаю, да, да, да. Да, конечно, конечно. И вот это, на самом деле,
[01:01:09.960 --> 01:01:14.600]  хорошая мысль. Мы будем рассматривать выпуклые задачи, гладкие задачи. Вот. На самом деле,
[01:01:14.600 --> 01:01:20.520]  условно, там нейронные сети, они не выпуклые, не гладкие. Вот. Но при этом, как показывает жизнь,
[01:01:20.520 --> 01:01:28.200]  алгоритмы на них работают ровно так, как работают в теории для выпуклых и для гладких задач. Это
[01:01:28.200 --> 01:01:33.320]  удивительно. Вот. Удивительный факт, что вот, ну вот, практические задачи, они действительно не
[01:01:33.320 --> 01:01:39.760]  настолько плохие. Там мы явно не делаем специальные ямки, чтобы алгоритм у нас облажался. Вот. Они
[01:01:39.760 --> 01:01:45.880]  лучше, они значительно лучше, у них лучше природа. И поэтому вот вся теория, которая действительно
[01:01:45.880 --> 01:01:50.920]  строится сейчас в оптимизации, она более чем релевантна. Несмотря на то, что многие практические
[01:01:50.920 --> 01:01:56.680]  задачи, они, ну, именно с точки зрения, именно описания, они ни разу не какие-то красивые. Вот.
[01:01:56.680 --> 01:02:04.160]  Но можно предположить, что они близки к красивым задачам, хорошим задачам. И поэтому вот эта теория,
[01:02:04.160 --> 01:02:09.760]  она в том числе перекладывается и на нейронные сети. Вот. Но к этому мы, соответственно, придем уже
[01:02:09.760 --> 01:02:17.920]  на следующей лекции, и там поговорим про свойства, функции, которые мы будем предполагать. Вот. А сейчас
[01:02:17.920 --> 01:02:23.080]  я вообще покажу, на какого рода сходимости мы вообще будем смотреть. Потому что эта безобразие,
[01:02:23.080 --> 01:02:30.360]  с которой растет экспоненциальность с размерностью, это ужас. Поэтому мы будем говорить про более,
[01:02:30.720 --> 01:02:36.760]  конечно, классные сходимости с точки зрения числа итераций. Вот. В частности, сублинейная,
[01:02:36.760 --> 01:02:42.840]  линейная, сверхлинейная, квадратичная. Сублинейная сходимость, сходимость как
[01:02:42.840 --> 01:02:47.080]  полюном от к, ну, деление на полюном от к. Например, один делить на k,
[01:02:47.080 --> 01:02:52.440]  один делить на k в квадрате. Нормальная сходимость, но можно лучше. Можно сходиться
[01:02:52.440 --> 01:02:57.520]  с линейной скоростью, также со скоростью геометрической прогрессии. Когда вы к решению
[01:02:57.520 --> 01:03:01.120]  приближаетесь, выедая постоянно вот этот множитель Q.
[01:03:01.120 --> 01:03:04.640]  Это значительно быстрее, чем полином.
[01:03:04.640 --> 01:03:07.640]  Можно вообще сверхлинейно, то есть тут в K добавить
[01:03:07.640 --> 01:03:10.000]  еще дополнительную степень, и тогда у вас выед будет
[01:03:10.000 --> 01:03:11.000]  еще быстрее.
[01:03:11.000 --> 01:03:14.520]  А можно еще круче, можно вот в степень Q поставить
[01:03:14.520 --> 01:03:15.520]  2 в степени K.
[01:03:15.520 --> 01:03:18.560]  Понятно, что у вас экспоненты 2 в степени K будут расти
[01:03:18.560 --> 01:03:22.720]  быстрее, чем в любой полином K в степени P, ну понятно,
[01:03:22.720 --> 01:03:23.720]  вы симптотики.
[01:03:24.720 --> 01:03:28.120]  Вот такие 4 вида сходимости, со всеми мы в некотором
[01:03:28.120 --> 01:03:32.240]  смысле столкнемся, поговорим про, в каких алгоритмах они
[01:03:32.240 --> 01:03:33.240]  наблюдаются.
[01:03:33.240 --> 01:03:38.200]  Ну а на графиках это все безобразие выглядит следующим
[01:03:38.200 --> 01:03:39.200]  образом.
[01:03:39.200 --> 01:03:42.680]  А, кстати, да, вопрос, почему скорость геометрической
[01:03:42.680 --> 01:03:47.440]  прогрессии называется линейной?
[01:03:47.440 --> 01:03:50.520]  В агарифмической шкале она линейна?
[01:03:50.520 --> 01:03:53.560]  Ну, просто принято в теории оптимизации, потому что
[01:03:53.560 --> 01:03:56.480]  мы хотим все более точное решение, мы спускаемся как
[01:03:56.480 --> 01:03:59.560]  бы, то есть берем одну сотую решение, потом одну тысячную
[01:03:59.560 --> 01:04:03.640]  там условно, и так далее, там 10-6, и поэтому вот в теории
[01:04:03.640 --> 01:04:08.920]  оптимизации принято по оси Y строить лагарифмическую
[01:04:08.920 --> 01:04:12.200]  шкалу, вот, она удобнее просто, и вот на этой лагарифмической
[01:04:12.200 --> 01:04:17.440]  шкале как раз у линейной сходимости у вас как раз
[01:04:17.440 --> 01:04:21.360]  будет график выглядеть как линия, поэтому и называется
[01:04:21.360 --> 01:04:22.360]  линейной.
[01:04:22.360 --> 01:04:26.040]  Вот, ну, соответственно, здесь я добавляю уже супер
[01:04:26.040 --> 01:04:30.000]  линейный, видно, что все работает значительно лучше,
[01:04:30.000 --> 01:04:35.040]  ну и как добивочка квадратичная, вот, более того, действительно,
[01:04:35.040 --> 01:04:38.000]  будут методы, которые работают как квадратичные, то есть
[01:04:38.000 --> 01:04:40.840]  условно там градиентный спуск работает линейно,
[01:04:40.840 --> 01:04:45.680]  а, например, метод Ньютон, он уже сходится квадратично,
[01:04:45.680 --> 01:04:51.360]  значительно более быстрый метод, вот, так, все, на сегодня
[01:04:51.360 --> 01:04:52.760]  у меня последний слайд, на сегодня все.

[00:00.000 --> 00:07.840]  Сегодня такая, в каком-то смысле, выдаваляющая некоторые
[00:07.840 --> 00:13.640]  предварительные итоги лекции про то, о чём мы обсуждали
[00:13.640 --> 00:19.440]  последние, типа, полтора месяца, и здесь мы придётся
[00:19.440 --> 00:21.840]  вспомнить всё то, с чём мы начинали, там, в частности,
[00:21.840 --> 00:23.920]  там, сопряжённые конусы, вот это всё, сейчас оно
[00:23.920 --> 00:27.520]  на счёт неистовы использоваться, вот, и я надеюсь, станет
[00:27.520 --> 00:29.600]  более-менее понятно, зачем все эти теоретические
[00:29.600 --> 00:30.600]  построения были необходимы.
[00:30.600 --> 00:31.600]  Вот.
[00:31.600 --> 00:34.880]  В следующий раз будет промежуточная лекция между теорией
[00:34.880 --> 00:38.640]  и методами, когда мы посмотрим на то, как автоматически
[00:38.640 --> 00:41.720]  унифицировать форму задачи, которую вы можете записывать
[00:41.720 --> 00:42.720]  помощью формул.
[00:42.720 --> 00:43.720]  Вот.
[00:43.720 --> 00:49.240]  То есть, как они преобразуются к коническому виду, про
[00:49.240 --> 00:51.840]  который сейчас ещё будет сказано дополнительно,
[00:51.840 --> 00:57.360]  вот, и как процедура построения, приведения задачи к стандартной
[00:57.840 --> 01:00.400]  форме, связана с процедурой проверки, то, что вы написали
[01:00.400 --> 01:02.960]  в упаковку задачи, а не то, что пришло вам в голову,
[01:02.960 --> 01:03.960]  просто так.
[01:03.960 --> 01:04.960]  Вот.
[01:04.960 --> 01:05.960]  Это всё будет в следующий раз.
[01:05.960 --> 01:08.960]  И через раз, соответственно, уже начнём методы.
[01:08.960 --> 01:09.960]  Вот.
[01:09.960 --> 01:10.960]  Так.
[01:10.960 --> 01:17.200]  Но прежде чем перейти, собственно, к тому, что, да, к тому,
[01:17.200 --> 01:20.720]  что сегодня про двойственность продолжим говорить, и,
[01:20.720 --> 01:22.640]  в частности, про коническую двойственность, что это
[01:22.640 --> 01:25.720]  такое откуда-то наберётся, и чем она примечательна.
[01:25.800 --> 01:28.680]  Почему есть просто двойственность ещё конической двойственности,
[01:28.680 --> 01:30.760]  чем она, как бы, почему её выделяет в отдельный термин.
[01:30.760 --> 01:31.760]  Так.
[01:31.760 --> 01:37.600]  В прошлый раз мы обсудили, закончили обсуждать условия
[01:37.600 --> 01:42.360]  оптимальности, двойственность, условия слейтера, условия
[01:42.360 --> 01:43.360]  ККТ.
[01:43.360 --> 01:44.360]  Вот.
[01:44.360 --> 01:47.880]  Сейчас я их немножко повторю, чтобы ещё раз вам напомнить
[01:47.880 --> 01:48.880]  о том, как они выглядят.
[01:48.880 --> 01:49.880]  Вот.
[01:49.880 --> 01:54.280]  Это просто слайды, summary слайды с прошлой недели.
[01:54.280 --> 01:55.280]  Вот.
[01:55.280 --> 01:58.520]  Значит, если у вас есть тройка значений, которые
[01:58.520 --> 02:02.040]  являются решением прямой двойственной задачи и сильной
[02:02.040 --> 02:05.040]  двойственности выполняется, то есть оптимальные значения
[02:05.040 --> 02:09.880]  на х и на лямбда и м со звёздочками совпадают, то справедливо
[02:09.880 --> 02:11.480]  следующие пятёрка условий.
[02:11.480 --> 02:12.480]  Вот.
[02:12.480 --> 02:13.480]  Довольно очевидно.
[02:13.480 --> 02:17.760]  Тут вот мы показывали, что если выполнена сильная
[02:17.760 --> 02:21.280]  двойственность, то у нас есть соотношение между
[02:21.280 --> 02:28.280]  тем, что минимизация лагранжана по х совпадает с лагранжаном
[02:28.280 --> 02:31.000]  в точке х со звёздочкой, которая есть решение исходной
[02:31.000 --> 02:32.000]  задачи прямой.
[02:32.000 --> 02:33.000]  Вот.
[02:33.000 --> 02:35.000]  И, собственно, вот это вот отсюда как раз-таки и
[02:35.000 --> 02:36.000]  берётся.
[02:36.000 --> 02:39.280]  Значит, допустимость х со звёздочкой в исходной
[02:39.280 --> 02:42.120]  задаче, допустимость μ в двойственной, дополняющая
[02:42.120 --> 02:45.800]  нежесткость в условии дополняющей нежесткости и сационарность
[02:45.800 --> 02:47.880]  лагранжана по прямым переменам.
[02:47.880 --> 02:51.000]  То есть равен с нулю градиента лагранжана по х в точке
[02:51.000 --> 02:54.160]  х со звёздочкой при условии, что мы этот лагранжан считаем
[02:54.160 --> 02:56.480]  в лямбду со звёздочкой и миду со звёздочкой.
[02:56.480 --> 02:57.480]  Это надо выучить.
[02:57.480 --> 02:58.480]  Вот.
[02:58.480 --> 03:02.840]  Значит, в случае выпуклой задачи у нас было два утверждения.
[03:02.840 --> 03:05.800]  Первое утверждение в одну сторону.
[03:05.800 --> 03:08.880]  Оно нам говорит о том, что если есть выпуклая задача
[03:08.880 --> 03:13.960]  и есть три вектора, для которых выполнены условия ККТ,
[03:13.960 --> 03:17.320]  тогда у нас есть сильная двойственность и эти три
[03:17.320 --> 03:20.560]  вектора есть решение прямой двойственной задачи.
[03:20.560 --> 03:21.560]  Вот.
[03:21.560 --> 03:25.680]  То есть в чём принципиальное отличие вот этого условия
[03:25.680 --> 03:26.680]  от вот этого условия?
[03:26.680 --> 03:29.680]  То есть второе условие нам говорит о том, что есть
[03:29.680 --> 03:32.760]  из-за чего выпуклые выполнены условия слейтера, тогда
[03:32.760 --> 03:37.320]  х решение прямой задачи, это равносильно тому, что
[03:37.320 --> 03:40.480]  существуют лямбда и мю такие, что для них, для всех этих
[03:40.480 --> 03:43.000]  трёх векторов выполнены ККТ.
[03:43.000 --> 03:44.000]  Вот.
[03:44.000 --> 03:50.960]  Здесь не утверждается никаких, то есть не оговариваются
[03:50.960 --> 03:54.200]  никакие условия, при которых эти лямбда и мю будут существовать.
[03:54.200 --> 03:56.880]  То есть тут мы говорим просто.
[03:56.880 --> 04:01.440]  Если каких-то трёх, три мы взяли, подставили, получили
[04:01.440 --> 04:02.440]  это ответ.
[04:02.440 --> 04:03.440]  Вот.
[04:03.440 --> 04:10.440]  Здесь же утверждается, что существование х будет
[04:10.440 --> 04:13.320]  решением только в том случае, когда найдутся такие лямбда
[04:13.320 --> 04:14.320]  и мю.
[04:14.320 --> 04:15.320]  Вот.
[04:15.320 --> 04:18.360]  То есть тут немножко более как бы общий и инструментальный
[04:18.360 --> 04:22.440]  факт, что мы всего лишь добавив свой слейтер, гарантируем
[04:22.440 --> 04:24.440]  существование таких векторов.
[04:24.440 --> 04:25.440]  Вот.
[04:25.440 --> 04:28.960]  Это отличие, которое я ещё раз хочу проговорить.
[04:28.960 --> 04:33.680]  Значит, то, что, по-моему, в прошлый раз, я не помню,
[04:33.680 --> 04:36.800]  честно говоря, насколько подробно обсудили, о том,
[04:36.800 --> 04:39.760]  что мы можем, мы уже вроде-то обсуждали про то, как мы
[04:39.760 --> 04:41.960]  можем переформулировать задачи в эквивалентном
[04:41.960 --> 04:44.720]  виде, через над-график, через вот, новых равенств,
[04:44.720 --> 04:46.840]  неравенств, добавление новых переменных.
[04:46.840 --> 04:47.840]  Вот.
[04:47.840 --> 04:52.680]  При этом задачи будут эквивалентные, но построенные для них
[04:52.680 --> 04:54.960]  двойственные задачи могут быть не эквивалентные.
[04:54.960 --> 04:56.560]  То есть они будут просто по-разному записываться.
[04:56.560 --> 04:57.560]  Вот.
[04:57.560 --> 05:02.160]  И в зависимости от этого вы можете получить как более,
[05:02.160 --> 05:04.400]  то есть вы можете переписать исходную задачу некоторым
[05:04.400 --> 05:07.680]  образом и получить соответствующие двойственные задачи, которые
[05:07.680 --> 05:09.680]  будут решаться проще, например.
[05:09.680 --> 05:10.680]  Вот.
[05:10.680 --> 05:13.160]  Например, типа, стандартный приём, вы хотим написать
[05:13.160 --> 05:14.920]  двойственную задачу для вот такой задачи.
[05:14.920 --> 05:18.040]  Но тут есть некоторые проблемы, эта задача без ограничений.
[05:18.040 --> 05:21.240]  Тут непонятно, как эта двойственная будет выглядеть.
[05:21.240 --> 05:22.240]  Вот.
[05:22.240 --> 05:25.640]  Чтобы было, процедура построения двойственной была более
[05:25.640 --> 05:29.280]  содержательной, предлагается ввести новую перемену y
[05:29.280 --> 05:31.960]  и сказать, что ах-b просто равно y.
[05:31.960 --> 05:35.960]  Тогда у нас мы расширяем множество переменных,
[05:35.960 --> 05:37.520]  на которых мы хотим что-то решить.
[05:37.640 --> 05:40.960]  Теперь у нас это типа m плюс n, x и y.
[05:40.960 --> 05:41.960]  Вот.
[05:41.960 --> 05:44.640]  Но зато появляется ограничение типа равенства, и двойственную
[05:44.640 --> 05:47.640]  задача уже понятно, как строить.
[05:47.640 --> 05:50.960]  Понятно ли это преобразование?
[05:50.960 --> 05:51.960]  Прекрасно.
[05:51.960 --> 05:57.640]  Второй приём более хитрый, потому что он напрямую опирается
[05:57.640 --> 06:00.800]  на тот факт, что когда мы строили двойственную функцию,
[06:00.800 --> 06:07.960]  то мы искали инфиумы Лагранжана по всему множеству допустимых
[06:07.960 --> 06:14.920]  х, не используя в поиске инфиума никакие ограничения.
[06:14.920 --> 06:17.400]  То есть взяли ограничения, которые у нас были, отправили
[06:17.400 --> 06:21.520]  их в Лагранжан и забыли про них в поиске инфиума.
[06:21.520 --> 06:24.520]  Все об этом помнят, да?
[06:24.520 --> 06:25.520]  Окей.
[06:26.240 --> 06:29.120]  Альтернативный способ, ну не альтернативный способ,
[06:29.120 --> 06:33.400]  а как бы некоторая вариация на тему заключается в том,
[06:33.400 --> 06:36.520]  что вот если у нас была какая-нибудь такая задача,
[06:36.520 --> 06:40.840]  то мы вот это вот условие можем из функциональных
[06:40.840 --> 06:43.840]  ограничений, которые у нас записывались бы в Лагранжане,
[06:43.840 --> 06:48.120]  отправлялись, включить их в это вот неявное условие,
[06:48.120 --> 06:51.520]  что у нас там х лежал в множестве d, если вы помните, такое
[06:51.520 --> 06:52.520]  обозначение было.
[06:52.520 --> 06:53.520]  Вот.
[06:53.520 --> 06:55.440]  И соответственно, когда будем писать Лагранжан, то
[06:55.440 --> 07:00.680]  в него уйдет только вот это ограничение, но инфиум
[07:00.680 --> 07:02.160]  будет искаться по вот этому множеству.
[07:02.160 --> 07:06.520]  То есть у вас есть как бы такой вот баланс между
[07:06.520 --> 07:09.200]  тем, насколько легко вы будете считать инфиум из
[07:09.200 --> 07:13.240]  Лагранжана, то есть если у вас, например, тут х из
[07:13.240 --> 07:16.800]  rn, и вы ищете минимум просто функции на rn, ну типа берем
[07:16.800 --> 07:18.800]  производный, подставляем, все получается.
[07:18.800 --> 07:19.800]  Вот.
[07:19.800 --> 07:23.760]  Но это добавляет вам ограничений в двойственную задачу, потому
[07:23.840 --> 07:26.440]  что наличие таких ограничений приводит к наличию ограничений
[07:26.440 --> 07:28.800]  типа неравенств на соответствующий множитель Лагранжа.
[07:28.800 --> 07:34.160]  Понятно, да, Лойко?
[07:34.160 --> 07:37.320]  Но, как альтернатива?
[07:37.320 --> 07:39.520]  Давайте вот мы видим, что эти ограничения довольно
[07:39.520 --> 07:42.520]  просты, всего лишь там х от а до б.
[07:42.520 --> 07:44.600]  Называется так называемым коробочного типа ограничения.
[07:44.600 --> 07:45.600]  Вот.
[07:45.600 --> 07:50.120]  Давайте мы их отправим просто вот в это множество, при
[07:50.120 --> 07:51.160]  этом что у нас будет?
[07:51.160 --> 07:54.160]  Если мы будем решать двойственную задачу чуть сложнее, то
[07:54.160 --> 07:58.280]  есть теперь нам инфиум Лагранжана искать не на всем пространстве,
[07:58.280 --> 08:00.680]  а только на тех х, которые от минус одного до одного.
[08:00.680 --> 08:05.840]  Это, наверное, чуть более сложно, чем для любых х.
[08:05.840 --> 08:06.840]  Вот.
[08:06.840 --> 08:07.840]  Зато что получаем?
[08:07.840 --> 08:10.800]  Получаем, что когда мы будем выписывать двойственную
[08:10.800 --> 08:13.400]  задачу, у нас никаких ограничений типа неравенств не будет,
[08:13.400 --> 08:14.400]  потому что у нас только равенства остались.
[08:14.400 --> 08:15.400]  Вот.
[08:15.400 --> 08:18.960]  То есть нам стало сложнее найти двойственную функцию,
[08:18.960 --> 08:24.000]  и стало проще стало выглядеть двойственная задача.
[08:24.000 --> 08:27.280]  Понятие на это как бы переливание из одного ведра в другое,
[08:27.280 --> 08:28.760]  грубо говоря, сложно, проблема.
[08:28.760 --> 08:31.720]  То есть как-то ноу фриланч, он здесь заключается в том,
[08:31.720 --> 08:34.560]  что в каком-то месте вам становится труднее.
[08:34.560 --> 08:36.640]  Либо вам труднее в том, что у вас двойственная задача
[08:36.640 --> 08:38.920]  сложнее, либо вам труднее в том, что ее двойственную
[08:38.920 --> 08:40.320]  функцию найти не так просто.
[08:40.320 --> 08:45.520]  Понятна логика.
[08:45.520 --> 08:46.520]  Прекрасно.
[08:47.360 --> 08:51.920]  Все это сейчас и в следующий раз будет активно нужно,
[08:51.920 --> 08:55.560]  потому что мы будем каким-то некоторым образом стандартизировать
[08:55.560 --> 08:56.840]  получаемые задачи.
[08:56.840 --> 08:58.720]  Так, ну пример.
[08:58.720 --> 09:01.120]  Наверное, на семинарах всем про это должны были рассказывать,
[09:01.120 --> 09:03.120]  в общем-то, я надеюсь, по крайней мере.
[09:03.120 --> 09:04.880]  Если кому-то не рассказывали, то я сейчас расскажу как
[09:04.880 --> 09:08.560]  раз таки, лекции для этого нужны, чтобы как-то унифицировать
[09:08.560 --> 09:11.120]  в целом то, что происходит в курсе.
[09:11.120 --> 09:13.400]  Значит, если у нас вот такая вот задача, ограничение
[09:13.400 --> 09:18.480]  типа неравенств, больше-больше либо равно, повторяю, вроде
[09:18.480 --> 09:20.680]  я уже про это говорил, что поскольку мы весь наш
[09:20.680 --> 09:23.680]  формализм строили в предположении, что у нас ограничение типа
[09:23.680 --> 09:25.640]  неравенств меньше либо равно, то надо умножить на
[09:25.640 --> 09:27.520]  минус в динечку сначала, перед тем как записывать
[09:27.520 --> 09:30.520]  лагранжа, вот поэтому тут вот этот минус начинает
[09:30.520 --> 09:31.520]  фигурировать.
[09:31.520 --> 09:34.000]  Вот лагранжа у нас выглядит вот так, мы на нее внимательно
[09:34.000 --> 09:37.800]  смотрим и с удивлением обнаружим, что наша функция лагранж
[09:37.800 --> 09:42.640]  есть не что иное, как линейная функция по х.
[09:42.640 --> 09:49.520]  Ну, вроде бы всем, надеюсь, это видно, всем это видно,
[09:49.520 --> 09:52.640]  просто вынесли, расписали, взяли, ну, то есть линейный
[09:52.640 --> 09:55.760]  алгебр там, а b транспонированный, это b транспонированный
[09:55.760 --> 09:58.160]  на A транспонированный, в общем-то здесь пока ничего
[09:58.160 --> 09:59.160]  более сложного нет.
[09:59.160 --> 10:03.400]  Нам надо найти минимум по всем х, поскольку мы все
[10:03.400 --> 10:06.320]  ограничения, которые у нас были, отправили в лагранжа,
[10:06.320 --> 10:09.280]  но линейная функция при всех х это минус бесконечность,
[10:09.760 --> 10:13.520]  она не ограничена снизу, вот, почти всегда за вычетом
[10:13.520 --> 10:17.160]  случая, когда у нас вот это ноль, когда она просто
[10:17.160 --> 10:22.320]  равна константе, вот, поэтому двойственная функция будет
[10:22.320 --> 10:25.440]  иметь такой вид, что при условии, что у нас выполнено
[10:25.440 --> 10:28.760]  такое равенство, у нас она равна минус лямин транспонированный.
[10:28.760 --> 10:33.360]  Суммируя это все в двойственную задачу, мы получим следующий
[10:33.360 --> 10:38.520]  не совсем очевидный ее вид, возможно, откуда он взялся,
[10:38.520 --> 10:42.320]  первое, мы должны максимизировать двойственную функцию, но
[10:42.320 --> 10:45.000]  тут стоит минус, поэтому давайте мы заменим максимум
[10:45.000 --> 10:48.040]  на минимум и уберем этот минус, осталась вот эта
[10:48.040 --> 10:49.040]  история.
[10:49.040 --> 10:52.880]  Далее, у нас есть ограничение, при котором эта двойственная
[10:52.880 --> 10:56.560]  функция конечна, это вот это равенство, но по мимо
[10:56.560 --> 10:58.800]  этого равенства у нас еще есть ограничение на μ,
[10:58.800 --> 11:01.640]  потому что μ это множество лагранжа для ограничений
[11:01.640 --> 11:05.920]  типа неравенств, и μ должно быть больше либо равно нуля,
[11:06.280 --> 11:10.080]  ну и соответственно мы отсюда можем μ выразить, это просто
[11:10.080 --> 11:12.280]  c плюс а транспонированная лямба, и μ больше либо равно
[11:12.280 --> 11:15.320]  нуля, значит, что а транспонированная лямба плюс c больше либо
[11:15.320 --> 11:25.280]  равно нуля, записали, все ли преобразования понятны?
[11:25.280 --> 11:34.280]  Поднимите руки, кому все понятно?
[11:34.640 --> 11:42.880]  Вот это да, можно, то есть мы получим лямбду, потом
[11:42.880 --> 11:45.880]  сюда поставим лямбду, получим мю, мю равно вот этой штуке,
[11:45.880 --> 11:48.800]  все должно выполняться по построению.
[11:48.800 --> 11:54.640]  Вот, значит, это простейший случай, и на программирование
[11:54.640 --> 12:00.280]  все, то есть тут порой, ну то есть есть путь, по которому
[12:00.280 --> 12:04.680]  говорят, типа давайте мы посмотрим на c, а, b и неравенство,
[12:04.680 --> 12:08.000]  и запомним, куда надо что подставлять, что b уходит
[12:08.000 --> 12:12.280]  в линейную функцию сюда, а ограничение типа ну там
[12:12.280 --> 12:14.080]  равенств, неравенств, оно просто комбинируется
[12:14.080 --> 12:16.880]  из а транспонированной c, ну в общем, я не люблю так
[12:16.880 --> 12:19.560]  делать, в общем, приятно понимать, откуда что берется,
[12:19.560 --> 12:22.280]  и вывод к тому же здесь, типа на три строчки, вот,
[12:22.280 --> 12:25.680]  очень рекомендую разобраться, откуда это берется, и сильно
[12:25.680 --> 12:30.000]  не запоминать конкретные формулы, а уметь их уводить,
[12:30.000 --> 12:33.200]  к тому же здесь это несложно, вот, значит, соответственно,
[12:33.200 --> 12:34.840]  устоя оптимальности для линейного программирования
[12:34.840 --> 12:38.000]  будут иметь следующий вид, довольно несложный,
[12:38.000 --> 12:41.200]  у нас будет, а х отверточка равно b, х больше нуля, это
[12:41.200 --> 12:49.440]  просто допустимость в исходной задаче, вот эта штука, это
[12:49.440 --> 12:50.440]  что такое?
[12:50.440 --> 12:59.000]  И там еще что-то появится, да, вот эта штука, это что
[12:59.000 --> 13:00.000]  такое?
[13:00.000 --> 13:01.000]  Откуда она взялась?
[13:01.000 --> 13:18.160]  Ну а что с предыдущим слайдом, мы же к ККТ пишем, к какому
[13:18.160 --> 13:27.240]  условию из ККТ соответствует это равенство, да, это градиент
[13:27.240 --> 13:33.320]  Лагранжаана, вот он, и вот в оптимальном лямбда-имю
[13:33.320 --> 13:36.680]  созвелось, куда оно уже бы травянули, вот эта линейная
[13:36.680 --> 13:38.960]  функция, градиент равен просто константному вектору,
[13:38.960 --> 13:42.920]  от которой это к сане зависит, а то зависит от лямбда-имю,
[13:42.920 --> 13:45.760]  наполняющая не жесткость и допустимость двойственной
[13:45.760 --> 13:46.760]  задачи.
[13:46.760 --> 13:51.560]  Личный вопрос, возможно, там отпечатка, да, там
[13:52.560 --> 14:02.320]  Да, значит, вопрос, который возникает, допустим, мы решили
[14:02.320 --> 14:08.160]  вот эту задачу, как нам по найденной лямбда со звездочкой
[14:08.160 --> 14:15.080]  найти х со звездочкой, имея под рукой условия оптимальности,
[14:15.080 --> 14:32.840]  да, какие будут идеи, так, ну мы за звездочками найдем
[14:32.840 --> 14:38.640]  отсюда, да, это хорошо, дальше что делаем, есть четвертое
[14:38.640 --> 14:52.440]  что она нам говорит?
[14:52.440 --> 14:55.840]  Ну давайте, заканчивайте мысли, вы все правильно говорите,
[14:55.840 --> 14:56.840]  не стесняйтесь.
[14:56.840 --> 15:04.720]  Четвертое условие, произведение мю на х равно нулю, вы получили
[15:04.720 --> 15:08.480]  мю, там какие-то чиселки стоят.
[15:08.480 --> 15:18.640]  Сейчас мы живем в мире, где у нас есть только лямбда
[15:18.640 --> 15:23.200]  со звездочкой и мю со звездочкой, нам надо х со звездочка получить
[15:23.200 --> 15:31.480]  каким-то образом, так, прекрасно, мю не 0, х 0, значит, какая-то
[15:31.480 --> 15:35.400]  часть х мы сразу понимаем, как к чему равна, вопрос,
[15:35.400 --> 15:40.920]  как понять, чему равна оставшаяся, да, вы правы, первая нам
[15:40.920 --> 15:45.640]  помогает, и тут как бы пока что неочевидный факт, который
[15:45.640 --> 15:47.800]  будет доказан, я надеюсь, в свое время, примерно через
[15:47.800 --> 15:51.840]  месяц, может быть через полтора, вот о том, что вот
[15:51.840 --> 15:55.840]  эта штука, это что на самом деле такое, это мы берем
[15:55.840 --> 15:58.880]  столбцы матрицы А и взвешиваем их с коэффициентами из
[15:58.880 --> 16:04.440]  вектора Х, все помнят эту интерпретацию, умножение
[16:04.440 --> 16:09.520]  матрицы на вектор, я надеюсь, прекрасно, матрица в нашей
[16:09.520 --> 16:13.640]  задаче исходной, она чаще всего типа длинная, то есть
[16:13.640 --> 16:17.880]  там столбцов больше, чем строк, соответственно,
[16:17.880 --> 16:23.360]  вот это вот операция выпиливания, которая, в относении к тому,
[16:23.360 --> 16:26.120]  что мы часть х за нуляем, это значит, что мы часть столбцов
[16:26.160 --> 16:29.240]  матрица выкидываем из этой линейной комбинации, ну
[16:29.240 --> 16:31.920]  на ноль умножаются, они как бы не участвуют в процедуре
[16:31.920 --> 16:35.520]  получения вектора В, так вот, то, что у нас останется
[16:35.520 --> 16:39.240]  в некотором регулярном случае, сейчас пока не буду
[16:39.240 --> 16:42.960]  вдаваться подробностей, что это значит, вот, будет,
[16:42.960 --> 16:46.720]  то, что у нас останется, будет содержать невырожденную
[16:46.720 --> 16:51.000]  под матрицу матрица, то есть у нас останется ровно
[16:51.000 --> 16:56.640]  м не нулевых элементов, и в результате, ну и у матрицы
[16:56.640 --> 17:00.520]  а м строк, и то, что мы получим под матрицу из некоторых
[17:00.520 --> 17:04.040]  м столбцов и м строк, будет невырожденным, которое,
[17:04.040 --> 17:07.040]  что позволит нам, собственно, решить получившуюся квадратную
[17:07.040 --> 17:13.760]  систему, получить соответствующие сиксы. Понятно ли тонкое
[17:13.760 --> 17:18.840]  место в этом выводе, и как его, как оно используется
[17:18.840 --> 17:31.240]  в общем. Так, по диаметрии, кто все понял? Так, окей,
[17:31.240 --> 17:33.800]  ну вот почему она будет квадратной, будет показана
[17:33.800 --> 17:36.960]  и рассказана на лекции определения программирования,
[17:36.960 --> 17:41.320]  то есть пока, пока верим, вот, и места, в которых она
[17:41.320 --> 17:43.880]  будет не совсем квадратной, мы их тоже обговорим в соответствующий
[17:43.880 --> 17:51.400]  момент. Вот, значит, утверждение такое, что если допустимое
[17:51.400 --> 17:54.920]  множество не пусто, то будет выполнена сильная двойственность,
[17:54.920 --> 17:59.320]  вот, и оно, это утверждение следует напрямую из того,
[17:59.320 --> 18:02.600]  о чем мы говорили про условия слейтера, поскольку мы,
[18:02.600 --> 18:05.360]  вот здесь вот у нас, давайте посмотрим на допустимое
[18:05.360 --> 18:08.680]  множество, это что такое, это вот по сути подпространство,
[18:08.680 --> 18:11.280]  если матрица длинная, вот, и в этом подпространстве
[18:11.280 --> 18:13.680]  мы берем только те иксы, которые больше либо равны
[18:13.680 --> 18:18.560]  нуля, вот, ну поскольку тут, если мы, то есть если оно
[18:18.560 --> 18:26.280]  не пусто, то мы можем, ну, как бы, и это подпространство,
[18:26.280 --> 18:30.160]  которое мы можем бесконечно в обе стороны двигать, вот,
[18:30.160 --> 18:35.800]  то коэффициент больше нуля точно найдется, поскольку
[18:35.800 --> 18:37.840]  мы можем там почти любое число получить в этом
[18:37.840 --> 18:43.360]  подпространстве, вот, поэтому, значит, для исследения
[18:43.360 --> 18:46.400]  программирования проблем с необходимостью проверять
[18:46.400 --> 18:48.840]  условия слейтера нет, потому что по виду этого множества
[18:48.840 --> 18:54.120]  просто из явного вида следует, что условие, что найдется
[18:54.120 --> 19:01.680]  икской с строго положительными весами будет работать, вот,
[19:01.680 --> 19:06.180]  поэтому возникает вопрос, что будет, если допусти
[19:06.180 --> 19:10.060]  множество пусто, вот, как это диагностируется, ну,
[19:10.060 --> 19:11.900]  вот это диагностируется с помощью следующего утверждения
[19:11.900 --> 19:16.220]  о том, что если допустима множество пусто, то двойственная
[19:16.220 --> 19:19.540]  задача окажется неограниченной, вот, то есть если вот в исходной
[19:19.540 --> 19:22.580]  задачи у вас получилось, что таких иксов вообще
[19:22.580 --> 19:28.180]  нет, не то что он там один, который только ноль, но
[19:28.180 --> 19:33.340]  вообще их нет, то вот в этой задаче, в которой я так
[19:33.340 --> 19:38.260]  быстро пролистал, вы получите неограниченное, то есть можно
[19:38.260 --> 19:40.620]  выбирать лямбду так, что вот эта штука цельевая
[19:40.620 --> 19:44.940]  функция будет неограничено уменьшаться, и сейчас попытаемся
[19:44.940 --> 19:50.860]  понять, как так получается. Тут нам на помощь придет
[19:50.860 --> 19:53.100]  лемма-фаркаша, о которой мы говорили некоторое время
[19:53.100 --> 19:56.500]  назад. Наше допустимое множество исходной задачи имеет такой
[19:56.500 --> 20:03.460]  вид. Значит, если оно пусто, значит у нас по лемме-фаркаше
[20:03.460 --> 20:06.820]  существует вектор, такой, что скалярное произведение
[20:06.820 --> 20:11.140]  с правой части отрицательно, а результат умножения слева
[20:11.140 --> 20:13.980]  на матрицу А по компоненту на больше ли браве нуля.
[20:13.980 --> 20:21.980]  Все ли помнят лемма-фаркаш? В общем, она тут приведена.
[20:21.980 --> 20:26.480]  Это вот протеорема отделимости, когда мы строили, ну мы говорили
[20:26.560 --> 20:29.480]  что у нас есть вектор В, и он либо лежит в конусе
[20:29.480 --> 20:32.360]  натянутой на столбце матрицы, а либо не лежит. Поскольку
[20:32.360 --> 20:34.640]  конус натянутый на столбце матрицы является выпуклым
[20:34.640 --> 20:39.600]  и замкнутым, то факт не принадлежности этому конусу
[20:39.600 --> 20:41.920]  вылечает за собой существование разделяющей дверь плоскости.
[20:41.920 --> 20:45.880]  Я надеюсь, что представить себе картинку довольно
[20:45.880 --> 20:50.440]  несложно или сложно. Надо ли рисовать картинку
[20:50.440 --> 20:56.200]  про лемма-фаркаш еще раз? Так, в понятном обосновании.
[20:56.200 --> 21:02.560]  Супер. Значит, нашли такой вектор P. Ура. Чедвоздная
[21:02.560 --> 21:06.040]  сдача имеет такой вид, только что я ее показывал.
[21:06.040 --> 21:09.720]  Пусть у нас лямда с крышкой этот это умножить на P где-то
[21:09.720 --> 21:17.520]  больше нуля. Тогда если мы подставим лямду с крышкой
[21:17.520 --> 21:21.560]  в целевую функцию, получим вот такое выражение. Поскольку
[21:21.560 --> 21:25.680]  P транспонировано на B меньше нуля, то при достаточно
[21:25.680 --> 21:27.920]  большом тете получим стремление к минимуму бесконечности.
[21:27.920 --> 21:37.960]  Вместе с тем, поскольку P транспонировано на A больше
[21:37.960 --> 21:42.160]  нуля, то увеличение P ведет к тому, что вот эта штука
[21:42.160 --> 21:46.280]  будет достаточно большой, чтобы получившаяся лямда
[21:46.280 --> 21:51.320]  с крышкой стала допустима. Поэтому с одной стороны,
[21:51.320 --> 22:00.440]  увеличивая T, вы все сильнее становитесь в нужном полупространстве
[22:00.440 --> 22:02.720]  и вместе с тем целевая функция становится бесконечно маленькой.
[22:02.720 --> 22:17.080]  Да, получили неограничность. Понятно ли, что произошло?
[22:17.080 --> 22:20.680]  Если непонятно, то спросите.
[22:29.800 --> 22:33.520]  Поднимите руки, кто разобрался.
[22:38.120 --> 22:44.320]  Вы не разобрались? Нужно, давайте, конечно.
[22:44.560 --> 23:04.560]  Нет, нет, нет, смотрите за стрелочкой. Это больше нуля, поэтому вы как бы это
[23:04.560 --> 23:08.200]  увеличиваете. Чтобы векторица не было, какие-то отрицательные числа, вы все
[23:08.200 --> 23:13.400]  равно без качества уходите, и это будет выполняться.
[23:25.440 --> 23:32.560]  Так, смотрите, я понял вопрос про граничные всякие случаи, да? Сейчас еще не
[23:32.600 --> 23:40.640]  придумаю, секунду. А, то есть, если паттерн спонированное на равну нулю, что будет?
[23:40.640 --> 23:53.280]  То есть, если это ноль, это ноль, а, ну тут, кажется, какие-то проблемы с рангом матрицы
[23:53.280 --> 24:06.000]  получаются, да? Ну, матрица полного ранга, поэтому не существует ни нулевого вектора,
[24:06.000 --> 24:13.360]  перемножение на который дает нулевой вектор. Вот, наверное, так. Можно это как-то объяснить.
[24:13.720 --> 24:17.560]  А все поняли вопрос? Не беспокойствуй вашего коллеги.
[24:17.560 --> 24:29.760]  Хотя, сейчас, вот да, я тоже сейчас об этом подумал, что частично нули. Так, что делать
[24:29.760 --> 24:51.720]  с частичными нулями? Часть ноль, часть не ноль. Что сломается? Частично ноль. Так, хорошо,
[24:51.720 --> 24:57.480]  давайте я сейчас не хочу сильно долго думать, я тут напишу некоторые пояснения по поводу частичных
[24:57.480 --> 25:02.080]  нулей. Вот, так, восьмой слайд. Вот, так, это значит, мы уже нашли одну опечатку и
[25:02.080 --> 25:10.360]  одно недостаточно полное обоснование. Хорошо, лекция проходит не зря. Так, окей, да, за вычетом
[25:10.360 --> 25:16.760]  вот этого небольшого нюанса, в принципе, вроде как все работает. Теперь переходим, то есть,
[25:16.760 --> 25:22.040]  вы разобрали только, что мы с вами двойство для линейного программирования, которое, если вы
[25:22.040 --> 25:29.080]  помните, было простейшим примером конической оптимизации, когда у нас их лежал в конусе
[25:29.080 --> 25:34.160]  положительный актант под названием. Тут не зря, это красненьким выделено, хотя, наверное,
[25:34.160 --> 25:42.480]  на экране. Теперь, собственно, перейдем от простейшего нетресетного актанта к произвольному
[25:42.480 --> 25:51.840]  коническому множеству, которое, типа, обобщенным неравенцем представляется. Так, вопрос
[25:51.840 --> 26:00.240]  все ли помню, что это за значок? Перед тем, как идти дальше. Так, вижу, что не все помнят. Этот
[26:00.240 --> 26:09.840]  значок означает, что минус hgt от x лежит в конусе. То есть, у нас была там матрица положительно
[26:09.840 --> 26:17.880]  полуопределена, там норма вторая, было некоторое соотношение, это мы все записывали как n+, то
[26:17.880 --> 26:24.640]  либо матрица лежит в конусе положительных полуопределенных матриц, либо вектор из n
[26:24.640 --> 26:31.400]  плюс одной компонента лежит в соответствующем конусе, порожденной нормой нектом. Удалось ли
[26:31.400 --> 26:43.080]  вспомнить? Вот эта вот величина минус h, ну, тут вот это меньше либо равно k, значит,
[26:43.080 --> 26:48.280]  при умножении на минус единицу у нас получается, что минус hgt от x больше либо равно 0 в смысле
[26:48.280 --> 26:56.920]  конуса, это значит, что минус hgt лежит в конусе. В конусе k. Написано k, это конус некоторый,
[26:56.920 --> 27:04.160]  ну, там, правильный, там замкнутый, содержит 0, ну, 0 всегда содержит, неправильно говорю,
[27:04.160 --> 27:11.680]  замкнутый, выпуклый, и ему не принадлежит прямая, чтобы там были выполнены все эти соотношения
[27:12.040 --> 27:16.840]  порядка, про которые вам, наверное, много раз уже рассказывали, там, транзитивность,
[27:16.840 --> 27:20.240]  рефлексивность, вся эта история, она следует напрямую из геометрии выпуклого стихонуса.
[27:20.240 --> 27:27.480]  Запишем Lagrangian для этой задачи, может быть, не совсем привычная форма, но я надеюсь, вы сейчас
[27:27.480 --> 27:32.840]  немного присмотритесь и увидите, что это ровно тот же самый Lagrangian, который был ранее, только
[27:32.840 --> 27:39.880]  теперь g и h это типа вектор функций, которые типа g жирный это вектор из gитых, h жирный это вектор из
[27:39.880 --> 27:45.680]  h житых. А так, суммирование с коэффициентом лямбда и tµ, это просто, конечно, произведение записано.
[27:45.680 --> 27:52.480]  Ну, и отчуда двойственная задача будет записываться точно так же, только теперь у нас
[27:52.480 --> 28:03.760]  μ станет необходимым, чтобы он принадлежал спряженному конусу. Понятно ли почему?
[28:03.760 --> 28:14.880]  Смотрите, что мы хотели, чтобы у нас, когда мы выводили двойственные задачи, у нас была
[28:14.880 --> 28:20.760]  некоторая надобность в том, чтобы, ну, то есть вот это все еще по-прежнему, независимо от того,
[28:20.760 --> 28:29.560]  что происходит вот здесь, вот является, чем является выпуклой функции, ну, линейной функции при
[28:29.560 --> 28:36.760]  фиксированном иксе. Тут ничего не поменялось. Но мы хотели, чтобы, когда до этого обсуждали,
[28:36.760 --> 28:43.440]  что у нас ажитое было без вот этого конуса просто меньше либо равно нуля. И когда мы здесь записывали,
[28:43.440 --> 28:49.760]  мы требовали, чтобы μ было больше либо равно нуля. Помните, да, что мы там оценку снизу получали,
[28:49.760 --> 28:55.940]  потом и максимизировали. И это нас приводило к такому виду задачи. Только здесь был не ка со
[28:55.940 --> 29:05.540]  звездочкой, был не ка со звездочкой, а просто было больше нуля без ка со звездочкой. Спойлер просто
[29:05.540 --> 29:11.900]  потому, что сопряженный к положительному актанту это он сам. Тут, в общем-то, все абсолютно согласуется
[29:11.900 --> 29:18.180]  с тем частным случаем, который был сначала разобран. Что нам надо теперь? Теперь нам надо, чтобы вот
[29:18.180 --> 29:32.940]  эта штука была какой? Чтобы она была меньше либо равно нуля. Чтобы все те предположения и
[29:32.940 --> 29:36.340]  выводы, которые мы делали ранее, выполнялись, надо потребовать, чтобы вот эта штука была меньше либо
[29:36.340 --> 29:45.540]  равно нуля. Но это означает, что у нас же для аж мы знаем, что минус аж лежит в конусе. А раз у нас
[29:45.540 --> 29:50.780]  минус аж лежит в конусе, то все те вектора, для которых это отрицательное, это просто элементы
[29:50.780 --> 30:00.540]  сопряженного конуса. Так, хорошо. Все ли помнишь, что такое сопряженный конус? Зачем с этого? Нет.
[30:00.540 --> 30:06.020]  Окей, давайте я напишу тогда. Да, наверное, здесь нужно добавить слайд про напоминание основных
[30:06.020 --> 30:14.740]  определений. Наверное, тяжеловато так сходу напоминать. Смотрите, у нас был конус. Так,
[30:14.740 --> 30:22.820]  тут надо какой-то цвет. Был конус, это конус. У нас был сопряженный конус. Это набор таких
[30:22.820 --> 30:33.140]  у, что скаллярный произведение у на х было больше либо равно для любого вектора из конуса. Вот.
[30:33.140 --> 30:42.020]  Ну и, соответственно, поскольку вот здесь вот у нас есть некоторая история с тем, что у нас
[30:42.020 --> 30:49.340]  минус аж жито лежит в конусе, и нам нужно, чтобы это было меньше нуля в скаллярном произведении,
[30:49.340 --> 30:54.740]  а не больше нуля, как было. Ну, то есть вот это вот больше нуля, оно становится меньше нуля,
[30:54.740 --> 31:03.740]  потому что у нас минус аж жито лежит в конусе. Поэтому соответствующий множественный лагранж
[31:03.740 --> 31:12.940]  должны лежать в сопряженном конусе. Удалось ли разобраться в нагромождении формул,
[31:12.940 --> 31:25.260]  который приведен на слайде? Так, поднимите руки, кто понял? На что?
[31:25.260 --> 31:51.940]  Вот эта штука, она... Сейчас, я, наверное, понял вопрос. Сейчас скажу секунду.
[31:51.940 --> 32:04.420]  Ну, тут типа для любого х, который лежит, допустим, в области определения, минус аж жито лежит в конусе,
[32:04.420 --> 32:15.380]  вот так. Ну, типа все образы лежат в конусе. Поэтому для всех образов надо потребовать,
[32:15.380 --> 32:20.260]  иначе когда мы будем считать инфимум по х, мы не сможем сказать, что это любые х.
[32:20.260 --> 32:40.060]  Смотрите, да, это вопрос решается следующим образом. Просто вот эта штука, то есть, видите,
[32:40.060 --> 32:47.420]  здесь, я понял, что здесь не хватает. Тут не хватает индекса j, наверное, для полной картины. То есть,
[32:47.420 --> 32:54.140]  типа у вас для каждого нерайса может быть свой конус. Понятно, да? Вот, и, соответственно,
[32:54.140 --> 33:00.180]  вы вот этот кусочек, который хотите, вы его в качестве пересечения можете взять. В пересечении
[33:00.180 --> 33:04.500]  конусов, и у вас все получится тогда. Просто это сопряжение будет считаться чуть более хитрой,
[33:04.500 --> 33:12.300]  чем если бы это был декартом произведения. Ну, на самом деле, может быть, тут индекс j-то и не
[33:12.300 --> 33:17.020]  нужен, а нужно просто сказать, что этот конус может быть декартом произведения конусов,
[33:17.020 --> 33:22.100]  часть из которых будет константа какая-нибудь. Ну, типа, единицы, все остальное мы спроецируемся на
[33:22.100 --> 33:28.660]  подпространство. И тем самым вот этот кусочек, который нам нужен, мы его как бы вырезаем за счет
[33:28.660 --> 33:34.780]  того, что смотрим просто срез. Типа конус вот такой, а мы вот так вот его режем и получаем то,
[33:34.780 --> 33:49.900]  что нам нужно. Вот, да. Так, еще какие-нибудь вопросы есть? Зачем все это надо? Да, ну, в общем,
[33:49.900 --> 33:54.060]  сейчас я скажу, сейчас сначала приведу к ККТ, потом скажу, зачем все это надо. ККТ записывается
[33:54.060 --> 34:00.020]  точно так же, только теперь все, что у нас касалось конуса, меняется на его сопряжение. Все
[34:00.020 --> 34:07.220]  остальное не меняется. Вот. Этого легко достаточно показать, просто взяв выкладские не сильно
[34:07.220 --> 34:12.780]  многочисленные с прошлой лекции и увидеть, что разница между ними только в том, только вот
[34:12.780 --> 34:17.380]  здесь вот. А все остальное, что у нас было, оно как было, так и станет. Там никакого существенного
[34:17.380 --> 34:21.620]  использования того, что у нас неравенство, это неравенство в обычном привычном нам смысле,
[34:21.620 --> 34:27.540]  просто мы чисто с нулем сравниваем, нигде не использовалось на самом деле. Поэтому замена
[34:27.540 --> 34:34.460]  обычного неравенства на обобщенное приводит только к появлению вот этой вот модификации. А все
[34:34.460 --> 34:41.980]  остальное будет работать точно так же. Вот. В общем, поэтому все утверждения, которые мы делали для
[34:41.980 --> 34:52.820]  все утверждения, которые мы делали для произвольных неравенств, будут работать и для обобщенных там
[34:52.820 --> 34:57.340]  парусовисейтеров и все такое, только вот да, усовисейтер будет вот так вот выглядеть теперь. То есть
[34:57.340 --> 35:03.580]  нам будет важно, чтобы было строго, то есть чтобы он лежал именно внутри конуса, короче говоря,
[35:03.580 --> 35:14.620]  минус FIT от X с крышкой. Вот. Так. Ну да, 20 минут, нормально. Теперь давайте посмотрим, собственно,
[35:14.620 --> 35:19.780]  на то, как будет выглядеть двойственная задача для задачи оптимизации в конической форме.
[35:19.780 --> 35:24.700]  Напоминаю, как это выглядит. У нас линейная целевая функция, линейное ограничение равенства и
[35:24.700 --> 35:35.860]  ограничение на X, его принадлежность к конусу. Минус X, больше нуля, да, X лежит в конусе. Все
[35:35.860 --> 35:41.460]  то же самое, что было в линейном программировании, только теперь кассоведочка появилась. И сейчас,
[35:41.500 --> 35:52.700]  наверное, как раз-таки будет правильным показать, почему это так. Я думаю, за 5 минут мы его вырву.
[35:52.700 --> 36:09.740]  То есть что было, ой, была вот такая задача. Правильно хоть написал? Да, вроде.
[36:09.740 --> 36:16.820]  Что происходит? Строим для него лагранжаан. C транспонированный X плюс лямбда транспонированный,
[36:16.820 --> 36:29.180]  AX минус B. И наша двойственная функция же от лямбда. Ой, это инфимум. Ни у кого пока вопросов нет.
[36:29.180 --> 36:50.060]  Че надо еще добавить? Вопрос на понимание. Да, надо добавить, что здесь мы берем инфим только
[36:50.060 --> 36:58.220]  по тем иксам, которые уже лежат в нашем конусе, поскольку мы их не включили в
[36:58.300 --> 37:08.540]  лагранжаан. Понятно, откуда это взялось? То есть ровно то, о чем я говорил несколько слайдов назад,
[37:08.540 --> 37:17.700]  когда про разные формы задачи мы обсуждали. Прекрасно. Смотрим дальше. Это что такое?
[37:17.700 --> 37:25.700]  Во-первых, тут есть часть, которая не зависит от икса. Мы ее сразу давайте выкинем. И у нас останется C плюс,
[37:25.700 --> 37:52.900]  так вроде. Добавляйся. Всем понятно, что произошло? Просто вынесли внимание и смотрим,
[37:52.900 --> 37:59.140]  что получается. Получается, что наша двойственная функция – это инфимум такого замечательного
[37:59.140 --> 38:05.380]  скалярного произведения, которое есть линейная функция по иксу. Что мы знаем про это инфимум?
[38:22.900 --> 38:47.340]  Громче. Не стесняйтесь. Не совсем. Ну, почти не совсем так. Достаточно аккуратно. Смотрите,
[38:47.420 --> 38:57.180]  мы видим, что нам нужно, чтобы этот инфимум… его как-то получить надо. Что получить инфимум,
[38:57.180 --> 39:14.940]  что надо сделать? Как искать инфимум? Тревожная тишина.
[39:14.940 --> 39:32.820]  Любую непонятную ситуацию дифференцируй. Смотрите, что мы знаем. Мы уже знаем, что у нас,
[39:32.820 --> 39:43.140]  если мы смотрим на такие выражения, что-то видит скалярное произведение нашего элемента с каким-то
[39:43.140 --> 39:46.740]  вектором, то мы знаем, что есть вот такая вот оценка, что эта штука всегда больше левра нуля,
[39:46.740 --> 40:04.300]  если наш вектор будет из конуса лежать. Так? Почему рано инфимум? Ну да, хочется сказать,
[40:04.300 --> 40:11.460]  конечно, что мы вот эту вот инфимум, говорим, что это ноль при условии, что это лежит в нашем
[40:11.460 --> 40:22.620]  сопряженном конусе. Потому что для всех иксов из этого конуса это выражение больше левра нуля.
[40:22.620 --> 40:33.820]  Понятно, я надеюсь. Теперь давайте поймём почему… да, давайте теперь сделаем перерыв и вопрос на
[40:33.820 --> 40:38.660]  перерыв понять, почему, если эта штука не лежит в конусе, то тут будут какие-то проблемы.
[40:38.660 --> 40:53.220]  Вопрос понятен? Да, нет. Потерялись на предыдущем шаге, да?
[40:53.220 --> 41:16.660]  Да, то, что сломается. Да, вот, собственно, это действительно проблема, на которую надо подумать.
[41:16.660 --> 41:22.060]  Так, всё, пять минут, да, пять минут вроде. Да, короче, в 32 минуты продолжим. Работает,
[41:22.060 --> 41:50.300]  давайте расскажите всем. Ага. Да, то есть если эта штука не лежит,
[41:50.300 --> 41:57.420]  то внукс получается неограничен, и мы получаем минус бесконечности. Давайте ещё раз. Смотрите,
[41:57.420 --> 42:03.900]  если эта штука не в сопряженном конусе, это значит, что существует икс из конуса,
[42:03.900 --> 42:09.820]  на котором это скалярное произведение отрицательно. Это значит, что умножив этот самый икс на положительное
[42:09.820 --> 42:16.580]  число, мы не выводим его из конуса, но делаем значение бесконечно маленьким. Бесконечно маленьким,
[42:16.580 --> 42:22.340]  в смысле, минус бесконечности, а не веським к нулю. Вот. Получаем минус бесконечности,
[42:22.340 --> 42:35.940]  получаем неограничность. Не понял и громче. Это никак не мешает, поскольку всё то же самое можно
[42:35.940 --> 42:42.740]  проделать для, ну, короче говоря, в результате у вас тоже получится конус, и для того же самого
[42:43.140 --> 42:51.220]  причине конусов конус, в общем. Да, так что тут все эти оговорки, они технические в каком-то смысле.
[42:51.220 --> 43:08.380]  Так, вопросы, предложения, замечания. Всё понятно? Таким образом, получаем вот такую вот, а ну да,
[43:08.380 --> 43:18.060]  я же не договорил. Значит, получаем, что у нас двойственная функция, минус лямба транспонированная
[43:18.060 --> 43:31.660]  b, будьте здоровы, мы её максимизируем, при условии каком, что c плюс a t лямбда лежит в сопряженном конусе.
[43:31.660 --> 43:46.860]  Окей? Но, ну мы можем как бы поменять максим на минимум, и возможно тут еще как бы некоторая
[43:46.860 --> 43:51.780]  замена переменных типа лямбда на минус лямбда, но по-моему это вот тут не, а нет, это ровно тут,
[43:51.780 --> 44:01.740]  это и тут и сделано, да? Что, ну смотрите, это что значит? c плюс a t лямбда больше либо равна нулю,
[44:01.740 --> 44:10.340]  в смысле ка со звездочкой, ну и c больше либо равна, в смысле ка со звездочкой, чем a t лямбда. Вот,
[44:10.340 --> 44:25.900]  ну и что получилось то, что написано на слайде? a t лямбда меньше либо равно. Так, там какая-то
[44:25.900 --> 44:34.420]  проблема с минусом возникла, надо будет перепроверить. Ладно, в общем, здесь с точностью до, тут с точностью
[44:34.420 --> 44:41.060]  до минуса или вот здесь вот какая-то проблема со знаком. Вот, в общем, идея в том, что зная ка и то,
[44:41.060 --> 44:48.620]  что он, если он самый сопряженный, то вот это ка со звездочкой вы знаете сразу. То есть, что произошло?
[44:48.620 --> 44:55.300]  Вы, допустим, у вас есть функция, которая строит, то есть у вас есть функция, которая принимает на
[44:55.300 --> 45:02.380]  вход c, a, b и конус в каком-то виде. Вот, имея эту информацию, вы автоматически можете построить двойственную
[45:02.380 --> 45:08.460]  задачу, просто сказав, что целевая функция линейная, тут b стоит, тут стоит a транспонированная, тут c,
[45:08.460 --> 45:15.420]  тут ка со звездочкой. То есть, вы ничего не делаете, вы просто привели задачу к такому виду и сказали,
[45:15.420 --> 45:20.580]  ага, ну а теперь я просто подставляю и получаю правильный ответ. То есть, уже никакой там
[45:20.580 --> 45:28.460]  инфим благранжа, но все вот это сидит внутри ка со звездочкой, ничего не надо делать. Понятно ли
[45:28.460 --> 45:42.020]  сила этого аппарата? Или не очень? То есть, зная таблицу конус его сопряженный, вы сразу получаете
[45:42.020 --> 45:52.020]  способ записи двойственной задачи. Окей, значит, вот именно для того, чтобы вот это все можно было
[45:52.020 --> 46:01.380]  настолько автоматически записывать, мы и изучали конусы сопряженные, конусы и двойственные,
[46:01.380 --> 46:07.540]  все эти постановки. Потому что в дальнейшем мы будем решать задачи понятное дело как. Мы будем
[46:07.540 --> 46:11.580]  записывать, у нас будет прямая задача, у нас будет двойственная задача, мы будем обновлять
[46:11.580 --> 46:16.620]  одновременно и прямые перемены, и двойственные. И таким образом контролирует зазор двойственности,
[46:16.620 --> 46:21.220]  который здесь как бы автоматически вы можете посчитать, поскольку у вас все функции линейные,
[46:21.220 --> 46:28.380]  вы знаете, как они выглядят. Вы можете контролировать сходимость вашего метода. Вот такое чудо природы.
[46:28.380 --> 46:50.500]  Понятно ли перспектива такого подхода? Да, нет. Непонятно. Давайте еще раз. Вы
[46:50.500 --> 46:54.780]  сейчас я подумаю, может быть стоит лучше перенести это обсуждение на следующий раз,
[46:54.780 --> 47:01.780]  когда мы более конкретно про это поговорим. То есть смотрите, когда вы решаете исходную задачу,
[47:01.780 --> 47:08.180]  вы можете ее как-то решать. Но для того, чтобы проверить, что ваш метод, который ее решает,
[47:08.180 --> 47:14.500]  который, ну то есть метод, который решает, это такая некоторая штука, функция, которая генерирует
[47:14.500 --> 47:20.300]  по следованию иксов, которые сходятся к решению. То есть x1, x2, x3, x5, x10, x100. И мы хотим,
[47:20.300 --> 47:27.500]  чтобы в конце мы пришли как со звездочкой, которая будет решением. Вот. Чтобы проверить,
[47:27.500 --> 47:33.420]  что ваш текущий x близок, далек от решения, где вы сейчас находитесь, надо вам еще что-то
[47:33.420 --> 47:39.220]  итерироваться или уже нет, надо, ну стандартный и самый качественный функционал сходимости,
[47:39.220 --> 47:46.580]  это взор двойственности. Это понятно. Мы это проходили прошлого, или надо напомнить. Было,
[47:46.580 --> 47:55.540]  отлично. Он всегда больше либо ровне нуля. Можно. Это разница между вот этим значением и вот этим
[47:55.540 --> 48:05.260]  значением. Вот. Соответственно, если вы имеете одновременно и прямую задачу, и двойственную,
[48:05.260 --> 48:12.660]  то вы можете итерировать здесь x, чтобы это стало поменьше, а здесь итерировать лямду,
[48:12.660 --> 48:21.260]  чтобы это стало побольше. И измеряя разницу между получаемыми значениями целевых функций,
[48:21.260 --> 48:31.380]  получить x и лямда такие, что вот это отличается от вот этого на 10-6, условно говоря, и это будет
[48:31.380 --> 48:38.460]  индикатором того, что вы пришли к решению прямой двойственной. Вот. Но в чем магия-то? В том,
[48:38.460 --> 48:44.740]  что вот эту задачу вы не записываете руками, вы передаете солверу вот эти параметры,
[48:44.740 --> 48:52.860]  конус C, A, B, и он внутри себя автоматически все это делает. Поскольку вот для этой формы эта
[48:52.860 --> 48:59.100]  форма автоматически строится. То есть не надо ничего делать, по сути дела. Все за вас сделано.
[48:59.100 --> 49:08.460]  Понятно ли почему теперь и что происходит? Окей. В следующий раз про то, как именно это
[49:08.460 --> 49:16.740]  происходит и что делать, когда у вас вот такая вот, когда у вас не вот коническая форма, а вот
[49:16.740 --> 49:21.220]  такая вот, и тут просто допустим стоит меньше либо равно нуля, тут линейная, тут выпукла,
[49:21.220 --> 49:25.580]  тут выпукла. Как произвольно выпуклую задачу сводить коническую, в следующий раз будет подробно
[49:25.580 --> 49:30.700]  рассказано. Потому что пока это некоторая дыра между нашим предложением, то есть мы
[49:30.700 --> 49:34.100]  обсудили, что происходит с выпуклым задачей, мы обсудили, что происходит с коническими,
[49:34.100 --> 49:46.620]  а как по-одно привести к другому пока непонятно. Вот. Так, да, теперь, ой, ну вот, мое время пропало,
[49:46.620 --> 49:54.100]  один слов. Теперь посмотрим, что происходит, как строить двойственную задачу для задачи
[49:54.100 --> 50:01.060]  по определенной оптимизации. Исходная задача у нас была в таком виде, и мы обсуждали, что это то
[50:01.060 --> 50:07.100]  же самое, что и линейное программирование, только теперь у нас вместо строк с транспонированной и
[50:07.100 --> 50:14.900]  аитых полупоявились матрицы. А так, следы, это все еще скалярные произведения. Только не для
[50:14.900 --> 50:26.060]  векторов, а для матриц. Помните, было такое? Двойственная задача ставится, внимание,
[50:26.060 --> 50:30.820]  абсолютно так же, как и для, то есть смотрите, что происходит, здесь линейная функция,
[50:30.820 --> 50:43.780]  тут тоже линейная функция, и это наш конус. Вот что мы имеем, и теперь мы можем явно написать
[50:43.780 --> 50:50.060]  аналогичную формулу только с учетом специфики того, что это матрица, и что тут у нас следы
[50:50.060 --> 51:07.860]  фигурируют. Давайте поймем, как это делается. Так, сейчас я перепишу исходную формулу. Ой, так.
[51:13.780 --> 51:31.820]  Так, и тут, в общем-то, все уже написано, да, плюс-минус. Так.
[51:43.780 --> 51:53.900]  Так вроде получше, да? Так, ну давайте смотреть, что у нас имеется. То есть первое, что мы замечаем,
[51:53.900 --> 52:02.580]  что вот это условие то же самое, что у нас х лежит в Sn+. Конус самосопряженный, поэтому то,
[52:02.580 --> 52:09.380]  что у нас будет там какое-то неравенство, у нас все еще будет Sn плюс сопряжение, это все еще
[52:09.380 --> 52:17.820]  Sn плюс, это мы получали. Ну вот. Теперь давайте смотреть, что нам надо. Нам надо, чтобы целевая
[52:17.820 --> 52:24.540]  функция была лямдо-транспонированная на B. Так, что у нас тут B? У нас была исходная задача вот
[52:24.540 --> 52:29.580]  такого вида, B это был вектор, тут вектор B тоже присутствует, просто он записан по-компонентно.
[52:29.580 --> 52:37.300]  B это наши элементы вектора B. То есть наша целевая функция в двойственной задаче, это лямдо-транспонированная
[52:37.300 --> 52:48.900]  B, а тут все хорошо. Двойственная функция. Интереснее, что будет происходить в плане ограничений.
[52:48.900 --> 53:01.540]  В ограничениях у нас фигурирует выражение вида C плюс транспонированная лямдо. Что это такое?
[53:01.540 --> 53:06.060]  Значит, C это тот вектор, который фигурировал целевой функцией исходной задачи.
[53:06.060 --> 53:18.420]  Вот. В нашем случае это матрица C. Это выражение будет конвертироваться в некотором смысле. Так,
[53:18.420 --> 53:25.980]  давайте я напишу так. C плюс АТ лямдо. Тут такую стрелочку большую. Это что такое? Это C плюс. Теперь
[53:25.980 --> 53:40.620]  давайте поймем, что такое АТ лямдо. АТ лямдо это... Что это? Точнее так, не что это, а какая у этого
[53:40.620 --> 53:52.300]  интерпретация? Так, а вот если вы скажете громче, уберете руку от рта, будет совсем отлично. Еще раз,
[53:52.300 --> 54:09.860]  еще раз. Нет. Взвешенная сумма столбцов матрица транспонированная, хотя бы. Почему? Именно так.
[54:09.940 --> 54:16.980]  Меняете строки столбцы и получаете, что это взвешенная сумма строк.
[54:16.980 --> 54:35.620]  Непонятно. Давайте я распишу. Что было? Было изначально АХ равно B. Вот это выражение это сумма АИТХИТ,
[54:35.620 --> 54:38.020]  где АИТ это столбец.
[54:38.020 --> 55:07.620]  Так? Да нет. Да почему? X. Смотрите еще раз. Вот у вас матрица. Умножите на вектор.
[55:07.620 --> 55:13.620]  Вы строку умножаете на столбец. Следующую строку на столбец. Это значит, что вы каждый столбец
[55:13.620 --> 55:21.580]  умножаете на соответствующий индекс и складываете. Получаете вектор. То есть думаете об этом никак,
[55:21.580 --> 55:28.620]  что вы получаете компоненты вектора. А как вы получаете весь вектор? Все, разобрались. Все, шикарно.
[55:28.620 --> 55:36.020]  Так. Возвращаемся к нашей записи. Считат транспонированной лямбда. Это мы взвешиваем строки
[55:36.020 --> 55:51.140]  матрицы А с коэффициентами лямбда ИТ. Осознали? Шикарно. А здесь у нас были строки матрицы А умножались
[55:51.140 --> 55:59.460]  на вектор Х. Получались бы АИТы. Теперь в нашей задаче вместо строк матрицы А у нас матрицы АИТы,
[55:59.460 --> 56:10.420]  которые также умножаются скалярно на матрицу Х, которая нам неизвестна. Поэтому, чтобы получить
[56:10.420 --> 56:16.180]  аналог А транспонированная лямбда, нам нужно посчитать взвешенную сумму АИТых.
[56:16.180 --> 56:25.780]  Потому что они играют роль в строк матрица, который изначально фигурировали в исходной
[56:25.780 --> 56:49.140]  задаче. Кто осознал аналогию? Поднимите руки. А кто не осознал аналогию? Поднимите руки.
[56:49.140 --> 57:01.500]  Не очень, да? Ну смотрите, как бы еще объяснить. Когда мы записывали двойственную задачу,
[57:01.500 --> 57:08.060]  вот тут понятно, что происходит. Вот А транспонированная лямбда, это мы складываем
[57:08.060 --> 57:17.260]  строки матрицы А с весами лямбда ИТ. Это понятно. Отлично. Теперь смотрим на вот эту задачу. Вот здесь у
[57:17.260 --> 57:23.460]  нас след. Это скалярное произведение матрицы АИТы на Х. В случае исходной задачи у нас было
[57:23.460 --> 57:29.900]  скалярные произведения строки матрицы АИТы на вектор Х. Каждое такое скалярное произведение у нас
[57:29.900 --> 57:37.740]  заменилось на произведение матрицы. Поэтому теперь в исходной задаче строка матрицы АИТы
[57:37.780 --> 57:49.980]  превратилась в полноценную отдельную матрицу АИТую. Так, вот этот момент понятен или не очень?
[57:49.980 --> 57:56.500]  Это понятен. Хорошо. Теперь, когда мы смотрим на вот это вот выражение, то это линейная комбинация
[57:56.500 --> 58:04.260]  строк АИТых. Вот она. Линейная комбинация строк с теми же самыми коэффициентами лямбда.
[58:08.740 --> 58:16.820]  Лучше стало. Окей. Замечательно. Получили вот это выражение, которое теперь должно лежать в сопряженном конусе.
[58:16.820 --> 58:27.100]  Так, ну как-то так. Давайте посмотрим, что получается в исходном. Да, там вот с точностью до знака,
[58:27.100 --> 58:37.180]  как я уже сказал. Тут, кстати, более понятно, откуда что берется. То есть мы изначально должны
[58:37.180 --> 58:47.660]  были максимизировать минус лямбда транспонированная В. Сейчас, я тут что-то...
[58:47.660 --> 59:05.180]  Будто бы что-то сломалось по пути. Надо понять, что именно сломалось. Лежит в конусе, да.
[59:05.180 --> 59:25.140]  Да, в общем понятно, что произошло. Вот. При условии, что C плюс вот эта самая сумма лежит в сопряженном конусе.
[59:25.140 --> 59:35.340]  Да, могу-могу, но тут немножко надо будет повычитывать формулы на предмет лишнего минуса,
[59:35.340 --> 59:40.180]  который записался в слайдах, похоже. Или если не записался, то хотя бы написать пояснее,
[59:40.180 --> 59:53.220]  откуда это взялось. Пока это немного не соответствует друг другу. Так, больше-меньше линейная комбинация,
[59:53.220 --> 01:00:04.140]  плюс C максимум минус лямбда ТБ. Да, то есть от сюда получаем, что минус лямбда ТБ и тут вот
[01:00:04.140 --> 01:00:13.980]  замечательное ограничение. Заметьте, что исходная задача у нас была на матрицу, а двойственная задача
[01:00:13.980 --> 01:00:25.740]  стала на вектор. Видно разницу? Вот, то есть вроде как стало полегче, но ограничение типа
[01:00:25.740 --> 01:00:32.940]  неположительной определенности, положительной полуопределенности, оно все еще осталось. Вот, так.
[01:00:32.940 --> 01:00:40.300]  Да, значит, хитрый момент. Условие слейтера для этой задачи формулируется так, что теперь нам
[01:00:40.300 --> 01:00:46.860]  нужна матрица Х такая, что она будет строго положительно определена, то есть лежать именно
[01:00:46.860 --> 01:00:54.180]  внутри конуса и выполнена соответственно равенством. Важный момент, который, собственно, пока мы
[01:00:54.180 --> 01:00:59.620]  активно пользовались тем, что это примерно линейное программирование, там все то же самое,
[01:00:59.620 --> 01:01:13.780]  оказывается, что здесь такой, что называется, такого легкого, легкой процедуры решения уже не
[01:01:13.780 --> 01:01:19.340]  будет, и не всегда, если множество не пусто, то для него будет выполнен условие слейтера. Вот,
[01:01:19.340 --> 01:01:25.540]  и тут оно становится существенным. Вот, и сейчас посмотрим конкретный пример, в котором это будет
[01:01:25.540 --> 01:01:37.660]  себя ярко проявлять. Рассмотрим вот такую вот простую задачу. Она является, значит, полуопределенной
[01:01:37.660 --> 01:01:52.820]  оптимизацией, хотя кажется, что нет. Всем ли понятно, в каком виде что здесь представляется. То
[01:01:52.820 --> 01:01:58.140]  есть, вот эта задача, это есть не что иное, как вот эта задача на самом деле. Ну да,
[01:01:58.140 --> 01:02:13.500]  устоишь до знака. То есть, линейная функция здесь, и линейная комбинация матриц с коэффициентами
[01:02:13.500 --> 01:02:16.420]  положительно полуопределенна.
[01:02:22.420 --> 01:02:33.780]  Все осознали, все увидели. Отлично, давайте, что такое матрица С? Да, матрица С единица вот здесь,
[01:02:33.900 --> 01:02:41.380]  все остальные нолики. Вот, давайте сейчас распишем эту задачу. Так, сейчас я сначала продублирую, чтобы.
[01:02:41.380 --> 01:02:47.580]  Да, конечно.
[01:03:03.780 --> 01:03:19.220]  Так, тонала первых всего лишь двумерная. На плоскости можно нарисовать при желании. То есть,
[01:03:19.220 --> 01:03:34.500]  вот эта штука, это что такое? Это единица плюс x1, на что? На 0, 1, 0, все нули, большие нули,
[01:03:34.500 --> 01:03:52.740]  плюс x2, на что? 1, 0, 1, 1, 0, и тут вот так. И вот так. Давайте найдем решение. Чтобы найти решение,
[01:03:52.740 --> 01:03:59.500]  надо сначала допустимо множество оценить. Ну, как-то проанализировать. Какое здесь допустимо множество?
[01:03:59.500 --> 01:04:04.500]  Как это все на самом деле расписывается?
[01:04:17.260 --> 01:04:22.540]  Ну да, критерис или вестра, конечно. То есть, что у нас есть? У нас есть, я буду писать, x2
[01:04:22.540 --> 01:04:34.140]  плюс 1 больше либо равно 0, x1 больше либо равно 0, и самое важное, кажется, что x2 в квадрате,
[01:04:34.140 --> 01:04:43.380]  в чем больше либо равно 0, да? Ой, что-то не то. А, минус x2 в квадрате больше либо равно 0, да,
[01:04:43.380 --> 01:04:56.860]  вот так. Но отсюда уже следует, что x2 равно 0. Понятно ли, откуда я это взял? Поделитесь,
[01:04:56.860 --> 01:05:06.620]  если кто понял, откуда я это взял. Так, отлично, ура. Нашли ответ. Это 0, да? Всем понятно. Супер.
[01:05:06.620 --> 01:05:12.100]  Теперь, смотрите, важный момент. Когда это все решается на компьютере, то никто критерий
[01:05:12.100 --> 01:05:18.340]  севестра не проверяет никогда, потому что это масштаб проблем, я думаю, понятен, да? Типа,
[01:05:18.340 --> 01:05:24.660]  у вас матрица N на N, и у вас там неравенства этих нелинейных будет довольно много. Поэтому,
[01:05:24.660 --> 01:05:33.260]  когда вы вот так вот пишете, то solver автоматически приводит задачу к такому виду и строит двойственную
[01:05:33.260 --> 01:05:54.940]  для нее. Понимая, какие матрицы A где... Понимая, какие матрицы C, A и T, где тут вектор B, и для задачи
[01:05:54.940 --> 01:06:01.900]  в таком виде есть автоматический инструмент, который строит вот такую вот. Ну, то есть, как бы,
[01:06:01.900 --> 01:06:05.860]  я еще не сказал, что если вы возьмете двойственную для вот этой, вы получите вот эту. Они как бы друг
[01:06:05.860 --> 01:06:15.180]  в друга переходят. Это несложно показать достаточно. Давайте же проделаем эту манипуляцию. Тут,
[01:06:15.180 --> 01:06:19.060]  кстати, по-моему, ответ уже написан. Да, ну так и допустим 0, что написали, оптимальное значение 0.
[01:06:19.060 --> 01:06:26.100]  Да, давайте не будем подглядывать слайды и выведем, как будет выглядеть двойственная руками, чтобы
[01:06:26.100 --> 01:06:34.540]  показать, как это все записывается. Значит, нам нужно... Что? Мы ищем матрицу. Давайте я ее у обозначу.
[01:06:34.540 --> 01:06:48.540]  Тут, значит, C у, A и T, у равно 0, и у вот такой. Значит, мы уже поняли, что C у нас это вот это,
[01:06:48.620 --> 01:06:56.940]  это у нас A1, это у нас A2. Ой, тут B и T. И осталось понять, вектор B у нас какой.
[01:07:01.940 --> 01:07:03.220]  Почему равен вектор B?
[01:07:03.220 --> 01:07:20.780]  Тут остался так много саундных переменных, которые мы и не использовали.
[01:07:20.780 --> 01:07:37.820]  0,1, да, конечно. Потому что у нас вектор B сидит вот здесь. Это, на самом деле,
[01:07:37.820 --> 01:07:46.540]  наша B-транспонированная Х. Х из двух компонентов, там сфигурирует Х2, поэтому 0,1. Прекрасно
[01:07:46.540 --> 01:07:51.620]  записываем, как это выглядит. Смотрите, какой у нас... То есть теперь у нас было две переменных,
[01:07:51.620 --> 01:07:57.180]  стало N на N-1 пополам переменных. Потому что нам целую матрицу теперь надо определить.
[01:07:57.180 --> 01:08:09.380]  След матрицы C на некоторую матрицу. Чему равен? След произведения. Матрица C имеет довольно
[01:08:09.380 --> 01:08:19.260]  специфичный вид. Но Y1,1, конечно. То есть нам надо фактически определить Y1,1 при условии,
[01:08:19.260 --> 01:08:26.140]  что Y у нас вот такой, и выполнено еще два равенства. Давайте их поймем, какие это будут равенства.
[01:08:26.140 --> 01:08:37.060]  А1 мы знаем, и А2 тоже знаем, и B1, B2 тоже знаем. Давайте продиктуйте, что получится.
[01:08:37.060 --> 01:09:04.820]  Y2,2,0 и еще раз, что-то не то. То есть тут же есть довольно хитрые единички,
[01:09:04.940 --> 01:09:17.660]  которые начнут играть свою роль немаловаженную. Ну, след от произведения матрицы на вот такую
[01:09:17.660 --> 01:09:38.940]  матрицу. Чему равен? Тяжело как-то идет. Громче, не поэлементное. Но если вы вытягиваете,
[01:09:38.940 --> 01:09:47.060]  просто мне проще вытягивать три произведения вектора 3 на 3, чем вытягивать вектор 9 на 9,
[01:09:47.060 --> 01:09:53.660]  умножать его, а потом склад. Параллельно три штуки 3 на 3 проще удержать в голове,
[01:09:53.660 --> 01:10:12.620]  чем одну большую 9-мерный вектор. Да, это правда. Так, давайте проверим. Вроде правда, да? Минус
[01:10:12.620 --> 01:10:19.780]  Y1 на 1 почему-то. Почему? А, потому что там стоит что-нибудь в знаке, да? Ага, понятно. Ладно, сейчас.
[01:10:19.780 --> 01:10:36.380]  То есть, да, с равенством, со следами все было в порядке. Вот. Матрица C.
[01:10:36.380 --> 01:11:00.700]  Я скажу, в чем было дело. Давайте мы посмотрим сейчас. Да, тут вот, видите, все-таки вот эта штука,
[01:11:00.740 --> 01:11:09.900]  она сыграла какую-то роль. Так, исходное значение у нас было минус X2, да, и мы от этого минуса избавились,
[01:11:09.900 --> 01:11:23.700]  сказав, что у нас вектор минус ушел из этого и минус остался здесь. Так, давайте сейчас я поставлю
[01:11:23.700 --> 01:11:32.060]  здесь минус, чтобы не сломалась двойственность. Сейчас увидите, что произойдет, если там минус
[01:11:32.060 --> 01:11:44.380]  не будет. Вот, и да, надо будет до конца починить знаки в слайдах. Так, окей, по модулю вот этого
[01:11:44.380 --> 01:11:51.420]  минуса, я надеюсь, понятно, откуда это взялось. Супер. Теперь давайте решим эту задачу. Она,
[01:11:51.420 --> 01:11:57.180]  оказывается, не такой сложной, как может показаться на первый взгляд. Смотрите, вот эта штука нам
[01:11:57.180 --> 01:12:04.100]  дает к систему неравенств на элементы матрицы Y, который у нас фигурирует вон там и вот здесь.
[01:12:04.100 --> 01:12:15.100]  Давайте сейчас, я даже не полюнюсь, ее аккуратно распишу. Хотя половину писать не надо, ну ладно.
[01:12:22.260 --> 01:12:38.420]  Вот так. И мы знаем про ее положительную полуопределенность. Что здесь будет нас интересовать?
[01:12:38.420 --> 01:12:55.220]  То есть Y1,1 у нас тут уже заведомо больше или бравее нуля. А, Y2,2,0, отлично. Круто, да? Это
[01:12:55.220 --> 01:13:04.140]  значит, что вот это вот minor, если мы на него посмотрим, что он нам скажет? Он нам скажет,
[01:13:04.140 --> 01:13:21.180]  что Y2,3 на Y3,2 меньше или бравее нуля. Что теперь надо вспомнить? О чем я немножечко забыл,
[01:13:21.180 --> 01:13:34.380]  записывая матрицу. В точку. Она симметрична. Поэтому вот здесь вот на самом деле фигурирует Y2,3 в
[01:13:34.380 --> 01:13:42.900]  квадрате. И поэтому вот это вот первое, второе равенство замечательное, оно оказывается,
[01:13:42.900 --> 01:13:53.900]  что Y1,1 в точности равен 1. А и наш ответ минус 1. Теперь смотрите, то есть тут D со звездочкой
[01:13:53.900 --> 01:14:01.660]  минус 1. Если бы тут минуса не было, то мы бы сломали двойственность, потому что получили бы,
[01:14:01.660 --> 01:14:04.980]  что значение двойственной функции больше, чем оптимальное значение прямой задачи,
[01:14:04.980 --> 01:14:12.740]  которая равна нулю. Вот поэтому должен быть минус и в знаках, которые фигурируют в слайдах
[01:14:12.740 --> 01:14:27.660]  вкралась какая-то неточность. Отсюда следствие. Да, ответ минус 1. Смотрите, что получилось. У нас
[01:14:27.660 --> 01:14:34.100]  есть одна задача, есть другая задача. И там, и там допустим множество непустых. И зазор двойственности
[01:14:34.100 --> 01:14:51.780]  положителен, равен 1. В чем проблема? В чем проблема? Да, именно так. Условие сеттера не выполняет.
[01:14:51.780 --> 01:14:58.900]  Это значит, что для, вот здесь, точнее, наверное, правильно показывать, вот для, вот допустим,
[01:14:58.900 --> 01:15:04.980]  не существует строго положительно определенной матрицы, для которой было бы выполнено вот эти
[01:15:04.980 --> 01:15:11.420]  ограничения равенства. Ну да, для которой были бы выполнены эти ограничения равенства. Вот таким
[01:15:11.420 --> 01:15:17.300]  образом это существенно отличает задачу полуопределенной оптимизации от линейного
[01:15:17.300 --> 01:15:22.280]  программирования, в котором, значит, если не пусто, то точная сильная двойственность будет выполнена,
[01:15:22.280 --> 01:15:26.880]  все будет работать замечательно. Вот здесь возможны вот такие вот патологические
[01:15:26.880 --> 01:15:32.320]  эффекты, которые будут в свою очередь влиять на то, как solver будут
[01:15:32.320 --> 01:15:36.080]  оперировать и что они вам будут дать, потому что у вас вот двойственные
[01:15:36.080 --> 01:15:38.960]  переменные, которые будут соответствовать этой задаче, просто бесконечно будут
[01:15:38.960 --> 01:15:45.320]  улетать и вам будут говорить, что что-нибудь там infeasible или unbounded,
[01:15:45.320 --> 01:15:50.920]  наверное unbounded. При этом решения существуют, все в порядке, и там и там решения
[01:15:50.920 --> 01:15:57.880]  существуют, просто они разные, не совпадают. Так, вопрос есть по этому
[01:15:57.880 --> 01:16:04.240]  примеру и по, в целом, по двойственности и, в частности, конической двойственности,
[01:16:04.240 --> 01:16:10.000]  которая существенным образом упрощает получение двойственной функции, двойственной
[01:16:10.000 --> 01:16:12.240]  задачи.
[01:16:21.240 --> 01:16:29.600]  Так, всем все понятно? Красота. Так, ну тогда давайте заканчиваем. Значит, сегодня мы
[01:16:29.600 --> 01:16:37.000]  обсудили, что, обсудили, какие преобразования можно делать с задачами, чтобы
[01:16:37.000 --> 01:16:41.560]  получать двойственные задачи различного вида, посмотрели на двойственные задачи
[01:16:41.560 --> 01:16:45.960]  для линейного программирования, для общенных неравенств и для конической
[01:16:45.960 --> 01:16:49.960]  двойственности. Посмотрели, в чем разница между полупределенной оптимизацией
[01:16:49.960 --> 01:16:54.800]  и линейным программированием. В следующий раз, собственно, как я уже анонсировал,
[01:16:54.800 --> 01:16:59.440]  будем смотреть на то, как из произвольной задачи выпукла оптимизации собрать
[01:16:59.440 --> 01:17:05.720]  коническую задачу. То есть, откуда берутся конусы, почему они, ну то есть, и почему
[01:17:05.720 --> 01:17:08.960]  зачастую бывает так, что они там самосопряженные, и что большинство
[01:17:08.960 --> 01:17:14.240]  задачи из них так или иначе записываются. Ну там плюс еще экспоненциальный есть, вот.
[01:17:14.240 --> 01:17:23.320]  И как запись задачи в конической форме, ну то есть, как запись задачи двойственной в конической
[01:17:23.320 --> 01:17:30.600]  форме, можно обратно преобразовать в задачу в обычной форме виде неравенств, например. Это все
[01:17:30.600 --> 01:17:36.440]  довольно делается прямолинейно. Ну, просто, у вас был условно, у вас было в исходной, значит,
[01:17:36.440 --> 01:17:44.040]  ограничение вида там ax-b первая норма меньше либо равно чему-то, вы говорите, окей, это значит,
[01:17:44.040 --> 01:17:49.840]  что у меня вектор ax-b, то, что стоит справа, лежит в конусе, порожденном первой нормой, вот. Раз
[01:17:49.840 --> 01:17:54.680]  эти перемены лежат в этом конусе, значит, спряженные будут лежать в конусе, порожденном
[01:17:54.680 --> 01:18:00.120]  бесконечной нормой. И, соответственно, просто выписывается то, что набор переменных лежит в
[01:18:00.120 --> 01:18:08.040]  таком конусе виде неравенств. И это, кстати, еще один момент, о котором тут в этих слайдах я не
[01:18:08.040 --> 01:18:14.360]  говорю, но буду говорить в следующих, что такой подход позволяет строить двойственные к задачам,
[01:18:14.360 --> 01:18:19.520]  ну, упрощает построение двойственных к задачам, которых целевые функции или ограничения не
[01:18:19.520 --> 01:18:24.160]  являются дифференцируемыми. То есть мы существенным образом в процессе построения опирались на то,
[01:18:24.160 --> 01:18:31.960]  что мы там можем градиент посчитать, что у f0, что и у ограничений. Вот, при этом никто не обещал,
[01:18:31.960 --> 01:18:39.240]  никаких ограничений при именно получении этих самых величин использовано не было. Поэтому,
[01:18:39.240 --> 01:18:42.440]  если у вас там идет что-то не дифференцируемое, то это не значит, что двойственные задачи не
[01:18:42.440 --> 01:18:48.680]  существуют. Просто, чтобы ее найти, надо немножко там, ну, либо пользоваться техникой для негладких
[01:18:48.680 --> 01:18:53.320]  задач. Я ее специально не рассказываю, потому что, чтобы ее нормально рассказать, надо там,
[01:18:53.320 --> 01:18:56.920]  типа, две-три лекции потратить, а выхлопать от нее на практике не очень много, потому что все в
[01:18:56.920 --> 01:19:01.880]  итоге сводится конической двойственности, которая записывается автоматически. Поэтому я решил
[01:19:01.880 --> 01:19:08.120]  лучше про это поподробнее поговорить, чем рассказывать про обобщение градиентов на негладкие функции.
[01:19:08.120 --> 01:19:15.960]  В общем, следующий раз будет, как я уже сказал, небольшое введение в пакет решения задач
[01:19:15.960 --> 01:19:21.640]  оптимизации, на каких принципах они строятся, и посмотрим, в чем разница в их использовании,
[01:19:21.640 --> 01:19:29.240]  в частности, удобство versus обобщаемость некоторая. Ну, не обобщаемость, а широта класса задач,
[01:19:29.240 --> 01:19:34.960]  на которых вы можете тот или иной инструмент использовать. Так, на этом все удивительно вовремя,
[01:19:34.960 --> 01:19:40.840]  даже все успели. В общем, 14 слайдов, видимо, это оптимальное количество, более-менее, в окрестности того,
[01:19:40.840 --> 01:19:46.120]  сколько можно успеть рассказать. Всем спасибо, в следующий раз, надеюсь, продолжим.

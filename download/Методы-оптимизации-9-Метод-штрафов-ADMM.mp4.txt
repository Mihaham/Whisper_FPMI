[00:00.000 --> 00:14.400]  Десятое занятие. И сегодня, соответственно, мы говорим про мета штрафов. Вот, и связанный с ним метод,
[00:14.400 --> 00:24.280]  один из самых популярных тоже методов оптимизации, ADMM. Так, хорошо, хорошо. Ну что, мы все еще продолжаем
[00:24.280 --> 00:30.720]  с вами решать задачу оптимизации. Все еще у этой задачи есть какие-то ограничения. Вот. В данном случае
[00:30.720 --> 00:38.800]  я пока ограничиваюсь только ограничением вида равенств. Ну и, соответственно, сразу же у меня
[00:38.800 --> 00:45.640]  возникает к вам такой вопрос. Давайте я возьму некоторую константу RO, просто какое-то число,
[00:45.640 --> 00:51.840]  и немного модифицирую мою целевую функцию. Ограничения я не трогаю. Соответственно,
[00:51.840 --> 00:59.440]  в целевую функцию я вписываю вот такой вот дополнительный член. Такую вот добавку. Вот.
[00:59.440 --> 01:06.600]  Что можно сказать? Изменилась ли у меня задача? Как тогда поменяется решение, если я вот добавил
[01:06.600 --> 01:17.360]  вот такой вот кусок туда? Так, минимум мог увеличиться. Не увеличится? Именно значение
[01:17.360 --> 01:20.880]  минимума или точка значения. Так, у кого еще какие версии?
[01:32.400 --> 01:38.560]  Да, то есть на множестве, на котором мы решаем эту задачку, а оно вот оно, оно у нас задается с
[01:38.560 --> 01:47.800]  помощью ограничений. На этом множестве вот эта добавка чему равна? Нулю. Поэтому я добавил за
[01:47.800 --> 01:52.320]  бесплатно считать. То есть она мне вообще никак не поменяла задачу. Просто на том множестве,
[01:52.320 --> 01:57.720]  на котором я решаю задачку, там ничего не происходит. Там, соответственно, эта добавка ноль, и она на нее
[01:57.720 --> 02:02.400]  никак не влияет. Вот. Поэтому вот то, что сейчас написано здесь, это две эквивалентные задачи,
[02:02.400 --> 02:13.240]  верхняя и нижняя. Соответственно, да. Но. Где была, соответственно, нижняя задача? Хочется ее теперь
[02:13.240 --> 02:20.160]  сделать более дружелюбной. И, соответственно, уже в данном случае мы убираем ограничения. Убираем
[02:20.160 --> 02:27.560]  ограничения и оставляем уже задачу безусловной оптимизации. Мы решаем ее на Rd. Но вот, соответственно,
[02:27.560 --> 02:34.960]  функция целевая, которую я теперь хочу минимизировать, она выглядит следующим образом. Это не просто моя
[02:34.960 --> 02:41.640]  исходная целевая функция, ну плюс вот эта добавка, которую, соответственно, называют штрафом. Штрафом за
[02:41.640 --> 02:47.320]  то, что вы не удовлетворяете ограничениям. Соответственно, удовлетворяете ограничениям, штраф равен нулю.
[02:47.320 --> 02:55.560]  Не удовлетворяете. Штраф, соответственно, начинает расти в зависимости, понятно, от 3, ну и в зависимости от того,
[02:55.560 --> 03:01.160]  насколько вы не удовлетворяете ограничениям. В итоге мы получаем вот такую вот целевую функцию, с которой и
[03:01.160 --> 03:11.400]  хотим взаимодействовать. Ну а сейчас что думаете, сильно ухудшилась задача или нет? Какие вот первые
[03:11.400 --> 03:20.800]  мысли, когда вы смотрите вот на что-то вот такое и вспоминаете исходную задачу? Не эквалентно, да. Как это
[03:20.800 --> 03:27.560]  можно понять? Какие вот первые мысли есть? Будут ли выполняться ограничения или нет?
[03:27.560 --> 03:41.640]  Да, соответственно, тут, если мы, например, ров возьмем не особо большим, то, например, значение штрафа,
[03:41.640 --> 03:49.120]  ну оно может быть вообще не особо влияет на целевую функцию. И в принципе, то есть под штрафной
[03:49.120 --> 03:53.880]  функцией может стоять, ну вот значение, которое стоит здесь, оно может быть не нулевым, вот, и даже
[03:53.880 --> 04:02.320]  довольно большим, если рост совсем маленький, вот. В связи с этим как бы возникают какие-то первые
[04:02.320 --> 04:08.160]  наблюдения. Да, соответственно, хорошо, что мы перешли от задач с ограничением к задаче без
[04:08.160 --> 04:16.520]  ограничений. Такую задачу приятнее решать, методы мы для нее знаем, но, соответственно, предполагается,
[04:16.520 --> 04:23.080]  что вот здесь мы уже за пределы наших ограничений можем спокойно выходить, то есть вот такой штраф,
[04:23.080 --> 04:27.400]  он не запрещает нам выходить за ограничения, ну пошли мы куда-то в бок, ну штраф начал
[04:27.400 --> 04:32.480]  потихоньку расти, вот. Ну, соответственно, не проблема, возможно, при этом там у меня целевая
[04:32.480 --> 04:39.040]  функция уменьшается, ну, то есть вот функция f. Мы же для этого и решаем на задачу на множестве,
[04:39.040 --> 04:43.920]  потому что, скорее всего, глобальный минимум у него находится где-то в другом месте. Ну вот уходим мы
[04:43.920 --> 04:49.480]  от штрафа. Штраф увеличивается, а функция при этом уменьшается и непонятно, что уменьшается
[04:49.480 --> 04:56.400]  быстрее. Соответственно, это в некотором смысле тоже борьба и баланс здесь. Единственное,
[04:56.400 --> 05:04.280]  что вот хорошая интуиция, когда мы устремим бесконечность, и что у меня будет, когда x
[05:04.280 --> 05:12.840]  удовлетворяет ограничениям. Чему это равно? Только что обговаривали. Это просто f, да, это просто моя
[05:12.840 --> 05:18.720]  целевая функция, а если x не удовлетворяет ограничениям, то чему это равно, если rho? Это плюс
[05:18.720 --> 05:24.040]  бесконечность. То есть вот в такой интуиции это действительно работает хорошо, rho большое, прям
[05:24.040 --> 05:30.120]  огромное, ну и тогда реально мы по факту работаем на нашем целевом множестве, которое у нас было
[05:30.120 --> 05:36.080]  изначально задано с помощью ограничений, а соответственно, вне этого множества у нас просто
[05:36.080 --> 05:43.880]  функция равна плюс бесконечности, и мы туда не заходим. Вот. Это хорошо, это хорошо, вот. Поэтому
[05:43.880 --> 05:51.720]  все же появляется какая-то надежда, что взяв достаточно большой rho, можно вот эту задачу,
[05:51.720 --> 05:58.920]  которая у нас написана, отрешать так, что действительно получено здесь решение будет
[05:58.920 --> 06:07.800]  неплохо, так, ипроксимировать то решение исходной задачи, которая у нас была. Окей. Почему нет? Давайте
[06:07.800 --> 06:14.600]  разбираться, насколько вообще мы хорошо это все можем сделать. Вот. Мы прикоснемся только к каким-то
[06:14.600 --> 06:21.160]  основным результатам, но сначала покажу, как, например, это можно сделать для задачи с ограничением
[06:21.160 --> 06:28.760]  типа неравенства, как их можно запихать штраф, вот, аналогичным образом, что нужно сделать мысли и
[06:28.760 --> 06:37.880]  идеи. Неравенство с неотрицательными коэффициентами. Как это записать? Ну вот,
[06:37.880 --> 06:43.320]  добавляем, например, к штрафу неравенства, вот, какой-нибудь, вот так вот просто добавляем,
[06:43.320 --> 06:52.040]  но это вроде не совсем правильно, потому что кажется, что... Вот. Вот. Это хорошо. То есть,
[06:52.040 --> 07:02.200]  вот так вот, например, нормально. То есть, как раз, если у меня g меньше нуля, вот, у меня срабатывает
[07:02.200 --> 07:07.680]  в максиме ноль, и, соответственно, к штрафу прибавляется ничего. Вот. Если, соответственно,
[07:07.680 --> 07:13.880]  g начинает расти и становится больше нуля, срабатывает, соответственно, уже штраф. Ну, здесь
[07:13.880 --> 07:20.560]  можно еще квадратик и квадратик дописать, чтобы все четко совсем было. Вот. Понятная идея, да? Как,
[07:20.560 --> 07:24.000]  соответственно, сюда можно еще запихать что-то вида неравенства.
[07:24.000 --> 07:38.240]  Квадрат, квадрат, квадрат. А какая разница, я снаружи или внутри поставлю? Вот. Ну, все равно у
[07:38.240 --> 07:45.360]  меня функция положительная. Ну, можно, наверное, и без квадратов. Да, это правда. Вот. Ну, можно и
[07:45.360 --> 07:49.360]  с квадратом. То есть, здесь квадрат необязательный. Штрафуем и штрафуем. Функция положительная.
[07:49.360 --> 07:59.760]  Окей. Так. Хорошо. Хорошо. Идем дальше. Давайте попробуем потихонечку,
[07:59.760 --> 08:05.440]  по чуть-чуть вообще поразбираться за свойствами, которые у нас есть у этой задачки. Вот. Чтобы
[08:05.440 --> 08:11.280]  понять, насколько вообще мы делаем что-то адекватное. Первое свойство довольно простое.
[08:11.280 --> 08:15.560]  Первое свойство довольно простое. Пусть мы нашли, у нас есть решение х со звездой,
[08:15.560 --> 08:22.120]  наша исходная задача с ограничениями. Вот. И есть х ро со звездой. Это решение нашей
[08:22.120 --> 08:26.880]  новой безусловной задачи оптимизации, но, соответственно, с дополнительным штрафом. Нашей
[08:26.880 --> 08:34.400]  вот штрафной задачи. Вот. Ну, хочется показать, что будет выполнено следующее отношение. Оказывается,
[08:34.400 --> 08:41.440]  значение в точке, вот, которую мы нашли с помощью штрафной задачи, будет меньше либо равно,
[08:41.440 --> 08:48.880]  чем значение в реальной целевой точке, которую мы ищем в исходной задаче. Фак довольно простое,
[08:48.880 --> 08:54.160]  что мы делаем. Давайте посмотрим. Скажем, что если у нас х со звездой, это решение нашей
[08:54.160 --> 09:03.160]  исходной задачи. Почему выполнено вот это равенство, как думаете? Да. Потому что х со звездой это решение.
[09:03.160 --> 09:10.360]  Задача. Значит, он удовлетворяет всем ограничениям. Значит, штрафная функция для нее равна нулю. Дальше,
[09:10.360 --> 09:14.280]  соответственно, что я говорю? Ну, давайте я разморожу точку, потому что х со звездой это теперь
[09:14.280 --> 09:20.800]  не минимум. Возможно, не глобальный минимум моей функции штрафной. И здесь будет какая-то, может быть,
[09:20.800 --> 09:26.680]  другая точка стоять, которая дает минимум штрафной функции. Ну, вот я ее и подставляю. Понятно,
[09:26.680 --> 09:32.600]  что здесь будет меньше либо равно. Ну, а дальше, как я рассуждаю? Говорю, что у меня штраф,
[09:32.600 --> 09:40.160]  эта вещь не отрицательная. Поэтому, если я уберу из функции штраф, то, соответственно, функция станет
[09:40.160 --> 09:48.520]  только меньше. Ну, не больше, не увеличится. Окей? Все довольно просто. Но главное, какую интуицию это
[09:48.520 --> 09:55.040]  нам дает? То есть, то, что точку мы нашли, в ней функция будет обязательно меньше. Ну, не больше,
[09:55.840 --> 10:03.160]  но в случае, когда меньше. Что это значит, если у нас вот так вот выполнилось? Что это значит?
[10:08.000 --> 10:14.080]  Ну, может ли такое быть, если мы, например, находимся в пределах наших ограничений?
[10:17.840 --> 10:22.400]  Да, мы попали в множество за пределами, потому что просто по определению глобального минимума
[10:22.400 --> 10:29.400]  задачи. X звездой — это решение, глобальный минимум, когда он, соответственно, меньше либо равен
[10:29.400 --> 10:35.120]  значению в нем, меньше либо равно любому значению в пределах этого множества. А здесь, соответственно,
[10:35.120 --> 10:39.800]  мы получаем то, что у нас значение в нашей точке, которое мы нашли, может быть, вообще меньше.
[10:39.800 --> 10:45.520]  Вот получается, мы точно за пределами находимся. Ну, либо, если у нас равенство, то мы прям идеально
[10:45.520 --> 10:52.000]  попали. Чаще реализуется как раз второй случай. И действительно, когда вы применяете метод со
[10:52.000 --> 10:58.920]  штрафами, вы по факту выходите за пределы множества. Вот, плохо это ли хорошо, зависит от конкретной задачки,
[10:58.920 --> 11:04.240]  потому что где-то это позволительно, вот, где-то непозволительно, потому что в любом случае у вас
[11:04.240 --> 11:10.400]  есть вычислительные какие-то огрехи, и даже решая точную задачу, например, исходную какую-то
[11:10.400 --> 11:18.200]  другими методами, вы все равно получите решение, которое, скорее всего, из-за каких-то небольших
[11:18.200 --> 11:24.040]  вычислительных огрехов на какие-то малые порядки отличается от, ну, неудовлетворяя от
[11:24.040 --> 11:29.440]  ограничений. Ну, в пределах погрешности какой-то машины точности. Ну, и здесь то же самое, понятно,
[11:29.440 --> 11:35.160]  что подбирая большое РО, можно вытянуть. Ну, вот это мы сейчас как раз и попробуем понять. Вот,
[11:35.160 --> 11:42.440]  большое РО, соответственно, кажется, что это может помочь. Ну, давайте, соответственно, попробуем это
[11:42.440 --> 11:48.680]  подоказывать. Первое свойство, которое связано с подбором РО, это то, что увеличение РО
[11:48.680 --> 11:59.560]  влечет за собой вот такое вот соотношение. То есть, значение штрафа с увеличением РО уменьшается.
[11:59.560 --> 12:05.800]  Ну, не увеличивается. Не увеличивается. То есть, берем больше РО, значение штрафной функции в том
[12:05.800 --> 12:11.880]  решении, которое мы найдем, минимизируя, безусловно, задачу со штрафом, станет меньше. То есть,
[12:11.880 --> 12:19.440]  чем больше РО, тем, соответственно, меньше значение штрафной функции, тем меньше вы выходите за
[12:19.440 --> 12:29.960]  пределы ограничений. Вот. Давайте, на отдельном листике черкану доказательства здесь. Давайте,
[12:29.960 --> 12:35.560]  тут на самом деле не сложно, просто нужно в две стороны записать. Пусть у меня есть сначала одна
[12:35.560 --> 12:44.040]  задачка, которая со штрафом, например, РО2, которая меньше. И есть, соответственно, вот эти ограничения.
[12:44.040 --> 12:55.760]  Вот. У этой задачки есть свое решение. Например, обзовем его XRO2 со звездой. Это решение.
[12:55.760 --> 13:04.840]  Вот. Ну, тогда по свойству решения, так как мы решаем задачу безусловную, да, и если бы даже
[13:04.840 --> 13:15.160]  условная была, у нас что бы справедливо. Здесь РО1 со звездой. Вот это будет решение задачи с РО1.
[13:15.160 --> 13:26.000]  Плюс РО2 сумма штрафов. Здесь XRO1 со звездой. Ну, это все понятно, будет меньше либо равно,
[13:26.000 --> 13:40.320]  чем значение в точке РО2. Так. Ну, это просто из определения того, что я работаю с минимумом моей
[13:40.320 --> 13:49.160]  штрафной задачи. Окей? Здесь вроде ничего сложного. Аналогичную вещь я могу записать и для задачи,
[13:49.160 --> 14:00.440]  когда я поменяю РО2 на РО1 и в обратную сторону. То есть подставил теперь РО1 и здесь будет
[14:00.440 --> 14:16.560]  X со звездой РО2 плюс РО1, аж и в квадрате X со звездой РО2 меньше либо равно, чем f X со звездой РО1 плюс РО1 и штрафы.
[14:19.160 --> 14:31.400]  Здесь согласны? Так. Теперь я суммирую эти две строчки. Вот. Видно, что будут по красоте уходить
[14:31.400 --> 14:40.560]  вот какие-то такие слагаемые, которые завязаны на функцию f, и соответственно вылезет что-то
[14:40.560 --> 14:47.280]  довольно хорошее. Давайте сразу я подобные приведу вот раз-два и раз-два-раз-два. Сейчас там чем-то с рожкой
[14:47.280 --> 15:03.560]  будет. РО2 минус РО1 сумма ошитых X со звездой РО1 меньше либо равно, чем РО2 РО1 сумма ошитых
[15:03.560 --> 15:13.640]  в квадрате X со звездой РО2. Вот. Ну а все, дальше я говорю то, что у меня по предположению РО2 меньше,
[15:13.640 --> 15:21.360]  чем РО, или наоборот, или как-то как это предполагал. Что у меня тогда получится? Ну тут неважно,
[15:21.360 --> 15:28.600]  давайте пусть будет РО2 больше, чем РО1, тогда я когда разделю обе части неравенства на РО2
[15:28.600 --> 15:36.160]  делить на РО1, минус РО1, знак не поменяется, и тогда у меня окажется, что значение штрафа
[15:36.160 --> 15:46.360]  точки в решении задачи с РО1 будет больше либо равно, чем значение точки в решении задачи с РО2.
[15:46.360 --> 15:51.160]  Окей? В формулировке может быть по-другому было, там РО1 было больше, ну это неважно,
[15:51.760 --> 15:59.280]  поменять местами. Все, здесь простой факт, который в принципе нам дает хорошую интуицию в плане того,
[15:59.280 --> 16:07.840]  что мы хотя бы понимаем, что увеличение РО может привести к каким-то вещам в душе того,
[16:07.840 --> 16:14.480]  что действительно у нас штраф хотя бы не будет увеличиваться, штраф хотя бы не будет увеличиваться.
[16:14.480 --> 16:23.240]  Так, сейчас более такая теорема существенная, выглядит она следующим образом, немного так
[16:23.240 --> 16:30.320]  замудренно формулируется, пусть у нас опять же есть функции вот эти, наша задачка с ограничениями,
[16:30.320 --> 16:36.960]  эти функции являются непрерывными. Есть у нас множество х звезды, это множество решений
[16:36.960 --> 16:44.000]  исходной задачи, предполагается, что оно не пусто. Дальше соответственно что? Дальше мы
[16:44.000 --> 16:54.400]  рассматриваем вот такого рода множество. Это просто соответственно значение, в этом множество U
[16:54.400 --> 17:01.000]  входят те точки, где соответственно у меня f'х меньше, чем f'х со звездой. f'х со звездой это у меня
[17:01.000 --> 17:08.200]  значение соответственно на моем множестве, которое задано ограничениями, а f'х это соответственно
[17:08.200 --> 17:14.720]  тут уже в ВУ могут вводить точки, которые и ограничения не удовлетворяют. И предполагается,
[17:14.720 --> 17:22.840]  что это множество ограничено. Ограничено, сейчас мы как раз, ну давайте можно сразу обсудить зачем
[17:22.840 --> 17:28.920]  вообще это нужно. Потому что мы сейчас будем как раз работать с штрафной функцией, как-то говорить
[17:28.920 --> 17:34.760]  про ее решения, где они лежат относительно решений нашей исходной задачи и зачем я вообще
[17:34.760 --> 17:40.800]  ограничу вот это множество U. В пределах самого множества, на котором мы решаем нашу исходную
[17:40.800 --> 17:45.920]  задачу с ограничениями, с функцией может быть все хорошо. Она действительно там принимает какое-то
[17:45.920 --> 17:52.320]  конечное значение, возможно даже уникальное, если повезет. Это минимально ее значение. Что
[17:52.320 --> 17:59.600]  может происходить за пределами этого множества никто не знает. Ушли за пределы, вышли, наткнулись
[17:59.600 --> 18:03.880]  например на такую ситуацию, когда даже выпукла функция, просто уходит минус бесконечности.
[18:03.880 --> 18:11.280]  Соответственно да, чтобы этого избежать, работать с штрафными функциями в том числе этого хотим
[18:11.280 --> 18:17.800]  избежать, потому что добавили какую-то линейную функцию и она у вас сразу минус бесконечность
[18:17.800 --> 18:23.800]  поползла. Этого хочется избежать. Соответственно предполагается, что вот у нас функция f, ну вот те
[18:23.800 --> 18:30.080]  значения, которые меньше, чем fх звездой, ариол обитания ограничен. Ариол обитания ограничен,
[18:30.080 --> 18:34.600]  чтобы как раз не уползти в минус бесконечность и работать с штрафными функциями, хотя бы какие-то
[18:34.600 --> 18:42.040]  гарантии иметь. Иметь то, что мы не решаем какую-то бесполезную задачу. Ну то есть бессмысленно
[18:42.040 --> 18:48.520]  решать задачу, которая там минимум где-то в минус бесконечности болтается. Вот. А про что вообще
[18:48.520 --> 18:54.840]  теорема? А теорема про то, что вот если у нас есть какое-то Эпсилон произвольное, тогда существует
[18:54.840 --> 19:02.880]  некоторый подбор параметра РО и оказывается, что вот множество решений штрафной задачи,
[19:02.880 --> 19:11.160]  как раз на самом деле вот это условие гарантирует, что это множество не пусто. Вот если исходное
[19:11.160 --> 19:17.000]  множество х звезды было не пусто. Для любых РО, которые больше подобранного нашего РО Эпсилон,
[19:17.000 --> 19:27.360]  окажется так, что вот множество решений задачи штрафной будет лежать в некоторой Эпсилон окрестности
[19:27.360 --> 19:34.400]  исходного множества решений. Понятна суть, да? То есть берем нашим исходное множество,
[19:34.400 --> 19:39.080]  чуть-чуть его раздуваем на Эпсилон. Ну вот здесь вот как раз чуть-чуть отступаем от него на Эпсилон.
[19:39.080 --> 19:46.280]  Был шарик, например, стал шарик плюс Эпсилон. Немного раздули и оказывается, что вот можно всегда
[19:46.280 --> 19:52.640]  подобрать такое РО, что множество решений уже задач со штрафом будет лежать в пределах вот этого
[19:52.640 --> 20:00.320]  раздутого шарика. Понятно, что если хочется взять довольно маленькое Эпсилон, вот видимо берется
[20:00.320 --> 20:05.960]  большой РО и получается, что вы почти совпали с точки зрения именно вот этих множеств решений,
[20:05.960 --> 20:13.160]  где они относительно друг от друга лежат. Окей? Окей. Ну давайте попробуем это подоказывать.
[20:13.160 --> 20:19.760]  Давайте попробуем это подоказывать. Вот. Пойдем от противного. Пойдем от противного и соответственно
[20:19.760 --> 20:31.120]  будем предполагать, что у нас ну вот так случится, что множество не лежит. Ну и соответственно,
[20:31.120 --> 20:38.840]  что там предполагалось у нас существование РО Эпсилон и для него следовало, что для любого РО больше,
[20:38.840 --> 20:51.200]  РО Эпсилон у вас соответственно х РО со звездой кладывалось в это. А теперь я хочу пойти от противного
[20:51.200 --> 21:02.600]  и говорю то, что у меня существует некоторая последовательность РО ИТ, который устремляется
[21:02.600 --> 21:09.720]  в бесконечность. Ну вот здесь понятно РО. Оно просто больше, чем никакого либо РО Эпсилон и понятно,
[21:09.720 --> 21:15.240]  что если я не устремлю эту последовательность плохую в бесконечность, я просто РО Эпсилон могу чуть-чуть
[21:15.240 --> 21:24.360]  поднять и тогда вот у меня все, которые не попали, они зажуются и у меня все РО будут нормальными. Вот. Я
[21:24.360 --> 21:31.560]  ввожу последовательность РО ИТ, который устремляю к бесконечности и вот для этой последовательности
[21:31.560 --> 21:44.880]  у меня х ИТ со звездой не вкладывается в х Эпсилон. Вот. Второе утверждение я могу, кстати, переписать
[21:44.880 --> 21:56.600]  чуть даже по-другому. Я могу сказать, что существует некоторый х ИТ со звездой. Такой,
[21:56.600 --> 22:03.080]  что он не лежит в х Эпсилон. Так, ну это по факту как раз то, что у меня не вкладывается. Существует
[22:03.080 --> 22:11.600]  элемент, который где-то лежит за пределами. Окей. Понятно суть, да? Что произошло? Как мы идем от противного?
[22:11.600 --> 22:22.160]  Вот. Хорошо. Так. Это мы с вами предположили. Давайте, соответственно, подумаем. Подумаем,
[22:22.160 --> 22:36.800]  что мы можем сказать про значение в точке и х И. Х И я все еще напишу давайте из РО ИТ. Он в РО ИТ лежит,
[22:36.800 --> 22:46.400]  а в РО Эпсилон не лежит. Что мы можем сказать, например, как соотносятся вот эти два, две вещи.
[22:46.400 --> 22:51.800]  Значение в точке х И со звездой и значение в точке х со звездой, где х со звездой это решение.
[22:51.800 --> 23:01.960]  Исходная задача с ограничением. Что мы можем сказать? Давайте без стремлений. Просто вот есть одно
[23:01.960 --> 23:07.240]  значение, есть другое. Что можно? Меньше либо равно. Почему? Потому что мы только что с вами это
[23:07.240 --> 23:14.800]  доказали. Х ИТ со звездой это же у меня решение задач со штрафом, где штрафы мне определяются РО ИТ.
[23:14.800 --> 23:20.800]  Так? А мы поняли то, что эти решения они меньше либо равны чем значение функции в этих решениях,
[23:20.800 --> 23:31.440]  меньше либо равно, чем значение функции в исходной, в решении исходной задачи. Согласны? Вот. Допишу,
[23:31.440 --> 23:37.880]  чтобы это было понятно. Мы это вроде как только что с вами доказали и довольно просто доказали,
[23:37.880 --> 23:44.200]  там в одну строчку это все делалось. Хорошо. Поняли, что у нас вот так вот. Но, и как раз тут
[23:44.200 --> 23:50.120]  тут срабатывает то предположение, которое я делал насчет множества У. Что множество У у меня
[23:50.120 --> 23:58.960]  ограничено вот тех значений. Получается, что вот х, отсюда получается, что у меня х ИТ вот этот,
[23:58.960 --> 24:12.160]  лежит Ву, которая ограничена. Так? Хорошо. Понятно, что мы вот ввели какую-то последовательность и х ИТ,
[24:12.160 --> 24:17.200]  х ИТ со звездой это тоже некоторая последовательность. Но про х ИТ со звездой мы узнали,
[24:17.200 --> 24:22.880]  что она вот эта последовательность лежит некоторым ограниченным множестве. Вот. Что мы
[24:22.880 --> 24:31.520]  тогда можем сказать про такую последовательность? Какие факты? Да, это первый курс. Вот она теорема
[24:31.520 --> 24:40.800]  Бальцана-Вирштрасы. Бальцана-Вирштрасы, что у вас существует, у ограниченной последовательности
[24:40.800 --> 24:46.760]  обязательно существует один частичный предел. Ну, вот одна сходящаяся под последовательность.
[24:46.760 --> 24:55.440]  Дальше, соответственно, Бальцана-Вирштрасы теорема. Вот. И мы рассматриваем уже вот эти точечки х
[24:55.440 --> 25:04.320]  ИТ со звездой, которые сходятся к некоторой х ИТ со звездой. Это мы нашли как раз сходящуюся
[25:04.320 --> 25:17.360]  под последовательность. Вот. Окей? Вот. Сходящаяся под последовательность. Хорошо. Ну, я на самом деле не зря
[25:17.360 --> 25:23.760]  брал еще и непрерывные функции. Что у меня функция f непрерывна, что у меня функция h непрерывна.
[25:23.760 --> 25:33.520]  Что мы можем сказать теперь про значение функций? Вот так. Как мы можем, например, связать значение
[25:33.520 --> 25:43.640]  функции вот это, вот это и, соответственно, ну давайте их пока вот эти два значения свяжем. Что мы
[25:43.640 --> 25:52.240]  можем сказать, если у меня функция f непрерывна, если у меня х ита стильдой стремится к х стильдой,
[25:52.240 --> 25:58.280]  что будет? Ну, тоже стремится, потому что есть непрерывность. Это эквивалентное определение
[25:58.280 --> 26:04.040]  непрерывности. Согласны? Вот. То есть вот здесь в пределе и стремящемся к бесконечности будет
[26:04.040 --> 26:11.800]  просто равенство. Это одно из эквивалентных определений непрерывности. Вот. Функции.
[26:11.800 --> 26:17.120]  То есть под последовательность точки стремится. Окей. Супер. Хорошо. Мы поняли то, что у меня
[26:17.120 --> 26:24.240]  под последовательность вот значение вот это будет стремиться сюда. Вот. А это что значит? Это
[26:24.240 --> 26:36.440]  что значит? То, что у меня значение х звездой. Я же знаю, что у меня вот эти все f иты, они меньше
[26:36.440 --> 26:45.120]  либо равны, меньше либо равны, меньше либо равны, чем f от х звездой. Так? Но тогда f стильдой у меня
[26:45.120 --> 26:58.960]  будет меньше либо равна, чем f от х звездой. Так? Хорошо. Так. Запомним этот факт. Это первый факт.
[26:58.960 --> 27:04.280]  То есть пока он нам ни о чем не говорит. То есть это понятное свойство, которое в принципе следовало
[27:04.280 --> 27:11.440]  из того, что мы берем x иты. X иты это вот решение нашей задачи со штрафами. Ну и окей. Возможно
[27:11.440 --> 27:16.840]  просто x стильдой со звездой. Ну тоже лежит за пределами множества. Ну это просто какое-то
[27:16.840 --> 27:22.040]  более классное решение, которое еще глубже опускается. Ну тоже лежит за пределами множества. Просто
[27:22.040 --> 27:27.120]  ближе к честному глобальному минимуму безусловной задачи оптимизации, если бы у нас в исходной
[27:27.120 --> 27:35.520]  задачи не было бы ограничений. Вот. Но на самом деле сейчас мы покажем, что x стильдой оно еще и
[27:35.520 --> 27:42.600]  лежит в самом множестве. Лежит в самом множестве исходном с ограничениями. А значит x стильдой
[27:42.600 --> 27:48.520]  будет являться решением. Потому что вот это как раз определение глобального минимума.
[27:48.520 --> 27:55.760]  Потому что x со звездой у нас это что? Это глобальный минимум. Если другая точка,
[27:55.760 --> 28:00.560]  которая тоже принадлежит множеству, дает значение функции меньше либо равно, то понятно,
[28:00.560 --> 28:07.280]  что она тоже глобальный минимум на таком множестве. Вот. Давайте подоказываем. Сейчас,
[28:07.280 --> 28:15.880]  если я себе освобожу. Так, бла-бла-бла, это мы с вами доказали. Так, это мы с вами обсудили. Так,
[28:15.880 --> 28:24.080]  теперь, соответственно, доказываем то, что у нас выполняются ограничения. То есть хочу доказать,
[28:24.080 --> 28:35.200]  то, что у меня x стильдой со звездой удовлетворяет ограничения. Ну, еще раз суду от противного.
[28:35.200 --> 28:40.360]  Получается такая вложенность у нас от противного. Мы начали исходно доказывать от противного. Сейчас
[28:40.360 --> 28:46.120]  еще этот факт докажем от противного. Ну и выруливаем в итоге к исходному. Так, ну от противного
[28:46.120 --> 28:54.840]  что тут? Говорим то, что раз хотим от противного, поэтому предположим, что какое-то есть ограничение,
[28:54.840 --> 29:03.440]  которому вот хотя бы единственное ограничение наше x стильдочкой не удовлетворяет. Вот. Ну и посмотрим,
[29:03.440 --> 29:09.960]  что соответственно будет. Опять же, в силу того, что у меня функции h непрерывные,
[29:09.960 --> 29:21.480]  функции h непрерывные. Что я могу сказать? Я могу сказать, что вот значение h Катова в точках x
[29:21.480 --> 29:27.840]  и t со звездой с тильдой. То есть наши вот эти последовательности, исходящие к x тильдой. Они
[29:27.840 --> 29:38.880]  будут больше либо равны, чем 1 вторая h, давайте по модулю возьму, по модулю, h с тильдой со звездой,
[29:38.880 --> 29:44.000]  которая соответственно не равно нулю, которая там по модулю соответственно будет прям больше нуля.
[29:44.000 --> 29:54.840]  Вот. Почему так? Ну это понятно для достаточно больших и. Так? Опять же, все это происходит из-за
[29:54.840 --> 30:02.960]  непрерывности. Согласны? Потому что у меня соответственно вот это должно стремиться к h Катому x
[30:02.960 --> 30:08.880]  с тильдой. Вот. И понятно, что в какой-то момент мы просто действительно окажемся в епсилонокрестности
[30:08.880 --> 30:19.080]  вот этой точки, вот этой точке. Ну соответственно выйдем за пределы 1 вторая этой точки. То есть
[30:19.080 --> 30:25.680]  просто значение станет ближе. Ближе для какого-то епсилона. Ну и соответственно для таких и это
[30:25.680 --> 30:39.280]  будет выполняться. Окей? Хорошо. Так. Ну тогда, что я могу теперь сказать про мою, про мою вот эту
[30:39.280 --> 30:48.640]  функцию. Давайте я прям, для каждой x с тильдой я выпишу его функцию, подставлю сюда x и t со звездочкой.
[30:48.640 --> 30:57.600]  Ну и что там будет получаться? С тильдой, со звездочкой, плюс ρ и тильда ограничения
[30:57.600 --> 31:09.760]  h и t в квадрате x и t. Вот так вот. Окей. Выписал. Что будет происходить, когда я вот это все безобразие
[31:09.760 --> 31:20.840]  устремлю по и к бесконечности. Куда она будет стремиться? Бесконечности. Почему? Потому что
[31:20.840 --> 31:27.440]  первый член у нас стремится просто к f x с тильдой, что в принципе просто какое-то конечное число.
[31:27.440 --> 31:33.280]  Но когда я еще вот это начинаю устремлять, то есть одно из ограничений у меня точно не срабатывает.
[31:33.280 --> 31:38.560]  Ограничение h с тильдой у меня не срабатывает. По предположению наше мы идем от противного.
[31:38.560 --> 31:45.000]  Вот оно не срабатывает. Тут появляется какое-то значение. Тут начинаются значения вот эти h и t,
[31:45.000 --> 31:52.720]  x и t, быть больше нуля конкретно. Вот даже больше, чем 1 вторая h с тильдой. Вот это. Сейчас 1 вторая.
[31:52.720 --> 31:59.840]  h x со звездочкой с какого-то момента. Но при этом еще и ρ улетает в бесконечность,
[31:59.840 --> 32:04.160]  потому что это же тоже последовательность, которая зависит от и. Вот. Ну и когда, соответственно,
[32:04.160 --> 32:14.000]  ρ умножится на какое-то неотрицательное число, которое строго больше, чем 1 вторая h с тильдой,
[32:14.000 --> 32:18.680]  то соответственно у меня это все в бесконечности и улетит. Так.
[32:18.680 --> 32:34.120]  Согласны? Хорошо. Так. Ну и в принципе на самом деле на этом мы с вами и заканчиваем.
[32:34.120 --> 32:40.440]  Так как почему? Потому что у нас мы с вами знаем то, что у меня есть значение в точке x со звездой
[32:40.440 --> 32:49.720]  и значение для любой функции вот здесь вот. Вот здесь вот. Должно быть меньше, чем значение f
[32:49.720 --> 32:56.720]  от x со звездой. Вот. А получается так, что у меня, я какое бы вот и не взял, вот какое бы я,
[32:56.720 --> 33:02.920]  какому бы у меня не равнялось f от x со звездой в силу того, что вот это выражение у меня стремится
[33:02.920 --> 33:09.240]  к бесконечности, я всегда буду подбирать номеры так, чтобы у меня значение функции просто уходило
[33:09.240 --> 33:15.380]  выше. Вот. Получили противоречие? Противоречие пока только тому, что у меня ограничения не
[33:15.380 --> 33:22.680]  выполнены. А это значит, что вот на данный момент мы поняли что? То что у меня x с тильдой со звездой
[33:22.680 --> 33:31.000]  удовлетворяет ограничениям. Раз. Вот. Плюс f в точке x с тильдой меньше, чем f от x со звездой. Меньше ли
[33:31.000 --> 33:37.440]  баронотчимов от x звездой. Это значит, что у меня x tilde со звездой это решение
[33:37.440 --> 33:43.800]  исходной задачи. Просто по определению.
[33:43.800 --> 33:52.960]  Вот. Так. Хорошо. Ну и осталось дойти до финального противоречия.
[33:52.960 --> 34:00.360]  В силу того, что у меня последовательность x tilde и t стремится к x tilde,
[34:00.360 --> 34:09.200]  то когда-то я всегда буду оказываться в эпсилонокрестности точки x tilde,
[34:09.200 --> 34:15.800]  которая лежит в пределах x звездой. Согласны? Ну и получается, что я буду
[34:15.800 --> 34:23.320]  оказываться и вот в этом множестве. А мы сказали, что вроде как мы строили
[34:23.320 --> 34:28.000]  последовательность изначально вот эту x situ, так что вот они не лежат в
[34:28.000 --> 34:34.080]  эпсилонокрестности множества x звездой. Окей? Понятно здесь, да? То есть получилось,
[34:34.080 --> 34:39.040]  что действительно для любого эпсила мы подбираем ρ и у нас все это лежит
[34:39.040 --> 34:45.920]  компактненько. Вот эти два множества, они можно считать, там, эпсилон совпадают.
[34:45.920 --> 34:53.360]  Хорошо? Хорошо. Так. Чуть-чуть быстренько обсуждаем, что мы поняли про метод
[34:53.360 --> 34:59.680]  штрафов. Супер-плюс это то, что условная задача становится безусловной. Поняли то,
[34:59.680 --> 35:03.440]  что с увеличением ρ приближаемся к исходной задаче тоже понятное хорошее
[35:03.440 --> 35:10.960]  свойство. Вот. Но это же это же свойство в некотором смысле и нехорошее, потому
[35:10.960 --> 35:17.000]  что, во-первых, а мы выходим за пределы множества и это происходит ну вот для
[35:17.000 --> 35:21.440]  любого, чаще всего происходит для любого ρ неравного плюс бесконечности, а на
[35:21.440 --> 35:25.280]  практике это всегда так. Вот. И, возможно, для каких-то задач просто так нельзя
[35:25.280 --> 35:30.280]  делать, ну, в силу постановки. Просто какие-то у вас ограничения задают, например,
[35:30.280 --> 35:35.200]  ресурсы, которые просто нельзя менять в плане того, что выходить за их
[35:35.200 --> 35:43.080]  пределы. Вот. Ну и, соответственно, с точки зрения вообще решения задачи вот
[35:43.080 --> 35:46.600]  этой штрафной, нашу штрафную функцию, увеличение ρ это вообще ни разу не
[35:46.600 --> 35:51.680]  хорошо. Как вы понимаете, с увеличением ρ у вас просто растут некоторые
[35:51.680 --> 35:55.680]  свойства данной задачи, например, константа липщицы градиента. Даже несмотря на то,
[35:55.680 --> 35:59.440]  что, например, в ашке у вас может, аш у вас может представлять собой что-то в духе
[35:59.440 --> 36:06.440]  ax-b, когда вы это возведете в квадрат, у вас там выскочит вот это ρ перед ним, ну и,
[36:06.440 --> 36:10.520]  соответственно, вы начнете раздувать эту задачу, увеличивая ρ. При этом все еще
[36:10.520 --> 36:17.320]  будет болтаться исходная задача. То есть, которая, например, вот эта моя задача может
[36:17.320 --> 36:23.440]  быть у вас μ сильно выпуклой и l гладкой. Вот. Для этой задачи вы там с помощью
[36:23.440 --> 36:29.120]  оценки спектра можете вычислить, что, например, она там лямбда гладкая и,
[36:29.120 --> 36:33.360]  например, не сильно выпуклая. Вот. Не сильно выпуклая, просто потому что матрица не
[36:33.360 --> 36:38.800]  очень хорошая. Вот. И тогда что, у вас константа просто липшится, будет расти
[36:38.800 --> 36:44.440]  градиента. Вот. А константа μ при этом меняться не будет. Потому что она тянется из
[36:44.440 --> 36:49.440]  исходной функции, которую вы не меняете. Вот. А тогда будет расти сложность там
[36:49.440 --> 36:52.480]  градиентного спуска, которой вы будете решать штрафную задачу, ну или там
[36:52.480 --> 36:56.800]  ускоренного градиентного спуска. Вот. С увеличением ρ. Ну а как мы поняли, ρ
[36:56.800 --> 37:01.400]  увеличивать вроде как надо, чтобы получать более качественное решение. И вот здесь
[37:01.400 --> 37:06.800]  в некотором смысле это борьба. Что мне делать? Брать большое ρ и, соответственно,
[37:06.800 --> 37:14.000]  обузить мою исходную задачу, ну вот, страфную задачу. Вот. Либо брать маленькое ρ. Вот.
[37:14.000 --> 37:20.840]  Получать хорошую задачку с точки зрения вычислительных вопросов. Но вопрос,
[37:20.840 --> 37:24.280]  тогда возникают уже вопросы к качеству решения. Насколько она
[37:24.280 --> 37:34.160]  удовлетворяет ограничениям. Так. В этой части все. Вопросы.
[37:35.160 --> 37:43.320]  Если нет, тогда перерыв. Так. Ну что, давайте продолжать. Давайте продолжать. И сейчас
[37:43.320 --> 37:48.960]  как раз разговор пойдет про второй метод, который сегодня хотелось бы
[37:48.960 --> 37:54.040]  рассмотреть. Он супер популярный, все еще используется во множестве оптимизационных
[37:54.040 --> 38:00.840]  библиотек. Ну давайте, во-первых, поймем откуда вообще берется он и, соответственно,
[38:00.840 --> 38:05.760]  его интуиция. Смотрим на вот такую задачку. Тут я даже упростил. До этого у нас были
[38:05.760 --> 38:13.360]  произвольные ограничения. Теперь у нас ограничения типа равенств. Хорошо. Так. Пишем
[38:13.360 --> 38:21.840]  лагранжан. Лагранжан. Вот. Из лагранжана, как вы знаете, можно получить двойственную
[38:21.840 --> 38:30.960]  функцию. Вот. И, опять же, если у вас есть выполнено условие слейтера, пожалуйста,
[38:30.960 --> 38:38.240]  решение двойственной задачи более чем... Точнее, максимизация двойственной
[38:38.240 --> 38:42.840]  функции, решение двойственной задачи действительно вам даст какие-то хорошие
[38:42.840 --> 38:53.120]  результаты. Ну и, соответственно, да. Давайте попробуем сделать что-то в духе градиентного
[38:53.120 --> 39:01.000]  метода, но не для исходной задачки или для экстраградиентной методы, как решили лагранжан,
[39:01.000 --> 39:10.000]  а что-нибудь для двойственной задачи, для двойственной функции. Определение двойственной
[39:10.000 --> 39:14.800]  функции, соответственно, выписано. Дальше что? Раз так как мне двойственные функции нужно
[39:14.800 --> 39:22.320]  максимизировать, я делаю тоже как бы градиентный метод, только вместо спуска я делаю подъем. Иду вверх.
[39:22.320 --> 39:29.120]  Шаг альфа. Ну и, соответственно, градиент, он тут берется по лямдо. По лямдо понятно,
[39:29.120 --> 39:36.520]  потому что единственная переменная функции g это лямдо. Но просто функция g выглядит не совсем
[39:36.520 --> 39:44.160]  очевидно, то есть это вот минимум какой-то от лагранжана. Ну давайте чуть-чуть это попробуем
[39:44.160 --> 39:54.760]  попереписывать. Это с прошлого слайда. А теперь я сделаю вот так. Я скажу, что вот xкт это просто
[39:54.760 --> 40:03.240]  вот arg-минимум, то есть то значение x, на котором достигается вот этот минимум. Вот, я этот arg-минимум
[40:03.240 --> 40:08.720]  нахожу и, соответственно, могу его подставить теперь вот в то выражение, которое у меня было
[40:08.720 --> 40:18.600]  здесь. Вот. И тут уже минимум пропадает. Согласны? Вот. Просто чуть-чуть переписал, дополнительным шагом
[40:18.600 --> 40:25.920]  сделал эту минимизацию в явном виде. Нашел значение xкт плюс один, на котором это все безобразие
[40:25.920 --> 40:33.840]  достигается. Вот. И подставил его в нашу задачку уже в шаг градиентного метода по двойственной
[40:33.840 --> 40:43.160]  функции. Все. Вся идея. Ну, соответственно, такой метод называется, соответственно, у нас двойственный
[40:43.160 --> 40:49.920]  подъем. Двойственный подъем. Популярный, понятный метод. Тут еще чуть-чуть переписал, потому что видно,
[40:49.920 --> 40:56.760]  что вот я этот градиент беру по лямбда. Вот это у меня от лямбда не зависит. Ну, а все, что осталось,
[40:56.760 --> 41:05.920]  соответственно, оно и влетело в градиент. Вот оно. Окей. Все. Двойственный подъем. Двойственный подъем.
[41:05.920 --> 41:16.400]  Во. Это вопрос важный. И тут, соответственно, два варианта. То есть в ADMM это тоже вопрос возникнет.
[41:16.400 --> 41:23.240]  Первый вариант. То, что у вас функция f является дружественность с точки зрения вычисления
[41:23.240 --> 41:29.520]  аргминиума. Ну, что это значит? Это то, что вы этот аргминиум просто считаете бесплатно. Вот. Такой
[41:29.520 --> 41:33.560]  концепция мы сегодня в доказательствах и будем придерживаться. Ну, в единственном доказательстве,
[41:33.560 --> 41:38.700]  которое осталось в сходимости ADMM, что вот функция просто, когда мы считаем этот аргминиум, в явном
[41:38.700 --> 41:45.620]  виде дает вам ответ. Вы можете его там выразить. Функция f простая просто-напросто. Так. Второй
[41:45.620 --> 41:50.340]  вариант. Второй вариант. То, что вы этот аргминиум честно считаете каким-то методом оптимизации. И
[41:50.340 --> 41:57.300]  тогда при анализе уже всего метода, который у нас здесь есть, двойственного подъема, вам нужно
[41:57.300 --> 42:02.860]  учитывать, что вот этот XCAD у вас выплевывается с некоторой ошибкой. Что это нечестный аргминиум,
[42:02.860 --> 42:11.620]  а это некоторая посчитанная с помощью численного метода версия этого аргминиума. Ну, и у вас есть
[42:11.620 --> 42:17.300]  какие-то гарантии, что XCAD и плюс один отличаются от реального аргминиума не сильно. Вот. На
[42:17.300 --> 42:22.460]  какой-нибудь там эпсилон. Ну и, соответственно, эта ошибка у вас тянется и, понятно, будут влиятельны
[42:22.460 --> 42:27.500]  итоговый результат. Мы пока предполагаем, что вот все дружественно, аргминиум мы считаем честно,
[42:27.500 --> 42:36.620]  в технические детали, как учесть эту ошибку, мы уже не полезем. Вот. Чуть-чуть модифицирую исходную
[42:36.620 --> 42:43.460]  задачу. Поняли мы уже с вами то, что вот такого рода модификации делаются у нас за бесплатно. Она
[42:43.460 --> 42:49.860]  никак не влияет на нашу задачу. Вот. Всего того, что на множестве ограничений просто такая добавочка,
[42:49.860 --> 43:03.260]  это ноль. Вот. Пожалуйста. Градиент, вот у этого аргминиума мы градиент не считаем. Градиент
[43:03.260 --> 43:09.700]  вот этого, вот это. Смотрите, мы решаем задачу минимума. Вот. У меня есть какой-то X, который
[43:09.700 --> 43:15.540]  просто дает этот минимум. Ну, это же задача оптимизации. Задача оптимизации. Есть значение
[43:15.540 --> 43:20.180]  минимум, а есть значение аргминиум, которое дает это минимум. Ну, пусть это... Я это обзываю
[43:20.180 --> 43:28.300]  xкт плюс один. Вот. Ну и, соответственно, что? Если я знаю значение... Вектор, на котором минимум
[43:28.300 --> 43:35.380]  достигается, я его могу подставить вот туда. Но я его подставляю. Вот. И все. И получаю как раз
[43:35.380 --> 43:52.340]  минимум. Вот. Значение g в точке λкт, который... А почему должно быть по-другому? Потому что это
[43:52.340 --> 43:57.740]  же какое-то значение. Каким оператором я действую потом, мне абсолютно неважно. Я вот вычислил значение
[43:57.740 --> 44:14.060]  функции здесь. Так. Ну и все. Зависит от лямбды. Да, может быть, тут надо чего-нибудь еще выдумать. Да.
[44:14.060 --> 44:22.420]  Скорее всего, тут все корректно, потому что там есть какие-нибудь теоремы в духе Демья
[44:22.420 --> 44:31.740]  Новоданскина, где можно менять градиенты минимум местами. Вот. Ну давайте будем считать,
[44:31.740 --> 44:37.500]  что вот так нам это просто нужно для некоторой интуиции. Вот. Я подумаю, что там нужно четко,
[44:37.500 --> 44:45.340]  чтобы четко это все выражалось. Вот. Скорее всего, нужно какие-то действительно условия докинуть. Ну
[44:45.340 --> 44:57.300]  смысл вот такой вот. Находим x ката, делаем шаг. Так. Хорошо. Про добавочку. Добавили вот
[44:57.300 --> 45:05.020]  эту трошку. Ни на что она не влияет. Лагранжан чуть-чуть поменялся в связи с этим. Добавилась вот
[45:05.020 --> 45:14.300]  эта вот дополнительная... дополнительный кусочек. На самом деле, его добавляют обычно не для того,
[45:14.300 --> 45:20.140]  чтобы как-то в теории что-то получить. Это скорее вот выполняют функцию регулизатора, чтобы задача,
[45:20.140 --> 45:27.340]  например, стала иметь более классные свойства в плане сходимости. Вот. Чтобы обеспечить
[45:27.340 --> 45:31.980]  устойчивость сходимости, потому что исходная задача, она просто выпукла вогнуто, если у вас
[45:31.980 --> 45:40.820]  функция f выпукла. Вот. И какие-то свойства сходимости действительно могут, ну, быть не самые классные в связи
[45:40.820 --> 45:47.260]  с этим. Добавляю вот этот регулизатор, чтобы просто улучшить качество сходимости на практике. Вот.
[45:47.260 --> 45:53.380]  В точке зрения теории, это не особо нужная вещь. То есть, в первую очередь, вот такое
[45:53.380 --> 46:01.620]  добавление, которое называется агментация, это практический трюк. Вот. Ну и, соответственно,
[46:01.620 --> 46:08.100]  для него тоже можно записать вот двойственный метод типа двойственного подъема. Только здесь
[46:08.100 --> 46:15.660]  единственное, что меняется a. Lagrangian на уже аугментированный, вот с этой добавочкой. Плюс
[46:15.660 --> 46:24.260]  специально, специально меняют шаг. Вот. Был какой-то шаг альфа, берут его равным ro, где ro это вот ровно
[46:24.260 --> 46:31.580]  параметры агментации. Ну, просто так делают, чтобы избавиться от двух параметров. Потому что ro это
[46:31.580 --> 46:36.700]  в некотором смысле параметр вашего метода, насколько нужно его взять большим, маленьким и так далее. Вот.
[46:36.700 --> 46:43.020]  Плюс шаг еще, это параметры метода. Два параметра подбирать, в принципе, уже не так приятно,
[46:43.020 --> 46:50.940]  поэтому почему бы их не объединить и решают сделать вот так. Ну, вот такой вот метод, двойственный подъем.
[46:50.940 --> 46:55.260]  Нам, в принципе, он особо и не нужен сейчас, просто чтобы понимать идею, откуда это все безобразие
[46:55.260 --> 47:02.900]  берется. Вот. А то, что нам нужно сейчас, это вот такая вот задача. Видно, что она становится сложнее. То
[47:02.900 --> 47:08.900]  есть, тут появляется вторая группа переменных. Это y. То есть, теперь есть не только x, по которым мы
[47:08.900 --> 47:15.380]  минимизируем, есть еще перемены y, по которым мы также минимизируем. И ограничения становятся более
[47:15.380 --> 47:23.180]  сложными. Это уже сумма двух. Вот. axby равно c. То есть, вот такой вот длинный вектор у меня появляется.
[47:23.180 --> 47:30.940]  На самом деле, если покопаться в применениях как раз ADMM метода, то становится понятно, откуда
[47:30.940 --> 47:38.220]  берутся вторые функции здесь. Ну, например, давайте я просто какую-нибудь так на скидку сделаю задачку,
[47:38.220 --> 47:43.500]  чтобы было понятно. Не знаю. Скорее всего, вы на семинарах даже сталкивались с чем-то таким в духе того,
[47:43.500 --> 47:48.660]  что возникала какая-то вторая группа переменных для минимизации, по которой нужно было
[47:48.660 --> 47:53.900]  минимизировать. Она возникает чаще всего искусственным образом. Ну, вот какая-нибудь классика
[47:53.900 --> 48:04.300]  машинного обучения. Функция потерь, линейная модель плюс регулиризатор. Я думаю, уже в некотором смысле
[48:04.300 --> 48:14.540]  вы с таким знакомы. Ну, что у нас тут? Действует просто. Data set умножается на x, получается линейная
[48:14.540 --> 48:19.740]  комбинация. Дальше штрафуете с реальными лейбами. Как вот на такую задачу можно посмотреть
[48:19.740 --> 48:25.820]  чуть по-другому. То есть, вообще, это, безусловно, задача. Вы ее, в том числе, в домашнем задании
[48:25.820 --> 48:34.220]  решали. Безусловно. Но что я могу сделать? Я могу сделать вот так вот. Я могу ввести дополнительные
[48:34.220 --> 48:43.700]  ограничения. Ну, давайте я вот сюда y напишу. Вот y, x у меня остается из Rd, y вводится новый,
[48:43.700 --> 48:52.780]  не обязательно, даже из Rd, давайте пусть будет Rn. Вот. И добавляется ограничение, что у меня y равен
[48:52.780 --> 49:03.780]  ax. Вот. Видно, да? Ну и, соответственно, появляется вот такая вот задача, которая написана здесь по
[49:03.780 --> 49:08.700]  факту из исходной задачи машинного обучения, что довольно просто. На самом деле, вот такого рода
[49:08.700 --> 49:14.940]  задачи, они возникают очень много где. То есть, там прямо целая книжка есть про то, как вот можно
[49:14.940 --> 49:21.820]  разные задачи переформулировать под вот такую, чтобы решать АДММ. Это один из примерчиков. И
[49:21.820 --> 49:28.380]  часто, действительно, y это просто вспомогательная группа переменных. Ну и тут, кстати, видно то,
[49:28.380 --> 49:36.020]  что у меня вот эти функции f и g, действительно, могут быть довольно дружественными. Потому что l и
[49:36.020 --> 49:43.300]  r, ну это какие-то, l это loss функция, r регулиризатор. Чаще всего это простые функции. Вот. Когда вот,
[49:43.300 --> 49:50.700]  соответственно, l уже идет в придачу вместе с матрицей A, там не так все хорошо. Вот. Понятно,
[49:50.700 --> 49:57.700]  что там за матрица A меняются свойства, а теперь мы как бы вытащили матрицу A в ограничение. Ну и,
[49:57.700 --> 50:02.740]  соответственно, у меня теперь l болтается одна, и я могу все ее хорошие свойства, она, например,
[50:02.740 --> 50:09.660]  тоже может быть обычная квадратичная там какая-то функция в духе там y-b в квадрате. Вот. Я
[50:09.660 --> 50:13.700]  ее могу использовать. Она действительно будет дружественной с точки зрения вычисления каких-то
[50:13.700 --> 50:27.140]  аргминемов по ней. Так. Это, соответственно, раз. Дальше агментацию добавляем. Пишем Lagrangian. Вот.
[50:27.140 --> 50:35.140]  Легко проверить, что такой Lagrangian порождает выпукловогнутую задачу, если у вас, давайте допишу,
[50:35.140 --> 50:45.380]  f и g выпуклые. f и g выпуклые. Вот. Получается выпукловогнутая седловая задача. Ну окей. Хорошо.
[50:45.380 --> 50:51.860]  Это, в принципе, мы с вами видели и для более простой задачи без агментации и без. Это проверять
[50:51.860 --> 50:58.980]  не будем. Можно как упражнение это сделать. Так. Так. И, соответственно, вот так вот выглядит метод
[50:58.980 --> 51:09.780]  ADMM. Та же самая идея, что в двойственном подъеме. По переменным минимизации как бы делаем минимум,
[51:09.780 --> 51:18.260]  вот. А потом делаем шаг спуска по переменной максимизации. Вот. Только вот здесь вот ключевая
[51:18.260 --> 51:24.300]  особенность, почему он и называется ADMM, что у вас здесь Alternating Direction. Alternating Direction
[51:24.300 --> 51:31.060]  методов multipliers. Multipliers понятно, потому что у вас двойственные множители. А Alternating Direction,
[51:31.060 --> 51:36.180]  потому что вот в двойственном методе у нас была минимизация по x, здесь у нас минимизация по x,
[51:36.180 --> 51:43.340]  y. Вот. И по факту, если переносить двойственный метод прям дословно, то вы должны вот вместо вот
[51:43.340 --> 51:48.380]  этих двух строчек, которые я вот здесь вот написал, вот этих двух строчек заменить их на одну строчку
[51:48.380 --> 51:57.420]  поиска минимума по x и по y одновременно. Вот. Но здесь поступают в некотором смысле и 3,
[51:57.420 --> 52:05.780]  упрощая задачу, вы сначала минимизируете по x при фиксированном y и лямбде, а потом минимизируете
[52:05.780 --> 52:12.540]  по y. Вот. И получается, вот это как-то называется альтернированная минимизация, когда у вас там
[52:12.540 --> 52:17.860]  функция зависит от двух групп переменных, и вы сначала по одной минимизируете, потом по другой,
[52:17.860 --> 52:21.660]  потом еще раз по одной, по другой. Но здесь просто сначала по одной, потом по другой,
[52:21.660 --> 52:26.780]  поэтому он и называется Alternating Direction. Потому что вот есть у вас два этих шага сначала по x,
[52:26.780 --> 52:39.340]  потом по y. Окей. Вот. Так вот выглядит АДММ. Немного страшная идея. Такая с точки зрения практики
[52:39.340 --> 52:46.380]  понятная. Вот. И действительно это классно работает. Сейчас попробуем доказать, как это все безобразие
[52:46.380 --> 52:54.540]  сходится. Вот. Буду доказывать чуть-чуть другую версию. Что-то у меня не схлапывалось вчера
[52:54.540 --> 53:00.580]  от доказательства предыдущего алгоритма. Я чуть-чуть поменял строчки. У меня до этого в предыдущем
[53:00.580 --> 53:07.700]  алгоритме x вот здесь вот был. Вот. Я сначала по x делал минимизацию, потом по y, потом делал шаг по
[53:07.700 --> 53:13.340]  лямбде. Здесь, ну на самом деле ничего особо и не меняется. Я сначала по y делал минимизацию,
[53:13.340 --> 53:19.460]  потом шаг по лямбде. Вот. Потом минимизацию по x. Но это же по факту то же самое. Если я цикл запущу,
[53:19.460 --> 53:27.060]  вот у меня что прошло? y, лямбда, x. Окей. Но я же могу как говорить? Вот так. Сейчас, например,
[53:27.060 --> 53:33.540]  нахожусь на минимизации x, перехожу на следующую итерацию, буду уже минимизировать по y и потом
[53:33.540 --> 53:38.260]  по лямбде. Но это ровно то же самое, что было. Это только в начальных итерациях будет работать
[53:38.260 --> 53:44.660]  чуть по-другому, потому что вы сначала обновляете y, потом лямбда, потом x. Но по факту, если правильно
[53:44.660 --> 53:50.260]  инициализировать там предыдущий алгоритм или этот по-другому инициализировать, то они совпадут.
[53:50.260 --> 53:55.580]  Я просто чуть-чуть поменял порядок, но суть-то осталась та же. Вот. Какие действия, как цикли
[53:55.580 --> 54:00.620]  выполнять, тут особо это не важно. Что у меня просто не получалось с предыдущим, здесь получилось
[54:00.620 --> 54:05.500]  полегче. Вот. Покрасивше доказательства. Давайте я вам вот это оставляю. В принципе,
[54:05.500 --> 54:09.140]  это в презентации у вас есть, и мы сейчас пойдем доказывать. То есть, метод видите,
[54:09.140 --> 54:20.220]  Lagrangian видите. Потому что на слайдах у меня этого не доказательства нет. Будем, соответственно,
[54:20.220 --> 54:27.860]  все это доказывать сейчас в режиме реального времени. Поехали. Давайте что-нибудь поделаем.
[54:27.860 --> 54:32.380]  Давайте вспомним вообще, что происходило в доказательстве, ну вот, например, какого
[54:32.380 --> 54:40.460]  текстра градиентного метода или градиентного спуска. Но у нас же вот что-то вылезало в духе f от x,
[54:40.460 --> 54:48.860]  ну там kt, например, xkt минус x звездой, ну или здесь просто x стояло, а справа было что-то в духе
[54:48.860 --> 55:00.660]  xkt минус x в квадрате минус xkt плюс 1 минус x в квадрате. Так? Ну, в качестве оператора f у вас там
[55:00.660 --> 55:10.900]  либо был просто градиент f от xkt, либо градиент по x, антиградиент по y, например. Ну вот,
[55:10.900 --> 55:18.540]  что-то вот такое, да? Было. Теперь вот хочется получить что-то вот то же самое. Вот такого рода
[55:18.540 --> 55:28.940]  результат, похожий вот на вот это, но для DMM метода. Ну, соответственно, что? Если не знаете,
[55:28.940 --> 55:34.020]  что делать, в принципе, для градиентного спуска это тоже в стратегии работает. Берете строчку и
[55:34.020 --> 55:38.860]  пишете для нее условия оптимальности. Просто для градиентного спуска там условия оптимальности
[55:38.860 --> 55:44.740]  считаете записаны за вас, а тут, видите, вы аргуминиум должны искать. Должны искать аргуминиум.
[55:44.740 --> 55:54.860]  Вот, у нас в первом случае по y, да. По y давайте писать условия оптимальности для вот строчки
[55:54.860 --> 56:04.260]  с y. Эта строчка номер два. Как она будет выглядеть, кто понимает условия оптимальности? Градиент равен нулю, алгорит
[56:04.260 --> 56:24.820]  перед вами. Так. Так, на обложе от y, дальше что? Там еще РО болтается, вроде
[56:24.820 --> 56:33.580]  как? Нет? Или не болтается? А, там с лямбдой? Да-да-да-да-да. На обложе, да, все правильно. Сначала,
[56:33.580 --> 56:48.500]  дальше что у нас там? B транспонированная лямбда, все правильно. Дальше, дальше с РО поехали, да. РО. Что
[56:48.500 --> 56:58.660]  там будет? С РО. Кто понимает? Там квадратичная просто задачка, как из нее градиент быстро взять.
[57:06.660 --> 57:12.700]  B транспонирована бы скакивать из скобки, как множитель просто перед y. А дальше скобка
[57:12.700 --> 57:21.460]  просто остается. А в скобке у нас, соответственно, ax плюс by минус c. Это просто условия оптимальности
[57:21.460 --> 57:30.820]  записано, только тут единственное, что нужно выставить, это написать альфа каты, x каты. Вот. Но, и оно
[57:30.820 --> 57:37.860]  все равно нулю. Все понимают, что произошло. Я просто для строчки, где выбираем argmin по y,
[57:37.860 --> 57:43.140]  написал условия оптимальности для этой задачи, приравнял градиент к нулю. Но это же условия
[57:43.140 --> 57:48.180]  эквивалента тому, что как раз оно выполняется для y катова плюс один, так как он же argmin у нас.
[58:00.420 --> 58:06.100]  Согласны? Так. Дальше по лямди. По лямди там довольно просто, потому что там,
[58:06.100 --> 58:12.340]  видите, там все записано, я просто вот так и оставлю. Давайте я выпишу вот так вот. Лямда к плюс
[58:12.340 --> 58:27.100]  один минус лямда к равно ro axk плюс by к плюс один минус c. Так. И аналогичную вещь мне
[58:27.100 --> 58:35.380]  надо еще записать для x. Там по x мы тоже argmin берем, поэтому и записываю для него f xk
[58:35.380 --> 58:46.380]  плюс один. Так. Дальше соответственно что? Будет выскакивать а транспонированная лямда
[58:46.380 --> 58:58.940]  к плюс ro а транспонированная axk плюс бета. Так, только тут надо не ошибиться. Лямда к плюс один там.
[58:58.940 --> 59:08.700]  Y к плюс один минус c равно 0. Все? Понятно здесь, да? Просто я выписал для каждой строчки условия
[59:08.700 --> 59:13.740]  оптимальности. Для строчки с лямдой я просто вообще там ничего не выписал, приравнял. Аргумен?
[59:13.740 --> 59:27.060]  А, да, да, да. Вот. Здесь окей? Так. И теперь вот хочу чуть-чуть поиграться. Давайте мне для
[59:27.060 --> 59:33.140]  этой задачки, чтобы привести к тому виду, который у меня наверху, нужно найти вот оператор f.
[59:33.140 --> 59:42.460]  Оператор f, который там, ну давайте я от z буду обозначать, z у меня пусть будет это вектор x,
[59:42.460 --> 59:52.020]  y, лямда. Вот. Оператор f в данном случае это просто градиент по x, так градиент по y и
[59:52.940 --> 01:00:02.100]  антиградиент по лямду. Это ровно то же самое, как мы с вами делали для экстраградиентного метода. Вот.
[01:00:02.100 --> 01:00:13.100]  Там потом, когда вы эти градиенты подставите, вот вы для этого потом просто выпуклостью воспользуетесь,
[01:00:13.100 --> 01:00:18.140]  или выпуклостью-вогнутостью, у вас там все красиво будет схлопываться, будете получать значение
[01:00:18.140 --> 01:00:24.940]  просто разности функций. Вот. Окей. Здесь я выражаю, что градиент по x это будет у меня,
[01:00:24.940 --> 01:00:40.220]  что градиент f от x плюс a транспонированная лямда. Хорошо. Так. И что-то еще. А, давайте я вот так
[01:00:40.220 --> 01:00:50.580]  еще. Давайте тут без rho. Вот давайте пусть будет обычный Lagrangian, без rho. Вот так. По лямдию
[01:00:50.580 --> 01:00:58.140]  у меня будет чему равен градиент? Это будет просто ax плюс bi, а нет, это по y, по y у меня надо
[01:00:58.140 --> 01:01:07.260]  сначала по y, там будет nabla g от y плюс b транспонированная лямда. И по лямдии антиградиент это
[01:01:07.260 --> 01:01:16.580]  минус ax плюс bi минус c. Вот. Важная деталь, что здесь я убрал вот эту рожку и смотрю на Lagrangian
[01:01:16.580 --> 01:01:23.740]  без рожки. Потому что по факту рожка, как мы понимаем, она в реальности на решение исходной
[01:01:23.740 --> 01:01:30.180]  задачи не влияет. А мы когда делаем именно, решаем этот Lagrangian, мы ведь не штраф вводим,
[01:01:30.180 --> 01:01:36.580]  то есть мы реально решаем исходную задачу и хотим получить решение исходной задачи. Вот. Я
[01:01:36.580 --> 01:01:40.740]  убрал здесь рожку. Ну и, соответственно, да, мне хочется теперь, видимо, получить,
[01:01:40.740 --> 01:01:47.820]  так как вот здесь вот у меня возникают f от xкат и градиент, g yкат и плюс один. Но я хочу,
[01:01:47.820 --> 01:01:55.660]  видимо, получить f от zкат и плюс один. Вот. Вот его скалярное произведение на что-то. Лучше вот
[01:01:55.660 --> 01:02:06.460]  на что-то вот такое. Так. Хорошо. Хорошо. Давайте попытаемся это сделать как-то по группиру,
[01:02:06.460 --> 01:02:14.060]  перегруппируем. Вот. В принципе, вот для x уже что-то вырисовывается. Такое вот хорошее. f
[01:02:14.060 --> 01:02:28.380]  xкат плюс один плюс а транспонированная лямбда ка плюс один. Так вот равно. Так. Равно
[01:02:28.380 --> 01:02:42.180]  ро а транспонированные а икс один плюс б у ка плюс один минус ц. Так.
[01:02:42.180 --> 01:03:00.100]  Дальше мне нужно получить по y. По y у меня g y ка плюс один. Но вот тут с b не совпадает. Тут
[01:03:00.100 --> 01:03:07.460]  не лямбда ка плюс один стоит. Вот. Поэтому я добавлю вычту. Вот здесь поставлю b транспонированная
[01:03:07.460 --> 01:03:14.260]  лямбда ка плюс один. Но вот тогда справа мне нужно будет это компенсировать. Я все еще напишу
[01:03:14.260 --> 01:03:27.900]  ро. Ро у меня точно это остается. Ро b транспонированная а икс ка плюс б у ка плюс один минус ц. Вот. И
[01:03:27.900 --> 01:03:36.900]  тут нужно добавить и вычесть. То есть вот b транспонированная лямбда ка плюс один минус лямбда ка. Вот.
[01:03:36.900 --> 01:03:46.900]  Это компенсация за то, что я вот здесь вот поменял точку. Справа да да справа давайте вот здесь
[01:03:46.900 --> 01:04:04.180]  поставлю тогда вот минус вот сюда. Вот. Хорошо. Так. И мне осталось чуть-чуть опять же поиграться вот
[01:04:04.180 --> 01:04:11.140]  с этим. С градиентом по лямбда. Чтобы у меня был антиградиент по лямбда. А это должно у меня
[01:04:11.140 --> 01:04:23.740]  вылезти соответственно минус а икс ка плюс один. Плюс в скобочках это все так. Ага. И теперь это
[01:04:23.740 --> 01:04:39.100]  так так так так так. Что там вылезет один делить на ро. Лямбда ка плюс один минус лямбда ка.
[01:04:39.100 --> 01:04:50.900]  Так вроде все норм и соответственно из-за того что я вот здесь менял точку мне ее нужно компенсировать.
[01:04:50.900 --> 01:05:13.180]  Вроде бы. Правильно. Так. Так. Хорошо. Вот такого. Какое-то вот такое вот у меня выражение получилось.
[01:05:13.180 --> 01:05:20.940]  И смотрите какую вот тут замечается такая особенность. Такая особенность которая на
[01:05:20.940 --> 01:05:26.780]  самом деле возникает в градиентном спуске. Мы просто это так не расписывали. Тут вылезают разности
[01:05:26.780 --> 01:05:33.860]  соседних значений. Вылезают разности соседних значений. Вот единственное что вот вот с этими
[01:05:33.860 --> 01:05:38.940]  выражениями до конца не понятно что делать. Потому что там пока вот такое вот такая красота не
[01:05:38.940 --> 01:05:47.620]  вылезла. Но на самом деле ее можно сейчас повытаскивать. Этим мы и займемся. Ну давайте
[01:05:47.620 --> 01:06:05.700]  сейчас как мы это сделаем. Как мы это сделаем. Споргалку свою гляну. Все понял. Так. Делается
[01:06:05.700 --> 01:06:15.240]  следующее. Делается следующее. Смотрите. У нас мы знаем что лямбда к плюс один минус лямбда к это
[01:06:15.240 --> 01:06:29.540]  РО АХК плюс БЙК плюс один минус С. Вот. А здесь то по факту это же и возникает только ущел
[01:06:29.540 --> 01:06:39.860]  умноженное на B транспонированное. Поэтому здесь меняемся просто. Так да меняем вот этот выражение на
[01:06:39.860 --> 01:07:00.420]  лямбда к плюс один минус лямбда к. Вот. Так. На РО. Так так так. А убрать РО да. Убрать РО. Все. Поменял.
[01:07:00.420 --> 01:07:07.140]  Осталось верхнее выражение. Там чуть чуть более проблемно конечно. Там опять нужно чуть-чуть
[01:07:07.140 --> 01:07:20.140]  точку поменять. ХК плюс один плюс БЙК плюс один минус С. И тут из-за того что я точку поменял у меня РО АХК
[01:07:20.140 --> 01:07:30.060]  минус ХК плюс один. Вот. Вот такое вот выражение. По факту у меня вот это там стоит умножить на
[01:07:30.060 --> 01:07:34.220]  транспонированные. А мне нужно будет соответственно это все поменять на
[01:07:34.220 --> 01:07:53.780]  А транспонированные лямбда к плюс один минус лямбда к минус А транспонированные РО АХ минус ХК плюс один.
[01:07:54.780 --> 01:07:55.780]  Согласны?
[01:07:55.780 --> 01:08:04.140]  Ну вот что-то вот такое. Что-то вот такое. Вся соль теперь вы видите у меня везде выскакивают вот эти
[01:08:04.140 --> 01:08:15.940]  разности. Вот эти разности. Давайте я это перенесу на следующую страничку. И мы с этим красиво обойдемся.
[01:08:15.940 --> 01:08:17.380]  Красиво обойдемся.
[01:08:34.380 --> 01:08:42.620]  Вот. Я хочу вот это все безобразие записать в виде матрицы. Я хочу это все записать в виде матрицы.
[01:08:42.620 --> 01:08:56.700]  Так. Ну давайте попробуем это сделать. Так. Матрица П и атона умножается на вектор ХК плюс один
[01:08:56.700 --> 01:09:06.980]  минус ХК на вектор YК плюс один минус YК на вектор лямбда к плюс один минус лямбда к. Вот. Надо
[01:09:06.980 --> 01:09:24.580]  найти сейчас вот эту матрицу. Найти вот эту матрицу. Ну давайте искать. Так. Ну что у меня тут?
[01:09:24.580 --> 01:09:32.740]  Как у меня все безобразие умножается? В первой строчке у меня вылезает лямбда и вылезает разница
[01:09:32.740 --> 01:09:44.060]  по Х. С Х она вылезает положительная, а транспонированная АРО. Так. Дальше по Y ничего там не вылезает,
[01:09:44.060 --> 01:09:53.500]  по лямбдом вылезает отрицательная. А транспонированная. Во второй строчке у меня лезет только разница по
[01:09:53.500 --> 01:10:06.060]  лямбдом. Да? По лямбдом. Я правильно сейчас записал? Там за нуляется. Ну ладно, пока напишем,
[01:10:06.060 --> 01:10:16.460]  как есть. Ноль, ноль, по лямбдом. Там получается два бета транспонированное с минусом. И в последней
[01:10:16.460 --> 01:10:25.660]  строчке у меня опять вылезает А, ноль, один делить на РО и на единичную матрицу. Так. Вот что-то типа такого,
[01:10:25.660 --> 01:10:31.380]  что-то типа такого. Кажется, я где-то вот накосипорил, вот здесь должен быть ноль. Вот где-то я со
[01:10:31.380 --> 01:10:48.380]  знаком напутал. Где-то я чуть-чуть напутал со знаком тут. Так. Это я откуда убрал? Для Y я записывал,
[01:10:48.380 --> 01:10:59.740]  подставил. Здесь, зря стер, конечно. Так, подставил. Ну вот здесь, видимо, где-то минус должен быть.
[01:10:59.740 --> 01:11:09.100]  Потерял этот минус. Вот этого множителя не должно быть. Так, ладно. Поверим нас, что тут ноль.
[01:11:09.100 --> 01:11:17.700]  Получается вот такая матрица. Получается вот такая матрица. И на самом деле это уже,
[01:11:17.700 --> 01:11:24.540]  считайте, финал доказательства. Вот. Потому что что у нас получилось? У нас получилось, что f от x
[01:11:24.540 --> 01:11:37.020]  ка плюс один. Ну это очень похоже на то, что мы всегда получали так. Равно p на разницу. Давайте только
[01:11:37.020 --> 01:11:51.580]  не x, а z. Тут z у меня. Полный этот вектор. Это вектор из х, у и лямб. Вот. Как раз у меня вот это f
[01:11:51.580 --> 01:11:58.140]  z ка плюс один. Он равняется, как мы поняли, матрицу умножить на просто на превращение z ки.
[01:12:04.140 --> 01:12:12.300]  При а, вот здесь, да? Во, правильно. Это правильно. Вот. Супер. Так. И здесь я еще должен скалярно
[01:12:12.300 --> 01:12:23.620]  умножить на z ка минус z. Вот. Хорошо. Хорошо. Так. Дальше что делается? Дальше что делаем? Дальше вот
[01:12:23.620 --> 01:12:30.260]  это вот то, что у меня написано, расписываем чуть похитрее. Ну то есть это вот на самом деле скалярное
[01:12:30.260 --> 01:12:36.380]  произведение, вот которое второе, оно просто выражается через нормы. Ну точнее, такие немного
[01:12:36.380 --> 01:12:45.300]  хитрые нормы. Вот. Давайте я запишу выражение, и мы просто увидим, что оно действительно выполняется.
[01:12:45.300 --> 01:13:03.380]  z ка минус z на p. Так. Минус одна вторая, z ка плюс один минус z, p, z ка плюс один минус z,
[01:13:03.380 --> 01:13:13.740]  и здесь соответственно минус одна вторая, да? Все правильно. z ка минус z ка плюс один,
[01:13:13.740 --> 01:13:25.980]  p, z ка минус z ка плюс один. Вот. Договорились? Договорились. Давайте посмотрим, что это
[01:13:25.980 --> 01:13:37.860]  действительно так, что тут я вам нигде сильно не наврал. Так. Какого рода тут вообще слагаемые могут
[01:13:37.860 --> 01:13:48.140]  встречаться? z на p на z. Вот. Было ли оно у нас до этого? Такого у нас до этого не было. Но оно и
[01:13:48.140 --> 01:13:59.100]  уничтожится. Вот оно соответственно уничтожается. Вот здесь вот. Так. Вот. Времени просто не так
[01:13:59.100 --> 01:14:03.980]  много остается. Вот давайте вот это утверждение, оно просто в качестве упражнения. Тут действительно
[01:14:03.980 --> 01:14:11.340]  ничего такого сверхъестественного, это нужно просто раскрыть аккуратненько. Вот. Смотрите,
[01:14:11.340 --> 01:14:19.940]  смотрите, что еще хорошо, вот эта матрица положительно-определенная. Вот. Тоже можно
[01:14:19.940 --> 01:14:24.860]  спокойно проверить, что она действительно является положительно-определенной. Ну что у вас там,
[01:14:24.860 --> 01:14:31.500]  если эти нули убрать, вот этот крест нулевой, у вас останется матрица 4 на 4, а транспонированная а
[01:14:31.500 --> 01:14:38.380]  минус а транспонированная а. Ну этого ноль, да, в детерминате. Минора у нее положительная. Вот. Она
[01:14:38.380 --> 01:14:43.220]  действительно положительно-определенная, а значит с помощью вот этой матрицы можно индуцировать
[01:14:43.220 --> 01:14:50.660]  норму. Вот. И она на самом деле здесь индуцирована. Вот. Потому что, видите, вот здесь вот. Вот это
[01:14:50.660 --> 01:14:57.100]  выражение, которое написано. Вот это. Я же могу, например, взять корень из матрицы и вот так вот ее
[01:14:57.100 --> 01:15:03.260]  раскидать. Она еще и симметрична у меня. Вот. И сделать что-то типа такого. То есть получается,
[01:15:03.260 --> 01:15:11.020]  что это просто необычная евкридовая норма, а такая норма, которая порождена матрицей. Вот. Обозначается
[01:15:11.020 --> 01:15:16.700]  это вот так вот, это ее просто определение. Матричная норма, например, вот. Которая порождена матрицей
[01:15:16.700 --> 01:15:27.820]  а. Норма порожденной матрицы. Это вот просто ax. Так вот. Вот. На эту матрицу мы просто умножаем. Вот.
[01:15:27.820 --> 01:15:34.600]  это оно и есть. И здесь соответственно в чем мне вылезает zk минус z по норме p в квадрате
[01:15:34.600 --> 01:15:47.160]  минус zk плюс один минус z по норме p в квадрате и минус zk плюс один минус zk в квадрате по
[01:15:47.160 --> 01:15:58.040]  норме p. Окей? Вот. Ну вот такое вот выражение получается. То есть видно, что уже получили
[01:15:58.040 --> 01:16:06.040]  что-то очень такое прям то, что хотелось. Вот это оператор, столярное произведение оператора на
[01:16:06.040 --> 01:16:14.240]  точке, на нужденном разности, а здесь разность норм, разность норм, то есть там kt минус k плюс
[01:16:14.240 --> 01:16:20.880]  первая и что какая-то вообще неположительная, которую мы можем просто убрать и сказать,
[01:16:20.880 --> 01:16:27.120]  что нет. Потому что оценка же сверху пишется. Вот. И я могу здесь просто поставить знак меньше
[01:16:27.120 --> 01:16:34.400]  либо равно. Вот. Все. То есть получили то, что мы обычно получаем. Так. Дальше. Дальше делаются
[01:16:34.400 --> 01:16:42.840]  стандартные шаги. Кто помнит, как там вот f превращалась для седловых задач в разности? Кто
[01:16:42.840 --> 01:16:50.840]  помнит? Что там нужно было сделать, когда мы экстраградиент доказывали? Вот как мы дальше
[01:16:50.840 --> 01:17:00.600]  расписывали вот этот для седловой задачи вот эту f? Да. По выпукловогнутости. То есть мы говорили,
[01:17:00.600 --> 01:17:08.160]  что у нас по x было выпукло, поэтому там возникнет разность нужная. Так. Ну давайте я здесь тогда
[01:17:08.160 --> 01:17:16.040]  напишу l, x. Давайте я вот так вот, для вот этой я напомню, как делается. Вот для нашего случая
[01:17:16.040 --> 01:17:28.360]  там аналогичность. Все это проделывается. x, x. Давайте kt и лямбда, kt, kt, kt, kt, x. Здесь лямбда,
[01:17:28.360 --> 01:17:38.840]  kt, лямбда. Вот у нас вот что-то вот такое. И мы, соответственно, это снизу. Думаю, просто. И мы снизу
[01:17:38.840 --> 01:17:43.640]  это оцениваем по выпуклости. По x у нас функция выпукло. Как мы это можем оценить?
[01:17:43.640 --> 01:18:04.120]  Как l, x, kt, лямбда, kt, минус лямбда, x, лямбда, k. Так, это просто выпуклось по x. Я первую строчку
[01:18:04.120 --> 01:18:18.280]  расписал. Так. Вогнутость по y, вогнутость по, ой, не по y, а по, соответственно, лямбде. По лямбде
[01:18:18.280 --> 01:18:34.400]  она что даст мне? x, kt, лямбда. Так, минус x, kt, лямбда, k. Это вогнутость. Вот здесь вот. Вогнутость.
[01:18:34.400 --> 01:18:42.480]  Выпуклость. Вот. И что в итоге это дает? Это дает то, что у вас уничтожаются вот эти кусочки. Остается l,
[01:18:42.480 --> 01:18:52.600]  x, лямбда, k, с минусом плюс l, x, k, лямбда. Вот. А дальше мы как? Максимизировали по лямбде,
[01:18:52.600 --> 01:19:02.920]  минимизировали по x и получали критерий gap. Да? Зазор. Окей? Согласны? Кто помнит, что мы действительно
[01:19:02.920 --> 01:19:08.440]  так делали, когда доказывали там для экстра градиента? Вот. И здесь то же самое. У вас возникает вот
[01:19:08.440 --> 01:19:18.040]  это же выражение здесь. Вот оно. Вы его расписываете. У вас получается здесь l по переменной
[01:19:18.040 --> 01:19:26.640]  максимизации здесь x, kt, y, kt. Только с плюсом. Плюс один. Плюс один. Лямбда. И, соответственно,
[01:19:26.640 --> 01:19:35.760]  здесь l, x, y, лямбда, k, плюс один. Вот. Это слева, справа. Что вы, соответственно, дальше делаете?
[01:19:35.760 --> 01:19:46.880]  Суммируете. Суммируете. По всем k. От нуля до k минус 1. Внизу все уничтожается, кроме нулевого и
[01:19:46.880 --> 01:19:52.120]  последнего. Последний с отрицательным знаком. Соответственно, его просто убираем. Остается вот
[01:19:52.120 --> 01:20:00.240]  такая вот разность z нулевого минус z в этой норме. Раскрываете, соответственно, по эту норму,
[01:20:00.240 --> 01:20:05.520]  по определению. Все. И все красиво получается. Получается сходимость. Здесь у вас тоже все
[01:20:05.520 --> 01:20:12.240]  суммируется. По янсону, по выпуклости, вогнутости вы приходите к средней точке. Вот. Показываю теорему,
[01:20:12.240 --> 01:20:21.560]  как выглядит. Вот она средняя точка. Вот она. Z квадрате. Матрица та же самая. Матрица,
[01:20:21.560 --> 01:20:29.600]  я знал, как выглядит. У меня был этой беты. Lagrangian нулевой, то есть без рожки. Понятно, да? Ну и
[01:20:29.600 --> 01:20:36.360]  кратенько про ADMM. Это действительно метод, который является ключевым во многих солверах, в том числе,
[01:20:36.360 --> 01:20:43.040]  который вы будете проходить в рамках этого курса. Более того, во многих солверах он является
[01:20:43.040 --> 01:20:50.560]  дефолтным. То есть, если вы не указываете конкретный, он использует ADMM. Связано это с тем, что вот та постановка,
[01:20:50.560 --> 01:20:58.000]  которая вроде кажется не особо козистой, с X, с Y, она на самом деле действительно включает
[01:20:58.000 --> 01:21:07.280]  в себя очень много частных случаев. Вот. И поэтому ADMM реально покрывает большой спектр задачек,
[01:21:07.280 --> 01:21:11.560]  которые необходимо решать на практике. Именно вот такой вот выпуклой оптимизации с ограничениями.
[01:21:11.560 --> 01:21:18.480]  Вот. Ну и такая, что ли, ключевая его особенность, то что вот здесь вот штраф-то он все же не как
[01:21:18.480 --> 01:21:23.600]  вот метод из штрихных функций. Это прям так необходимая вещь, которую нужно устремлять в
[01:21:23.600 --> 01:21:28.000]  бесконечности и делать большим. Здесь это вспомогательная вещь для ускорения сходимости.
[01:21:28.000 --> 01:21:34.160]  Потому что по факту вы решаете задачу Lagrange, вы решаете седловую задачу и вы могли бы ее решить
[01:21:34.160 --> 01:21:39.320]  и без вот этого дополнительного штрафа. Штраф это регулиризатор здесь, бесплатный регулиризатор,
[01:21:39.320 --> 01:21:47.040]  который не портит саму задачу, но улучшает именно сходимость на практике. Вот. Вот это вот такая
[01:21:47.040 --> 01:21:52.880]  ключевая особенность и отличие от тех штрафов, которые мы заработали с вами в начале лекции.
[01:21:52.880 --> 01:21:57.200]  Все, спасибо. В конце немного сумбурненько, ну да ладно. Все.

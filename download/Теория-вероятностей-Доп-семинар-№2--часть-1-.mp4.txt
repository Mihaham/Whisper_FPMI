[00:00.000 --> 00:10.360]  Ну давайте начнем. Я Кате в общем сбросил задачки, которые мы хотим сегодня разобрать,
[00:10.360 --> 00:16.560]  ну и как обычно, наверное, разберем какую-нибудь теорию, чтобы задачке было проще решать и чтобы
[00:16.560 --> 00:24.360]  понималось, что происходит вообще в целом. Вот. Сегодня у нас в меню характеристические функции,
[00:24.360 --> 00:31.440]  сходимости и предельные теоремы, гауссовские векторы и условные математические ожидания. Так,
[00:31.440 --> 00:36.720]  еще скажу, что вот тот вариант, который я скинул Кате, это пробный вариант прошлого года. Вот.
[00:36.720 --> 00:41.520]  Вряд ли будет что-то сильно от меня отличаться в этом году. Вот. Но пробного варианта этого года
[00:41.520 --> 00:47.800]  пока что еще нет. Вот. Поэтому ориентируемся на пробный вариант прошлого года. Собственно,
[00:47.800 --> 00:51.960]  ну давайте начинать тогда. Начнем с вами с характеристической функции.
[00:51.960 --> 01:11.040]  Собственно, начнем с определения, наверное. Итак, пусть у нас есть какая-нибудь случайная
[01:11.040 --> 01:17.400]  величина, тогда ее характеристическая функция — это следующий объект.
[01:17.400 --> 01:29.560]  Математическое ожидание E в степени и ксито. Вот. Возможно, не совсем понятно, как интегрировать
[01:29.560 --> 01:35.320]  комплексные функции. Ну вот, здесь можно воспользоваться формулой Эйлера и свести это
[01:35.320 --> 01:48.680]  уже к обычным вещественным интегралам. Вот. И тут уже никакой магии нет. Обычные матожи,
[01:48.680 --> 01:55.320]  которые мы учились считать в первой половине семиастра. Отлично. С определением разобрались.
[01:55.320 --> 02:00.920]  Утверждение — для любой случайной величины существует характеристическая функция. Ну,
[02:00.920 --> 02:10.880]  кажется, что это правда. Потому что, заметим, следующая. Модуль характеристической функции
[02:10.880 --> 02:18.400]  меньше либо равен единичке, ну так как это комплексные экспоненты. Здравствуйте. Вот. Так
[02:18.400 --> 02:23.760]  что эта функция точно интегрируема и никаких проблем вот здесь вот у вас не будет. Так что
[02:23.760 --> 02:30.960]  характеристические функции существуют всегда. Отлично. Ну и давайте что-нибудь тогда посчитаем.
[02:30.960 --> 02:37.520]  Посчитаем пару примеров дискретных и, наверное, пару примеров непрерывных. Начнем с дискретных
[02:37.520 --> 02:51.040]  примеров. Первый пример. Си равняется константе С. Вот. Как выглядит характеристическая функция
[02:51.040 --> 03:13.600]  такой случайной величины? Ладно. Наверное, вопрос сложный. Ответ — Е в степени И, Ц, П. Ну да. Это
[03:13.600 --> 03:18.920]  у нас выстрелит чуть позже. Просто давайте вот какие-то базовые вещи тоже повторим. Пусть у нас
[03:19.040 --> 03:27.400]  С имеет Бернулевское распределение с параметром П. Тогда характеристическая функция такой случайной
[03:27.400 --> 03:41.600]  величины будет Е в степени И, так Т на П плюс 1 минус П. Вот. Ну действительно у нас случайная
[03:41.600 --> 03:46.320]  величина принимает два значения 1 и 0. Здесь мы подставляем вместо С единичку вероятность единички
[03:46.320 --> 03:50.520]  это П. Потом здесь у нас такая же экспонента получается, но мы в нее нолик подставляем,
[03:50.520 --> 03:55.360]  оно в единичку превращается и умножаем на вероятность нолика 1 минус П. Вот. Что-то
[03:55.360 --> 04:03.000]  понятное. Ну давайте теперь что-нибудь посложнее. Пусть у нас С имеет нормальное распределение с
[04:03.000 --> 04:08.560]  параметром 0.1. То бишь стандартное нормальное распределение. Давайте посчитаем ее характеристическую
[04:08.560 --> 04:17.160]  функцию. Вот. Ну здесь воспользуемся с вами формулой Эйлера сразу.
[04:28.800 --> 04:34.520]  Получается что-то вот такое. Заметим, что плотность у нас симметрична относительно 0,
[04:34.520 --> 04:40.960]  поэтому четная. Поэтому вот эта функция вот здесь под интегралом будет нечетная. Соответственно
[04:40.960 --> 04:49.600]  интеграл это нечетная функция. Ну и в общем вот это слагаемое зануляется. Так что у нас остается
[04:49.600 --> 05:00.840]  только один интегральчик. Вот этот вот. Перепишем его как-нибудь в интегральной форме. Домножим на
[05:00.840 --> 05:03.800]  плотность по определению математического ожидания в абсолютно непрерывном случае.
[05:04.520 --> 05:16.000]  Вот. Ну и дальше утверждается, что такие интегралы вы умеете считать. Там можно либо по частям
[05:16.000 --> 05:25.800]  посчитать, либо брать производную того, что под интегралом. В общем утверждение фиксия Т это
[05:25.800 --> 05:39.880]  Е в степени минус Т в кладате пополам. Вот. Какие-то выкладки отпустим. Отлично. Ну смотрите,
[05:39.880 --> 05:44.920]  в общем случае у нас характеристическая функция, она как бы комплекснозначная. Ну потому что вот у
[05:44.920 --> 05:51.680]  вас есть действительная часть и вот есть мнимая часть. А вот здесь у нас получилась характеристическая
[05:51.680 --> 05:57.720]  функция, которая принимает только действительные значения. Вот. А существует ли какой-то вот как бы
[05:57.720 --> 06:03.040]  признак того, что у вас характеристическая функция будет действительно значной? Существует. Это
[06:03.040 --> 06:14.920]  вот свойство. Так. Здесь неудобно будет. Ну ладно. Свойство. Давайте его назовем два. Это было,
[06:14.920 --> 06:28.880]  если что, свойство один. Вот. Свойство два о чем гласит. Что если у вас распределение симметрично,
[06:28.880 --> 06:40.920]  симметрично распределена случайно величина, то характеристическая функция ее
[06:40.920 --> 06:54.680]  бесчастно значна, обратно верно так же. Вот. Ну доказывается это очень просто на самом деле. В
[06:54.680 --> 07:01.480]  первом случае, то есть давайте стрелочку сначала одну какую-нибудь докажем. Пусть у вас распределение
[07:01.480 --> 07:07.840]  симметрично, то есть стрелочка вправо. Просуждения абсолютно такие же, как и с нормальным распределением
[07:07.840 --> 07:13.120]  с параметрами 0,1. У вас просто мнимая часть зумляется, потому что у вас там будет интегральчик от
[07:13.120 --> 07:25.080]  нечетной функции. То есть в эту сторону давайте считать, что доказали. Вот. Давайте покажем теперь
[07:25.080 --> 07:33.640]  это в обратную сторону. Соответственно, что мы хотим с вами заметить? Первое, давайте мы с вами заметим
[07:33.640 --> 07:44.760]  вот такое равенство. Это верно для всех характеристических функций и более-менее очевидно,
[07:44.760 --> 07:49.600]  ну потому что вы вот здесь вот если минус допишите, вы можете его либо к аргументу добавить,
[07:49.600 --> 07:56.120]  либо вот как бы приклеить к случайной величине. Поэтому вот это более-менее понятно, ну и
[07:56.120 --> 08:00.760]  линейное преобразование нас чуть позже также появится. Вот. Дальше мы с вами воспользуемся
[08:00.760 --> 08:18.320]  вот таким вот свойством. Это на самом деле свойство 3. Что у нас в общем о связи характеристической
[08:18.320 --> 08:27.040]  функции с сопряженным значением. Ну более-менее понятно, что если вы возьмете сопряжение
[08:27.040 --> 08:32.520]  характеристической функции, то у вас по сути поменяется знак вот при имнимой части. Вот.
[08:32.520 --> 08:39.360]  А соответственно, почему вот это равенство верно? Потому что вот этот минус, который у вас здесь
[08:39.360 --> 08:43.600]  появится, можно занести как бы под синус. Синус нечетная функция, поэтому минус у вас
[08:43.600 --> 08:49.360]  сюда внутрь залазит. А косинус четная, в него как бы можно минус тоже запихнуть и ничего не
[08:49.360 --> 08:53.280]  изменится. Соответственно, вот это равенство верно тоже для всех характеристических функций. Это
[08:53.280 --> 08:58.640]  свойство 3 вот проверяется просто по определению. Вот. Ну и последнее, что мы хотим заметить, так как у
[08:58.640 --> 09:04.560]  нас по условию характеристическая функция вещественна, сопряжение ее никак не меняет. То есть это
[09:04.560 --> 09:10.840]  фикси от m. Итого мы получили, что характеристическая функция случайной величины совпадает с
[09:10.840 --> 09:14.880]  характеристической функцией минус случайная величина. Вот. Соответственно, характеристические
[09:14.880 --> 09:21.280]  функции, их совпадают, значит распределение симметрично. Давайте так аккуратно скажем.
[09:21.280 --> 09:29.440]  Вот. Супер. Ну и соответственно, вот хороший вот пример, что если у вас стандартная нормальная
[09:29.440 --> 09:34.640]  случайная величина с параметром 0.1, она симметрична относительно нуля, и у нее действительно значенная
[09:34.640 --> 09:41.960]  характеристическая функция. Так, идем дальше. Дальше давайте еще посчитаем характеристическую
[09:41.960 --> 09:46.400]  функцию экспоненциального распределения, экспоненциальной случайной величины. Она нам сегодня
[09:46.560 --> 09:52.160]  тоже в задачке пригодится. Потом мы просто сошлемся на вот тот результат, который сейчас получим.
[09:52.160 --> 10:02.960]  Так. Куда мне ногу положил? Ну ладно. Итак, пусть кси у нас имеет экспоненциальное распределение
[10:02.960 --> 10:12.600]  с параметром лямбда. Хотим посчитать характеристическую функцию. Вот. Ну, здесь можно опять-таки
[10:12.600 --> 10:17.800]  формула эйлера воспользоваться. Можно немножко схитрить и сказать, что и это константы, и просто
[10:17.800 --> 10:23.080]  экспоненты поинтегрировать. Ну давайте в этот раз допустим так сделаем. У нас получается интегральчик
[10:23.080 --> 10:29.640]  е в степени xt на плотность экспоненциальной случайной величины. Плотность экспоненциальной случайной
[10:29.640 --> 10:39.520]  величины с параметром лямбда это лямбда е в степени минус лямбда х dx. Ну и там индикатор еще,
[10:39.520 --> 10:46.440]  поэтому от 0 до бесконечности. В плотности экспоненциального распределения зашит индикатор
[10:46.440 --> 10:54.320]  х больше либо равен нуля, поэтому мы его сразу вот в пределы интегрирования вписали. Вот. Ну что
[10:54.320 --> 11:01.680]  хотим заметить? Константы у нас выносятся из-под интеграла, и внутри у нас можно собрать экспоненты.
[11:01.680 --> 11:12.800]  Значит у нас х выносится, здесь будет ит минус лямбда dx. Вот. Ну и вот здесь давайте мы комплексный
[11:12.800 --> 11:17.240]  анализ конечно не знаем, но скажем, что и это такая же константа, как все остальные, с которыми мы
[11:17.240 --> 11:26.400]  здесь работаем, кроме ха, поэтому мы можем с вами вот на это все домножить х и соответственно поделить тоже.
[11:26.400 --> 11:43.640]  Ит минус лямбда, прошу прощения. Вот. И у нас получается, нужно посмотреть еще вот,
[11:43.640 --> 11:53.360]  чему у нас будет равен вот этот интегральчик. Ну интегрируем экспоненты у нас по идее получается
[11:53.360 --> 12:03.040]  то же самое в постановке от нуля до бесконечности. Вот. Ну в бесконечности,
[12:03.040 --> 12:15.000]  я здесь минус потерял или не потерял, минус лямбда х. Да, у нас х вот к минусу лямбда относится. В общем,
[12:15.000 --> 12:19.520]  бесконечности у вас в эту штучку зановится, потому что у вас вещественная часть к нулю стремится. Вот.
[12:19.520 --> 12:29.120]  А в нуле это будет единичка. Поэтому ответ, сверить, что я не ошибся. Вот здесь еще из формулы
[12:29.120 --> 12:36.080]  Ньютона Лебницы минус вылезет, поэтому минус мы можем к нам сюда закинуть и ответ вот такой. Вот.
[12:36.080 --> 12:52.040]  Этот результат нам пригодится сегодня еще. Ну мы только начали. Еще не вечер. Я думаю,
[12:52.040 --> 13:01.280]  скоро мы дойдем до какого-нибудь подобия катарсиса. Минут через 10 там будет что-то интересное. Вот. Но
[13:01.280 --> 13:05.920]  пока что все, что мы с вами научились сделать, это просто считать какие-то характеристические
[13:05.920 --> 13:10.120]  функции стандартных распределений, которые у нас до этого встречались. Давайте какие-нибудь
[13:10.120 --> 13:34.440]  дальше свойства исследуем. Так, это я стирать пока не буду. Давайте посмотрим на характеристическую
[13:34.440 --> 13:40.000]  функцию линейного преобразования. Вот пусть у вас есть случайная величина кси, и вы знаете,
[13:40.000 --> 13:47.880]  что ей соответствует характеристическая функция фи кси от н. Вопрос. А как будет
[13:47.880 --> 13:53.400]  выглядеть характеристическая функция линейного преобразования? Ну то есть домножили на константу
[13:53.400 --> 13:59.000]  и прибавили какую-то константу. Утверждение. Ее можно выразить вот в терминах характеристической
[13:59.000 --> 14:04.840]  функции исходной случайной величины. Доказательства. Ну давайте просто по определению попробуем
[14:04.840 --> 14:20.560]  найти характеристическую функцию вот этого чуда. По определению альфа кси плюс бета. И т еще. Вот.
[14:20.560 --> 14:27.520]  Дальше здесь можем в принципе просто раскрыть скобочки. Заметим, что вот одно слагаемое никак
[14:27.520 --> 14:35.080]  от кси не зависит, поэтому мы его можем за от ожидания вынести. Будет и бета. А здесь останется
[14:35.080 --> 14:44.880]  математика. Математическое ожидание. Е в степени и альфа кси. Вот. Ну и вот это альфа мы можем
[14:44.880 --> 14:52.040]  прилепить к т к нашему параметру. И тогда мы с вами получим, что на самом деле это и бета
[14:52.040 --> 15:03.440]  на характеристическую функцию кси в точке альфа. Вот. Замечательный результат. Прекрасная компания.
[15:03.440 --> 15:11.320]  Вот. А для чего мы это сделали с вами? Ну давайте теперь найдем характеристическую функцию произвольного
[15:11.320 --> 15:16.920]  нормального распределения. Произвольное нормальное распределение. То есть у него есть какие-то параметры
[15:17.240 --> 15:26.120]  а и сигма квадрат. Мы с вами знаем, что такая вот случайная величина может быть получена из
[15:26.120 --> 15:32.000]  стандартной нормальной случайной величины путем домножения на константу и добавления константы.
[15:32.000 --> 15:41.480]  Домножить нужно на сигма, а прибавить нужно а. Вот. Ну и соответственно, пользуясь вот результатом,
[15:41.480 --> 15:44.880]  который мы вот здесь получили для линейного преобразования случайной величины, мы найдем
[15:44.880 --> 15:56.120]  сами характеристическую функцию произвольной нормальной случайной величины. Фиксиат. А будет
[15:56.120 --> 16:13.120]  просто е в степени иат минус так сигма квадрат, т квадрат пополам. Вот. Этот результат нам тоже
[16:13.120 --> 16:19.920]  еще поможет и пригодится. Можете его как-нибудь в рамочку ввести. Отлично. Почитали с вами
[16:19.920 --> 16:25.440]  характеристическую функцию нормальной случайной величины с произвольными параметрами. Дальше.
[16:25.440 --> 16:33.320]  Что еще позволяет удобно делать характеристические функции? Ну, сворачивать распределение. То есть
[16:33.320 --> 16:40.120]  искать распределение суммы двух случайных величин независимых. То есть раньше у нас,
[16:40.120 --> 16:47.400]  чтобы найти распределение суммы, мы с вами сворачивали плотности, и там достаточно
[16:47.400 --> 16:51.200]  неприятные интегралы были. Если вы пробовали доказать, что сумма двух нормальных случайных
[16:51.200 --> 16:55.000]  величин независимых это нормальная случайная величина, то вы, наверное, знаете, что там
[16:55.000 --> 16:58.880]  неприятная интеграла вылазит. Вот. А если нет, то мы сейчас с вами аккуратно это через
[16:58.880 --> 17:05.600]  характеристические функции докажем. Итак, утверждение. Пусть у нас есть две независимых
[17:05.600 --> 17:18.440]  случайных величины к сиэтам, а тогда характеристическая функция суммы это просто произведение
[17:18.440 --> 17:28.560]  характеристических функций. Вот. Этот факт, в принципе, тоже несложно доказать. По определению,
[17:28.560 --> 17:35.200]  характеристическая функция суммы это вот просто математическое ожидание вот такого вот объекта,
[17:35.200 --> 17:44.600]  вот. Дальше у нас это распадается на два множителя. Всего того, что к сиэту у вас независимы,
[17:44.600 --> 17:50.240]  мы можем разбить на произведение двух математических ожиданий. Вот. Такое свойство
[17:50.240 --> 17:59.120]  сумма тоже у нас было. И это Т. Вот. Ну и, соответственно, а это по определению уже две
[17:59.120 --> 18:08.080]  характеристические функции исходных случайных величин. Замечательно. Ну и давайте с вами вот
[18:08.080 --> 18:14.720]  такой полезный факт докажем. Достаточно, на самом деле, практический. Что вот если у вас есть к си,
[18:14.720 --> 18:22.520]  нормальная случайная величина с параметрами a1 в σ1 в квадрате, и есть случайная величина это,
[18:22.520 --> 18:32.160]  которая также нормально с параметрами a2 σ2 в квадрате, и они независимы, то их сумма
[18:32.160 --> 18:36.200]  тоже нормальное распределение, имеет тоже нормальное распределение с параметрами a1
[18:36.200 --> 18:57.040]  плюс a2 σ1 в квадрате плюс σ2 в квадрате. Так, σ1 в квадрате плюс σ2 в квадрате. Вот. Ну,
[18:57.040 --> 19:00.240]  доказательства просто полностью строятся на предыдущем факте, который мы с вами только что
[19:00.240 --> 19:17.840]  доказали. Запишем с вами характеристические функции. Просто найдем с вами характеристическую
[19:17.840 --> 19:25.760]  функцию к си плюс это по вот тому утверждению выше. Это просто произведение соответствующей
[19:25.760 --> 19:30.840]  характеристической функции. Ну и заметим, что там можно то, что у нас находится в показателе
[19:30.840 --> 19:42.760]  экспонента сгруппировать правильно, и у нас получится a1 плюс a2 t, а минус получается
[19:42.760 --> 19:52.480]  σ1 в квадрате плюс σ2 в квадрате, t в квадрате по полам. Вот. Просто перемножив две вот такие
[19:52.480 --> 20:00.520]  функции соответствующими параметрами, вы получите вот ровно вот такое выражение. Супер. На самом деле
[20:00.520 --> 20:04.080]  этот факт действительно очень полезен, потому что на практике, если у вас там курьеры примерно
[20:04.080 --> 20:08.560]  нормальное распределение времени доставки имеют, время готовки тоже нормальное, то вот суммарно
[20:08.560 --> 20:14.120]  время от поступления заказа до доставки его клиенту тоже имеет нормальное распределение. Этим
[20:14.120 --> 20:19.520]  можно как-то пользоваться на практике, если вы какие-нибудь там лабы делаете. Где-то это может
[20:19.520 --> 20:27.360]  пригодиться, в общем. Супер. Но кто нам сказал, что вот этот объект, который у нас получился, это все
[20:27.360 --> 20:32.000]  еще характеристическая функция какой-то случайной величины, и почему именно нормальные случайные
[20:32.000 --> 20:47.240]  величины? Вот. Ответ на этот вопрос дает теорема о единственности. В общем, утверждение такое,
[20:47.240 --> 20:54.840]  что если у вас две случайные величины совпадают по распределению, то у вас по точечно равны их
[20:54.840 --> 21:01.640]  характеристические функции. Вот. И теперь уже в принципе можно начать отвечать на вопрос,
[21:01.640 --> 21:06.920]  почему хар функция это классно, и это очень удобный инструмент для доказательства чего-то.
[21:06.920 --> 21:21.560]  Как бы сейчас немножко вот такой гуманитарный, гуманитарная минутка. То есть, смотрите, у нас
[21:21.560 --> 21:27.400]  были с вами просто какие-то меры вероятностные на прямой, и там в силу теоремы короттадора они
[21:27.400 --> 21:32.720]  взаимооднозначно соответствуют функциям распределения на прямой. А теперь у нас появился новый объект,
[21:32.720 --> 21:41.240]  характеристические функции. И вот в силу теоремы о единственности у вас как бы вот здесь везде
[21:41.240 --> 21:47.080]  однозначные соотношения между ними стоят. И теперь, если вы хотите доказать, что сумма двух,
[21:47.080 --> 21:51.560]  допустим, нормальных случайных величин, это тоже нормальная случайная величина, вы можете
[21:51.560 --> 21:55.600]  сделать, по сути, обратное преобразование фурье, посчитав характеристическую функцию, перейти вот в
[21:55.600 --> 22:01.920]  это пространство. Здесь воспользоваться теоремой о свертке, которая очень просто выглядит. Вот.
[22:02.020 --> 22:06.960]  Здесь вы доказываете этот факт, а затем в силу теоремы о единственности вы можете обратно
[22:06.960 --> 22:10.920]  вернуться как бы к вашим случайным величинам от характеристических функций, и, по сути, вы
[22:10.920 --> 22:15.480]  для ваших случайных величин доказали какое-то утверждение более простым образом. То есть и
[22:15.480 --> 22:19.280]  в принципе все доказательства, которые строятся на характеристических функциях, строятся на том,
[22:19.280 --> 22:23.440]  что вы вот как бы ищите у случайных величин характеристические функции, для них что-то
[22:23.440 --> 22:27.720]  доказываете какой-то результат, более просто. И возвращаетесь обратно в исходное пространство и
[22:27.720 --> 22:30.660]  и говорите, что вот, в CPT у вас получается предельная
[22:30.660 --> 22:33.160]  случайная величина, а это m0,1, например.
[22:33.160 --> 22:37.120]  Окей?
[22:37.120 --> 22:38.120]  Более-менее, думаю, понятно.
[22:38.120 --> 22:40.840]  Так, давайте дальше.
[22:40.840 --> 22:45.080]  Соответственно, какие еще удобные штуки нам позволяют
[22:45.080 --> 22:47.160]  делать характеристические функции?
[22:47.160 --> 22:49.440]  Ну, на самом деле, при помощи характеристических функций
[22:49.440 --> 22:51.000]  можно вычислять моменты очень легко.
[22:51.000 --> 23:02.920]  Момент случайных величин и давайте дифференцирование.
[23:02.920 --> 23:05.040]  Ну ладно, это писать не будем, просто моменты случайных
[23:05.040 --> 23:06.040]  величин.
[23:06.040 --> 23:07.520]  Значит, утверждение.
[23:07.520 --> 23:20.640]  Пусть у вас существует абсолютный n-тый момент у случайной
[23:20.640 --> 23:21.640]  штуки.
[23:21.640 --> 23:24.000]  То есть, просто вот таком от ожидания определено.
[23:24.000 --> 23:28.240]  Ну и пусть будет меньше бесконечности, например.
[23:28.240 --> 23:29.520]  Тогда можно сказать следующее.
[23:29.520 --> 23:39.160]  Можно зачем-то, ну сейчас поймем зачем, рассматривать
[23:39.160 --> 23:43.760]  производные характеристической функции, которые на самом
[23:43.760 --> 23:44.760]  деле будут равны следующему.
[23:44.760 --> 23:55.800]  И более того, можно удобно искать моменты, то есть
[23:55.800 --> 23:57.680]  от ожидания случайной величины в какой-то степени.
[23:57.680 --> 24:04.560]  Вот это утверждение без доказательства.
[24:04.560 --> 24:09.080]  Мы сейчас просто обсудим неправильную идею доказательства.
[24:09.080 --> 24:11.120]  Правильная у вас там на лекциях будет.
[24:11.120 --> 24:13.000]  В общем, идейно можно смотреть на это так.
[24:13.160 --> 24:15.320]  Вот это же у вас по сути интеграл с параметром.
[24:15.320 --> 24:17.520]  И вот здесь мы просто с вами по сути по параметру
[24:17.520 --> 24:18.640]  дифференцируем.
[24:18.640 --> 24:22.680]  Если вы продифференцируете вот это выражение по параметру,
[24:22.680 --> 24:25.180]  то у вас как раз таки вот x из-под экспонента будет
[24:25.180 --> 24:28.600]  вылазить s раз, и поэтому вот это равенство более-менее
[24:28.600 --> 24:29.600]  кажется верным.
[24:29.600 --> 24:30.600]  Вот.
[24:30.600 --> 24:34.200]  А дальше, если мы просто поставим вот в это выражение
[24:34.200 --> 24:38.960]  нолик, то у вас вот эта экспонента станет единичкой.
[24:38.960 --> 24:41.800]  У вас останется математическое ожидание x в степени s.
[24:41.800 --> 24:42.800]  Вот.
[24:42.800 --> 24:45.360]  Мы видим на y в степени s, мы выразим с вами математическое
[24:45.360 --> 24:47.480]  ожидание x в этой степени.
[24:47.480 --> 24:48.480]  Вот.
[24:48.480 --> 24:51.360]  Ну и можно какой-нибудь пример рассмотреть.
[24:51.360 --> 24:56.920]  То есть теперь вместо того, чтобы искать математическое
[24:56.920 --> 24:59.120]  ожидание нормальной случайной величины или дисперсию,
[24:59.120 --> 25:02.800]  например, интегрируя там что-то неприятное, можно
[25:02.800 --> 25:05.080]  просто дифференцировать характеристическую функцию.
[25:05.080 --> 25:06.080]  Вот.
[25:06.080 --> 25:11.040]  Давайте попробуем найти математическое ожидание,
[25:11.040 --> 25:14.960]  пусть x у нас имеет нормальное распределение с параметрами
[25:14.960 --> 25:17.400]  a sigma квадрат, давайте найдем с вами математическое
[25:17.400 --> 25:18.400]  ожидание x.
[25:18.400 --> 25:24.120]  Ну вот, в силу сказанного выше, это получается будет
[25:24.120 --> 25:29.840]  первая производная в нуле поделить на i, а соответственно
[25:29.840 --> 25:31.360]  характеристическую функцию я стер.
[25:31.360 --> 25:35.480]  Ну вот, это почти она, давайте я ее чуть-чуть подправлю.
[25:35.480 --> 25:39.680]  Уберу просто второй параметр, потому что сейчас у нас
[25:39.680 --> 25:47.200]  вот такая у нас характеристическая функция, вот у этой случайной
[25:47.200 --> 25:48.200]  величины.
[25:48.200 --> 25:50.960]  Соответственно, давайте ее продиференцируем разок
[25:50.960 --> 25:52.720]  и поставим в точке ноль.
[25:52.720 --> 25:53.720]  Вот.
[25:53.720 --> 25:59.880]  Ну если будем дифференцировать, так iat, что у нас будет?
[25:59.880 --> 26:05.400]  Значит, производное того, что там это у нас ia, минус
[26:05.600 --> 26:10.560]  производная вот этой штучки по t, получается у нас двоечка
[26:10.560 --> 26:14.040]  спускается, два сокращаем, ну и sigma квадрат дам.
[26:14.040 --> 26:25.360]  Ну и на экспоненту, вот давайте я здесь многоточие
[26:25.360 --> 26:27.880]  поставлю, я по сути переписал все то, что в показателе
[26:27.880 --> 26:28.880]  было.
[26:28.880 --> 26:33.000]  Теперь мы это подставляем в точке ноль, все что у вас
[26:33.000 --> 26:36.960]  было в экспоненте в показателе у вас превращается в нолик,
[26:36.960 --> 26:39.440]  потому что у вас вот эта слагаемая зануляется, эта
[26:39.440 --> 26:42.360]  слагаемая зануляется, поэтому вы e возбудите в степень
[26:42.360 --> 26:43.360]  ноль, это будет единичка.
[26:43.360 --> 26:47.440]  Здесь я тачку потерял.
[26:47.440 --> 26:51.400]  Зануляется вот эта слагаемая, и у вас остается только
[26:51.400 --> 26:52.400]  ia.
[26:52.400 --> 26:56.080]  Соответственно, по вот этому утверждению нужно еще
[26:56.080 --> 26:57.080]  на i поделить.
[26:57.080 --> 27:01.520]  Ну и вот если вы на i поделите, у вас как раз-таки получится
[27:01.520 --> 27:04.080]  тот факт, который мы уже с вами и так знаем, в отождании
[27:04.080 --> 27:08.400]  икси это вот просто первый параметр нормальной случайной
[27:08.400 --> 27:11.400]  величины.
[27:11.400 --> 27:13.680]  Соответственно, так может быть удобнее искать, чем
[27:13.680 --> 27:16.360]  интегрировать что-то, ну всегда там брать производную
[27:16.360 --> 27:20.200]  проще, чем интегрировать, поэтому так вот можно все
[27:20.200 --> 27:22.120]  моменты искать случайной величины, если вам в какой-то
[27:22.120 --> 27:23.120]  задаче это может понадобиться.
[27:23.120 --> 27:28.720]  Да, потому что мы подставляем с вами, вот мы сначала взяли
[27:28.720 --> 27:32.120]  производную, а потом ее в точке ноль, и поделили
[27:32.120 --> 27:34.080]  на i в соответствующей степени.
[27:34.080 --> 27:36.080]  Вот эта слагаемая занулилась, потому что у нас тут т-шечка
[27:36.080 --> 27:38.240]  одна осталась, вот все, что здесь в показателе тоже
[27:38.240 --> 27:39.920]  занулилась, потому что там т-шечки были.
[27:39.920 --> 27:43.960]  Ну и соответственно здесь будет а.
[27:43.960 --> 27:46.760]  Ну и если хотите, можете там тоже самое для дисперсии
[27:46.760 --> 27:49.280]  посчитать, то есть найти второй момент, а потом просто
[27:49.280 --> 27:52.080]  вот из него вычесть икси в квадрате, и у вас тоже получится
[27:52.080 --> 27:53.080]  сигма квадрат.
[27:53.920 --> 27:59.320]  Вот, супер, тоже удобно на самом деле, и полезно.
[27:59.320 --> 28:04.920]  Так, что дальше у нас, теорему единственности мы с вами
[28:04.920 --> 28:08.240]  обсудили, вот, давайте еще немножко вот вернемся
[28:08.240 --> 28:11.160]  к теореме единственности.
[28:11.160 --> 28:21.480]  Вот если вам дали плотность, более-менее понятно, как
[28:21.640 --> 28:24.960]  считать характеристическую функцию, но просто по определению.
[28:24.960 --> 28:30.400]  А вот если вам дали характеристическую функцию, как найти плотность?
[28:30.400 --> 28:33.540]  Ответ на этот вопрос, грубо говоря мы знаем, как с Вами
[28:33.540 --> 28:36.060]  от распределения переходить к характеристическим функциям,
[28:36.060 --> 28:38.180]  мы просто считаем с вами по определению обратное
[28:38.180 --> 28:41.220]  преобразование фурье, а как вернуться из характеристических
[28:41.220 --> 28:44.360]  функций в пространство распределений или плотностей,
[28:44.360 --> 28:47.340]  если у Вас плотность существует.
[28:47.340 --> 28:50.600]  Ответ на этот вопрос дает теорема об обращение.
[28:50.600 --> 29:03.320]  Так, там, в общем, у нее два случая есть, то есть у вас
[29:03.320 --> 29:06.840]  же не всегда плотность существует, и вот в случае, когда у вас
[29:06.840 --> 29:08.680]  плотность не существует, мы сейчас рассматривать
[29:08.680 --> 29:14.360]  не будем, там нужно функции распределения выражать.
[29:14.360 --> 29:17.200]  То есть мы вот этот пункт с вами не рассматриваем
[29:17.200 --> 29:18.200]  сейчас.
[29:18.200 --> 29:21.800]  Мы рассмотрим с вами второй пункт, то есть пусть у вас
[29:21.800 --> 29:27.720]  модуль характеристической функции интегрируем, то
[29:27.720 --> 29:36.560]  есть меньше бесконечности, тогда плотность можно найти,
[29:36.560 --> 29:39.600]  по сути, как прямое преобразование фурье от характеристической
[29:39.600 --> 29:40.600]  функции.
[29:40.600 --> 29:57.280]  Вот, ну здесь ДТ, короче, то есть вы там в мотоне, наверное,
[29:57.280 --> 30:01.480]  рассматривали тоже преобразование фурье, и вот у вас там каждый
[30:01.480 --> 30:03.280]  раз был коэффициент один поделить на корень из
[30:03.280 --> 30:08.320]  двух пи, вот, а здесь мы просто с вами, когда считаем преобразование
[30:09.000 --> 30:11.200]  из плотностей в характеристические функции, мы ни на какой умножитель
[30:11.200 --> 30:13.960]  не домножаем, вот, ну соответственно, чтобы мы вернулись к той
[30:13.960 --> 30:15.800]  же плотности, видимо, значит, нужно будет домножить вот
[30:15.800 --> 30:19.360]  один на два пи, вот, ну, это просто как константа расставить.
[30:19.360 --> 30:25.680]  Это, значит, теорема об обращении.
[30:25.680 --> 30:29.440]  Ну и давайте небольшую задачку с вами решим.
[30:29.440 --> 30:32.840]  Она почти, наверное, была у вас в семинарских листочках,
[30:32.840 --> 30:36.280]  но она заслуживает внимания, и сделаем какие-то выводы
[30:36.280 --> 30:36.800]  из нее.
[30:48.800 --> 30:50.400]  Так, так, так, так, так, так.
[30:50.400 --> 30:58.200]  Задачка, собственно, была такая, она о нескольких пунктах.
[30:58.200 --> 31:00.920]  Мы что-то с вами пропустим, потому что будем считать,
[31:00.920 --> 31:04.200]  что это было на семинарах, а что-то вот детально обсудим.
[31:05.120 --> 31:09.200]  Итак, первое, первый пункт, если вы знаете, что плотность
[31:09.200 --> 31:12.480]  вашей случайной величины имеет следующий вид,
[31:19.480 --> 31:22.560]  ну, то есть распределение лапласа, посчитайте ее
[31:22.560 --> 31:25.240]  характеристическую функцию, вот, но мы сейчас с вами
[31:25.240 --> 31:26.840]  считать не будем, будем считать, что вы это сделали
[31:26.840 --> 31:31.560]  на семинарах, поэтому просто сразу напишем, что ее характеристическая
[31:31.920 --> 31:34.920]  функция имеет вот такой вид.
[31:34.920 --> 31:40.640]  Тут формулейлеры можно воспользоваться, а потом
[31:40.640 --> 31:43.640]  там два раза по частям интегральчик взять, косинус
[31:43.640 --> 31:48.160]  на вот какое-то такое выражение, ну и там все берется, сейчас
[31:48.160 --> 31:51.480]  не хочется на это время тратить, вот, то есть мы
[31:51.480 --> 31:54.040]  имеем плотность и посчитали характеристическую функцию,
[31:54.040 --> 31:57.280]  вот, это мы пропустили с вами, но ничего страшного,
[31:57.280 --> 31:58.280]  поверим.
[31:59.000 --> 32:05.800]  Второе, рассмотрим случайную величину кси, так, давайте
[32:05.800 --> 32:08.920]  теперь это здесь будет, имеющая распределение
[32:08.920 --> 32:17.040]  коши с параметрами 0, ну тоже давайте, альфа, соответственно
[32:17.040 --> 32:21.640]  плотность у нее, у такой случайной величины, это
[32:22.000 --> 32:35.000]  альфа поделить на пи альфа в квадрате плюс х в квадрате.
[32:35.000 --> 32:40.720]  А, собственно, в чем проблема? Вообще, исходная задача
[32:40.720 --> 32:43.080]  состоит в следующем, найдите как бы характеристическую
[32:43.080 --> 32:46.480]  функцию для распределения коши, вот, ну такой вот интеграл
[32:46.480 --> 32:51.720]  считать, если вы это домножите на ИТЕ в степени ИТХ, в общем
[32:51.720 --> 32:55.280]  очень сложно, если бы ТФКП прошли курс, то вы бы такой
[32:55.280 --> 32:57.720]  интегральщик очень легко взяли, но если бы вот ТФКП
[32:57.720 --> 33:00.040]  еще не проходили, то такой интеграл очень сложно
[33:00.040 --> 33:03.360]  берется, но здесь есть один лайфхак, этот лайфхак
[33:03.360 --> 33:06.640]  заключается в том, чтобы воспользоваться формулой
[33:06.640 --> 33:09.200]  утеряемой об обращении, ну и собственно давайте
[33:09.200 --> 33:10.200]  это аккуратно сделаем.
[33:16.480 --> 33:26.720]  Да, да, да, идея в том, чтобы свести к первому, то есть
[33:26.720 --> 33:33.080]  решение этой задачи очень простое, первое, для распределения
[33:33.080 --> 33:37.520]  лапласа выполнено теорема об обращении, ну просто
[33:37.520 --> 33:39.600]  потому что вот этот модуль этой функции интегрируем,
[33:39.600 --> 33:42.280]  там все хорошо с этим, поэтому мы попадаем с вами во второй
[33:42.280 --> 33:45.240]  пункт теоремы об обращении, соответственно мы можем
[33:45.240 --> 33:47.480]  восстановить плотность, мы знаем с вами и плотность,
[33:47.480 --> 33:49.760]  и характеристическую функцию, ну вот давайте вот это вот
[33:49.760 --> 33:52.840]  равенство из теорем об обращении запишем, если мы
[33:52.840 --> 33:56.480]  его запишем, то мы получим следующее, альфа пополам,
[33:56.480 --> 34:02.640]  е в степени минус альфа моли х, это плотность, это
[34:02.640 --> 34:08.640]  интеграл по r, так, е в степени, так, тут еще константа,
[34:08.640 --> 34:14.560]  1 на 2 π, е в степени минус и ТХ на характеристическую
[34:15.040 --> 34:21.040]  функцию, характеристическая функция у нас вот такая,
[34:21.040 --> 34:28.040]  вот, это формула обращения для распределения лапласа,
[34:36.960 --> 34:39.600]  отлично, значит вот у нас формула обращения для
[34:39.600 --> 34:42.760]  распределения лапласа, а что мы с вами хотим заметить,
[34:42.840 --> 34:45.840]  по определению, чтобы посчитать характеристическую функцию
[34:45.840 --> 34:51.360]  каши, мы должны с вами какой интегральчик взять, ну
[34:51.360 --> 34:55.160]  мы должны с вами взять интегральчик, е в степени
[34:55.160 --> 35:02.160]  и ТХ на плотность, плотность распределения каши, вот
[35:02.160 --> 35:10.680]  такая, вот, ну и давайте заметим, смотрите, вот это равенство
[35:10.720 --> 35:13.480]  у нас выполнено в силу теоремы обращения, а вот это вот
[35:13.480 --> 35:16.360]  уж больно напоминает вот это выражение, ну давайте
[35:16.360 --> 35:20.560]  мы с вами какими-то манипуляциями, там, домножением на какие-то
[35:20.560 --> 35:22.440]  константы приведем вот это выражение к выражению
[35:22.440 --> 35:25.960]  вот такого вида, а что для этого нужно сделать, ну
[35:25.960 --> 35:32.880]  вот смотрите, π мы можем вот сюда внутрь занести,
[35:32.880 --> 35:35.600]  на двоечку, видимо, мы можем домножить обе части, у вас
[35:35.600 --> 35:38.760]  вот эта двоечка пропадет, а, видимо, мы можем еще поделить
[35:38.760 --> 35:40.520]  обе части на алифа, потому что, ну, параметры больше
[35:40.520 --> 35:45.880]  нуля, ну супер, и у нас теперь в правой части
[35:45.880 --> 35:48.040]  получился вот тот интеграл, который нам нужно было посчитать
[35:48.040 --> 35:52.680]  изначально, ну, за одним исключением вот здесь минус
[35:52.680 --> 35:54.600]  есть, но этот минус не так страшен, потому что мы
[35:54.600 --> 35:56.880]  можем сделать с вами замену переменных, Т поменять на
[35:56.880 --> 36:00.400]  минус Т, вот, здесь тогда у вас минус уйдет, в силу
[36:00.400 --> 36:05.960]  симметрии ничего не сломается, все, задача решена, то есть
[36:05.960 --> 36:13.200]  ответ, характеристическая функция, случайные величины,
[36:13.200 --> 36:32.200]  имеющие распределение каши, имеет вот такой вид, а, может
[36:32.200 --> 36:33.200]  вопрос повторить?
[36:33.440 --> 36:42.600]  Ну вот почему у нас там есть, да, смотри, вот это, это
[36:42.600 --> 36:49.040]  по определению просто характеристическая функция, в точке Т, это вот просто
[36:49.040 --> 36:57.320]  определение мы использовались, окей, все, супер, посчитали
[36:57.320 --> 36:59.640]  с вами характеристическую функцию, ну и, собственно,
[36:59.640 --> 37:04.240]  мы считали это не просто так, это контрпример о том,
[37:04.240 --> 37:17.400]  что свертка необратима, утверждение следующее,
[37:17.400 --> 37:22.040]  утверждение следующее, значит, давайте вспомним с вами
[37:22.040 --> 37:28.480]  свертку, если у вас случайные величины независимы, тогда
[37:28.480 --> 37:32.840]  у вас характеристическая функция суммы, это произведение
[37:32.840 --> 37:40.280]  характеристических функций, что-то такое было, вот, ну
[37:40.280 --> 37:43.520]  вот, тут стрелочка вправо была, а вот сейчас мы с вами
[37:43.520 --> 37:52.000]  по сути приведем контрпример, что стрелочки влево нет,
[37:52.000 --> 37:54.320]  в обратную сторону неверно, то есть если характеристическая
[37:54.320 --> 37:57.180]  функция суммы двух случайных величин распалась в произведении
[37:57.180 --> 37:59.740]  характеристической функции, из этого не следует, что
[37:59.740 --> 38:04.380]  у вас случайная величина независима, ну вот, доказательства,
[38:04.380 --> 38:11.940]  контрпримеры.
[38:11.940 --> 38:16.100]  Рассмотрим с вами кси, а случайную величину имеющую
[38:16.100 --> 38:21.820]  распределение коши с параметром 0.1, тогда ее характеристическая
[38:22.660 --> 38:26.700]  функция, ну вот, мы с вами ее посчитали уже, то есть
[38:26.700 --> 38:28.380]  это е в степени минус модульта.
[38:28.380 --> 38:36.780]  И давайте с вами посмотрим на вот такое равенство.
[38:36.780 --> 38:56.980]  Ну, заметим, что оно верно, вот, ну, почему, потому что
[38:56.980 --> 38:59.160]  здесь у вас по сути будут вот два таких выражения,
[38:59.160 --> 39:00.900]  они у вас при перемножении здесь будут е в степени
[39:00.900 --> 39:01.900]  минус два модульт.
[39:01.900 --> 39:06.340]  А слева у вас два кси, то есть у вас двоечка здесь
[39:06.340 --> 39:08.780]  под модулем будет, двоечка вылезет из-под модулей,
[39:08.780 --> 39:11.380]  и по сути у вас вот это равенство верное сейчас.
[39:11.380 --> 39:17.340]  Но понятно, что кси зависит с кси, то есть здесь вот
[39:17.340 --> 39:18.340]  независимости такой нет.
[39:18.340 --> 39:21.060]  Ну и, собственно, мы с вами привели контрпример к
[39:21.060 --> 39:26.900]  тому, что свертка в обратную сторону не работает.
[39:26.900 --> 39:30.420]  Ну это как бы тоже такой интересный факт, который
[39:30.420 --> 39:31.420]  на экзамен могут спросить.
[39:32.420 --> 39:33.420]  Супер.
[39:33.420 --> 39:38.900]  А тут еще кое-что видно на самом деле, что сумма
[39:38.900 --> 39:43.960]  двух случайных величин, имеющих распределение
[39:43.960 --> 39:48.900]  каши, ну, давайте допустим с параметром 0,1, даже если
[39:48.900 --> 39:57.860]  они независимы, имеют распределение каши с параметрами 0,2.
[39:57.860 --> 40:03.140]  Ну и вот тоже хороший контрпример, по сути у вас
[40:03.140 --> 40:04.140]  ЗБЧ не выполняется.
[40:04.140 --> 40:07.700]  Ну и УЗБЧ тоже получается.
[40:07.700 --> 40:09.900]  Смотрите, у распределения каши нет математического
[40:09.900 --> 40:10.900]  ожидания.
[40:10.900 --> 40:14.500]  Это более-менее известный факт, думаю, вы все это
[40:14.500 --> 40:15.500]  знаете.
[40:15.500 --> 40:20.940]  Вот, а как у нас выглядит формулировка УЗБЧ?
[40:20.940 --> 40:23.660]  Это вот немножко сбежим вперед, но это контрпример
[40:23.660 --> 40:26.460]  мы сейчас с вами посмотрим, то есть у нас есть формулировка
[40:26.460 --> 40:31.420]  УЗБЧ такая, что если у вас у последовательности
[40:31.420 --> 40:37.940]  независимых случайных величин, независимых одинаково распределенных
[40:37.940 --> 40:45.940]  случайных величин, а существует математическое ожидание,
[40:45.940 --> 40:47.940]  тогда у вас выполнена вот такая сходимость.
[40:47.940 --> 40:51.180]  Sn на n сходится к математическому ожиданию кси.
[40:51.180 --> 40:55.460]  Ну и тут даже почти наверно можно выписать, потому что
[40:55.620 --> 40:56.620]  это усиленный закон больших чисел.
[40:56.620 --> 41:02.860]  Вот, и соответственно, в случае распределения каши
[41:02.860 --> 41:03.860]  это не работает.
[41:03.860 --> 41:06.300]  То есть смотрите, сумма двух случайных величин, имеющих
[41:06.300 --> 41:09.140]  распределение каши, имеет тоже распределение каши,
[41:09.140 --> 41:11.700]  но вот в нашем случае с параметром 0.2, если мы с вами
[41:11.700 --> 41:14.780]  отнормируем вот на эту n-ку, то у вас здесь будет все
[41:14.780 --> 41:20.860]  время распределение каши с параметром 0.1.
[41:20.860 --> 41:23.620]  Ну и понятно, что это сходится к распределению каши с
[41:23.620 --> 41:24.620]  параметром 0.1.
[41:24.620 --> 41:34.180]  Вот, таким образом, как видите, условия на существование
[41:34.180 --> 41:38.180]  математического ожидания в УЗБЧ, ну и в УЗБЧ тоже существенны.
[41:38.180 --> 41:41.820]  Если мы откажемся от вот этого условия, то сходимости
[41:41.820 --> 41:44.820]  у вас уже не будет, Константин.
[41:44.820 --> 41:49.420]  Вот, супер, эту задачку мы с вами решили, мы почти
[41:49.420 --> 41:53.580]  все по теории, это мы обсудили.
[41:54.420 --> 41:57.620]  Это мы обсудили и это мы обсудили.
[41:57.620 --> 42:01.300]  Все, давайте тогда задачи решать.
[42:01.300 --> 42:07.020]  Соответственно, что могут дать на характеристические
[42:07.020 --> 42:11.340]  функции в контрольной работе?
[42:11.340 --> 42:15.860]  Сейчас сначала с вами обсудим, что могут дать, какие задачи,
[42:15.860 --> 42:21.060]  какого вида, затем какую-то задачу разберем из прошлогоднего
[42:21.060 --> 42:22.060]  пробника.
[42:22.740 --> 42:23.740]  Так, какие задачи?
[42:27.740 --> 42:33.820]  Все мы, к сожалению, обсудить не успеем, поэтому я в порядке
[42:33.820 --> 42:35.780]  вероятности появления того или иного типа их вот
[42:35.780 --> 42:39.340]  сейчас отсортирую, самые вероятные будут сверху, маловероятные
[42:39.340 --> 42:40.340]  снизу.
[42:40.340 --> 42:43.100]  Значит, ну вот, задачка как у нас, то есть посчитать
[42:43.100 --> 42:50.780]  характеристическую функцию чего-то, ну там какой-нибудь
[42:50.780 --> 42:53.420]  суммы произведения, ну вот, в нашем случае, ксен
[42:53.420 --> 42:57.300]  минус кси, в общем, функции от нескольких случайных
[42:57.300 --> 42:58.300]  величин.
[42:58.300 --> 43:02.940]  А что еще могут дать?
[43:02.940 --> 43:05.660]  На самом деле, вот смотрите, вы же знаете, что понять
[43:05.660 --> 43:08.420]  является функцией плотности или не является очень просто.
[43:08.420 --> 43:10.500]  Там достаточно условий, это просто чтобы функция
[43:10.500 --> 43:13.300]  была не отрицательная, и второе условие, чтобы она
[43:13.300 --> 43:15.900]  интегральщика от нее была равна единичке.
[43:15.900 --> 43:18.260]  А вот проверить, является ли функция характеристическая
[43:18.260 --> 43:20.220]  на самом деле вообще не тривиальна.
[43:20.220 --> 43:23.780]  Вот, и задача второго вида, это вот проверить, является
[43:23.780 --> 43:31.700]  ли данная функция, которую вам дадут, характеристическая.
[43:31.700 --> 43:33.860]  Вот, такие задачки очень волоснов любят давать.
[43:33.860 --> 43:37.140]  А может там где-нибудь на экзамене дать тоже.
[43:37.140 --> 43:42.740]  Вот, ну что делать, в этот момент молиться, а потом
[43:42.740 --> 43:45.460]  вспоминать теорему Бохна-Рахинчина критерий.
[43:45.460 --> 43:56.020]  Вот, можно попробовать проверить функцию на не отрицательную
[43:56.020 --> 43:57.020]  определенность.
[43:57.020 --> 44:00.220]  Вот, и можно к ним там вспомогать на утверждение еще использовать.
[44:00.220 --> 44:02.580]  Я потом к ним ссылки наверное оставлю.
[44:02.580 --> 44:05.260]  Ну либо если вы хотите сказать, что функция не является
[44:05.260 --> 44:06.780]  характеристической, достаточно показать, что какое-то из
[44:06.780 --> 44:09.260]  свойств, которые мы сегодня с вами обсудили, не выполняется.
[44:09.260 --> 44:21.860]  Вот, ну есть там еще на самом деле несколько теорем,
[44:21.860 --> 44:24.940]  я не знаю какие у вас, может быть вам семинарист какие-нибудь
[44:24.940 --> 44:25.940]  давал.
[44:25.940 --> 44:28.140]  Там на самом деле очень много признаков, критерий
[44:28.140 --> 44:30.540]  один очень сложный, но есть очень много признаков.
[44:30.540 --> 44:32.620]  Вот, и может быть там каким-то признаком можно будет воспользоваться.
[44:32.620 --> 44:35.660]  Вот, такие задачи мы сейчас разбирать не будем.
[44:35.660 --> 44:38.780]  Вот, и третий вид задач, это на теорему непрерывности.
[44:38.780 --> 44:42.620]  Мы вот такие задачи обсудим чуть позже.
[44:42.620 --> 44:47.540]  По сути о чем это?
[44:47.540 --> 44:51.220]  Если у вас есть сходимость по распределению, то это
[44:51.220 --> 44:53.980]  практически, давайте я как-нибудь вот так нарисую,
[44:53.980 --> 44:56.940]  это практически сходимость характеристических функций
[44:56.940 --> 44:57.940]  в каждой точке.
[44:57.940 --> 45:06.940]  Ну там, за небольшой оговоркой.
[45:07.620 --> 45:10.660]  Вот, такие задачи мы обсудим чуть позже, когда сходимости
[45:10.660 --> 45:11.980]  обсудим во втором разделе.
[45:11.980 --> 45:17.340]  Вот, и давайте сейчас тогда решим задачу один, которая
[45:17.340 --> 45:19.220]  вот в предложенном варианте имеется.
[45:19.220 --> 45:36.220]  Так, если что, у меня все решения есть затеханные,
[45:36.300 --> 45:38.740]  но мы сейчас с вами перерешаем задачи, если найдем ошибки,
[45:38.740 --> 45:42.700]  то я поправлю и потом пришлю решение задач куда-нибудь.
[45:42.700 --> 45:51.700]  Вот, у всех условия есть или нужно записать его?
[45:51.700 --> 45:55.980]  Ну, давайте, ладно, кратненько напишем.
[45:55.980 --> 45:58.540]  Значит, у нас есть две независимые случайно величины.
[45:58.540 --> 46:05.460]  Кси имеет экспоненциальное распределение с параметром
[46:05.460 --> 46:09.580]  2, это имеет равномерное распределение на отрезке
[46:09.580 --> 46:11.580]  минус 1,1.
[46:11.580 --> 46:16.100]  И, собственно, вопрос данной задачи, найдите характеристическую
[46:16.100 --> 46:21.460]  функцию, то есть вводится новая случайная величина,
[46:21.460 --> 46:26.060]  кси это минус кси, и вопрос данной задачи, найти характеристическую
[46:26.100 --> 46:29.100]  функцию.
[46:29.100 --> 46:35.860]  То есть первый тип задачи, это вот как от какого-то
[46:35.860 --> 46:38.860]  преобразования случайных величин найти характеристическую
[46:38.860 --> 46:39.860]  функцию.
[46:39.860 --> 46:40.860]  Ну, давайте решать.
[46:40.860 --> 46:48.140]  А первый шаг в этой задаче, это, наверное, заметить,
[46:48.140 --> 46:51.660]  что вот считать характеристическую функцию сначала вот этого,
[46:51.660 --> 46:55.580]  а потом как-то пытаться вот с этим объединять неприятно.
[46:55.580 --> 46:56.580]  Почему?
[46:56.580 --> 46:57.580]  Потому что у вас вот эти две случайные величины
[46:57.580 --> 47:00.860]  уже будут зависимы, а независимые сворачивать мы не умеем.
[47:00.860 --> 47:03.100]  То есть, наверное, первый шаг, это справедливо заметить,
[47:03.100 --> 47:09.980]  что, короче, на множители разложить вот это выражение.
[47:09.980 --> 47:12.940]  Ну и понятно, что вот эта случайная величина будет
[47:12.940 --> 47:13.940]  уже от вот этой независима.
[47:13.940 --> 47:18.540]  Ну и давайте какое-нибудь новое обозначение для
[47:18.540 --> 47:19.540]  нее придумаем.
[47:19.540 --> 47:20.540]  Это штрих, например.
[47:20.540 --> 47:21.540]  Супер.
[47:21.540 --> 47:27.300]  Но мы с вами не обсуждали, как искать характеристическую
[47:27.300 --> 47:29.060]  функцию произведения двух независимых случайных
[47:29.060 --> 47:30.060]  величин.
[47:30.060 --> 47:31.060]  Ну вот самое время.
[47:31.060 --> 47:34.940]  Давайте сначала запишем по определению, что от нас
[47:34.940 --> 47:35.940]  хотят.
[47:35.940 --> 47:49.140]  По определению характеристической функции, от нас очень хотят,
[47:50.140 --> 47:55.140]  чтобы мы посчитали математическое ожидание E в степени i,
[47:55.140 --> 48:02.140]  xi, e, a, t.
[48:02.140 --> 48:05.940]  Дальше мы можем с вами воспользоваться...
[48:05.940 --> 48:08.940]  Это штрих, так?
[48:08.940 --> 48:09.940]  Да.
[48:09.940 --> 48:10.940]  Спасибо большое.
[48:10.940 --> 48:15.940]  Это штрих.
[48:15.940 --> 48:22.940]  Вот такую характеристическую функцию от нас, вот такой
[48:22.940 --> 48:24.940]  интегральщик от нас хотят посчитать.
[48:24.940 --> 48:27.020]  Давайте воспользуемся определением математического ожидания
[48:27.020 --> 48:28.020]  просто здесь.
[48:28.020 --> 48:38.260]  Это у нас по сути интегральщик, и xi это штрих t, умножить
[48:38.260 --> 48:40.860]  на совместную плотность, xi это штрих.
[48:40.860 --> 48:55.860]  Так, сейчас, x, y, давайте введем тогда в точку x, y.
[48:55.860 --> 48:56.860]  Вот так.
[48:56.860 --> 48:59.860]  Это просто определение математического ожидания.
[48:59.860 --> 49:02.060]  Я думаю, вы помните, что математическое ожидание
[49:02.060 --> 49:06.740]  функции от вектора случайного, это просто вот такой вот
[49:07.740 --> 49:10.740]  интегральщик, функции на совместную плотность.
[49:10.740 --> 49:11.740]  Вот.
[49:11.740 --> 49:12.740]  Ну что мы из этого знаем?
[49:12.740 --> 49:16.740]  Знаем ли мы с вами совместную плотность?
[49:16.740 --> 49:22.740]  Да, потому что у нас xi это штрих независимая, поэтому
[49:22.740 --> 49:25.740]  в силу независимости у вас совместная плотность
[49:25.740 --> 49:27.740]  двух случайных лечений, это просто произведение
[49:27.740 --> 49:28.740]  плотностей.
[49:28.740 --> 49:29.740]  Вот.
[49:29.740 --> 49:32.740]  Ну и дальше в силу теоремы Фубини просто переходим
[49:32.740 --> 49:33.740]  к повторному интегралу.
[49:34.740 --> 49:37.740]  То есть здесь сразу мы два действия делаем.
[49:37.740 --> 49:38.740]  Первое переходим к повторному интегралу.
[49:44.740 --> 49:45.740]  Вот.
[49:45.740 --> 49:47.740]  А второе, у нас вот эта плотность распадается на произведение
[49:47.740 --> 49:48.740]  плотностей.
[49:53.740 --> 49:54.740]  Вот.
[49:54.740 --> 49:57.740]  Давайте одну плотность вот внутрь засунем.
[49:57.740 --> 50:02.740]  Давайте допустим плотность xi dx.
[50:02.740 --> 50:05.740]  Вот это получается первый интегральщик.
[50:05.740 --> 50:11.740]  И здесь у нас еще будет плотность штрих у, второй интегральщик.
[50:11.740 --> 50:13.740]  Ну просто теоремы Фубини.
[50:13.740 --> 50:14.740]  Вот.
[50:14.740 --> 50:16.740]  К повторному интегралу перешли.
[50:19.740 --> 50:20.740]  Так.
[50:20.740 --> 50:22.740]  Вот это что такое?
[50:27.740 --> 50:32.740]  Ну вот, на что-то похоже.
[50:32.740 --> 50:37.740]  Да, это характеристическая функция.
[50:37.740 --> 50:38.740]  Вопрос чего?
[50:38.740 --> 50:41.740]  Случайная величина xi видимо, раз у нас плотность xi.
[50:41.740 --> 50:44.740]  И вопрос в какой точке?
[50:44.740 --> 50:46.740]  В точке y.
[50:46.740 --> 50:47.740]  Вот.
[50:47.740 --> 50:50.740]  То есть теперь мы это можем сами на самом деле упростить
[50:50.740 --> 50:53.740]  и записать это следующим образом.
[50:53.740 --> 50:55.740]  Это у нас получается математическое ожидание.
[50:55.740 --> 50:58.740]  И фикси в точке
[51:03.740 --> 51:04.740]  ty.
[51:05.740 --> 51:06.740]  Вот.
[51:07.740 --> 51:10.740]  Вроде никакой магии не произошло, все более-менее понятно.
[51:11.740 --> 51:14.740]  То есть просто здесь же у вас y по сути фиксированный,
[51:14.740 --> 51:15.740]  то есть y это константа.
[51:15.740 --> 51:19.740]  Вот это это характеристическая функция xi в точке yt.
[51:19.740 --> 51:21.740]  Собственно, вот она.
[51:21.740 --> 51:24.740]  Ну и по сути мы вот свернули вот к такому виду.
[51:24.740 --> 51:26.740]  А это штрих куда пропало?
[51:27.740 --> 51:30.740]  Ну вот это штрих у нас вот здесь спрятано в плотности.
[51:30.740 --> 51:35.740]  Вот в этом математическом ожидании у вас как раз таки плотность это штриха скрыта.
[51:35.740 --> 51:37.740]  Давайте вот здесь напишем математическое ожидание по вот это штрих,
[51:37.740 --> 51:39.740]  чтобы ничего не терялось.
[51:39.740 --> 51:42.740]  Ну и собственно самая достаточная вот эта вот формула.
[51:45.740 --> 51:46.740]  Вот это.
[51:46.740 --> 51:50.740]  То есть самое главное равенство здесь между вот этим утверждением,
[51:50.740 --> 51:53.740]  вот этой записью и вот этой записью.
[51:53.740 --> 51:58.740]  Потому что сейчас мы ее будем пользоваться, чтобы досчитать.
[51:58.740 --> 51:59.740]  Все.
[52:00.740 --> 52:01.740]  Так.
[52:04.740 --> 52:08.740]  Я же говорил, что нам пригодится характеристическая функция экспоненциальной случайной величины.
[52:11.740 --> 52:13.740]  И вот она нам сейчас пригодится.
[52:13.740 --> 52:19.740]  А у xi у нас как раз таки экспоненциальное распределение с параметром 2.
[52:19.740 --> 52:27.740]  Следовательно, характеристическая функция случайной величины xi это лямда на лямда минус и т.
[52:30.740 --> 52:31.740]  Вроде бы так.
[52:33.740 --> 52:36.740]  А, ну прошу прощения, лямда у нас два, поэтому можно двоечку подставить.
[52:41.740 --> 52:42.740]  Вот.
[52:42.740 --> 52:43.740]  Ну и все.
[52:43.740 --> 52:44.740]  Что от нас требуется?
[52:44.740 --> 52:45.740]  По сути требуется.
[52:47.740 --> 52:48.740]  Посчитать.
[52:49.740 --> 52:50.740]  Вот.
[52:50.740 --> 52:53.740]  Теперь возвращаемся вот к той записи в правой части доски.
[52:53.740 --> 52:55.740]  От нас требуется посчитать.
[52:55.740 --> 52:57.740]  Математическое ожидание.
[53:07.740 --> 53:08.740]  Ладно, здесь можно.
[53:09.740 --> 53:10.740]  В общем, здесь лучше тогда.
[53:10.740 --> 53:11.740]  Давайте мы так сделаем.
[53:11.740 --> 53:12.740]  У же это у нас по сути это и есть.
[53:12.740 --> 53:14.740]  Мы поэтому здесь перепишем с вами вот так.
[53:19.740 --> 53:20.740]  И вот тогда.
[53:20.740 --> 53:21.740]  Все.
[53:28.740 --> 53:29.740]  Вот.
[53:29.740 --> 53:30.740]  Ну, а это уже считается вроде как не очень сложно.
[53:34.740 --> 53:38.740]  А просто по определению математического ожидания это будет интегральчик.
[53:38.740 --> 53:39.740]  По R вот этой функции.
[53:44.740 --> 53:46.740]  Вот здесь уже переходим к переменным.
[53:47.740 --> 53:48.740]  И плотность.
[53:49.740 --> 53:50.740]  Штрих с точки Y.
[53:51.740 --> 53:52.740]  Для Y.
[53:53.740 --> 53:54.740]  Вот.
[53:54.740 --> 53:56.740]  А плотность, это штрих.
[53:57.740 --> 53:58.740]  Чему у нас равна?
[54:02.740 --> 54:04.740]  Видимо, это одна вторая штриха.
[54:04.740 --> 54:05.740]  Видимо, это одна вторая.
[54:06.740 --> 54:07.740]  На индикатор.
[54:08.740 --> 54:11.740]  А Y принадлежит от минус двух до двух.
[54:12.740 --> 54:13.740]  От минус двух до нуля.
[54:13.740 --> 54:16.740]  Потому что у нас изначально была случайная величина от минус единички до единички.
[54:16.740 --> 54:18.740]  Здесь мы вычли из ее единичку.
[54:18.740 --> 54:20.740]  Она осталась также равномерно распредлена.
[54:20.740 --> 54:22.740]  Только теперь на новом отрезке минус два ноль.
[54:23.740 --> 54:24.740]  Вот.
[54:24.740 --> 54:26.740]  И, соответственно, вот эта плотность у нас вот так представлена.
[54:29.740 --> 54:30.740]  Ну, и то есть.
[54:30.740 --> 54:33.740]  Нам нужно посчитать интегральщик от минус двух до нуля.
[54:35.740 --> 54:36.740]  Вот.
[54:36.740 --> 54:37.740]  Одна вторая двоечка сократится.
[54:37.740 --> 54:39.740]  Получается, будет единичка на два.
[54:39.740 --> 54:40.740]  Минус лямбда T.
[54:44.740 --> 54:45.740]  Лямбда и T.
[54:46.740 --> 54:47.740]  Все.
[54:47.740 --> 54:48.740]  Тишечка не теряется.
[54:55.740 --> 54:56.740]  Так.
[54:56.740 --> 54:57.740]  Сейчас, одну секунду.
[54:57.740 --> 54:58.740]  Так.
[54:58.740 --> 54:59.740]  Два.
[54:59.740 --> 55:00.740]  Два минус и T.
[55:00.740 --> 55:01.740]  Два.
[55:01.740 --> 55:02.740]  Два минус.
[55:02.740 --> 55:03.740]  Так.
[55:03.740 --> 55:06.740]  Два минус и T у нас, по идее, было.
[55:06.740 --> 55:07.740]  Мы подставляем вместо T.
[55:07.740 --> 55:08.740]  TY.
[55:08.740 --> 55:09.740]  TY.
[55:09.740 --> 55:12.740]  Ну, ТТТ штрих, по сути.
[55:12.740 --> 55:13.740]  Вот.
[55:13.740 --> 55:15.740]  А теперь переходим к переменам.
[55:15.740 --> 55:16.740]  И у нас здесь остается.
[55:19.740 --> 55:20.740]  И.
[55:20.740 --> 55:21.740]  TY.
[55:21.740 --> 55:22.740]  Вот.
[55:23.740 --> 55:24.740]  И.
[55:24.740 --> 55:25.740]  TY.
[55:25.740 --> 55:28.740]  Вот так вот, вот такой интегральщик.
[55:35.740 --> 55:36.740]  TY.
[55:36.740 --> 55:37.740]  Да.
[55:37.740 --> 55:38.740]  Спасибо большое.
[55:38.740 --> 55:41.740]  Немножко потерялся в обозначениях.
[55:43.740 --> 55:44.740]  Вот.
[55:45.740 --> 55:48.740]  Ну, как такой интеграл дочитывать, более-менее понятно.
[55:48.740 --> 55:54.580]  Домножаем просто на сопряженный знаменатель. У вас будет просто сумма двух интегралов.
[55:54.580 --> 55:59.740]  Один будет вещественный, из второго ишечка у вас вылазит, и второй интегральчик тоже посчитать надо.
[55:59.740 --> 56:07.580]  Можем еще один шаг сделать.
[56:18.740 --> 56:27.300]  Так, если мы сделаем еще один шаг, то по сути давайте домножим на сопряженный знаменатель.
[56:27.300 --> 56:36.140]  2 плюс и ty. У нас по сути будет интегральчик от минус 2 до нуля,
[56:36.140 --> 56:48.860]  плюс и ty, а в знаменателе у нас будет, видимо, 4 плюс ty в квадрате.
[56:48.860 --> 56:55.780]  Ну это разбивается на два интеграла по линейности просто.
[57:06.140 --> 57:15.260]  Вот это что-то с арктангенсом будет, вот, и второй интегральчик будет вот такой.
[57:15.260 --> 57:26.380]  Ty на 4 плюс ty в квадрате dy. Вот, ну а здесь просто вот это выражение можно под диффенциал засунуть,
[57:26.380 --> 57:34.620]  ну это будет логарифм какой-то. Ну и там в ответе будет как раз таки арктангенс чего-то там,
[57:34.620 --> 57:43.540]  плюс логарифм, ну плюс и на логарифм. Вот, и это итоговое. Давайте я выпишу просто ответ тогда,
[57:43.540 --> 58:02.820]  наверное. Ответ у меня получился вот такой. Арктангенс. Так, да.
[58:04.620 --> 58:18.020]  Так, давайте вот так. И это вот сюда. Арктанг с t на t, минус t пополам,
[58:18.020 --> 58:39.420]  на логарифм t в квадрате плюс 1 поделить на t. Все. Есть вопросы по этой задачке? Вроде вот, в общем,
[58:39.420 --> 58:46.100]  на самом деле, вот если самое главное, что в этой задаче есть, это вот переход вот отсюда вот к такому
[58:46.100 --> 58:56.900]  выражению. Получается от непонятной как считающейся хор функции, а к мотожиданию хор функции от
[58:56.900 --> 59:06.700]  случайной величины. Вот. И в принципе, этим можно пользоваться фактом на контрольной, если у вас там
[59:06.700 --> 59:17.100]  будет хор функция произведения двух независимых случайных величин. Так. Окей вопросы? Ну тогда
[59:17.100 --> 59:28.780]  пойдемте дальше. Соответственно, с характеристическими функциями мы, наверное, закончили. Да. Переходим к следующему
[59:28.780 --> 59:42.100]  разделу. Следующий раздел у нас это сходимость и предельная теория. Собственно, я вот тут где-то
[59:42.100 --> 59:47.180]  мог там с константами по ошибаться. Я потом, в общем, пришли в решение затеханное. Там я вроде не
[59:47.180 --> 01:00:16.580]  должен был ошибиться, хотя тоже мог. Так, так, так, так. Сходимости и предельная теория.
[01:00:17.180 --> 01:00:33.500]  Значит, начнем мы с вами с сходимости скалярных, то есть в одномерном случае. Потом посмотрим на
[01:00:33.500 --> 01:00:38.900]  сходимости векторов, а потом уже какие-нибудь преобразования векторов будем рассматривать и
[01:00:38.900 --> 01:00:49.500]  сходимости их. Так, собственно. Собственно, вот. Вообще мы рассматривали в курсе четыре вида
[01:00:49.500 --> 01:00:56.420]  сходимости, но сейчас мы с вами обсудим только три. То есть эта сходимость, вот, наверное,
[01:00:56.420 --> 01:01:08.260]  по определению. Это получается же вероятность тех аминь, где у вас нет сходимости, равно нулю.
[01:01:08.260 --> 01:01:17.620]  Так, сходимость по мере. По определению, это вот такое выражение.
[01:01:27.300 --> 01:01:39.020]  И сходимость по распределению. Для любой f непрерывно-ограничной
[01:01:39.020 --> 01:01:45.780]  сходятся математические ожидания.
[01:01:45.780 --> 01:02:00.540]  Вот. Наверное, определение долго обсуждать не будем. Перейдем к чему-нибудь более интересному.
[01:02:00.540 --> 01:02:04.420]  Давайте сначала поймем с вами, как эти сходимости между собой связаны.
[01:02:04.420 --> 01:02:23.220]  Значит так, утверждение. Связь сходимости. Ну, видов сходимости. Ну, более-менее. Думаю,
[01:02:23.220 --> 01:02:27.540]  что вы все знаете, что сходимости почти наверное следует сходимость по вероятности, а сходимости
[01:02:27.540 --> 01:02:32.260]  по вероятности следует сходимость по распределению. Вот. Стрелочки этим доказывать с вами не будем.
[01:02:32.260 --> 01:02:38.500]  Они у вас будут доказываться, ну, или уже доказаны были на лекциях. А, соответственно,
[01:02:38.500 --> 01:02:46.740]  дальше хочется, наверное, показать, что вот обратные стрелочки не верны в общем случае,
[01:02:46.740 --> 01:02:53.700]  то есть попроводить контрпримеры. Два контрпримера, соответственно, нам нужно,
[01:02:53.700 --> 01:02:58.340]  когда есть сходимость по вероятности и нет сходимости почти наверное. Это первый контрпример.
[01:02:58.340 --> 01:03:09.780]  Ну, здесь базовый пример риса. Думаю, все помните. Там, в общем, рассматривается вероятностное
[01:03:09.780 --> 01:03:17.060]  пространство просто отрезочек 0,1. В качестве сигма алгебры рассматривается баррельская сигма
[01:03:17.060 --> 01:03:24.940]  алгебра под множество отрезка 0,1. А в качестве меры рассматривается вот стандартная мера либега.
[01:03:24.940 --> 01:03:34.540]  Вот. И дальше строится последовательность множеств. Такого вида.
[01:03:34.540 --> 01:03:46.380]  И дальше строится последовательность как раз таки случайных величин,
[01:03:46.380 --> 01:03:54.380]  которые просто являются индикаторами этих множеств. И уже из них строится итоговая
[01:03:54.380 --> 01:04:05.660]  последовательность. И так далее. Ну, если графически вот это все изображает, что я нарисовал,
[01:04:05.660 --> 01:04:14.020]  то, в общем, там первая случайная величина. Это, по сути, просто константа 1 на всем отрезке 0,1.
[01:04:14.020 --> 01:04:24.660]  Вторая случайная величина. Это просто получается константа на первой половинке отрезка. То есть
[01:04:24.660 --> 01:04:32.700]  от 0 до индикатора, в общем, отрезка от 0 до 1,2. То есть функция вот так выглядит. А третья, так,
[01:04:32.700 --> 01:04:44.060]  это первая, это вторая. Третья выглядит следующим образом. Это уже будет индикатор отрезка от 1,2 до 1,
[01:04:44.060 --> 01:04:49.660]  и так далее. То есть в чем суть? Вы покрыли весь отрезок, потом вы покрыли его сначала одну
[01:04:49.660 --> 01:04:54.380]  половинку, потом вторую половинку. Потом разбиваете на три части их. То есть дальше берете следующую
[01:04:54.380 --> 01:04:58.900]  функцию. Четвертая. Это будет индикатор первой, третьей отрезка. Пятая функция индикатор второй,
[01:04:58.900 --> 01:05:03.340]  третьей отрезка. Шестая функция индикатор третьей, третьей отрезка. Дальше делим на четыре части.
[01:05:03.340 --> 01:05:14.620]  Можно четверть, не особо важно. В чем суть? В том, что если вы зафиксируете какую-то омега,
[01:05:14.620 --> 01:05:21.140]  то вот смотрите, у вас эта омега, во-первых, при фиксированном омега у вас последовательность
[01:05:21.140 --> 01:05:25.660]  превращается в обычную числовую последовательность. То есть если омега зафиксирована, то у вас будет
[01:05:25.660 --> 01:05:37.180]  кси1 от омега, там кси1-1, кси2-1 от омега, кси2-2 от омега. Ну вот такая последовательность,
[01:05:37.180 --> 01:05:41.620]  просто числовая. И соответственно, если вы зафиксируете какую-то омега, то вот у вас здесь
[01:05:41.620 --> 01:05:48.180]  будет единичка. Ну потом единичка будет одна из вот этих двух. Один из этих двух элементов будет
[01:05:48.180 --> 01:05:54.820]  единичкой. То есть если вы выбрали омега, которая от 0 до 1 и второй, то у вас получается вторая
[01:05:54.820 --> 01:06:00.260]  функция, его накроют. Если в правом половинке, то третья. Вот. И в чем суть? Каждая омега,
[01:06:00.260 --> 01:06:06.580]  у вас накрывается первая функция, накрывается 2 или 3, накрываются 3, 4, получается 4, 5 или 6,
[01:06:06.580 --> 01:06:12.040]  накрывается 7, 8, 9 или 10. И так далее. То есть грубо говоря, у вас в этой последовательности
[01:06:12.040 --> 01:06:17.740]  бесконечно встречаются единички. Они встречаются все реже, реже и реже, но они встречаются
[01:06:17.740 --> 01:06:23.860]  бесконечно. Ну и получается у вас нет сходимости вот этой последовательности к 0. Собственно
[01:06:23.860 --> 01:06:27.860]  сходимость почти наверно мы не имеем. Ну на самом деле вообще просто ни в одной
[01:06:27.860 --> 01:06:31.900]  точке у нас сходимости нет. То есть при любом фиксированном омега вот эти
[01:06:31.900 --> 01:06:36.900]  рассуждения верны. Почему есть сходимость по мере? Ну потому что меры тех точек,
[01:06:36.900 --> 01:06:41.300]  где вы отличаетесь больше чем на Эпсилон, это по сути длина вот этого отрезка. И как
[01:06:41.300 --> 01:06:46.240]  видите, она уменьшается. И соответственно вот это вот определение выполнено. А вот
[01:06:46.240 --> 01:06:50.720]  это нет. Ну и все. То есть мы имеем с вами сходимость по вероятности и не имеем
[01:06:50.720 --> 01:06:53.480]  сходимости почти наверно. Соответственно первый контрпример мы с
[01:06:53.480 --> 01:06:59.000]  вами построили. Вот. Если чуть более детально нужно, я могу что-то еще сказать,
[01:06:59.000 --> 01:07:03.600]  но вроде как более-менее все понятно. Это там на витами было где-то еще. Больше
[01:07:03.600 --> 01:07:13.840]  наверно не имеет смысла повторять. Так, далее. Второй контрпример. Значит ксен у
[01:07:13.840 --> 01:07:28.280]  нас сходится по распределению, но у нас нет сходимости по вероятности. Вот. Здесь
[01:07:28.280 --> 01:07:32.540]  контрпример такой. Обычно очень искусственно строится. То есть рассматривается какое-нибудь
[01:07:32.540 --> 01:07:38.640]  пространство из двух элементарных сходов. Вот. И соответственно все ксенты
[01:07:38.640 --> 01:07:43.040]  полагаются так. Так. Давайте аккуратно зададим. Значит это пространство элементарных
[01:07:43.040 --> 01:07:47.680]  сходов в качестве сигма алгебры, так как дискретное пространство. Возьмем просто множество
[01:07:47.680 --> 01:07:52.400]  всех подножеств. В качестве меры, ну возьмем равномерно распределенную меру на этих
[01:07:52.400 --> 01:08:01.720]  точках. То есть мера такая, что вероятность омега 1 равняется вероятности омега 2 и равняется
[01:08:01.720 --> 01:08:15.240]  1 и 2. Что имеем? Соответственно, определим с вами ксен следующим образом. Так,
[01:08:15.240 --> 01:08:25.600]  давай так. Ксен от омега равняется единичка, если омега равняется омега 1, и 0, если омега
[01:08:25.600 --> 01:08:31.480]  равняется омега 2. А предельную случайную величину определим симметрично наоборот.
[01:08:45.480 --> 01:08:51.440]  Таким вот образом мы определили. Вот. Ну и что мы по сути с вами имеем? В пределе у вас,
[01:08:51.520 --> 01:08:57.000]  ну по сути, обе случайные величины имеют Бернулевское распределение. Они немножко по
[01:08:57.000 --> 01:09:01.520]  разному устроены, но у обоих этих случайных величин, у этой последовательности и у предельных
[01:09:01.520 --> 01:09:09.520]  случайной величины одно и то же распределение Бернулевское с параметром 1 и 2. Вот. Соответственно,
[01:09:09.520 --> 01:09:12.340]  сходимость по распределению будет. У вас просто совпадают распределения, вследовательно у вас
[01:09:12.340 --> 01:09:17.800]  будут совпадать математические ожидания. Вот в этом определении у вас каждый элемент вот в этой
[01:09:17.800 --> 01:09:20.400]  последовательности математических ожиданий будет совпадать с предельным,
[01:09:20.400 --> 01:09:26.680]  поэтому вот эта сходимость у восточной есть, а почему не будет сходимости по
[01:09:26.680 --> 01:09:31.300]  вероятности, ну потому что давайте вот зафиксируем любой епсилон, давайте
[01:09:31.300 --> 01:09:39.200]  зафиксируем епсилон равный 1 и 2, и рассмотрим с вами вероятность того, что
[01:09:39.200 --> 01:09:53.760]  ксин минус кси больше епсилона, вот, ну при любом омега у вас вот здесь значение
[01:09:53.760 --> 01:09:57.520]  противоположное, поэтому вот это всегда единичка, соответственно для любого омега
[01:09:57.520 --> 01:10:01.640]  у вас вот эта разность больше, чем епсилон, равная 1 и 2, ну и соответственно вот это
[01:10:01.640 --> 01:10:11.080]  просто равно 1, и не сходится к нулю. Такой вот искусственный пример, но он, собственно,
[01:10:11.080 --> 01:10:15.640]  показывает, что сходимость по распределению не следует сходимости по вероятности. Так,
[01:10:15.640 --> 01:10:23.960]  по этому конфпремеру вопрос есть? Ну я думаю, что все понятно. Так, идем дальше. Дальше.
[01:10:31.640 --> 01:10:40.760]  Ну задачек на них скорее всего не будет. Ну как бы лп-сходимость, наверное, у вас там
[01:10:40.760 --> 01:10:46.400]  только во время определения дневника встречалась, а дальше она достаточно редко встречается в
[01:10:46.400 --> 01:10:59.040]  приложениях, по крайней мере в тех, которые мы обсуждаем. Так, идем дальше. Но все-таки вот
[01:10:59.080 --> 01:11:07.600]  эти стрелочки иногда можно оборачивать. То есть в общем случае исходимости по распределению не
[01:11:07.600 --> 01:11:10.960]  следует сходимости по вероятности, исходимости по вероятности не следует сходимости почти
[01:11:10.960 --> 01:11:19.720]  наверно. Но есть как бы такие частные случаи, когда у вас как бы эти стрелочки верны. Собственно,
[01:11:19.760 --> 01:11:30.800]  давайте это обозначим так. Когда можно стрелочки оборачивать? Я так и напишу.
[01:11:30.800 --> 01:11:44.760]  Ну вот первую стрелочку можно обернуть в случае дискретного вероятностного пространства.
[01:11:49.720 --> 01:12:03.720]  Ну ладно, давайте если у вас дискретно вероятностное пространство, то исходимости по мере, то есть по
[01:12:03.720 --> 01:12:13.520]  вероятности, у вас будет следовать сходимость почти наверно. Но это действительно так. Давайте
[01:12:13.520 --> 01:12:22.080]  просто с вами запишем определение сходимости по вероятности. Зафиксируем какой-нибудь
[01:12:22.080 --> 01:12:26.080]  элементарный исход из тех, которых у нас есть. У нас элементарных исходов всего счетное число,
[01:12:26.080 --> 01:12:33.320]  поэтому можем его зафиксировать. И у нас из этого определения выполнена следующая сходимость.
[01:12:33.320 --> 01:12:47.880]  Так? Вроде так. А это просто определение сходимости по вероятности. Соответственно,
[01:12:47.880 --> 01:12:53.840]  если вот это сходится к нулю, значит с какого-то момента это будет меньше. Ну существует номер,
[01:12:53.840 --> 01:12:58.440]  начиная с которого вероятность вот этого множества будет меньше, чем вероятность вот этого омега,
[01:12:58.440 --> 01:13:03.120]  который мы с вами зафиксировали. Соответственно, вот это омега здесь уже лежать не может. И
[01:13:03.120 --> 01:13:08.720]  аналогичное рассуждение можно для всех омега провернуть. То есть на самом деле у вас вот в этом
[01:13:08.720 --> 01:13:13.560]  множестве ни одно омега не предлежит. Соответственно, у вас все омеги попадают в дополнение к этому
[01:13:13.560 --> 01:13:23.920]  событию, а значит у вас есть сходимость почти наверно. Есть вопросы? Вот. Ну думаю, тоже не
[01:13:23.920 --> 01:13:28.920]  очень сложно. Вот. И более интересный случай это, конечно, вот когда мы можем оборачивать вот эту
[01:13:28.920 --> 01:13:40.240]  стрелочку. Для этого нужно сейчас теориям Александрова сказать. Да, ладно, сейчас.
[01:13:40.240 --> 01:14:01.640]  Собственно, если идти в каком-то хронологическом порядке, то мы можем с вами сначала рассмотреть
[01:14:01.640 --> 01:14:07.680]  какое-то дискретное вероятностное пространство и изучить в нем сходимость по распределению немножко
[01:14:07.680 --> 01:14:22.080]  подробнее. Давайте зафиксируем с вами дискретное вероятностное пространство. Например, связанное
[01:14:22.080 --> 01:14:27.720]  со случайной величиной кси, которая принимает, допустим, пусть только натуральные значения. Такая
[01:14:27.720 --> 01:14:34.840]  задачка, по-моему, у вас висточки должны были быть. То есть давайте так запишем это. Сумма вероятностей
[01:14:34.840 --> 01:14:43.280]  того, что кси равняется n, где n принадлежит натуральным числам, равна 1. То есть кси принимает
[01:14:43.280 --> 01:14:54.200]  только натуральные значения. Тогда ну и кси и кси n. То есть рассматриваем последовательность
[01:14:54.200 --> 01:14:59.800]  случайных величин и предельную случайную величину какую-то, которую принимают только натуральные
[01:14:59.800 --> 01:15:12.080]  значения. Так вот, тогда сходимость по распределению в дискретном случае равносильна тому, что для любого n.
[01:15:12.080 --> 01:15:20.640]  Так, только сейчас будет коллапс обозначений. Давайте мы здесь лучше вот этот наказчик поменяем,
[01:15:20.640 --> 01:15:22.360]  чтобы с n не конфликтовало.
[01:15:22.360 --> 01:15:51.160]  То есть в дискретном случае сходимость по распределению эквивалентна тому, что вы в каждой
[01:15:51.160 --> 01:15:57.360]  точке, то есть вы принимаете только натуральные значения и значит сходимость по распределению
[01:15:57.360 --> 01:16:03.240]  эквивалентна тому, что у вас есть сходимость в каждой вот натуральной точке таких вот вероятностей.
[01:16:03.240 --> 01:16:14.320]  Вот, но это доказывается очень просто на самом деле. Давайте в одну сторону это покажем. Давайте
[01:16:14.320 --> 01:16:21.000]  покажем это вправо допустим. В левой чуть сложнее, но мы покажем вправо. То есть из
[01:16:21.000 --> 01:16:26.200]  определения сходимости по распределению у нас верно, что для любой f непрерывно ограниченный
[01:16:26.200 --> 01:16:37.760]  математическое ожидание f кси от m сходится к математическому ожиданию f от кси. Давайте
[01:16:37.760 --> 01:16:45.760]  мы с вами рассмотрим функцию f, f кату. Ну вот, я ее сейчас графически нарисую. Она в точке k
[01:16:45.760 --> 01:16:51.560]  будет принимать значение 1, а затем как-то вот так вот по непрерывности продолжена вот в общем
[01:16:51.560 --> 01:17:00.440]  вот такая вот горочка. В точке k минус 1 она будет 0, в точке k плюс 1 она будет 0, в точке k она
[01:17:00.440 --> 01:17:04.800]  будет 1. А дальше как-нибудь по непрерывности просто это продолжить. Вот, соответственно такую
[01:17:04.800 --> 01:17:10.880]  функцию, такая функция попадает под определение вот непрерывно ограниченной функции сходимости по
[01:17:10.880 --> 01:17:19.080]  распределению. Значит, ну ее можем применить. Супер. Ну и если вы просто запишете определение вот
[01:17:19.080 --> 01:17:22.920]  этого математического ожидания, то у вас по сути здесь и получится, что вероятность того,
[01:17:22.920 --> 01:17:29.080]  что кси равняется k, потому что во всех остальных точках у вас эта функция зановляется, соответственно
[01:17:29.080 --> 01:17:33.280]  у вас остается только одно слагаемое. Значение единичка умножается на вероятность этого значения.
[01:17:33.280 --> 01:17:42.240]  Получается только вот эта вероятность, которую мы ищем. Кси равняется k. То есть это выполнено.
[01:17:42.240 --> 01:17:49.160]  В обратную сторону чуть сложнее и сейчас это обсуждать не будем. То есть как бы если вот
[01:17:49.160 --> 01:17:53.040]  смотреть на дискретный случай такой достаточно простой, то как бы сходимость по распределению
[01:17:53.040 --> 01:17:56.720]  эквивалентна тому, что вы в каждой точке сходитесь, вот ваша вероятность в каждой
[01:17:56.720 --> 01:18:02.880]  точке сходится. Далее была попытка обобщить вот этот результат на непрерывный случай. Ну
[01:18:02.880 --> 01:18:05.520]  вот в непрерывном случае у нас как бы вероятность каждой точки равна нулю,
[01:18:05.520 --> 01:18:10.240]  поэтому что-то такое писать не совсем логично. Вот. Но зато можно сформулировать теорему
[01:18:10.240 --> 01:18:17.080]  Александрова, которая по сути дает какой-то похожий результат в непрерывном случае.
[01:18:17.080 --> 01:18:29.440]  Теорема Александрова. Ну это или следствие, там в общем в зависимости от того, как читает лекции,
[01:18:29.440 --> 01:18:32.480]  это может быть следствием, а может быть и самой теорема Александрова.
[01:18:32.480 --> 01:18:47.520]  Сходимость по распределению по сути эквивалентна тому, что для любой точки,
[01:18:47.520 --> 01:18:53.320]  сейчас я поясню эту запись, у вас есть сходимость функции распределения.
[01:18:53.320 --> 01:19:08.240]  Вот. Вот такая запись обозначает множество точек непрерывности предельной функции распределения.
[01:19:08.240 --> 01:19:15.640]  Ну то есть по сути мы с вами здесь имеем сходимость вот таких вот вероятностей.
[01:19:15.640 --> 01:19:25.240]  Какое-то вот обобщение вот того результата в дискретном случае,
[01:19:25.240 --> 01:19:27.640]  которое просто доказывается на неперерывный случай.
[01:19:27.640 --> 01:19:35.920]  А что здесь хочется сказать? Вот это существенно. Давайте рассмотрим пример.
[01:19:35.920 --> 01:19:44.240]  Пример такой. Пусть у нас есть последовательность случайных величин со следующими функциями
[01:19:44.520 --> 01:19:48.320]  распределения. Вот у первой случайной величины функция распределения будет вот такая.
[01:19:48.320 --> 01:19:54.200]  Ну то есть вот здесь эта единичка. И вот. В общем она вот так выглядит.
[01:19:54.200 --> 01:19:58.540]  А у второй случайной величины будет просто уголок острее.
[01:19:58.540 --> 01:20:06.140]  То есть будет какая-то случайная величина вот такая вот с такой функцией распределения.
[01:20:06.140 --> 01:20:10.440]  А третья функция распределения будет в еще более острый уголок вот таким.
[01:20:10.440 --> 01:20:13.600]  И соответственно более-менее понятно, что предельная функция распределения
[01:20:13.600 --> 01:20:18.600]  Это просто функция распределения константы, то есть вот такая штука.
[01:20:18.600 --> 01:20:24.600]  Но вот для каждой функции вот этой у вас в 0 значение 0.
[01:20:24.600 --> 01:20:29.600]  А что у предельной? По непрерывности справа здесь по идее должен быть 1.
[01:20:29.600 --> 01:20:35.600]  На самом деле не совсем так, но утверждать, что есть сходимость вот в этой точке мы не можем.
[01:20:35.600 --> 01:20:40.600]  То есть для всех точек непрерывности, где вот ваша итоговая функция распределения непрерывна,
[01:20:40.600 --> 01:20:45.600]  у вас сходимость есть честная, а вот в точках разрыва итоговая функция распределения
[01:20:45.600 --> 01:20:48.600]  вы про сходимость ничего говорить не можете.
[01:20:48.600 --> 01:20:54.600]  Ну и собственно вот про это теорема Александрова и говорит.
[01:20:54.600 --> 01:21:00.600]  То есть как бы сходимость по распределению как-то в терминах функции распределения выражается.
[01:21:00.600 --> 01:21:03.600]  И это мы сейчас будем использовать.
[01:21:03.600 --> 01:21:06.600]  Напоминаю, что мы хотели с вами обратить стрелочку.
[01:21:06.600 --> 01:21:13.600]  То есть мы хотели привести какой-то частный случай, когда сходимость по распределению следует сходимость по вероятности.
[01:21:18.600 --> 01:21:23.600]  Так вот, утверждение. Оно нам сегодня еще не раз пригодится.
[01:21:23.600 --> 01:21:28.600]  А утверждение следующее, что если у нас последовательность случайных величин
[01:21:28.600 --> 01:21:36.600]  сходится по распределению Константия, тогда она сходится и по вероятности.
[01:21:39.600 --> 01:21:46.600]  Ну, стрелочка справа налево более-менее очевидна, потому что изходимости по вероятности всегда следует изходимость по распределению.
[01:21:46.600 --> 01:21:49.600]  Гораздо интереснее стрелочка вправо.
[01:21:49.600 --> 01:21:54.600]  Ну и здесь по сути нужно просто два раза применить теорему Александрова, который мы только что выписали.
[01:21:54.600 --> 01:21:59.600]  Давайте запишем определение сходимости по вероятности.
[01:21:59.600 --> 01:22:08.600]  То есть для любого Эпсилона вероятность того, что к 7 минус Константа С больше Эпсилон должно стремиться, по идее, к нулю.
[01:22:08.600 --> 01:22:11.600]  Ну давайте мы это сейчас как-нибудь распишем.
[01:22:18.600 --> 01:22:21.600]  По сути просто модуль раскрыли.
[01:22:25.600 --> 01:22:28.600]  Вот что-то такое получается.
[01:22:28.600 --> 01:22:30.600]  И что здесь хочется заметить?
[01:22:30.600 --> 01:22:38.600]  Ну смотрите, вот это же по сути практически функция единичка минус функция распределения в точке С плюс Эпсилон.
[01:22:38.600 --> 01:22:57.600]  А вот это практически функция распределения в точке С минус Эпсилон.
[01:22:57.600 --> 01:23:01.600]  Придельная функция распределения у нас выглядит вот так.
[01:23:01.600 --> 01:23:05.600]  То есть у нас предельная случайная величина это Константа, вот здесь по утверждению.
[01:23:05.600 --> 01:23:08.600]  Соответственно предельная функция распределения имеет вот такой вид.
[01:23:14.600 --> 01:23:17.600]  То есть про сходимость функции распределения в точке С мы ничего говорить не можем.
[01:23:17.600 --> 01:23:20.600]  Но вот для всех остальных точек мы уже про сходимость что-то знаем.
[01:23:20.600 --> 01:23:25.600]  Мы знаем, что какую бы точку в праве Ц вы не закрепили, она будет стремиться,
[01:23:25.600 --> 01:23:30.600]  последовательность функции распределения в этой точке к С будет стремиться к единичке.
[01:23:30.600 --> 01:23:37.840]  соответственно, вот это будет стремиться к нулю. А если вы влево отступите, то
[01:23:37.840 --> 01:23:41.320]  предельная функция распределения равна нулю, значит и все f-ксиенты в этой
[01:23:41.320 --> 01:23:44.520]  точке будут стремиться к нулю. Соответственно, вся вот эта сумма
[01:23:44.520 --> 01:23:48.280]  стремится к нулю, ну и мы получили с вами, что вот это выражение сходит к нулю.
[01:23:48.280 --> 01:23:53.240]  То есть мы с вами получили, что исходимость по распределению в
[01:23:53.240 --> 01:24:02.120]  константе, следует исходимость по вероятности в константе. Вот. Есть вопросы?
[01:24:02.680 --> 01:24:14.200]  Жаль. Так, идем дальше. Стрелочки мы с вами пообращали, теперь можно переходить к
[01:24:14.200 --> 01:24:16.960]  векторной исходимости.
[01:24:32.440 --> 01:24:36.440]  Так, векторная исходимость.
[01:24:36.440 --> 01:24:45.200]  Ну вот, на самом деле гораздо интереснее рассматривать не исходимость случайных
[01:24:45.200 --> 01:24:50.480]  величин, а исходимость случайных векторов. И вот там на самом деле такие же
[01:24:50.480 --> 01:25:01.320]  виды исходимости, как и для случайных величин. Вот. Обозначение ровно такие же.
[01:25:01.360 --> 01:25:06.080]  Вот. Определения немножко меняются. Ну, как меняются
[01:25:06.080 --> 01:25:10.200]  определения? Здесь не так.
[01:25:16.600 --> 01:25:21.320]  Вот здесь просто модуль меняется на норму, потому что мы теперь с векторами
[01:25:21.320 --> 01:25:23.720]  работаем.
[01:25:31.320 --> 01:25:36.000]  Так, ну и здесь у нас теперь мы к векторам применяем функции, поэтому у нас теперь
[01:25:36.000 --> 01:25:46.120]  для любой f ограниченный непрерывный, f действует у нас из r в, ну давайте мы с
[01:25:46.120 --> 01:25:50.280]  вами скажем, что мы живем в ммерном пространстве, потому что n это у нас
[01:25:50.280 --> 01:25:58.800]  индекс у последовательности, живем в ммерном пространстве. Вот. Есть такая же
[01:25:58.800 --> 01:26:02.520]  исходимость моментов.
[01:26:09.800 --> 01:26:16.680]  Супер. Что с этим можно делать? Ну, пока ничего. Пока можно понять, как связана
[01:26:16.680 --> 01:26:21.360]  исходимость векторов и по компонентной исходимость. То есть правда ли, что если
[01:26:21.360 --> 01:26:26.240]  у вас сходятся компоненты в том или ином смысле, тогда у вас сходятся и вектора в
[01:26:26.240 --> 01:26:31.200]  том же смысле. Ну и вот ответ на этот вопрос положителен для сходимости по
[01:26:31.200 --> 01:26:35.480]  почти наверное и по вероятности. А для сходимости по распределению есть небольшая
[01:26:35.480 --> 01:26:39.200]  проблема, из-за которой собственно у вас появилась вторая задача в контрольной
[01:26:39.200 --> 01:26:45.560]  работе. Если бы этой проблемы не было, эти задачи бы не решались. Ну в смысле, они
[01:26:45.560 --> 01:26:54.040]  были бы неинтересны. Итак, следующий подпараграф связь по компонентной и
[01:26:54.480 --> 01:26:57.480]  векторной исходимости.
[01:27:03.320 --> 01:27:12.200]  Значит, утверждение такое. Вот если у вас вектора сходится почти наверное, это
[01:27:12.200 --> 01:27:16.800]  эквивалентно тому, что и каждые компоненты будут сходиться почти наверное.
[01:27:17.800 --> 01:27:30.000]  Если у вас вектора сходится по вероятности, это эквивалентно также тому, что и у вас
[01:27:30.000 --> 01:27:41.200]  компоненты сходятся по вероятности. Если у вас вектора сходится по распределению,
[01:27:41.600 --> 01:27:46.760]  то из этого следует, что у вас компоненты сходятся по распределению. В обратную
[01:27:46.760 --> 01:27:55.920]  сторону стрелочка не верна. Вот. Ну и давайте как-то вот чисто идейно обсудим
[01:27:55.920 --> 01:27:59.960]  доказательства этого утверждения.
[01:28:00.840 --> 01:28:03.840]  Оно не очень сложное.
[01:28:12.040 --> 01:28:17.960]  Значит, для сходимости почти наверное, но это мы докажем аккуратно. То есть нам
[01:28:17.960 --> 01:28:21.600]  такие омеги подходят, где сходятся
[01:28:23.760 --> 01:28:28.440]  наши вектора и где сходятся наши компоненты.
[01:28:34.880 --> 01:28:39.120]  Ну вот, с курсом от анализа мы знаем, что сходимость вектора эквивалентна тому,
[01:28:39.440 --> 01:28:43.800]  что сходится каждого компонента, ну, в обычном смысле. А теперь, если у вас вектор
[01:28:43.800 --> 01:28:49.060]  сходится почти наверное, значит вероятность вот этого множества равна единичке.
[01:28:49.520 --> 01:28:52.920]  Соответственно, вот это множество вложено в каждый из них. Значит, у каждого вот
[01:28:52.920 --> 01:28:56.200]  этого множества из пересечения вероятность хотя бы единичка. Ну, значит
[01:28:56.200 --> 01:29:01.960]  единичка. В обратную сторону предположим, что у вас все компоненты сходятся и у
[01:29:01.960 --> 01:29:05.840]  каждого вот такого множества вероятность единичка. Пересечение множества единичной
[01:29:05.840 --> 01:29:11.000]  конечного числа тоже множество единичные меры. Соответственно, ну тоже доказали.
[01:29:11.000 --> 01:29:22.240]  Первый пункт доказан. Пункт номер два. Так, здесь немножко поинтереснее. Здесь нужно следующее,
[01:29:22.240 --> 01:29:45.320]  заметите. Значит, первое, что есть вот такая вложенность. Так, ну почему? Если у вас хотя бы одна
[01:29:45.320 --> 01:29:49.360]  компонента отличается больше чем на эпсилон, то понятно, что и норма вашего вектора отличается
[01:29:49.360 --> 01:29:53.560]  больше чем на эпсилон. Ну вот, от предельного. Это более-менее понятно, если вы просто вот
[01:29:53.560 --> 01:30:01.320]  эту норму запишете. Это обычная евклидовая норма. То есть мы имеем сами здесь сумму кснk
[01:30:01.320 --> 01:30:11.440]  минус кск в квадрате. Пока от единички даем. Соответственно, если у вас одно слагаемое хотя бы
[01:30:11.440 --> 01:30:18.120]  эпсилон, то вот эта штучка тоже хотя бы эпсилон будет. Супер. То есть вот это событие вложено вот в
[01:30:18.120 --> 01:30:27.560]  это. То есть если у вас вот это теперь значит, если у вас вероятность вот этих вот событий
[01:30:27.560 --> 01:30:40.800]  сходится к нулю. Сейчас, одну секунду. Тут же вложение в другую сторону нужно было.
[01:30:40.800 --> 01:30:55.040]  Так, друзья, подсказывайте. Или я буду думать сейчас.
[01:31:10.800 --> 01:31:16.800]  А нет, ну все верно. Теперь, да... чуть затупил. Соответственно вероятность, если вот у вас
[01:31:16.800 --> 01:31:23.160]  вероятность вот этих множеств по определению векторной сходимости стремится к нулю, то получается
[01:31:23.160 --> 01:31:26.880]  для каждой компоненты у вас вероятность вот этих множеств тоже будетobookт средса к нулю.
[01:31:26.880 --> 01:31:30.960]  Потому что у вас здесь есть вот такая монотонность. Соответственно, если у вас есть
[01:31:30.960 --> 01:31:36.120]  векторная сходимость по вероятности, то у вас есть и покомпонентная сходимость по вероятности.
[01:31:36.120 --> 01:31:39.600]  Потому что вот по определению векторной сходимости по вероятности вот они стремятся к нулю.
[01:31:39.600 --> 01:31:44.280]  в силу вложенности, вот эта вероятность меньше, чем вероятность вот этого множества,
[01:31:44.280 --> 01:31:48.400]  значит она тоже стремится к нулю. Я просто стрелочку перепутал, да.
[01:31:48.400 --> 01:31:52.200]  Так, и второе вложение, которое стоит заметить, это вот такое вложение.
[01:32:09.600 --> 01:32:18.080]  Вот. А почему это вложение верно? Ну, смотрите, предположим, что оно неверно, то есть у нас
[01:32:18.080 --> 01:32:22.880]  модуль вектора отличается больше, чем на Эпсилон, а модуль каждой координаты, мы в объединение не
[01:32:22.880 --> 01:32:29.880]  попали, значит модуль каждой координаты отличается меньше, чем на Эпсилон поделить на корень из М.
[01:32:29.880 --> 01:32:38.880]  Ну, тогда если вы вот эту оценку подставите в определение нормы, то у вас здесь как раз-таки
[01:32:38.880 --> 01:32:44.000]  корень из М в квадрате даст М, М слагаемых, М очко уйдет, здесь будет Эпсилон в квадрате,
[01:32:44.000 --> 01:32:49.600]  просто Эпсилон. То есть мы изначально предположили, что у нас норма вектора больше, чем Эпсилон,
[01:32:49.600 --> 01:32:54.080]  и мы сюда не попадаем, но при этом мы с вами получаем, что норма вектора меньше либо равна,
[01:32:54.080 --> 01:32:58.120]  чем Эпсилон. Противоречие. Соответственно, у нас есть вот такое вложение. Поэтому,
[01:32:58.120 --> 01:33:10.920]  если здесь у нас конечное число последовательных событий стремится к нулю, то их объединение
[01:33:10.920 --> 01:33:14.560]  тоже будет стремиться к нулю и, соответственно, вот это будет стремиться к нулю. Тоже опять-таки
[01:33:14.560 --> 01:33:20.520]  по монотонности, потому что здесь вот такая влажность есть. Все. Так, есть вопросы по
[01:33:20.520 --> 01:33:29.960]  вот этому доказательству? Так, давай тогда первая пункт, где сходимость почти наверная. Смотри,
[01:33:29.960 --> 01:33:35.480]  предположим, у нас есть сходимость вектора. Покажем, что есть сходимость компонента. Если
[01:33:35.480 --> 01:33:40.680]  есть сходимость вектора, значит вероятность вот этого множества единичка. Если множество
[01:33:40.680 --> 01:33:45.520]  принадлежит пересечению каких-то множеств, значит оно лежит в каждом из них. Значит,
[01:33:45.520 --> 01:34:00.840]  вот это множество, ОМЕГ, принадлежит каждому вот такому событию. Нет, здесь, что каты компонента
[01:34:00.840 --> 01:34:09.560]  сходятся. Окей? Вот, то есть смотри, вот это множество у тебя вложено в каждое множество из
[01:34:09.560 --> 01:34:16.000]  вот этого пересечения, значит вероятность каждого вот такого множества больше либо равна чем единичка,
[01:34:16.000 --> 01:34:24.680]  ну получается единичка. Окей? Так, вот здесь я перепутал просто стороны, в которые я доказывал,
[01:34:24.680 --> 01:34:30.840]  но сейчас вроде все нормально. То есть вот это вложение понятно всем, да? И вот это вложение
[01:34:30.840 --> 01:34:45.000]  вроде тоже более-менее понятно. Окей? Ну, могу еще раз повторить, если надо. Ну, думаю... ну все,
[01:34:45.000 --> 01:34:54.000]  ладно, да. Идем дальше тогда. А, что дальше хотелось рассказать? Так, сейчас. Это мы поговорили
[01:34:54.000 --> 01:35:04.680]  про теория моих следований сходимости. Это мы поговорили о связи по компонентной и векторной
[01:35:04.680 --> 01:35:10.560]  сходимости. Еще мы не обсудили сходимость, связь сходимости вот в случае сходимости по распределению.
[01:35:10.560 --> 01:35:22.280]  Почему стрелочка вправо верна? Ну вот, давайте это докажем. Ну, потому что мы можем с вами
[01:35:22.280 --> 01:35:44.240]  рассмотреть вот такие выражения. Сейчас я запишу, потом объясню. Вот. Смотрите,
[01:35:44.240 --> 01:35:49.480]  p-катая это проектор, ну получается, просто который от вектора оставляет только одну координату
[01:35:49.960 --> 01:35:57.600]  вот. Вот эта функция все еще непрерывно и ограничена. Соответственно, вот эта сходимость
[01:35:57.600 --> 01:36:07.200]  выполнена просто по определению сходимости по распределению для векторов. Справедливо? Ну ладно,
[01:36:07.200 --> 01:36:16.920]  думаю, что справедливо. Вот. Ну и, собственно, вот если мы возьмем в качестве f непрерывно
[01:36:16.920 --> 01:36:20.400]  ограниченную функцию, то композиции непрерывно ограничены непрерывно и будут непрерывно ограничены.
[01:36:20.400 --> 01:36:27.560]  Соответственно, мы с вами тем самым показали, что каждая компонента сходится по распределению.
[01:36:27.560 --> 01:36:37.200]  Так, это понятно? Окей. Теперь давайте приведем контрпример, почему вот здесь вот, вот здесь везде
[01:36:37.200 --> 01:36:44.320]  были эквивалентности, а вот здесь вот стрелочки в обратную сторону нет. Контрпример здесь банальный.
[01:36:44.320 --> 01:37:02.720]  Значит, можно рассмотреть. Соответственно, мы сейчас доказываем, что нет стрелочки вправо.
[01:37:02.720 --> 01:37:13.920]  Можно рассмотреть последовательность xn, ttn. xn будет равно x, ttn будет равно theta. То есть,
[01:37:13.920 --> 01:37:17.800]  грубо говоря, это последовательность, в которой на каждой позиции стоит одна и
[01:37:17.800 --> 01:37:22.320]  та же случайная величина. Ну причем вот эти случайные величины x это независимые и имеют
[01:37:22.320 --> 01:37:34.320]  одинаковое распределение. Вот. То есть более-менее очевидно, что xn сходится по распределению x,
[01:37:34.320 --> 01:37:39.920]  просто потому что у вас каждая случайная величина в этой последовательности имеет распределение x.
[01:37:39.920 --> 01:37:46.000]  Но на самом деле очевидно и вот такое вот утверждение, что ttn тоже сходится по распределению
[01:37:46.000 --> 01:37:53.080]  x. Ну потому что у вас распределение x и это совпадают. Вот. Следовательно, вот предположим,
[01:37:53.080 --> 01:37:55.840]  что мы составили вектор из двух таких последовательных случайных величин,
[01:37:55.840 --> 01:38:02.520]  тогда по идее этот вектор сходится x, x. Но это неправда, потому что мы положили с вами независимые
[01:38:02.520 --> 01:38:07.200]  случайные величины, значит вот этот случайный вектор мог как-то вот рандомно на всю плоскость
[01:38:07.200 --> 01:38:17.320]  попадать. То есть куда угодно. А вот этот вектор у вас распределен на прямой y равно x. Ну и понятно,
[01:38:17.320 --> 01:38:31.200]  что вот эта сходимость уже неверна. Так. Это понятно? Да, мне кажется, я просто леница начал.
[01:38:31.200 --> 01:38:55.480]  Да, сейчас. Так, на секундочку. Так, какой вопрос? Что-то противоположное?
[01:38:55.480 --> 01:39:10.360]  Ну да, так как это непрерывная ограниченная функция. А почему она ограниченная?
[01:39:20.360 --> 01:39:24.480]  Ну вот не совсем очевидно. Ладно. В общем, давай тогда еще раз я вот пример повторю,
[01:39:24.480 --> 01:39:29.440]  потому что кто-то попросил повторить. Значит, смотрите. Мы зафиксировали с вами две случайные
[01:39:29.440 --> 01:39:34.880]  величины с одинаковым распределением, и они независимы изначально были. Вот просто рассмотрим
[01:39:34.880 --> 01:39:39.800]  последовательность, в которую вот все компоненты это просто одна и та же случайная величина x. Вторая
[01:39:39.800 --> 01:39:45.480]  последовательность, такая последовательность будет, у которой каждый компонент это. Вот. Ну вот эта
[01:39:45.480 --> 01:39:50.640]  сходимость более-менее очевидна, но на самом деле вот такая сходимость тоже будет верна, просто
[01:39:50.640 --> 01:39:56.160]  потому что у вас кси и это по распределению совпадают, тогда если бы мы составили из них вектор, у нас по идее должна быть
[01:39:56.720 --> 01:40:02.900]  выполнена вот такая сходимость, но понятно, что такой сходимости у нас нет, потому что вот эти случайные величины как бы независимы,
[01:40:03.880 --> 01:40:07.440]  ну и а вот эти уже зависимы, то есть вот эти две
[01:40:08.680 --> 01:40:10.680]  распределения вот этого вектора предельного
[01:40:10.840 --> 01:40:15.140]  находятся напрямую у равно х, а распределение каждой компоненты
[01:40:16.040 --> 01:40:18.560]  оно могло быть где угодно на плоскости.
[01:40:20.720 --> 01:40:26.200]  Вот, ладно, надеюсь, что понятно. Собственно, это мы с вами обсудили,
[01:40:33.560 --> 01:40:39.640]  это мы обсудили, то есть из сходимости компонент по распределению не следует сходимость векторов по распределению,
[01:40:41.320 --> 01:40:46.120]  но как всегда есть какое-то исключение и вот это исключение это Лемма Слуцкого.
[01:40:46.760 --> 01:40:48.760]  Давайте обсудим с вами Лему Слуцкого.
[01:41:01.880 --> 01:41:05.280]  На самом деле вот эту стрелочку можно в частном случае обернуть.
[01:41:07.080 --> 01:41:11.240]  В каком? Ну, если у вас одна последовательность сходится по распределению
[01:41:11.240 --> 01:41:14.560]  х, а вторая последовательность сходится по распределению константия,
[01:41:15.960 --> 01:41:21.640]  тогда у вас сходимость векторов составленный из этих компонент будет.
[01:41:25.320 --> 01:41:32.280]  То есть в общем случае это неверно, но если одна из компонент сходится константе, тогда вот такая сходимость векторная будет верна.
[01:41:34.720 --> 01:41:36.720]  Окей.
[01:41:36.720 --> 01:41:38.720]  Это замечательно.
[01:41:38.800 --> 01:41:44.200]  Этим мы будем пользоваться чуть позже. Так, Лему Слуцкого я сказал.
[01:41:45.720 --> 01:41:47.720]  Дальше, для векторов на самом деле
[01:41:49.640 --> 01:41:55.760]  взаимосвязь сходимости такая же, как и для случайных величин, то есть все та же диаграмма коверна и сходимости почти наверно векторов следует
[01:41:56.440 --> 01:42:02.680]  сходимость по вероятности векторов и следует сходимость по распределению векторов. Ну и там когда можно обращать и
[01:42:03.200 --> 01:42:05.200]  контрпримеры на самом деле примерно такие же.
[01:42:06.240 --> 01:42:08.240]  Вот.
[01:42:09.720 --> 01:42:15.040]  Можно еще, например, интересный интересное утверждение быстро доказать. Вот такое.
[01:42:17.120 --> 01:42:21.360]  Так, давайте напишу его вот на новой доске. Утверждение следующее.
[01:42:25.560 --> 01:42:31.480]  Помните, что у нас в одномерном случае сходимость константе по распределению была эквивалентна сходимости константе по вероятности.
[01:42:32.120 --> 01:42:34.120]  Ну вот в многомерном случае это тоже верно.
[01:42:35.720 --> 01:42:37.720]  Утверждение.
[01:42:38.480 --> 01:42:40.480]  Пусть у вас
[01:42:41.920 --> 01:42:45.720]  ну вот вектор ксен сходится к вектору константа
[01:42:49.680 --> 01:42:54.400]  по распределению, тогда у вас будет на самом деле верна и сходимость по вероятности.
[01:42:58.000 --> 01:43:00.000]  Как вот это доказывать? Есть идеи?
[01:43:07.720 --> 01:43:09.720]  Так.
[01:43:09.720 --> 01:43:17.640]  Ну вот у вас сходимость векторов. И сходимости векторов следует сходимость компонента. То есть мы знаем, что по компонентной сходимость будет.
[01:43:21.720 --> 01:43:23.720]  По распределению.
[01:43:27.000 --> 01:43:32.600]  Так, только здесь опять коллизия обозначений. Давайте мы к. Вот так назовем.
[01:43:32.800 --> 01:43:40.600]  Вот. Здесь у нас уже одномерные сходимости по распределению. Одномерные сходимости по распределению можно превратить в одномерные сходимости по вероятности.
[01:43:41.600 --> 01:43:49.600]  Вот. Здесь у нас уже одномерные сходимости по распределению. Одномерные сходимости по распределению можно превратить в одномерные сходимости по вероятности.
[01:43:49.600 --> 01:43:56.600]  Вот. А для сходимости по вероятности уже верно, что из сходимости компонент следует сходимость векторов.
[01:43:56.600 --> 01:44:01.600]  То есть из вот этих вот компонент можно собрать вектор. Вот такой.
[01:44:01.600 --> 01:44:05.600]  Ну в общем это будет по сути опять наш вектор.
[01:44:06.600 --> 01:44:12.600]  Ксен, который будет сходится к C1 ЦК по вероятности. Вот.
[01:44:12.600 --> 01:44:17.600]  То есть в одномерном случае мы это через лему Александрова доказывали. Теориям Александрова.
[01:44:17.600 --> 01:44:22.600]  А сейчас мы будем использовать Л.C1 ЦК.
[01:44:24.600 --> 01:44:26.600]  Вот.
[01:44:26.600 --> 01:44:34.600]  То есть в одномерном случае мы это через лему Александрова доказывали, теорему Александрова.
[01:44:34.600 --> 01:44:39.600]  А сейчас мы вот просто таким вот хитрым способом перешли к одномерному случаю,
[01:44:39.600 --> 01:44:44.600]  а потом обратно из компонент собрали вектор и доказали это утверждение.
[01:44:44.600 --> 01:44:47.600]  Супер. Это тоже может где-то пригодиться.
[01:44:47.600 --> 01:44:50.600]  Так, и теперь уже почти переходим к задаче.
[01:44:50.600 --> 01:44:57.600]  Обсудим еще очень важную теорему, которая называется теорема наследования исходимости.
[01:45:11.600 --> 01:45:15.600]  Ну, исходимости по вероятности всегда следуют исходимость по распределению.
[01:45:16.600 --> 01:45:22.600]  То есть я до этого сказал, что для векторов соотношение между исходимостьми точно такое же, как и для случайных величин.
[01:45:22.600 --> 01:45:25.600]  То есть исходимости по вероятности следует исходимость по распределению.
[01:45:25.600 --> 01:45:28.600]  Ну и как бы только содержаченную часть этого утверждения записал.
[01:45:28.600 --> 01:45:32.600]  Конечно, можно здесь в обе стороны стрелочки нарисовать.
[01:45:32.600 --> 01:45:35.600]  Так, супер. Идем дальше.
[01:45:35.600 --> 01:45:38.600]  Теорема наследования исходимости.
[01:45:39.600 --> 01:45:42.600]  Почти катарсис уже готов.
[01:45:42.600 --> 01:45:45.600]  Так, в чем ее суть?
[01:45:45.600 --> 01:45:50.600]  Вот, жили вы в одном пространстве, и была у вас исходимость в каком-то смысле.
[01:45:50.600 --> 01:45:54.600]  По распределению почти, наверное, по вероятности.
[01:45:54.600 --> 01:45:58.600]  И захотелось вам применить какую-нибудь непрерывную функцию.
[01:45:58.600 --> 01:46:03.600]  Ну вот, мы сейчас будем говорить, что вот она просто непрерывная, либо локально непрерывная.
[01:46:03.600 --> 01:46:05.600]  Что бы это ни значило.
[01:46:06.600 --> 01:46:08.600]  Что бы это ни значило.
[01:46:08.600 --> 01:46:11.600]  Так вот, теорема наследования исходимости утверждает, что вот в новом пространстве,
[01:46:11.600 --> 01:46:14.600]  после того, как вы подействуете на вектор или случайные величины,
[01:46:14.600 --> 01:46:18.600]  на последовательность случайных величин, у вас исходимость сохранится в том же смысле.
[01:46:26.600 --> 01:46:31.600]  Вот. То есть у вас вектора сходились, вы применили какую-то непрерывную функцию
[01:46:31.600 --> 01:46:36.600]  к тому, что слева стоит, к тому, что справа, и исходимость у вас сохранилась в том же смысле,
[01:46:36.600 --> 01:46:38.600]  но уже в новом пространстве, грубо говоря.
[01:46:42.600 --> 01:46:47.600]  Ну, там есть более тонкое условие, там связано с мерой единичка, короче,
[01:46:47.600 --> 01:46:50.600]  но мы сейчас это опустим, у нас будут непрерывные функции.
[01:46:50.600 --> 01:46:55.600]  Давайте для решения задач считать, что функция либо непрерывная, либо локально непрерывная.
[01:46:55.600 --> 01:46:58.600]  То есть в окрестности какой-то точки, которая нас интересует.
[01:46:59.600 --> 01:47:01.600]  Так, ну это более-менее понятно.
[01:47:01.600 --> 01:47:06.600]  И теперь давайте поймем с вами, почему чаще всего в задаче номер два на контрольной работе
[01:47:06.600 --> 01:47:08.600]  будет сходимость по распределению.
[01:47:08.600 --> 01:47:14.600]  Ну, потому что для сходимости более сильных, то есть для сходимости по вероятности
[01:47:14.600 --> 01:47:18.600]  или сходимости почти наверное, похожие задачки были бы очень простые.
[01:47:20.600 --> 01:47:22.600]  Ну, то есть, предположим, у вас задача.
[01:47:22.600 --> 01:47:24.600]  Давайте такая вот гипотетическая задача на контрольной.
[01:47:28.600 --> 01:47:33.600]  Вот вам дали последовательность Xn, которая сходится по вероятности Qi,
[01:47:33.600 --> 01:47:37.600]  вам дали последовательность Ytn, которая сходится по вероятности к Theta,
[01:47:37.600 --> 01:47:40.340]  и вам дали последовательность, ну, какой-нибудь счёт Nz square,
[01:47:40.600 --> 01:47:44.600]  которая сходится по вероятности X Ц.
[01:47:45.600 --> 01:47:50.600]  И вас спросят, а к чему сходится X q?
[01:47:50.600 --> 01:47:59.100]  вероятности z. И вас спросят, а к чему сходится ксен, там
[01:47:59.100 --> 01:48:12.200]  допустим плюс ттн, поделить на ztn. Да, здесь тн должен
[01:48:12.200 --> 01:48:14.840]  быть. И вот вас могут спросить, к чему вот это сходится
[01:48:14.840 --> 01:48:21.620]  по вероятности. Ну, задачи ни о чем. Как такие задачи
[01:48:21.620 --> 01:48:26.160]  решать? Шаг первый. Вы составляете вектор из компонент, потому
[01:48:26.160 --> 01:48:28.480]  что из компонентной сходимости следует векторная сходимость.
[01:48:28.480 --> 01:48:38.640]  В силу вот этого утверждения, которое на доске еще осталось,
[01:48:38.640 --> 01:48:46.440]  у вас есть сходимость векторов. Ну, замечательно. А дальше
[01:48:46.440 --> 01:48:51.280]  пользуемся теоремой наследования сходимости. Для функции, ну,
[01:48:51.280 --> 01:48:54.840]  от трех аргументов получается x плюс y поделить на z. И все,
[01:48:54.840 --> 01:49:00.440]  и получаем, что вот это все сходится кси плюс это поделить
[01:49:00.440 --> 01:49:07.260]  на z. Ну, задачи ни о чем. Вот. Плюс напишу теорема
[01:49:07.260 --> 01:49:13.140]  наследования сходимости. И, собственно, почему на
[01:49:13.140 --> 01:49:16.860]  контрольной появляются задачи на сходимость по распределению?
[01:49:16.860 --> 01:49:20.420]  Потому что для них вот первый шаг проблематичен. То есть
[01:49:20.420 --> 01:49:23.980]  нельзя просто взять и составить вектор, чтобы он сходился
[01:49:23.980 --> 01:49:25.920]  по распределению и потом применить к нему теорему
[01:49:25.920 --> 01:49:28.780]  наследования сходимости. Поэтому там нужно что-то изобретать.
[01:49:28.780 --> 01:49:44.300]  И это что-то, это дельта-метод. Ну вот, в частности, так вот,
[01:49:44.300 --> 01:49:47.500]  небольшое лирическое отступление. Если мы знаем с вами теорему
[01:49:47.500 --> 01:49:50.860]  о связи по компонентной сходимости и векторной сходимости, и
[01:49:50.860 --> 01:49:53.060]  знаем теорему наследования сходимости, мы знаем с
[01:49:53.060 --> 01:49:57.980]  вами, что, допустим, предел суммы — это сумма пределов.
[01:49:57.980 --> 01:50:00.700]  Ну и для произведений то же самое. То есть, в принципе,
[01:50:00.700 --> 01:50:03.300]  все какие-то базовые свойства очень легко уводятся с учетом
[01:50:03.300 --> 01:50:06.260]  вот этих двух фактов. Для всех сходимости, кроме сходимости
[01:50:06.260 --> 01:50:12.940]  по распределению. Ключевая проблема там в том, что мы
[01:50:12.940 --> 01:50:15.140]  не можем составить вектор из компонент, чтобы он сходился.
[01:50:15.140 --> 01:50:20.060]  Вот. Давайте я хорошо доску помою, а то что-то грязно
[01:50:21.060 --> 01:50:35.420]  осталось. Как у вас настроение? А чего так грустно? Ладно,
[01:50:35.420 --> 01:50:41.220]  некоторые вопросы должны остаться без ответа. Пока.
[01:50:41.220 --> 01:51:04.860]  Так, идем дальше. Дельте метод. Собственно, чтобы решать
[01:51:04.860 --> 01:51:08.580]  задачки, которые, скорее всего, у вас будут в листочке,
[01:51:08.580 --> 01:51:10.460]  нужно уметь пользоваться дельте методом. Давайте мы
[01:51:10.580 --> 01:51:14.300]  сформулируем и аккуратно докажем. Пусть у вас есть сходимость
[01:51:14.300 --> 01:51:19.500]  по распределению откуда-то и пусть у вас есть числовая
[01:51:19.500 --> 01:51:22.260]  последовательность, которая сходится к нулю. Пусть у
[01:51:22.260 --> 01:51:25.940]  вас также есть какая-то точка А. И пусть у вас есть
[01:51:25.940 --> 01:51:29.780]  H-непрерывная функция. Даже так, она будет дифференцируемая
[01:51:29.780 --> 01:51:58.100]  в точке А. Тогда верно следующее. Ну вот, такая сходимость
[01:51:58.100 --> 01:52:00.460]  верна. Давайте я ее аккуратно на одну доску постараюсь
[01:52:00.460 --> 01:52:18.820]  уместить. Кси на производную точку А. Какая дельта? А,
[01:52:18.820 --> 01:52:21.020]  ну дельта метода, потому что маленькие превращения.
[01:52:21.020 --> 01:52:22.460]  Потому что, по сути, смотрите, здесь же у вас практически
[01:52:22.460 --> 01:52:25.580]  производная записана. Как бы это, грубо говоря, вероятностная
[01:52:25.580 --> 01:52:29.020]  производная какая-то. Вот, поэтому это дельта метод.
[01:52:29.020 --> 01:52:33.940]  Дельта, потому что маленькие превращения. Да, сходимость
[01:52:33.940 --> 01:52:36.580]  по распределению будет. Вот как раз-таки это инструмент,
[01:52:36.580 --> 01:52:39.780]  при помощи которого мы будем решать задачи. Вот, обсудим
[01:52:39.780 --> 01:52:42.060]  как решать дальше. Сначала давайте докажем и поймем,
[01:52:42.060 --> 01:52:45.020]  что вот эта штука действительно работает, а потом попробуем
[01:52:45.020 --> 01:52:48.660]  применить ее в задачке. Вот, ну доказательство очень
[01:52:48.660 --> 01:52:57.660]  простое. Первое, мы можем функцию, в силу того, что
[01:52:57.660 --> 01:53:00.700]  она дифференцируема в какой-то окрестности, представить
[01:53:00.700 --> 01:53:09.700]  в следующем виде по формуле Тейлора. Вот в таком виде,
[01:53:09.700 --> 01:53:14.660]  например. Ну там, равенство писать не совсем корректно,
[01:53:15.500 --> 01:53:21.140]  но какой-то эквивалентный писать. Вот. Ну и дальше
[01:53:21.140 --> 01:53:24.220]  можем вести вспомогательную функцию. Давайте это я так
[01:53:24.220 --> 01:53:27.220]  как-нибудь на пальцах быстро объясню. Значит, если х...
[01:53:27.220 --> 01:53:44.220]  Если х не равен нулю, то вот такое выражение, а если
[01:53:44.340 --> 01:53:47.460]  х равен нулю, то вот, определим по непрерывности производной.
[01:53:47.460 --> 01:53:50.900]  Ну вот, функция будет локально непрерывна в окрестности
[01:53:50.900 --> 01:53:53.860]  точки А. Соответственно, чтобы получить вот это вот
[01:53:53.860 --> 01:53:58.860]  утверждение, что нужно заметить? Ну первое. Так, это у нас
[01:53:58.860 --> 01:54:02.780]  наверное был первый шаг. Вот это второй шаг. Давайте
[01:54:02.780 --> 01:54:09.780]  третий шаг. Ксиенбен сходится к нулю. Почему? По распределению.
[01:54:10.180 --> 01:54:15.460]  Почему она сходится к нулю? Ну потому что у нас мы
[01:54:15.460 --> 01:54:18.300]  пользуемся по сути леммой Судского. У вас bn сходится
[01:54:18.300 --> 01:54:22.300]  к константе, ксин сходится кси по распределению. Что-то
[01:54:22.300 --> 01:54:24.420]  одно сходится к константе, а второе сходится по распределению.
[01:54:24.420 --> 01:54:28.340]  Значит, мы можем перемножать, допустим. И все аккуратно
[01:54:28.340 --> 01:54:31.060]  получается. А дальше мы просто применяем непрерывную
[01:54:31.060 --> 01:54:32.540]  функцию, то есть пользуемся теоремой наследования
[01:54:32.540 --> 01:54:36.740]  сходимости. И у нас получается что? Ну, пользуемся определением
[01:54:36.780 --> 01:54:43.780]  h от x, то есть у нас h от ксиенбен сходится к h от нуля. Вот. А что, например, из этого
[01:54:50.580 --> 01:54:54.060]  следует? Если мы распишем вот это вот определение,
[01:54:54.060 --> 01:55:01.060]  которое у нас есть, плюс ксиенбен, плюс h от a поделить на ксиенбен,
[01:55:02.060 --> 01:55:08.060]  сходится к h от нуля, h от нуля у нас определена как h3t. Окей? Ну, практически то, что нам
[01:55:17.220 --> 01:55:18.900]  нужно, с учетом того, что у нас только правда ксиен
[01:55:18.900 --> 01:55:21.140]  здесь знаменателя, но смотрите, здесь у нас есть сходимость
[01:55:21.140 --> 01:55:24.140]  константе по распределению, значит, мы можем домножить
[01:55:24.140 --> 01:55:28.140]  на ксиен, сходящийся кси. Соответственно, кси у вас
[01:55:28.140 --> 01:55:31.060]  здесь уйдет, а здесь у вас появится кси в пределе.
[01:55:31.060 --> 01:55:33.460]  То есть у вас пользовались дважды леммой Слуцкого и
[01:55:33.460 --> 01:55:40.460]  те реммой о наследовании сходимости. Окей? Справедливо.
[01:55:41.780 --> 01:55:47.620]  Доказали. Еще вот такое небольшое замечание,
[01:55:47.620 --> 01:55:50.060]  смотрите, иногда в задачах встречается дельта метод
[01:55:50.060 --> 01:55:53.060]  второго порядка, то есть что делать, если у вас вдруг
[01:55:53.060 --> 01:55:55.740]  у функций, которые вы собираетесь применять, вот эта производная
[01:55:55.740 --> 01:55:58.740]  нулевая? Ну, на самом деле, нужно просто ряд Тейлора
[01:55:58.740 --> 01:56:02.420]  разложить до второго порядка, вот это слагаемого за нулица
[01:56:02.420 --> 01:56:06.420]  и, по сути, записать дельта метод для второго порядка.
[01:56:06.420 --> 01:56:12.420]  Окей? Ну, то есть смотрите, если вот функция, которую
[01:56:12.420 --> 01:56:15.460]  вы применяете имеет первую производную нулевую в точке,
[01:56:15.460 --> 01:56:18.100]  такое возможно, то у вас будет сходимость к нулю,
[01:56:18.100 --> 01:56:19.940]  а это может быть неправда, потому что вы, может быть,
[01:56:19.940 --> 01:56:23.580]  не достаточно порядок малости рассмотрели, может быть,
[01:56:23.580 --> 01:56:25.820]  нужно было до второй производной раскладывать формул Тейлора.
[01:56:25.820 --> 01:56:29.820]  Тогда вы просто раскладываете до второй производной,
[01:56:29.820 --> 01:56:35.820]  вот, там у вас получается будет плюс х квадрат пополам
[01:56:35.820 --> 01:56:42.820]  аш-два штрихата, вот. Ну и тогда у вас, по сути, вот
[01:56:42.820 --> 01:56:48.820]  это если за нулица, вот это можно перенести влево,
[01:56:48.820 --> 01:56:51.900]  поделить на х квадрати, и, в принципе, у вас получится
[01:56:52.140 --> 01:56:57.140]  что-то похожее. Давайте я запишу тогда аккуратно,
[01:56:57.140 --> 01:56:59.900]  потому что это может вам пригодиться. В общем, если
[01:56:59.900 --> 01:57:02.780]  мы проделаем все абсолютно так же, то мы получим с
[01:57:02.780 --> 01:57:04.940]  вами дельт метод второго порядка. Давайте тогда мы
[01:57:04.940 --> 01:57:08.620]  сейчас это доказательство переделаем теперь для второго
[01:57:08.620 --> 01:57:15.620]  порядка. То есть, предположим, что у нас первый производный
[01:57:21.980 --> 01:57:24.980]  за нулилась, но нам не повезло, и функция дважды дифференцируема.
[01:57:24.980 --> 01:57:27.820]  Тогда у нас, по сути, теперь вот эта слагаемая здесь
[01:57:27.820 --> 01:57:32.340]  доминирующая, мы аш от х теперь определяем похожим
[01:57:32.340 --> 01:57:36.860]  образом, только делим на х квадрате, а здесь определяем
[01:57:36.860 --> 01:57:43.860]  как аш-два штриха пополам. Да? Так же опять пользуемся
[01:57:44.660 --> 01:57:47.260]  леммой Слуцкого, применяем вот к этой функции, получаем
[01:57:47.260 --> 01:57:51.940]  вот такую сходимость, и в итоге записываем определение,
[01:57:51.940 --> 01:57:58.700]  здесь у нас будет кси-н в квадрате, б-н в квадрате.
[01:57:58.700 --> 01:58:03.980]  Окей. И все это сходится теперь ко второй производной
[01:58:03.980 --> 01:58:10.500]  пополам. Вот. Ну и, по сути, осталось только
[01:58:10.500 --> 01:58:15.340]  домножить на кси-н в квадрате, которая сходится кси в квадрате.
[01:58:15.460 --> 01:58:17.860]  Вот эта сходимость верна, потому что у нас кси-н сходится
[01:58:17.860 --> 01:58:21.500]  кси по распределению, применяем функцию квадрата, вот такая
[01:58:21.500 --> 01:58:24.140]  сходимость будет выполнена, домножим обе части, ну левую
[01:58:24.140 --> 01:58:26.660]  часть на кси-н в квадрате, правую часть на кси в квадрате,
[01:58:26.660 --> 01:58:33.660]  и у вас получится вот такое утверждение. То есть давайте
[01:58:33.660 --> 01:58:37.700]  я выпишу его. То есть вот это дельта-метод, а дельта-метод
[01:58:37.700 --> 01:58:38.460]  второго порядка.
[01:58:45.460 --> 01:59:00.460]  Дельта-метод второго порядка записывается следующим
[01:59:00.460 --> 01:59:04.340]  образом. Значит, пусть у нас есть сходимость по распределению,
[01:59:04.340 --> 01:59:08.340]  пусть у нас есть последовательность b-n, которая сходится к нулю,
[01:59:08.340 --> 01:59:11.980]  пусть у нас есть точка a, и h дважды дифференцируемо
[01:59:11.980 --> 01:59:24.980]  в окрестности точка a. Ну, точка. И что мы еще потребуем?
[01:59:24.980 --> 01:59:27.380]  А, ну мы потребуем, чтобы первая производная была равна
[01:59:27.380 --> 01:59:38.380]  нулю в точке a. Вот. Соответственно тогда, тогда верна сходится
[01:59:41.980 --> 01:59:48.980]  на следующую сходимость. h от a плюс кси-н b-н, минус h от
[01:59:48.980 --> 01:59:55.980]  a, поделить на b-n в квадрате, сходится кси в квадрате
[01:59:55.980 --> 02:00:03.980]  на h дважды триха в точке a пополам. Да, ну вот ровно то же самое,
[02:00:03.980 --> 02:00:07.980]  что и было раньше. Вот. Это дельта-метод второго порядка,
[02:00:07.980 --> 02:00:10.980]  иногда может встретиться. Сейчас поймем, как определить,
[02:00:10.980 --> 02:00:13.980]  что скорее всего дельта-метод второго порядка. Вот. Но обычно
[02:00:13.980 --> 02:00:15.980]  дельта-метод первого порядка вполне достаточно для решения
[02:00:15.980 --> 02:00:18.980]  задач. Вот. Ну и собственно мы готовы решить задачу.
[02:00:18.980 --> 02:00:26.980]  Ну, да. Надо только понять, какую функцию применять.
[02:00:26.980 --> 02:00:32.980]  Сейчас поймем. Ну, нужно просто понять, какая функция,
[02:00:32.980 --> 02:00:36.980]  к чему применяем, и поймем, что если в этой точке ноль,
[02:00:36.980 --> 02:00:38.980]  тогда да. Тогда действительно дельта-метод второго порядка.
[02:00:38.980 --> 02:00:55.980]  Итак, это задача номер два. Задача номер два. Смотрите.
[02:00:55.980 --> 02:00:58.980]  Перед задачей два хочется сказать, вот, а откуда вообще
[02:00:58.980 --> 02:01:01.980]  брать вот эти сходимости, которые нам нужны для решения задач?
[02:01:01.980 --> 02:01:04.980]  Ну вот, основными источниками сходимости у нас на самом деле
[02:01:04.980 --> 02:01:12.980]  является ЗБЧ. Ну, ЗБЧ давайте так. СН на Н сходится к
[02:01:12.980 --> 02:01:18.980]  мотожиданию КСИ. Вот. Почти, наверное. Если мотожидание
[02:01:18.980 --> 02:01:22.980]  КСИ конечное. И второй источник сходимости у нас, вот,
[02:01:22.980 --> 02:01:24.980]  сходимости по распределению, это вообще на самом деле
[02:01:24.980 --> 02:01:28.980]  единственный источник, это ЦПТ. Ну, можем записать
[02:01:28.980 --> 02:01:42.980]  это в следующей форме. Вот. Ну, давайте так, для краткости.
[02:01:42.980 --> 02:01:52.980]  СН это сумма ксиитых, по и от 1 до n. А х средняя это СН
[02:01:52.980 --> 02:01:56.980]  поделить на n. Ну, то есть какие-то базовые предельные
[02:01:56.980 --> 02:02:01.980]  теоремы, которые у нас в курсе есть и которые по сути являются
[02:02:01.980 --> 02:02:05.980]  источниками сходимости каких-то. Тогда на самом деле все задачи
[02:02:05.980 --> 02:02:09.980]  на слабую сходимость, это вот просто взять из источника
[02:02:09.980 --> 02:02:12.980]  слабой сходимости, то есть из ЦПТ, сходимость и применить
[02:02:12.980 --> 02:02:17.980]  к ней дельта метод. Вот. Ну, возможно, если будет что-то
[02:02:17.980 --> 02:02:19.980]  сложнее, то можно там Лему Слуцкого применить, что-то
[02:02:19.980 --> 02:02:22.980]  еще там, короче, как-нибудь это повертеть. Но в общем
[02:02:22.980 --> 02:02:25.980]  случае у вас есть ЦПТ, который дает вам сходимость
[02:02:25.980 --> 02:02:29.980]  по распределению. И если у вас есть сходимость по распределению,
[02:02:29.980 --> 02:02:31.980]  дальше вы пользуетесь как раз таки уже дельтам методам,
[02:02:31.980 --> 02:02:33.980]  которые мы с вами обсудили. Вот. Ну и, соответственно,
[02:02:33.980 --> 02:02:43.980]  задача два. Я уже написал. Так, условия второй задачи.
[02:02:43.980 --> 02:02:58.980]  Пусть у нас случайная величина ХН. Так, независимые, одинаково
[02:02:58.980 --> 02:03:02.980]  распределенные, имеют экспоненциальное распределение
[02:03:02.980 --> 02:03:12.980]  с параметром тета. Вот, соответственно, X среднее
[02:03:12.980 --> 02:03:30.980]  это SN на N. Необходимо найти. К чему сходится вот такая
[02:03:30.980 --> 02:03:38.980]  штука. Собственно, в этом заключается задача. Вот.
[02:03:38.980 --> 02:03:41.980]  Ну, скорее всего, короче, похожие задачи будут задачи
[02:03:41.980 --> 02:03:45.980]  на дельтаметод. И, в принципе, они все решаются очень
[02:03:45.980 --> 02:03:49.980]  однотипно. Вы полагаете, вот, чтобы воспользоваться
[02:03:49.980 --> 02:03:52.980]  дельтаметодом первого порядка, здесь будет дельтамет
[02:03:52.980 --> 02:03:55.980]  первого порядка, нам нужно определить вот все, что в
[02:03:55.980 --> 02:03:58.980]  условии есть. То есть какую-то последовательность, чаще
[02:03:58.980 --> 02:04:02.980]  всего последовательность, это 1 на корень из N, сходящуюся
[02:04:02.980 --> 02:04:10.980]  к нулю. Определить точку. Ну, вот эта точка, это, сейчас
[02:04:10.980 --> 02:04:15.980]  поймем какая, это 1 на тета. Немножко в другом порядке
[02:04:15.980 --> 02:04:18.980]  определяю, сейчас будет понятно, почему это 1 на тета.
[02:04:18.980 --> 02:04:20.980]  Нам нужна сходимость по распределению какая-то. Ну,
[02:04:20.980 --> 02:04:22.980]  сходимость по распределению мы с вами умеем брать только
[02:04:22.980 --> 02:04:28.980]  из CPT, по сути. Из CPT у нас следует, что корень из N,
[02:04:28.980 --> 02:04:33.980]  х средняя, минус математическое ожидание экспоненциальной
[02:04:33.980 --> 02:04:39.980]  случайной величины, это 1 на тета, сходится к нормальному
[02:04:39.980 --> 02:04:43.980]  закону с параметрами 0, 1 на тетов квадрате.
[02:04:48.980 --> 02:04:53.980]  Сейчас, сейчас скажу. Так, это вот так, так. Значит,
[02:04:53.980 --> 02:04:55.980]  смотрите, мы определили последовательность, в таких задачах
[02:04:55.980 --> 02:04:59.980]  часть всего 1 на корень из N. Определили, положили
[02:04:59.980 --> 02:05:01.980]  какую-то сходимость по распределению. Вот мы ее
[02:05:01.980 --> 02:05:04.980]  взяли из CPT для тех независимо, одинаково распределенных
[02:05:04.980 --> 02:05:06.980]  случайных величин, которые нам даны в условиях. Вот
[02:05:06.980 --> 02:05:12.980]  она. А как подобрать точку? Ну, точка, вот это обычно,
[02:05:12.980 --> 02:05:15.980]  то, что в CPT справа. Ну, мы сейчас поймем, почему. То
[02:05:15.980 --> 02:05:18.980]  есть, у этого есть, конечно, глубинный смысл, но вот давайте
[02:05:18.980 --> 02:05:20.980]  пока считать, что это вот то, что вот тут справа в CPT
[02:05:20.980 --> 02:05:24.980]  написано. И нам чего-то еще не хватает, нам не хватает
[02:05:24.980 --> 02:05:28.980]  функции. Ну, вот функция обычно это то вот, то преобразование,
[02:05:28.980 --> 02:05:31.980]  которое вот то, что с левой или с правой переводит в то,
[02:05:31.980 --> 02:05:33.980]  что с левой или с правой здесь, соответственно. То
[02:05:33.980 --> 02:05:36.980]  есть, функция в нашем случае, это 1 поделить, давайте
[02:05:36.980 --> 02:05:39.980]  какую-нибудь букву новую придумаем, M, например. 1
[02:05:39.980 --> 02:05:45.980]  поделить на M в квадрате. Вот, то есть, сейчас, возможно,
[02:05:45.980 --> 02:05:47.980]  это как магия выглядит, что я все подобрал типа на
[02:05:47.980 --> 02:05:51.980]  угад, но на самом деле это более-менее понятный извод
[02:05:51.980 --> 02:05:54.980]  того, что от нас требуется. Ну, и давайте просто применим
[02:05:54.980 --> 02:05:59.980]  дельта-метод. Мы с вами дельта-метод уже доказали.
[02:06:11.980 --> 02:06:15.980]  Так, давайте еще раз выпишу дельта-метод.
[02:06:16.980 --> 02:06:20.980]  Вот, без условий.
[02:06:34.980 --> 02:06:38.980]  Вот. Ну, и давайте просто применять те бенты, ашки
[02:06:38.980 --> 02:06:41.980]  и последовательности, которые мы с вами определили.
[02:06:41.980 --> 02:06:44.980]  Ну, что мы с вами имеем? Во-первых, давайте вот начнем
[02:06:44.980 --> 02:06:47.980]  с самой глубокой уровней вложенности. Ксиен умножим
[02:06:47.980 --> 02:06:50.980]  на bn. То есть, ксиен это у нас вот эта сходимость
[02:06:50.980 --> 02:06:53.980]  по распределению. Если мы ее умножим на bn, то у нас
[02:06:53.980 --> 02:06:59.980]  вот этот корень из n сократится. Дальше. Мы добавляем точку
[02:06:59.980 --> 02:07:02.980]  a. Ну, собственно, поэтому мы в качестве точки a положили
[02:07:02.980 --> 02:07:04.980]  вот это, потому что когда мы домножим на bn, у нас корень
[02:07:04.980 --> 02:07:08.980]  из n ушел, остается и к средней минус 1 на θ, и как раз мы
[02:07:08.980 --> 02:07:13.980]  добавляем 1 на θ вот здесь, и у вас корень из n ушел,
[02:07:13.980 --> 02:07:16.980]  корень из n ушел за счет bn, вот это ушло за счет того,
[02:07:16.980 --> 02:07:20.980]  что мы точку a добавили с вами. Осталось только х средняя.
[02:07:20.980 --> 02:07:25.980]  И к х среднему, вот здесь, мы применяем функцию 1 поделить
[02:07:25.980 --> 02:07:28.980]  на m в квадрате. То есть, по сути, переворачиваем
[02:07:28.980 --> 02:07:35.980]  эту штуку. И вот эта слагаемая все превращается в 1 поделить
[02:07:35.980 --> 02:07:46.980]  на х средняя в квадрате. Затем. Минус h от a. h в точке a это
[02:07:46.980 --> 02:07:52.980]  просто получается у нас будет 5 в квадрате. Затем мы
[02:07:52.980 --> 02:07:55.980]  это делим на bn. Последность bn у нас 1 на корень из n, ну
[02:07:55.980 --> 02:07:59.980]  значит соответственно домножаем на корень из n. Ура! В левой
[02:07:59.980 --> 02:08:02.980]  части мы получили ровно то, что от нас требовалось
[02:08:02.980 --> 02:08:05.980]  по условию задачи. Вот. Соответственно, то, что будет
[02:08:05.980 --> 02:08:10.980]  справа, нам дает дельтаметод. Из дельтаметода следует,
[02:08:10.980 --> 02:08:16.980]  что справа у нас здесь будет исходная предельная
[02:08:16.980 --> 02:08:18.980]  случайная величина. Исходная предельная случайная
[02:08:18.980 --> 02:08:20.980]  величина, это у нас, вот это нормальная случайная
[02:08:20.980 --> 02:08:30.980]  величина. Домноженная на производную в точке a.
[02:08:30.980 --> 02:08:37.980]  Производная 1 на m в квадрате это у нас минус 2 на m в
[02:08:37.980 --> 02:08:44.980]  кубе, видимо. Так, минус 2. Ну да. Вот такое. И
[02:08:44.980 --> 02:08:47.980]  подставляем вместо m 1 на тета. Соответственно, здесь
[02:08:47.980 --> 02:08:51.980]  у нас будет тета в третьей в числителе. То есть вот
[02:08:51.980 --> 02:08:56.980]  это все мы домножаем на 2 тета в третьей. Вот. Ну и
[02:08:56.980 --> 02:08:58.980]  хочется понять, какой предел будет. Мы можем вот эту
[02:08:58.980 --> 02:09:01.980]  константу занести, ну, домножить вот эту нормальную
[02:09:01.980 --> 02:09:04.980]  случайную величину на константу. И тогда мы с вами
[02:09:04.980 --> 02:09:08.980]  получим нормальное распределение с параметрами 0, а
[02:09:08.980 --> 02:09:20.980]  4 тета в четвертый. Справедливо? Вот. Сейчас. Только у меня,
[02:09:20.980 --> 02:09:22.980]  по-моему, другой коэффициент получился. Сейчас я быстро
[02:09:22.980 --> 02:09:27.980]  посмотрю свое решение. Минус никак не влияет, потому
[02:09:27.980 --> 02:09:30.980]  что у вас вот нормальная случайная величина, она
[02:09:30.980 --> 02:09:33.980]  симметрична относительно нуля. Поэтому домножай ее
[02:09:33.980 --> 02:09:35.980]  на минус, не домножай, она не изменится.
[02:09:43.980 --> 02:09:46.980]  Еще раз. Смотрите, если у вас кси имеет нормальное
[02:09:46.980 --> 02:09:51.980]  распределение с параметрами a sigma квадрат, то вот альфа
[02:09:51.980 --> 02:09:55.980]  кси плюс бета у вас имеет нормальное распределение
[02:09:55.980 --> 02:10:03.980]  с параметрами альфа, а плюс бета, а дисперсия просто
[02:10:03.980 --> 02:10:09.980]  умножается на альфа в квадрате. Ну, то есть, если ты
[02:10:09.980 --> 02:10:11.980]  сдвигаешь случайную величину, у нее среднее меняется.
[02:10:11.980 --> 02:10:13.980]  Если ты ее домножаешь, то у нее домножается среднее,
[02:10:13.980 --> 02:10:20.980]  а дисперсия на квадрат. Вот. Это можно из характеристических
[02:10:20.980 --> 02:10:22.980]  функций, например, показать.
[02:10:29.980 --> 02:10:32.980]  Да. Ну, то есть, если у вас среднее находилось
[02:10:32.980 --> 02:10:35.980]  в точке 5, если вы домножите такую случайную величину
[02:10:35.980 --> 02:10:38.980]  на 10, у вас теперь среднее будет в точке 50 находиться.
[02:10:38.980 --> 02:10:45.980]  Ну, более-менее это логично. А, так. Смотрите, не совсем
[02:10:45.980 --> 02:10:50.980]  вот это не очевидно, да? Можно просто вспомнить линейное
[02:10:50.980 --> 02:10:54.980]  преобразование характеристической функции. Ну, то есть, характеристическая
[02:10:54.980 --> 02:10:59.980]  функция линейного преобразования. Так, почему у нас ответ
[02:10:59.980 --> 02:11:04.980]  такой получился? Да вопрос? Или мы на него ответили?
[02:11:04.980 --> 02:11:14.980]  Да, сейчас я посмотрю. А, нет, 4 тета в четвертую
[02:11:14.980 --> 02:11:16.980]  у меня тоже получилось. Я, правда, это в 6 утра решал.
[02:11:16.980 --> 02:11:28.980]  Вроде тоже не ошибся. Да. Вот это как получилось?
[02:11:28.980 --> 02:11:35.980]  Да. Да, да, все верно. Давайте аккуратно еще раз.
[02:11:35.980 --> 02:11:39.980]  Смотрим. Ну, вот, во-первых, ксен сходится к си. Вот у нас
[02:11:39.980 --> 02:11:48.980]  сходимость из CPT. Вот эта сходимость. Далее. Мы домножаем
[02:11:48.980 --> 02:11:55.980]  ее на bn. По сути, у нас теперь корень из n отсюда уходит.
[02:11:55.980 --> 02:11:58.980]  Ну, ксен bn это просто будет вот это без корня из n.
[02:11:58.980 --> 02:12:02.980]  Будет вот эта штучка. Мы добавили a, точку a мы положили
[02:12:02.980 --> 02:12:06.980]  1 на тета. Соответственно, вот это убралось. Осталось
[02:12:06.980 --> 02:12:09.980]  только x среднее. И теперь мы к x среднему вот здесь
[02:12:09.980 --> 02:12:13.980]  внутри применили функцию h. Функцию h у нас это 1 на
[02:12:13.980 --> 02:12:28.980]  m в квадрате. Справедливо? Да. Ну, смотри, вот это,
[02:12:28.980 --> 02:12:31.980]  это ксен, которая сходится по распределению кси. То
[02:12:31.980 --> 02:12:38.980]  есть вот это это кси, вот это это ксен. Да. Что мы делаем
[02:12:38.980 --> 02:12:44.980]  с ксен? Мы домножаем на bn. Bn мы как раз положили 1
[02:12:44.980 --> 02:12:46.980]  на корень из n. Корень из n сократилась. Добавили
[02:12:46.980 --> 02:12:50.980]  константу, вот это ушло. Применили h, получилось. Все,
[02:12:50.980 --> 02:12:56.980]  задачку мы решили. Если бы у нас был не корень из
[02:12:56.980 --> 02:13:02.980]  n от m, то мы тогда брали bn как 1 на n. Мы брали bn как
[02:13:02.980 --> 02:13:06.980]  1 на корень из n, потому что у нас множество другого
[02:13:06.980 --> 02:13:11.980]  корня из n. Да. В общем, вот здесь, сейчас давайте
[02:13:11.980 --> 02:13:14.980]  тогда небольшое. Так, есть ли вопросы по решению вот
[02:13:14.980 --> 02:13:16.980]  этой задачи? То есть она вот правда в две строчки
[02:13:16.980 --> 02:13:21.980]  решилась. Мы просто определили с вами нужные все компоненты
[02:13:21.980 --> 02:13:23.980]  для того, чтобы применить дельта метод, и применили
[02:13:23.980 --> 02:13:26.980]  дельта метод. Больше мы ничего с веркости основания
[02:13:26.980 --> 02:13:35.980]  сделали. Нет. Ответ нет. Нужно ли брать в качестве
[02:13:35.980 --> 02:13:38.980]  bn что-то другое? Чаще всего в таких задачах всегда берется
[02:13:38.980 --> 02:13:41.980]  1 на корень из n. Вот у нас в домашке была задача, сейчас
[02:13:41.980 --> 02:13:58.980]  покажу какая. У нас в домашке была задача, ну там в листочке.
[02:13:58.980 --> 02:14:02.980]  В листочке у нас была задачка такого вида. n косинус там
[02:14:02.980 --> 02:14:10.980]  от чего-то там, минус единица, сходится по распределению
[02:14:10.980 --> 02:14:15.980]  к чему-то там. А ну вот понять к чему. Вот такая задачка
[02:14:15.980 --> 02:14:20.980]  у нас была дома. В чем здесь фишка? Здесь фишка в том,
[02:14:20.980 --> 02:14:24.980]  что это просто дельта метод второго порядка. Почему?
[02:14:24.980 --> 02:14:26.980]  Ну потому что на самом деле косинус у вас там точка
[02:14:26.980 --> 02:14:29.980]  ноль рассматривается, и у вас первая производная
[02:14:29.980 --> 02:14:33.980]  косинуса в нуле будет нолик. Ну то есть это будет sin x,
[02:14:33.980 --> 02:14:36.980]  в точке ноль это будет ноль. Производная косинус
[02:14:36.980 --> 02:14:41.980]  минус sin в нуле ноль. Вот. И вы помните вот как раз дельта
[02:14:41.980 --> 02:14:44.980]  метод второго порядка, у нас вот здесь в знаменателе
[02:14:44.980 --> 02:14:47.980]  bn в квадрате выскакивает. И соответственно вот этот
[02:14:47.980 --> 02:14:49.980]  множитель n просто появился от того, что мы в качестве
[02:14:49.980 --> 02:14:52.980]  bn все также взяли 1 поделить на корень из n, но мы просто
[02:14:52.980 --> 02:14:54.980]  видите вот здесь вот делим теперь на bn в квадрате.
[02:14:54.980 --> 02:14:56.980]  Поэтому у вас вот здесь вот n в квадрате появляется,
[02:14:56.980 --> 02:15:00.980]  ну просто n появляется. То есть в принципе еще раз,
[02:15:00.980 --> 02:15:03.980]  план решения таких задач всегда такой. bn всегда 1
[02:15:03.980 --> 02:15:08.980]  поделить на корень из n. В точка a берется просто вот
[02:15:08.980 --> 02:15:12.980]  правая часть из CPT. Ну по сути точка a это математическое
[02:15:12.980 --> 02:15:15.980]  задание вот тех случайных личин, для которых CPT для
[02:15:15.980 --> 02:15:19.980]  которых вы рассматриваете. А xn сходится по распределению
[02:15:19.980 --> 02:15:26.980]  x. Вот это просто берется из CPT непосредственно. Вот.
[02:15:26.980 --> 02:15:29.980]  А в качестве функции берется то, что ну вот, вот здесь
[02:15:29.980 --> 02:15:34.980]  бы cos мы взяли. Ну вот. Это очевидно из того, что у вас
[02:15:34.980 --> 02:15:44.980]  написано в левой части. Вот. Ну и дальше вы просто
[02:15:44.980 --> 02:15:46.980]  смотрите, какая функция скорее всего нужно применить.
[02:15:46.980 --> 02:15:49.980]  Если у этой функции вот в этой точке вдруг окажется
[02:15:49.980 --> 02:15:51.980]  первая производная нулевая, то вы применяете дельтамет
[02:15:51.980 --> 02:15:55.980]  с второго порядка. А если нет, то вам первого будет
[02:15:55.980 --> 02:15:58.980]  достаточно. И все. Вот эта задача просто была на дельтамет
[02:15:58.980 --> 02:16:03.980]  с второго порядка. Так. Остались ли вопросы какие-то
[02:16:03.980 --> 02:16:09.980]  по сходимостим? То есть еще раз, основная идея в том,
[02:16:09.980 --> 02:16:12.980]  что у нас есть сходимость компонентная, из CPT или
[02:16:12.980 --> 02:16:17.980]  из ABG, вот откуда-то мы можем их брать. Затем, если мы
[02:16:17.980 --> 02:16:20.980]  работаем со сходимостьми сильными, например, по вероятности
[02:16:20.980 --> 02:16:22.980]  или почти наверное, то мы можем стакать из таких
[02:16:22.980 --> 02:16:25.980]  по компонентной сходимости сходимости векторов, а потом
[02:16:25.980 --> 02:16:27.980]  к ним применять непрерывные функции и получать нужные
[02:16:27.980 --> 02:16:30.980]  нам сходимости. С сходимостью по распределению такое
[02:16:30.980 --> 02:16:32.980]  не работает. С сходимостью по распределению нужно
[02:16:32.980 --> 02:16:35.980]  хитрить, и для этого вот есть дельтаметод. А как
[02:16:35.980 --> 02:16:40.980]  пользоваться дельтаметодом? Ну вот. Нужно подобрать
[02:16:40.980 --> 02:16:45.980]  правильно bn, точку a, xn и h. И все правильно в правильной
[02:16:45.980 --> 02:16:47.980]  последовательности применить. И по идее тогда такие задачки
[02:16:47.980 --> 02:16:50.980]  решаются. Возможно, потом это нужно еще подтюнить,
[02:16:50.980 --> 02:16:52.980]  воспользовавшись еще раз леммой Слуцкого, ну например,
[02:16:52.980 --> 02:16:54.980]  на что-нибудь домножить. То есть сейчас у вас будет
[02:16:54.980 --> 02:16:56.980]  сходиться в случайной величине, можно это домножить
[02:16:56.980 --> 02:16:58.980]  на какую-нибудь константу или на какую-нибудь случайную
[02:16:58.980 --> 02:17:01.980]  величину, которая сходится к константе. Ну, на последовательные
[02:17:01.980 --> 02:17:02.260]  случайные величины, которые сходятся
[02:17:02.260 --> 02:17:04.980]  к констanте. То есть возможно еще после
[02:17:04.980 --> 02:17:06.980]  применения дельтаметодом нужно будет это чуть
[02:17:06.980 --> 02:17:11.980]  доделать при помощи ну вот, леммы Слуцкого, например.
[02:17:11.980 --> 02:17:16.980]  Но в целом, идея решения такая вот. Берем дельтаметод,
[02:17:16.980 --> 02:17:23.980]  Берем дальнейший метод, применяем и готово.
[02:17:23.980 --> 02:17:26.980]  База.
[02:17:26.980 --> 02:17:27.980]  Так, ну ладно, идем дальше.
[02:17:27.980 --> 02:17:35.980]  А сколько времени?
[02:17:35.980 --> 02:17:37.980]  Не-не-не.
[02:17:37.980 --> 02:17:41.980]  Без перерывов сегодня.
[02:17:41.980 --> 02:17:44.980]  Ну, сейчас будет интересная тема, по встрече пойдет.
[02:17:44.980 --> 02:17:49.980]  А потом будет очень интересная тема.
[02:17:49.980 --> 02:17:50.980]  А?
[02:17:50.980 --> 02:17:59.980]  А потом, я надеюсь, мы уже спать ляжем.
[02:17:59.980 --> 02:18:09.980]  Условные математические ожидания.
[02:18:09.980 --> 02:18:10.980]  Доживем.
[02:18:10.980 --> 02:18:15.980]  У меня пока что рекорд был пятичасовой дополнительный семинар.
[02:18:15.980 --> 02:18:20.980]  Если вы хотите, вы готовы убить рекорды.
[02:18:20.980 --> 02:18:22.980]  Да, не, ну...
[02:18:22.980 --> 02:18:24.980]  Сколько времени осталось?
[02:18:24.980 --> 02:18:25.980]  Ну, за это как?
[02:18:25.980 --> 02:18:31.980]  Ещё хватит девять минут, полчаса.
[02:18:31.980 --> 02:18:35.980]  Ну, дальше будем записывать, значит, стенографией займемся.
[02:18:35.980 --> 02:18:39.980]  Можно зарисовывать будет доску.
[02:18:39.980 --> 02:18:42.980]  Так, ну ладно, вопрос такой.
[02:18:42.980 --> 02:18:46.980]  По векторам более-менее понятно, что происходит?
[02:18:46.980 --> 02:18:50.980]  Ну, то есть, у вас есть одномерные исходимости какие-то, они как-то между собой связаны.
[02:18:50.980 --> 02:18:55.980]  Есть какие-то всякие обходные пути, когда там у вас и слабые исходимости следуют сильные.
[02:18:55.980 --> 02:18:59.980]  Это всё можно как-то применять, там теория моноследования исходимости пользоваться.
[02:18:59.980 --> 02:19:02.980]  И вот квинтэссенция – это дельта-метод.
[02:19:02.980 --> 02:19:05.980]  Универсальная штука, которая позволяет решать задачу.
[02:19:05.980 --> 02:19:08.980]  Так, ну давайте, тема 3 – гауссовские векторы.
[02:19:17.980 --> 02:19:20.980]  Смотрите, гауссовские...
[02:19:20.980 --> 02:19:24.980]  Ну вот, в одномерном случае нормальные случайные величины играют ключевую роль.
[02:19:29.980 --> 02:19:32.980]  Хотя...
[02:19:32.980 --> 02:19:36.980]  Перед гауссовскими векторами нужно ещё кое-что сказать.
[02:19:36.980 --> 02:19:40.980]  Извините, что я вас... перебил.
[02:19:40.980 --> 02:19:43.980]  Я сейчас вспомнил, что я очень интересную вещь ещё хотел рассказать.
[02:19:43.980 --> 02:19:47.980]  Значит, смотрите, у нас была центральная предельная теорема.
[02:19:49.980 --> 02:19:51.980]  Ещё раз обсудим.
[02:19:51.980 --> 02:19:55.980]  То есть, чтобы CPT выполнялось, вам нужно, чтобы у вас была последовательность
[02:19:55.980 --> 02:20:00.980]  независимо от одинаково распределённых случайных величин с конечной дисперсией.
[02:20:01.980 --> 02:20:04.980]  Тогда у вас выполнена CPT.
[02:20:04.980 --> 02:20:07.980]  Вот, мы в такой форме её запишем.
[02:20:15.980 --> 02:20:17.980]  Вот.
[02:20:17.980 --> 02:20:21.980]  Ещё из таких заинтересных предельных теорем у нас есть УЗБЧ, который мы уже тоже обсуждали.
[02:20:21.980 --> 02:20:26.980]  То есть, если математическое ожидание кси Катова меньше бесконечности,
[02:20:26.980 --> 02:20:30.980]  и кси Н это у нас независимо от одинаково распределённых случайных величин,
[02:20:33.980 --> 02:20:39.980]  тогда СН на Н у нас сходится к математическому ожиданию кси.
[02:20:39.980 --> 02:20:41.980]  Первого, например.
[02:20:41.980 --> 02:20:45.980]  И УЗБЧ у нас ещё был, это вот слабый закон больших чисел.
[02:20:47.980 --> 02:20:50.980]  Ну, мы знаем, например, с вами в форме Чубышова,
[02:20:50.980 --> 02:20:56.980]  что если у вас математическое ожидание конечно...
[02:20:58.980 --> 02:21:00.980]  Ну, давайте тоже дисперсия будет конечна.
[02:21:05.980 --> 02:21:09.980]  Тогда вот выполнена слабая УЗБЧ в форме Чубышова.
[02:21:09.980 --> 02:21:13.980]  Но по сути, результат ровно такой же, только сходимость не почти наверная, как в УЗБЧ.
[02:21:15.980 --> 02:21:17.980]  А по мере.
[02:21:20.980 --> 02:21:21.980]  Вот.
[02:21:21.980 --> 02:21:26.980]  И просто хочется ещё пару каких-то замечаний сказать о том, как вот эти предельные теоремы между собой связаны.
[02:21:29.980 --> 02:21:30.980]  Вот что хочется сказать.
[02:21:30.980 --> 02:21:31.980]  Первое.
[02:21:31.980 --> 02:21:33.980]  Я это говорил на своём семинаре.
[02:21:33.980 --> 02:21:37.980]  Из СПТ очевидно, что следует УЗБЧ.
[02:21:39.980 --> 02:21:40.980]  Ну, более-менее понятно.
[02:21:40.980 --> 02:21:44.980]  Чтобы было выполнено СПТ, вам нужна конечная дисперсия,
[02:21:44.980 --> 02:21:47.980]  ну и тут у вас тоже слабый УЗБЧ, у вас тоже конечная дисперсия.
[02:21:47.980 --> 02:21:49.980]  Это можно показать непосредственно.
[02:21:49.980 --> 02:21:50.980]  Ну, каким образом?
[02:21:50.980 --> 02:21:51.980]  То есть вот у вас...
[02:21:51.980 --> 02:21:52.980]  Давайте запишем СПТ.
[02:22:01.980 --> 02:22:02.980]  И применим лему Слуцкого.
[02:22:03.980 --> 02:22:05.980]  Домножим на 1, поделить на корень и зе на обе части.
[02:22:11.980 --> 02:22:14.980]  Если мы это сделаем, то у нас справа будет нолик,
[02:22:14.980 --> 02:22:20.980]  слева у нас будет и к среднему минусу от ожидания ОКСИ стремиться к нулю по распределению.
[02:22:20.980 --> 02:22:25.980]  Но сходимость константия по распределению и по вероятности эквивалентна.
[02:22:30.980 --> 02:22:31.980]  Справедливо?
[02:22:32.980 --> 02:22:33.980]  Всё.
[02:22:33.980 --> 02:22:35.980]  То есть мы с вами только что показали, что СПТ следует УЗБЧ.
[02:22:37.980 --> 02:22:40.980]  На самом деле, вот здесь хочется ещё немножко к характеристическим функциям вернуться.
[02:22:41.980 --> 02:22:43.980]  А характеристические функции – это очень мощный инструмент.
[02:22:43.980 --> 02:22:45.980]  И вот кто-то спрашивал, зачем они нужны.
[02:22:45.980 --> 02:22:49.980]  В основном характеристические функции нужны для того, чтобы всякие предельные законы доказывать.
[02:22:51.980 --> 02:22:52.980]  Давайте...
[02:22:52.980 --> 02:22:57.980]  Вот мы знаем с вами доказательства слабого закона больших чисел через неравенство ЧБШО,
[02:22:57.980 --> 02:22:58.980]  мы это все делали.
[02:22:59.980 --> 02:23:03.980]  Но на самом деле ЗБЧ можно доказать и через характеристические функции.
[02:23:04.980 --> 02:23:05.980]  Нет, нет.
[02:23:05.980 --> 02:23:06.980]  Кто-то спрашивал больше.
[02:23:06.980 --> 02:23:07.980]  То есть какой-то теор...
[02:23:07.980 --> 02:23:09.980]  Ну, не даже не теор, а просто...
[02:23:12.980 --> 02:23:14.980]  Это просто удобный инструмент.
[02:23:15.980 --> 02:23:16.980]  Да.
[02:23:17.980 --> 02:23:20.980]  Помогает доказывать сходимости, помогает какие-то штуки проще доказывать.
[02:23:21.980 --> 02:23:25.980]  То есть по сути, ещё раз, идея такая, чтобы в исходном пространстве делать преобразование фурье,
[02:23:25.980 --> 02:23:28.980]  там что-то доказываете, в силу теоремы единственности возвращаетесь обратно.
[02:23:28.980 --> 02:23:30.980]  Соответственно, вы доказали для исходных случайных лечений.
[02:23:31.980 --> 02:23:32.980]  Отлично.
[02:23:32.980 --> 02:23:33.980]  Так, ну...
[02:23:34.980 --> 02:23:37.980]  Давайте ЗБЧ аккуратно докажем через Хар функции.
[02:23:52.980 --> 02:23:53.980]  Смотрите.
[02:23:53.980 --> 02:23:57.980]  Как мы уже с вами обсуждали, у характеристических функций обычно рассматриваются производные.
[02:23:58.980 --> 02:24:02.980]  Ну и на самом деле характеристическую функцию можно просто разложить в ряд Тейлора,
[02:24:02.980 --> 02:24:04.980]  так как это экспоненты какая-то обычная.
[02:24:04.980 --> 02:24:07.980]  И если у вас есть математическое ожидание дисперсия,
[02:24:07.980 --> 02:24:15.980]  то на самом деле ваша характеристическая функция представима в таком виде.
[02:24:28.980 --> 02:24:29.980]  Вот.
[02:24:29.980 --> 02:24:33.980]  Ну это по сути просто разложение экспонента в ряд Тейлора.
[02:24:35.980 --> 02:24:39.980]  Вот производные, а вот там типа точка Т, Т квадрата и так далее.
[02:24:40.980 --> 02:24:41.980]  Вот.
[02:24:41.980 --> 02:24:45.980]  Соответственно, давайте попробуем с вами доказать ЗБЧ через характеристические функции.
[02:24:46.980 --> 02:24:49.980]  Так, значит, давайте сначала сформулируем теоремы непрерывности.
[02:24:58.980 --> 02:25:02.980]  Теорема о непрерывности связывает сходимость по распределению и сходимость характеристических функций.
[02:25:03.980 --> 02:25:09.980]  Значит, у теоремы о непрерывности, по сути, два пункта.
[02:25:09.980 --> 02:25:14.980]  Первый пункт, что если у вас последовательность случайных величин сходится по распределению,
[02:25:14.980 --> 02:25:19.980]  тогда у вас и характеристические функции соответствующие сходятся в каждой точке.
[02:25:24.980 --> 02:25:26.980]  Это, думаю, более-менее понятно.
[02:25:26.980 --> 02:25:27.980]  Вот.
[02:25:27.980 --> 02:25:29.980]  В обратную сторону на самом деле тоже верно.
[02:25:29.980 --> 02:25:35.980]  То есть, если у вас есть последовательность каких-то функций характеристических,
[02:25:35.980 --> 02:25:39.980]  и они сходятся к какой-то функции фиат Т,
[02:25:39.980 --> 02:25:41.980]  если фиат Т непрерывно в нуле,
[02:25:46.980 --> 02:25:54.980]  то фиат Т – это характеристическая функция предельной случайной величины вот этих ксенок.
[02:25:54.980 --> 02:25:58.980]  То есть то ксен сходится ксим по распределению.
[02:25:58.980 --> 02:26:04.980]  В некотором смысле сходимость по распределению задается сходимостью характеристических функций.
[02:26:04.980 --> 02:26:06.980]  Практически в обе стороны это верно,
[02:26:06.980 --> 02:26:10.980]  здесь вот с небольшой оговорочкой, что предельная функция должна быть непрерывна в нуле.
[02:26:10.980 --> 02:26:15.980]  Если это так, то этоothо эквивалентное, как бы, два эквивалента условия.
[02:26:15.980 --> 02:26:19.980]  То есть сходимость по распределению можно сдавать как сходимость в силу теоремы Александровой
[02:26:19.980 --> 02:26:21.980]  сходимость функций распределения в каждой точке,
[02:26:21.980 --> 02:26:25.660]  можно задавать через характеристические функции, сходимости характеристических
[02:26:25.660 --> 02:26:26.660]  функций в каждой точке.
[02:26:26.660 --> 02:26:27.660]  Вот.
[02:26:27.660 --> 02:26:30.740]  Ну и собственно давайте с учетом этого докажем
[02:26:30.740 --> 02:26:31.980]  все-таки уже за быча.
[02:26:31.980 --> 02:26:58.060]  Значит, что у нас за быча есть?
[02:26:58.060 --> 02:26:59.980]  Ну пусть у нас есть какие-нибудь случайные величины, которые
[02:26:59.980 --> 02:27:00.980]  просто...
[02:27:00.980 --> 02:27:03.700]  Ну пусть будут независимые и имеют конечное математическое
[02:27:03.700 --> 02:27:05.620]  ожидание и дисперсия.
[02:27:05.620 --> 02:27:09.420]  Тогда у ксен есть характеристическая функция.
[02:27:09.420 --> 02:27:16.180]  У каждого слагайма, у каждой ксенки они одинаково распределены.
[02:27:16.180 --> 02:27:17.180]  Вот.
[02:27:17.180 --> 02:27:20.940]  И у них есть какая-то характеристическая функция, которую вот в силу такого
[02:27:20.940 --> 02:27:23.420]  разложения можно разложить вот до первого порядка.
[02:27:23.420 --> 02:27:36.100]  Что-нибудь вот такое, да?
[02:27:36.100 --> 02:27:37.460]  Дальше.
[02:27:37.460 --> 02:27:39.140]  Давайте попробуем с вами посчитать характеристическую
[02:27:39.140 --> 02:27:46.380]  функцию sn на n.
[02:27:46.380 --> 02:27:47.700]  Характеристическая функция линейного преобразования
[02:27:47.700 --> 02:27:51.500]  это просто характеристическая функция sn в точке t поделить
[02:27:52.380 --> 02:27:53.380]  на n.
[02:27:53.380 --> 02:27:54.380]  Вот.
[02:27:54.380 --> 02:27:56.420]  А характеристическая функция sn, так как у нас эти случайные
[02:27:56.420 --> 02:27:58.620]  величины считаются что независимы, то это просто
[02:27:58.620 --> 02:28:00.860]  произведение тех характеристических функций, которые у нас выше
[02:28:00.860 --> 02:28:01.860]  записаны.
[02:28:01.860 --> 02:28:07.900]  То есть это будет единичка плюс иt математическое ожидание
[02:28:07.900 --> 02:28:13.900]  кси в степени n, ну вот так, в общем.
[02:28:13.900 --> 02:28:14.900]  Ну а это замечательный предел.
[02:28:15.500 --> 02:28:21.940]  Это просто е в степени иt математического ожидания
[02:28:21.940 --> 02:28:22.940]  кси.
[02:28:22.940 --> 02:28:26.060]  А это характеристическая функция константы.
[02:28:26.060 --> 02:28:27.060]  Мы это уже с вами сегодня показывали.
[02:28:27.060 --> 02:28:31.300]  Если у вас характеристическая функция итt, то это характеристическая
[02:28:31.300 --> 02:28:32.860]  функция константы.
[02:28:32.860 --> 02:28:37.960]  Таким образом, sn на n в силу теоремы непрерывности
[02:28:37.960 --> 02:28:40.660]  у вас сходится по распределению константии математической
[02:28:40.660 --> 02:28:41.660]  ожидания кси.
[02:28:41.660 --> 02:28:44.300]  Ну а сходимость по распределению константии эквивалентна
[02:28:44.300 --> 02:28:46.780]  сходимости к константе по вероятности.
[02:28:46.780 --> 02:28:51.180]  Это мы с вами тоже сегодня доказывали.
[02:28:51.180 --> 02:28:57.660]  Ну вот всё.
[02:28:57.660 --> 02:29:01.140]  Таким образом мы с вами доказали ЗБЧ через характеристические
[02:29:01.140 --> 02:29:02.140]  функции.
[02:29:02.140 --> 02:29:03.140]  Вот.
[02:29:03.140 --> 02:29:04.620]  Ну и на самом деле идея доказательства ЦПТ точно
[02:29:04.620 --> 02:29:05.620]  такая же.
[02:29:05.620 --> 02:29:09.460]  Вы раскладываете вашу характеристическую функцию до второго порядка.
[02:29:09.460 --> 02:29:10.460]  Дальше.
[02:29:10.460 --> 02:29:12.140]  Обратите внимание, что мы там нормируем случайные
[02:29:12.140 --> 02:29:13.140]  величины.
[02:29:13.140 --> 02:29:16.660]  Мы из СН вычитаем математические ожидания.
[02:29:16.660 --> 02:29:18.620]  По сути мы вот это зануляем.
[02:29:18.620 --> 02:29:20.620]  У нас остаются только вот эти слагаемые.
[02:29:20.620 --> 02:29:22.820]  И опять-таки пользуемся замечательным пределом.
[02:29:22.820 --> 02:29:26.180]  У нас всё вот это, чудо, вот похожее чудо, будет
[02:29:26.180 --> 02:29:30.420]  сходиться к Е в степени минус Т в квадрате пополам.
[02:29:30.420 --> 02:29:32.580]  Ну а это характеристическая функция стандарта нормальной
[02:29:32.580 --> 02:29:33.580]  случайной величины.
[02:29:33.580 --> 02:29:34.580]  Ну и всё, собственно.
[02:29:34.580 --> 02:29:36.860]  Всё доказательство ЦПТ в том, чтобы отнормировать
[02:29:36.860 --> 02:29:40.940]  случайные величины, разложить их до второго порядка, применить
[02:29:40.940 --> 02:29:43.260]  второй замечательный предел и получить, что предельная
[02:29:43.260 --> 02:29:45.540]  функция у вас вот такая.
[02:29:45.540 --> 02:29:47.500]  А это у вас характеристическая функция для нормальной
[02:29:47.500 --> 02:29:48.500]  случайной величины.
[02:29:48.500 --> 02:29:49.500]  ЦПТ доказано.
[02:29:49.500 --> 02:29:53.540]  ЗБЧ вот тоже через Х функцию доказали.
[02:29:53.540 --> 02:29:56.540]  Ну вот у ЗБЧ только доказать через Х функции нельзя,
[02:29:56.540 --> 02:29:58.700]  потому что там почти наверная сходимость, а она уже никак
[02:29:58.700 --> 02:30:00.780]  не связана с сходимостью по распределению.
[02:30:00.780 --> 02:30:01.780]  Вот.
[02:30:01.780 --> 02:30:07.220]  Так, теперь про предельные теоремы вроде как всё.
[02:30:07.220 --> 02:30:09.100]  Теперь действительно можно переходить к гауссовским
[02:30:09.100 --> 02:30:10.100]  векторам.
[02:30:10.260 --> 02:30:12.180]  Давайте, может, правда, отдохнём две минуты.
[02:30:12.180 --> 02:30:13.180]  Надо?
[02:30:13.180 --> 02:30:17.380]  Ну или здесь станет меньше.
[02:30:17.380 --> 02:30:18.380]  Я не боюсь.
[02:30:18.380 --> 02:30:23.420]  Однажды на мой концерт придёт один человек.
[02:30:23.420 --> 02:30:27.740]  Не, это песня просто есть такая.
[02:30:27.740 --> 02:30:28.740]  Я взгляну ему в глаза.
[02:30:28.740 --> 02:30:32.060]  А дальше я забыл слова.

[00:00.000 --> 00:15.000]  Так, добрый день. Сегодня у нас предпоследняя уже лекция получается.
[00:15.000 --> 00:20.000]  Мы поговорим подробно про верояностные вычисления.
[00:20.000 --> 00:26.000]  Ну а на последний, видимо, будет какое-то окончание про вероятность.
[00:26.000 --> 00:30.000]  Да, и еще про некоторые темы поговорим.
[00:30.000 --> 00:46.000]  Вот так, вкратце напомню, что мы изучали в прошлый раз.
[00:46.000 --> 00:54.000]  Мы изучали в целом понятие вероятных вычислений и несколько примеров.
[00:54.000 --> 01:07.000]  В случае, когда ответ бинарный, да или нет, то верояностные вычисления означают, что есть некоторая вероятность и ошибки.
[01:07.000 --> 01:15.000]  То есть может быть так, что настоящий ответ нет, а алгоритм отвечает да, ну или наоборот настоящий ответ да, а алгоритм отвечает нет.
[01:15.000 --> 01:22.000]  Соответственно, мы изучили несколько примеров.
[01:22.000 --> 01:31.000]  Пример проверка числа на простоту и проверка многочленов на равенство.
[01:31.000 --> 01:39.000]  Лему Шварца-Зиппеля, ну ладно, я думаю, можно пропустить.
[01:39.000 --> 01:51.000]  Интуитивно, по крайней мере, должно быть понятно, что два многочлена редко совпадают в большом числе точек.
[01:51.000 --> 01:57.000]  Ну и сейчас начнем с того, что поговорим про верояностные классы.
[01:57.000 --> 02:01.000]  Мы немножко поговорили про верояностные алгоритмы, про верояностные задачи.
[02:01.000 --> 02:09.000]  Теперь поговорим про верояностные классы.
[02:09.000 --> 02:17.000]  Ну и начать на самом деле с того, что такая модель используется.
[02:17.000 --> 02:25.000]  Модель вычислений – это верояностная машина тьюринга.
[02:25.000 --> 02:29.000]  Ее можно по-разному определять.
[02:29.000 --> 02:35.000]  Формально сама по себе машина как математический объект.
[02:35.000 --> 02:41.000]  Это то же самое, что нетерминированная машина.
[02:41.000 --> 02:51.000]  Формально как математический объект.
[02:51.000 --> 03:01.000]  Верояностная машина – это то же самое, что недетерминированная.
[03:01.000 --> 03:09.000]  То есть там тоже есть всеобычные множества, там множество символов алфавита, множество состояний,
[03:09.000 --> 03:17.000]  какие-то выделенные состояния и функции перехода, которые являются многозначными.
[03:17.000 --> 03:25.000]  Функция перехода является многозначной.
[03:25.000 --> 03:29.000]  Но отличие следующее.
[03:29.000 --> 03:45.000]  Недетерминированная машина магически угадывает правильную ветвь вычислений.
[03:45.000 --> 03:53.000]  Правильный переход в каждый момент.
[03:53.000 --> 04:01.000]  А верояностная просто идет по случайной ветве.
[04:01.000 --> 04:09.000]  Соответственно ответ верояностной машины – это про случайная величина.
[04:09.000 --> 04:19.000]  Удобнее всего предполагать, что каждый раз у нас есть два возможных перехода.
[04:19.000 --> 04:29.000]  Удобнее всего предполагать, что функция двузначная.
[04:29.000 --> 04:39.000]  То есть каждый раз есть два допустимых следующих шага.
[04:39.000 --> 04:49.000]  То есть в каждом состоянии есть два допустимых шага.
[04:49.000 --> 04:55.000]  Ну и тогда не нужно возиться с тем, как это работает.
[04:55.000 --> 05:11.000]  То есть в каждом состоянии есть два допустимых следующих шага.
[05:11.000 --> 05:21.000]  Ну и тогда не нужно возиться с тем, какое распределение вероятностей.
[05:21.000 --> 05:31.000]  Если каждый раз всего два варианта, то просто каждый из них будет равновероятен независимо от других.
[05:31.000 --> 05:43.000]  Либо альтернативно, как и с недетерминированной машиной можно говорить терминных сертификатов.
[05:43.000 --> 05:53.000]  Альтернативно есть просто два аргумента.
[05:53.000 --> 05:59.000]  М от XR.
[05:59.000 --> 06:09.000]  X это собственно аргумент.
[06:09.000 --> 06:17.000]  А R это случайные биты.
[06:17.000 --> 06:23.000]  Ну а уж откуда эти случайные биты берутся, это не очень важно.
[06:23.000 --> 06:31.000]  Может быть они в самом начале сгенерированы, может они генерируются по ходу дела по одному.
[06:31.000 --> 06:41.000]  Может быть не в самом начале, но в бесконечном количестве.
[06:41.000 --> 06:55.000]  Вот эти вариации чуть-чуть к разным вариантам приводят, особенно когда мы считаем память, особенно когда она логарифмическая.
[06:55.000 --> 07:05.000]  Когда речь идет только о времени, то это все неважно.
[07:05.000 --> 07:19.000]  Теперь все классы построены вокруг двух вероятностей.
[07:19.000 --> 07:47.000]  Говорит, что если X лежит в A, то тогда вероятность по случайному R, что машина от XR выдаст единицу.
[07:47.000 --> 07:53.000]  Ну какая это сейчас я попозже напишу, потому что это для разных классов по-разному.
[07:53.000 --> 08:09.000]  Ну и соответственно, если X лежит у вас чертой, то тогда получается, что вероятность по R,
[08:09.000 --> 08:19.000]  что M от XR равно единице, тоже какая-то.
[08:19.000 --> 08:25.000]  Соответственно, идея в первом случае должна быть большая, а вероятность в втором случае маленькая.
[08:25.000 --> 08:39.000]  Ну и теперь посмотрим на конкретные классы.
[08:39.000 --> 08:49.000]  Ну тут начнем с самого важного класса.
[08:49.000 --> 08:59.000]  Самый важный класс это класс BPP.
[08:59.000 --> 09:25.000]  Так, значит, если BPP, когда здесь будет больше чем 2 треть, а здесь будет меньше, чем 1 треть.
[09:25.000 --> 09:37.000]  То есть, соответственно, вероятность, получается, близко к единице, какие-то пороги должны быть.
[09:37.000 --> 09:43.000]  Значит, этот класс с двухсторонней ошибкой.
[09:43.000 --> 09:49.000]  Вообще BPP расшифровывается как bounded error probabilistic polynomial.
[09:49.000 --> 09:59.000]  Соответственно, bounded error означает, что ограниченная ошибка, то есть вероятность того, что ответ неправильный, будет меньше, чем 1 треть.
[09:59.000 --> 10:07.000]  При этом, что важно, это ошибка двусторонняя. Здесь больше 2 третьей, здесь меньше 1 треть.
[10:07.000 --> 10:19.000]  Так, дальше. На самом деле, если вспомните, что мы вот тут проходили, и в случае многочленов, и в случае простых чисел,
[10:19.000 --> 10:33.000]  получалось, что, соответственно, в одном случае ошибки точно не было.
[10:33.000 --> 10:41.000]  То есть тут, в случае теста Милля Рабина, простое число точно признавалось простым, а составное, с ней сравнится, тоже было простым.
[10:41.000 --> 10:49.000]  В случае многочленов, тут получалось, что если многочлены одинаковы, они точно будут признаны одинаковыми.
[10:49.000 --> 10:59.000]  Если они не разные, то с небольшой вероятностью могут тоже быть признаны одинаковыми.
[10:59.000 --> 11:05.000]  И с этим связаны два класса с затронившей ошибкой.
[11:05.000 --> 11:13.000]  Один называется RP, и это означает randomised polynomial.
[11:13.000 --> 11:21.000]  В данном контексте так сложилось, что randomised и probabilistic – это разные слова.
[11:21.000 --> 11:45.000]  Так, если RP, то тут будет больше, чем 1 вторая, а тут просто равно 0.
[11:45.000 --> 11:55.000]  То есть получается, что если реальный ответ «да», то и машина скажет «да» с хотя бы 1 вторая.
[11:55.000 --> 12:03.000]  Если реальный ответ «нет», то машина точно скажет «нет», и ни за что не скажет «да».
[12:03.000 --> 12:13.000]  Есть симметричные случаи – это QRP.
[12:13.000 --> 12:21.000]  Это означает, что здесь равно единице.
[12:21.000 --> 12:27.000]  Это как раз случай простых чисел. Если число простое, то точно скажут, что оно простое.
[12:27.000 --> 12:31.000]  Если составное, то непонятно.
[12:31.000 --> 12:39.000]  Тут, соответственно, будет меньше, чем 1 вторая.
[12:39.000 --> 12:47.000]  Это QRP.
[12:47.000 --> 12:57.000]  Теперь есть класс просто PP без буквы B.
[12:57.000 --> 13:01.000]  Без буквы B – это с неограниченной ошибкой.
[13:01.000 --> 13:07.000]  Но если разрешить ошибку 1 вторая и там и там, то тогда у нас что угодно сюда подойдет.
[13:07.000 --> 13:13.000]  Мы всегда можем просто кидать монетку и такой ответ давать.
[13:13.000 --> 13:19.000]  Поэтому так не подойдет.
[13:19.000 --> 13:29.000]  Например, можно написать больше или правдно в 1 и 2.
[13:29.000 --> 13:35.000]  Тут, соответственно, строго меньше, чем 1 вторая.
[13:35.000 --> 13:43.000]  Чем PP лучше, чем BPP?
[13:43.000 --> 13:49.000]  Больше он тем, что какая бы ни была машина, она какой-то язык определит.
[13:49.000 --> 13:57.000]  Потому что здесь проблема в том, что машина может выдавать не больше 2-х третьей, не меньше 1-х третьей,
[13:57.000 --> 14:03.000]  а что-то промежуточное, тогда такая машина вообще никакого языка не определит.
[14:03.000 --> 14:15.000]  А вот в случае PP получается, что любая машина какой-то язык определяет.
[14:15.000 --> 14:21.000]  Дальше еще есть важный язык ZPP.
[14:21.000 --> 14:27.000]  ZPP это zero error, нулевая ошибка.
[14:27.000 --> 14:29.000]  Пробавилистик polynomial.
[14:29.000 --> 14:35.000]  Здесь, соответственно, тут будет равно 1, тут будет равно 0.
[14:35.000 --> 14:37.000]  Почему это не просто P?
[14:37.000 --> 14:49.000]  А дело в том, что в этом случае zero error получается только ожидаемое время работы.
[14:49.000 --> 14:51.000]  Полиномиально.
[14:51.000 --> 14:55.000]  Да, в худшем случае может быть экспоненциально.
[14:55.000 --> 15:01.000]  Или даже в принципе больше.
[15:01.000 --> 15:05.000]  Ну и вообще-то сюда же можно еще отнести NP и QNP.
[15:05.000 --> 15:09.000]  Да, можно писать, что NP.
[15:09.000 --> 15:21.000]  В случае NP тут будет здесь больше 0, а здесь равно 0.
[15:21.000 --> 15:33.000]  Ну и в случае SQNP тут будет равно 1, а тут будет меньше 1.
[15:33.000 --> 15:43.000]  Вот такие классы.
[15:43.000 --> 15:47.000]  Дальше определение у всех одинаковое.
[15:47.000 --> 15:51.000]  То есть язык A лежит в классе одном из этих.
[15:51.000 --> 15:59.000]  Есть существует такая машина M, вычисляемое заполиномиальное время,
[15:59.000 --> 16:05.000]  что, если X лежит в A, то вероятность того, что машина вы достанется,
[16:05.000 --> 16:09.000]  нужно брать сверху соответствующие условия,
[16:09.000 --> 16:15.000]  а если X не принудится, то вероятность, соответственно, нужно брать снизу условия.
[16:15.000 --> 16:19.000]  Ну, соответственно, понятно, почему NP и QNP сюда подходят.
[16:19.000 --> 16:23.000]  Потому что в NP, если ответ да, значит такой R есть.
[16:23.000 --> 16:27.000]  И значит, можно его случайно взять на расположительную вероятность.
[16:27.000 --> 16:33.000]  Если такого R нет, то его случайно взять нельзя.
[16:41.000 --> 16:43.000]  Одну секунду.
[16:43.000 --> 16:51.000]  Тут можно теперь поговорить про то, как эти классы соотносятся друг с другом
[16:51.000 --> 16:57.000]  и с разными другими классами, которые мы уже изучили,
[16:57.000 --> 17:03.000]  которые связаны со схемами, с полиномиальной иерархией,
[17:03.000 --> 17:09.000]  с полиномиальной памятью и так далее.
[17:09.000 --> 17:17.000]  Но давайте нарисую некоторую диаграмму.
[17:17.000 --> 17:21.000]  Ну, в самом низу у нас будет класс P.
[17:21.000 --> 17:25.000]  Значит, класс P, которого тут, конечно, нигде нет.
[17:33.000 --> 17:39.000]  Значит, дальше будет класс ZPP.
[17:39.000 --> 17:45.000]  И он не просто будет, а у нас будет равняться пересечению.
[17:45.000 --> 17:53.000]  И у нас будет равняться RP в пересечении с QRP.
[18:05.000 --> 18:09.000]  Получается, P, конечно, вложено в ZPP,
[18:09.000 --> 18:13.000]  потому что можно случайно это вообще игнорировать,
[18:13.000 --> 18:15.000]  просто вычислить ответ.
[18:15.000 --> 18:19.000]  Дальше чуть выше будут самети.
[18:19.000 --> 18:23.000]  Я буду соблюдать цветовое кодирование.
[18:23.000 --> 18:25.000]  Самети RP и QRP.
[18:25.000 --> 18:35.000]  Так, 4P.
[18:35.000 --> 18:39.000]  И тут QRP.
[18:39.000 --> 18:43.000]  So, QRP.
[18:45.000 --> 18:49.000]  Так.
[18:49.000 --> 18:53.000]  4RP.
[18:53.000 --> 18:59.000]  И QRP.
[18:59.000 --> 19:05.000]  Значит, дальше будет получаться, что здесь...
[19:05.000 --> 19:33.000]  Здесь bpp, bpp, np, np co-np, и на самом верху pp.
[19:33.000 --> 19:45.000]  Вот, соответственно, тут получается вот так вот.
[19:45.000 --> 19:55.000]  Значит, тут получается вот так вот.
[19:55.000 --> 20:05.000]  Значит, вот так вот, вот так вот.
[20:05.000 --> 20:24.000]  Так, и, соответственно, вот так вот.
[20:24.000 --> 20:32.000]  Вот, ну и самое вверху можно еще, скажем, pspace.
[20:32.000 --> 20:49.000]  Значит, вот здесь можно написать pspace, который тут точно над всем этим будет.
[20:49.000 --> 21:01.000]  Так, ну вот, в общем, вот такая вот картинка.
[21:01.000 --> 21:05.000]  Некоторые из этих ложений совершенно очевидны.
[21:05.000 --> 21:13.000]  Значит, сначала очевидные обсудим, потом будем доказывать неочевидные.
[21:13.000 --> 21:22.000]  Так, ну а то, что p вложено в zpp.
[21:22.000 --> 21:33.000]  Ну, мы вроде уже обсудили, что можно просто взять тот же самый поэполиментальный алгрит, который дает точный ответ.
[21:33.000 --> 21:47.000]  И, соответственно, просто игнорируя случайные биты, тот же самый ответ выдавать.
[21:47.000 --> 21:57.000]  В общем, мы понимаем, что время будет и в худшем случае, да, время будет и в худшем случае пальномиальное, и ожидаемо будет пальномиальное.
[21:57.000 --> 22:02.000]  И, в общем, вот это вот более-менее понятно.
[22:02.000 --> 22:06.000]  Так, теперь то, что zpp...
[22:06.000 --> 22:16.000]  Ну, не, значит, это вообще терминес, немножко сложнее, наверное, попозже докажем.
[22:16.000 --> 22:21.000]  Ну, давайте я, наверное, буду все вложения кратко описывать.
[22:21.000 --> 22:24.000]  Они, в общем-то, не то, что совсем очевидно.
[22:24.000 --> 22:31.000]  Так, значит, p вложено в zpp.
[22:31.000 --> 22:49.000]  Значит, тут просто игнорируем r, используем старый алгоритм.
[22:49.000 --> 23:01.000]  Так, значит, rp вложено в np.
[23:01.000 --> 23:05.000]  Ну, это как раз очевидно.
[23:05.000 --> 23:22.000]  На что, если вероятность больше, чем одна вторая, то она больше нуля.
[23:22.000 --> 23:30.000]  Если внимательно посмотреть на то, что у нас вот здесь вот, то есть тут у нас равно нулю, равно нулю, тут больше одной второй, тут больше нуля.
[23:30.000 --> 23:41.000]  Конечно, ясно, что из того, что больше одна вторая следует.
[23:41.000 --> 23:50.000]  Ну и на самом деле аналогично то, что qrp вложено в qnp.
[23:50.000 --> 24:02.000]  Да, то есть тут qrp вложено в qnp, это аналогично.
[24:02.000 --> 24:09.000]  Ну и на самом деле то, что bpp вложено в pp, тоже аналогично.
[24:09.000 --> 24:19.000]  Вот, а именно, значит, если больше двух третьей,
[24:19.000 --> 24:31.000]  Следователь, да, больше одной второй.
[24:31.000 --> 24:41.000]  Ну, а меньше одной третьей, значит, меньше одной второй.
[24:41.000 --> 24:51.000]  Меньше одной третьей.
[24:51.000 --> 25:01.000]  Меньше одной третьей.
[25:01.000 --> 25:11.000]  Следовательно, меньше одной второй.
[25:21.000 --> 25:31.000]  Так, теперь.
[25:31.000 --> 25:37.000]  Ну вот, например, rp вложено в bpp.
[25:37.000 --> 25:45.000]  Значит, rp вложено в bpp.
[25:45.000 --> 25:53.000]  Значит, это происходит, потому что, ну а тут уже так просто не получится.
[25:53.000 --> 25:57.000]  Потому что, смотрите, конечно, если равно нулю, то это меньше одной третьей.
[25:57.000 --> 26:01.000]  Но если больше одной второй, то еще не знаю, что больше двух третьей.
[26:01.000 --> 26:09.000]  Но можно амблифицировать, то есть можно запустить два раза.
[26:09.000 --> 26:17.000]  Ну, допустим, алгоритм дважды.
[26:17.000 --> 26:28.000]  И в качестве результата возьмем
[26:28.000 --> 26:52.000]  Вот, тогда получается, что если x, значит, если x лежит у вас чертой,
[26:52.000 --> 27:12.000]  если если лежит у вас чертой, то тогда получается, что вероятность, значит, вероятность того, что m от xr равно нулю
[27:12.000 --> 27:20.000]  и m от xs равно нулю.
[27:20.000 --> 27:34.000]  Значит, это будет произведение вероятности, значит, вероятность того, что m от xr равно нулю.
[27:34.000 --> 27:52.000]  Умножить на вероятность.
[27:52.000 --> 27:56.000]  Тут m от xs равно нулю.
[27:56.000 --> 28:06.000]  В общем, это ноль. Да, потому что 2 нуля умножаем.
[28:06.000 --> 28:10.000]  А, не, подождите, тут же дизью мы писали.
[28:10.000 --> 28:20.000]  Так, тогда лучше, нет, не так надо писать, надо писать, что не равно умножить, а меньше либо равно, чем сумма, которая все равно ноль.
[28:20.000 --> 28:28.000]  Так, меньше либо равно, чем сумма, которая равна нулю.
[28:28.000 --> 28:40.000]  Ну и понятно, что если у нас вероятность m, значит, она ноль.
[28:40.000 --> 28:52.000]  Так, если x лежит ва, значит, тогда вероятность...
[28:52.000 --> 29:02.000]  Ой, подождите, извините, я все правильно писал. Извините, простая вещь.
[29:02.000 --> 29:08.000]  Потому что тут же у нас что мы считаем? Мы считаем вероятность...
[29:08.000 --> 29:14.000]  Ой, нет, извините, что я запутался.
[29:14.000 --> 29:22.000]  Надо вот тут равно единице, дизьюнце.
[29:22.000 --> 29:34.000]  Вот, да, теперь правильно. То, что на первом запуске равно единице или на втором запуске равно единице.
[29:34.000 --> 29:44.000]  Вот, значит, теперь если x лежит ва, если x лежит ва, тогда вероятность того, что m...
[29:44.000 --> 29:48.000]  То же самое, да.
[29:48.000 --> 30:00.000]  m от xr равно единице или m от xs равно единице.
[30:00.000 --> 30:12.000]  И вот тут нужно сказать, что это 1 минус вероятность того, что m от xr равно нулю.
[30:18.000 --> 30:24.000]  И m от xs равно нулю.
[30:24.000 --> 30:32.000]  И вот это равно, значит, 1 минус. Вот тут уже можно взять произведение, потому что это независимая реализация.
[30:32.000 --> 30:36.000]  m от xr равно нулю.
[30:36.000 --> 30:46.000]  Множить на вероятность того, что m от xs равно нулю.
[30:46.000 --> 30:54.000]  И вот это получается больше.
[30:54.000 --> 31:00.000]  Значит, вот это получается меньше 1 и 2. Это меньше 1 и 2. Мы это перемножаем, берём с минусом.
[31:00.000 --> 31:04.000]  Это будет больше, чем 1 минус.
[31:04.000 --> 31:10.000]  Одна вторая, умножить на одну вторую.
[31:10.000 --> 31:16.000]  Вот это 3 четверти.
[31:16.000 --> 31:24.000]  Вот, и это больше, чем 2 третьи.
[31:24.000 --> 31:30.000]  Вот, да, ну, в общем, немножко круто получилось, на самом деле очень просто.
[31:30.000 --> 31:38.000]  Просто если много раз запускаем один тот же тест, то результат получает всё более-более надёжный.
[31:38.000 --> 31:44.000]  Так, начинается, получается qrp.
[31:44.000 --> 31:48.000]  qrp вложено в bpp.
[31:48.000 --> 31:54.000]  Просто аналогично.
[31:54.000 --> 32:00.000]  Так, это мы вот это вот.
[32:00.000 --> 32:06.000]  Ещё пять вот этих связок остаётся.
[32:06.000 --> 32:16.000]  Почему np вложено в pp?
[32:16.000 --> 32:22.000]  np вложено в pp.
[32:22.000 --> 32:32.000]  А тут это вот почему.
[32:32.000 --> 32:44.000]  Тут получается, что мы берём такую штуку v от x.
[32:44.000 --> 32:50.000]  Пусть у нас v для np, а m мы для pp строим.
[32:50.000 --> 32:58.000]  m от x и один бит, и оставшийся r.
[32:58.000 --> 33:04.000]  Это будет равняться следующему.
[33:04.000 --> 33:18.000]  Единица. Единица, если sigma равно нулю.
[33:18.000 --> 33:32.000]  Вот, значит, ar не равно слову из всех нулей.
[33:32.000 --> 33:40.000]  Дальше 0, если sigma равно нулю.
[33:40.000 --> 33:46.000]  ar тоже из всех нулей.
[33:46.000 --> 34:08.000]  И дальше будет v от xr, если sigma равно единице.
[34:08.000 --> 34:12.000]  Теперь смотрите, тут ровно половина.
[34:12.000 --> 34:18.000]  Половина без одной штуки единиц возьмётся вот отсюда.
[34:18.000 --> 34:34.000]  Теперь смотрите, получается, что количество таких пар sigma r,
[34:34.000 --> 34:42.000]  что m от x и sigma r равно единице.
[34:42.000 --> 34:54.000]  Это будет там 2 в степени q минус 1.
[34:54.000 --> 35:08.000]  И плюс количество таких r, что v от xr равно единице.
[35:08.000 --> 35:18.000]  Соответственно, если у нас x лежит 2,
[35:18.000 --> 35:26.000]  то вот эта штука больше единицы, больше нуля.
[35:26.000 --> 35:30.000]  Ну, больше равно единицы получается.
[35:30.000 --> 35:36.000]  Больше либо равно единицы.
[35:36.000 --> 35:46.000]  Ну и получается, что такая штука будет больше равно, чем 2 в степени q.
[35:46.000 --> 36:04.000]  А это равно 1 и 2 умножить на 2 в степени q плюс 1.
[36:04.000 --> 36:14.000]  Ну а соответственно, хотя половина получается.
[36:14.000 --> 36:36.000]  Если x лежит 2 с чертой, то эта штука равняется 2 в степени q минус 1.
[36:36.000 --> 36:48.000]  Это будет меньше, чем 1 и 2 умножить на 2 в степени q плюс 1.
[36:48.000 --> 36:56.000]  Ну вот, поэтому NP вложена в PP.
[36:56.000 --> 37:10.000]  Ну и CoNP вложена в PP аналогично.
[37:10.000 --> 37:16.000]  Вообще в этой картине все симметрично.
[37:16.000 --> 37:24.000]  Все симметричные вложения делаются точно так же, как исходные.
[37:24.000 --> 37:34.000]  Ну что у нас осталось? PP вложена в PSPACE.
[37:34.000 --> 37:44.000]  Ну это очень легко, на самом деле.
[37:44.000 --> 37:58.000]  Что просто на памяти, которая излина r, можно перебирать все r.
[37:58.000 --> 38:18.000]  Значит, запускать m от xr, подсчитывать долю единиц и сравнивать с половиной.
[38:18.000 --> 38:28.000]  Долю сравнивать с одной-второй.
[38:48.000 --> 38:54.000]  Вот, поэтому PP вложена в PSPACE.
[38:54.000 --> 39:08.000]  Единственное, что у нас осталось, это ZPP равняется rp пересечения square p.
[39:08.000 --> 39:14.000]  Так, ну это из двух частей.
[39:14.000 --> 39:18.000]  То есть лево-направо есть вложение, и справа-налево есть вложение.
[39:18.000 --> 39:26.000]  Так, ну почему ZPP вложена в rp?
[39:26.000 --> 39:36.000]  Значит, смотрите, пусть среднее время работы равно какому-то TAT.
[39:36.000 --> 39:50.000]  Тогда, поскольку время от величины не отрицательное, то можно применить неравенство Маркова.
[39:50.000 --> 40:14.000]  По неравенству Маркова, вероятность того, что время работы больше, чем два TAT, будет меньше одной-второй.
[40:14.000 --> 40:42.000]  Вот так получается алгоритм такой.
[40:42.000 --> 40:58.000]  Значит, запустим алгоритм на два TAT шагов.
[40:58.000 --> 41:08.000]  Если остановился, то выдадем тот же ответ.
[41:08.000 --> 41:26.000]  Если не остановился, то выдадем ноль.
[41:26.000 --> 41:30.000]  Смотрите, что получается.
[41:30.000 --> 41:34.000]  Смотрим на определение.
[41:34.000 --> 41:38.000]  Если ответ у нас вытянут, то он точно правильный.
[41:38.000 --> 41:42.000]  Дальше что получается?
[41:42.000 --> 41:54.000]  Если мы запускаем на два TAT, то, скорее всего, время успеет завершиться.
[41:54.000 --> 41:58.000]  Причем тут важно это ожидаемое время.
[41:58.000 --> 42:10.000]  Это означает, что для любого фиксированного X среднее время усредненное по r будет пальномиальным.
[42:10.000 --> 42:12.000]  Это не усреднение между X.
[42:12.000 --> 42:16.000]  Это именно для каждого X усреднение по случайным битам.
[42:16.000 --> 42:28.000]  Но получается, что если ответ один, то сериальность больше, чем одна-вторая, успеет завершиться.
[42:28.000 --> 42:30.000]  И выдаст один.
[42:30.000 --> 42:34.000]  Поэтому тут больше одна-вторая будет.
[42:34.000 --> 42:38.000]  Если ответ ноль, то в любом случае будет ответ ноль.
[42:38.000 --> 42:42.000]  Либо потому что ZPP завершился, либо потому что мы не нашли ответа.
[42:42.000 --> 42:50.000]  Тогда мы выведем ноль.
[42:50.000 --> 43:00.000]  Дальше ZPP вложено в QRP.
[43:00.000 --> 43:02.000]  Значено.
[43:02.000 --> 43:08.000]  Но выдаем в конце единицу.
[43:08.000 --> 43:12.000]  И тогда неопределивший перерастывает на другую сторону.
[43:12.000 --> 43:16.000]  Это же все правильно.
[43:16.000 --> 43:24.000]  Поэтому слева направо вложено.
[43:24.000 --> 43:28.000]  И остается в другую сторону.
[43:28.000 --> 43:42.000]  Почему QRP вложено в ZPP?
[43:42.000 --> 43:46.000]  Тут дело вот с чем.
[43:52.000 --> 43:54.000]  Смотрите.
[43:54.000 --> 43:56.000]  Настройной ошибкой нужно быть аккуратным.
[43:56.000 --> 44:00.000]  Потому что тут немножко раньше нулевую с другой стороны.
[44:00.000 --> 44:08.000]  Если не лежит ва, то точно машина выдаст ноль.
[44:08.000 --> 44:14.000]  Но если она выдала ноль, то это не значит, что она точно лежит ва.
[44:14.000 --> 44:18.000]  Наоборот, если она выдала один, то она точно лежит ва.
[44:18.000 --> 44:20.000]  А если она выдала ноль, то она может быть и отсюда, и отсюда.
[44:20.000 --> 44:26.000]  Получается такая двойственность, что если не лежит ва, то точно выдаст ноль.
[44:26.000 --> 44:32.000]  Но если выдала ноль, то не значит, что точно не лежит.
[44:32.000 --> 44:38.000]  А наоборот, если выдала один, то точно лежит.
[44:38.000 --> 44:48.000]  Хорошо, пусть у нас язык лежит в пересечении.
[44:48.000 --> 44:52.000]  РП в пересечении QRP.
[44:56.000 --> 45:00.000]  Пусть А лежит в РП в пересечении QRP.
[45:00.000 --> 45:02.000]  Что это значит?
[45:02.000 --> 45:10.000]  Значит, что есть два алгоритма V и W.
[45:10.000 --> 45:14.000]  Со следующими свойствами.
[45:14.000 --> 45:38.000]  Если у нас X лежит ва, то, следовательно, вероятность того, что V от XR равно единице,
[45:38.000 --> 45:44.000]  значит, это будет больше, чем 1 вторая.
[45:44.000 --> 46:02.000]  Если X лежит ва с чертой, то тогда вероятность того, что V от XR равно единице, это равно нулю.
[46:02.000 --> 46:20.000]  И, соответственно, также, если X лежит ва, то тогда вероятность того, что W от XS равно единице, это равно единице.
[46:20.000 --> 46:48.000]  Значит, если X лежит ва с чертой, то тогда получается, что вероятность того, что W от XS равно единице, это будет меньше, чем 1 вторая.
[46:48.000 --> 46:54.000]  Можно на это самое еще посмотреть.
[46:54.000 --> 47:20.000]  Значит, если W от XR равно единице, то тогда получается, что X лежит ва с чертой.
[47:20.000 --> 47:22.000]  Нет, тут, наоборот, равно нулю.
[47:22.000 --> 47:30.000]  Их лежит ва с чертой.
[47:30.000 --> 47:38.000]  Ну и получаем такой алгоритм.
[47:38.000 --> 48:00.000]  Значит, запускаем V от XR и W от XS.
[48:00.000 --> 48:02.000]  Ну и получаем такой алгоритм.
[48:30.000 --> 48:46.000]  Так, да, прошу прощения.
[48:46.000 --> 48:50.000]  Хорошо, значит, запускаем V от XR и W от XS.
[48:50.000 --> 49:10.000]  Вот, соответственно, если V от XR равно единице, то возвращаем единицу.
[49:10.000 --> 49:22.000]  Значит, если W от XS равно нулю, то возвращаем ноль.
[49:22.000 --> 49:50.000]  Вот, а если не то, не другое, то просто запускаем еще раунд.
[49:50.000 --> 49:58.000]  Вот, что же получается?
[49:58.000 --> 50:26.000]  Получается, что вероятность в любом случае, независимо от того, лежит ли X ва,
[50:26.000 --> 50:50.000]  будет верно следующее, что вероятность того,
[50:50.000 --> 51:16.000]  что V от XR равно нулю и W от XS равно единице, значит, это будет меньше, чем 1 вторая.
[51:16.000 --> 51:38.000]  Вот, то есть в каждом раунде вероятность завершения
[51:38.000 --> 51:52.000]  будет больше 1 второй.
[51:52.000 --> 52:04.000]  Ну а то есть стандартная задача, значит, тогда среднее число совершенных раундов
[52:04.000 --> 52:24.000]  будет меньше двух.
[52:24.000 --> 52:40.000]  Так, значит, среднее число совершенных раундов будет меньше двух.
[52:40.000 --> 53:00.000]  Ну а тогда, поскольку каждый раунд полинамиален,
[53:00.000 --> 53:12.000]  то среднее число шагов тоже полинамиально.
[53:12.000 --> 53:20.000]  Ну вот, поэтому, значит, получилось, что в ZWP это будет лежать.
[53:20.000 --> 53:26.000]  Вот, но в принципе есть такая вариация определения, что здесь у нас, в принципе,
[53:26.000 --> 53:36.000]  возможно бесконечная ветвь, то есть может быть так, что адрес вообще никогда не закончит работу.
[53:36.000 --> 53:42.000]  Ну а на самом деле можно принудительно обрезать, да, если нам очень долго не везет,
[53:42.000 --> 53:46.000]  то мы можем просто полный перебор экспоненциально организовать какой-то момент.
[53:46.000 --> 53:52.000]  И если у нас с экспоненциальной малой вероятностью происходит экспоненциально большой перебор,
[53:52.000 --> 53:58.000]  то это в среднем ничего страшного.
[53:58.000 --> 54:04.000]  Так, ну ладно, есть какие-нибудь вопросы.
[54:04.000 --> 54:10.000]  Получается, что мы изучили вот такую картину из классов.
[54:10.000 --> 54:22.000]  Так, что еще надо сказать?
[54:22.000 --> 54:30.000]  Ну, значит, есть теперь общая идея амплификации.
[54:30.000 --> 54:42.000]  Идея амплификации, значит, уменьшение ошибки.
[54:42.000 --> 54:56.000]  Значит, например, если рассмотреть класс языков А,
[54:56.000 --> 55:22.000]  для которых при х лежит ВА, верно, что вероятность,
[55:22.000 --> 55:34.000]  значит, равна единице будет больше ноль, чем какая-то там альфа.
[55:34.000 --> 55:50.000]  А при х лежащем ВА с чертой верно, что вероятность того, что м от х равно единице, равно нулю,
[55:50.000 --> 56:10.000]  то при любой константе альфа, но только строго от нуля до единицы,
[56:10.000 --> 56:28.000]  то при любой константе альфа от нуля до единицы получится также РП.
[56:28.000 --> 56:52.000]  Вот, более того, альфа может зависеть, значит, может быть функцией от n равного длине х.
[56:52.000 --> 57:16.000]  И при любом альфа, который будет от единицы делить на полином от n до единицы минус один делить на два в степени полином от n, будет все еще РП.
[57:16.000 --> 57:36.000]  Вот, значит, для БПП будет аналогично.
[57:36.000 --> 57:50.000]  Значит, для БПП будет так.
[57:50.000 --> 58:14.000]  Да, значит, пусть у нас при х лежит ВА, при х лежащем ВА верно, что вероятность того, что м от х равно единице, значит, это будет больше, чем бета.
[58:14.000 --> 58:24.000]  Больше, чем гамма, чтобы гамма была больше бета.
[58:24.000 --> 58:53.000]  Значит, при х лежащем ВА с чертой, при х лежащем ВА с чертой верно, что вероятность того, что м от х равно единицы будет меньше, чем бета.
[58:53.000 --> 59:12.000]  Вот тогда получается, что и при любых константах ноль меньше бета, меньше гамма и меньше единицы получится БПП.
[59:12.000 --> 59:34.000]  Да, и также при любых функциях, когда у нас бета больше, чем один делить на два в степени полинома от n,
[59:34.000 --> 59:58.000]  да, значит, дальше гамма минус бета больше, чем один делить на просто полинома от n, а, соответственно, гамма больше, чем один минус один делить на два в степени полинома от n.
[59:58.000 --> 01:00:08.000]  Значит, тоже получится БПП.
[01:00:08.000 --> 01:00:15.000]  Вот, но идея очень простая. Идея, на самом деле, та же самая, которую мы вот здесь уже использовали.
[01:00:15.000 --> 01:00:23.000]  Только мы вместо двух раз запустим полиномиальное число раз.
[01:00:23.000 --> 01:00:44.000]  Да, то есть тут будет получаться, что будем запускать алгоритм полиномиальное число раз.
[01:00:44.000 --> 01:01:01.000]  Соответственно, в случае РП, в случае РП брать дезъюнцию.
[01:01:01.000 --> 01:01:11.000]  Вот, а в случае БПП брать по большинству.
[01:01:11.000 --> 01:01:17.000]  Значит, в случае РП брать дезъюнцию, в случае БПП брать по большинству.
[01:01:17.000 --> 01:01:27.000]  Ну и тогда, значит, в одном случае просто будет довольно простое соображение.
[01:01:27.000 --> 01:01:39.000]  В общем, из тех же соображений, что и раньше.
[01:01:39.000 --> 01:01:51.000]  То есть мы просто умножаем. И идея такая, что...
[01:01:51.000 --> 01:02:09.000]  Да, смотрите, у нас, значит, в случае РП получается, что вероятность того, что вот эта дезъюнция равна нулю.
[01:02:09.000 --> 01:02:28.000]  То есть что для любого I, M от X рытое равно нулю.
[01:02:28.000 --> 01:02:38.000]  То есть это у нас будет 1 минус альфа в степени число повторений, в степени Q.
[01:02:38.000 --> 01:02:49.000]  Соответственно, если у нас альфа это 1 делить на полином, значит, если альфа это 1 делить на P от N,
[01:02:49.000 --> 01:03:04.000]  А Q от N будет равно P от N на какой-нибудь еще...
[01:03:04.000 --> 01:03:08.000]  Ну, на T от N давайте.
[01:03:08.000 --> 01:03:27.000]  Вот тогда, соответственно, вот это вот 1 минус альфа вкутай будет равняться 1 минус 1 делить на P от N в степени P от N, T от N.
[01:03:27.000 --> 01:03:38.000]  И это примерно равняется экспоненту в степени минус T от N.
[01:03:38.000 --> 01:03:57.000]  Ну, а соответственно, вероятность того, что это единица, как раз то, что нас интересует, будет примерно равно 1 минус E в степени минус T от N,
[01:03:57.000 --> 01:04:05.000]  Ну, что, собственно, у нас вот здесь вот написано.
[01:04:05.000 --> 01:04:34.000]  Ну, для расчета по большинству и оценки ошибки
[01:04:34.000 --> 01:04:44.000]  Нужно использовать неравенство больших уклонений.
[01:04:44.000 --> 01:04:51.000]  Например, так называемое неравенство черного.
[01:04:51.000 --> 01:04:59.000]  Интуиция такая...
[01:04:59.000 --> 01:05:14.000]  А, ну да, по большинству это если...
[01:05:14.000 --> 01:05:19.000]  Это если симметрично относительно 1 и 2, тогда по большинству.
[01:05:19.000 --> 01:05:35.000]  Да, соответственно, если не симметрично, то тогда где-то там середины нужно сравнивать.
[01:05:35.000 --> 01:05:40.000]  Значит, большинству...
[01:05:40.000 --> 01:05:43.000]  Так, а дать я вот тут.
[01:05:43.000 --> 01:05:57.000]  То есть тут точнее нужно сравнивать с
[01:05:57.000 --> 01:06:07.000]  бета плюс гамма пополам.
[01:06:07.000 --> 01:06:14.000]  Так, интуиция. Ну, интуиция такая, что
[01:06:14.000 --> 01:06:20.000]  мат ожидания
[01:06:20.000 --> 01:06:26.000]  отличается от вот этого бета плюс гамма пополам.
[01:06:26.000 --> 01:06:34.000]  Ожидание чего? Ожидаемой долей единиц.
[01:06:34.000 --> 01:06:40.000]  Ожидание доли единиц отличается от бета плюс гамма пополам.
[01:06:40.000 --> 01:06:47.000]  Хотя бы на единицу делить на...
[01:06:47.000 --> 01:06:50.000]  Ну, давайте я так и пишу два полинома от n.
[01:06:50.000 --> 01:06:58.000]  Дай мне суду, что вот одна вторая вот от этого полинома.
[01:06:58.000 --> 01:07:11.000]  Да, соответственно, при этом вероятность
[01:07:11.000 --> 01:07:15.000]  отклонения
[01:07:15.000 --> 01:07:25.000]  на, соответственно, k-сильма, значит, на k-стандартных...
[01:07:25.000 --> 01:07:29.000]  14 отклонений на k-стандартных отклонениях
[01:07:29.000 --> 01:07:37.000]  экспоненциально убывает
[01:07:37.000 --> 01:07:41.000]  по k. Вот. И это называется CPT.
[01:07:41.000 --> 01:07:48.000]  CPT. В этом и заключается, что
[01:07:48.000 --> 01:07:53.000]  хвосты, что все стремится к гауссовскому распределению,
[01:07:53.000 --> 01:07:58.000]  а хвосты у гауссовского распределения экспоненциально убывают.
[01:07:58.000 --> 01:08:08.000]  Ну и, соответственно, стандартное отклонение
[01:08:08.000 --> 01:08:11.000]  имеет порядок...
[01:08:11.000 --> 01:08:19.000]  Так, значит, 1 делить на корень
[01:08:19.000 --> 01:08:22.000]  из числа повторений.
[01:08:22.000 --> 01:08:24.000]  Q это у нас получается.
[01:08:24.000 --> 01:08:34.000]  Значит, где Q, это число повторений.
[01:08:34.000 --> 01:08:38.000]  Да, и стандартное отклонение имеет порядок 1 делить на корень из Q.
[01:08:38.000 --> 01:08:40.000]  Дальше, где Q, это число повторений.
[01:08:40.000 --> 01:08:44.000]  Вот. Ну и, соответственно, нужно
[01:08:44.000 --> 01:08:52.000]  сделать столько повторений, чтобы
[01:08:52.000 --> 01:08:58.000]  вот это вот было бы сильно меньше,
[01:08:58.000 --> 01:09:02.000]  чем, соответственно, вот это вот.
[01:09:02.000 --> 01:09:08.000]  Ну и это полиномиальное число повторений.
[01:09:08.000 --> 01:09:16.000]  Вот это полиномиальное число повторений.
[01:09:16.000 --> 01:09:20.000]  Ну а зачем нужно конкретно нераз черного?
[01:09:20.000 --> 01:09:24.000]  Ну, идея просто в том, что CPT же предельная теорема.
[01:09:24.000 --> 01:09:29.000]  То есть это в пределе стремиться к гауссияне.
[01:09:29.000 --> 01:09:33.000]  А нам нужно не в пределе, а через какое-то конкретное число шагов.
[01:09:33.000 --> 01:09:37.000]  И чтобы вот это как бы вес этого хвоста оценивать
[01:09:37.000 --> 01:09:42.000]  для конкретного числа шагов, нужно иметь...
[01:09:42.000 --> 01:09:46.000]  Значит, нужно уметь оценивать уже этот размер хвоста
[01:09:46.000 --> 01:09:48.000]  на каком-то конечном этапе.
[01:09:48.000 --> 01:09:57.000]  Но для этого вот такие неравенства и больших уклонений нужны.
[01:09:57.000 --> 01:10:01.000]  Вот так.
[01:10:01.000 --> 01:10:04.000]  Ну и, наверное, последнее, что я сегодня расскажу,
[01:10:04.000 --> 01:10:07.000]  значит, это теорема Эйдлмана.
[01:10:07.000 --> 01:10:14.000]  Теорема Эйдлмана, которая связывает
[01:10:14.000 --> 01:10:18.000]  два наших последних раздела.
[01:10:18.000 --> 01:10:29.000]  Конечно, что BPP тоже вложена в P slash Poli.
[01:10:42.000 --> 01:10:45.000]  Так, и это, доказывается, так.
[01:10:48.000 --> 01:10:59.000]  Значит, смотрите, сделаем столько повторов,
[01:10:59.000 --> 01:11:06.000]  что вероятности ошибки станет меньше...
[01:11:06.000 --> 01:11:11.000]  Ну, один делить на два в степени m.
[01:11:11.000 --> 01:11:18.000]  Значит, меньше, чем один делить на два в степени m.
[01:11:18.000 --> 01:11:28.000]  То есть меньше, чем один делить на два в степени m.
[01:11:28.000 --> 01:11:33.000]  Делить на два в степени m.
[01:11:33.000 --> 01:11:47.000]  Значит, то есть меньше, чем число слов х длины n.
[01:11:47.000 --> 01:11:49.000]  Вот, тогда получается...
[01:11:49.000 --> 01:12:05.000]  Значит, тогда каждому из х не подходит доля r.
[01:12:05.000 --> 01:12:13.000]  Ну, меньше, значит, меньше, чем вот это вот один делить на два в степени m.
[01:12:13.000 --> 01:12:24.000]  Значит, всего х в степени n штук.
[01:12:24.000 --> 01:12:42.000]  Поэтому найдется такое r подходящее для всех х.
[01:12:42.000 --> 01:12:48.000]  Но после этого для всех х данная длина.
[01:12:48.000 --> 01:12:52.000]  Значит, длина n.
[01:12:52.000 --> 01:12:58.000]  Значит, после этого такое r можно зафиксировать.
[01:12:58.000 --> 01:13:07.000]  Такое r можно зафиксировать как часть подсказки
[01:13:07.000 --> 01:13:27.000]  и затем переделать машину Turing m в схему стандартным образом.
[01:13:27.000 --> 01:13:31.000]  Можно сказать, что вот эта r будет подсказка
[01:13:31.000 --> 01:13:34.000]  и дальше использовать уже то, что мы доказывали,
[01:13:34.000 --> 01:13:37.000]  что машина с подсказкой, это есть то же самое.
[01:13:37.000 --> 01:13:40.000]  То есть можно сказать, что r будет вся подсказка
[01:13:40.000 --> 01:13:42.000]  и пользоваться старой теоремой.
[01:13:42.000 --> 01:13:44.000]  Можно сказать, что это часть подсказки,
[01:13:44.000 --> 01:13:46.000]  а другая часть это описание машины Turing m.
[01:13:46.000 --> 01:13:57.000]  В схему запаиваем.
[01:13:57.000 --> 01:14:03.000]  Ну вот, собственно, это вся теорема.
[01:14:03.000 --> 01:14:13.000]  Ладно, так, ну что, не скиньте вопросы.
[01:14:13.000 --> 01:14:17.000]  Хорошо, спасибо, что слушали.
[01:14:17.000 --> 01:14:19.000]  В следующий раз последняя лекция.

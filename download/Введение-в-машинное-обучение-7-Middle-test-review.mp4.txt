[00:00.000 --> 00:09.640]  Ну что, коллеги, экран видно?
[00:09.640 --> 00:15.180]  Ну тогда, наверное, поехали, сука, в более удобном положении
[00:15.180 --> 00:17.720]  работать не умеет, ну тогда я буду стоять где-то вот
[00:17.720 --> 00:18.720]  здесь.
[00:18.720 --> 00:26.160]  Итак, предположение ID независимо одинаково распределённое
[00:26.160 --> 00:30.920]  применяется обычно к наблюдениям, ну да, абсолютно верно,
[00:30.920 --> 00:31.920]  к признакам.
[00:31.920 --> 00:34.840]  Так, кто может обосновать, почему к признакам оно
[00:34.840 --> 00:35.840]  применяется?
[00:35.840 --> 00:36.840]  Есть такие?
[00:36.840 --> 00:37.840]  Признакам, очевидно, нет.
[00:37.840 --> 00:38.840]  Так.
[00:38.840 --> 00:57.080]  Признаки полцы, разные столбцы, они независимо одинаково
[00:57.080 --> 00:58.080]  распределённые.
[00:58.080 --> 00:59.080]  Смотрите, контрпример.
[00:59.080 --> 01:02.000]  У меня есть один признак, не знаю там, котика или
[01:02.000 --> 01:07.160]  собачка, второй признак среднее количество калорий,
[01:07.160 --> 01:08.160]  употребляемых в сутки.
[01:08.160 --> 01:12.720]  Одно непрерывная величина, другое дискретное, бинарное.
[01:12.720 --> 01:15.560]  Кажется, они точно не могут быть одинаково распределённые.
[01:15.560 --> 01:16.560]  Правильно?
[01:16.560 --> 01:19.240]  Собственно, независимость признаков это то, о чём
[01:19.240 --> 01:21.440]  мы всегда мечтаем, потому что у нас тогда модель гораздо
[01:21.440 --> 01:25.080]  лучше работать будет, но при этом мы именно что мечтаем.
[01:25.080 --> 01:28.040]  А уж одинаковая распределённость это что-то крайне странное,
[01:28.040 --> 01:29.840]  потому что если у нас все признаки пришли из одного
[01:29.840 --> 01:32.480]  распределения, то чё-то явно не так идёт, у нас признаки
[01:32.480 --> 01:35.360]  могут быть вообще разного типа, разные могут быть
[01:35.360 --> 01:37.760]  дискретные и непрерывные, упорядоченные и упорядоченные
[01:37.760 --> 01:38.760]  и так далее.
[01:38.760 --> 01:41.200]  Так что нет, конечно, признаками это не относится.
[01:41.200 --> 01:42.200]  Тут все согласны, правильно?
[01:42.200 --> 01:56.560]  Да, они могут быть, но мы как правило этого не требуем
[01:56.560 --> 01:58.320]  от наших данных абсолютно.
[01:58.320 --> 02:01.320]  То есть такое может случиться, но в общем случае условно
[02:01.320 --> 02:03.720]  ID от данных мы хотим примерно всегда, потому что иначе
[02:03.720 --> 02:06.960]  у нас статистика, она нам не гарантирует, что у нас
[02:06.960 --> 02:07.960]  что-то там сойдётся.
[02:07.960 --> 02:09.800]  Если у нас два распределения, одна модель с ними работать
[02:09.800 --> 02:11.280]  будет далеко не всегда.
[02:11.280 --> 02:14.880]  А если у нас данные ID, мы спокойны.
[02:14.880 --> 02:19.360]  Вот, значением целевой переменной, ну тут по минимуму,
[02:19.360 --> 02:20.360]  что?
[02:20.360 --> 02:28.600]  Предположение к байсиксам классификатору применяется
[02:28.600 --> 02:32.360]  лишь частично, мы предполагаем, что признаки независимы,
[02:32.360 --> 02:33.680]  при условии таргета причём.
[02:33.680 --> 02:36.200]  А вот то, что они одинаково распределены, нет, мы же
[02:36.200 --> 02:38.400]  с вами явно параметризовали сами и выбрали, какое распределение
[02:38.400 --> 02:39.400]  будет.
[02:39.400 --> 02:41.240]  Причём можем выбрать, что они из разных нормальных
[02:41.240 --> 02:43.280]  распределений или вообще разные распределения на
[02:43.280 --> 02:44.760]  них поставить.
[02:44.760 --> 02:47.480]  Один может быть из поассона, другой биномиальный, третий
[02:47.480 --> 02:48.480]  нормальный.
[02:48.480 --> 02:49.480]  Пожалуйста.
[02:49.480 --> 02:51.640]  И вот ошибки в данных.
[02:51.640 --> 02:53.560]  Тут ошибка на самом деле в форме, потому что здесь
[02:53.560 --> 02:55.600]  должны были быть галки вместо единственного варианта
[02:55.600 --> 02:59.240]  ответа, потому что ошибки-то у нас тоже могут быть случайными
[02:59.240 --> 03:02.160]  величинами и если они случайные величины, то очень хочется,
[03:02.160 --> 03:05.080]  чтобы они были независимо одинаково распределённые.
[03:05.080 --> 03:07.120]  Независимые краски, мы с вами помните теориям Гаусс
[03:07.120 --> 03:08.120]  Маркова?
[03:08.120 --> 03:09.520]  Ну там говорили, что у них корреляция должна быть
[03:09.520 --> 03:10.520]  равна нулю.
[03:10.520 --> 03:11.680]  На всякий случай.
[03:11.680 --> 03:13.520]  Нескоррелированность, это более сильные условия
[03:13.520 --> 03:17.200]  или более слабые условия?
[03:17.200 --> 03:22.080]  Если корреляция случайных величин ноль, они независимы?
[03:22.080 --> 03:26.200]  Если они независимы, всё, пожалуйста, вот за этим следите.
[03:26.200 --> 03:28.800]  Из нулевой корреляции не следует независимость.
[03:28.800 --> 03:30.800]  Из независимости следует норевая корреляция.
[03:31.800 --> 03:33.880]  Потому что иначе вас могут на этом поймать.
[03:33.880 --> 03:36.280]  Я не помню, там есть какая-то очень красивая штука, там
[03:36.280 --> 03:38.080]  что-то кубическое что ли.
[03:38.080 --> 03:40.040]  Короче, на Википедии есть пример, ну его можно на самом
[03:40.040 --> 03:42.760]  деле придумать на доске за две минуты, когда корреляция
[03:42.760 --> 03:45.120]  у вас будет ноль, но при этом там не то, что независимо,
[03:45.120 --> 03:47.320]  там вообще как бы одна величина через другую выражается
[03:47.320 --> 03:48.320]  детерминированно практически.
[03:48.320 --> 03:51.320]  А?
[03:51.320 --> 04:00.160]  Ага, ну это далеко не общий случай, правильно?
[04:00.400 --> 04:03.280]  То есть для некоторых случайных величин, если они не скоррелированы,
[04:03.280 --> 04:05.560]  значит независимо, если вы знаете что-то об оприорном
[04:05.560 --> 04:07.200]  распределении, из которого они пришли.
[04:07.200 --> 04:09.760]  Но в общем, в случае это не работает.
[04:09.760 --> 04:10.760]  Едем дальше.
[04:10.760 --> 04:12.720]  Среди признаков, в которых описывается объект, есть
[04:12.720 --> 04:14.880]  цены в рублях и расстояние в километрах.
[04:14.880 --> 04:16.920]  Обучили на этом первый раз КНН.
[04:16.920 --> 04:19.680]  Потом поменяли признаки, точнее поменяли цены в рублях
[04:19.680 --> 04:22.160]  на цены в евро, табличка у нас стала другая, обучили
[04:22.160 --> 04:23.160]  второй КНН.
[04:23.160 --> 04:25.440]  Вопрос предсказания первого и второго КННа одинаковые
[04:25.440 --> 04:26.440]  или нет?
[04:26.440 --> 04:27.440]  Ну большинство.
[04:27.440 --> 04:28.440]  Что?
[04:28.680 --> 04:29.680]  Что?
[04:31.160 --> 04:33.680]  Во, замечательный комментарий, но тут про это, собственно,
[04:33.680 --> 04:34.680]  не слово.
[04:34.680 --> 04:36.800]  Если данные отнормированы, то абсолютно все равно,
[04:36.800 --> 04:38.600]  потому что перевод из одной валюты в другую, просто
[04:38.600 --> 04:39.960]  домножение на какую-то константу.
[04:39.960 --> 04:41.840]  Все равно потом отнормируемся в 0.1, например.
[04:43.840 --> 04:47.640]  Если нормировки нет, но именно на этот вопрос и направлен.
[04:47.640 --> 04:50.920]  Если у вас нет нормировки, шкалы данных играют значение.
[04:50.920 --> 04:53.720]  Поэтому если вы данные не отнормировали, как большинство
[04:53.720 --> 04:56.520]  и ответило, КНН будет предсказывать по-другому, почему.
[04:56.520 --> 04:57.520]  Расстояния поменялись.
[04:58.520 --> 05:01.520]  Собственно, я вас спрашивал еще в самом начале.
[05:01.520 --> 05:05.560]  У нас нормировка может быть зашита в КНН, а мы не знаем,
[05:05.560 --> 05:06.560]  какая именно реальность.
[05:06.560 --> 05:07.560]  Сейчас, поглядите.
[05:07.560 --> 05:08.840]  КНН просто считает расстояние.
[05:08.840 --> 05:12.240]  Окей, хорошо, я могу потом поправить вопрос.
[05:12.240 --> 05:13.840]  Главное, чтобы еще раз, смотрите.
[05:13.840 --> 05:16.640]  Тут все вопросы, грубо говоря, главное, чтобы вы поняли,
[05:16.640 --> 05:17.640]  когда да, когда нет.
[05:17.640 --> 05:20.320]  В данном случае, если есть нормировка, логично, что
[05:20.320 --> 05:21.400]  абсолютно неважно.
[05:21.400 --> 05:24.040]  Если нормировки нет, я надеюсь, для всех логично, что работать
[05:24.040 --> 05:25.040]  будут по-разному.
[05:25.040 --> 05:26.040]  Правильно?
[05:26.040 --> 05:27.040]  Поэтому будьте с этим осторожны.
[05:29.040 --> 05:32.120]  Да, если у вас есть какие-то вопросы, вы их, пожалуйста,
[05:32.120 --> 05:33.120]  задавайте.
[05:33.120 --> 05:35.320]  Какой способ регуляризации в линейной регрессии имеет
[05:35.320 --> 05:37.040]  тенденцию к отбору признаков?
[05:37.040 --> 05:43.600]  Большинство L1, 12% L2, 10% оба, 10% не один из очень перечисленных.
[05:43.600 --> 05:48.400]  Вопрос, что с этими 33% примерно людей есть кто-нибудь, кто
[05:48.400 --> 05:49.600]  может обосновать, почему.
[05:49.600 --> 05:51.240]  Что такое отбор признаков?
[05:51.240 --> 05:52.720]  Вот, что такое отбор признаков?
[05:52.720 --> 05:58.360]  Если мы вспомним, как у нас с вами ведет себя L1 регуляризация,
[05:58.880 --> 06:01.320]  мы можем вспомнить, что при использовании L1 регуляризации
[06:01.320 --> 06:04.000]  можно обнаружить перед некоторыми признаками весовой
[06:04.000 --> 06:06.000]  коэффициент ровно равный нулю.
[06:06.000 --> 06:10.040]  То есть не просто какая-то малая величина, а именно
[06:10.040 --> 06:11.040]  ноль.
[06:11.040 --> 06:14.600]  На бульдальской на практике нет еще?
[06:14.600 --> 06:15.600]  Тут вопрос в аудитории.
[06:15.600 --> 06:17.560]  Стоит еще раз разобрать, почему это происходит или
[06:17.560 --> 06:18.560]  нет?
[06:18.560 --> 06:23.240]  Да, хорошо, давайте тогда переведем фокус на доску.
[06:23.240 --> 06:24.240]  Нам это пригодится.
[06:24.240 --> 06:25.240]  И карандаш.
[06:26.120 --> 06:30.320]  Опять же, я приведу обоснование, которое скорее интуитивно
[06:30.320 --> 06:31.320]  понятно.
[06:31.320 --> 06:32.320]  Оно не единственное.
[06:32.320 --> 06:33.320]  Но тем не менее.
[06:33.320 --> 06:34.320]  Еще раз, смотрите.
[06:34.320 --> 06:38.080]  Вот у нас с вами есть значение нашей функции эмпирического
[06:38.080 --> 06:39.080]  риска.
[06:39.080 --> 06:40.880]  То есть то, как мы штрафуем нашу МАДЫ.
[06:40.880 --> 06:49.040]  У нас это в общем случае L, это наш ЛОС, плюс R, это
[06:49.040 --> 06:52.400]  не левый правый как в деревьях, это регуляризатор.
[06:52.400 --> 06:55.880]  И давайте скажем, что мы с вами градиентные методы
[06:55.880 --> 06:57.960]  оптимизации сейчас применяем, хорошо?
[06:57.960 --> 07:04.560]  В таком случае, когда мы с вами считаем dq по dОмега,
[07:04.560 --> 07:11.000]  это логично, что dl по dОмега плюс dr по dОмега.
[07:11.000 --> 07:13.640]  Логично, производная сумма, сумма производных.
[07:13.640 --> 07:18.840]  Эта часть у нас от регуляризации не зависит, верно?
[07:18.840 --> 07:21.200]  То есть пускай она у нас чему-то там равна.
[07:21.200 --> 07:25.200]  В какой-то точке у нас производная от функции потерь уже будет
[07:25.200 --> 07:27.920]  порядка, давайте какой-нибудь другой цвет возьму, тоже
[07:27.920 --> 07:28.920]  яркий.
[07:28.920 --> 07:31.600]  Вот этот цвет видно?
[07:31.600 --> 07:32.600]  Вот.
[07:32.600 --> 07:37.440]  Эта штука у нас там порядка 10 минус 3, например, предположим.
[07:37.440 --> 07:39.480]  Давайте посмотрим на производную.
[07:39.480 --> 07:47.280]  Так, а тут если я пишу видно, вот d норма омеги первая
[07:47.280 --> 07:49.760]  по, давайте омега итой.
[07:49.760 --> 07:54.000]  Возьмем какой-то элемент по dОмега итой, чему будет
[07:54.000 --> 07:55.000]  равен?
[07:55.000 --> 08:01.920]  Bingo, сигнум омега итой, это всем понятно, правильно?
[08:01.920 --> 08:04.760]  Потому что первая норма это на сумму модулей просто,
[08:04.760 --> 08:07.360]  тогда производная итового элемента это просто сигнум,
[08:07.360 --> 08:09.560]  если больше нуля плюс один, если меньше нуля минус
[08:09.560 --> 08:10.560]  один, если ноль-ноль.
[08:10.560 --> 08:11.560]  Верно?
[08:11.560 --> 08:14.520]  Получается, что у нас вот эта штука всегда порядка
[08:14.520 --> 08:15.520]  единиц.
[08:16.120 --> 08:20.440]  Ну, куда у нас попрет по антиградиенту изменения
[08:20.440 --> 08:21.440]  весов?
[08:21.440 --> 08:24.400]  Туда, где у нас градиент больше, логично.
[08:24.400 --> 08:27.480]  Поэтому если у нас вклад вот этой части будет меньше
[08:27.480 --> 08:29.960]  чем вот этой, выгоднее с точки зрения градиента,
[08:29.960 --> 08:31.760]  это просто устойчиво максимум минимум будет.
[08:31.760 --> 08:34.800]  Пойти туда, где у нас будет достигаться ноль, а ноль
[08:34.800 --> 08:36.600]  в единственном случае, когда вес сам равен нулю.
[08:36.600 --> 08:40.400]  То есть если признак не сильно влияет на пункт
[08:40.400 --> 08:43.400]  с ошибки, то есть производная пункт с ошибки по весу этого
[08:43.480 --> 08:44.480]  признака очень маленькая.
[08:44.480 --> 08:46.960]  Маленькая, опять же, от чего это зависит?
[08:46.960 --> 08:49.680]  Ну обычно здесь вот какой-то коэффициент регуляризации
[08:49.680 --> 08:51.880]  сидит и здесь на самом деле сидит лямбда.
[08:51.880 --> 08:54.880]  То есть вы сами контролируете, насколько вам важно признаки
[08:54.880 --> 08:55.880]  учитывать.
[08:55.880 --> 09:00.280]  Если у вас вот этот вот один на лямбда больше, чем
[09:00.280 --> 09:03.000]  тот вклад, который делает признак сюда, то он будет
[09:03.000 --> 09:04.520]  занулён, потому что он менее важен.
[09:04.520 --> 09:07.280]  Чем вам нужно?
[09:07.280 --> 09:08.280]  Понятно?
[09:08.280 --> 09:09.280]  Всем понятно сейчас.
[09:09.280 --> 09:12.880]  Это вот самое простое за всё время, что я здесь
[09:12.880 --> 09:13.880]  нахожусь.
[09:13.880 --> 09:16.800]  Наверное, объяснение, которое приходило в голову.
[09:16.800 --> 09:19.600]  Тут вроде очевидно, что одно доминирует другое.
[09:19.600 --> 09:22.640]  Вам выгоднее здесь получить как бы нулевое значение
[09:22.640 --> 09:26.200]  производной, чем здесь какое-то.
[09:26.200 --> 09:27.200]  Понятно?
[09:27.200 --> 09:28.200]  Нет?
[09:28.200 --> 09:29.200]  Всем понятно.
[09:29.200 --> 09:30.200]  Вам понятно?
[09:30.200 --> 09:31.200]  Всё, супер.
[09:31.200 --> 09:32.200]  Можем двигаться дальше?
[09:32.200 --> 09:33.200]  Супер.
[09:33.200 --> 09:38.560]  И на всякий случай для самопроверки.
[09:38.560 --> 09:40.040]  Почему со второй нормы это не прохватывает?
[09:40.040 --> 09:46.640]  Бинго, потому что здесь при эпсилон малом значении
[09:46.640 --> 09:49.760]  омега у вас производной будет 2 омега, поэтому при
[09:49.760 --> 09:52.240]  уменьшении омеги по весу у вас производной будет
[09:52.240 --> 09:54.720]  убывать к нулю линейно.
[09:54.720 --> 09:56.560]  Чем меньше омега, тем меньше производной, в какой-то
[09:56.560 --> 09:59.280]  момент они друг другу равновесят и нормально.
[09:59.280 --> 10:01.840]  L1 так не работает, тут либо 1, либо 0.
[10:01.840 --> 10:02.840]  Либо панель пропал.
[10:02.840 --> 10:03.840]  Уловили?
[10:03.840 --> 10:04.840]  Едем дальше.
[10:04.840 --> 10:12.960]  На какой энтропии?
[10:12.960 --> 10:13.960]  Каких графичках, где энтропия?
[10:13.960 --> 10:14.960]  В смысле там?
[10:14.960 --> 10:15.960]  В смысле здесь?
[10:15.960 --> 10:25.240]  Погодите, сейчас у нас будут варианты ответа, где у вас
[10:25.240 --> 10:28.360]  много вариантов, вопросов, где много вариантов ответа,
[10:28.360 --> 10:31.280]  там у вас будет мультимальное распределение.
[10:31.280 --> 10:33.000]  И там мы посмотрим, что вы там на выбирали.
[10:33.000 --> 10:37.280]  Но на самом деле это тоже можете считать, грубо говоря,
[10:37.280 --> 10:39.040]  вот ваше распределение ответ, перестроите это в
[10:39.040 --> 10:40.040]  дистаграмму, посчитайте энтропию.
[10:40.040 --> 10:45.040]  Поняли, нет?
[10:45.040 --> 10:52.760]  Вам посчитать?
[10:52.760 --> 10:54.200]  Вам на пальцах или на калькуляторе?
[10:54.200 --> 11:00.320]  Ну, короче, видно, что здесь, грубо говоря, треть студентов
[11:00.320 --> 11:01.880]  куда-то пошла не в ту степь.
[11:01.880 --> 11:04.240]  Надеюсь, сейчас эта треть разобралась, будь то в
[11:04.240 --> 11:07.240]  аудитории или в онлайне.
[11:07.240 --> 11:08.240]  Едем дальше.
[11:08.240 --> 11:11.640]  Аналитическое решение, задача линейной регрессии.
[11:11.640 --> 11:13.840]  Ну, ошибка МСЕ, это вот то, что я вам рекомендовал
[11:13.840 --> 11:19.560]  продиференцировать руками в домашке, xtx-1 и xty.
[11:19.560 --> 11:23.080]  Это полезно уметь выводить, еще более полезно уметь
[11:23.080 --> 11:26.080]  понимать, что это можно вывести в любой момент дня
[11:26.080 --> 11:28.800]  и ночи, просто помнить то, что полезно, вас могут где-нибудь
[11:28.800 --> 11:29.800]  спросить об этом.
[11:29.800 --> 11:44.560]  xtx, xty, xt… Сейчас, вы имеете в виду в каком-то из следующих
[11:44.560 --> 11:45.560]  вопросов?
[11:45.560 --> 11:46.560]  Ну, это нормально.
[11:46.560 --> 11:53.560]  Ну, ребята, хоро… Спасибо, еще раз, так как этот тест
[11:53.560 --> 11:56.960]  не оценивается, ничего страшного, здесь мы с вами
[11:56.960 --> 12:00.400]  видим, что большинство, 77% ответило благо верно.
[12:00.400 --> 12:01.400]  xtx-1, xty.
[12:01.400 --> 12:05.360]  Что делать, если вам попалась какая-то подобная формула,
[12:05.360 --> 12:08.960]  пусть что-то другое, пусть это не про функцию среднеквадратичную,
[12:08.960 --> 12:11.360]  короче, вам вот кровь из носа надо ответить, не
[12:11.360 --> 12:13.360]  знаю, какое-то внутреннее тестирование на работе,
[12:13.360 --> 12:16.280]  какой-нибудь джемат сдаете, короче, что-то, а вы кровь
[12:16.280 --> 12:17.280]  из носа ничего не помните.
[12:17.280 --> 12:20.520]  Что делать с такими формулами, как вы думаете?
[12:20.520 --> 12:23.520]  Бинго смот… Гуглить, нельзя гуглить, если вы там на джемате
[12:23.520 --> 12:24.520]  сидите.
[12:24.520 --> 12:26.640]  Сверяйте размерности, тут просто по размерностям
[12:26.640 --> 12:28.000]  большинство формул не сочетается.
[12:28.000 --> 12:32.160]  Ну, у вас не работает матрица ленинный оператор, вы не
[12:32.160 --> 12:34.120]  можете ленинным оператором воздействовать на не пойми
[12:34.120 --> 12:35.120]  что.
[12:35.120 --> 12:41.080]  Или так, например, то есть всегда ищите какие-то
[12:41.080 --> 12:43.840]  крайние условия, если вы не знаете, что делать.
[12:43.840 --> 12:46.320]  Не думайте, что раз я не знаю формул, значит все плохо,
[12:46.320 --> 12:50.000]  если это тест, его можно немного обойти.
[12:50.000 --> 12:53.280]  Тесты поэтому и не очень популярны у нас на экзамене,
[12:53.280 --> 12:54.680]  у нас именно умстный опрос.
[12:54.760 --> 12:55.760]  Там как бы можно доп.
[12:55.760 --> 12:56.760]  вопрос задать и все понятно.
[12:56.760 --> 12:59.440]  Так, ну тут вроде комментарии излишни, правильно?
[12:59.440 --> 13:01.400]  В лекции этого водилось, вывести это на бумажке
[13:01.400 --> 13:02.400]  надо 30 секунд.
[13:02.400 --> 13:07.880]  Я не знаю, что еще сказать.
[13:07.880 --> 13:09.760]  Вот вам, пожалуйста, распределение другое.
[13:09.760 --> 13:11.960]  Домножение всех значений признаков обучающей выборке
[13:11.960 --> 13:15.200]  на 10 приведет к игнорированию числительной ошибки.
[13:15.200 --> 13:17.920]  Изменению решения задачи минимизации МСЕ линейной
[13:17.920 --> 13:18.920]  модели.
[13:18.920 --> 13:23.920]  Да, нет, вот тут просто ответила 50% ровно 50 на
[13:23.920 --> 13:24.920]  50.
[13:24.920 --> 13:27.640]  Кто за то, что да?
[13:27.640 --> 13:28.640]  Кто за то, что нет?
[13:28.640 --> 13:32.480]  Кто насчет остальной половины аудитории?
[13:32.480 --> 13:33.480]  Вы сдержались?
[13:33.480 --> 13:38.680]  Ну давайте, кто-нибудь проинтерпретируйте, вот, кто за то, что да, почему.
[13:38.680 --> 13:47.440]  Нет, веса поменяются логично, раз вас на 10 домножили.
[13:47.440 --> 13:50.480]  Изменится предсказание, давайте так, хорошая поправка,
[13:50.480 --> 13:51.480]  вы правы, надо будет потом.
[13:52.040 --> 13:54.880]  То есть мы именно предсказание меняем, потому что, условно,
[13:54.880 --> 13:56.880]  домножили признаки на 10, логично, что веса должны
[13:56.880 --> 14:00.600]  на 10 упасть в 10 раз, потому что иначе смысла нет.
[14:00.600 --> 14:07.640]  Ну что, кто за то, что да?
[14:07.640 --> 14:09.360]  Все коэффициенты должны упасть 10 раз.
[14:09.360 --> 14:27.000]  Ну я ровно поэтому, я их спрашиваю, вот вы за то,
[14:27.000 --> 14:30.840]  что ничего не изменится, правильно?
[14:30.840 --> 14:33.840]  Кроме этого что-то изменится?
[14:33.840 --> 14:39.240]  Ну ребят, опять же, ребят, хорошо, вы правы, как бы,
[14:39.320 --> 14:43.120]  тесты, грубо говоря, они не идеальные, я их составлял,
[14:43.120 --> 14:45.120]  я был немножко обусловлен на свое состояние сознания,
[14:45.120 --> 14:47.120]  вот вы говорите, сейчас было не очень понятно, окей,
[14:47.120 --> 14:49.760]  в будущем поправим, сейчас давайте тогда именно разберемся
[14:49.760 --> 14:51.800]  концептуальном, а не в том, что вот тест неправильно
[14:51.800 --> 14:52.800]  составлен.
[14:52.800 --> 14:57.640]  Нет, таргет вообще не трогаем, все, смотрите, мы взяли
[14:57.640 --> 15:00.120]  только признаки, поделили в 10 раз, все.
[15:00.120 --> 15:02.680]  Логично, что у нас оптимальные коэффициенты стали в 10 раз
[15:02.680 --> 15:03.680]  больше, правильно?
[15:03.680 --> 15:07.600]  Ну или тут что, умножили на 10 домножение, тогда оптимальные
[15:07.600 --> 15:11.320]  коэффициенты стали в 10 раз меньше, верно?
[15:11.320 --> 15:14.240]  Кроме этого что-то поменяется?
[15:14.240 --> 15:17.240]  Проблем вроде нет, правильно?
[15:17.240 --> 15:21.520]  Оценка наших предсказаний не поменяется, с этим все
[15:21.520 --> 15:22.520]  согласны?
[15:22.520 --> 15:23.520]  Да?
[15:23.520 --> 15:24.520]  Нет?
[15:24.520 --> 15:28.560]  Супер, ну по сути так и есть, у нас вроде никаких проблем
[15:28.560 --> 15:31.600]  с этим быть не должно, тем более мы сказали, что игнорирую
[15:31.600 --> 15:33.520]  вычислительные ошибки, потому что на самом деле
[15:33.520 --> 15:37.440]  если вы домножите ваше значение параметров там на 10 восьмой,
[15:37.440 --> 15:39.560]  то у вас вычислительные ошибки появятся, причем
[15:39.560 --> 15:42.320]  может появиться достаточно здоровое, но просто потому,
[15:42.320 --> 15:45.040]  что у вас уже где-то там далеко, ну ладно, 10 восьмой
[15:45.040 --> 15:48.080]  еще может нормально, 10-25, у вас скорее всего пойдут
[15:48.080 --> 15:49.320]  в вычислительный характер проблемы.
[15:49.320 --> 15:54.440]  А если мы мое минимизируем, что-то поменяется?
[15:54.440 --> 16:04.840]  А почему у нас на 20% меньше ответов «да»?
[16:04.840 --> 16:11.360]  Ну вот, смотрите, абсолютно индиферентно какая у вас
[16:11.360 --> 16:14.200]  функция ошибки, правильно?
[16:14.200 --> 16:16.640]  Если вы решаете задачу, вы домножили, игнорируя
[16:16.640 --> 16:18.480]  вычислительные проблемы, домножили на 10, поделили
[16:18.480 --> 16:19.800]  на 10, абсолютно неважно.
[16:19.800 --> 16:22.800]  Все, хорошо.
[16:22.800 --> 16:26.520]  Мнение решения задачи минимизации МСЕ плюс Л2-регулизация,
[16:27.360 --> 16:32.360]  что тут можно сказать?
[16:32.360 --> 16:36.280]  Так, а тут говорят не поменяется.
[16:36.280 --> 16:56.080]  Ну-ка.
[16:56.080 --> 16:58.320]  Я на всякий случай уберу свободный член просто для
[16:58.320 --> 16:59.320]  удобства.
[16:59.320 --> 17:02.320]  Хорошо?
[17:02.320 --> 17:15.320]  Вот мы это умножаем на минимум, правильно?
[17:15.320 --> 17:16.320]  Все согласны?
[17:16.320 --> 17:17.320]  Устремляем к минимуму.
[17:17.320 --> 17:18.320]  Тут все согласны.
[17:18.320 --> 17:19.320]  Нигде опечатки нет.
[17:19.320 --> 17:22.560]  Ну на всякий случай это вот условно по какому-то
[17:22.560 --> 17:26.360]  там множеству иксов, я не знаю, так как это в матричной
[17:26.360 --> 17:29.320]  форме, так норма этого вектора, все, норма вектора остатка.
[17:29.320 --> 17:30.320]  Ошибка.
[17:30.320 --> 17:31.320]  Все понятно.
[17:31.320 --> 17:32.320]  Хорошо.
[17:32.320 --> 17:35.120]  Если мы с вами домножили все признаки на 10, у нас
[17:35.120 --> 17:38.920]  нормы вектора весов, каждый элемент стал в 10 раз меньше,
[17:38.920 --> 17:39.920]  правильно?
[17:39.920 --> 17:43.120]  То есть мы по сути можем сказать, что это у нас первый
[17:43.120 --> 17:48.080]  случай, здесь Омега-1, соответственно, во втором случае у нас будет
[17:48.080 --> 17:51.760]  х1, х2, Омега-2.
[17:51.760 --> 17:57.040]  Мы с вами знаем, что Омега-2 это получается 1 десятая,
[17:57.040 --> 17:58.040]  Омега-1.
[17:58.040 --> 17:59.040]  Верно?
[17:59.040 --> 18:00.040]  Что?
[18:00.040 --> 18:04.200]  Ну раз признаки домножили в 10 раз, значит Омега должна
[18:04.200 --> 18:05.200]  упасть 10 раз.
[18:05.200 --> 18:06.200]  Верно?
[18:06.200 --> 18:07.200]  Вот.
[18:07.200 --> 18:12.080]  Получается эта штука, минус y, y не менялся, 2 в квадрате,
[18:12.080 --> 18:17.280]  плюс лямбда, теперь делить на корень с 10, норма опять
[18:17.280 --> 18:26.760]  же, Омега-1, Омега-1, а, хорошо, да-да, вы правы.
[18:26.760 --> 18:42.760]  Так, Омега-2, 1 десятая, Омега-1, стоп, чего?
[18:42.760 --> 18:48.760]  Все правильно, не корень, да, на 100 в смысле поделить
[18:48.760 --> 18:49.760]  и не умножить.
[18:49.760 --> 18:55.960]  Все, все, все, я думаю, поделить на 100, да, норма Омега-1,
[18:55.960 --> 18:56.960]  квадрат.
[18:56.960 --> 18:59.040]  Ну а тут, соответственно, на самом деле это что?
[18:59.040 --> 19:02.640]  Это эквивалентно х1, Омега-1, все согласны?
[19:02.640 --> 19:08.640]  Ну так что, получится то же самое решение?
[19:08.640 --> 19:09.640]  Убедились?
[19:10.640 --> 19:16.640]  Если вы вот эту задачу минимизации решаете, у вас получается,
[19:16.640 --> 19:20.960]  что вот это, вот это, короче, эквивалентны ли эти две
[19:20.960 --> 19:22.960]  задачи минимизации?
[19:22.960 --> 19:25.640]  Решение одно и то же будет или нет?
[19:25.640 --> 19:26.640]  А?
[19:28.640 --> 19:29.640]  Чего подставить?
[19:32.640 --> 19:35.440]  Не, смотрите еще раз, вот это все мы минимизируем
[19:35.440 --> 19:39.160]  по Омеге, я на самом деле это записал, чтобы было
[19:39.160 --> 19:40.160]  понятно откуда переходы.
[19:40.160 --> 19:44.160]  Х2, Омега-2 это то же самое, что х1, Омега-1, правильно?
[19:44.160 --> 19:47.160]  Это наше предсказание.
[19:47.160 --> 19:50.160]  Х2 это где мы все умножили в 10 раз?
[19:56.160 --> 19:59.160]  Да, не совсем корректно, но тем не менее у нас
[19:59.160 --> 20:02.160]  коэффициент регуляризации-то упадет, поэтому решение
[20:02.160 --> 20:05.160]  получится другим, у нас оптимум съедет.
[20:05.160 --> 20:07.160]  Уловили?
[20:09.160 --> 20:14.160]  Оптимальное решение без регуляризации у нас было
[20:14.160 --> 20:16.160]  вот такое, правильно?
[20:16.160 --> 20:20.160]  То есть, если у нас х умножили в 10 раз по всем признакам,
[20:20.160 --> 20:25.160]  то по идее Омега должна просто упасть в 10 раз по
[20:25.160 --> 20:27.160]  всем признакам и получим ту же самую оценку.
[20:27.160 --> 20:30.160]  Но при этом у нас коэффициент регуляризации, по сути,
[20:30.160 --> 20:33.160]  стал 100 раз меньше, потому что все веса упали.
[20:33.160 --> 20:36.160]  Так что решение на самом деле оптимальное будет другое.
[20:36.160 --> 20:38.160]  Именно поэтому, помните, мы с вами когда говорили
[20:38.160 --> 20:41.160]  о первой регуляризации, что будьте очень осторожны
[20:41.160 --> 20:43.160]  с регуляризацией, потому что вам нормировка данных
[20:43.160 --> 20:45.160]  вообще играет роль.
[20:45.160 --> 20:47.160]  Это просто такой демо-случай.
[20:47.160 --> 20:49.160]  В общем случае, если у вас один признак сильно имеет
[20:49.160 --> 20:52.160]  больше шкалу, чем другие, у него сильно меньше вес,
[20:52.160 --> 20:55.160]  у него больше значимость в точке зрения регуляризации,
[20:55.160 --> 20:58.160]  потому что она его не трогает, а остальные занижает.
[20:58.160 --> 21:01.160]  Улавливаете?
[21:01.160 --> 21:04.160]  Так что понятно, что здесь у нас с ответами.
[21:04.160 --> 21:07.160]  Здесь большинство 68% ответили, что приведет
[21:07.160 --> 21:09.160]  к изменению решения.
[21:09.160 --> 21:12.160]  Логично, что случай L2 регуляризации изменится?
[21:12.160 --> 21:16.160]  Случай L1 что-то поменяется, нет?
[21:16.160 --> 21:19.160]  Ну коэффициент регуляризации здесь просто будет
[21:19.160 --> 21:23.160]  на 10, а не на 100, но все равно поменяется, верно?
[21:23.160 --> 21:26.160]  Все понятно? Разобрались?
[21:26.160 --> 21:28.160]  Супер.
[21:28.160 --> 21:32.160]  Чего-чего?
[21:32.160 --> 21:34.160]  Тести L1...
[21:36.160 --> 21:39.160]  А, не, это вторая норма, это просто Омега-1.
[21:43.160 --> 21:45.160]  Я ничего не понял.
[21:45.160 --> 21:48.160]  А, мое плюс, окей, не, это я просто в голове уже.
[21:48.160 --> 21:51.160]  Если L1 регуляризации тоже поменяется, правильно?
[21:51.160 --> 21:54.160]  Я просто подумал, что там написано МЦЕ плюс L1, там
[21:54.160 --> 21:56.160]  мое плюс L2, суть та же самая.
[21:56.160 --> 21:58.160]  У вас регуляризатор все равно станет другим.
[21:58.160 --> 22:00.160]  Все, разобрались.
[22:00.160 --> 22:02.160]  Супер.
[22:02.160 --> 22:04.160]  И этим дальше.
[22:04.160 --> 22:06.160]  После обучения на очень большой выборке линейная
[22:06.160 --> 22:08.160]  регрессия в режиме inference работает медленнее, чем
[22:08.160 --> 22:10.160]  кнн, быстрее, чем кнн, работает так же.
[22:10.160 --> 22:13.160]  Ну, абсолютно большинство говорит, что да, работает
[22:13.160 --> 22:16.160]  быстрее. Я надеюсь, это всем понятно?
[22:18.160 --> 22:21.160]  Линейная регрессия на одном объекте работает за
[22:21.160 --> 22:23.160]  линию, где за линию от количества признаков.
[22:23.160 --> 22:27.160]  Все, вам надо умножить веса на признаки и сложить.
[22:27.160 --> 22:31.160]  Кнн работает за квадрат, потому что вам нужно посчитать
[22:31.160 --> 22:33.160]  расстояние от этой точки до всех.
[22:33.160 --> 22:36.160]  Соответственно, у вас количество точек на количество
[22:36.160 --> 22:38.160]  признаков.
[22:38.160 --> 22:41.160]  Режим inference это режим применения.
[22:43.160 --> 22:45.160]  Обучение.
[22:45.160 --> 22:47.160]  Ну, это, скажем так, просто терминолог, то есть режим
[22:47.160 --> 22:49.160]  обучения. А?
[22:49.160 --> 22:51.160]  Ну, train inference, да, или train evaluation.
[22:51.160 --> 22:53.160]  Вот два варианта.
[22:53.160 --> 22:55.160]  Просто, а?
[22:55.160 --> 22:58.160]  Вот плохо слово тест, именно inference.
[22:58.160 --> 23:00.160]  Что такое infra-inference?
[23:00.160 --> 23:03.160]  Вот вы модель построили, и отдали ее заказчику, она
[23:03.160 --> 23:04.160]  теперь где-то крутится.
[23:04.160 --> 23:06.160]  Вот это model inference.
[23:06.160 --> 23:09.160]  У вас отдельно, на самом деле, кстати, 8 числа, по-моему,
[23:09.160 --> 23:12.160]  будет открытая лекция по MLEOps о том, как свои модельки
[23:12.160 --> 23:16.160]  гонять в около продакшн, скажем так, пока на коленке.
[23:16.160 --> 23:19.160]  Это мы как раз сейчас договорились с специалистом из
[23:19.160 --> 23:21.160]  Альфа-банк.
[23:22.160 --> 23:23.160]  Да.
[23:23.160 --> 23:26.160]  Не, это будет открытая лекция, потому в четверг в 17.05
[23:26.160 --> 23:29.160]  мы заранее объявление сделаем, просто я сегодня, скажем
[23:29.160 --> 23:32.160]  так, аудиторию заблоронировал, сразу вам об этом говорю.
[23:32.160 --> 23:34.160]  То есть у нас есть некоторые открытые лекции, я думаю,
[23:34.160 --> 23:36.160]  кто-то из вас подписан на журнальный клуб, там иногда
[23:36.160 --> 23:38.160]  будут какие-то выступления.
[23:38.160 --> 23:40.160]  На них можно прийти послушать, обычно не онлайн, потому
[23:40.160 --> 23:42.160]  что все это началось во время ковида.
[23:42.160 --> 23:44.160]  Но очные выступления мы тоже рады проводить.
[23:44.160 --> 23:47.160]  Так что вот проведем здесь, он расскажет что-то
[23:47.160 --> 23:49.160]  любопытное.
[23:50.160 --> 23:53.160]  Ну ссылочку тогда скину, грубо говоря, туда приходит
[23:53.160 --> 23:56.160]  как правило или какие-то люди извне, кто хотят просто
[23:56.160 --> 23:59.160]  рассказать о своих научеств семинар, говоря простым
[23:59.160 --> 24:00.160]  языком.
[24:00.160 --> 24:02.160]  То есть туда приходит кто-то рассказывать о чем-то
[24:02.160 --> 24:04.160]  любопытном, на тему правил машинного обучения или
[24:04.160 --> 24:05.160]  около.
[24:05.160 --> 24:08.160]  Например, вот недавно договорился с коллегой, познакомился
[24:08.160 --> 24:11.160]  с коллегой из Фиана, который занимается квантами
[24:11.160 --> 24:13.160]  вычислением, у них там квантовый компьютер на
[24:13.160 --> 24:15.160]  холодных запутанных ионах.
[24:15.160 --> 24:17.160]  Я очень плохо в этом деле разбираюсь, если что, поэтому
[24:17.160 --> 24:19.160]  если я где-то там ляпнул что-то не так, спросить
[24:19.160 --> 24:20.160]  пожалуйста.
[24:20.160 --> 24:22.160]  Вот, я надеюсь с ним тоже удастся договориться, он
[24:22.160 --> 24:25.160]  тоже придет и расскажет чем там можно в квантовых
[24:25.160 --> 24:27.160]  вычислениях позаниматься сейчас в России.
[24:27.160 --> 24:29.160]  Тема достаточно любопытная.
[24:29.160 --> 24:30.160]  Вот.
[24:30.160 --> 24:32.160]  Так, ну тут вроде все понятно.
[24:32.160 --> 24:34.160]  КНН сильно медленнее, ровно поэтому линейные модели
[24:34.160 --> 24:35.160]  очень любят на практике.
[24:35.160 --> 24:37.160]  Они линейные, они простые.
[24:37.160 --> 24:40.160]  Они работают вот-то вот на вот таких вот часах там
[24:40.160 --> 24:43.160]  или знаю на телефоне со скоростью в лед.
[24:43.160 --> 24:46.160]  А всякие сложные нейронки, хотя они сильно проще, чем
[24:46.160 --> 24:49.160]  КНН тоже на больших данных, работают чуть медленнее.
[24:49.160 --> 24:51.160]  Поэтому линейные модели себе вряд ли когда-либо
[24:51.160 --> 24:52.160]  и живут.
[24:52.160 --> 24:55.160]  Всегда будет нужна скорость.
[24:55.160 --> 24:57.160]  Так, L1-02 регулизация.
[24:57.160 --> 25:00.160]  Линейная регрессия работает с логистической регрессией.
[25:00.160 --> 25:02.160]  Каждый раз одно и то же.
[25:02.160 --> 25:04.160]  Коллеги, почему на логистическую регрессию на 20 человек
[25:04.160 --> 25:07.160]  меньше обращать внимание?
[25:07.160 --> 25:10.160]  В аудитории есть кто-то, кто за то, что линейная регрессия
[25:10.160 --> 25:13.160]  да, логистическая регрессия нет?
[25:13.160 --> 25:15.160]  То есть в аудитории все за оба, правильно?
[25:15.160 --> 25:19.160]  Ну понятно, или не сознаемся.
[25:19.160 --> 25:21.160]  Все нормально, так как ответа не видно кто.
[25:21.160 --> 25:24.160]  Я надеюсь, все понимают, что с точки зрения применимости регуляризации
[25:24.160 --> 25:27.160]  абсолютно индиферентная линейная регрессия, логистическая,
[25:27.160 --> 25:29.160]  там какой-нибудь SVM.
[25:29.160 --> 25:31.160]  Если у вас дифференцируемая функция потери, вы к ней
[25:31.160 --> 25:33.160]  можете любой регуляризатор прилепить и с ней работать.
[25:33.160 --> 25:35.160]  Применимо.
[25:35.160 --> 25:36.160]  КНН-ом?
[25:36.160 --> 25:39.160]  В КНН-е есть параметры?
[25:39.160 --> 25:41.160]  Это вообще не параметрический метод.
[25:41.160 --> 25:43.160]  У вас гиперпараметры есть, которые вы сами выбрали, а дальше он работает.
[25:43.160 --> 25:45.160]  Все.
[25:45.160 --> 25:47.160]  На Nn-байсе есть параметры?
[25:47.160 --> 25:49.160]  Не угадали.
[25:49.160 --> 25:51.160]  На Nn-байсе есть параметры?
[25:54.160 --> 25:57.160]  Да, например, если вы параметрическое, семейство
[25:57.160 --> 26:00.160]  распределений выбрали, нормально, у вас есть параметр распределения.
[26:00.160 --> 26:03.160]  Вы можете его сделать и не параметрическим, ну ваши
[26:03.160 --> 26:05.160]  место распределения можете это использовать.
[26:05.160 --> 26:06.160]  Ну какого-то априорного распределения, какого-то
[26:06.160 --> 26:08.160]  аналитической функции.
[26:08.160 --> 26:10.160]  Например, да, гистограмма построили, все.
[26:10.160 --> 26:14.200]  Все. Никаких параметров у вас нет, у вас просто выбручена функция распределения,
[26:14.200 --> 26:19.880]  тогда мед стал, по сути, никаких параметров он не имеет. Согласились? Поэтому там никой
[26:19.880 --> 26:33.480]  ль два регулизации, ну и смысла в ней как-то непонятна. А что мы там собрались регулизировать?
[26:33.480 --> 26:39.440]  То есть фактически он иногда имеет параметры, но что означает ограничение нормы
[26:39.440 --> 26:44.080]  вектора параметров, особенно с учётом того, что может быть много признаков у всех,
[26:44.080 --> 26:49.420]  разное распределение всех, разные параметры, чуть значить не понятно. То есть может быть в каком-то
[26:49.420 --> 26:52.880]  случае это будет полезно, если мы хотим там понить дисперсию, но это какой-то очень-очень
[26:52.880 --> 26:59.880]  узкий случай, явно здесь не оговорено. Так, логистическая регрессия. Стремится найти
[26:59.880 --> 27:05.080]  линейную гиперплоскость между двумя классами. Вопрос по русскому языку. Линейную гиперплоскость
[27:05.080 --> 27:18.080]  это не тавтология? Бывает нелинейная гиперплоскость? Ну да, согласен, но по сути если у нас гиперплоскость,
[27:18.080 --> 27:21.480]  то она вроде по определению линейная, иначе это уже гиперповерсность какая-то, там уже может быть
[27:21.480 --> 27:27.040]  что угодно. Ну так, на всякий случай. Линейную, короче, гиперповерсность становится гиперплоскость
[27:27.040 --> 27:39.040]  между точками. Всё правильно. 55%. Ну можно так сказать, она регрессирует вам вероятности,
[27:39.040 --> 27:46.880]  но это классификатор. Ну ровно для этого этот вопрос здесь сидит. Логистическая регрессия. Так,
[27:46.880 --> 27:54.640]  повторить чё такое логистическая регрессия? Кто за? Почему это плоскость? Вот сейчас повторим,
[27:54.640 --> 28:08.680]  почему это плоскость. Омега икс плюс б задает что? Можно камеру повернуть, пожалуйста? Что
[28:08.680 --> 28:16.240]  задает омега икс плюс б? Ну в общем случае гиперплоскость, правильно? Вот. Если у нас с вами
[28:16.240 --> 28:21.840]  есть какая-то гиперплоскость, в данном случае она будет, так как у меня доска двумерная,
[28:21.840 --> 28:31.400]  это будет вот прямая. Омега где? Что такое омега? Это нормально, это прямой, верно? Вот,
[28:31.400 --> 28:37.920]  допустим, вот это омега. Давайте опять же я скажу, что b равный нулю, поэтому эта штука проходит
[28:37.920 --> 28:48.240]  через начало координат, хорошо? Всё, договорились? Двумерный случай. Вот наша омега. Итак,
[28:48.240 --> 28:54.120]  если у нас классикатор с вами есть, то что мы с вами делаем по сути? Мы говорим, что сверху у нас
[28:54.120 --> 29:04.480]  точки вот белые, вот они тут сидят, а снизу точки зелёные, вот они вот тут сидят, окей? Вот ваш
[29:04.480 --> 29:09.000]  классикатор. И теперь мы говорим, хорошо, хочу предсказывать, что такое, где класс, где один,
[29:09.000 --> 29:16.080]  где другой. Как нам это понять? Ну, давайте тогда посмотрим на что. На сигнум вот этой штуки,
[29:16.080 --> 29:28.840]  можете тогда по знаку вот этой величины понять, в каком классе находится точка? Как понять? Ну вот,
[29:28.840 --> 29:35.320]  собственно, если выше, если больше нуля, значит у нас вектор на точку сонаправлен вектором нормальной
[29:35.320 --> 29:40.120]  плоскости, в скалярном произведении их положительное, значит оно с этой стороны. Если в скалярном
[29:40.120 --> 29:46.200]  произведении у них отрицательное, значит с другой стороны, верно? Но теперь мы понимаем, что это всё,
[29:46.200 --> 29:52.120]  конечно, замечательно, только нам как-то хочется не просто метку класса получить, а что-то более
[29:52.120 --> 29:56.040]  уверенное, потому что нам функция потери, подходящая нужда. Что мы говорим? Окей, давайте попробуем
[29:56.040 --> 30:01.920]  предсказывать не просто метку класса, а вероятность метки класса. Тогда мы говорим, что в каждой точке
[30:01.920 --> 30:09.080]  нашего пространства мы париметризуем какую случайную величину. Два исхода. У какой величины два
[30:09.080 --> 30:16.840]  исхода? Бернулевскую. Все помнят что такое бернулевская случайная величина? Кто не помнит?
[30:16.840 --> 30:21.960]  Бернулевская случайная величина, подбрасывание монетки, случайная величина с двумя исходами.
[30:21.960 --> 30:30.080]  Единственный параметр вероятности положительного исхода. Всё. С каждой точки пространства у вас
[30:30.080 --> 30:34.760]  может быть объект, правильно? Для каждого объекта вы хотите уметь предсказывать вероятность того,
[30:34.760 --> 30:38.800]  что это объект того или иного класса, правильно? То есть для любой точки мы говорим, хочу вот эту
[30:38.800 --> 30:50.040]  точку. Он нам говорит, вероятность того, что это зеленый, там 0.27, например. 0.72. Понятно, да? И
[30:50.040 --> 30:54.160]  остается только вопрос, как нам научиться эту вероятность предсказывать? Ну вот, собственно,
[30:54.160 --> 30:59.760]  я кидал ссылку на Quora, где вот такой вот матан вывода, почему сегмоида, исходя из экспоненциального
[30:59.760 --> 31:05.400]  семейства распределения и так далее. Мы говорим, если коротко, что смотрите, сегмоида, она умеет
[31:05.400 --> 31:14.400]  отображать R в 0.1. Замечательно. Вероятность у нас сидит в 0.1, R. Вот вам, пожалуйста, отображение из
[31:14.400 --> 31:21.240]  нашего признака пространства в R. Всё. Применили, получили? Поэтому она называется логистическая
[31:21.240 --> 31:25.960]  регрессия, она нам регрессирует вероятность. Но это метод классификации при этом. На этом часто в
[31:25.960 --> 31:31.400]  начале краски многие ловятся, ровно потому, что это же регрессия. По сути, регрессия классификации мало
[31:31.400 --> 31:35.680]  чем отличается. Регрессия, с одной стороны, проще, потому что у вас, как правило, одна переменная,
[31:35.680 --> 31:40.280]  если это скалярная регрессия. С другой стороны, классификация проще, потому что у вас не
[31:40.280 --> 31:45.640]  континуальное множество ответов, непрерывное какое-то множество, куда вы отображаетесь, а просто конечное
[31:45.640 --> 31:51.320]  число класса. Но с другой стороны, мы обычно предсказываем не метку класса, а вероятность метки
[31:51.320 --> 31:57.160]  класса, поэтому обычно вектор просто предсказываем и всё. Вектор вероятности. Или до Softmax просто
[31:57.160 --> 32:03.280]  вектор каких-то величин, которые Softmax отобразим вектор вероятности. Что такое Softmax, помните? Или
[32:03.280 --> 32:10.960]  написать на доске? Смотрите, у меня получилось теперь вот не одна такая плоскость, а три штуки.
[32:10.960 --> 32:19.680]  Вот первая условно, вот какая-нибудь вторая, вот какая-нибудь третья. И соответственно,
[32:19.680 --> 32:30.080]  у каждой из них есть какой-то вектор нормалия. Вот, здесь соответственно. Ну, у нас многоклассовая
[32:30.080 --> 32:34.000]  классификация, тогда у нас, допустим, на три класса пытаемся построить. Для каждого класса,
[32:34.000 --> 32:38.160]  по сути, строится свой классификатор, который говорит один против всех остальных. Поэтому у нас
[32:38.160 --> 32:43.240]  теперь для каждого объекта есть, по сути, что такое? Ну, вот это вот, скажем так, расстояние со
[32:43.240 --> 32:46.840]  знаком до этой гиперплоскости. Вот это скалярное произведение, до вот этой гиперплоскости, до вот
[32:46.840 --> 32:58.800]  этой и до вот этой. У вас получается какой-то вектор, виды там 4 и 2, 0 и 7 и 8 и 2. Ну, отсюда вроде
[32:58.800 --> 33:05.920]  с одной стороны понятно, кто ближе всех? Вот этот ближе всех гиперплоскост, правильно? Кто глубже всех
[33:05.920 --> 33:12.920]  в своем классе? Вот этот. Но как-то с вероятностями не очень. Поэтому мы с вами можем отсюда перейти
[33:12.920 --> 33:22.760]  к вероятностям. Каким образом? Ну, а? Это расстояние, ну вот, смотрите. Это краски омега 2 на х. Вот она.
[33:22.760 --> 33:34.000]  Ну, не омега 2, а омега 0. Это омега 1 на х, это омега 2 на х. Понятно? Супер. Мы отсюда можем перейти
[33:34.000 --> 33:39.120]  к вероятностям. Ну, для этого можно просто применить softmax, softmax функция вида E в степени х,
[33:39.120 --> 33:48.840]  делить на x и на сумму по g E в степени x g. То есть она к вектору применяется, получается вы все
[33:48.840 --> 33:57.600]  элементы засовываете в экспоненту и суммируете. Тогда у вас вектор перейдет во что-то вроде 0.2,
[33:57.600 --> 34:14.640]  0.01 и 0.79. Вот вам ваша вероятность. А? Суммируется. Ну, на формулу посмотрите. У вас в знаменателе
[34:14.640 --> 34:19.880]  всегда сидит сумма экспонентов по всем элементам вектора, а обчислители элементов вектора.
[34:19.880 --> 34:26.880]  По определению в единицу перейдет плюс экспонента, она всегда у вас что? Не отрицательная. Верно? Все
[34:26.880 --> 34:43.880]  работает. Так, разобрались? Попросы, комментарии, предложения? Да. Логистическая регрессия
[34:43.880 --> 34:50.720]  метапорных векторов отличается оптимизируемой функцией потерь. Ну да, у вас и там и там получается
[34:50.720 --> 34:56.600]  линейная плоскость, гиперплоскость у вас строится, но метапорных векторов вы минимизируете
[34:56.600 --> 35:07.960]  максимум из 0 и 1 минус margin плюс вторая норма. Вот это вы на минимум отправляете,
[35:07.960 --> 35:20.400]  а в логистической регрессии вы pi log pi сумма. В бинарном случае от 1 до 2 плюс соответственно
[35:20.400 --> 35:28.440]  какой-нибудь регуляризатор, в общем случае не обязательно. Второй отправляете на минимум. Вот,
[35:28.440 --> 35:34.000]  это просто два разных подхода, они выводятся из разных идей, хотя по факту и то и другое,
[35:34.000 --> 35:43.960]  линейный классикат просто функции потерь разные. Ну если записать по-другому, это получается
[35:43.960 --> 35:59.800]  pi log pi. Вы правы, здесь q, конечно. p и p1 log q плюс 1 минус p log 1 минус q. Ну это эквивалент,
[35:59.800 --> 36:05.760]  надеюсь вы понимаете. У вас два класса, тогда сумма вероятностей. Что такое? 100 процентов,
[36:06.760 --> 36:15.440]  то есть p1 на log q1 плюс p2 на log q2, то же самое, что 1 лог q1, 1 минус p1 на log 1 минус q1.
[36:15.440 --> 36:23.440]  Это предсказанная наша вероятность. Вот эта штука, это наша сегмоида от omega x плюс b.
[36:23.440 --> 36:34.160]  То есть это ваша истинная вероятность, а это предсказанная. Перефразирую, что это такое. Это
[36:34.160 --> 36:48.480]  мат ожидания логарифма q по вашей выборке. Согласились? Так, давайте по одному и по громче.
[36:48.480 --> 37:02.640]  Хатин, спасибо. Спасибо. А, pity приходит из нашего, из нашей выборки. Мы же с вами,
[37:02.640 --> 37:05.920]  это истинные вероятности, откуда они у нас берутся. Мы же мат ожидаем по выборке,
[37:05.920 --> 37:11.800]  правильно? pity это истинная выборка, это истинные метки класса, то есть если объект класса 1,
[37:11.800 --> 37:16.200]  вероятность 1 класса 100 процентов, если класс 2, вероятность 2 класса 100 процентов.
[37:16.200 --> 37:27.840]  Это кроссонтропия. Ну смотрите, нет, смотрите, давайте так. Если убрать минус, то вот эту штуку
[37:27.840 --> 37:32.600]  надо отправить на максимум, потому что это именно логарифм от ожидания логарифма по
[37:32.600 --> 37:38.440]  правдоподобию. Так как я написал на минимум, тут нужен минус и тут тоже нужен минус. Но это
[37:38.440 --> 37:42.080]  именно функция потерь, которую мы минимизируем, потому что мы привыкли обычно задачи минимизации
[37:42.080 --> 37:57.680]  решать. Вопросы еще? Интропия по определению с минусом. Минус p log p. Ну сумма по всем элементам.
[37:57.680 --> 38:05.480]  Как себя проверить? Интропия снизу ограничена чем? Нулем. Как бы невыраженный случай,
[38:05.480 --> 38:12.960]  интропия равна нулю. Если у вас p log p, то p всегда от 0 до 1, и логарифм числа от 0 до 1 какой?
[38:12.960 --> 38:23.960]  Отрицательный. Значит, нужен минус. Так, еще вопрос есть? По этому вопросу. Так, а, ну мы тут это с вами.
[38:23.960 --> 38:29.520]  Замереться найти нелинейную границу между двумя классами из-за сигмоидной функции. Но тут
[38:29.520 --> 38:36.640]  третья ответила да, я надеюсь, что понятно, что нет, правильно? В чем тонкость? Логистическая
[38:36.640 --> 38:41.840]  регрессия задает нелинейное отображение, потому что мы отображаемся из пространства признаков
[38:41.840 --> 38:49.000]  во вероятностное пространство. Отображение нелинейное, гиперплоскость линейная. Какой бы это в
[38:49.000 --> 38:54.280]  талоге не было. Почему я на это обращаю внимание? Мы с вами вот на прошлом занятии говорили, давайте
[38:54.280 --> 38:58.960]  еще раз. Посмотрим. Говорили про градиентный бустинг. Помните, мы с вами там говорили,
[38:58.960 --> 39:08.960]  что по факту у нас бустинг это ансамбль вида. Роит на фит от х сумма пои, правильно? Классический
[39:08.960 --> 39:13.360]  вопрос, я его уже задавал, но тем не менее. Имеет ли смысл в качестве фит использовать
[39:13.360 --> 39:22.040]  линейные регрессии? Чего? Ну, я сразу просто к этому перешел. Это то же самое, потому что если
[39:22.040 --> 39:26.600]  фит линейная регрессия, то это линейная комбинация линейных отображений, что есть линейное отображение.
[39:26.600 --> 39:40.400]  Если фит это логистическая регрессия, что? Ну, видимо-невидимо, а давайте подумаем, почему можно.
[39:40.400 --> 39:51.360]  Смотрите, если фит, вот линейная регрессия, вы сказали, смысл не имеет. Все, красный крест не
[39:51.360 --> 40:01.080]  работает. Я иду дальше. Если фит логистическая регрессия, ну, давай тогда перепишем. В каком-то
[40:01.080 --> 40:17.640]  виде, допустим, это получается сумма пои ρi, с одного там до t, с одного до t-1, ρi на стигмоида от
[40:17.640 --> 40:31.600]  ωi x плюс bi и плюс, соответственно, то? ρt стигмоида от ωt x bt. Что это, линейная комбинация линейных
[40:31.600 --> 40:38.240]  отображений? Нет. Так что вы, используя линейную комбинацию нелинейных отображений, можете построить
[40:38.240 --> 40:44.680]  более сложную решающую поверхность. Простой пример, как бы классический пример. Все, тут зеленая галка.
[40:44.840 --> 40:54.520]  Работает. Простой пример, вы с помощью логистической регрессии можете решить вот такой случай. Тут у вас
[40:54.520 --> 41:02.160]  сидят белые точкики, а тут у вас сидят зеленые точкики. Вы можете, на самом деле, построить себе
[41:02.160 --> 41:09.440]  множество классикаторов, которые как-то вот так вот это начнут делить, и тем не менее у вас получится
[41:09.440 --> 41:16.360]  отделить одно от другого. Задача линей не разрешимая, бустингом решать можно. Мы с помощью
[41:16.360 --> 41:25.960]  композиции слабых классикаторов выходим, по сути, за класс линейных оценок. Поняли? Так.
[41:25.960 --> 41:35.360]  Не обязательно. Нет, у вас нет ограничений. Это скорее, скажем так, евристическая просто
[41:35.360 --> 41:41.040]  ситуация, что обычно люди используют один тот же класс моделей, который бустит. Потому что,
[41:41.040 --> 41:46.040]  на самом деле, не суть важна, если вы используете простые деревья или там, ну, логистическая
[41:46.040 --> 41:49.800]  регрессия достаточно редко бустингу подвергается, но иногда подвергается. Знают два случая, когда она
[41:49.800 --> 41:55.000]  даже outperform-ила остальных. В смысле, по скорости и другим там параметрам. То есть, суть в чем?
[41:55.000 --> 42:00.360]  Обычно выбирается какой-то класс моделей, параметрически неважно, деревьев, что-нибудь,
[42:00.360 --> 42:06.760]  вы можете хоть КНН сюда запихнуть. И потом из них строится ансамбль. Обычно выбирают какие-то
[42:06.760 --> 42:12.040]  модели, которые достаточно быстро можно строить и инферить одновременно. КНН, например, не будет
[42:12.040 --> 42:18.560]  сюда ставить. Почему? Дорого. Он и так квадратичный, а тут еще будет N-моделей, где N-глубина ансамбля.
[42:18.560 --> 42:23.440]  По факту, вы можете на первом шаге построить градиентный бустинг. Первый шаг у вас будет
[42:23.440 --> 42:28.160]  линейная регрессия, логистическая регрессия, второй какой-нибудь дерево, третий какой-нибудь
[42:28.160 --> 42:35.080]  КНН, четвертый наивный байс. Вам никто не мешает. Только вопрос, а зачем? Если можно обосновать
[42:35.080 --> 42:41.640]  зачем, может попробовать. В правиловом смысле нет. Что делает бустинг? Давайте еще раз такой вопрос
[42:41.640 --> 42:50.560]  вам общий. Бустинг подбирает новые информативные признаки? Кто за то, что да? Кто за то, что нет?
[42:50.560 --> 42:53.600]  Где остальная часть аудитории?
[42:58.160 --> 43:03.800]  В каком-то смысле это отличная обработка исходных данных. Что вы имеете?
[43:03.800 --> 43:19.680]  Почти. Только вы сейчас не про бустинг говорите. У вас на входе во все эти модели что лежит?
[43:19.680 --> 43:26.200]  У вас исходная X на входе во все модели лежит, правильно? Так что градиентный бустинг у него не
[43:26.200 --> 43:34.480]  меняется. Признаковое пространство, над которым работает модель. Все модели отображают из X.
[43:34.480 --> 43:45.000]  То есть для любой И, это отображение из X. Это наш признаковое пространство. Но вот вопрос куда?
[43:45.000 --> 44:04.920]  Куда отображаем? В 0.1 вероятности нет. У нас целевое пространство меняется каждый раз.
[44:04.920 --> 44:12.160]  Что делает градиентный бустинг? Мы с вами с помощью модели пытаемся опроксимировать
[44:12.160 --> 44:30.920]  антиградиент. У нас есть наш DL от ансамбль F от X и Y и по DF от X и. Вот наш антиградиент в данной точке.
[44:30.920 --> 45:00.920]  Помните? Мы сказали, что вот это DRI. Вот оно. Даже не DRI, просто RIT. И теперь наш с вами FIT от X и Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от
[45:00.920 --> 45:30.920]  Y и по DF от Y и
[45:30.920 --> 46:00.920]  по DF от Y и
[46:00.920 --> 46:25.920]  по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
[46:25.920 --> 46:50.920]  по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от
[46:50.920 --> 47:15.920]  Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
[47:15.920 --> 47:45.920]  по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
[47:45.920 --> 48:15.920]  по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
[48:15.920 --> 48:45.920]  по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF
[48:45.920 --> 49:15.920]  от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
[49:15.920 --> 49:45.920]  по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
[49:45.920 --> 50:15.920]  по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и
[50:15.920 --> 50:24.200]  по DF от Y и по DF от Y и по Y и по DF от Y и по DF от Y и по DF от Y и по DF от Y и по DF с
[50:24.200 --> 50:34.400]  от нуля до двух симметричная п лог п симметричная функция
[50:34.400 --> 50:46.360]  п лог путочнее кажется несимметричная кажется там сидит логаритм
[50:46.360 --> 51:16.360]  в оооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооо
[51:16.360 --> 51:20.360]  ооооооооооооооооо-
[51:46.360 --> 51:50.400]  справа. Логарифма у нас убывает очень быстро, правильно?
[51:55.040 --> 51:59.840]  Так, а?
[51:59.840 --> 52:11.480]  Ладно, может проект сомнительный, я перегнул?
[52:18.560 --> 52:26.720]  Справедливо. Хорошо, ну короче, да, вы правы, спасибо. Короче, вот у вас-то
[52:26.720 --> 52:39.200]  функция. Вот. Вот у нас логистический лоз, верно? Так, а куда я делаю этот
[52:39.200 --> 52:50.440]  зеленый? Согласны, не согласны? Да, а сегмой уже там, в сегмой здесь идет маржин, по
[52:50.520 --> 53:05.160]  сути. Ну, омега икс наш. Ну, давайте банально посмотрим, если у нас объект сидит в
[53:05.160 --> 53:10.880]  глубине чужого класса, правильно? Мы используем градиентную оптимизацию, считаем антиградиент.
[53:11.880 --> 53:25.480]  Градиент здесь какой? Минус один. Да, нет. Градиент здесь какой? Наверное, будет больше,
[53:25.480 --> 53:30.480]  чем минус один, это у нас все-таки тангенс угла наклона, правильно? Производные сегмойды какая
[53:30.480 --> 53:38.800]  максимум? Минимум. Максимум. Максимум. Максимум. Нет. Ну, можете продиференцировать,
[53:38.800 --> 53:43.440]  производные сегмойды, на самом деле это сегмойда на один минус сегмойда, максимальное назначение 0,25.
[53:43.440 --> 53:53.040]  Или меньше? Короче, если эта штука в какой-то момент растет быстрее, чем 4х, то у вас будет
[53:53.040 --> 54:03.640]  больше влияние этого градиента на ваше предсказание. Окей? Всем окей? Все спокойны.
[54:09.680 --> 54:20.440]  Может быть. Окей, хорошо, справедливо, да. Хорошо, ладно, да, убедили, надо, видимо, как-то переформулировать
[54:20.440 --> 54:26.440]  вопрос. То есть логика именно в том, что у вас сам хинжелос на самом деле пострадает от выбросов
[54:26.440 --> 54:33.360]  больше, потому что он растет просто слева. Ой, господи, логлос слева от нуля растет быстрее. Вот. Я на самом
[54:33.360 --> 54:36.760]  деле именно на это пытался обратить внимание, вы правы, надо чуть-чуть вопрос переформулировать,
[54:36.760 --> 54:45.460]  потому что там еще сегмойда вмешивается. Да, я к этому, я именно, вопрос был про то,
[54:45.460 --> 54:49.080]  какую лоз на самом деле страдает больше, просто вот так я его описал. На самом деле, вы правы,
[54:49.080 --> 54:54.160]  надо чуть-чуть переформулировать вопрос, иначе можно с этими производными запутаться. Окей?
[54:54.160 --> 55:08.080]  Вопросы еще здесь есть? Это логлос, да. Иван? Ну простите мне мой подчерк.
[55:08.080 --> 55:30.520]  Давайте. Кто? А что такое семейство сегмойд?
[55:38.080 --> 55:46.840]  А, ну чисто теоретически да, но сегмойда конкретно это вот функция ровно 1 на 1 плюс e в степени минус x.
[55:46.840 --> 55:56.440]  Но вообще, да, вы правы, вы ее можете там сделать более пологой, менее пологой и так далее. Не-не-не.
[55:56.440 --> 56:01.920]  Смотрите, почему мы этот параметр не берем? Это, кстати, хороший вопрос. Почему смысла растягивать
[56:01.920 --> 56:19.520]  туда-сюда сегмойда особо нет? Смотрите, сегмойда вот линейно туда-сюда растягивает. Да, но я к чему
[56:19.520 --> 56:24.960]  говорю, у вас под сегмойдой сидит обычно ωx плюс b, правильно? У вас ровно ω и отвечает за то,
[56:24.960 --> 56:32.560]  как вы туда-сюда растягиваете все. У вас же аргумент меняется как раз. Так что то, что вы меняете,
[56:32.560 --> 56:40.640]  то же самое меняется, когда вы параметры меняете. Так, ладно, едем дальше. Выберите
[56:40.640 --> 56:53.920]  корректную функцию hinge-loss, но только что написали максимум 0,1 margin. А? Архтангенс?
[56:53.920 --> 57:06.320]  Может геверболический тангенс? Архтангенс я ни разу не встречал. Более того, я не очень помню,
[57:06.320 --> 57:22.080]  какие у него области значений. Ну да, конечно, это архангенс. Что? Четкую сегмойда? Или задайте
[57:22.080 --> 57:33.760]  вопрос или погрубче. Функция 1 делить на 1 плюс e в степени минус x. А, вы имеете в виду,
[57:33.760 --> 57:43.200]  что можно еще какую-то функцию задать такую же? Или что? Которая отображает r в 0,1? Да,
[57:43.200 --> 57:47.760]  можно кучу функций задать. Почему именно сегмойда? Вот, я говорю, можем попытаться с вами поднять в
[57:47.760 --> 57:55.120]  опен, открыть вору, прочитать большой талмут об этом. То есть, я, к сожалению, сейчас в ходу даже
[57:55.120 --> 57:59.760]  не готов на самом деле этот вывод воспроизвести. Скорее, я вот к лектору по статам тярверу отправлю.
[57:59.760 --> 58:05.040]  Вот, лектор по статам тярверу должен это делать, наверное, среди ночи и проснувшись. Я так делать
[58:05.040 --> 58:11.120]  не умею, я с этим выводом сам два часа один раз просидел, вывел, но по памяти я вряд ли
[58:11.120 --> 58:16.800]  его воспроизвезу, скажу честно. Не, опять же, если есть такой запрос, вы об этом скажите, я могу
[58:16.800 --> 58:21.000]  в этом разобраться, и мы с вами это еще раз проделаем. Но за ненадобностью я за полгода петь забуду.
[58:21.000 --> 58:32.960]  Так, тут вопрос еще есть? Нет? Нет? Нет? Хорошо. Так, едем дальше. Что общего у Писей и Кэннон?
[58:32.960 --> 58:38.400]  Обожаю этот вопрос. Оба являются алгоритмами обучения и безучителя. 15 процентов. Успех.
[58:38.400 --> 58:56.600]  Оба являются особыми формами регуляризации. 2,2 процента. Классно. Хорошо, два мисклика. Они
[58:56.600 --> 59:01.680]  оба требуют нормировки данных, если вы не знаете, что в конкретном случае это не так. Лоично,
[59:01.680 --> 59:07.680]  потому что если у вас нет нормировки, в Кэнноне вы считаете квадратичную вторую норму, помолча…
[59:07.680 --> 59:15.760]  окей, не втопросите. Которую, скорее всего, от шкала зависит. Вот. В Писей вы считаете
[59:15.760 --> 59:21.560]  норму Фрабениуса, которая очень похожа на Евклидову норму, согласны? По каждому объекту, по отдельности.
[59:21.560 --> 59:27.240]  Поэтому тоже зависит от шкала, если отнормируете, у вас все станет по-другому. Они оба позволяют
[59:27.240 --> 59:31.960]  уменьшить размерность набора данных и упростить вычисление. Желтый ответ, логично правильный,
[59:31.960 --> 59:37.160]  про снижение размерности данных. В принципе, я могу сам себе сделать такую подколку, что с
[59:37.160 --> 59:41.920]  помощью любой модели, которая решает задачи обучения с учителем, можно стичить размерность.
[59:41.920 --> 59:49.840]  Например, как? Вы вместо объекта можете выдавать какую-нибудь статистику по его соседям. Ну,
[59:49.840 --> 59:56.320]  например, распределение вероятности класса по его соседям. Вот вам, пожалуйста, было много признаков,
[59:56.800 --> 01:00:03.720]  то есть чисто технически можно. На практике такое я видел не знаю когда, поэтому это скорее такой,
[01:00:03.720 --> 01:00:10.560]  знаете, именно вот подколка, что на самом деле можно, но смысла вроде в этом особо нет. Да? Кого?
[01:00:10.560 --> 01:00:20.080]  А, смотрите, у вас есть какая-то модель, правильно? Она отображает, допустим, объект в ответ, верно?
[01:00:20.080 --> 01:00:25.920]  Ну, например, дерево. Оно вам умеет объект описывать в терминах вероятности меток классов.
[01:00:25.920 --> 01:00:31.560]  Верно? Для каждого объекта, допустим, у вас там было 100-500 признаков, теперь вы для него, знаете,
[01:00:31.560 --> 01:00:37.240]  вектор вероятности на три класса. Но чисто технически, вот вы, теперь умеете тремя чиселками
[01:00:37.240 --> 01:00:42.000]  описывать каждый объект. Чисто формально, вот вам снижение размерности. С КНН-ом то же самое,
[01:00:42.000 --> 01:00:53.320]  но это именно что, знаете, такой надо копаться. Ответ. Первая и вторая главные, обожаю этот ответ,
[01:00:53.320 --> 01:01:00.080]  первая и вторая главные компоненты артагональны. В любом случае 49,5, только если исходные признаки
[01:01:00.080 --> 01:01:08.120]  независимы, 46,2, прям круто. Ну что, признавайтесь, кто за то, что они в любом случае независимы? А?
[01:01:08.120 --> 01:01:20.920]  А, люди не поняли, что это такое? Хорошо. Главные компоненты есть, нет главных компонент. ПЦА это да,
[01:01:20.920 --> 01:01:27.200]  нет главных компонент. Principle component analysis, метод главных компонент по-русски. Первые,
[01:01:27.200 --> 01:01:37.600]  вторые главные компоненты артагональны. Вопрос всегда или нет? Кто за то, что всегда? Нет,
[01:01:37.600 --> 01:01:59.720]  это компоненты. Лекция номер четыре. Так, я понял, хорошо. Справедливо, в одномерном пространстве
[01:01:59.720 --> 01:02:06.920]  вообще одна главная компонента принято. Размерность не меньше двух. Хорошо, коллеги,
[01:02:06.920 --> 01:02:18.520]  давайте так. Кто такое собственные векторы, кто помнит? Что насчет остальных? Хорошо, классно.
[01:02:18.520 --> 01:02:25.320]  Давайте, у нас есть с вами положительно определенная матрица какая-то, квадратная. У нее собственные
[01:02:25.320 --> 01:02:42.000]  векторы какие? Есть, классно. Так, а если она еще и симметричная? Вот чего вы говорите. Собственные
[01:02:42.000 --> 01:02:49.920]  значения какие? Действительные. А собственные векторы? Ребят, если у вас такая матрица,
[01:02:49.920 --> 01:02:56.520]  то вы можете ее перевести в базис из собственных векторов, а она примет какой вид? Если в каком-то
[01:02:56.520 --> 01:03:03.720]  базисе она имеет диагональный вид, что значит? Что все векторы базисные артагональны друг другу.
[01:03:03.720 --> 01:03:10.720]  Согласны? То, что у вас не на диагонали стоит как раз и их скалярное произведение. Если они все равны
[01:03:10.720 --> 01:03:16.920]  нулю, значит все что? Артагональны. Главные компоненты артагональны по определению. Потому
[01:03:17.000 --> 01:03:20.720]  что мы с вами берем коверационную матрицу XTX, которая положительно определенная и симметричная,
[01:03:20.720 --> 01:03:43.240]  и строим ее разложение. Прием. Смена базиса приводит к изменению скалярного произведения.
[01:03:43.720 --> 01:03:49.640]  Переопределению, в смысле функцию другой выбрать. Что такое скалярное произведение?
[01:03:49.640 --> 01:03:56.200]  Скалярное произведение это отображение из декартового произведения наше векторовое
[01:03:56.200 --> 01:04:02.280]  пространство на самого себя в R. Согласны? Классно. Если мы его задали с вами каким-то образом,
[01:04:02.280 --> 01:04:07.480]  например, просто вот дот-продукт классический по элементной перемножении и сумма, вас от смены
[01:04:07.480 --> 01:04:20.320]  базиса чуть меняется, то у вас функция так и задается. Вам не надо чтобы... Любое линейное
[01:04:20.320 --> 01:04:23.760]  отображение это поворот, растяжение, сжатие. У вас свойства артагональны не меняются.
[01:04:23.760 --> 01:04:33.760]  Как-то не были. Были. Мы просто в их базис перешли.
[01:04:33.760 --> 01:04:41.920]  X, XTX. Но чтобы она точно была положительно определенная, квадратно и симметрично.
[01:04:41.920 --> 01:04:54.640]  Да, именно. А так как у нас СВД, метод главных компонентов, он именно что применяется,
[01:04:54.640 --> 01:05:01.760]  по сути мы расложение строим вот этой матрицы XTX. Поэтому главные компоненты у нас всегда там,
[01:05:01.760 --> 01:05:06.360]  во-первых, есть, во-вторых, они артагональны друг к другу. За исключением того случая, когда у нас
[01:05:06.360 --> 01:05:11.080]  на самом деле эффективно размер пространства сильно ниже. Ну словно если вы плоскость запихнете
[01:05:11.080 --> 01:05:15.680]  в пятимерное пространство, там будет две главные компоненты, остальные будут просто нулевые. Их нет.
[01:05:15.680 --> 01:05:25.680]  У них будут нулевые собственные значения. Согласны? Так, давайте так. Сейчас вот задаю
[01:05:25.680 --> 01:05:30.100]  вопрос в аудиторию. Тут что-то непонятно, потому что сейчас я начинаю теряться, что именно непонятно
[01:05:30.100 --> 01:05:42.620]  еще надо разобрать. Смотрите, собственно, векторы, по-моему, вообще всегда артагональны или как бы
[01:05:42.620 --> 01:05:58.420]  их нет? Нет? Хорошо, да, согласен. Согласен. Нет, да, на самом деле это правильное.
[01:06:00.100 --> 01:06:04.800]  Вот, да, но мы всегда можем... Вот, вы правы, это, кстати, тоже хороший вопрос. Я имел в виду невыраженные
[01:06:04.800 --> 01:06:09.220]  случаи, но на всякий случай. В каком случае у нас вот эти главные компоненты или, собственно, векторы
[01:06:09.220 --> 01:06:13.720]  определены неоднозначно? Не с точностью до знака вообще неоднозначно. Вот вам типичный вопрос
[01:06:13.720 --> 01:06:35.080]  собеседования. Ага, да, например. Может пример такой выборке привезти? Ну, я вас понял, но зрительно
[01:06:35.080 --> 01:06:41.080]  проще, я думаю, людям воспринять вот так. Вот, у вас точки ровно на единичной окружности лежат.
[01:06:41.080 --> 01:06:47.320]  Какие у вас здесь главные векторы? Фу, главные компоненты. Любая пара артагональных векторов,
[01:06:47.320 --> 01:06:54.920]  вот вам две компоненты, в которых у вас все раскладывается. Нет, ну, смотрите. Хорошо,
[01:06:54.920 --> 01:07:00.600]  по определению главные компоненты это направление наибольшей дисперсии последовательно,
[01:07:00.600 --> 01:07:06.200]  которые артагональны друг к другу. Так что главные компоненты у вас всегда будут артагональны друг
[01:07:06.200 --> 01:07:12.000]  к другу, если они есть. Почему? Потому что вы, когда проводите первую главную компоненту,
[01:07:12.000 --> 01:07:17.400]  вы на нее проецируете все ваши точки и говорите, вдоль этого направления вся дисперсия объяснена.
[01:07:17.400 --> 01:07:22.640]  Поэтому вторая главная компонента имеет смысл только в артагональном направлении, в других
[01:07:22.640 --> 01:07:27.920]  направлениях вся дисперсия уже известна. То есть, по сути, теперь, когда вы вот на это расстояние
[01:07:27.920 --> 01:07:32.240]  все спроецируете, у вас точки будут только как распределены. У вас будет распределение точек
[01:07:32.280 --> 01:07:37.920]  вдоль вот этого направления. Во всех остальных направлениях дисперсии уже нет. Все, смысл нет.
[01:07:37.920 --> 01:07:45.240]  Там проводить главную компоненту. Мне кажется, вообще потерялись. Особенно вот левая часть аудитории,
[01:07:45.240 --> 01:07:51.040]  там уже тишина, все куда-то в телефон шли. Сложно, непонятно? Сложно вырубай.
[01:08:02.240 --> 01:08:13.640]  Так, короче, что всем рекомендую сделать. Есть книжка, которая одна из рекомендованных.
[01:08:13.640 --> 01:08:21.520]  Называется Deep Learning Book. В ней есть вводная глава, введение она называется. Там нет никакого
[01:08:21.520 --> 01:08:27.160]  диплернига, там введение. Вот там есть кусок Полиналу. Там 25 страниц примерно. Бога ради,
[01:08:27.160 --> 01:08:34.480]  прочитайте. Причем всем абсолютно полезно будет, даже если вы докой себя считаете, в Линале,
[01:08:34.480 --> 01:08:39.840]  в Тиарвере, в Мотанее, в Машинки во всем сразу. Скажем так, я ее читал с удовольствием. Причем
[01:08:39.840 --> 01:08:47.000]  год назад перечитывал. Просто написанный классный пример. И там все будет понятно. Можете читать,
[01:08:47.000 --> 01:08:50.880]  вот сегодня у нас никакого нового материала нет, вот вам по сути домашним. Прочитайте эту главу.
[01:08:50.880 --> 01:08:59.560]  Прям прочитать. Хорошо? Нет, главу Полиналу, конкретное изведение, там страница 25. Я не помню,
[01:08:59.560 --> 01:09:05.480]  примерно сколько, может 40, но короче, за пару часов читается достаточно легко. За часа четыре,
[01:09:05.480 --> 01:09:10.720]  если вы еще руками воспроизводите те выводы, которые там есть, всякие выводы. То есть, кроме как
[01:09:10.720 --> 01:09:14.760]  это проделать или разобраться где-то по учебнику, хоть беклимишеву, хоть кого угодно, у вас другого
[01:09:14.760 --> 01:09:20.840]  варианта нет. То есть, надо разобраться, это один из вариантов как. Он не единственный, но работает. Так,
[01:09:20.840 --> 01:09:27.120]  хорошо. Что у нас там дальше? Логистические регрессии или 1 или 2? Кажется, мы куда-то уехали.
[01:09:27.120 --> 01:09:34.220]  Первая, вторая, главный компонент. Короче, для того, чтобы у вас хотя бы что-то из этого вопроса
[01:09:34.220 --> 01:09:39.240]  стало. Что такое главный компонент? Главный компоненты, вот по определению, это направление
[01:09:39.240 --> 01:09:45.600]  наибольшей дисперсии в ваших данных. Всегда. И вы раскладываетесь по направлению наибольшей дисперсии.
[01:09:45.600 --> 01:09:51.560]  По сути, вы выставляете базу сначала в направлении наибольшей дисперсии раз, вдоль него вы все объяснили.
[01:09:51.560 --> 01:09:57.080]  Логично, что вдоль этого направления у вас дисперсии больше никакой нет, правильно? Логично, что вдоль
[01:09:57.080 --> 01:10:02.640]  любого неортагонального этому вектору направлению у вас компоненты дисперсии, вот кое смысл по крайней
[01:10:02.640 --> 01:10:08.400]  мере, будет нулевой, потому что здесь дисперсии никакой. Поэтому каждое следующее направление наибольшей
[01:10:08.400 --> 01:10:13.640]  дисперсии будет по определению ортагонально всем предыдущим. Потому что иначе оно им не ортагонально и
[01:10:13.640 --> 01:10:21.040]  просто поворотом на какой-то угол вы избавляетесь от вот этой вот космоса на ноль составляющей. Так что
[01:10:21.040 --> 01:10:26.840]  главный компоненты ортагонально по определению. И второй вопрос для самопроверки. Он был на начале
[01:10:26.840 --> 01:10:31.600]  прошлого занятия, там еще так забавно. Лекция обрезана в самом начале, надо будет, наверное, потом
[01:10:31.600 --> 01:10:37.640]  поправить. Там начинается с фразы коллегии, звучит как Олег. Я сначала подвис, какой Олег и почему
[01:10:37.640 --> 01:10:45.080]  лекция начинается со слова Олег. Это были коллеги. Итак, у вас выборка двумерная, например, ну или в
[01:10:45.080 --> 01:10:50.680]  общем случае н-мерная, все отнормированное, все замечательно. Мы с вами взяли и построили
[01:10:50.680 --> 01:10:56.520]  продолжение по главным компонентам и перешли теперь в базис из главных компонентов. Была
[01:10:56.520 --> 01:11:05.160]  двумерная выборка, взяли две главных компоненты и в них описали наши данные. Внимание, вопрос.
[01:11:05.160 --> 01:11:11.880]  У нас матрица X исходная и итоговая одинаковая или нет? То есть, говоря по-другому, у нас есть X
[01:11:11.880 --> 01:11:20.040]  оригинальный, у нас есть PCA. Мы PCA обучили на X оригинальном и применили к X оригинальному,
[01:11:20.040 --> 01:11:32.880]  получили X transformed. Это у нас краски PCA. Прям вот из калерновский вам интерфейс. Fit transform,
[01:11:32.880 --> 01:11:41.640]  господи, как длинно. X ориг. И причем число компонент равно размерности пространства.
[01:11:41.640 --> 01:11:55.880]  Верно ли, что вот это равно друг другу? Кто за то, что да? А в общем случае почему?
[01:11:55.880 --> 01:12:04.440]  Бинго. В общем случае у вас информация не потеряется, а матрица будет другая.
[01:12:04.440 --> 01:12:14.120]  Простей еще пример. Вот вам тот же самый эллипс, вот у вас исходная X1, X2. Где тут главные компоненты
[01:12:14.120 --> 01:12:22.800]  будут? Вот так. В новых компонентах эллипс у вас внезапно ряжет на бок. У вас информация не
[01:12:22.800 --> 01:12:26.680]  потеряется, вы все еще знаете все, что было изначально. Но вы повернули, по сути,
[01:12:26.680 --> 01:12:41.720]  ваше пространство. Так что эти матрицы не совпадают. Да. Но у вас отображение, по сути,
[01:12:41.720 --> 01:12:48.120]  артагональное на матрицу из значений, которые сжатие и растяжение вдоль каждой из главных
[01:12:48.120 --> 01:12:56.120]  компонентов. А потом опять артагонально. Вот. То есть если вы оставили, ну короче,
[01:12:56.120 --> 01:13:01.000]  сигму у вас осталось, то да вы просто повернули как-то. У вас по сути байлист стал теперь другой.
[01:13:01.000 --> 01:13:20.680]  Вот. Так, тут вопросы еще есть? Где не было? Так. Да, сингулярное разложение делаем матрице X,
[01:13:20.680 --> 01:13:29.360]  правильно? А это что такое? Мы можем с вами сделать, мы его делаем с вами через что? По сути,
[01:13:29.360 --> 01:13:35.280]  через eigen decomposition, как это разложение по собственным векторам по-русски. Матрица краски
[01:13:35.280 --> 01:13:53.440]  xtx. Берем и делаем разложение по собственным векторам матрицы xtx. Все. Конец. Потому что мы
[01:13:53.440 --> 01:13:58.440]  хотим разложить матрицу, которая гарантированно имеет собственные вектора, байс собственных
[01:13:58.440 --> 01:14:02.920]  векторов. Симметричную, не отрицательно определенную или положительно определенную. Слушайте,
[01:14:02.920 --> 01:14:10.360]  я уже не понял. Она может быть выраженной, поэтому должна быть не отрицательно определенная,
[01:14:10.360 --> 01:14:18.960]  по идее. Так, ладно, если компонент не более двух, понятно. Решающее дерево задача регрессии.
[01:14:18.960 --> 01:14:26.640]  Предсказывает только константное значение в каждом листе. О, вот это, кстати, дурацкий вопрос в
[01:14:26.720 --> 01:14:31.680]  плане, что, опять же, тут должны были быть все варианты ответа доступны, а не один из всех.
[01:14:31.680 --> 01:14:40.440]  Так, давайте так. У вас силы еще есть, потому что ваши скучающие лица меня начинают... А? Чего?
[01:14:40.440 --> 01:14:49.680]  Сложно? Два часа сложно. Ну, давайте тогда еще десять минут на оставшиеся вопросы и пойдем,
[01:14:49.680 --> 01:14:54.640]  больше вас сегодня ничем грузить не буду. Идет? Давайте. Тут, как раз, интересный вопрос пошли.
[01:14:54.640 --> 01:14:58.960]  Во-первых, ну, дерево предсказывает только константное значение в каждом листе. Согласны?
[01:14:58.960 --> 01:15:04.720]  Согласны. На всякий случай есть куча различных способов придумать более хорошие модели,
[01:15:04.720 --> 01:15:09.160]  и в том числе были статьи на тему того, что давайте в лист дерева запихнем линейную модель,
[01:15:09.160 --> 01:15:13.880]  и там у нас будет не константа, а линейная модель. Такое бывает, я надеюсь, что вы понимаете сейчас,
[01:15:13.880 --> 01:15:19.400]  что вы в листе можете на самом деле любую оценку делать, не обязательно константную. Верно? У вас
[01:15:19.400 --> 01:15:24.680]  в листе просто лежит подвыборка. Какую статистику вы по подвыборке посчитаете, или какую модель вы
[01:15:24.680 --> 01:15:29.600]  натянете на эту подвыборку, от вас зависит. Но так как обычно используют достаточно простые модели
[01:15:29.600 --> 01:15:36.440]  просто в ансамбле, используют обычно просто деревья, где константы лежат. Договорились? Понятно? Супер.
[01:15:36.440 --> 01:15:43.000]  Вот вопросы два и три. Ну ладно, третий, решающий деревень используется для решения задачи регрессии,
[01:15:43.000 --> 01:15:50.000]  тут опять 12 процентов издеваются, видимо. Мисклик. Я надеюсь очень сильно. Или это шутка на тему того,
[01:15:50.000 --> 01:15:56.000]  что не оценивается, значит напишем что угодно. Ну или если нет, то, пожалуйста, разбирайтесь,
[01:15:56.000 --> 01:16:00.520]  что я могу сказать. Давайте с вопросом номер два разберемся, ответом номер два, который красненький,
[01:16:00.520 --> 01:16:07.160]  разберемся и поймем, что имел в виду автор, то есть привет я. Марик, на самом деле вопрос
[01:16:07.160 --> 01:16:13.640]  достаточно важный. Почему? Потому что линейные модели и деревья не вроде как идут в начале, там еще у нас есть
[01:16:13.640 --> 01:16:19.640]  кнн, но с кнн попроще, там просто расстояние считается, там есть наивный байс, с ним тоже попроще, обычно там плотность,
[01:16:19.640 --> 01:16:28.760]  чем дальше от моды мы находимся, чем она меньше. А вот с деревом не очень понятно. Смотрите, что у нас
[01:16:28.760 --> 01:16:33.840]  делается линейная модель? Линейная модель, по сути, у нас выцепляет линейный тренд из наших данных, согласны?
[01:16:33.840 --> 01:16:40.080]  То есть она примерно пытается хотя бы линейную оценку сделать нашей зависимости. Если у нас есть
[01:16:40.080 --> 01:16:45.200]  какие-то квадратичные зависимости или более высокого порядка, мы их никаким образом не уловим. Почему я говорю
[01:16:45.200 --> 01:16:49.600]  про более высокий порядок? Все помнят Ряд Тейлора, правильно? Вот у нас там есть линейный компонент,
[01:16:49.600 --> 01:16:54.800]  квадратичный, третий, четвертый, пятый, так далее. Мы, по сути, с вами первый компонент пытаемся вытащить. Не обязательно, на самом
[01:16:54.800 --> 01:16:59.560]  деле мы с вами можем учитывать и остальные, поэтому она как-то повернется, но тем не менее. Проблем в чем?
[01:16:59.560 --> 01:17:05.840]  Проблема в том, что дерево не этим занимается. И с деревом нужно быть аккуратным в каком смысле.
[01:17:05.840 --> 01:17:12.760]  Представьте себе, что у нас вот точки как-то вот так вот были накиданы, вот они вот здесь, а пространство у
[01:17:12.760 --> 01:17:18.600]  нас все целиком такое. Давайте камеру повернем. Пространство у нас большое, вот оно прям широкое,
[01:17:18.600 --> 01:17:25.040]  и дерево умеет хорошо различать наши точки вот здесь, где-то. Оно все вот эти разделяющие
[01:17:25.040 --> 01:17:32.960]  гиперплоскости понастроит вот тут. Вот выпуклая оболочка наших точек, тех зелененьких. Вот здесь она тоже
[01:17:32.960 --> 01:17:37.680]  будет делать какое-то предсказание, потому что здесь вот эта гиперплоска, например, последняя, вот, вот так,
[01:17:37.680 --> 01:17:43.560]  вот так, типа это все еще к какому-то листу относится. Но при этом здесь дерево на самом деле
[01:17:43.560 --> 01:17:47.440]  предсказывает какой-то бред, потому что оно просто не рассчитано на экстраполяцию за предел вот
[01:17:47.440 --> 01:17:54.400]  этой самой выпуклой оболочки. Оно там никогда ничего не видело. Логично? Линейная модель, она хотя бы
[01:17:54.400 --> 01:18:00.400]  линейно экстраполировать умеет. Вот вам тоже простой пример, вот у вас какая-нибудь штуковина.
[01:18:00.400 --> 01:18:15.480]  Вот. Ну, короче, типа шумные данные. Вот у нас это ось X, это ось Y. Линейная модель у нас каким-то
[01:18:15.640 --> 01:18:21.840]  образом построит что-нибудь вот такое. И даже далеко куда-нибудь вон туда она все равно будет хотя бы
[01:18:21.840 --> 01:18:28.400]  линейный член, то что оно куда-то растет, линейный тренд имеет учитывать. Правильно? Дерево построит
[01:18:28.400 --> 01:18:41.320]  что-нибудь вот такое. У кого ошибка здесь будет больше? Учитывайте, что дерево не умеет в экстраполяцию
[01:18:41.320 --> 01:18:46.320]  за границы вашей имена обучающей выбраки выпуклой оболочки именно с точки зрения вашего
[01:18:46.320 --> 01:18:58.080]  признакового пространства. Можно? Тогда будет. Линейный тренд, да. И смотрите, моя цель ровно в том,
[01:18:58.080 --> 01:19:02.240]  чтобы вы это начали понимать на самом деле. Потому что то, что кто-то когда-то сказал, что дерево
[01:19:02.240 --> 01:19:07.760]  ведет себя так, классно, честь и хвала автору, но при этом дерево это просто отображение. Это просто
[01:19:07.760 --> 01:19:12.240]  способ поделить пространство на куски. Каждому куску соответствует под выбраку, каждый под
[01:19:12.240 --> 01:19:17.160]  выбраке может соответствовать не только константной модели, любая другая. Другое дело, что сложную модель
[01:19:17.160 --> 01:19:23.040]  туда запихивать дорого, вам дорого будет обучать. Но они сложные, может быть и полезны. По крайней
[01:19:23.040 --> 01:19:31.320]  мере, линейные оценки, правда, в листьях пытаются делать. Ну, у вас в каждом листе, по сути так,
[01:19:31.320 --> 01:19:36.120]  какая-то линейная модель задается. Ну, в регрессе, например, да, у вас будет что-нибудь такое,
[01:19:36.120 --> 01:19:41.680]  тут вот так, тут как-нибудь вот так, тут вот так и так далее. Но это в таком случае. Если это
[01:19:41.680 --> 01:19:46.200]  многомерно, это уже сложнее себе представить. По сути, у вас в каждом месте будет что-то, причем на
[01:19:46.200 --> 01:19:49.600]  самом деле не обязательно, вот, обращу внимание, не обязательно, чтобы она у вас была вообще
[01:19:49.600 --> 01:20:00.040]  непрерывная в таком случае. У вас абсолютно спокойно могут быть скачки. Не-не, краски здесь,
[01:20:00.040 --> 01:20:05.440]  просто решающее дерево. Именно поэтому я ответство донсуну, то, что дерево само по себе, вот,
[01:20:05.440 --> 01:20:09.160]  просто классическое, которое мы с вами разбирали, оно в экстраполяцию за пределы этой выборки не
[01:20:09.160 --> 01:20:14.680]  умеет. Просто чтобы это понимали. Это не всегда происходит. И вот опять же вопрос для самопроверки.
[01:20:14.680 --> 01:20:18.680]  Он немножко выходит за границей того, что мы обсуждали, но полезно. У нас временных рядов пока
[01:20:18.680 --> 01:20:23.440]  вообще не было. Вот представьте себе, временной ряд, да, данные зависит от времени, биржа какая-нибудь,
[01:20:23.440 --> 01:20:29.320]  или там не знаю. Количество студентов на лекции, как правило, экспедиционально бывает. Ну,
[01:20:29.400 --> 01:20:41.920]  с кодом семестр, да. Ну как, оно, оно ведет себя как-то вот так. И потом примерно насыщается. Ну да. Ну
[01:20:41.920 --> 01:20:50.360]  вот, оно, грубо говоря, здесь у нас какая-то асинтета есть, которая нормально. Чего? Ну,
[01:20:50.360 --> 01:20:58.040]  это оценка на глаз, но хотите, могу проверить. На самом деле, у меня была такая идея, это, скажем так,
[01:20:58.040 --> 01:21:02.840]  пафосная вещь, но пафосная в плане, абсолютно бесполезная, но пафосная. Сделать как раз какой-то
[01:21:02.840 --> 01:21:08.680]  сайт-проект. У меня валяется пара один, пока живой, Jetson Nano. На него можно прицепить камеру,
[01:21:08.680 --> 01:21:13.240]  ну и сделать штуковину, просто написать, набросать софт, который считает количество людей в аудитории.
[01:21:13.240 --> 01:21:19.320]  Это, я не знаю там, дело полчаса. Вот, сначала следующего семестра поставить в элекционную аудиторию,
[01:21:19.320 --> 01:21:30.920]  заставить его считать любопытно. Я вам больше скажу, к концу следующего
[01:21:30.920 --> 01:21:34.880]  семестра, ну, я думаю, вы и так про это знаете, мы вам расскажем, как вы можете просто надеть
[01:21:34.880 --> 01:21:40.960]  специальную футболку и парсеру станет вообще плохо, он найдет на изображении панду и, я не знаю,
[01:21:40.960 --> 01:21:50.400]  и кого-нибудь еще. Да, и Майкла Джексона. А, вот это нечто, это вы про этот мемасик,
[01:21:50.400 --> 01:21:55.400]  где распознавалка, короче. Была просто фотография, знаете, вот на камерах есть распознавание лиц и
[01:21:55.400 --> 01:22:01.520]  так далее, да. А кто-то написал софт, понятно делал это. Я надеюсь, что это мем, где штука по лицу
[01:22:01.520 --> 01:22:05.640]  примерно оценивает возраст. И там сидит автор довольно такой, его в вебке снимает, пишет там
[01:22:05.640 --> 01:22:12.160]  примерно 27 лет и заднего в темном проеме двери еще какое-то лицо, она находит примерно 300 лет.
[01:22:12.160 --> 01:22:23.120]  Короче, кот смотрит не просто в пустоту, вы поняли. Ладно, короче, с этим вопросом понятно,
[01:22:23.120 --> 01:22:27.720]  почему я говорю про временные ряды. Вы можете подумать, что значит для прогнозирования временных
[01:22:27.720 --> 01:22:35.720]  рядов деревья подходят плохо. Может прийти такая мысль, правильно? Неправильно. Почему? Потому
[01:22:35.720 --> 01:22:40.520]  что зависит от того, в каком признаковом пространстве вы живете. Обычно временные ряды не в признаках оси
[01:22:40.520 --> 01:22:45.560]  времени рассматривают. Там, как правило, переходят каким-нибудь дельтом за предыдущие значения,
[01:22:45.560 --> 01:22:51.280]  например, значение 10 тиков назад, 100 тиков назад и так далее. Поэтому вполне возможно, что ваши все
[01:22:51.280 --> 01:22:56.520]  новые данные находятся внутри выпуклой оболочки все еще. Поэтому не стоит где решающий деревьев
[01:22:56.520 --> 01:23:01.600]  кидывать, зависит от того, каких признаков они живут. Просто вот такие уже тонкие вопросы,
[01:23:01.600 --> 01:23:07.800]  о них сначала не задумываетесь обычно. Ну может вы задумывались, но средним нет. Ладно, мы почти все.
[01:23:07.800 --> 01:23:15.200]  В случае наличия пропусков решающие деревья что? Продолжают работать. Честь им их вала, они просто
[01:23:15.200 --> 01:23:19.280]  идут в левое правое под дерево, потом усредняем с теми весами, которые были на обучении. Все,
[01:23:19.280 --> 01:23:24.160]  тут вроде комментарии излишни. Они не способны автоматически заполнить пропуски, они просто
[01:23:24.160 --> 01:23:29.760]  делают оценку без этого знания. Причем, опять же, упражнение для самопроверки. Кажется,
[01:23:29.760 --> 01:23:35.960]  я обещал вас уже отпустить. Сейчас отпущу. Упражнение для самопроверки. Что делает решающий дерево
[01:23:35.960 --> 01:23:44.760]  на самом деле? Оно оценивает вам, по сути, выбороченное распределение, правильно? При условии
[01:23:44.760 --> 01:23:50.480]  того, что тот или иной признак больше или меньше чему-то, правильно? То есть, по сути, у вас дерево
[01:23:50.480 --> 01:23:57.840]  считает вам какое-то вот такое распределение при условии, допустим, х итой больше 5. Ну,
[01:23:57.840 --> 01:24:04.000]  х итой, давайте верхний индекс, что это именно признак? Верно? По сути, когда вы усредняете левое
[01:24:04.000 --> 01:24:08.880]  и правое под дерево, что у вас происходит? Вы просто-напросто выберете распределение при
[01:24:08.880 --> 01:24:15.240]  условии х итой больше 5, при условии х итой меньше или равен 5 и их усредняете между собой. По сути,
[01:24:15.240 --> 01:24:19.840]  вы от этого условия просто избавляетесь. То есть, вы получаете безусловно на это значение, на
[01:24:19.840 --> 01:24:29.320]  этот признак распределения, вот все. Понятно, непонятно я сейчас сказал? Хорошо. В отличие от
[01:24:29.320 --> 01:24:33.880]  линейных моделей, способность делать предсказания даже при наличии пропусков данных, да, не должны
[01:24:33.880 --> 01:24:41.280]  применяться, должны. Формула энтропии. Ну, формула энтропии мы с вами вроде уже писали. Минус сумма по
[01:24:41.280 --> 01:24:54.200]  И, П лог П. Ну П это и лог П. Нет, ну зеленый как бы зеленый ОК. Да, минус тоже потерял, это уже мой
[01:24:54.200 --> 01:25:04.760]  косяк. Да, синий и красный, но тут как бы я не знаю, что сказать. Я надеюсь, что ошибок дальше не будет.
[01:25:04.760 --> 01:25:11.800]  Так, и вроде почти все. В выборку добавили большое количество признаков, скоррелированных с одним из
[01:25:11.800 --> 01:25:17.160]  уже существующих. Классический вопрос, на экзамене кому-то из восточных падет. Существенно
[01:25:17.160 --> 01:25:21.880]  повлияет на процесс построения ансамбля типа boosting из деревьев решений, никак не повлияет на
[01:25:21.880 --> 01:25:26.600]  процесс построения ансамбля типа boosting из деревьев решений. Но игнорирую вычислительную
[01:25:26.600 --> 01:25:31.640]  сложность большинство за то, что никак не повлияет. Обосновать можете в двух словах?
[01:25:31.640 --> 01:25:52.800]  В boosting есть под выборки? Именно, если у вас просто boosting, то одному дереву по барабану игнорирую
[01:25:52.800 --> 01:25:58.680]  вычислительную сложность. Ансамбль типа boosting. У вас есть bagging и есть boosting,
[01:25:58.680 --> 01:26:04.240]  это две разные вещи. Bagging будет плохо, на самом деле bagging опять же все равно.
[01:26:04.240 --> 01:26:09.640]  Bagging со случайными подпространствами будет плохо, boosting и дифферентно. Одному дереву
[01:26:09.640 --> 01:26:18.280]  и дифферентно, boosting деревьев тоже и дифферентно. Согласны? Одному дереву почему и дифферентно?
[01:26:18.280 --> 01:26:24.400]  Оно выбирает оптимальный признак. Если у вас признак, который был продубилирован оптимальный,
[01:26:24.400 --> 01:26:30.400]  выберет будет выбран любой из них, правильно? Разбиение сохранится на под выборке. Если
[01:26:30.400 --> 01:26:42.440]  добавленный признак не оптимальный, он просто не будет выбран и все. Нет, boosting это не bootstrap,
[01:26:42.440 --> 01:26:48.840]  а не bagging. Boosting это то, что мы с вами на прошлой лекции разбирали. Хорошо? Так, тут понятно,
[01:26:48.840 --> 01:26:59.600]  едем дальше. Дерево работает, дерево каждый признак рассматривает по отдельности и просто
[01:26:59.600 --> 01:27:04.360]  жадным образом выбирает оптимальный. Поэтому если у вас там куча скоррелированных, оно просто
[01:27:04.360 --> 01:27:09.040]  выберет один из оптимальных, если их несколько, и продолжит работать. Ему все равно на их
[01:27:09.040 --> 01:27:15.440]  взаимодействие. Boosting каждое дерево строит по отдельности после других, поэтому это сохраняется.
[01:27:15.440 --> 01:27:20.880]  При построении random forest, там случайное подпространство признаковое, там может быть
[01:27:20.880 --> 01:27:24.480]  проблема, потому что в него может попасть только этот признак, например, тогда дерево только на
[01:27:24.480 --> 01:27:32.120]  этом признаке. Это бесполезное дерево. Boosting над линейными регрессиями обсуждали. Смысл особо
[01:27:32.120 --> 01:27:37.280]  не имеет, похоже на половину пирога какой-то. Позволяет получить нелинейную разделяющую
[01:27:37.280 --> 01:27:46.440]  поверхность над линейными регрессиями. Нет. Позволяет избежать переобучения. Нет. Не имеет смысла. Да.
[01:27:46.440 --> 01:27:53.800]  Градиенты Boosting требуют от функций потерь. Быть дифференцируемой, быть ограниченной, быть
[01:27:53.800 --> 01:28:00.080]  гладкой. Так, ну ограниченная вроде нам ни к чему, более того у нас куча квадратичных функций потерь.
[01:28:00.080 --> 01:28:07.760]  Ограничена? Нет. Уже не работает. Быть гладкой. Вообще по бравон на самом деле нам в каждой точке
[01:28:07.760 --> 01:28:13.880]  нужна производная и всё. Нам больше ничего не надо. Так что дифференцируемая должна быть. Гладкой? Нет.
[01:28:13.880 --> 01:28:21.080]  Ну и всё, вопрос кончились. Я вас поздравляю, вы достаточно уверенно и отлично ответили на
[01:28:21.080 --> 01:28:26.000]  вопрос. Я надеюсь сегодня вам удалось разобраться каким-то нетривиальным моментом. В этом и была
[01:28:26.000 --> 01:28:32.640]  суть данного занятия. Спасибо за внимание. Домашка прочитать голову про линейную алгебру.

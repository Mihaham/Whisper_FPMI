[00:00.000 --> 00:10.800]  Сегодня у нас лекция про метод Ньютона, первая часть,
[00:10.800 --> 00:15.680]  и потом успеем поговорим про Юнтонский метод.
[00:15.680 --> 00:18.200]  Мы по-прежнему пока что живем в мире, где есть только
[00:18.200 --> 00:20.320]  безусловные задачи, в следующей неделе будем смотреть
[00:20.320 --> 00:24.600]  на условные задачи, и таким образом там еще 2-3, скорее
[00:24.600 --> 00:26.920]  наверное 3, чем 2, лекции.
[00:26.920 --> 00:28.960]  Мы посвятим тому, что происходит, когда у вас есть какие-то
[00:28.960 --> 00:31.640]  ограничения, выбирать метод в зависимости от вида
[00:31.640 --> 00:37.400]  этих ограничений, и что есть с промежуточного между
[00:37.400 --> 00:40.640]  солверами общего вида, которые решают предвольные, выпуклые,
[00:40.640 --> 00:42.880]  ну не предвольные, задачи выпуклого, оптимизации
[00:42.880 --> 00:47.800]  в конической форме с некоторыми предопределенными конусами,
[00:47.800 --> 00:54.480]  и некоторые выпуклые задачи, у которых структура ограничений
[00:54.480 --> 00:57.600]  чуть проще, чем что-то совсем произвольное.
[00:57.600 --> 01:01.200]  Пока живем в таком мире, почему нам будет важно
[01:01.200 --> 01:04.720]  посмотреть на метод Ньютона именно сейчас, и почему он
[01:04.720 --> 01:08.240]  еще важен для решения задач общего вида, мы тоже,
[01:08.240 --> 01:10.360]  я думаю, через неделю, через две.
[01:10.360 --> 01:13.040]  На следующей лекции будет метод проекции градиента,
[01:13.040 --> 01:16.000]  проэксимальный метод, метод Франко Вульф, также известный
[01:16.000 --> 01:20.000]  как метод солвого градиента, и соответственно на следующей
[01:20.000 --> 01:22.800]  очередь, неделю, уже я надеюсь поговорить про методы внутренней
[01:22.800 --> 01:26.440]  точки, и то, как они работают в контексте именно задач
[01:26.520 --> 01:29.240]  общего вида с коническими ограничениями.
[01:29.240 --> 01:32.600]  Ну и последнее будет про солверы, пакеты, то, как это
[01:32.600 --> 01:36.200]  все реализовано в практических каких-то штуках.
[01:36.200 --> 01:38.320]  Итак, метод Ньютона, что происходит?
[01:38.320 --> 01:43.840]  Все у нас атерационно, x0 переходит в x1, переходит в x2.
[01:43.840 --> 01:46.600]  Правила этих переходов мы будем строить, основываясь
[01:46.600 --> 01:49.520]  на том, что мы вместо исходной задачи минимизируем вот
[01:49.520 --> 01:52.520]  такую квадратичную оппроксимацию исходной функции.
[01:52.520 --> 01:54.320]  Я бы даже сказал вот такую.
[01:54.400 --> 01:59.000]  Плюс одна вторая, транспонированная, гессиан появляется на h.
[01:59.000 --> 02:00.000]  Вот минимум по h.
[02:00.000 --> 02:02.000]  Вот таким образом мы получаем наше направление.
[02:02.000 --> 02:06.080]  Эта функция является выпуклой, и условие, что гессиан у
[02:06.080 --> 02:10.040]  нас положительно определен, понятно ли этот момент,
[02:10.040 --> 02:11.040]  или нужно пояснить.
[02:11.040 --> 02:12.640]  Если понятно, поставьте, пожалуйста, в чате плюс.
[02:12.640 --> 02:15.520]  Один вижу, второй, все вот это установили, прекрасно.
[02:15.520 --> 02:19.360]  Ну и соответственно условие первого порядка на поиск
[02:19.360 --> 02:22.560]  минимума будет записываться вот так, просто градиент
[02:22.560 --> 02:26.120]  в нашей точке h будет ноль, и поэтому надо будет наше
[02:26.120 --> 02:29.120]  направление h со звездочкой, это решение линейной системы.
[02:29.120 --> 02:30.120]  Вот.
[02:30.120 --> 02:33.840]  Важно понимать, что условно говоря, вызов функции, которые
[02:33.840 --> 02:36.880]  решает линейную систему, не тождественен вызову
[02:36.880 --> 02:39.920]  функции вида, ну давайте, раз еще я начал Питоновской
[02:39.920 --> 02:40.920]  нодации писать.
[02:40.920 --> 02:45.280]  Сейчас, секунду, вот так, умножить на там минус, да,
[02:45.280 --> 02:46.280]  какой-то.
[02:46.280 --> 02:47.280]  Минус f' на x.
[02:47.280 --> 02:50.120]  Вот это разные вещи с точки зрения численноустойчивости
[02:50.120 --> 02:52.400]  и алгоритмов, которые используются.
[02:52.400 --> 02:55.840]  Поэтому вот то, что обведено желтеньким, оно предпочтительнее.
[02:55.840 --> 03:00.320]  Вот это лучше, чем то, что вычисляется через NP-Linal
[03:00.320 --> 03:01.320]  Inf.
[03:01.320 --> 03:03.240]  Вот, потому что вычисление обратной матрицы, в принципе,
[03:03.240 --> 03:05.720]  очень устойчивая операция с точки, ну вот, в арифметике
[03:05.720 --> 03:06.720]  с плавающей точкой.
[03:06.720 --> 03:09.160]  Вот, поэтому, в общем, так делать не надо.
[03:09.160 --> 03:10.160]  Good.
[03:10.160 --> 03:12.080]  Значит, в итоге наш метод будет записываться таким
[03:12.080 --> 03:13.080]  вот образом.
[03:13.080 --> 03:17.480]  xкт-, я напишу сейчас аналитическое решение, но все, что касается
[03:17.480 --> 03:20.320]  его численно реализации уже выше было сказано.
[03:20.320 --> 03:21.560]  Это называется мет Ньютона.
[03:21.640 --> 03:24.080]  То есть понятно, что его можно применять, только
[03:24.080 --> 03:26.200]  когда вы знаете, что у вас в любой точке есть гисян,
[03:26.200 --> 03:28.120]  потому что иначе будет что-то странное.
[03:28.120 --> 03:29.120]  Вот.
[03:29.120 --> 03:30.880]  Значит, что про него можно сказать?
[03:30.880 --> 03:34.720]  Ну, уже глядя на эту штуку, свойства какие.
[03:34.720 --> 03:37.760]  Первое, то требует N квадрат памяти.
[03:37.760 --> 03:41.360]  Второе, требует N куб времени на итерацию.
[03:41.360 --> 03:43.720]  При этом мы помним, что все методы, которые оперировали
[03:43.720 --> 03:46.320]  только градиентами, они были тому от N.
[03:46.320 --> 03:48.760]  На одну итерацию просто надо было сложить что-то.
[03:48.760 --> 03:51.160]  Сложить какие-то, ну, линейные комбинаты векторов какие-нибудь,
[03:51.560 --> 03:54.400]  тут, наверное, очень кривые цифры, ничего не понятно.
[03:54.400 --> 03:57.640]  Это N квадрат, потому что хранение гисяна.
[03:57.640 --> 04:01.560]  То есть видно, что в целом метод гораздо более ресурсно
[04:01.560 --> 04:05.560]  затратен, нежели метод первого порядка, с которым мы обсуждали ранее.
[04:05.560 --> 04:08.240]  Понятно ли утверждение, понятно ли, почему так происходит?
[04:08.240 --> 04:09.240]  Можно ли идти дальше?
[04:09.240 --> 04:09.760]  Вполне.
[04:09.760 --> 04:10.560]  Прекрасно.
[04:10.560 --> 04:11.920]  Вот.
[04:11.920 --> 04:15.640]  Значит, раз мы усложнили себе жизнь на каждой итерации,
[04:15.640 --> 04:17.880]  то, наверное, хочется что-то получить взамен.
[04:18.720 --> 04:22.600]  И вот взамен мы получаем некоторые более продвинутые резорты по сходимости.
[04:22.600 --> 04:24.000]  Сходимость, как обычно.
[04:24.000 --> 04:24.960]  Тут будет два пункта.
[04:24.960 --> 04:28.200]  Первый пункт будет нам говорить, что в целом она сверхлинейная,
[04:29.200 --> 04:32.160]  а второй пункт будет говорить, что если мы там еще кое-что докрутим,
[04:32.160 --> 04:33.160]  то она станет квадратичной.
[04:33.160 --> 04:34.280]  Вот.
[04:34.280 --> 04:35.400]  Так, сейчас.
[04:35.400 --> 04:36.200]  Бла-бла-бла.
[04:36.200 --> 04:37.360]  Наверное, так не надо делать.
[04:37.360 --> 04:38.360]  Секунду.
[04:39.760 --> 04:44.800]  Что можно сказать про эту самую сверхлинейную сходимость?
[04:44.800 --> 04:47.000]  Вот, ну, давайте я так сделаю.
[04:47.760 --> 04:49.880]  Пусть у нас х-звездочка – это локальный минимум.
[04:49.880 --> 04:50.400]  Вот.
[04:50.400 --> 04:53.120]  Ну, мы знаем тогда, что в нем там градиент ноль,
[04:53.120 --> 04:56.120]  и гессиан, ну, у нас, видимо, положить-то придет.
[04:56.120 --> 04:56.360]  Вот.
[04:56.360 --> 05:00.680]  Можем вокруг нуля градиента представить его,
[05:00.680 --> 05:03.680]  разложить рядом Тейлора относительно некоторые точки хк.
[05:03.680 --> 05:04.600]  Ну да.
[05:04.600 --> 05:06.240]  Собственно, гессиан на что?
[05:06.240 --> 05:08.000]  На х-звездочка минус хк.
[05:08.000 --> 05:09.480]  Плюс или мало от нормы.
[05:09.480 --> 05:09.680]  Вот.
[05:09.680 --> 05:10.720]  Ну, в общем, понятное.
[05:10.720 --> 05:11.720]  Понятное преобразование.
[05:11.720 --> 05:12.840]  То есть, мы находим точки хк.
[05:12.840 --> 05:15.040]  Хотим смотреть, что у нас будет в х-звездочке.
[05:15.040 --> 05:16.440]  Мы знаем значение градиента.
[05:17.360 --> 05:19.360]  Равенство точное, потому что есть добавочные.
[05:19.360 --> 05:21.080]  Добавочная слагаемая умалая.
[05:21.080 --> 05:21.960]  Вот.
[05:21.960 --> 05:24.920]  Восответственно, что у нас из этого будет следовать?
[05:24.920 --> 05:26.040]  Можем это все умножить.
[05:26.040 --> 05:27.880]  Поскольку у нас вот это вот все выполнено,
[05:27.880 --> 05:31.200]  то мы можем обе части умножить на обратный гессиан формально.
[05:31.200 --> 05:35.360]  Ну, тут все объекты, которые у нас тут используются,
[05:35.360 --> 05:36.320]  они существуют.
[05:36.320 --> 05:38.280]  Поэтому, в общем, можно с ними спокойно оперировать.
[05:38.280 --> 05:39.440]  Вот.
[05:39.440 --> 05:42.040]  Давайте я подробно попытаюсь расписать.
[05:42.040 --> 05:43.680]  f' хк.
[05:43.680 --> 05:46.520]  Тут будет плюс х-звездочка минус хк.
[05:46.520 --> 05:49.440]  То же самое малое, потому что норма обратного гессиана
[05:49.440 --> 05:51.080]  также будет ограничена, поскольку, ну,
[05:51.080 --> 05:52.960]  может быть, она будет ограничена какой-то большой константой
[05:52.960 --> 05:54.960]  в случае каких-то проблем с обусловленностью.
[05:54.960 --> 05:59.120]  Но, тем не менее, ничего выраженного мы тут не получим.
[05:59.120 --> 06:00.040]  Вот.
[06:00.040 --> 06:00.840]  Что мы видим?
[06:00.840 --> 06:04.000]  Мы видим, что у нас есть хк.
[06:04.000 --> 06:06.840]  И выражение, которое очень напоминает то направление,
[06:06.840 --> 06:10.400]  которое у нас фигурировало в процессе обновления
[06:10.400 --> 06:12.480]  нашего хк в методе Ньютона,
[06:12.480 --> 06:14.760]  в котором мы получили вот буквально только что.
[06:14.760 --> 06:15.680]  Вот это выражение.
[06:15.680 --> 06:16.680]  Видно?
[06:16.680 --> 06:17.480]  Видна указка, да?
[06:17.480 --> 06:18.000]  Я надеюсь.
[06:18.000 --> 06:18.440]  Да, видно.
[06:18.440 --> 06:19.600]  Да, все прекрасно.
[06:19.600 --> 06:20.880]  Вот.
[06:20.880 --> 06:22.840]  Что это нам, в общем-то, дает?
[06:22.840 --> 06:24.600]  Дает то, что мы можем переупорядочить
[06:24.600 --> 06:26.440]  все наши замечательные выкладки.
[06:26.440 --> 06:28.600]  Именно сказать, что хк минус,
[06:28.600 --> 06:34.720]  ну, собственно, направление f'-1 хк f' хк.
[06:34.720 --> 06:35.120]  Вот.
[06:35.120 --> 06:37.560]  Ну и, собственно, из этого еще вычесть х со звездочкой.
[06:37.560 --> 06:38.480]  Вот.
[06:38.480 --> 06:40.360]  И сказать, что...
[06:40.360 --> 06:41.600]  Да, а это у нас ноль.
[06:41.640 --> 06:44.080]  Мы пропускаем этот этап и просто сравним с нолью.
[06:44.080 --> 06:44.360]  Вот.
[06:44.360 --> 06:48.040]  И эта штука у нас умалая от хк-х со звездочкой.
[06:48.040 --> 06:49.840]  Ну, это, по сути дела, что получается?
[06:49.840 --> 06:53.200]  Что хк плюс 1 минус х со звездочкой
[06:53.200 --> 06:56.960]  это умалая от нормы хк минус х со звездочкой.
[06:56.960 --> 06:57.400]  Вот.
[06:57.400 --> 07:00.200]  Ну и значит, что если мы теперь перейдем
[07:00.200 --> 07:02.680]  к пределу приказ стремящегося к бесконечности
[07:02.680 --> 07:07.040]  между нормами хк плюс 1 минус х со звездочкой
[07:07.040 --> 07:10.800]  норме хк минус х со звездочкой, то
[07:10.800 --> 07:12.360]  это же, по сути дела, что будет?
[07:12.360 --> 07:14.760]  Это будет предело отношения между умалым.
[07:14.760 --> 07:15.120]  Вот.
[07:15.120 --> 07:18.160]  И тем аргументом, который стоит под умалым.
[07:18.160 --> 07:20.640]  Что ж такое, хочется написать, минус в нижнем регистре.
[07:20.640 --> 07:21.960]  А это ноль по определению.
[07:21.960 --> 07:23.560]  И значит, сходимость будет сверхлинийная.
[07:23.560 --> 07:24.880]  Потому что если бы это было
[07:24.880 --> 07:26.320]  число как какой-то меньший единиц,
[07:26.320 --> 07:27.840]  то это было означало, что
[07:27.840 --> 07:29.600]  сходимость линейная и мы каждый раз
[07:29.600 --> 07:31.160]  хк раз уменьшаем эту самую норму.
[07:31.160 --> 07:33.080]  А поскольку мы уменьшаем быстрее, чем
[07:33.080 --> 07:35.440]  чем в константное число раз,
[07:35.440 --> 07:37.720]  вот, следовательно, получается сверхлинийную сходимость.
[07:37.720 --> 07:38.200]  Вот.
[07:38.200 --> 07:40.520]  Это было, видите, достаточно общий такой
[07:40.520 --> 07:43.680]  некий результат, который, тем не менее...
[07:44.560 --> 07:45.920]  Сейчас скажу что-то.
[07:46.560 --> 07:47.560]  Сейчас, секунду.
[07:47.560 --> 07:48.080]  Да.
[07:48.640 --> 07:50.800]  Важно подчеркнуть, что
[07:50.800 --> 07:52.080]  все это
[07:52.440 --> 07:54.800]  должно выполняться в случае, когда у нас
[07:54.800 --> 07:57.160]  хк достаточно близко
[07:58.280 --> 07:59.800]  ну, близко находится
[08:00.360 --> 08:01.080]  со звездочкой.
[08:01.080 --> 08:01.280]  Вот.
[08:01.280 --> 08:03.720]  То есть это, насколько я понимаю,
[08:03.720 --> 08:05.960]  обусловлено ровно тем, чтобы был выполнен дотаральство.
[08:05.960 --> 08:06.400]  Вот.
[08:06.400 --> 08:07.680]  Что у нас...
[08:07.720 --> 08:09.840]  Вот, надо день написать со звездочкой.
[08:09.840 --> 08:10.080]  Ну вот.
[08:10.080 --> 08:12.000]  То есть все вот это, вот сверхлиния сходимость,
[08:12.000 --> 08:13.200]  сколько напишу, локальная.
[08:13.200 --> 08:13.560]  Вот.
[08:14.160 --> 08:15.760]  Это важно, потому что
[08:15.760 --> 08:19.040]  пример ситуации, когда у нас все разваливается,
[08:19.040 --> 08:23.120]  типа, функция fiat t, например, равна вот такому вот выражению.
[08:23.120 --> 08:24.280]  Я привожу этот пример.
[08:24.280 --> 08:26.640]  Проиллюстрировать, зачем, в принципе,
[08:26.640 --> 08:29.640]  нужно отдельно оговаривать про локальность.
[08:30.160 --> 08:33.440]  Потому что, ну, наверное, нужно небольшое отступление сделать
[08:33.440 --> 08:36.520]  и сказать, что вот есть метод Newton для решения
[08:38.040 --> 08:39.760]  задачи вот такой, что обсудили.
[08:39.760 --> 08:41.960]  Есть метод Newton для решения задачи вот такой.
[08:41.960 --> 08:44.120]  И вот они по сути дела эквивалентны,
[08:44.120 --> 08:46.480]  потому что вот эта задача в случае выпуклой функции
[08:46.480 --> 08:47.800]  сводится к задаче вот такой,
[08:47.800 --> 08:49.360]  в которой можно сказать, что это g от x.
[08:49.360 --> 08:51.960]  Поэтому можем посмотреть вопрос
[08:51.960 --> 08:54.200]  решения нелинейного уравнения
[08:54.200 --> 08:55.640]  очевидным решением 0.
[08:55.640 --> 08:56.520]  Вот.
[08:56.520 --> 08:57.560]  И, ну, здесь
[08:57.560 --> 09:00.960]  для такой штуки метод Newton, понятно, как будет записываться.
[09:00.960 --> 09:04.320]  Это будет xk минус матрицы якобе
[09:04.320 --> 09:06.960]  минус 1 на g от x.
[09:07.040 --> 09:10.360]  Нужно ли пояснять, почему здесь вот такое выражение?
[09:10.360 --> 09:12.880]  Или очевидно, или вы уже знаете это все?
[09:12.880 --> 09:14.480]  Ставьте плюс, если можно идти дальше,
[09:14.480 --> 09:16.280]  и минус, если нужно подробнее пояснить.
[09:16.280 --> 09:18.280]  Сейчас к чему относится вот этот комментарий,
[09:18.280 --> 09:19.080]  то есть плюс-минус?
[09:19.080 --> 09:21.360]  Этот комментарий относится к тому, что...
[09:21.360 --> 09:22.560]  Так, надо пояснить, все вижу.
[09:22.560 --> 09:25.800]  К тому, что мы сейчас покажем
[09:25.800 --> 09:28.240]  локальное устройство метода Newton
[09:28.240 --> 09:30.680]  на примере не решения задачи минимизации,
[09:30.680 --> 09:33.360]  а решения системы, ну, решения нелинейного уравнения.
[09:33.360 --> 09:35.160]  Допустим, что сводим одно и другое.
[09:35.160 --> 09:36.480]  Да, заодно же показать, действительно.
[09:37.000 --> 09:39.400]  Ну, то есть смотрите, что происходит.
[09:39.400 --> 09:40.920]  Вот мы хотим вот такое найти.
[09:40.920 --> 09:44.280]  Это мы вместо x подставляем xкат
[09:44.280 --> 09:47.720]  плюс линейную опроксимацию xкат на x минус xкат.
[09:47.720 --> 09:51.040]  Ну и отсюда x, наш новый, каплю в первый получается, да?
[09:51.040 --> 09:56.440]  Выражается понятно, что xкат и минус g' минус 1 от xкат
[09:56.440 --> 09:58.120]  вон g от xкат.
[09:58.120 --> 10:01.640]  Все векторы, потому что g это функция из rn в rn,
[10:01.640 --> 10:02.840]  система нелинейных уравнений.
[10:02.840 --> 10:05.280]  В общем, получаем, что методы совпадают,
[10:05.320 --> 10:07.440]  потому что если наш g это f',
[10:07.440 --> 10:10.720]  то g' это будет f' ровно тот самый гессиан,
[10:10.720 --> 10:11.920]  о котором мы до этого говорили.
[10:11.920 --> 10:13.640]  Понятно ли, почему так выглядит метод Ньютон
[10:13.640 --> 10:16.040]  для решения системы нелинейных уравнений теперь?
[10:16.040 --> 10:16.560]  А, гуд.
[10:16.560 --> 10:17.080]  Хорошо.
[10:17.080 --> 10:18.360]  Ну, давайте найдем производную.
[10:18.360 --> 10:20.880]  Я думаю, тут все стилисты как-то делаются.
[10:20.880 --> 10:23.520]  Э, что, одно и второе, да?
[10:23.520 --> 10:24.640]  Минус t.
[10:24.640 --> 10:26.400]  Сейчас опять не хватит места, но ничего.
[10:26.400 --> 10:27.600]  Конечная бумага, это.
[10:27.600 --> 10:28.040]  Добно.
[10:28.040 --> 10:30.320]  Так, бом-бом-бом-бом-бом.
[10:30.320 --> 10:32.280]  Одно второе на 2t.
[10:32.280 --> 10:34.600]  И, собственно, 1 плюс t квадрат
[10:34.600 --> 10:35.600]  минус одно второе.
[10:35.600 --> 10:37.120]  Вот, что-то такое.
[10:37.120 --> 10:39.040]  Это все, значит, будет у нас равно.
[10:39.040 --> 10:41.680]  Двойки уходят, t квадрат остается,
[10:41.680 --> 10:45.520]  и можно вынести, в общем, 1 плюс t квадрат.
[10:45.520 --> 10:47.880]  Так, давайте не будем, не будем торопиться.
[10:47.880 --> 10:49.720]  Это выносить, получается, единица
[10:49.720 --> 10:53.840]  минус квадрат умножить на 1 плюс t квадрат.
[10:53.840 --> 10:57.400]  Извините, а мы минус при показателе 1 плюс t квадрат забыли?
[10:57.400 --> 10:58.720]  Да, я...
[10:58.720 --> 10:59.960]  Сейчас, тут все неправильно.
[10:59.960 --> 11:02.560]  Я хотел сделать 1 плюс t квадрат, вот так, правильно?
[11:02.560 --> 11:04.240]  И минус t квадрат, вот.
[11:04.280 --> 11:05.480]  Делится на 1 плюс t квадрат.
[11:05.480 --> 11:06.520]  Это, так говорить, корректно.
[11:06.520 --> 11:07.200]  Все, отлично.
[11:07.200 --> 11:08.640]  Это сокращается,
[11:08.640 --> 11:10.800]  и мы получаем, что наше производное
[11:10.800 --> 11:13.120]  это 1 плюс t квадрат в степени минус 3 вторых.
[11:13.120 --> 11:13.600]  Вот.
[11:13.600 --> 11:16.720]  Ну и теперь, собственно, t к плюс 1
[11:16.720 --> 11:21.440]  это t к минус 1 плюс t к в квадрате
[11:21.440 --> 11:23.240]  в степени 3 вторых, потому что там минус стоит.
[11:23.240 --> 11:24.920]  И это все умножается на что?
[11:24.920 --> 11:29.800]  На t к и 1 плюс t к в квадрате минус 1 вторая.
[11:29.800 --> 11:33.440]  t к минус t к умножить на вот, ну понятно,
[11:33.440 --> 11:38.480]  на 1 плюс t к в квадрате минус t к в кубе.
[11:38.480 --> 11:39.360]  Вот такая это рация.
[11:39.360 --> 11:42.480]  То есть, ну, сходимость-то вообще-то почти кубическая, да, получается?
[11:42.480 --> 11:43.840]  Вот.
[11:43.840 --> 11:45.360]  Но это, но важно другое.
[11:45.360 --> 11:50.160]  Важно, что при норме t 0 меньше единицы мы будем сходиться.
[11:50.160 --> 11:52.800]  При норме t 0 равным единице
[11:52.800 --> 11:54.360]  будет, будет асцелировать все.
[11:54.360 --> 11:57.960]  Третье, если t 0 больше единицы, то расходится.
[11:58.560 --> 12:02.480]  Поэтому важно, чтобы мы попали именно вот сюда.
[12:02.480 --> 12:04.760]  Локальность важна. Понятен ли пример? Хорошо.
[12:04.760 --> 12:06.840]  Это мы про, был первый пункт,
[12:06.840 --> 12:08.320]  про сверхъянеисходимость.
[12:08.320 --> 12:10.520]  Теперь второй пункт, про квадратичную исходимость,
[12:10.520 --> 12:12.760]  собственно, ради чего все это задевается.
[12:12.760 --> 12:15.680]  Вот. И тут потребуется несколько условий.
[12:15.680 --> 12:16.840]  Именно три штуки.
[12:16.840 --> 12:19.560]  Пусть первый, гессиан-липшицов, уже не так нехило.
[12:19.560 --> 12:23.160]  Нормы, соответственно, там матричная вторая норма и подчинен,
[12:23.160 --> 12:24.680]  точнее, тут вот вторая норма Евкрида,
[12:24.680 --> 12:27.600]  вот тут вторая норма матричная, типа, максимальная.
[12:27.600 --> 12:29.480]  В данном случае собственное значение,
[12:29.480 --> 12:31.520]  потому что гессианы симметричные,
[12:31.520 --> 12:34.080]  в общем случае старше и сингулярное значение.
[12:34.080 --> 12:38.000]  Вот. Дальше x 0 достаточно...
[12:38.000 --> 12:40.600]  Дальше правильно сказать, что f от x сильно выпуклась
[12:40.600 --> 12:44.040]  в константы μ. Вот. И третье, что мы достаточно близки,
[12:44.040 --> 12:47.120]  а именно вот так. Вот. То есть, вот.
[12:47.120 --> 12:49.080]  И тогда из этого всего замечательных,
[12:49.080 --> 12:51.680]  из всех этих замечательных условий следует,
[12:51.680 --> 12:55.760]  что у нас x плюс 1 минус x со звездочкой
[12:55.760 --> 13:00.240]  будет меньше либо равно, чем m на норму x ка минус x со звездочкой
[13:00.240 --> 13:02.000]  вот тут вот самый квадрат появляется,
[13:02.000 --> 13:05.640]  на 2 μ минус m нормы x ка минус x со звездочкой.
[13:05.640 --> 13:08.000]  Вот. То есть, вот это вот наличие вот этого квадрата
[13:08.000 --> 13:10.320]  обеспечивает нам то, что сходимость будет квадратична.
[13:10.320 --> 13:12.320]  Вот так. Здесь двоечка. И здесь тоже. Вот.
[13:12.320 --> 13:14.240]  Теперь, то есть, мы будем...
[13:14.240 --> 13:16.040]  То есть, график сходимости будет выглядеть вот так.
[13:16.040 --> 13:17.560]  Сейчас я, наверное, его отдельно покажу.
[13:17.560 --> 13:21.960]  Типа, оп. И вот здесь вот было, грубо говоря, 10 минус 1.
[13:21.960 --> 13:28.200]  Дальше оценки какие? 10 минус 1, 10 минус 2, 10 минус 4 и 10 минус 8.
[13:28.200 --> 13:30.840]  Ну, короче, понятно, что дальше идет 10 минус 16.
[13:30.840 --> 13:34.200]  Вот. И мы сошлись за раз, два, три, четыре, четыре террация.
[13:34.200 --> 13:36.160]  То есть, это не тот, это не градиентный спуск,
[13:36.160 --> 13:37.960]  который там будет сходиться миллион лет
[13:37.960 --> 13:40.080]  до точно здесь минус там 10.
[13:40.080 --> 13:42.160]  Тут опять террация к 10 минус 16 сходится.
[13:42.160 --> 13:43.440]  Понятно ли?
[13:43.440 --> 13:46.280]  Формулу скорости сходимости я не успел выписать.
[13:46.280 --> 13:47.240]  Спасибо.
[13:47.240 --> 13:47.960]  Не выбирайте.
[13:47.960 --> 13:49.160]  Выбирайте, я тоже не успел.
[13:49.160 --> 13:50.160]  Да-да-да. Все.
[13:50.840 --> 13:53.760]  И я сейчас картину хотел показать, на которой это все делаю и старирую.
[13:53.760 --> 13:55.200]  Вот. Так.
[13:55.200 --> 13:58.000]  Для этого надо делать вот так и вот так.
[13:58.000 --> 13:59.520]  То есть, для вот такой вот задачи,
[13:59.520 --> 14:01.720]  которая, по сути дела, смысл которой в том,
[14:01.720 --> 14:05.200]  чтобы найти центр аналитический для многоугольника,
[14:05.200 --> 14:08.600]  это ли почему это связано как-то с центром многоугольника или нет,
[14:08.600 --> 14:10.440]  поставьте плюс, если понятно, почему связано,
[14:10.440 --> 14:11.520]  и минус, если непонятно.
[14:11.520 --> 14:12.760]  Непонятно, да?
[14:12.760 --> 14:13.960]  Ну ладно, давайте это вернем.
[14:13.960 --> 14:16.280]  Ну, хорошо, давайте пока запомним, как она выглядит.
[14:16.280 --> 14:19.760]  Я сейчас про метод скажу, потом вернемся и я немного смысл поясню.
[14:19.800 --> 14:22.200]  Вот. Ну и вот если запустить метод Newton и градиентный спуск,
[14:22.200 --> 14:23.840]  то они будут сходиться вот следующим образом.
[14:23.840 --> 14:25.720]  То есть, вот здесь вот будет некоторая область,
[14:25.720 --> 14:27.920]  в которой метод Newton будет сходиться линейно.
[14:27.920 --> 14:29.600]  Вот. Почему и как это будет?
[14:29.600 --> 14:32.120]  Почему так происходит, я чуть позже скажу, когда мы сейчас время докажем.
[14:32.120 --> 14:35.720]  Начиная с этой точки, он получается раз, два, три за три итерации,
[14:35.720 --> 14:38.720]  как вот было примерно, ну примерно оценка, которая у меня и была.
[14:38.720 --> 14:42.520]  Вот. Он благополучно сошелся к там 10 минус 8-ой.
[14:43.240 --> 14:46.760]  Блестяще, быстро, легко и без особо, ну, относительно легко.
[14:46.800 --> 14:50.120]  Без каких-либо проблем, именно связанных с симпточкой сходимости.
[14:50.120 --> 14:52.320]  Вот. Это вот, в общем, типа, если вы что-то такое видите,
[14:52.320 --> 14:55.040]  когда вы хотели получить метод Newton, значит, что вы все правильно сделали
[14:55.040 --> 14:57.440]  и, в принципе, все хорошо так и должно быть.
[14:57.440 --> 15:00.760]  То есть, если у вас метод Newton начинает медленно сходиться вот в стиле вот таком,
[15:00.760 --> 15:03.520]  вот это значит, что вы где-то там что-то неправильно, неправильно учили.
[15:03.520 --> 15:06.680]  Вот. Сейчас в процессе, я надеюсь, как-то отдельно упомянуть
[15:06.680 --> 15:10.560]  тем, какие потенциальные ошибки вы можете случайно сделать.
[15:10.560 --> 15:15.960]  Вот. Так. Теперь, собственно, про задачу, откуда она в целом может взяться.
[15:16.000 --> 15:18.320]  Ну, вот взяться она может быть... И следующая история.
[15:18.320 --> 15:21.200]  Надо решить, система не равен. А х меньше, либо равно единице,
[15:21.200 --> 15:24.440]  в условии, что модуль Са меньше, либо равен единице.
[15:24.440 --> 15:27.640]  Вот. Хотим найти какой-то х. Вот. При этом вот эта штука,
[15:27.640 --> 15:31.640]  я надеюсь, геометрия ясна, это пересечение гиперплоскостей.
[15:31.640 --> 15:33.080]  По сути дела, это многоугольник.
[15:33.080 --> 15:36.600]  Любой х, который лежит внутри, нам потом, нас устраивает.
[15:36.600 --> 15:39.440]  Вот это, вот это, вот это, вот это. Ну, плюс вот это. Вот.
[15:39.440 --> 15:43.160]  А как выбрать какой-то один? Вот. Ну, и отсюда...
[15:43.160 --> 15:44.440]  Парам-пам-пам. Да.
[15:44.440 --> 15:47.560]  Отсюда следует простая штука, что давайте сделаем так,
[15:47.560 --> 15:53.360]  чтобы он этот наш х отстоял от наших граней, как можно дальше.
[15:53.360 --> 15:56.360]  Нет ли логика? Окей. Вроде, ну, вроде осмысленно, да?
[15:56.360 --> 15:59.560]  Ну, вот. И чтобы это сделать, мы рассмотрим вот такую вот,
[15:59.560 --> 16:03.160]  типа, такое выражение, которое, ну, просто, то, насколько мы лекуатнули,
[16:03.160 --> 16:05.560]  ну, насколько хорошо у нас всё выполняется. Вот.
[16:05.560 --> 16:09.360]  И насколько мы хотим, чтобы оно стало каким? Побольше, больше или нуля,
[16:09.360 --> 16:11.360]  то мы возьмём и вот логариф от него сделаем.
[16:11.360 --> 16:13.480]  Потому что логариф-то только на положительных определён,
[16:13.480 --> 16:15.320]  поэтому как только мы будем приближаться к нулю,
[16:15.320 --> 16:17.800]  у нас всё будет разлетаться. Ну, и поскольку логариф –
[16:17.800 --> 16:20.800]  мы это ещё и такая штука, мы поставим сюда минус,
[16:20.800 --> 16:23.720]  чтобы он стал вот таким. Скажем, что мы хотим проминемизировать
[16:23.720 --> 16:27.040]  эту всю историю, ну, и по всем понятно, по всем неравенствам.
[16:27.040 --> 16:29.480]  Эм. Вот. Тем самым мы будем как бы балансировать
[16:29.480 --> 16:34.040]  принадлежность ИКСа нашему многоугольнику так, чтобы,
[16:34.040 --> 16:37.200]  ну, грубо говоря, все неравенства были выполнены
[16:37.200 --> 16:40.800]  максимальным зазором от границы. Понятно ли, откуда взялась установка?
[16:41.640 --> 16:44.680]  Прекрасно. Так, графики я показал, всё прекрасно.
[16:44.680 --> 16:46.840]  Давайте теперь, собственно, докажем это утверждение.
[16:46.840 --> 16:49.760]  Доказательство будет, ну, не очень быстро,
[16:49.760 --> 16:52.160]  но и в то же время максимально как бы инструментальным
[16:52.160 --> 16:55.120]  и понятным, я надеюсь. Для начала обозначим всё то,
[16:55.120 --> 16:58.520]  что нам нужно использовать, а именно ведём величину
[16:58.520 --> 17:00.880]  РК плюс один, которая будет равняться, собственно,
[17:00.880 --> 17:05.000]  нашей невязке. Вот. И после подстановки выражения
[17:05.000 --> 17:08.760]  для метода Ньютона мы получим, что эта вся история
[17:08.760 --> 17:12.440]  есть не что иное, как РКТ минус это самое направление.
[17:12.440 --> 17:18.840]  Вот. Дальше сделаем такой трюк. Распишем f'xk вот таким вот образом.
[17:18.840 --> 17:24.440]  f'xk-f'x' поскольку это ноль. Вот. И скажем, что это интеграл
[17:24.440 --> 17:31.360]  от 0 до 1 от гессиана на x' плюс tRK dt. То есть мы возьмём
[17:31.360 --> 17:35.960]  как подтресочек, соединяющий x' и xk и проинтегрируем
[17:35.960 --> 17:38.520]  второй производной вдоль этого отрезка. Я тут забыл написать
[17:38.520 --> 17:41.160]  ещё РК, чтобы размеры не совпали. То есть формула
[17:41.160 --> 17:44.520]  Ньютон-Лейбенца переписаны в периметрическом виде,
[17:44.520 --> 17:46.640]  так сказать. Понятно ли, откуда взялось?
[17:46.640 --> 17:51.280]  Да. Прекрасно. Ну, это наш f'xk и теперь мы можем его
[17:51.280 --> 17:56.400]  подставить вот сюда. Получим, что РК плюс один это РК
[17:56.400 --> 17:59.960]  минус гессиан, умноженный на интеграл. Вот. После
[17:59.960 --> 18:04.280]  чего мы благополучно выносим за знак, выносим множитель
[18:04.280 --> 18:07.520]  обратно гессиан. Получаем, что тут у нас… Сейчас, секунду.
[18:07.520 --> 18:10.160]  Так, рано. Потом, конечно, это сделаем. Сначала надо
[18:10.160 --> 18:13.000]  сказать, что, в сути дела, у нас теперь… Бла-бла-бла.
[18:13.000 --> 18:18.600]  Вот так dt ещё умножается на РК. Вот. Вынесли, в общем,
[18:18.600 --> 18:23.160]  то, что не зависит от t. После чего понимаем, что теперь
[18:23.160 --> 18:26.160]  наша норма РК плюс один. Меньше ли бравна норме,
[18:26.160 --> 18:29.400]  ну, обозначим… Так, не хватает немножко места. Обозначим
[18:29.400 --> 18:33.000]  вот эту штуку за g. Вот. Что это ну, норма g на норму
[18:33.000 --> 18:35.920]  g. Вот. И поймём теперь, что на самом деле мы почти
[18:35.920 --> 18:38.960]  уцели, потому что вот этот вот РК-1 уже есть. Теперь
[18:38.960 --> 18:41.920]  нам нужно оценить норму g так, чтобы второй множитель
[18:41.920 --> 18:43.920]  нормы РК появился, тогда у нас появится квадрат.
[18:43.920 --> 18:47.640]  Мы будем, собственно, дойдём до того, что мы изначально
[18:47.640 --> 18:49.920]  хотели получить. Давай теперь оценивать. Значит, что такое
[18:49.920 --> 18:54.040]  же kt? Ну, да, она, конечно, от ка зависит. Почему оно
[18:54.040 --> 18:57.080]  равно? Можно вынести теперь обратно гессиан, то, что
[18:57.080 --> 19:00.680]  хотел, что всё мы начали сделать. И внести единичную,
[19:00.760 --> 19:04.600]  получившуюся статус от единичной матрицы внутрь
[19:04.600 --> 19:08.680]  интеграла, чтобы делать хитрый трюк. Вот. А трюк,
[19:08.680 --> 19:11.360]  собственно, очень простой, что норма g у нас, это меньше
[19:11.360 --> 19:14.760]  либо равно, чем норма вот этой штуки в первом множителе.
[19:14.760 --> 19:18.240]  И потом интеграл от нормы разности. Интеграл от нормы
[19:18.240 --> 19:20.760]  разности у нас ограничен в силу того, что гессиан
[19:20.760 --> 19:24.200]  липшится. Вот. Поэтому это меньше либо равно, чем
[19:24.200 --> 19:28.160]  вот это. Здесь будет интеграл для одного m на норму разности
[19:28.160 --> 19:33.680]  аргумента. xk минус x звёздочка минус t r dt. Понятно ли,
[19:33.680 --> 19:35.680]  что происходило до этого момента? Ставьте, пожалуйста,
[19:35.680 --> 19:38.840]  плюс, если понятно, и минус, если в какой-то момент были
[19:38.840 --> 19:41.160]  непонятные преобразования или мотивации их приведения.
[19:41.160 --> 19:44.920]  Вижу два плюса. А вижу три. Отлично. Вот. А что получается-то?
[19:44.920 --> 19:47.640]  Вот эта штука, это же, ой. Вот эта штука, это же, на
[19:47.640 --> 19:51.440]  самом деле, rk. t у нас с 0 до 1 интегрируется. Поэтому
[19:51.440 --> 19:54.440]  можно переписать, что наш интеграл, который вот здесь,
[19:55.440 --> 20:00.760]  минус t r dt, по сути, норма rk, собственно, то, что вы хотели.
[20:00.760 --> 20:06.080]  0,1 m, а 1 минус t, потому что положительное число dt. Теперь
[20:06.080 --> 20:08.280]  надо посчитать интеграл. Давайте, давайте посчитайте
[20:08.280 --> 20:10.320]  этот интеграл, пожалуйста, и скажите мне, чему он будет
[20:10.320 --> 20:13.280]  равен. Вот. Немножко взбодримся с утра пораньше. У меня
[20:13.280 --> 20:16.480]  уже как эти штуки вычисляются. Ответ можно писать в чате.
[20:16.480 --> 20:20.760]  Ну, это t минус t квадрата пополам плюс какая-то констанция.
[20:20.760 --> 20:23.160]  У вас определённый интеграл для одного числа должно
[20:23.160 --> 20:27.600]  получиться. А, ой, точно, первообразно. Одна вторая, по идее.
[20:27.600 --> 20:30.480]  Ну да, вроде тоже должно получиться одна вторая. То есть, смотрите,
[20:30.480 --> 20:33.360]  по сути дела, эта оценка интеграл дала нам необходимый
[20:33.360 --> 20:36.120]  дополнительный множитель. Вот этот. Вот. И теперь осталось,
[20:36.120 --> 20:38.600]  как бы, вот эту штуку оценить. Вот. Но для того, чтобы ее
[20:38.600 --> 20:42.120]  оценить, нам потребуется вспомнить о том, что у нас из
[20:42.120 --> 20:45.560]  липчатогости следует следующая оценка на просто
[20:45.560 --> 20:49.800]  гессиан в точке xk. Вот. Через гессиан в точке x звёдочка минус
[20:49.800 --> 20:54.600]  m на норму rk и на единичную матрицу. То есть, это просто
[20:54.600 --> 21:00.000]  вывод того, вывод из того, как у нас. Ну что, вот если
[21:00.000 --> 21:06.200]  у нас вторая норма xk минус 2 минус x со звездочкой.
[21:06.200 --> 21:08.760]  Вот здесь вот вторая норма чего-то меньше. Это значит,
[21:08.760 --> 21:14.240]  что лямбда макс у этой штуки, он меньше, чем вот эта величина.
[21:14.240 --> 21:19.640]  А это значит, что если мы рассмотрим матрицу f2' xk
[21:19.640 --> 21:24.440]  минус f2' x со звездочкой минус вот то, что вот здесь стоит.
[21:24.440 --> 21:27.240]  Вот. То мы получим отрицательно определенную матрицу. Вот.
[21:27.240 --> 21:32.240]  Ну а раз так, то у нас, ну там, да, тут надо бы, наверное,
[21:32.240 --> 21:35.240]  было написать какая-нибудь ерунда со знаками опять начнется.
[21:35.240 --> 21:39.240]  А, ерунда со знаками начнется, да. Ну да, тут типа вот так
[21:39.240 --> 21:42.240]  надо поставить. В общем, от этого ничего не, свойство
[21:42.240 --> 21:44.240]  липчатогость от этого не поменяется, короче говоря.
[21:44.240 --> 21:48.640]  Вот. И теперь, когда мы переносим вот это вот сюда, то мы получаем
[21:48.640 --> 21:51.640]  ровно то, что стоит вот тут. Получилось ли уследить за этими
[21:51.640 --> 21:54.640]  стрелочками тем, что происходило? И так себе. А, вот это умножить
[21:54.640 --> 21:58.640]  на единичную матрицу. А, нет. Или лучше все-таки писать
[21:58.640 --> 22:02.640]  все подробнее. Лучше подробней. Окей, ладно. Давайте.
[22:02.640 --> 22:08.640]  Вот эта штука меньше, чем m на x минус xk минус x со звездочкой.
[22:08.640 --> 22:12.640]  Это наши условия липчатогости. Вот. Ну, то есть вот здесь стоит
[22:12.640 --> 22:16.640]  ровно то же самое. Вот. А здесь стоит уже вот это то же самое
[22:16.640 --> 22:20.640]  умножить на единичную матрицу. Да, и это отрицательно определено.
[22:20.640 --> 22:23.640]  Ну, потому что мы из максимального множителя вычли что-то, вычли
[22:23.640 --> 22:27.640]  матрицу, сдвинули спектр, грубо говоря. Вот. И раз
[22:27.640 --> 22:31.640]  была отрицательная величина, то вся матрица будет отрицательно
[22:31.640 --> 22:33.640]  определена. Понятно ли это преобразование? Да, да, да.
[22:33.640 --> 22:36.640]  А, все, отлично. Вот. Ну, собственно, да, это было
[22:36.640 --> 22:39.640]  пояснение вот к этому, к этой строчке. Теперь мы знаем, что у нас
[22:39.640 --> 22:43.640]  функция сильно выпукла, поэтому в выполнении к этой
[22:43.640 --> 22:48.640]  штуке мы имеем, что f' xk f' x
[22:48.640 --> 22:53.640]  со звездочкой m rk. Единичную матрицу мы сверху можем вот
[22:53.640 --> 22:56.640]  эту теперь величину подпереть mu на единичную матрицу в силу
[22:56.640 --> 22:59.640]  критерия второго порядка сильной выпуклости. То есть теперь у нас
[22:59.640 --> 23:04.640]  вот образуется mu минус m норма rk на единичную матрицу. Все, отлично.
[23:04.640 --> 23:08.640]  Почти победа. Вот раз такие соотношения справедливы для
[23:08.640 --> 23:11.640]  самих матриц, то для обратных будет справедливо ровно обратное.
[23:11.640 --> 23:15.640]  Неудивительно. Потому что, ну, лямбда макс, лямбда мин меняется
[23:15.640 --> 23:20.640]  местами при взять обратной. Вот. Ну и поэтому, значит, f'-1xk
[23:20.640 --> 23:25.640]  по норме меньше либо равно, чем 1 делить на mu минус m
[23:25.640 --> 23:29.640]  на норму r. Все. Значит, теперь объединяя вот это...
[23:29.640 --> 23:33.640]  Таль, давайте к ней. Объединяя вот это...
[23:33.640 --> 23:37.640]  Крупнее. А, это самое крупное, оказывается. И вот это,
[23:37.640 --> 23:40.640]  вставляя все это вместе, мы получим ровно нужный новый результат.
[23:40.640 --> 23:44.640]  Откуда берется квадрат справа? Ну, с правой части вроде, надеюсь, понятно.
[23:44.640 --> 23:49.640]  Вот. Теперь что? Теперь надо пояснить, зачем нужно вот это.
[23:49.640 --> 23:54.640]  Вот это нужно затем, что если мы вот эту величину распишем таким вот образом,
[23:54.640 --> 24:02.640]  что мы mxk минус x' делим на 2 mu минус mxk минус x' в квадрате
[24:02.640 --> 24:05.640]  и умножаем теперь на xk минус x'
[24:05.640 --> 24:08.640]  Надо быть уверенным, чтобы в начале у нас вот эта величина
[24:08.640 --> 24:10.640]  будет меньше 1. Ну, чтобы норма действительно была.
[24:10.640 --> 24:15.640]  И если мы как раз-таки потребуем, чтобы это было меньше 1 для равной 0,
[24:15.640 --> 24:17.640]  то мы получим вот эту вот оценку.
[24:17.640 --> 24:23.640]  То есть, тут все достаточно прямолинейно, без каких-либо хитрых штук.
[24:23.640 --> 24:27.640]  Нет ли доказательства и основные поводы из него? Вроде да.
[24:27.640 --> 24:32.640]  Хорошо. А теперь, значит, пара слов про то, как бороться с этой локальностью.
[24:32.640 --> 24:36.640]  Ну, наверное, не очевидно, но поскольку единственная штука,
[24:36.640 --> 24:40.640]  в которой мы еще не внедрили метод Ньютона, это размер шага,
[24:40.640 --> 24:43.640]  именно его настройка позволяет сделать сходимость глобальной.
[24:43.640 --> 24:46.640]  Называемый демпфированный метод Ньютона, я не очень люблю это название,
[24:46.640 --> 24:49.640]  но, в общем, записывается он понятным образом,
[24:49.640 --> 24:54.640]  с точностью как мы до этого обсуждали, когда у нас есть шаг, у нас есть направление.
[24:54.640 --> 24:58.640]  Ну, давайте шаг выделим желтеньким, направление выделим красненьким.
[24:58.640 --> 25:03.640]  И, значит, использование этого шага позволяет, во-первых, сделать сходимость глобальной,
[25:03.640 --> 25:07.640]  и, во-вторых, из, грубо говоря, области линейной сходимости мы,
[25:07.640 --> 25:11.640]  когда мы придем из этой области в область, где у нас квадратичная сходимость,
[25:11.640 --> 25:17.640]  то поэтому надо начинать с альфа ноль в бэктрекинге равным единице в адаптивном...
[25:17.640 --> 25:19.640]  Блин, плохо.
[25:19.640 --> 25:24.640]  То есть при адаптивном поиске альфа, ну, я вот так сделаю, ноль должно быть равно единице.
[25:24.640 --> 25:30.640]  Потому что если вы попали в нужную область, то взятие альфа ноль равно единице,
[25:30.640 --> 25:35.640]  и если для этого шага сходимость уже получается, что функцию убывает,
[25:35.640 --> 25:39.640]  скорее всего, это значит, что вы попали в эту самую область,
[25:39.640 --> 25:43.640]  и дальше, взяв этот шаг равный единице,
[25:43.640 --> 25:45.640]  вы получите уже квадратичную сходимость вместо линейной,
[25:45.640 --> 25:49.640]  которая присутствует, если делать шаг меньше единицы.
[25:49.640 --> 25:53.640]  Нет ли основная мотивация и то, как делается подбор шага в этом методе?
[25:53.640 --> 25:54.640]  Очень хорошо.
[25:54.640 --> 25:58.640]  Окей, так, 9.45, слушайте, хорошо, хороший темп.
[25:58.640 --> 26:01.640]  Теперь, собственно, еще важный момент про мета Ньютона,
[26:01.640 --> 26:03.640]  который уже был проиллюстрирован на картинке,
[26:03.640 --> 26:07.640]  это то, что метод Ньютона дает решение очень высокой точности.
[26:07.640 --> 26:10.640]  То есть если вам не нужна такая точность,
[26:10.640 --> 26:13.640]  то, возможно, вам лучше остановиться на методах первого порядка,
[26:13.640 --> 26:17.640]  которые, там, 10 мил в шестой вам дадут по норме градиентов какой-нибудь задачи,
[26:17.640 --> 26:18.640]  и на том спасибо.
[26:18.640 --> 26:22.640]  Вот, а если вам нужно только 10 мил в двенадцатый, там, из каких-то ображений,
[26:22.640 --> 26:26.640]  то только метод Ньютона вам сможет дать такие-такие-такие точности за разумное время.
[26:26.640 --> 26:28.640]  Это, наверное, финальная ремарка.
[26:28.640 --> 26:30.640]  В целом, то в целом,
[26:30.640 --> 26:34.640]  да, мы вроде бы уже обсудили более-менее все достойные недостатки,
[26:34.640 --> 26:37.640]  то есть память, высокая стоимость одной итерации,
[26:37.640 --> 26:41.640]  но за счет этого мы получаем квадратичную сходимость и высокую точность.
[26:41.640 --> 26:44.640]  Теперь, так, есть какие-то вопросы по первой половине?
[26:44.640 --> 26:45.640]  Конец.
[26:45.640 --> 26:47.640]  Конец, ну замечательно, очень хорошо.
[26:47.640 --> 26:50.640]  Надеюсь, что все действительно достаточно понятно.
[26:50.640 --> 26:54.640]  Теперь перейдем к квазинтуловским методам Ньютона,
[26:54.640 --> 26:56.640]  идея которых очень простая.
[26:56.640 --> 26:58.640]  Вот у нас был, так,
[26:58.640 --> 27:03.640]  вот у нас был градиентный спуск, который мы получали из вот следующей оценки.
[27:03.640 --> 27:04.640]  Вы помните?
[27:04.640 --> 27:06.640]  Сверху оценивали квадратично.
[27:06.640 --> 27:07.640]  Понимаете такое? Прекрасно.
[27:07.640 --> 27:08.640]  Это был градиентный спуск.
[27:08.640 --> 27:10.640]  Методы Ньютона у нас примерно такая же ситуация.
[27:10.640 --> 27:14.640]  Только там, ну, не оценка сверху, но, в общем, так, да?
[27:14.640 --> 27:19.640]  И они, эти методы, в своих свойствах, достоинствах и недостатках,
[27:19.640 --> 27:21.640]  являются комплементарными друг к другу.
[27:21.640 --> 27:23.640]  То есть то, что хорошо у одного, то плохо у другого.
[27:23.640 --> 27:27.640]  В частности, у градиентного спуска сходимость только линейная,
[27:27.640 --> 27:28.640]  у метод Ньютона квадратичная.
[27:28.640 --> 27:32.640]  С другой стороны, градиентный спуск требует всего лишь линейного числа,
[27:32.640 --> 27:34.640]  линейную сложность по памяти имеет,
[27:34.640 --> 27:36.640]  метод Ньютона имеет и квадратичную сложность по памяти,
[27:36.640 --> 27:39.640]  и кубическую по итерации, ну, по стоимость одной итерации.
[27:39.640 --> 27:41.640]  То есть вот они как бы друг друга дополняют.
[27:41.640 --> 27:46.640]  Поэтому возникла идея сказать, что давайте мы сделаем что-то промежуточное
[27:46.640 --> 27:50.640]  и будем нашу функцию приближать, вот, почти что точно так же, как метод Ньютон,
[27:50.640 --> 27:57.640]  он только вместо гессиана возьмем и будем его приближать
[27:57.640 --> 28:00.640]  к некоторой матрице, к другой, не уже не гессианам.
[28:00.640 --> 28:02.640]  То есть метод перестает быть методом второго порядка
[28:02.640 --> 28:06.640]  и возвращается семейство методов, которые используют только градиенты.
[28:06.640 --> 28:11.640]  Вот, и сейчас будем разбираться, как именно эту матрицу B нам куда мы выбрать.
[28:11.640 --> 28:14.640]  И для этого, ну, то есть понятно, что отсюда
[28:14.640 --> 28:17.640]  xk плюс 1 будет пересчитывать по тем же самым формулам,
[28:17.640 --> 28:21.640]  ну, тут вот 2 альфа к и все заработало, фей.
[28:23.640 --> 28:27.640]  Вот, то есть на самом деле нам как бы даже не матрица B нужна, а матрица B в минус 1.
[28:27.640 --> 28:28.640]  То есть что надо сделать?
[28:28.640 --> 28:34.640]  Надо чтобы сложность одной итерации стала хотя бы сложной меньше, чем N куб.
[28:34.640 --> 28:38.640]  Первое, второе, сходимость осталась хотя бы сверхлинейной,
[28:38.640 --> 28:40.640]  чтобы, ну, не сваливаться в линию сходимости градиентного метода.
[28:40.640 --> 28:44.640]  Вот, ну, вот сюда же как бы два пункта.
[28:44.640 --> 28:52.640]  Первый – это пересчет xk плюс 1, пересчет из xk xk плюс 1,
[28:52.640 --> 28:55.640]  а второй – это пересчет из Bk в Bk плюс 1.
[28:55.640 --> 28:59.640]  То есть пересчет матрицы и пересчет x должны быть суммарно недороже, чем N квадрат.
[28:59.640 --> 29:01.640]  Понятно ли, почему это надо?
[29:01.640 --> 29:02.640]  Вкратце, можно пояснить?
[29:02.640 --> 29:06.640]  Мы хотим, чтобы наш метод работал быстрее, чем метод Ньютона.
[29:06.640 --> 29:07.640]  Да.
[29:07.640 --> 29:10.640]  В методе Ньютона у нас стоимость решения линии системы N куб.
[29:10.640 --> 29:13.640]  Мы хотим, чтобы стоимость одной итерации была меньше, чем N куб.
[29:13.640 --> 29:18.640]  У нас в эту одну итерацию помещается не только вычисление пересчета xk плюс 1,
[29:18.640 --> 29:20.640]  но и обновление матрицы B.
[29:20.640 --> 29:24.640]  Поэтому вот это надо, ну, очень надо следить и про вот это, и про вот это.
[29:24.640 --> 29:27.640]  А что такое сверхлинейное? Можете напомнить?
[29:27.640 --> 29:29.640]  А, сверхлиния скорость сходимости, да, могу.
[29:29.640 --> 29:32.640]  То есть мы доказывали только, что метод Ньютон сходится квадратично.
[29:32.640 --> 29:35.640]  Теперь мы его как бы немножко огрубляем,
[29:35.640 --> 29:40.640]  но хотим огрубить не так сильно, чтобы, настолько плохо, чтобы стало типа линейное.
[29:40.640 --> 29:42.640]  Логично, согласитесь?
[29:42.640 --> 29:43.640]  Да, согласен.
[29:43.640 --> 29:44.640]  Хорошо.
[29:44.640 --> 29:47.640]  Так, и третий пункт со звездочкой, обычно.
[29:47.640 --> 29:54.640]  Хотим хранить B, то есть убрать, давайте, компактное хранение B.
[29:54.640 --> 29:58.640]  Вот, это типа, чтобы уйти от N квадрат памяти.
[29:58.640 --> 30:00.640]  Как это делается, вообще-то пока сход очень неочевидно.
[30:00.640 --> 30:03.640]  Вот, и я надеюсь, что ближе к концу мы все-таки успеем это разобрать.
[30:03.640 --> 30:04.640]  Вот, вот такие у нас требования.
[30:04.640 --> 30:08.640]  Значит, теперь, собственно, что мы к этому матрицу B можем,
[30:08.640 --> 30:12.640]  что называется, извлечь из каких-то соображений.
[30:12.640 --> 30:15.640]  Ну, вот тут хитрое некоторое место.
[30:15.640 --> 30:17.640]  Смотрите, вот когда у нас появилась вот эта вот оценка,
[30:17.640 --> 30:18.640]  давайте назовем FQ.
[30:18.640 --> 30:23.640]  Q у нас будет зависеть от направления H, и, собственно, так вот.
[30:23.640 --> 30:28.640]  Но здесь, в общем-то, когда мы все посчитали, нам все известно, кроме матрицы B.
[30:28.640 --> 30:33.640]  Давайте подумаем, что мы можем сказать о свойствах этой оценки,
[30:33.640 --> 30:35.640]  когда у нас есть некоторые вот куксы такие.
[30:35.640 --> 30:38.640]  Ну, то есть нету, что надо как-то B0 проинциализировать.
[30:38.640 --> 30:39.640]  Слушай, не вопрос.
[30:39.640 --> 30:40.640]  Инициализация.
[30:40.640 --> 30:44.640]  Ну, типа, лидичные матрицы можем проинциализировать, в общем-то, не беда.
[30:44.640 --> 30:47.640]  Если мы так сделаем, то у нас получится такое выражение.
[30:47.640 --> 30:50.640]  x1 равно x0 минус алифа 0 на градиент.
[30:50.640 --> 30:52.640]  В общем, получим просто градиентный спуск, это все понятно.
[30:52.640 --> 30:57.640]  Далее, точка, то есть это вот мы типа, вот у нас был тут x0, мы вот переехали в x1.
[30:57.640 --> 31:01.640]  Далее нам нужно из x1 каким-то образом перейти в x2.
[31:01.640 --> 31:03.640]  Как это должно быть, должно выглядеть?
[31:03.640 --> 31:09.640]  x2 это x1 минус альфа 1, b1 в минус 1, f' от x1.
[31:09.640 --> 31:12.640]  Известно все, кроме вот этой штуки.
[31:12.640 --> 31:13.640]  Понятно ли на каком мы сейчас этапе?
[31:13.640 --> 31:15.640]  Окей, я вижу 1+.
[31:15.640 --> 31:16.640]  Я не понял вопрос.
[31:16.640 --> 31:19.640]  Вопрос в том, понятно ли, что мы сейчас пытаемся получить
[31:19.640 --> 31:22.640]  и на каком этапе мы сейчас находимся в процессе этого получения.
[31:22.640 --> 31:28.640]  То есть мы сейчас пытаемся, исходя из того, как мы интеррируемся с одной точки в другой,
[31:28.640 --> 31:32.640]  понять, какие требования на матрицу бы наложить, чтобы ее можно было бы пересчитать.
[31:32.640 --> 31:33.640]  Да, понятно, хорошо.
[31:33.640 --> 31:34.640]  Все, хорошо.
[31:34.640 --> 31:36.640]  Смотрите, мы сейчас находимся в точке x1.
[31:36.640 --> 31:40.640]  И в точке x1 у нас есть вот эта вот замечательная модель f'
[31:40.640 --> 31:41.640]  Давайте я, может быть, даже запишу.
[31:41.640 --> 31:44.640]  f, ой, f' и есть вот единичку.
[31:44.640 --> 31:49.640]  То есть это f от x1 плюс колярное произведение на…
[31:49.640 --> 31:53.640]  Ну и плюс соответственно коэффициентик h транспонировано b1 h.
[31:53.640 --> 31:54.640]  Вот, это наша модель.
[31:54.640 --> 32:01.640]  Что мы можем потребовать от этой модели, чтобы в некотором смысле догнать ее точность?
[32:01.640 --> 32:02.640]  Как вы думаете?
[32:02.640 --> 32:06.640]  Знаете, что у нас помимо f' есть еще версус просто f от x.
[32:06.640 --> 32:07.640]  Честная наша функция.
[32:07.640 --> 32:13.640]  Которую мы можем по точкам оценить в плане модели черного ящика.
[32:13.640 --> 32:14.640]  Какие будут варианты?
[32:14.640 --> 32:22.640]  Мы можем по конкретному интерту h посмотреть на реальное значение x плюс h
[32:22.640 --> 32:25.640]  и посмотреть на h транспонированное b…
[32:25.640 --> 32:29.640]  И сказать, что мы хотим, чтобы вот это было значением h t b h.
[32:29.640 --> 32:34.640]  Смотрите, если я правильно понял, то вы, в общем-то, на правильном пути…
[32:34.640 --> 32:36.640]  Смотрите, мы находим сейчас точки x1.
[32:36.640 --> 32:42.640]  И куда из этой точки мы можем перейти, чтобы можно было сравниваться…
[32:42.640 --> 32:48.640]  Сравнив получившееся значение нашей модели точным значением функции.
[32:48.640 --> 32:50.640]  Или с точным значением чего-то, связанным с функцией.
[32:50.640 --> 32:53.640]  Первое направление, которое направится, это ноль.
[32:53.640 --> 32:54.640]  Не поверите.
[32:55.640 --> 32:56.640]  Надо бы…
[32:56.640 --> 33:05.640]  Хочется, чтобы наш градиент нашей функции в нуле был бы равен честному градиенту в x1.
[33:05.640 --> 33:07.640]  И, как вы можете убедиться, это автоматически выполняется.
[33:07.640 --> 33:08.640]  Тем ли это видно?
[33:08.640 --> 33:12.640]  Или нужно написать еще одну строчку с градиентом нашей модели?
[33:12.640 --> 33:13.640]  Видно.
[33:13.640 --> 33:14.640]  Кострин взяла a, гуд, Дмитрий.
[33:14.640 --> 33:19.640]  Нет ли вам, что если возьмете градиент и подставите ноль, то получите просто градиент с точки x1?
[33:19.640 --> 33:20.640]  Ага, вижу.
[33:20.640 --> 33:21.640]  Хорошо.
[33:21.640 --> 33:22.640]  Это первое.
[33:22.640 --> 33:24.640]  Второе направление, какое напрашивается?
[33:24.640 --> 33:25.640]  Единичный вектор.
[33:25.640 --> 33:28.640]  Что мы не знаем, что происходит в направлении единичного вектора?
[33:28.640 --> 33:30.640]  А, тогда то же самое, что в прошлый раз.
[33:30.640 --> 33:31.640]  Какое?
[33:31.640 --> 33:32.640]  h с предыдущего шага.
[33:32.640 --> 33:36.640]  Да, но только не h, а мы перейдем вот из этой точки теперь вот в эту точку.
[33:36.640 --> 33:39.640]  А это направление называется x0-x1.
[33:39.640 --> 33:40.640]  Вот.
[33:40.640 --> 33:41.640]  Тогда мы получим что?
[33:41.640 --> 33:46.640]  Что градиент f' точки x1 плюс там 1 на 2 альф, понятно?
[33:46.640 --> 33:47.640]  Сейчас.
[33:47.640 --> 33:48.640]  Нет, подожди.
[33:48.640 --> 33:49.640]  Да, понятно.
[33:49.640 --> 33:50.640]  1 на 2 альфа.
[33:50.640 --> 33:52.640]  Просто 1 альф, простите.
[33:52.640 --> 33:53.640]  Когда будет что здесь?
[33:53.640 --> 33:59.640]  B1 на x0-x1 должно выровняться f' от x0.
[33:59.640 --> 34:00.640]  Во.
[34:00.640 --> 34:01.640]  Внимание, вопрос.
[34:01.640 --> 34:02.640]  Сейчас, секунду.
[34:02.640 --> 34:04.640]  У меня куда-то потерялся.
[34:06.640 --> 34:08.640]  Сейчас какая-то проблема возникла со знаком.
[34:08.640 --> 34:10.640]  Ой, не со знаком, а с альфой.
[34:10.640 --> 34:13.640]  Но я верю, что сейчас я пойму, как ее правильно разрешить.
[34:13.640 --> 34:15.640]  А h у нас там точно...
[34:15.640 --> 34:16.640]  Ну да, x0-x1, да.
[34:17.640 --> 34:21.640]  Да, видимо правильнее смотреть пока что без шага.
[34:21.640 --> 34:22.640]  Да.
[34:22.640 --> 34:23.640]  Вот.
[34:23.640 --> 34:25.640]  Да, давайте пока без шага будем смотреть.
[34:25.640 --> 34:27.640]  То есть полная аналогия с тем, как метод у меня классический
[34:27.640 --> 34:29.640]  уводился без демпирования.
[34:29.640 --> 34:34.640]  И тогда с учетом этой поправки мы получим условия на матрицу
[34:34.640 --> 34:35.640]  B1 вида...
[34:35.640 --> 34:39.640]  B1 умножается на x0-x1, а тут будет разность градиентов.
[34:39.640 --> 34:42.640]  Вот этот вектор обычно в литературе обозначается s0.
[34:42.640 --> 34:43.640]  Ну, это соответственно y0.
[34:43.640 --> 34:44.640]  Вот.
[34:44.640 --> 34:49.640]  В принципе, уравнение, получающееся в результате B1 s0 равняется
[34:49.640 --> 34:53.640]  y0, называется в англоязычной теме аналогии second equation.
[34:53.640 --> 34:56.640]  Ну, на русский как хотите можно переводить более-менее
[34:56.640 --> 34:58.640]  там квазюмтовское уравнение, еще что-нибудь там.
[34:58.640 --> 34:59.640]  Вот.
[34:59.640 --> 35:00.640]  Отсюда надо найти B1.
[35:00.640 --> 35:02.640]  Кто понимает, какие тут возникают проблемы?
[35:02.640 --> 35:04.640]  Так, ну что, какие проблемы?
[35:04.640 --> 35:05.640]  Всегда ли можно решить?
[35:05.640 --> 35:07.640]  Сколько уравнений, сколько неизвестных?
[35:07.640 --> 35:08.640]  Единственное ли решение?
[35:08.640 --> 35:09.640]  Все как обычно.
[35:09.640 --> 35:10.640]  Думаете?
[35:10.640 --> 35:12.640]  Кажется, что сильно не единственное, потому что мы...
[35:12.640 --> 35:13.640]  Да, это правда.
[35:14.640 --> 35:15.640]  Прекрасно, да.
[35:15.640 --> 35:17.640]  Решение единственное, потому что переменные в порядке
[35:17.640 --> 35:20.640]  n2, хотя там немножко меньше, потому что симметрия есть,
[35:20.640 --> 35:21.640]  они единственные.
[35:21.640 --> 35:22.640]  Прекрасно.
[35:22.640 --> 35:23.640]  Всегда ли оно есть?
[35:23.640 --> 35:27.640]  Считывая, что матрицу B мы вот тут вот, что хотим?
[35:27.640 --> 35:28.640]  Чтобы...
[35:28.640 --> 35:29.640]  Нет.
[35:29.640 --> 35:32.640]  Чтобы B было равно Bt, и матрица B была бы положительно
[35:32.640 --> 35:35.640]  полуопределена, потому что это как бы, ну, гися
[35:35.640 --> 35:38.640]  нам приближает, поэтому очень хочется, чтобы этот
[35:38.640 --> 35:39.640]  свойств также отнаследовалось.
[35:39.640 --> 35:40.640]  Всегда ли будет решение?
[35:40.640 --> 35:41.640]  Решение.
[35:41.640 --> 35:43.640]  Все у таких вот неявных ограничений, я сказал.
[35:43.640 --> 35:44.640]  Вот хорошо.
[35:44.640 --> 35:45.640]  Что может пойти не так?
[35:45.640 --> 35:47.640]  Давайте исходить из того, что мы видим, и что может
[35:47.640 --> 35:48.640]  сломаться.
[35:48.640 --> 35:49.640]  Вижу, что идей особо нет.
[35:49.640 --> 35:50.640]  Ну, смотрите, что может сломаться.
[35:50.640 --> 35:52.640]  Сломаться может следующий момент, что если мы слева
[35:52.640 --> 35:56.640]  и справа умножим на S0, мы получим S0, B1, S0, а слева
[35:56.640 --> 35:57.640]  будет Y0, S0.
[35:57.640 --> 36:00.640]  И вот почему вот эта штука будет больше нуля, никто
[36:00.640 --> 36:01.640]  нам не сказал.
[36:01.640 --> 36:04.640]  То есть мы можем так промахнуться с выбором шага
[36:04.640 --> 36:07.640]  в 1, в 0, то вот это вот скалярное определение может стать
[36:07.640 --> 36:08.640]  отрицательным.
[36:08.640 --> 36:11.640]  Да, наша матрица B, она не положительно определена,
[36:11.640 --> 36:14.640]  потому что на векторе S0 квадратичная форма соответствующая
[36:14.640 --> 36:16.640]  принимает отрицательные значения.
[36:16.640 --> 36:17.640]  Нет ли проблем?
[36:17.640 --> 36:18.640]  Нет проблем.
[36:18.640 --> 36:19.640]  Хорошо.
[36:19.640 --> 36:22.640]  Чинится аккуратной настройкой размера шага вот на этапе
[36:22.640 --> 36:24.640]  обновления предыдущей точки.
[36:24.640 --> 36:26.640]  То есть на этом этапе мы альфа 0 подбираем не только
[36:26.640 --> 36:29.640]  так, чтобы значение функции уменьшилось, но и так, чтобы
[36:29.640 --> 36:31.640]  скалярное определение стало было положительным.
[36:31.640 --> 36:33.640]  Наши упражнения, проверьте, что при альфа на стоимящем
[36:33.640 --> 36:35.640]  сек нулю это будет выполнено.
[36:35.640 --> 36:37.640]  Или не будет выполнено, тогда можно будет обсудить
[36:37.640 --> 36:38.640]  в следующий раз, что ломает.
[36:38.640 --> 36:39.640]  Вот.
[36:39.640 --> 36:42.640]  Значит теперь, для того, чтобы зафорсить и заставить
[36:42.640 --> 36:45.640]  это решение, это уравнение иметь единственное решение,
[36:45.640 --> 36:47.640]  делают следующий шлюк.
[36:47.640 --> 36:50.640]  Говорят, что давайте мы будем решать вот такую задачу.
[36:50.640 --> 36:53.640]  То есть мы будем искать такую матрицу B из всех матриц,
[36:53.640 --> 36:56.640]  для которых наше уравнение выполнено, то она будет
[36:56.640 --> 37:00.640]  ближе всего к прошлой матрице, которая у нас уже была.
[37:00.640 --> 37:03.640]  Тем самым мы пытаемся сказать, что вот на этом графике у
[37:03.640 --> 37:06.640]  нас оценка гисяна в этой точке, оценка гисяна в этой
[37:06.640 --> 37:08.640]  точке, они не очень сильно начинают.
[37:08.640 --> 37:09.640]  Нет ли мотивации?
[37:09.640 --> 37:10.640]  Good.
[37:10.640 --> 37:13.640]  Значит, о чудо это все добро имеет аналитическое решение,
[37:13.640 --> 37:14.640]  вы не поверите.
[37:14.640 --> 37:16.640]  Я его сейчас запишу и мы немножко обсудим.
[37:16.640 --> 37:19.640]  Ну давайте даже, нет, давайте я сейчас не буду его
[37:19.640 --> 37:20.640]  записывать.
[37:20.640 --> 37:22.640]  Имеет аналитическое решение, пока так напишу.
[37:22.640 --> 37:23.640]  Вот.
[37:23.640 --> 37:24.640]  Значит мы к этому еще вернемся.
[37:24.640 --> 37:27.640]  Но перед тем, как кружаться в дебри, хочется простой
[37:27.640 --> 37:28.640]  метод рассказать.
[37:28.640 --> 37:29.640]  Называется метод Бразилая Бурвина.
[37:29.640 --> 37:30.640]  Вот.
[37:30.640 --> 37:32.640]  И очень все изящно, я надеюсь вы оцените.
[37:32.640 --> 37:35.640]  Посмотрите, когда мы рассматривали градиентный
[37:35.640 --> 37:37.640]  спуск, у нас было вот такое направление.
[37:37.640 --> 37:39.640]  Мы его можем переписать вот таким вот образом.
[37:39.640 --> 37:43.640]  И можем сказать, что это примерно равно нашему оценке
[37:43.640 --> 37:45.640]  на гисян минус первое ф штрих от х.
[37:45.640 --> 37:49.640]  То есть вот эта штука будет выполнять наш модель,
[37:49.640 --> 37:51.640]  выполнять роль модели нашего гисяна.
[37:51.640 --> 37:52.640]  Наша матрица B.
[37:52.640 --> 37:55.640]  Раз это матрица B, то для нее мы хотим получить квазинтонское
[37:55.640 --> 37:56.640]  уравнение.
[37:56.640 --> 37:57.640]  Ну вот check antiquation тут самый.
[37:57.640 --> 38:00.640]  То есть B на s ката равняется y катуум.
[38:01.640 --> 38:02.640]  Давайте подставим.
[38:02.640 --> 38:03.640]  Что такое?
[38:03.640 --> 38:05.640]  1 на альфа s ката равняется y катуум.
[38:05.640 --> 38:09.640]  То есть нам надо найти такую альфу, что x s ката был примерно
[38:09.640 --> 38:11.640]  равняется альфа на y ката.
[38:11.640 --> 38:12.640]  Как это делать?
[38:12.640 --> 38:15.640]  Это уравнение, ну в смысле уравнение, да, уравнение
[38:15.640 --> 38:17.640]  на альфа больше нуля.
[38:17.640 --> 38:19.640]  То есть это скалярное уравнение вообще.
[38:19.640 --> 38:20.640]  Как найти такой альфа?
[38:20.640 --> 38:21.640]  Ну и лучший.
[38:21.640 --> 38:22.640]  Есть ли понимание?
[38:22.640 --> 38:24.640]  У нас всегда были менее одинаковые подходы во всех
[38:24.640 --> 38:25.640]  таких случаях.
[38:25.640 --> 38:27.640]  В любой непонятной ситуации надо писать более менее
[38:27.640 --> 38:28.640]  одно и то же.
[38:28.640 --> 38:32.640]  Но нет, интуиция еще недостаточно развилась.
[38:32.640 --> 38:35.640]  Если нужно 1 вектор s данной приблизить к некоторым
[38:35.640 --> 38:38.640]  другим вектору, умноженным на неизвестное число.
[38:38.640 --> 38:41.640]  Такое число, чтобы приближение было максимально точно,
[38:41.640 --> 38:42.640]  то надо сделать.
[38:42.640 --> 38:44.640]  Ну частная норма, например.
[38:44.640 --> 38:45.640]  Чего-чего?
[38:45.640 --> 38:46.640]  Еще раз?
[38:46.640 --> 38:48.640]  Норму y1 поделить на норму sn.
[38:48.640 --> 38:50.640]  А почему так?
[38:50.640 --> 38:51.640]  Почему это будет наилучшая?
[38:51.640 --> 38:52.640]  Я скажу y10.
[38:52.640 --> 38:54.640]  То есть давайте я картинку нарисую.
[38:54.640 --> 38:57.640]  Вот у нас 1 вектор, s1, sn.
[38:57.640 --> 38:59.640]  И мы хотим приблизить его числом, умноженным на
[38:59.640 --> 39:00.640]  другой вектор.
[39:00.640 --> 39:02.640]  y1 и так далее yn.
[39:02.640 --> 39:03.640]  Как найти альфу?
[39:03.640 --> 39:05.640]  В каком ноль вы хотите лучшее приближение?
[39:05.640 --> 39:08.640]  Вот чтобы вот это равенство было как можно измерить.
[39:08.640 --> 39:09.640]  Давайте это измерим.
[39:09.640 --> 39:10.640]  Это прекрасный вопрос.
[39:10.640 --> 39:12.640]  Но можно измерить нормы разности.
[39:12.640 --> 39:13.640]  Отлично.
[39:13.640 --> 39:14.640]  Можно измерить нормы разности.
[39:14.640 --> 39:15.640]  Что дальше надо сделать?
[39:15.640 --> 39:16.640]  Решить задачу оптимизации.
[39:16.640 --> 39:18.640]  Ну гениально, конечно.
[39:18.640 --> 39:20.640]  В любой непонятной ситуации мы ставим какую-нибудь задачу
[39:20.640 --> 39:22.640]  оптимизации, которую благополучно можем решить.
[39:22.640 --> 39:25.640]  Эта штука минимизируется по не отрицательным альфу,
[39:25.640 --> 39:26.640]  квадраты.
[39:26.640 --> 39:28.640]  Давайте найдем градиент этой функции по альфу.
[39:28.640 --> 39:29.640]  Диктуйте.
[39:29.640 --> 39:31.640]  Это выпуклая задача, потому что у нас…
[39:31.640 --> 39:35.640]  Мне кажется минус 2 на то, что внутри скобка.
[39:35.640 --> 39:37.640]  Так, давайте одну вторую домножим, чтобы не страдать.
[39:37.640 --> 39:38.640]  Подожди.
[39:38.640 --> 39:39.640]  Сейчас.
[39:39.640 --> 39:41.640]  Минус ykt на то, что в скобках.
[39:41.640 --> 39:44.640]  Градиент должен быть числом, потому что аргумент
[39:44.640 --> 39:45.640]  числов.
[39:45.640 --> 39:46.640]  Так, вспоминаем, как считать градиент.
[39:46.640 --> 39:47.640]  В общем, полезным.
[39:47.640 --> 39:50.640]  Минус ykt транспонированное на то, что в скобках.
[39:50.640 --> 39:53.640]  Минус ykt транспонированное на то, что в скобках.
[39:53.640 --> 39:55.640]  Да, это похоже на правду.
[39:55.640 --> 39:59.640]  Это 0 при альфе, равный альфе со звездочкой.
[39:59.640 --> 40:00.640]  Давайте найдем альфу.
[40:00.640 --> 40:01.640]  Это же несложно.
[40:01.640 --> 40:05.640]  Получается, ykt транспонированное s-катой делить на норму
[40:05.640 --> 40:06.640]  ykt в квадрате.
[40:06.640 --> 40:07.640]  Чудо.
[40:07.640 --> 40:10.640]  Мы только что получили новый метод выбора шагов
[40:10.640 --> 40:11.640]  в градиентном спуске.
[40:11.640 --> 40:13.640]  Потому что мы начинали ровно с этого.
[40:13.640 --> 40:15.640]  Воспользовавшись методологией квадринтонских методов,
[40:15.640 --> 40:18.640]  подставив в квадринтонское уравнение нашу модель
[40:18.640 --> 40:22.640]  Дисяна, мы получили способ, который требует УАТН вообще
[40:22.640 --> 40:23.640]  вычислений.
[40:23.640 --> 40:24.640]  Все классно.
[40:24.640 --> 40:25.640]  И вот оказывается, что эта штука работает достаточно
[40:25.640 --> 40:26.640]  неплохо.
[40:26.640 --> 40:27.640]  Сейчас мы через некоторое время, я надеюсь, как раз
[40:27.640 --> 40:29.640]  успеем посмотреть, как именно она работает.
[40:29.640 --> 40:31.640]  Понятно ли, как получался метод?
[40:31.640 --> 40:32.640]  Куда он взялся?
[40:32.640 --> 40:33.640]  Каких соображений?
[40:33.640 --> 40:36.640]  Вот Андрею понятно, как там дела у остальных.
[40:36.640 --> 40:37.640]  Так, Дмитрий Гуд.
[40:37.640 --> 40:38.640]  Окей.
[40:38.640 --> 40:39.640]  Так, хорошо.
[40:39.640 --> 40:40.640]  Это была как бы простая история.
[40:40.640 --> 40:43.640]  Более сложная история, собственно, вот если немножко назад.
[40:43.640 --> 40:46.640]  Вот если идти по пути, в которой мы до этого как бы
[40:46.640 --> 40:47.640]  были.
[40:47.640 --> 40:51.640]  Говорили, что мы ищем такую B, которая близка, потом получаем
[40:51.640 --> 40:52.640]  из B, получаем H.
[40:52.640 --> 40:53.640]  То есть B-1.
[40:53.640 --> 40:55.640]  Там тоже все получается аналитически.
[40:55.640 --> 40:58.640]  И вот этот метод называется метод AFP по премиатурам,
[40:58.640 --> 41:00.640]  ну, по первым буквам имен автора.
[41:00.640 --> 41:05.640]  Вместо этого метод BFGLS по именам Бройден, Флейчер,
[41:05.640 --> 41:08.640]  Гольфар, Пшано, авторы, они предложили следующий трюк.
[41:08.640 --> 41:11.640]  Они говорят, а давайте мы вместо B будем минимизировать
[41:11.640 --> 41:13.640]  норму для сразу обратной матрицы.
[41:13.640 --> 41:17.640]  Ну, у нас задача перепишется как SKT равняется HYKT.
[41:17.640 --> 41:19.640]  Ну, и там понятно все симметрично.
[41:19.640 --> 41:22.640]  И вот у этой задачи тоже есть, ну, понятное, аналитическое
[41:22.640 --> 41:25.640]  решение, которое я сейчас попытаюсь записать.
[41:25.640 --> 41:27.640]  Мы с вами обсудим, как это все дело правильно считается.
[41:27.640 --> 41:30.640]  Это не самое, наверное, очевидное.
[41:30.640 --> 41:32.640]  Хотя, может быть, для вас это будет очевидно.
[41:32.640 --> 41:33.640]  Тут такое жуткое выражение.
[41:33.640 --> 41:35.640]  Сразу предупреждаю, чтобы вы не пугались.
[41:35.640 --> 41:39.640]  Это все выглядит жутко, но у существа ничего страшного
[41:39.640 --> 41:43.640]  и трофического, ну, ничего страшного и чего-либо,
[41:43.640 --> 41:46.640]  что предотвращало бы эффективное использование этого метода,
[41:46.640 --> 41:48.640]  тут как бы ничего не написано.
[41:48.640 --> 41:50.640]  И РОКАТ это вот такая штука.
[41:50.640 --> 41:52.640]  То есть, смотрите, мы получаем.
[41:52.640 --> 41:55.640]  То есть, у нас есть и S0, Y0 и H0.
[41:55.640 --> 41:58.640]  Мы эти штуки подставляем в эту формулу, получаем H1.
[41:58.640 --> 42:01.640]  Вопрос, как посчитать H1 за O от N квадрат?
[42:01.640 --> 42:02.640]  И можно ли это сделать?
[42:02.640 --> 42:03.640]  Это же была наша цель.
[42:03.640 --> 42:05.640]  Просто если делать все в лоб, то кажется, что тут,
[42:05.640 --> 42:08.640]  типа, умножение матрицы от N куб, все пропало.
[42:08.640 --> 42:09.640]  Мы с этим как-то побороться.
[42:09.640 --> 42:10.640]  Какие будут предложения?
[42:10.640 --> 42:12.640]  И вообще, видите ли вы эту проблему?
[42:12.640 --> 42:13.640]  Проблему побольше сказать?
[42:13.640 --> 42:14.640]  Да, нет.
[42:14.640 --> 42:15.640]  Что делать?
[42:15.640 --> 42:17.640]  Проблема видна в том, что можно H1 выразить, но,
[42:17.640 --> 42:19.640]  как вы уже сказали, это уже матрическое произведение.
[42:19.640 --> 42:20.640]  Хорошо.
[42:20.640 --> 42:21.640]  Да, отлично.
[42:21.640 --> 42:22.640]  Я рад, что видна проблема.
[42:22.640 --> 42:25.640]  Давайте думать и предлагайте идеи, как ее преодолеть.
[42:25.640 --> 42:27.640]  Значит, ответ кроме двух словах.
[42:27.640 --> 42:29.640]  Задача минимизации?
[42:29.640 --> 42:31.640]  Не тех двух словах.
[42:31.640 --> 42:32.640]  Нет, погодите.
[42:32.640 --> 42:33.640]  Тут уже мы ее решили.
[42:33.640 --> 42:34.640]  Понимаете, в чем проблема?
[42:35.640 --> 42:38.640]  Задача минимизации решается один раз.
[42:38.640 --> 42:40.640]  То есть, если мы тут уже получили некоторые решения,
[42:40.640 --> 42:42.640]  то дальше уже поздно.
[42:42.640 --> 42:44.640]  Что-то другое должно произойти.
[42:44.640 --> 42:46.640]  Ладно, один вопросов называется раскрыть скобки.
[42:46.640 --> 42:53.640]  Потому что если мы перепишем hk плюс 1 как k минус ro k s k y h
[42:53.640 --> 42:56.640]  минус ro k...
[42:56.640 --> 42:57.640]  на брат.
[42:57.640 --> 42:59.640]  Плюс ro k квадрат.
[42:59.640 --> 43:01.640]  Проверьте, правильно ли я раскрыл скобки?
[43:01.640 --> 43:02.640]  Да, нет.
[43:02.640 --> 43:03.640]  Выглядит правильно.
[43:03.640 --> 43:05.640]  Давайте теперь смотреть, что происходит.
[43:05.640 --> 43:07.640]  Понятно ли почему посчитать вот эту штуку?
[43:07.640 --> 43:08.640]  Это уатн квадрат.
[43:08.640 --> 43:10.640]  Умножить столбец на строку.
[43:10.640 --> 43:13.640]  При изведении двух матриц, одна из размеров из которых единица.
[43:13.640 --> 43:14.640]  Понятно.
[43:14.640 --> 43:15.640]  Да, нет.
[43:15.640 --> 43:16.640]  Вижу плюс, отлично.
[43:16.640 --> 43:17.640]  Едем дальше.
[43:17.640 --> 43:18.640]  Давайте теперь вот этим посмотрим.
[43:18.640 --> 43:20.640]  Значит, тут ro это число вообще.
[43:20.640 --> 43:21.640]  Это неважно.
[43:21.640 --> 43:23.640]  Мы его считаем за уатн.
[43:23.640 --> 43:24.640]  Скалярное произведение.
[43:24.640 --> 43:25.640]  Это легко.
[43:25.640 --> 43:28.640]  Дальше мы умножаем h на y за n квадрат.
[43:28.640 --> 43:30.640]  Умножаем вектор, столбец.
[43:30.640 --> 43:34.640]  Умножаем его же на sk транспонированное, что снова дает n квадрат.
[43:34.640 --> 43:38.640]  Тут как бы дальше надо скобки расставить правильно.
[43:38.640 --> 43:40.640]  У нас есть оптимизации.
[43:40.640 --> 43:41.640]  Первые два слова.
[43:41.640 --> 43:43.640]  Вторые два слова это раскрыть скобки.
[43:43.640 --> 43:45.640]  И еще два слова это расставить скобки.
[43:45.640 --> 43:47.640]  Чтобы все правильно быстро посчиталось.
[43:47.640 --> 43:48.640]  Ну и здесь та же самая история.
[43:48.640 --> 43:50.640]  Вот эта штука n квадрат.
[43:50.640 --> 43:52.640]  Это уатн.
[43:52.640 --> 43:53.640]  Число вообще получается.
[43:53.640 --> 43:56.640]  И вот это потом снова n квадрат.
[43:56.640 --> 43:59.640]  В итоге после раскрытия скобок у нас все операции происходят
[43:59.640 --> 44:00.640]  за уатн квадрат операцией.
[44:00.640 --> 44:02.640]  Ну за уатн квадрат все преобразования.
[44:02.640 --> 44:05.640]  Понятно ли каким образом нам удалось избежать сложности.
[44:05.640 --> 44:06.640]  Кубической сложности.
[44:06.640 --> 44:09.640]  Ну фокус мы с перемножением векторов в бестоматриц.
[44:09.640 --> 44:16.640]  Ну фокус в том, чтобы без формирования явного матрицы вида единицы плюс плюс там или минус ранг один.
[44:16.640 --> 44:18.640]  Вот который вот здесь вот.
[44:18.640 --> 44:23.640]  После раскрытия и последовательного перемножения получается, что мы просто умножаем матрицу на вектор.
[44:23.640 --> 44:26.640]  А потом матрицу ранг один явно формируем пока что.
[44:26.640 --> 44:30.640]  Потому что у нас в памяти вроде пока что мы можем все позвольте n квадрат.
[44:30.640 --> 44:32.640]  Боремся только с сложностью одной итерации.
[44:32.640 --> 44:35.640]  Ну все. Все или не все. Все ли понятно по этому методу.
[44:35.640 --> 44:37.640]  То есть общая логика такая.
[44:37.640 --> 44:39.640]  Вот мы здесь на этом этапе.
[44:39.640 --> 44:41.640]  Давайте еще раз напишу наверное.
[44:41.640 --> 44:43.640]  То есть у нас есть x0. Есть. Есть x0.
[44:43.640 --> 44:45.640]  Мы считаем x1.
[44:45.640 --> 44:47.640]  Потом считаем f' от x1.
[44:47.640 --> 44:48.640]  Потом считаем h1.
[44:48.640 --> 44:49.640]  Потом считаем x2.
[44:49.640 --> 44:51.640]  Потом считаем f' от x2.
[44:51.640 --> 44:55.640]  То есть тут последовательность выполнения шагов она немножко отличается от привычной.
[44:55.640 --> 44:59.640]  Потому что есть дополнительный шажочек со звездочкой, которое в отчислении h1.
[44:59.640 --> 45:00.640]  Шажочек плюс.
[45:00.640 --> 45:04.640]  Это вычление градиента не перед вычислением точки.
[45:04.640 --> 45:06.640]  А после вычления предыдущей точки.
[45:06.640 --> 45:08.640]  И в общем градиент посчитали.
[45:08.640 --> 45:09.640]  Посчитали h2.
[45:09.640 --> 45:11.640]  Ну и x3 соответственно потом пошел дальше.
[45:11.640 --> 45:14.640]  То есть тут немножко последовательность так хитро перекручивается.
[45:14.640 --> 45:16.640]  Понятно ли как это реализуется?
[45:16.640 --> 45:18.640]  То есть я псевдокод сейчас так набросал.
[45:18.640 --> 45:22.640]  Но я надеюсь что после там уже трех-пяти методов, которые мы изучили.
[45:22.640 --> 45:24.640]  Талант понятно как это кодится?
[45:24.640 --> 45:27.640]  Или есть еще не до конца понятно?
[45:27.640 --> 45:30.640]  Суть происходящего.
[45:30.640 --> 45:31.640]  Да, нет.
[45:31.640 --> 45:32.640]  Понятно.
[45:32.640 --> 45:33.640]  Понятно. Очень хорошо.
[45:33.640 --> 45:34.640]  Так здорово.
[45:34.640 --> 45:36.640]  Значит смотрите сейчас уже 10.23.
[45:36.640 --> 45:38.640]  Значит мы с вами разобрали какие методы сегодня.
[45:38.640 --> 45:40.640]  Ну DFP вкратце.
[45:40.640 --> 45:43.640]  ФГС чуть более подробно с анализом сложностей.
[45:43.640 --> 45:45.640]  И Бразилая Бурвейна тоже достаточно подробно.
[45:45.640 --> 45:47.640]  В следующий раз мы рассмотрим LBFGS.
[45:47.640 --> 45:51.640]  LBFGS собственно самый главный метод решения в большой размерности.
[45:51.640 --> 45:53.640]  Один из главных наряду с сопряженными градиентами.
[45:53.640 --> 45:54.640]  Вот.
[45:54.640 --> 45:58.640]  И пойдем уже там в максимальный градиентный метод.
[45:58.640 --> 46:00.640]  Там что у нас еще будет?
[46:00.640 --> 46:02.640]  И Франк Вольп.
[46:02.640 --> 46:04.640]  Это будет в следующий раз.
[46:04.640 --> 46:07.640]  Я надеюсь как раз вот эта штука не займет слишком много времени.
[46:07.640 --> 46:09.640]  Хотя там есть очень красивая идея.
[46:09.640 --> 46:12.640]  Но я постараюсь ее максимально непонятно и подробно прояснить.
[46:12.640 --> 46:13.640]  Так.
[46:13.640 --> 46:15.640]  Тогда на сегодня все.
[46:15.640 --> 46:17.640]  Большое спасибо за внимание и участие.
[46:17.640 --> 46:21.640]  В следующий раз переходим наконец-то к задачам с ограничениями.
[46:21.640 --> 46:23.640]  И посмотрим для каких задач.
[46:23.640 --> 46:26.640]  Какие важные операции типа проецирования.
[46:26.640 --> 46:29.640]  Типа вычисления минимума линейных функций на множестве.
[46:29.640 --> 46:31.640]  Как они могут помочь.
[46:31.640 --> 46:34.640]  Какие скорости сходимости будут у соответствующих методов.
[46:34.640 --> 46:36.640]  И почему это может быть быстрее.
[46:36.640 --> 46:38.640]  Ну то есть прям на примере посмотрим.
[46:38.640 --> 46:40.640]  Быстрее чем использование солвера.
[46:40.640 --> 46:43.640]  Который не знает ничего про структуру задач.
[46:43.640 --> 46:45.640]  И является таким General Purpose.
[46:45.640 --> 46:47.640]  Для общих целей.
[46:47.640 --> 46:48.640]  Все тогда.
[46:48.640 --> 46:49.640]  Большое спасибо.
[46:49.640 --> 46:50.640]  И до следующей недели.

[00:00.000 --> 00:16.000]  Здравствуйте, друзья. Как меня слышно? Слышно.
[00:16.000 --> 00:21.000]  Отлично. Так, ну давайте начинать.
[00:21.000 --> 00:32.000]  Ну мы в прошлый раз закончили с вами разговор по условным от ожиданиям, и сегодня таки перейдем к новой теме.
[00:32.000 --> 00:40.000]  Значит, у нас с вами впереди по плану всякие предельные законы, закон больших чисел, Центральная предельная теория.
[00:40.000 --> 00:44.000]  Ну короче говоря, сходимости, сходимости последовательств случайных величин.
[00:44.000 --> 00:52.000]  И давайте прежде чем приступить к вот этому завершающему блоку, поговорим вообще в принципе о последовательности случайных величин.
[00:52.000 --> 00:56.000]  То есть когда мы говорим про сходимость, у нас значит есть последовательство случайных величин.
[00:56.000 --> 01:01.000]  И последовательство случайных величин – это по сути случайный процесс. Что вообще такое случайный процесс?
[01:01.000 --> 01:11.000]  Это когда у вас есть прямое время, оно может быть дискретное, то есть время может быть дексировано натуральными числами, целыми числами.
[01:11.000 --> 01:17.000]  А может быть действительными числами. И в каждый момент времени у вас задана своя случайная величина.
[01:17.000 --> 01:22.000]  То есть по сути это некоторая динамика изменения случайной величины.
[01:22.000 --> 01:31.000]  Ну и вот простейший случай – это когда у вас время дискретно, и вы рассматриваете так называемое случайное блуждание.
[01:31.000 --> 01:35.000]  Причем самый простой случай – это простейшее симметричное случайное блуждание.
[01:36.000 --> 01:46.000]  Когда вы начинаете, ну скажем, из нуля, и в каждый момент времени вы шагаете с вероятности 1,2 вправо или влево, то есть на плюс единицу или на минус единицу.
[01:46.000 --> 01:51.000]  И в момент времени N у вас получается, что ваша случайная величина – это есть ваша позиция.
[01:51.000 --> 01:56.000]  То есть сумма N независимо случайных величин, которая принимает равновероятное значение 1 и минус 1.
[01:57.000 --> 02:06.000]  Вот. Давайте это дело формализуем, попробуем поизучать свойства такой последовательности.
[02:06.000 --> 02:09.000]  Ну и в частности поймом, как оно устроено в пределе.
[02:09.000 --> 02:15.000]  Будет такой естественный шаг, прежде чем поговорить в общем случае о пределах последовательств случайных величин.
[02:17.000 --> 02:20.000]  Так, значит, случайное блуждание.
[02:26.000 --> 02:36.000]  Ну, когда мы говорим про случайное блуждание, в принципе, это не обязательно случайное блуждание напрямой, оно не обязательно симметричное.
[02:36.000 --> 02:47.000]  То есть в общем случае у вас есть какое-то векторное пространство, допустим N, и вы в каждый момент времени сдвигаетесь на некоторый случайный вектор.
[02:47.000 --> 02:56.000]  Ну вот будем рассматривать только простейший случай, когда случайное блуждание напрямой, и мы сдвигаемся на единичку или на минус единичку.
[02:56.000 --> 03:12.000]  Значит пусть у нас есть независимый случайный величин, X1, X2 и так далее, независимый знак по распределённой случайной величины.
[03:12.000 --> 03:24.000]  И даже для простоты давайте читать, что они симметричны, то есть вероятность единички 1 на 2 и вероятность минус единички тоже вероятность равна 1 на 2.
[03:24.000 --> 03:49.000]  Ну рассмотрим такой процесс, такую последовательность случайной величины Sn, начнём с 0, то есть с 0 равно 0, константа, а дальше для любого N Sn это сумма первых N случайных величин.
[03:49.000 --> 03:59.000]  Такая последовательность называется простейшим симметричным случайным блужданием напрямой.
[04:19.000 --> 04:45.000]  В принципе дальнейшее, многое из того, что я буду говорить, можно будет применять для несимметричного случая, то есть когда у вас единичка минус единичка с разной вероятностью, ну или в принципе там можно шагать на разные значения вправо и влево, там вправо на 3, вправо на 2 и так далее.
[04:45.000 --> 04:55.000]  Но чтобы разобраться с методологией, давайте рассмотрим симметричный случай, гораздо больше всего интересного для этого случая можно доказать.
[04:55.000 --> 05:02.000]  Ну во-первых, давайте поймём какое распределение такого случайного блуждания, такого случайного процесса.
[05:02.000 --> 05:10.000]  И с этими словами зададимся вот к этому вопросу, пусть у нас есть какой-то момент времени N, чему равна вероятность того, что в момент времени N мы оказались в точке K?
[05:17.000 --> 05:26.000]  Ну понятно, что N плюс K должно быть чётным. Нельзя оказаться в момент времени N в такой точке K, что N плюс K нечётный.
[05:26.000 --> 05:34.000]  Если у вас нечётное количество шагов, то вы оказываетесь в нечётной точке. Если у вас чётное количество шагов, то вы оказываетесь в чётной точке.
[05:34.000 --> 05:44.000]  Это во-первых. Во-вторых, понятно, что вы не можете выйти выше за N и ниже за минус N. То есть по модулю K должно быть в пределах от N до минус N.
[05:44.000 --> 05:53.000]  Ну вот пусть K подходящий, то есть во-первых, N плюс K чётно, а во-вторых, по модулю K находится в пределах от минус N до N.
[05:56.000 --> 06:13.000]  Чему-то равна эта вероятность. Ну, смотрите, какая бы у вас ни была реализация, вот какую бы вы ни взяли конкретную последовательность шагов длины N,
[06:13.000 --> 06:22.000]  ну зафиксировали, в какие-то моменты вы сходили на единичку, в какие-то моменты на минус единичку. И вот пришли в K в конце.
[06:22.000 --> 06:32.000]  Понятно, что все эти шаги будут равномерноятными. Но у вас так как вероятность пойти вправо 1,2, вероятность пойти влево 1,2, то какой бы вы ни разом смотрели последовательность шагов,
[06:32.000 --> 06:40.000]  у него будет вероятность 1,2 в степени N. То есть у вас конкретный набор единичка минус единичек, и для конкретного набора его вероятность это 1,2 в степени N.
[06:40.000 --> 06:45.440]  Поэтому все способы прийти в момент k имеют одну и ту же вероятность 1-2 степени n.
[06:45.440 --> 06:51.040]  Поэтому чтобы посчитать эту вероятность, нужно умножить количество способов прийти в точку k на
[06:51.040 --> 06:57.760]  вероятность одной конкретной реализации, то есть на 1-2 степени n. Сколько всего есть способов прийти
[06:57.760 --> 07:05.760]  за время n в точку k? Ну понятно, сколько вам нужно сделать n плюс k пополам шагов вправо,
[07:05.760 --> 07:12.000]  и n минус k пополам шагов влево. То есть на самом деле вы решаете такую систему равней. Если у вас
[07:12.000 --> 07:19.640]  есть x шагов вправо и y шагов влево, то, во-первых, x плюс y должно быть равно n. Это
[07:19.640 --> 07:28.560]  общее количество шагов. А во-вторых, x минус y должно быть равно k. То есть значение, в которое
[07:28.560 --> 07:31.840]  вы пришли, это в точности x минус y. Количество шагов вправо минус количество шагов влево.
[07:31.840 --> 07:35.160]  Ну и решая его, получается, что x это n плюс k пополам.
[07:39.160 --> 07:46.240]  Поэтому вы придете в точку k тогда и только тогда, когда вы совершили ровно n плюс k пополам шагов
[07:46.240 --> 07:52.840]  вправо. А поэтому вероятность, поэтому количество способов это c из n по n плюс k пополам.
[07:52.840 --> 08:06.040]  И умножить на одну вторую степень n. Это если k хорошее. То есть, во-первых, модуль k меньше
[08:06.040 --> 08:17.120]  0, чем n. А во-вторых, k плюс n делится на два. Да, иначе вероятность просто ноль.
[08:17.120 --> 08:37.920]  Ну вот такое получается интересное распределение у наш ВСН. Ну и давайте, значит, когда мы говорим
[08:37.920 --> 08:41.600]  про случайный процесс, нам не только интересно распределение в каждый момент времени, нам
[08:41.600 --> 08:46.160]  интересно вот вся динамика. То есть смотреть на все распределения одновременно. То есть
[08:46.160 --> 08:52.320]  распределение случайного процесса целиком. Вот, давайте подумаем о том, как вообще устроены
[08:52.320 --> 08:59.240]  типичные траектории этого случайного процесса. Что вообще такое траектория? Значит, траектория
[08:59.240 --> 09:05.360]  это следующая вещь. Вы можете нарисовать такую графику. Давайте сфиксируем какой-то элементарный
[09:05.360 --> 09:09.640]  исход. У вас есть же вероятность на пространстве. Есть множество элементарных исходов. Вы можете
[09:09.640 --> 09:14.240]  взять какой-то совершенно конкретный элементарный исход, посчитать, посмотреть на значение ваших
[09:14.240 --> 09:18.200]  случайных величин на этом элементарном исходе. Вы получите детерминированную последовательность.
[09:18.200 --> 09:27.760]  С0 от Омега, с которой 0, конечно, равно. С1 от Омега это либо единичка, либо минус единичка. С2 от
[09:27.760 --> 09:31.840]  Омега это либо минус два, либо ноль, либо два и так далее. Да, вот получите как конкретную
[09:31.840 --> 09:40.640]  совершенно последовательность чисел. И вот она и называется траекторией. Траектория вашего
[09:40.680 --> 09:50.920]  случайного буждана. Вы можете нарисовать, грубо говоря, график этой траектории. Представить себе,
[09:50.920 --> 10:01.640]  как выглядит для вашего конкретного элементарного исхода, как выглядит траектория. Вы начинаете
[10:01.640 --> 10:07.280]  с нуля, потом вы идете, скажем, к единичку, потом вы можете пойти в два, потом можете спуститься,
[10:07.280 --> 10:14.920]  снова пойти вверх, несколько раз спуститься, нынче, ну и так далее. Да, вот любой такой график,
[10:14.920 --> 10:21.480]  это будет график траектории вашего случайного буждания. Понятно, что если мы берем какое-то
[10:21.480 --> 10:27.880]  маленькое количество шагов, способов нарисовать такую картину будет не так много. Но хочется понять,
[10:27.880 --> 10:33.360]  как выглядят траектории, когда шагов очень много. Как угодно, в принципе, могут выглядеть,
[10:33.360 --> 10:38.520]  но, наверное, есть какие-то типичные траектории, то есть те траектории, которые более вероятны,
[10:38.520 --> 10:44.840]  чем какие-то другие. Ну, скажем, слабо верится, что вы за н шагов пришли в точку n. Наверное,
[10:44.840 --> 10:49.600]  вы где-то будете пониже, чем n и повыше, чем минус n. Ну и вот насколько пониже, чем n,
[10:49.600 --> 10:55.600]  и насколько поменьше, чем минус n. Вообще, можно нарисовать какую-то область,
[10:55.600 --> 11:04.280]  оптимальную, внутри которой находятся почти все траектории вашего случайного буждания. Ну,
[11:04.280 --> 11:08.040]  что значит оптимальную? В том смысле, что если вы пытаетесь сузить, то у вас не получится. Ваше
[11:08.040 --> 11:13.160]  случайное буждание начнет находиться за пределами трудностей. Вот, и эту задачу можно решить. Мы
[11:13.160 --> 11:20.440]  сегодня об этом поговорим. Прежде чем поговорить, давайте решим вот такое вот полезное упражнение.
[11:20.920 --> 11:29.720]  В общем-то, это упражнение тоже важно для того, чтобы понять, какая у нас есть оптимальная область,
[11:29.720 --> 11:41.600]  в которой попадают все траектории, почти все траектории. Значит, упражнение. Спрашивается,
[11:41.600 --> 11:46.480]  какая вероятность того, что мы попали в точку k в момент времени n, и при этом ни разу ноль не
[11:46.480 --> 11:54.000]  пересекали. Ну, пусть k скажем положительно. Нас интересует вероятность того, что s1 больше 0,
[11:54.000 --> 12:03.520]  s2 больше 0 и так далее, sn – s1 больше 0, а в момент времени sn мы оказались в точке k.
[12:03.520 --> 12:10.040]  Ну, вот иными словами, раз у нас симметричный случай, мы хотим посмотреть, сколько бывает
[12:10.040 --> 12:15.480]  траекторий, которые приходят в точку k, но не пересекают. Наверное, многие знают, что речь идет
[12:15.480 --> 12:25.440]  практически с каталана. Давайте задачу быстренько решим. Как ее решить проще всего? Ну, смотрите,
[12:25.440 --> 12:32.880]  есть вот такой вот трюк. Во-первых, чтобы в момент времени 1 мы оказались в точке положительной,
[12:32.880 --> 12:36.720]  у нас есть только одна такая возможность – мы должны попасть в единицу. Так как мы в момент
[12:36.720 --> 12:40.440]  времени 1 можем оказаться либо в единицу, либо в минус единицу, но делать нечего, мы должны попасть
[12:41.000 --> 12:46.600]  в единицу. Ну, дальше вот есть у нас какой-то момент времени n, и мы, скажем, должны попасть в какую-то точку k.
[12:51.320 --> 12:55.240]  Воль не пересекай ни разу, но, кстати, понятно, что в момент времени 2 мы должны попасть, конечно,
[12:55.240 --> 13:04.960]  в точку 2. Но что нам здесь важно? Важно вот что, давайте задачу обратную рассмотрим, а сколько
[13:05.160 --> 13:10.040]  траекторий, которые пересекают 0? Да, то есть понятно, что количество траекторий,
[13:10.040 --> 13:14.320]  который не пересекает 0 – это количество всех траекторий, минус количество траекторий,
[13:14.320 --> 13:20.360]  которые пересекают 0, поэтому достаточно решить задачу для количества пересекающих траекторий.
[13:21.340 --> 13:30.120]  Вот. Ну 같 washing траекторий, который пересекает 0, может быть несколько раз. Вообще-то у нас 0.
[13:34.960 --> 13:42.320]  Давайте отметим момент первого пересечения с тулём и отразим нашу траекторию симметрично,
[13:42.320 --> 13:50.760]  начиная с этого момента относительно осепсис. То есть получим какую-то отражённую траекторию.
[13:50.760 --> 14:01.240]  Куда она пришла? Ну конечно она пришла в точку минус K.
[14:05.640 --> 14:09.960]  Понятно, что между всеми траекториями, которые пересекают 0 приходят в точку K,
[14:09.960 --> 14:13.740]  и всеми траекториями, которые ходят в точку минус K... FREE versa
[14:13.740 --> 14:17.460]  Раз уж пришла в точку минус, я имею в виду вот отсюда.
[14:17.460 --> 14:23.380]  это старцевая точка наших всех траекторий. Из единицы вышли. Так вот, значит, количество траекторий,
[14:23.380 --> 14:28.580]  которые пересекают ноль, приходит в точку k, и количество траекторий, которые приходят в точку
[14:28.580 --> 14:32.980]  минус k, оно просто одинаковое. Просто потому что, чтобы прийти из единицы в точку минус k, вам
[14:32.980 --> 14:39.100]  надо хотя бы один раз ноль пересечь. А значит, если вы отображаете относительно первого момента
[14:39.100 --> 14:44.940]  пересечения с нолем, вы получаете биекцию. У вас в обе стороны отображение работает. Поэтому
[14:44.940 --> 15:01.500]  количество траекторий, которые... Давайте это как-то обозначим. Пусть, скажем, n с индексами
[15:01.500 --> 15:08.940]  AB от XY, это количество траекторий,
[15:08.940 --> 15:19.900]  или давайте AB здесь уберем, n маленько напишем. Это количество траекторий,
[15:19.900 --> 15:30.380]  которые за время n приходят из точки x, точку y.
[15:30.380 --> 15:51.140]  Приходит из x выпрек. Ну и, скажем, n с волной то же самое, но еще есть дополнительное
[15:51.140 --> 15:59.380]  условие, что ни разу нельзя пересечь ноль. Пусть n с волной или, наоборот, можно пересечь сейчас.
[15:59.380 --> 16:04.660]  То есть, наоборот, можно пересечь. Значит, n с волной, n от XY, это количество траекторий,
[16:04.660 --> 16:12.860]  количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают
[16:12.860 --> 16:14.860]  ноль. Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:14.860 --> 16:16.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:16.860 --> 16:18.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:18.860 --> 16:20.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:20.860 --> 16:22.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:22.860 --> 16:24.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:24.860 --> 16:26.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:26.860 --> 16:28.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:28.860 --> 16:30.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:30.860 --> 16:32.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:32.860 --> 16:34.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:34.860 --> 16:36.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:36.860 --> 16:38.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:38.860 --> 16:40.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:40.860 --> 16:42.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:42.860 --> 16:44.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:44.860 --> 16:46.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:46.860 --> 16:48.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:48.860 --> 16:50.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:50.860 --> 16:52.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:52.860 --> 16:54.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:54.860 --> 16:56.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:56.860 --> 16:58.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[16:58.860 --> 17:00.860]  Это количество траекторий, которые за время n приходят из x выпрек, и хотя бы один раз пересекают ноль.
[17:00.860 --> 17:02.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:02.860 --> 17:04.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:04.860 --> 17:06.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:06.860 --> 17:08.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:08.860 --> 17:10.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:10.860 --> 17:12.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:12.860 --> 17:14.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:14.860 --> 17:16.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:16.860 --> 17:18.860]  Мы смотрим на количество траекторий, которые за время n-1.
[17:18.860 --> 17:20.860]  Мы смотрим на количество траекторий, которые за время n-1.
[17:20.860 --> 17:22.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:22.860 --> 17:24.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:24.860 --> 17:26.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:26.860 --> 17:28.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:28.860 --> 17:30.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:30.860 --> 17:32.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:32.860 --> 17:34.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:34.860 --> 17:36.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:36.860 --> 17:38.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:38.860 --> 17:40.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:40.860 --> 17:42.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:42.860 --> 17:44.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:44.860 --> 17:46.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:46.860 --> 17:48.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:48.860 --> 17:50.860]  На что мы смотрим? Мы смотрим на количество траекторий, которые за время n-1.
[17:50.860 --> 17:56.860]  Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
[17:56.860 --> 17:58.860]  Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
[17:58.860 --> 18:00.860]  Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
[18:00.860 --> 18:02.860]  Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
[18:02.860 --> 18:04.860]  Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
[18:04.860 --> 18:06.860]  Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
[18:06.860 --> 18:08.860]  Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
[18:08.860 --> 18:10.860]  Мы вместо n должны n-1 подставить, а вместо k должны подставить –k-1.
[18:10.860 --> 18:26.860]  С другой стороны, вот эта n с волной – это количество всех траекторий, минус количество тех траекторий, которые ноль не пересекают.
[18:40.860 --> 18:48.860]  Теперь смотрите, наша искомая вероятность. Мы ее теперь можем посчитать.
[18:56.860 --> 19:04.860]  Понятно, что нужно одну вторую степень n умножить просто на количество траекторий.
[19:04.860 --> 19:08.860]  Но количество траекторий я посчитал.
[19:08.860 --> 19:20.860]  Это количество траекторий за время n-1 из единиц в k, минус c из n-1, потом n-k пополам минус 1.
[19:20.860 --> 19:24.860]  Но не это надо умножить на 2.
[19:24.860 --> 19:36.860]  Тут надо внимательно помнить о том, что мы в первый момент времени шагнули в точку 1.
[19:36.860 --> 19:40.860]  Мы могли здесь в знаменателе написать одну вторую степень n-1, но это было неправильно,
[19:40.860 --> 19:44.860]  потому что у нас еще есть первый шаг, который мы делали в связи с 1-2.
[19:44.860 --> 19:50.860]  Давайте это приведем к какому-то более приятному виду.
[19:50.860 --> 19:56.860]  Количество траекторий из единиц в точку k мы умеем считать.
[19:56.860 --> 20:10.860]  c из n-1 по n-1 плюс k-1 пополам, минус c из n-1 по n-k пополам минус 1.
[20:10.860 --> 20:22.860]  Можно еще от минус единички избавиться и переписать это в виде c из n-1 по n-k пополам,
[20:22.860 --> 20:28.860]  минус c из n-1 по n-k пополам.
[20:28.860 --> 20:42.860]  Почему мы написали, где c из n-1 на степени n-1 плюс k-1?
[20:42.860 --> 20:50.860]  Это вот эта величина, это количество траекторий за время n-1 из точки единицы в точку k.
[20:50.860 --> 21:00.860]  То есть мы поднялись вверх на k-1. Мы к количеству шагов должны прибавить то, насколько мы вверх поднялись.
[21:00.860 --> 21:04.860]  Мы поднялись вверх на k-1, поэтому здесь n-1 плюс k-1 пополам.
[21:04.860 --> 21:12.860]  В предыдущем случае мы опускались вниз на k-1, и поэтому у нас было минус k-1.
[21:13.860 --> 21:17.860]  Это разные n и n-s-1.
[21:17.860 --> 21:19.860]  Но вот это...
[21:19.860 --> 21:21.860]  Просто там вот раньше стоит...
[21:21.860 --> 21:25.860]  Одно, вот это другое.
[21:25.860 --> 21:37.860]  Я чтобы найти количество непересекающихся путей, я вот из вот этой штуки вычел предыдущую и сюда просто переписал.
[21:42.860 --> 21:47.860]  Из количества траекторий из единиц в k я вычел количество траекторий из единиц в n-1.
[21:54.860 --> 21:56.860]  Так, хорошо.
[21:57.860 --> 21:59.860]  Есть ли какие-то вопросы?
[22:02.860 --> 22:07.860]  То есть просто вроде все-таки мы как-то одинаково обозначаем, да?
[22:07.860 --> 22:09.860]  И это разная вещь.
[22:09.860 --> 22:11.860]  Это кто?
[22:11.860 --> 22:15.860]  Ну, n и n-s-1 от единицы k.
[22:15.860 --> 22:18.860]  Просто это же одно и то же, да?
[22:18.860 --> 22:22.860]  Одно и то же, как и от единицы минус k?
[22:22.860 --> 22:25.860]  Ну, там просто окраина стоит, не знаю.
[22:25.860 --> 22:28.860]  Нет, не стоит, тут еще минус.
[22:28.860 --> 22:34.860]  А, все понятно. Все, спасибо. Я понял. Я думал, это тире. Спасибо.
[22:34.860 --> 22:44.860]  Я из этого вычислил как раз количество непересекающихся, непересекающих ноль траекторий, как правое выражение, минус левое.
[22:44.860 --> 22:49.860]  Откуда у меня здесь эта разность появилась, да? Я вычел из этой штуки вот эту штуку.
[22:52.860 --> 22:54.860]  Еще вопросы?
[23:04.860 --> 23:08.860]  Так, окей. Хорошо.
[23:08.860 --> 23:15.860]  Значит, из этого можно получить важное следствие, которое как раз вот там применяется в так называемом законе повторного логарифма.
[23:15.860 --> 23:19.860]  Это то само утверждение про то, в какой области лежат почти все траектории.
[23:19.860 --> 23:28.860]  Давайте возьмем, обозначим за m большой от m максимум.
[23:29.860 --> 23:40.860]  Обозначим за m большой от m максимум по всем k от нуля до m sk.
[23:44.860 --> 23:51.860]  Ну и посмотрим на распределение, на распределение этой случайной величины, а именно найдем вероятность того, что mn хотя бы х.
[23:51.860 --> 23:58.860]  Предлагается gch это событие с другим и с его дополнением.
[23:58.860 --> 24:01.860]  Представьте вероятность в виде сумм по деятельности.
[24:01.860 --> 24:06.860]  А именно вероятность того, что mn больше уночим х и sn больше уночим х.
[24:10.860 --> 24:17.860]  Плюс вероятность того, что mn больше уночим х и sn больше уночим х.
[24:17.860 --> 24:23.860]  Значит первая вероятность совместная, в ней одно условие сильнее, чем другое.
[24:23.860 --> 24:30.860]  Понятно, что если sn больше уночим х, если в момент времени вы оказались выше х, то и максимум под up.
[24:30.860 --> 24:32.860]  Может где-то там было еще больше.
[24:32.860 --> 24:38.860]  Поэтому условие sn больше уночим х сильнее, чем условие mn больше уночим х.
[24:38.860 --> 24:52.860]  А по так называемому принципу отражения, вот смотрите, что мы только что с вами на самом деле не явно доказали, когда мы вот эту формулу получили про вероятность того, что mn больше уночим х и sn больше уночим х.
[24:52.860 --> 25:11.860]  А по так называемому принципу отражения, вот смотрите, что мы только что с вами на самом деле не явно доказали, когда мы вот эту формулу получили про вероятность того, что мы в момент времени n оказались в точке k, не пересекая ноль.
[25:11.860 --> 25:20.860]  Мы с вами обратили внимание на то, что количество траекторий, которые ведут в точку k, пересекая ноль.
[25:20.860 --> 25:27.860]  И количество траекторий, которые ведут в точку минус k, не пересекая ноль, одинаковое.
[25:27.860 --> 25:40.860]  И по аналогичному соображению, вот эта вторая вероятность, она в точности равна вероятности того, что sn больше чем х.
[25:40.860 --> 25:43.860]  Я сейчас это еще раз поясню.
[25:43.860 --> 25:52.860]  Смотрите, еще раз навесим траекторию.
[25:52.860 --> 25:59.860]  Вот представьте, что есть какой-то уровень х.
[25:59.860 --> 26:04.860]  И есть там момент времени n.
[26:04.860 --> 26:10.860]  Мы смотрим на вероятность того, что sn меньше чем х.
[26:10.860 --> 26:15.860]  А при этом максимум больше. То есть вы как-то там шли.
[26:18.860 --> 26:23.860]  И в какой-то момент вы были выше х, но в момент времени n оказались ниже.
[26:23.860 --> 26:35.860]  Давайте возьмем первый момент пересечения с х и отразим нашу траекторию, начиная с этого момента относительно прямой параллельной оси абсциссы, проходящей через точку х.
[26:35.860 --> 26:40.860]  Продолжение траектории будет вот таким.
[26:40.860 --> 26:44.860]  Продолжение траектории...
[26:44.860 --> 26:48.860]  А нет, я же должен пересечь здесь.
[26:48.860 --> 26:53.860]  Вот так вот я должен сюда прийти, а потом прийти сюда.
[26:53.860 --> 26:58.860]  Это симметричная траектория. Мы попадаем в точку симметричной траектории.
[26:58.860 --> 27:03.860]  И мы видим, что у нас есть вот эта точка.
[27:03.860 --> 27:07.860]  Я должен сюда прийти, а потом прийти сюда.
[27:07.860 --> 27:12.860]  Это симметричная траектория. Мы попадаем в точку симметричную относительно точки х.
[27:12.860 --> 27:16.860]  То есть она лежит относительно прямой х.
[27:16.860 --> 27:19.860]  То есть она лежит выше этого уровня х.
[27:19.860 --> 27:26.860]  Из этого поярное дело следует, что количество траекторий, которые идут выше чем х, то есть таких, что в момент времени n оказались выше х,
[27:26.860 --> 27:32.860]  совпадает с количеством траекторий, которые ниже чем х, но при этом в какой-то момент пересекают х.
[27:32.860 --> 27:39.860]  То есть, раз траектория одинакова, то вероятность совместная того, что m ≥ x и Sn ≥ x,
[27:39.860 --> 27:44.860]  она просто совпадает с вероятностью того, что Sn ≥ x.
[27:44.860 --> 27:51.860]  Ну и для удобства давайте это перепишем как две вероятности того, что Sn ≥ x
[27:51.860 --> 28:00.860]  минус вероятность того, что Sn ≥ x.
[28:00.860 --> 28:09.860]  Вот. Назовем это дело утверждение 1. Это утверждение, которое будет использоваться для доказательства
[28:09.860 --> 28:14.860]  закона по второму логарифму. Мы его явно доказывать не будем, но я объясню идею.
[28:14.860 --> 28:25.860]  Значит, вероятность того, что m ≥ x равна две вероятности того, что Sn ≥ x.
[28:25.860 --> 28:31.860]  Ну или на самом деле, что то же самое в силу симметрии, это то же самое, что вероятность того, что m ≥ x.
[28:31.860 --> 28:38.860]  В силу симметрии вероятность того, что Sn ≥ x и вероятность того, что Sn ≤ x совпадает.
[28:38.860 --> 28:41.860]  Поэтому я здесь могу убрать двойку и написать вместо этого m.
[28:41.860 --> 28:43.860]  Минус вероятность того, что Sn ≥ x.
[28:43.860 --> 28:52.860]  Для каких-то больших n, какое бы ни было x, вероятность того, что Sn ≥ x очень маленькая.
[28:52.860 --> 28:59.660]  то есть в пределе распределение максимума оно такое же, как распределение модуля Sn.
[29:01.740 --> 29:06.700]  Если мы хотим в какой-то причине смотреть на распределение максимума, то мы можем при
[29:06.700 --> 29:14.220]  достаточно больших m заменить его на распределение модуля Sn. Вот это вот первая мысль,
[29:16.060 --> 29:20.060]  которая будет использоваться для доказательств закона повторного логарифма, который я сейчас
[29:20.060 --> 29:27.260]  сформулирую. Что же такое закон повторного логарифма? Это утверждение о том закон повторного
[29:27.260 --> 29:35.060]  логарифма, ZPL сокращённый. Закон повторного логарифма. Это очень довольно тяжело доказывающиеся
[29:35.060 --> 29:40.380]  утверждение о том, как раз в какой области лежат почти все траектории случайного блуждания.
[29:40.380 --> 29:49.980]  А ещё он звучит вот так. Вероятность того, что верхний предел при n стремящемся бесконечности
[29:49.980 --> 30:07.860]  Sn поделить на корень из 2n log-log-n равна единице, равна единице. Давайте попробуем
[30:07.860 --> 30:18.100]  проинтерпретировать, понять, что означает это утверждение. Верхний предел некоторой последовательности
[30:18.100 --> 30:22.780]  равна единице с вероятностью 1. Ну то есть это означает, что что значит вероятность 1? То есть
[30:22.780 --> 30:29.900]  для почти всех траекторий. Вероятельностная мера тех траекторий, для которых это свойство верно,
[30:29.900 --> 30:35.300]  она равна единице. Для почти всех траекторий это верно. Что же верно? А верно следующее, что вот если
[30:35.300 --> 30:42.100]  вы возьмёте траекторию и для неё рассмотрите вот эту вот последовательность Sn поделить на корень из
[30:42.100 --> 30:47.060]  2n log-log-n, то есть отномируете некоторым правильным образом траекторию, то вы получите, что верхний
[30:47.060 --> 30:51.700]  предел такой последовательности равен единице. То есть иными словами, что значит верхний предел равен
[30:51.700 --> 30:57.460]  единице? Это значит, что ваша последовательность, если вы чуть-чуть отойдёте от единицы, возьмёте
[30:57.460 --> 31:09.300]  1+, то значит ваша последовательность, она может быть больше, чем 1+, лишь конечное количество раз. Это
[31:09.300 --> 31:13.620]  во-первых, а во-вторых, если спустите чуть-чуть вниз, то есть рассмотрите 1-эпсилон, то это будет
[31:13.620 --> 31:19.900]  означать, что бесконечное количество раз ваша последовательность больше, чем 1-эпсилон. То есть
[31:19.900 --> 31:26.220]  это очень точное утверждение, которое говорит о том, что траектории должны быть следующей области.
[31:26.220 --> 31:28.300]  Давайте ещё раз начисуем картинку.
[31:39.180 --> 31:42.420]  Ну вот, можно нарисовать такую кривую.
[31:42.420 --> 31:55.020]  Это будет 1+, умножить на корень из 2n лог-лог-н.
[32:03.780 --> 32:06.300]  И можно ещё нарисовать то же самое, только 1-эпсилон.
[32:13.060 --> 32:15.500]  1-эпсилон на корень из 2n лог-лог-н.
[32:19.500 --> 32:29.060]  Ну, траектория симметричная в смысле своих свойств. Вот, поэтому снизу будет такая же картинка.
[32:29.060 --> 32:42.500]  Это 1-эпсилон, это минус 1+, на корень из 2n лог-лог-н.
[32:50.260 --> 32:57.540]  И чуть выше минус 1-эпсилон.
[32:59.060 --> 33:08.740]  Вот, и что нам говорит закон по второму логарику? Мы говорим, что если возьмём траекторию типичную,
[33:08.740 --> 33:13.860]  то есть те траектории, которые попадают в множество мира 1, то она будет бесконечно много раз
[33:13.860 --> 33:17.580]  пересекать чёрную кривую и только конечное количество раз пересекать синюю.
[33:30.060 --> 33:36.540]  Вот чёрную кривую она может пересекать бесконечно много раз, то есть чем дальше вы идёте по времени и вы
[33:36.540 --> 33:40.380]  всё будете находить всё новые и новые точки пересечения с чёрной кривой, а синюю кривой
[33:40.380 --> 33:46.220]  в какой-то момент она перестанет пересекаться. Вот что говорит закон по второму логарику.
[33:46.220 --> 33:55.620]  Мы его оставим без доказательств, но схема доказательств я поистину, то есть там тяжёлая
[33:55.620 --> 34:01.460]  техника, поэтому давайте ограничимся основными утверждениями, которые используются для его
[34:01.460 --> 34:05.100]  доказательств и методами доказательств. Доказательство основывается на лиме Борреля
[34:05.100 --> 34:07.540]  Контель. И что такое лима Борреля Контель?
[34:25.620 --> 34:44.660]  Давайте рассмотрим произвольную последовательность событий. Пусть 1, 2 и так далее, это какая-то
[34:44.660 --> 34:49.940]  бесконечная, счётная последовательность событий из нашего множества событий.
[34:49.940 --> 35:01.620]  Давайте рассмотрим событие, которое будет обозначаться вот так АНБЧ, и произносится это
[35:01.620 --> 35:09.140]  будет как АН бесконечно часто. То есть это есть множество тех элементарных исходов, которые
[35:09.140 --> 35:18.780]  попадают в бесконечно много событий. То есть как это определить в термах пересечений и объединений
[35:18.780 --> 35:28.060]  множеств? Как это определить в термах пересечений и объединений множеств? Но это надо, значит,
[35:28.060 --> 35:36.020]  всё объединить, а потом по одному выбрасывать. Если у вас элементарное событие содержится в
[35:36.020 --> 35:43.820]  бесконечном многих, то оно в таком множестве будет содержаться. То есть это есть пересечение по
[35:43.820 --> 35:52.520]  всем АН от единицы до бесконечности, объединений по всем К больше ноль, чем АН, а КАТы.
[35:59.180 --> 36:04.260]  Вот, значит, мы обозначаем, рассмотрим вот такое событие.
[36:04.260 --> 36:15.460]  Значит, тогда, первое, если сумма вероятностей АН меньше бесконечности,
[36:19.820 --> 36:27.860]  то вероятность того, что АН бесконечно часто, равна нулю.
[36:27.860 --> 36:51.660]  И второе, если сумма равна бесконечности и к тому же событие независимое в совокупности,
[36:51.660 --> 37:01.660]  то вероятность АН бесконечно часто равна единице.
[37:13.780 --> 37:17.420]  Так, ну это просто утверждение. Давайте быстренько докажем.
[37:21.660 --> 37:34.540]  Что такое вероятность АН бесконечно часто? Ну, это вероятность вот этого нашего пересечения объединений.
[37:43.140 --> 37:49.100]  Давайте посмотрим на события внутри первого пересечения. Давайте посмотрим на эти вот события.
[37:52.660 --> 37:59.660]  Они вложены. Да, это что такое? Вот мы взяли Н равно единице, тогда мы объединили вообще все А, все события 1, а 2 и так далее.
[37:59.660 --> 38:06.660]  Потом взяли Н равно 2, и значит новое объединение будет отличаться от предыдущего тем, что мы не объединяем с А1.
[38:06.660 --> 38:12.660]  Вот то А1, которое не содержится во всем объединении, мы его выкинули. Потом А2 выкинули, то МА3.
[38:12.660 --> 38:18.660]  То есть сужается вот это объединение, оно сужается. То есть это система вложенных событий.
[38:19.660 --> 38:32.660]  И по непрерывности вероятностной меры эта штука равна пределу приенстремящимся бесконечности, вероятности объединения пока больше Н чем А, окатых.
[38:32.660 --> 38:46.660]  А в свою очередь эта вероятность, как мы знаем, не превосходит сумма вероятностей.
[38:46.660 --> 39:09.660]  Ну то есть это остаточный член нашего ряда. А раз ряд сходится, то остаточный член стремится к нулю с Ростомэн, что и требовалось.
[39:09.660 --> 39:18.660]  А там вероятность, Окадович. Ой, конечно, спасибо большое.
[39:26.660 --> 39:28.660]  Хорошо, теперь второй пункт.
[39:33.660 --> 39:34.660]  Теперь второй пункт.
[39:40.660 --> 39:44.660]  Ну давайте перейдем к дополнению. То есть возьмем вероятность АН бесконечности часто.
[39:49.660 --> 39:54.660]  И напишем, что это единица минус вероятность дополнения.
[39:55.660 --> 40:07.660]  Ну то есть на самом деле вероятность объединения, вероятности единицкой бесконечности, пересечений, пока больше Н чем А, пока с чертвой.
[40:07.660 --> 40:29.660]  Ну опять, если я посмотрю на эти внутренние события, то эти события, наоборот, расширяются.
[40:37.660 --> 40:52.660]  Значит, я увеличиваю N и, соответственно, уменьшаю количество множеств, которые пересекаются. Чем меньше у меня множество пересекается, тем больше события.
[40:52.660 --> 41:03.660]  Значит, это система расширяющихся событий вложенных. И поэтому, опять, потеряемая непрерывность вероятностной меры, это есть просто предел вероятности, понимаете?
[41:03.660 --> 41:15.660]  Ну и давайте посмотрим на эту вероятность по знакам предела.
[41:15.660 --> 41:27.660]  Ну здесь нам, конечно, помогает независимость. По независимости это просто произведение про всем К больше Н чем N.
[41:28.660 --> 41:37.660]  Вероятности окатая с чертвой. Ну то есть единицы минус вероятности окатая.
[41:37.660 --> 41:47.660]  Ну дальше стандартный трюк. Когда вы знаете, что сумма равна бесконечности, и вам нужно посмотреть на произведение, вам нужно, конечно, произведение таких вероятностей это ноль,
[41:47.660 --> 41:57.660]  но вы можете использовать следующий стандартный трюк. Вот вы знаете про сумму вероятностей, а вам нужно посмотреть произведение единиц, единиц минус вероятностей, как от этого произведения перейти к сумме.
[41:57.660 --> 42:16.660]  Очень просто. Нужно взять экспонент от логарифма, то есть это будет экспонент от суммы логарифмов.
[42:16.660 --> 42:20.660]  Ну и заметьте, что логарифм единицы минус х меньше, чем минус х.
[42:20.660 --> 42:31.660]  Значит, по монотонности экспонента логарифма мы получаем экспоненту от суммы минус вероятности окатых.
[42:31.660 --> 42:50.660]  Так как весь ряд расходится, то и его часть, его остаточный счет тоже расходится, то есть сумма этих вероятностей равна минус бесконечности, а значит экспонента равна нулю.
[42:50.660 --> 43:06.660]  То есть это ноль, и значит наша исходная вероятность равна единице.
[43:06.660 --> 43:18.660]  Хочу обратить ваше внимание на то, что даже к пределу переходить не надо, какой бы вы ни взяли n, вероятность такого бесконечного пресечения равна нулю.
[43:18.660 --> 43:32.660]  Есть ли какие-то вопросы?
[43:32.660 --> 43:42.660]  Ну, значит, теперь вопрос у меня. Как применять? Я сказал, что эту лему можно применить для доказательств законоповторного логарифма.
[43:42.660 --> 43:49.660]  Как это сделать? В законоповторном логарифме написан верхний предел ранее единицы с вероятностью 1.
[43:49.660 --> 43:58.660]  Причем тут лему бороли контель. Но на самом деле вот это утверждение о том, что верхний предел чему-то равен, это как раз про то, что какое-то событие выполнено бесконечно часто.
[43:58.660 --> 44:06.660]  Помните же, я говорил, что если прибавим к единичке epsilon, то окажется, что не бесконечно часто мы пересекаем этот уровень.
[44:06.660 --> 44:12.660]  А если вычтем из единички epsilon, то окажется, что бесконечно часто пересекаем этот уровень.
[44:12.660 --> 44:24.660]  И давайте для доказательства законоповторного логарифма, точнее для демонстрации того, как можно доказать законоповторного логарифма, мы решим такую задачу.
[44:25.660 --> 44:29.660]  Она будет гораздо проще, чем законоповторный логарифм с силой независимости.
[44:29.660 --> 44:35.660]  Вот когда вы смотрите законоповторный логарифм, у вас там стоит под пределом, случайно, личная Sn.
[44:35.660 --> 44:40.660]  Если вы меняете n, Sn, конечно, зависимы. Причем зависимы довольно сильно.
[44:40.660 --> 44:44.660]  Например, Sn от Sn плюс 1 отличается всего на единицу.
[44:44.660 --> 44:50.660]  Поэтому в явном виде применять второй пункт лему бороли контеля у вас не получится.
[44:50.660 --> 44:59.660]  Но, тем не менее, применяется именно лему бороли контеля, только перед тем, как ее применить, немножко пострадать.
[44:59.660 --> 45:06.660]  Но давайте просто чтобы вы увидели, что это в точности условия лему бороли контеля, не считая независимости.
[45:06.660 --> 45:10.660]  Я решу немного другую задачу, но в таких же термах сформулированную.
[45:21.660 --> 45:31.660]  Пусть у вас есть последовательность независимых случайных величин, которые имеют экспоненциальное распределение с параметром 1.
[45:31.660 --> 45:47.660]  Необходимо доказать, что вероятность того, что верхний предел при этом стремящемся бесконечности
[45:47.660 --> 46:01.660]  Xn поделить на лог n равен единице. С вероятностью 1.
[46:01.660 --> 46:10.660]  Ну то есть, вот вы, да, интересны такое наблюдения. Если вы смотрите на то, как ведет себя последовательность экспоненциальных случайных величин,
[46:10.660 --> 46:17.660]  то вот траектория этой последовательности, она лежит на том, что у вас есть последовательность,
[46:17.660 --> 46:25.660]  Ну, то есть, вот вы, да, интересно такое наблюдение, если вы смотрите на то, как ведет себя последовательность
[46:25.660 --> 46:31.660]  экспоненциальных случайных величин, то вот траектория этой последовательности, она лежит в области
[46:31.660 --> 46:37.660]  между logn и –logn, да, и она оптимальна в смысле, что если чуть уменьшите, то уже за эту область будет выходить.
[46:37.660 --> 46:42.660]  Давайте это докажем.
[46:42.660 --> 46:48.660]  Ну, мы хотим доказать две вещи.
[46:48.660 --> 47:02.660]  Первая вещь – это то, что с вероятностью 1 верхний предел меньше, если брать на 1.
[47:02.660 --> 47:07.660]  Ну, а второе – это то, что он больше, если брать на 1.
[47:07.660 --> 47:21.660]  Что значит верхний предел меньше, если брать на 1?
[47:21.660 --> 47:32.660]  Это значит, что для любого епсилон, верхний предел какой-то последовательности, ну, в нашем случае такой,
[47:32.660 --> 47:40.660]  меньше, если брать на 1, это то же самое, что для любого епсилона больше 0.
[47:40.660 --> 47:57.660]  Неверно, что кси n поделить на logn больше, чем 1 плюс епсилон, бесконечно часто.
[47:57.660 --> 48:07.660]  Да, какой бы ни взяли епсилон, лишь конечное количество раз кси n поделить на logn будет больше, чем 1 плюс епсилон.
[48:10.660 --> 48:18.660]  Но давайте обозначим это событие за m.
[48:18.660 --> 48:31.660]  Найдем его вероятность.
[48:31.660 --> 48:45.660]  Но это в точности вероятность того, что кси n больше, чем logn умножить на 1 плюс епсилон.
[48:45.660 --> 48:55.660]  Это интеграл от плотности.
[48:55.660 --> 49:07.660]  Плотность экспоненциального распределения с проявлением 1, да, если это минус x dx.
[49:07.660 --> 49:18.660]  Понятно, что это есть n в степени минус 1 минус епсилон.
[49:18.660 --> 49:34.660]  Поэтому сумма вероятностей i n конечна.
[49:34.660 --> 49:44.660]  И значит вероятность того, что i n бесконечно часто равно 0.
[49:44.660 --> 49:54.660]  Формально мы еще не доделали задачу, потому что здесь для любого епсилона еще стоит.
[49:54.660 --> 50:06.660]  Возвращаемся к тому, что мы хотели. Мы хотели показать, что вероятность того, что верхний предел кси n поделить на logn меньше, чем 1, равна 1.
[50:06.660 --> 50:22.660]  Это событие, это в точности, как мы с вами выяснили, вероятность того, что для любого епсилона больше 0, не выполнена i n bч.
[50:22.660 --> 50:38.660]  А i n от епсилона зависит. Я просто опустил зависимость от епсилона, но i n зависит от епсилона.
[50:38.660 --> 51:00.660]  На самом деле это событие монотонно по епсилону, в том смысле, что если вы епсилон будете уменьшать, то событие от этого при этом будет вложено.
[51:00.660 --> 51:08.660]  И на самом деле можно рассматривать не континуальное множество епсилон, а счетное.
[51:08.660 --> 51:16.660]  То есть это все равно, что для любого епсилона вида 1mt, где n натуральное число.
[51:16.660 --> 51:22.660]  То есть вы можете рассматривать пересечение по всем натуральным m.
[51:22.660 --> 51:30.660]  Давайте я все-таки здесь напишу для удобства, что i n зависит от m.
[51:36.660 --> 51:40.660]  Почему так? Потому что мне важно, что я могу выбрать епсилон сколько угодно маленьким.
[51:40.660 --> 51:46.660]  Если у меня m сколько угодно большое, то я таким образом могу выбрать епсилон сколько угодно маленьким.
[51:46.660 --> 51:50.660]  Ну и пофигу, что он определенный вид имеет, это равносильные вещи.
[51:50.660 --> 51:54.660]  А это есть вероятность пересечения по всем m.
[51:57.660 --> 52:02.660]  p от m бесконечно часто отрицание.
[52:03.660 --> 52:05.660]  p не должно быть.
[52:21.660 --> 52:33.660]  Вот. Ну понятно, что если у вас есть счетное пересечение событий, которые имеют меру 1, то вся вероятность тоже будет 1.
[52:33.660 --> 52:35.660]  Но к тому же здесь еще и вложенные.
[52:35.660 --> 52:43.660]  Здесь с увлечением m события вложенные, и поэтому это по непрерывности есть предел вероятности.
[52:43.660 --> 52:49.660]  Преимущество бесконечности. Вероятность того, что i n от m бесконечно часто.
[52:51.660 --> 52:58.660]  Можно было бы этим не пользоваться в принципе, потому что у вас не в пределе даже единица.
[52:58.660 --> 53:01.660]  У вас каждое событие имеет вероятность 1.
[53:01.660 --> 53:06.660]  Поэтому и пересечение таких событий тоже будет иметь вероятность 1.
[53:06.660 --> 53:11.660]  Здесь даже еще более сильная вещь. Здесь можно в пределы перейти.
[53:11.660 --> 53:13.660]  Короче, единица, что и требуется.
[53:14.660 --> 53:18.660]  Вот. Ну и аналогично второй пункт.
[53:18.660 --> 53:23.660]  Когда вы смотрите на вероятность того, что верхний предел больше m единицы.
[53:33.660 --> 53:39.660]  Вы что говорите? Вы говорите, что верхний предел больше m единицы.
[53:44.660 --> 53:49.660]  Тогда и только тогда, когда для любого epsilon больше нуля.
[53:52.660 --> 53:58.660]  Кси n поделить на лог n больше равно чем 1 минус epsilon бесконечно часто.
[54:04.660 --> 54:09.660]  Ну и вы точно так же можете перейти к одной m вместо epsilon.
[54:09.660 --> 54:13.660]  Тогда и только тогда, когда для любого m натурального.
[54:14.660 --> 54:21.660]  Кси n поделить на лог n больше равно чем 1 минус 1 m бесконечно часто.
[54:25.660 --> 54:30.660]  Ну дальше вы обозначаете как выше это событие за n от m.
[54:30.660 --> 54:42.660]  И говорите, что сумма этих n вероятностей от n от m, она теперь равна сумме n в степени минус 1 плюс epsilon.
[54:44.660 --> 54:46.660]  Она бесконечная.
[54:47.660 --> 54:49.660]  Вместо epsilon должна быть одна m.
[54:51.660 --> 54:52.660]  Не суть.
[54:52.660 --> 55:01.660]  Она бесконечная, а значит, по Лемми-Боррели-Кантели вероятность того, что n от m бесконечно часто, они у вас независимые события.
[55:01.660 --> 55:03.660]  Вот в этом месте мы независимый используем.
[55:04.660 --> 55:05.660]  Равно единицы.
[55:09.660 --> 55:10.660]  Ну и все.
[55:11.660 --> 55:16.660]  И дальше вы снова обозначаете как выше это событие за n от m.
[55:17.660 --> 55:18.660]  Вот, ну и все.
[55:19.660 --> 55:25.660]  И дальше вы снова этим пользуетесь.
[55:27.660 --> 55:29.660]  Для доказательства исходного утверждения.
[55:31.660 --> 55:32.660]  Говорить, что...
[55:33.660 --> 55:37.660]  Ну опять событие вложено, поэтому это есть предел при м с применяющимся бесконечности.
[55:38.660 --> 55:45.660]  Что, в общем-то, на самом деле опять же не нужно, потому что событие не просто, вероятность события не просто не соединится, а для всех m она равна.
[55:47.660 --> 55:54.660]  Поэтому просто пересекаете счетное множество событий, вероятность которых она деится, получается событие вероятности 1.
[55:54.660 --> 55:56.660]  Но можно пределы перейти без разницы.
[56:02.660 --> 56:04.660]  И в общем получаем 1, что это у вас.
[56:07.660 --> 56:08.660]  Есть ли вопросы?
[56:17.660 --> 56:18.660]  Вот, ну прекрасно.
[56:18.660 --> 56:33.660]  Теперь, если вы посмотрите на это доказательства, вы увидите, что здесь важно распределение.
[56:34.660 --> 56:36.660]  То есть вы явно считали вероятность этих i n.
[56:37.660 --> 56:43.660]  Так как вы знали, что распределение экспоненциально, это в одном случае у вас получилось n степень минус 1 минус epsilon.
[56:44.660 --> 56:46.660]  А в другом случае n степень минус 1 плюс epsilon.
[56:49.660 --> 56:58.660]  Если мы вернемся к закону повторного логарифма, то мы увидим, что нам нужно знать распределение s n.
[56:59.660 --> 57:00.660]  Ну, мы его знаем.
[57:01.660 --> 57:02.660]  Я увижу, что он написал.
[57:02.660 --> 57:08.660]  Вероятность того, что s n равняется k, это есть c из n по n плюс k пополам на 2 степень минус n.
[57:09.660 --> 57:15.660]  Но вот этот вот биномиальный коэффициент, c из n по n плюс k пополам, непонятно, как его анализировать.
[57:15.660 --> 57:19.660]  Вам же в итоге нужно, чтобы вот этот порог, он был в виду корень из 2n log n.
[57:20.660 --> 57:26.660]  То есть когда вы вместо k подставляете корень из 2n log n, то вы получаете, что там если на epsilon сдвинется, то там будет бесконечность.
[57:27.660 --> 57:29.660]  Если вправо, если на epsilon влево, там будет минус бесконечность.
[57:30.660 --> 57:36.660]  И вам для этого нужно понимать асимптотику, как асимптотически ведет себя закон распределения s n.
[57:38.660 --> 57:43.660]  Ну и еще там технический момент, который в общем-то позволяет Lema-Barrelli-Cantelli применить.
[57:43.660 --> 57:46.660]  У вас все-таки, как я уже говорил, не независимая случайная величина.
[57:46.660 --> 57:48.660]  В общем, там нужно к максимуму перейти.
[57:49.660 --> 57:50.660]  И вот это вот утверждение 1.
[57:50.660 --> 57:52.660]  Но вот это давайте оставим за кадром.
[57:53.660 --> 57:56.660]  Подумаем о том, как понять, какое асимптотическое распределение s n.
[57:58.660 --> 58:03.660]  Ну вот есть тиремма уавролапласа про предельное поведение биномиального распределения.
[58:03.660 --> 58:06.660]  Я думаю, что Иван Генрихович вам ее формулировал.
[58:07.660 --> 58:08.660]  Надеюсь.
[58:08.660 --> 58:10.660]  Если нет, я в любом случае формулирую.
[58:10.660 --> 58:12.660]  Но видимо, не доказывал.
[58:13.660 --> 58:15.660]  Если я не прав, скажите, что доказывал.
[58:18.660 --> 58:20.660]  Значит, я ее сейчас докажу.
[58:20.660 --> 58:21.660]  Сформулирую и докажу.
[58:21.660 --> 58:23.660]  Но я ее сформулирую и докажу в более общем виде.
[58:23.660 --> 58:28.660]  Даже если вам Иван Генрихович формулировал, я докажу в более общем виде, именно который нужен здесь.
[58:28.660 --> 58:35.660]  Здесь важно, что нам нужно понимать распределение s n для правильно растущих k.
[58:35.660 --> 58:38.660]  То есть k должно быть порядка корень из двух n лог лог m.
[58:39.660 --> 58:40.660]  Вот.
[58:40.660 --> 58:48.660]  И вот это есть некоторая тиремма уавролапласа, которая позволяет для таких k понимать, какое предельное распределение.
[58:48.660 --> 58:52.660]  Это ее некоторое обобщение, которое я сейчас сформулирую.
[58:52.660 --> 58:55.660]  В общем, она будет состоять из двух частей.
[58:55.660 --> 58:58.660]  Из локального утверждения и интегрального утверждения.
[59:00.660 --> 59:02.660]  И будет сейчас два пункта.
[59:03.660 --> 59:07.660]  Первый я аккуратно докажу, а второй я просто прокомментирую.
[59:07.660 --> 59:10.660]  В общем, он практически явно следует из первого.
[59:10.660 --> 59:13.660]  Я думаю, что каждый из вас способен с таким упражнением справиться.
[59:22.660 --> 59:27.660]  Ну, давайте для удобства, для некоторой общности, будем формулировать ее для биномиального распределения.
[59:27.660 --> 59:31.660]  Потом я поясню, как мы от биномиального переходим к случайному блужданию.
[59:31.660 --> 59:39.660]  Вот пусть у нас есть x n, биномиальная случайная величина с параметром n и p.
[59:44.660 --> 59:47.660]  И пусть есть какая-то последняя параметра, которая у нас есть.
[59:47.660 --> 59:52.660]  И пусть есть какая-то последовательность phi от n, которая равна o мало и от n в степени 2,3.
[59:56.660 --> 59:57.660]  Тогда первое.
[01:00:03.660 --> 01:00:10.660]  Значит, supremum, по всем k, целым и отрицательным.
[01:00:12.660 --> 01:00:15.660]  Таким, что может быть, что может быть.
[01:00:16.660 --> 01:00:21.660]  Таким, что модуль k-np меньше броно, чем phi от n.
[01:00:24.660 --> 01:00:30.660]  От модуля разности вероятность того, что sn равняется k,
[01:00:34.660 --> 01:00:38.660]  уделить на 1, уделить на корень из 2 p, np, 1-p.
[01:00:46.660 --> 01:00:51.660]  Испонента от минус k, минус np в квадрате.
[01:00:53.660 --> 01:00:55.660]  Уделить на 2 np, 1-p.
[01:00:59.660 --> 01:01:04.660]  Эта дробь, она отстымит с единицей.
[01:01:05.660 --> 01:01:06.660]  Равномерно.
[01:01:07.660 --> 01:01:13.660]  То есть, если я возьму supremum этой дроби, минус 1, по всем возможным k,
[01:01:14.660 --> 01:01:16.660]  из моей окрестности, от ожидания.
[01:01:17.660 --> 01:01:19.660]  По этому от ожиданию случается.
[01:01:20.660 --> 01:01:22.660]  А здесь вместо sn должно быть xn.
[01:01:23.660 --> 01:01:32.660]  То вероятность попасть в точку k, она будет равномерно близка к вот этому выражению,
[01:01:32.660 --> 01:01:34.660]  к этому выражению, которое у знания цитатиста.
[01:01:35.660 --> 01:01:38.660]  Вторая часть – это интегральный вариант, который просто из первого выводится.
[01:01:39.660 --> 01:01:43.660]  Имея вот эту формулу, можно вывести следующее.
[01:01:45.660 --> 01:01:56.660]  Что supremum по всем, опять таким же точно k,
[01:02:02.660 --> 01:02:10.660]  а модуля разности между…
[01:02:14.660 --> 01:02:16.660]  Сейчас, секунду.
[01:02:16.660 --> 01:02:18.660]  Сейчас я подумаю, как это аккуратно писается.
[01:02:19.660 --> 01:02:20.660]  Ну, хорошо.
[01:02:21.660 --> 01:02:23.660]  Модули разности сейчас нет.
[01:02:25.660 --> 01:02:27.660]  Прошу прощения, давайте по-другому напишем.
[01:02:33.660 --> 01:02:34.660]  Ну, хорошо.
[01:02:35.660 --> 01:02:37.660]  Модули разности сейчас нет.
[01:02:39.660 --> 01:02:41.660]  Прошу прощения, давайте по-другому напишем.
[01:02:46.660 --> 01:02:48.660]  Значит, равномерно по всем k.
[01:02:55.660 --> 01:02:57.660]  Сейчас объясню, что значит равномерно.
[01:02:58.660 --> 01:03:00.660]  Целым не отрицательным.
[01:03:01.660 --> 01:03:04.660]  Таким, что модуль k-np меньше, чем ferret m.
[01:03:05.660 --> 01:03:07.660]  Таким, что модуль k-np меньше, чем ferret m.
[01:03:08.660 --> 01:03:10.660]  Таким, что модуль k-np меньше, чем ferret m.
[01:03:11.660 --> 01:03:13.660]  Таким, что модуль k-np меньше, чем ferret m.
[01:03:17.660 --> 01:03:20.660]  Вероятность того, что xn меньше, чем k…
[01:03:23.660 --> 01:03:25.660]  Разница с локальным вариантом.
[01:03:26.660 --> 01:03:28.660]  Потому что локальным вероятность того, что xn равняется k,
[01:03:29.660 --> 01:03:30.660]  а здесь вероятность того, что xn меньше, чем k.
[01:03:35.660 --> 01:03:39.660]  Тут даже не обязательно требовать того, чтобы k было целым,
[01:03:40.660 --> 01:03:51.860]  сделать его вообще произвольным, действительным. Давайте кадаем. Вероятность того, что xn
[01:03:51.860 --> 01:04:11.740]  между прочим k равна интегралу от минус бесконечности, от минус бесконечности,
[01:04:11.740 --> 01:04:30.700]  до k-np поделить на корень из np1-p, от 1 поделить на корень из 2p e в степени минус
[01:04:30.860 --> 01:04:39.260]  квадрате пополам dx. Это интеграл умножить на 1 плюс умало от единицы. Когда я говорю равномерно
[01:04:39.260 --> 01:04:45.940]  по всем k, я имею ввиду, что это равномерно спрятано в умало от единицы. То есть можно найти умало от
[01:04:45.940 --> 01:05:13.660]  единицы, которая будет не зависеть от k. Есть какие-то вопросы о формулировке теремоопроплоса?
[01:05:13.660 --> 01:05:25.340]  Если мы докажем первую часть, то на самом деле вторая часть из этого следует. Почему? Потому
[01:05:25.340 --> 01:05:30.100]  что вы просто берете вероятность того, что xn меньше 1чмk и представляете ее в виде суммы
[01:05:30.100 --> 01:05:39.140]  вероятностей. Каждую из вероятностей внутри суммы вы умеете приближать с помощью локального
[01:05:39.140 --> 01:05:51.060]  варианта, локального варианта теремомопроплоса и получите такую большую сумму вот этих экспонентов.
[01:05:51.060 --> 01:06:02.680]  Ну и приближаете после этого эту сумму интегралом. Это все выглядит абсолютно непонятно. Зачем? Как
[01:06:02.680 --> 01:06:06.700]  этим пользоваться? А еще формула очень страшная. У этого есть мотивация какая-то или это просто
[01:06:06.700 --> 01:06:12.340]  красивая теорема? Не переживайте. Во-первых, я уже частично объяснил, какая мотивация. Да,
[01:06:12.340 --> 01:06:19.900]  нам нужно, чтобы доказать закон по второму логарифу, нам нужно понимать какое в пределе распределение
[01:06:19.900 --> 01:06:30.460]  у см. И вот теорема овролоплоса это дает. Во-первых, во-вторых, на самом деле это не просто красивая
[01:06:30.460 --> 01:06:35.020]  форма, это центральная теорема теории вероятности, которая обобщается до так называемой центральной
[01:06:35.020 --> 01:06:41.260]  предельной теоремы. Ну это утверждение очень сильное. Мы тут некоторую равномерность утверждаем
[01:06:41.260 --> 01:06:46.460]  по всем К, который довольно сильно мог отличаться от марта ожидания. В общем случае, когда у нас
[01:06:46.460 --> 01:06:50.860]  есть произвольное распределение, здесь распределение биномиальное, аналогичное утверждение
[01:06:50.860 --> 01:06:56.100]  можно доказывать для произвольных распределений. Это называется центральная предельная теорема.
[01:06:56.100 --> 01:07:02.380]  И физический смысл я прямо сейчас пояснил. Я доказать уже не успею, но может быть я начну
[01:07:02.380 --> 01:07:06.460]  доказывать. Посмотрим, хотя бы напомню, что такое форма стилинга, который здесь надо использовать.
[01:07:06.460 --> 01:07:14.340]  А сейчас давайте я поясню физический смысл этой теоремы. Давайте решим такую задачу.
[01:07:14.340 --> 01:07:18.940]  Предположим, мы доказали. Решим вот такую вот задачу.
[01:07:18.940 --> 01:07:30.100]  Ну, не знаю, первое, что приходит в голову. Про монетку первое, что в голову приходит.
[01:07:30.100 --> 01:07:39.820]  Смотрите, вот мы с вами уже много раз говорили про монетку, про частотность выпадения орла или
[01:07:39.820 --> 01:07:45.340]  решки, которая одна-вторая. Что это значит? Это значит, что если вы будете очень много раз
[01:07:45.740 --> 01:07:52.940]  подбрасывать монетку и посчитаете, сколько раз у вас выпала решка, поделите на количество
[01:07:52.940 --> 01:07:56.960]  подбрасываний, то если это количество очень большое, то будет что-то близкое к одной-второй
[01:07:56.960 --> 01:08:06.100]  и чем больше это vardır тем ближе этоimming wyter. Crowbar мы с вами уже формально доказали и
[01:08:06.100 --> 01:08:12.900]  называют закон больших чисел. Закон больших чисел утверждает, что если, сейчас пр sung 강ines
[01:08:12.900 --> 01:08:16.820]  разделить, в частности, он утверждает, что если разделить количество решек на количество подбрасывания,
[01:08:16.820 --> 01:08:19.820]  то в пределе будет одна вторая. Он утверждает на самом деле нечто большее.
[01:08:19.820 --> 01:08:26.780]  Смотрите, вот что говорит закон больших честь. Он говорит, ну хорошо,
[01:08:26.780 --> 01:08:32.540]  пусть количество подбрасывания, давайте я напишу, ладно, пусть у нас есть монетка,
[01:08:32.540 --> 01:08:43.820]  xn это количество решек прием подбрасывания к симметричной монетке. Симметричная,
[01:08:43.820 --> 01:09:08.420]  значит убирает на срежке орла, одна вторая. Вот, что говорит ЗБЧ. ЗБЧ говорит вот что,
[01:09:08.420 --> 01:09:20.500]  пусть у вас есть какая-то растущая последовательность wn, которая стремится к бесконечности,
[01:09:20.500 --> 01:09:34.940]  тогда xn-n пополам поделить на корень из n wn должно быть близко к одной второй. Что
[01:09:34.940 --> 01:09:43.420]  значит близко к одной второй? Близко к чему? К нулю должно быть близко. Я из xn вычел средний,
[01:09:43.420 --> 01:09:49.140]  поделил на корень из n на w. То есть с одним словом вероятность того, что эта штука по модулю больше
[01:09:49.140 --> 01:09:56.380]  чем x, стремится к нулю. Это ЗБЧ в точности, который мы с вами уже формулировали. В частности,
[01:09:56.380 --> 01:10:05.980]  если я в знаменателе возьму hn, то есть если wn это корень из n, то здесь написано ничто иное,
[01:10:05.980 --> 01:10:15.620]  как среднее количество решек минус одна вторая мало. Чем больше n, тем меньше эта штука. Тем
[01:10:15.620 --> 01:10:20.980]  меньше отличается это усоединенное количество решек от одной второй. Ну а это еще более сильное
[01:10:20.980 --> 01:10:28.580]  утверждение, что в знаменателе можно написать не n, а вплоть до корни из n. Вопрос, а что будет
[01:10:28.580 --> 01:10:39.980]  если корень из n написать? Чему равна вероятность того, что xn-n пополам поделить на корень из n по модулю
[01:10:39.980 --> 01:10:51.620]  больше чем x? Будет стремиться к нулю или нет? Более практический вопрос. Пусть n очень большой,
[01:10:51.620 --> 01:11:05.180]  но я не знаю, скажем, миллион. Пусть n миллион. Ну что значит n миллион? Это значит,
[01:11:05.180 --> 01:11:11.860]  что решек должно быть примерно примерно 500 тысяч. Ну вот чему можем ли мы с какой-то большой
[01:11:11.860 --> 01:11:30.260]  точностью сказать, чему равна вероятность того, что xn больше чем, не знаю, 500-1000.
[01:11:35.180 --> 01:11:51.460]  Ну вы скажете, да не бойся, одна вторая. Понятно, что вероятность того, что xn больше
[01:11:51.460 --> 01:11:57.740]  чем 500 тысяч, это одна вторая. Почти. Потому что это как раз половина, это как раз ровно половина
[01:11:57.740 --> 01:12:03.180]  миллиона. Но я не сильно от 500 тысяч отошел, я там не 10 тысяч прибавил, а всего одну тысячу прибавил.
[01:12:03.180 --> 01:12:09.820]  Наверное, вероятность того, что xn больше чем 500-1000, тоже примерно одна вторая. Ничего подобного,
[01:12:09.820 --> 01:12:17.340]  она меньше чем одна вторая значительно. Если вы там вместо 501 тысячи напишете 505 тысяч,
[01:12:18.020 --> 01:12:22.740]  если вы посмотрите, наверное, что xn больше чем 505 тысяч, это будет примерно 0. То есть,
[01:12:22.740 --> 01:12:32.500]  иными словами из вот этой теоремы мавролапласа следует, что вот не просто возле 500 тысяч должно
[01:12:32.500 --> 01:12:39.940]  быть количество решек, а очень близко. На самом деле порядок, на который ваше среднее может
[01:12:39.940 --> 01:12:49.340]  отличаться от имперического среднего, может отличаться от теоретического среднего, оно там порядка
[01:12:49.340 --> 01:12:55.500]  корень из z. То есть, почему я здесь написал 1000, потому что 1000 это корень из миллиона. Давайте
[01:12:55.500 --> 01:13:05.220]  увидим, что я прав, применив теорему, которую я вверху сформулировал. Ну вот, прям вот этот
[01:13:05.220 --> 01:13:12.780]  частный случай будем решать. Вероятность того, что xn больше чем 501 тысячи. Это все в терминах
[01:13:12.780 --> 01:13:18.540]  более общей задачи, которую я вышли сформулировал. Вот если я возьму n равно миллион, вот здесь будет
[01:13:18.540 --> 01:13:32.140]  у меня стоять 1000. И тогда epsilon это будет типа единица. Это просто частный случай, задача,
[01:13:32.140 --> 01:13:39.780]  которую я сформулировал выше. И раз я говорю, что нифига к нулю не стремится, это вероятность. Это
[01:13:39.780 --> 01:13:47.580]  означает, что ответ нет. Вот здесь вот ответ нет. Не стремится к нулю. То есть, в законе больших
[01:13:47.580 --> 01:13:53.620]  чисел корень из n это оптимальная граница. Я могу корень из n умножать на любую растущую функцию,
[01:13:53.620 --> 01:14:01.460]  сколь угодно медленную, но убрать wn или написать констант вместо wn я не могу. Закон больших
[01:14:01.460 --> 01:14:08.140]  чисел именно в этом месте совершенно оптимальный. Но давайте в частном случае вот для этих конкретных
[01:14:08.140 --> 01:14:16.460]  чисел увидим, что мы можем применить наш теорим. Значит, xn имеет биномиальное распределение с
[01:14:16.460 --> 01:14:36.860]  параметрами 10 в шестое, 1 в второе. Вероятность того, что xn, ну давайте посмотрим, что такое np.
[01:14:36.860 --> 01:14:45.220]  np это 500 тысяч. Зачем мне нужна np? Потому что оно вот здесь вот стоит. Да, вот здесь есть np,
[01:14:45.220 --> 01:14:51.100]  еще есть корень из np, 1-p. Давайте сначала поймем, что такое np это 500 тысяч. Теперь
[01:14:51.100 --> 01:15:05.100]  поймем, что такое корень из np, 1-p. Ну это сколько? Это корень из миллиона умноженного до корня
[01:15:05.100 --> 01:15:20.820]  значения. Да, то есть это 1000 пополам или 500? 500. Значит, вероятность того, что xn больше чем 500-1000,
[01:15:20.820 --> 01:15:32.580]  это в точности вероятность того, что xn минус np больше чем 1000.
[01:15:32.580 --> 01:15:44.940]  А это в свою очередь в точности вероятность того, что xn минус np, хоть зрительно корень из np,
[01:15:44.940 --> 01:15:58.660]  1-p больше чем 2. Ну то есть если я из правой части не равен, вот это мое k.
[01:15:58.660 --> 01:16:09.580]  Это мое k. Если я из него вычту np и поделю на корень из np 1-p, я получу 2. Вот, ну применяю
[01:16:09.580 --> 01:16:21.020]  теорему муавролапласа. Да, я понимаю, что вероятность того, что xn больше чем k. Это
[01:16:21.020 --> 01:16:26.620]  есть 1 минус вероятность того, что xn меньше оно чем k. Это примерно равно, я сейчас это прогоментирую.
[01:16:26.620 --> 01:16:37.780]  Примерное равенство. Здесь есть нарушенная некая строгость. Значит, вот здесь есть мало от
[01:16:37.780 --> 01:16:43.100]  единицы. То есть иными словами, левая часть стремится к правой, если я мало от единицы уберу.
[01:16:43.100 --> 01:16:49.780]  Поэтому я, конечно, при фектированном n не могу сказать, что точно равно, но это n,
[01:16:49.780 --> 01:16:53.740]  вот этот миллион, он настолько большой, что ошибка будет очень маленькая. Я сейчас отдельно
[01:16:53.740 --> 01:16:59.940]  об этом скажу. Давайте сначала посчитаем, что тут получится. Это примерно единица минус
[01:16:59.940 --> 01:17:09.700]  интеграл от минус бесконечности до 2. От 1 поделить на корень из 2p, e в степени минус x
[01:17:10.700 --> 01:17:27.220]  Это табличный интеграл, то есть вы там можете написать функция Laplace в интернете или нормальное
[01:17:27.220 --> 01:17:32.940]  распределение или в квантире нормального распределения. Не суть, в общем, как к воду
[01:17:32.940 --> 01:17:49.460]  Найдете табличку значения вот этого интеграла. Открою табличку и посмотрю,
[01:17:49.460 --> 01:18:00.980]  чему равен вот этот интеграл от минус бесконечности до 2. Вот чему он равен.
[01:18:01.020 --> 01:18:10.100]  Он равен 0.9772, ну примерно. Он примерно равен 0.9772.
[01:18:16.420 --> 01:18:30.100]  Да, значит наша вероятность, это примерно 0.228. Ну то есть очень маленькая вероятность.
[01:18:30.980 --> 01:18:37.460]  Вероятность того, что их цен больше чем 501 тысяч, это 0.02. А если у вас стояло 505 тысяч,
[01:18:37.460 --> 01:18:42.980]  то там было после запятощего куча нулей. Вот, теперь по поводу вот этого примерного равенства.
[01:18:42.980 --> 01:18:53.300]  Есть теорема, которая называется Теорема Берри и Сейна. О том, с какой скоростью в интегральной
[01:18:53.300 --> 01:18:58.180]  теореме мавролапласта левая часть сходится с правой. Да, но то есть иными словами оценивает вот
[01:18:58.180 --> 01:19:03.780]  этого мало от единицы. Вот этого мало от единицы можно оценить там констант и поделить на коре низе.
[01:19:03.780 --> 01:19:14.660]  То есть в нашем случае ошибка будет в тысячных. Ошибка будет в тысячных, то есть вот ну может
[01:19:14.660 --> 01:19:19.900]  быть причем причем там будут какие-то там может быть на единицу в тысячных или даже в десяти
[01:19:19.900 --> 01:19:24.420]  тысячных. То есть ну максимум здесь будет тройка стоять после двойки. То есть вот в этом знаке
[01:19:24.420 --> 01:19:28.980]  может быть ошибка. Короче говоря, мы эту вероятность можем записать довольно точно.
[01:19:28.980 --> 01:19:32.300]  Есть ли какие-то вопросы?
[01:19:43.140 --> 01:19:49.860]  Ну я надеюсь, что я достаточно подробно пояснил, как применяется теорема мавролапласта и почему
[01:19:49.860 --> 01:19:55.940]  она действительно такое очень значимое дополнение к закону больших чисел. На самом деле она даже
[01:19:55.940 --> 01:20:00.660]  точнее, чем закон больших чисел. Из нее закон больших чисел можно вывести. То есть из того, что вы
[01:20:00.660 --> 01:20:07.380]  знаете, что когда в знаменателе коре низе, то эта вероятность не тривиальная в пределе. Из этого на
[01:20:07.380 --> 01:20:11.380]  самом деле следует, что если вы чуть-чуть увеличите знаменатель и сделать его растущим, то вероятность
[01:20:11.380 --> 01:20:15.940]  сразу будет в деле ноль. Поэтому закон больших чисел на самом деле из теорем мавролапласта вывозит.
[01:20:16.020 --> 01:20:20.180]  Теорем мавролапласта гораздо более сильного утверждения. Ну для бенмяльного распределения, но у нас
[01:20:20.180 --> 01:20:24.220]  в будущем еще будет центральная предельная теорема, которая работает вообще для всех распределений,
[01:20:24.220 --> 01:20:31.300]  а не только для бенмяльных. Хорошо, тогда докажем мы эту теорему в следующий раз. На сегодня все.
[01:20:31.300 --> 01:20:34.540]  Если есть какие-то вопросы, задавайте.
[01:20:34.540 --> 01:20:44.780]  Если вопросов нет, то всем до следующей субботы. Спасибо, до свидания.

[00:00.000 --> 00:15.040]  так соответственно дам сегодня новая лекция сегодня мы наконец-то переходим от оптимизации на
[00:15.040 --> 00:19.880]  безусловно задачи оптимизации когда мы минимизировали нашу функцию на всем
[00:19.880 --> 00:27.520]  пространстве rd теперь мы соответственно переходим в случае когда у нас наше множество
[00:27.520 --> 00:33.880]  на котором мы можем оптимизировать переменные оно ограничено соответственно да вот та же
[00:33.880 --> 00:37.840]  самая целевая функция которая нам нужно было минимизировать ну и соответственно теперь
[00:37.840 --> 00:45.120]  появляется какое-то множество x на котором соответственно мы должны эту функцию минимизировать
[00:45.120 --> 00:52.880]  раньше у нас было такое что у нас вот это множество x просто равнялась rd это была задача
[00:52.880 --> 00:58.640]  безусловной оптимизации теперь мы соответственно рассматриваем что в данном случае это множество x
[00:58.640 --> 01:07.760]  некоторое выпуклое множество вот и как вы понимаете в связи с тем что у нас теперь задача условная то
[01:07.760 --> 01:13.520]  есть теперь минимум наш глобальный минимум на всем пространстве rd он может не совпадать с тем
[01:13.520 --> 01:18.960]  минимумом который содержится на множестве x соответственно здесь изображена чашка на
[01:18.960 --> 01:25.500]  картинке вот этот центр чашки может быть за пределами вот этого тут многоугольника на котором
[01:25.500 --> 01:34.520]  мы и оптимизируем нашу функцию вот поэтому да с этим придется иметь дело вот мы соответственно
[01:34.520 --> 01:41.400]  будем смотреть как начнем смотреть сегодня как можно уже вот с этими множествами x взаимодействовать
[01:41.400 --> 01:48.280]  вот вот такой вопрос зачем вообще и появляются те ограничения и какая у них часто бывает суть
[01:48.280 --> 01:55.120]  может они вообще и не нужны мизировали бы и минимизировали на обычном множестве
[01:55.120 --> 02:08.960]  правильно ну то есть действительно задачи оптимизации во многом это какие-то физические
[02:08.960 --> 02:14.520]  задачи одно из применений это физические задачи экономические задачи соответственно у вас есть
[02:14.560 --> 02:19.980]  какие-то ограничения которые просто возникают в реальной жизни то есть ограничений какие-то на
[02:19.980 --> 02:25.800]  скорость ограничения какие-то на ресурсы просто у вас нету бесконечного числа денег хотя хотелось
[02:25.800 --> 02:32.300]  бы наверное вот нет у вас какого-то бесконечного числа древесины ну вот соответственно вы это
[02:32.300 --> 02:38.000]  все ограничивайте появляются какие-то ограничения как-то учитываете ограничение хотите там
[02:38.000 --> 02:43.520]  максимизировать свою прибыль минимизировать свои издержки и так далее вот более того часто
[02:43.520 --> 02:50.920]  Часто, например, ограничения возникают не совсем естественным путем, их можно вести дополнительно,
[02:50.920 --> 02:56.720]  просто чтобы в некотором смысле улучшить задачу или улучшить в некотором, опять же, смысле решение.
[02:56.720 --> 03:03.120]  Если сегодня успеем, я покажу примерчик, как, например, в машинном обучении можно использовать дополнительные хитрые ограничения,
[03:03.120 --> 03:10.000]  чтобы качество решения было в некотором смысле лучше.
[03:10.000 --> 03:18.000]  Ну, опять же, посмотрим, расскажу, почему оно лучше и почему, соответственно, нужны были эти ограничения.
[03:18.000 --> 03:25.000]  Ну, пока, соответственно, разбираемся с общим случаем. Целевая функция множество, выпукло множество, выпукла целевая функция.
[03:25.000 --> 03:29.000]  Поехали. Соответственно, да, это мы с вами в некотором смысле уже обсуждали.
[03:29.000 --> 03:35.000]  Условие оптимальности как раз нашей задачи, но оно уже не в безусловном случае.
[03:35.000 --> 03:39.000]  В безусловном случае я напоминаю, что у нас просто градиент равнялся нулю.
[03:39.000 --> 03:44.000]  Вот. Сила того, что теперь у нас просто глобальный минимум может не лежать во множестве х,
[03:44.000 --> 03:47.000]  ну, который глобальный минимум на rd, вот.
[03:47.000 --> 03:55.000]  Градиент, поэтому он в оптимуме, именно на х, он может быть и не давать ноль-ноль градиента, вот.
[03:55.000 --> 04:01.000]  Соответственно, справедливо вот такое вот условие. На лекции второй я давал его без доказательства, просто его показал.
[04:01.000 --> 04:06.000]  Вот. Сейчас мы его, соответственно, докажем, потому что оно нам будет нужно.
[04:06.000 --> 04:10.000]  Нужно. Вот. Ну, вот такое вот условие.
[04:10.000 --> 04:14.000]  Давайте доказывать. Давайте доказывать.
[04:14.000 --> 04:19.000]  Ну, давайте в одну сторону. Достаточное условие. Что это значит? Достаточность условия.
[04:22.000 --> 04:29.000]  Да, да. То есть в достаточном условии, что если мы предположим, что у нас вот выполнено вот это вот, соответственно, условие,
[04:29.000 --> 04:40.000]  записываем его, там, больше либо равно нуля, то из этого должно следовать, что у нас х звездой глобальный минимум на х,
[04:40.000 --> 04:52.000]  ну, понятно, функции f. Вот. Окей. Давайте сделаем все равно так же, как мы делали с вами в случае безусловной задачи,
[04:52.000 --> 04:56.000]  ну, достаточное условие. Что мы там записали? Мы там записали просто выпуклость.
[04:56.000 --> 05:00.000]  Можно, например, оценить значение функции f.x через выпуклость.
[05:03.000 --> 05:08.000]  Через выпуклость. Через определение выпуклости. Что там будет?
[05:13.000 --> 05:15.000]  Ну, вот что-то вот такое.
[05:15.000 --> 05:29.000]  Окей. Ну, это просто определение выпуклости. Вот. Как раз через, когда у нас функция непрерывно дифференцируема.
[05:32.000 --> 05:38.000]  Про это мы только что знаем, что в силу того, что мы доказываем достаточные условия, мы предположили, что это больше либо равно нуля,
[05:38.000 --> 05:45.000]  поэтому второе слагаемое здесь больше либо равно нуля, и, соответственно, что мы можем сказать, что вот то, что написано здесь,
[05:45.000 --> 06:04.000]  оно больше либо равно очень просто значения функции в точке х звездой, но это значит, что как раз по определению у нас х звездой глобальный минимум функции f на х.
[06:05.000 --> 06:12.000]  Все. Ну, то есть тут все просто. Ничего такого сверхъестественного мы тут и говорим.
[06:12.000 --> 06:24.000]  Дальше, соответственно, что? Хочется доказать необходимые условия, то есть в обратную сторону мы предполагаем, что у нас х является глобальным минимумом функции f на множестве х.
[06:24.000 --> 06:30.000]  Вот. И хотим, соответственно, доказать, что будет выполнено вот это условие, которое написано выше. В том числе вот такое условие.
[06:30.000 --> 06:49.000]  Пойдем от противного. Вот противного. Вот. И рассмотрим, соответственно, скажем, что у нас существует, существует некоторая точка, ну пусть будет х с тильдой, вот, из множества х нашего.
[06:49.000 --> 07:04.000]  Такая, что вот у нас вот это выражение, вот это скалярное произведение х со звездой х тильда х минус х со звездой меньше нуля строго, меньше нуля строго, вот, для некоторой точки х с тильдой.
[07:04.000 --> 07:26.000]  Вот. Рассмотрим семейство точек, как мы в принципе с вами уже и делали, которые параметризуются параметром лямбда, вот, в следующем виде, в следующем виде, лямбда х со звездой плюс 1 минус лямбда х с тильдой.
[07:26.000 --> 07:48.000]  Что мы можем сказать про х лямбда для любого лямбда от нуля до одного? Да, она лежит в х просто потому, что у нас что? Х выпукло множество, а просто по определению выпуклого множества любая линейная комбинация его точек, вот, она лежит в этом множестве.
[07:48.000 --> 08:13.000]  Соответственно, мы можем сказать, что х лямбда принадлежит множеству х. Окей, давайте посмотрим, как соответственно ведет себя, вот, соответственно, посмотрим, как ведет себя функция, которая зависит от вот этого х лямбда.
[08:13.000 --> 08:29.000]  Вот, тут х тильдой, только я уже делал с тильдой. Вот, как ведет себя функция, вот такая вот, в которой мы подставили значение нашей точки. Вот просто вот эту точку, семейство точек в зависимости от лямбда, они все принадлежат множеству х.
[08:29.000 --> 08:47.000]  Вот. Заметим, что производная вот этой функции по лямбда равна следующему выражению. Я надеюсь, вы понимаете, откуда это берется. Вы, скорее всего, в первом семинаре, когда вы дифференцировали по матрице, там в том числе было упражнение дифференцирование по сколяру.
[08:47.000 --> 09:01.000]  Вот. И, соответственно, вылезает вот такое вот выражение. Вот такое вот выражение. Производная нашей функции phi от лямбда по лямбда. По лямбда получается вот такое вот сколярное произведение.
[09:01.000 --> 09:10.000]  Что мы о нем можем сказать, например, при лямбда равном нулю? При лямбда равном нулю, что мы можем сказать об этом сколярном произведении?
[09:10.000 --> 09:30.000]  Оно меньше нуля, согласны? Просто потому что мы предположили, что существует точка х с тильдой, что будет выполнено, что вот это сколярное произведение меньше нуля.
[09:30.000 --> 09:48.000]  Вот. А значит, что в окрестности нуля именно функция phi от лямбда, она что в окрестности нуля? Ну, если производная равна меньше нуля, то что в окрестности нуля с ней происходит?
[09:48.000 --> 10:12.000]  Оно убывает. Убывает в некоторой окрестности нуля. Вот. Ну, значит, если оно убывает в некоторой окрестности нуля, то и функция f, которая вот зависит от х лямбда, убывает в некоторой окрестности лямбда.
[10:12.000 --> 10:26.000]  Вот. А значит, что у нас получается f от х со звездой будет больше, больше, чем некоторый f от х лямбда для некоторого малого лямбда?
[10:26.000 --> 10:37.000]  Так? Согласны? Функция убывает вдоль этого направления некоторой окрестности нуля. Ну, направление, соответственно, задается вектором х тильдой минус х со звездой.
[10:37.000 --> 10:50.000]  Вот. Ну и, соответственно, получается, что и значение функции в некоторые точки, а мы знаем, что вот эта точка, она у нас лежит во множестве х, просто потому что мы ее так определили.
[10:50.000 --> 11:03.000]  Ну, а значит, пришли к противоречию, противоречию, противоречию того, что у нас х со звездой – это глобальный минимум функции f на х. Окей?
[11:03.000 --> 11:11.000]  Все. Значит, условия доказаны в обе стороны. И необходимо, и достаточно всего доказано. Вот. Хорошо. Здесь это все та-та-та.
[11:11.000 --> 11:22.000]  Хорошо. Смотрите, в чем вопрос вообще. Появилось множество х. Соответственно, нам нужно рискать решение на нем. Запустим, например, обычный градиентный спуск.
[11:22.000 --> 11:32.000]  Вот. Он может уйти куда угодно. Просто глобальное решение в другом месте находится абсолютно далеко от множества х, которое мы рассматриваем. Вот. И, соответственно, мы туда и упремся.
[11:32.000 --> 11:41.000]  Как дешево, в некотором смысле, адаптировать, например, тот же градиентный спуск под уже условную задачу оптимизации?
[11:46.000 --> 11:55.000]  Штраф. Кстати, это классная идея. Мы это будем обсуждать через четыре лекции. Метод штрафов – так называемый тоже идея Нестеров-Немировский.
[11:55.000 --> 12:05.000]  Супер классный метод. Справедливости ради, вот как раз, в некотором смысле, Нестеров впервые стал популярен из-за метода штрафа. Ну, барьеров. Там есть и барьеры, и штрафы. Разные вещи.
[12:05.000 --> 12:15.000]  Ну, суть похожая. Вот. Штрафовать, когда вы приближаетесь к границе. Вот. А как раз Нестеров стал популярен не с ускоренным методом, потому что пока не находилось для него применения.
[12:15.000 --> 12:23.000]  А вот метод барьеров и штрафов. Вот. Это как раз то, на чем он в некотором смысле стал известен во всем мире.
[12:23.000 --> 12:35.000]  Ладно. Окей. Штрафы. Поняли. Просто добавляем штраф. Говорим, что если мы приближаемся к границе, добавляем, например, какую-нибудь там функцию, которая на бесконечности, просто за пределами ноль, что она бесконечности, она не дифференцируем, она не очень хорошая.
[12:35.000 --> 12:47.000]  Но можно как-то эту бесконечность в некотором смысле апроксимировать. Ну, вот как-то вот в духе того, что ставить логарифмические барьеры. Ну, это мы все обсудим. Вот. Чтобы функция просто рядом с границей множества уходила в бесконечность.
[12:47.000 --> 13:01.000]  Окей. Что еще можно делать? Что еще можно делать? Сегодня просто не про это разговор. Вот. А какие еще есть идеи? Ну, вот. Сделали мы шаг градиентного спуска. Вышли за пределы множества. Что можно сделать?
[13:01.000 --> 13:13.000]  Уменьшить шаг – вариант. Ну, а если мы, соответственно, уже где-то очень близко к границе. Вот. И уменьшение шага, оно, особенно в экспоненциальное число рас, оно, видите, сходимость сильно уменьшит.
[13:13.000 --> 13:23.000]  Потому что вы знаете, что там 1 делить на k будет сильно делать из линии на сходимость и сублинии. Вот. А экспоненциальное уменьшение шага, оно, конечно, так. Будет еще жестче. Вот.
[13:23.000 --> 13:35.000]  Ну, ладно. Смотрите. Просполирую идею. Давайте просто возвращать точку. Вышли градиентным спуском за пределы множества. Давайте просто вернем нас в это множество. Вот.
[13:35.000 --> 13:47.000]  Для этого используется оператор проекции. Для этого используется оператор проекции. Записывается он следующим образом. Соответственно, это просто некоторый аргуминимум.
[13:47.000 --> 13:58.000]  И мы ищем ближайшую точку. То есть, хотим испроектировать, например, точку y на наше множество x. И мы ищем ближайшую точку нашего множества x к этой точке y.
[13:58.000 --> 14:09.000]  Ну, опять же, исходя из Евклидова расстояния. Вот. Евклидова проекция выглядит следующим образом. Понятно, что можно проецироваться не только так. Вот.
[14:09.000 --> 14:15.000]  Но вот, соответственно, сегодня рассмотрим Евклидову проекцию. Ближайшая точка с точки зрения Евклидового расстояния к нашему множеству.
[14:15.000 --> 14:24.000]  Идея очень простая. Раз выходим за множество, давайте как только вышли сразу же у нас возвращать в него. Вот. Из-за его предела не выходить.
[14:24.000 --> 14:35.000]  Ну и в связи с этим градиентный спуск модифицируется довольно просто. К шагу просто добавляется этот оператор проекции. Вот. Вышли, зашли обратно.
[14:35.000 --> 14:45.000]  То есть, всегда x каты у нас будет в пределах множества. Хорошо. Давайте подоказываем какие-то свойства для этого оператора проекции.
[14:45.000 --> 14:54.000]  Просто потому что нам нужно понять вообще, будет это к чему-то сходиться или нет. Вот. Может быть он нам даст какие-то плохие артефакты из-за того, что мы проецируемся.
[14:54.000 --> 15:03.000]  Ну и ни к чему мы в итоге не сойдемся. Вот. Во-первых, важное свойство, то что оператор проекции у нас существует.
[15:03.000 --> 15:08.000]  И при этом еще и единственен. При этом он еще и единственен. Вот. Довольно простое свойство.
[15:08.000 --> 15:16.000]  А следует оно из того, что у нас задача проекции, она какая? Какая целевая функция там по свойствам?
[15:19.000 --> 15:26.000]  Что вы про нее можете сказать? Она выпукла вообще или нет? Вот эта целевая функция, когда мы проецируемся. Вот эта.
[15:28.000 --> 15:37.000]  Выпукла. Более того, она какая еще? Она сильно выпукла, да? А про сильно выпуклые задачи оптимизации мы знаем, что у них решение существует единственно.
[15:37.000 --> 15:43.000]  Вот. Уникальное, единственное решение. Поэтому, соответственно, оператор проекции всегда существует и всегда единственен.
[15:44.000 --> 15:52.000]  Хорошее свойство. Следующее свойство выглядит немного необычно, но оно в некотором смысле вспомогательное.
[15:52.000 --> 15:56.000]  Вспомогательное свойство, которое нам понадобится, чтобы доказать следующее свойство.
[15:56.000 --> 16:00.000]  Ну давайте вот это сначала докажем. Здесь, что у нас множество х выпуклое.
[16:01.000 --> 16:06.000]  Точка х берется из множества х обязательно, а вот точка у из любого множества.
[16:07.000 --> 16:12.000]  Вот. Ну просто из Rd. Соответственно, справедливо вот такое вот выражение.
[16:13.000 --> 16:21.000]  Хорошо, давайте попытаемся доказать. А доказывать мы будем с помощью вот этого условия, которое мы с вами только что доказали до этого.
[16:22.000 --> 16:29.000]  Условия оптимальности. Условия оптимальности для выпуклых функций на выпуклом множестве.
[16:30.000 --> 16:34.000]  Так, оператор проекции ведь тоже представляет собой решение задачи минимизации.
[16:34.000 --> 16:39.000]  Довольно простой, на самом деле, задачи минимизации на выпуклом множестве х.
[16:39.000 --> 16:43.000]  Вот. И, соответственно, для него тоже будет справедливо вот это свойство. Согласны?
[16:43.000 --> 16:48.000]  Вот. У нас какая задача? Мы минимизируем нашу целевую функцию d.
[16:49.000 --> 16:55.000]  Ну, например, от y, а где d представляет собой y-x. Ну, где x у нас точка.
[16:55.000 --> 17:00.000]  Или лучше наоборот. Давайте наоборот.
[17:00.000 --> 17:03.000]  Y у нас оно за пределами как раз.
[17:07.000 --> 17:10.000]  Так. y-x в квадрате.
[17:10.000 --> 17:13.000]  Вот такую задачу мы минимизировали на нашем множестве х.
[17:13.000 --> 17:18.000]  Соответственно, будет справедливо абсолютно это же условие оптимальности.
[17:18.000 --> 17:19.000]  Так.
[17:26.000 --> 17:29.000]  Здесь давайте я поставлю.
[17:29.000 --> 17:32.000]  Давайте вот так вот сделаем. Здесь z пусть будет у меня.
[17:32.000 --> 17:35.000]  Чтобы переменные неодинаковые были.
[17:35.000 --> 17:41.000]  Вот. Соответственно, да, здесь у меня этот x произвольный из x.
[17:41.000 --> 17:45.000]  Чему равен градиент? Градиент d. Чему он равен?
[17:47.000 --> 17:50.000]  Чему равен градиент d?
[17:50.000 --> 17:54.000]  Квадратичная задача. Чему равен градиент?
[18:03.000 --> 18:08.000]  2y-z. Кто согласен, кто не согласен?
[18:12.000 --> 18:14.000]  Минус, конечно, нужно добавить.
[18:14.000 --> 18:18.000]  Потому что вы когда же вы дифференцируете квадрат сначала, потом внутреннюю функцию еще.
[18:18.000 --> 18:22.000]  Из внутренней функции по z же вы дифференцируете. Там еще минус вылезет.
[18:23.000 --> 18:26.000]  Конечно. Получится вот так. z-y.
[18:26.000 --> 18:30.000]  Ну, это на самом деле эквивалент к тому, что вы просто вот так задачку перепишете.
[18:30.000 --> 18:34.000]  Квадратную развернуть же можно. Выражение под квадратом.
[18:34.000 --> 18:36.000]  2y-z-y.
[18:36.000 --> 18:39.000]  Вот. Ну и соответственно, что записываем?
[18:39.000 --> 18:43.000]  Записываем, что градиент 2y. Давайте 2y за сразу скалярное произведение.
[18:44.000 --> 18:46.000]  Здесь что будет? x-звездой.
[18:46.000 --> 18:48.000]  Минус y.
[18:48.000 --> 18:50.000]  Здесь будет x.
[18:50.000 --> 18:52.000]  Минус x-звездой.
[18:54.000 --> 18:56.000]  Так?
[18:56.000 --> 18:58.000]  Вот.
[18:58.000 --> 19:02.000]  А давайте вспомним, что же такое x-звездой для нашей задачи?
[19:02.000 --> 19:06.000]  x-звездой. Это чему равен для нашей задачи?
[19:07.000 --> 19:09.000]  Проекция точки y.
[19:09.000 --> 19:12.000]  Проекция точки y. Мы же ее искали. Это решение.
[19:12.000 --> 19:14.000]  Вот. Ну и все. Подставляем сюда.
[19:14.000 --> 19:16.000]  Я двоечку сразу сокращу, потому что она там...
[19:16.000 --> 19:19.000]  Знак больше либо равно 0. На число разделю.
[19:19.000 --> 19:22.000]  Что у нас получается? Проекция точки y.
[19:22.000 --> 19:24.000]  Минус y.
[19:24.000 --> 19:28.000]  x, который из множества x.
[19:28.000 --> 19:32.000]  Вот. И проекция точки y.
[19:32.000 --> 19:36.000]  Больше либо равно 0. Но это же ровно то, что нам нужно и было доказать.
[19:36.000 --> 19:38.000]  Согласны?
[19:38.000 --> 19:39.000]  Вот.
[19:39.000 --> 19:42.000]  Оно и написано сверху. Просто там поменено местами.
[19:42.000 --> 19:46.000]  В первом члене поменено местами y.
[19:46.000 --> 19:48.000]  Минус проекции y поменено местами.
[19:48.000 --> 19:50.000]  Соответственно, вот так. Одно и то же.
[19:50.000 --> 19:52.000]  Вот. Все. Доказательство закончено.
[19:52.000 --> 19:55.000]  Просто через условия оптимальности мы к этому пришли.
[19:55.000 --> 19:57.000]  Окей. Окей.
[19:59.000 --> 20:03.000]  Хорошо. И вот с помощью этого условия оптимальности,
[20:03.000 --> 20:07.000]  вот этого вспомогательного утверждения,
[20:07.000 --> 20:10.000]  я хочу доказать вот такое вот свойство.
[20:10.000 --> 20:13.000]  Оно называется по-английски non-expansiveness.
[20:13.000 --> 20:17.000]  По-русски, наверное, это как-то переводится как нерасширяемость.
[20:17.000 --> 20:19.000]  Вот. Оператора.
[20:19.000 --> 20:25.000]  Оператор проекции у вас не расширяет разность между x.
[20:25.000 --> 20:28.000]  То есть были у вас две точки какие-то x1 и x2.
[20:28.000 --> 20:32.000]  Если вы их спроекцируете на множество, то расстояние либо не изменится,
[20:32.000 --> 20:35.000]  либо еще уменьшится между ними. Вот.
[20:35.000 --> 20:38.000]  Ну, еще как это можно сказать? Ну, липшицевость.
[20:38.000 --> 20:40.000]  Один липшицевость оператора.
[20:40.000 --> 20:42.000]  Ну, хорошее свойство.
[20:42.000 --> 20:46.000]  Давайте его как раз и докажем с помощью вот этого вспомогательной вещи,
[20:46.000 --> 20:48.000]  которая у нас тут и была.
[20:48.000 --> 20:50.000]  Давайте я перепишу ее сюда.
[20:51.000 --> 20:53.000]  Ой, да-да-да. Здесь произвольные.
[20:53.000 --> 20:55.000]  Здесь произвольные x. Спасибо.
[20:55.000 --> 20:57.000]  Здесь, конечно, для произвольных действий.
[20:57.000 --> 21:00.000]  Там для x понятно, что там просто это эквивалентные вещи.
[21:00.000 --> 21:02.000]  Да, спасибо. Спасибо.
[21:02.000 --> 21:04.000]  Здесь, соответственно, произвольные x.
[21:05.000 --> 21:06.000]  Вот.
[21:06.000 --> 21:08.000]  Перепишу свойство со слайда или...
[21:08.000 --> 21:10.000]  Ладно, может быть его...
[21:10.000 --> 21:11.000]  Вот.
[21:11.000 --> 21:13.000]  А в таком виде я тогда его оставлю здесь сразу.
[21:13.000 --> 21:14.000]  Пусть оно вот будет.
[21:14.000 --> 21:16.000]  Это просто свойство с предыдущего слайда.
[21:16.000 --> 21:18.000]  Здесь как раз x берется.
[21:18.000 --> 21:21.000]  x берется из x, вот этот.
[21:21.000 --> 21:23.000]  А x1 и x2 как раз они произволен.
[21:23.000 --> 21:25.000]  Здесь пока только x1 есть.
[21:25.000 --> 21:28.000]  Вот. Это просто свойство с предыдущего слайда.
[21:28.000 --> 21:31.000]  Для любого x и x1 вот оно будет справедливо.
[21:31.000 --> 21:34.000]  И x соответственно у нас принадлежит множеству.
[21:34.000 --> 21:39.000]  Вот. В качестве x давайте я возьму проекцию точки x2.
[21:39.000 --> 21:41.000]  Точки x2.
[21:41.000 --> 21:45.000]  И получу x1-проекция x1.
[21:45.000 --> 21:52.000]  Проекция x2-проекция x1.
[21:52.000 --> 21:54.000]  Это меньше либо равно 0.
[21:54.000 --> 21:57.000]  Вот. Аналогичным образом, просто поменяя переменные местами,
[21:57.000 --> 22:01.000]  я могу получить, что x2-проекция x2.
[22:01.000 --> 22:05.000]  Проекция x1-проекция x2.
[22:05.000 --> 22:07.000]  Меньше либо равна 0.
[22:07.000 --> 22:09.000]  Вот. Меньше либо равна 0.
[22:09.000 --> 22:12.000]  Теперь я вот эти два кусочка складываю.
[22:12.000 --> 22:13.000]  Складываю.
[22:13.000 --> 22:15.000]  Ну давайте что-нибудь получим из этого.
[22:15.000 --> 22:16.000]  Сложим и получим.
[22:16.000 --> 22:18.000]  Вот. Что тут получается?
[22:18.000 --> 22:22.000]  x1-проекция x1.
[22:22.000 --> 22:27.000]  Дальше, соответственно, минус x2 плюс проекция x2.
[22:27.000 --> 22:33.000]  И здесь общая будет проекция x2-проекция x1.
[22:33.000 --> 22:36.000]  Меньше либо равно 0.
[22:36.000 --> 22:38.000]  Согласны?
[22:38.000 --> 22:40.000]  Сложили. Сложили. Вроде все хорошо.
[22:40.000 --> 22:43.000]  Вот. Дальше я чуть-чуть сгруппирую это все безобразие
[22:43.000 --> 22:47.000]  следующим образом, следующим образом.
[22:47.000 --> 22:51.000]  Справа оставлю, то есть вот только вот эти члены
[22:51.000 --> 22:53.000]  с проекцией, вот.
[22:53.000 --> 22:55.000]  А все остальное перенесу влево.
[23:03.000 --> 23:06.000]  Это справа, ой, слева наоборот осталось, слева осталось.
[23:06.000 --> 23:10.000]  А вправо у меня ушло, соответственно, что там будет?
[23:10.000 --> 23:17.000]  x2-x1-проекция x2-проекция x1.
[23:17.000 --> 23:22.000]  Ну, что теперь у меня слева стоит?
[23:22.000 --> 23:24.000]  Что у меня слева стоит?
[23:24.000 --> 23:28.000]  Квадрат просто, да, стоит слева, слева стоит просто квадрат.
[23:28.000 --> 23:30.000]  Так.
[23:30.000 --> 23:31.000]  Как раз.
[23:31.000 --> 23:33.000]  Во, докрутил до этого.
[23:33.000 --> 23:36.000]  Слева у меня стоит квадрат.
[23:39.000 --> 23:42.000]  Ну, а справа вот это вот скалярное произведение.
[23:42.000 --> 23:43.000]  Вот.
[23:43.000 --> 23:45.000]  Что я могу применить к сексуальности?
[23:45.000 --> 23:46.000]  Вот.
[23:46.000 --> 23:49.000]  Что я могу применить к сексуально-скалярному произведению,
[23:49.000 --> 23:52.000]  чтобы его как-то оценить через нормы?
[23:52.000 --> 23:54.000]  КБШ.
[23:54.000 --> 23:56.000]  Соответственно, применяю КБШ.
[23:56.000 --> 23:57.000]  Вот.
[23:57.000 --> 23:59.000]  И что у меня здесь получается?
[23:59.000 --> 24:01.000]  Разность проекций.
[24:04.000 --> 24:06.000]  И разность x.
[24:06.000 --> 24:08.000]  Ну и все, готово.
[24:08.000 --> 24:12.000]  Видно, что один у меня квадрат тут сокращается,
[24:12.000 --> 24:15.000]  и это уходит, и ровно то, что нам нужно было доказать.
[24:15.000 --> 24:16.000]  Вот.
[24:16.000 --> 24:18.000]  Получается, что действительно оператор проекции
[24:18.000 --> 24:20.000]  только может уменьшать расстояние,
[24:20.000 --> 24:23.000]  ну, не увеличивать расстояние между точками.
[24:23.000 --> 24:24.000]  Между точками.
[24:24.000 --> 24:25.000]  Вот.
[24:25.000 --> 24:27.000]  Хорошее свойство.
[24:27.000 --> 24:29.000]  Последнее свойство, которое нам понадобится.
[24:29.000 --> 24:30.000]  Вот такое вот.
[24:30.000 --> 24:34.000]  Оказывается, что вот оператор проекции,
[24:34.000 --> 24:37.000]  у оператора проекции, ну, вот в некотором смысле есть такая,
[24:37.000 --> 24:41.000]  что ли, у градиентного спуска с оператором проекции
[24:41.000 --> 24:43.000]  есть стационарная точка.
[24:43.000 --> 24:44.000]  Угу.
[24:44.000 --> 24:46.000]  Ну, такое вот свойство.
[24:46.000 --> 24:48.000]  Ну, вот пусть если у нас там градиентный спуск
[24:48.000 --> 24:50.000]  это оператор, с проекцией это оператор t,
[24:50.000 --> 24:52.000]  ну, у него засыпихивана точка x звездой,
[24:52.000 --> 24:55.000]  ну, оказывается, она отображается в точку x звездой.
[24:55.000 --> 24:56.000]  Вот.
[24:56.000 --> 24:59.000]  Ну, грубо говоря, то, что если вы нашли уже оптимальную точку
[24:59.000 --> 25:01.000]  на вашем множестве x, вот,
[25:01.000 --> 25:03.000]  и пытаетесь сделать градиентный шаг,
[25:03.000 --> 25:05.000]  вы уже никуда, понятно, не уйдете.
[25:05.000 --> 25:06.000]  Вот.
[25:06.000 --> 25:08.000]  Вы просто пытаетесь выйти за пределы множества
[25:08.000 --> 25:10.000]  и вас возвращают обратно.
[25:10.000 --> 25:11.000]  Что логично.
[25:11.000 --> 25:13.000]  Что просто минимума больше нигде нет.
[25:13.000 --> 25:15.000]  Вам градиент показывает на направлении убывания,
[25:15.000 --> 25:17.000]  вы по нему пытаетесь идти,
[25:17.000 --> 25:20.000]  но это вас сразу же уводит за пределы множества.
[25:20.000 --> 25:24.000]  В пределах множества ничего такого хорошего и нету.
[25:24.000 --> 25:26.000]  Ну, давайте это формально докажем.
[25:26.000 --> 25:28.000]  Формально докажем.
[25:28.000 --> 25:30.000]  Следующим образом.
[25:30.000 --> 25:32.000]  Так как у нас оператор проекции,
[25:32.000 --> 25:35.000]  вот этого x звездой
[25:35.000 --> 25:39.000]  минус гамма f от x звездой,
[25:39.000 --> 25:41.000]  это же просто аргуминимум,
[25:41.000 --> 25:43.000]  поэтому давайте, ну, так и запишем.
[25:43.000 --> 25:45.000]  Аргуминимум, вот, x
[25:45.000 --> 25:47.000]  из некоторого множества x,
[25:47.000 --> 25:49.000]  x минус x звездой
[25:49.000 --> 25:51.000]  минус гамма
[25:51.000 --> 25:53.000]  f от x звездой
[25:53.000 --> 25:55.000]  в квадрате.
[25:55.000 --> 25:57.000]  Окей.
[25:57.000 --> 25:59.000]  Давайте чуть-чуть поиграемся
[25:59.000 --> 26:01.000]  с выражением под аргуминимум,
[26:01.000 --> 26:03.000]  я его просто пораскрываю.
[26:03.000 --> 26:05.000]  Этот квадрат, соответственно, у меня там были с квадрат,
[26:05.000 --> 26:07.000]  я его раскрою.
[26:07.000 --> 26:09.000]  В квадрате.
[26:09.000 --> 26:11.000]  Минус...
[26:11.000 --> 26:13.000]  Что там будет сейчас получаться?
[26:13.000 --> 26:15.000]  Минус...
[26:15.000 --> 26:17.000]  А, тут плюс.
[26:17.000 --> 26:19.000]  Минус был, состав плюс.
[26:19.000 --> 26:21.000]  Вот.
[26:21.000 --> 26:23.000]  Плюс 2 гамма f от x звездой.
[26:23.000 --> 26:25.000]  x минус x звездой.
[26:25.000 --> 26:27.000]  Скалярно.
[26:29.000 --> 26:31.000]  Плюс гамма в квадрате
[26:31.000 --> 26:33.000]  f от x звездой.
[26:33.000 --> 26:35.000]  В квадрате.
[26:35.000 --> 26:37.000]  Окей.
[26:37.000 --> 26:39.000]  С точки зрения
[26:39.000 --> 26:41.000]  вообще аргминимума,
[26:41.000 --> 26:43.000]  кто понимает, мне вообще важно
[26:43.000 --> 26:45.000]  вот этот последний член
[26:45.000 --> 26:47.000]  с нормой градиента.
[26:47.000 --> 26:49.000]  Именно аргминимума.
[26:49.000 --> 26:51.000]  Он от x не зависит,
[26:51.000 --> 26:53.000]  поэтому, ну,
[26:53.000 --> 26:55.000]  именно значение,
[26:55.000 --> 26:57.000]  когда я спрашиваю аргминимум,
[26:57.000 --> 26:59.000]  мне же нужно просто вернуть значение x.
[26:59.000 --> 27:01.000]  Вот. А когда я уж, конечно,
[27:01.000 --> 27:03.000]  спрашиваю значение функции,
[27:03.000 --> 27:05.000]  нужно знать, что вот какие-то дополнительные
[27:05.000 --> 27:07.000]  константы, которые могут ее уменьшать или
[27:07.000 --> 27:09.000]  увеличивать. Вот. Но именно с точки зрения
[27:09.000 --> 27:11.000]  x, вот это нам вообще не важно.
[27:11.000 --> 27:13.000]  То есть, он не влияет на значение
[27:13.000 --> 27:15.000]  x. Вот. Поэтому
[27:15.000 --> 27:17.000]  из аргминимума можно последний кусочек
[27:17.000 --> 27:19.000]  выкинуть. Последний кусочек
[27:19.000 --> 27:21.000]  выкинуть. А теперь скажите мне,
[27:21.000 --> 27:23.000]  что вы можете сказать про
[27:23.000 --> 27:25.000]  первый кусок и второй кусок?
[27:25.000 --> 27:27.000]  Ну, про первый кусок вы можете сказать, что он
[27:27.000 --> 27:29.000]  больше либо равен нуля точно, да?
[27:29.000 --> 27:31.000]  Вот. А второй?
[27:31.000 --> 27:33.000]  Второй.
[27:33.000 --> 27:35.000]  Если гамма положительная.
[27:39.000 --> 27:41.000]  Зачем уже оценивать? Если это же
[27:41.000 --> 27:43.000]  ровно то, что у нас было в условии
[27:43.000 --> 27:45.000]  оптимальности. В условии
[27:45.000 --> 27:47.000]  оптимальности доказали, что вот это
[27:47.000 --> 27:49.000]  оно у нас всегда какое?
[27:49.000 --> 27:51.000]  Ну, это всегда какое?
[27:51.000 --> 27:53.000]  Вот.
[27:53.000 --> 27:55.000]  А оно всегда какое?
[27:55.000 --> 27:57.000]  Больше либо равно нуля.
[27:57.000 --> 27:59.000]  Получается, что вот то выражение,
[27:59.000 --> 28:01.000]  которое у нас написано здесь, это сумма выражений,
[28:01.000 --> 28:03.000]  которые больше либо равны нуля.
[28:03.000 --> 28:05.000]  Так? То есть, ну, в лучшем
[28:05.000 --> 28:07.000]  случае они равны нулю. Да?
[28:07.000 --> 28:09.000]  И ноль, как вы понимаете, он
[28:09.000 --> 28:11.000]  достижим. В какой точке?
[28:11.000 --> 28:13.000]  Иксозвездой. Всё.
[28:13.000 --> 28:15.000]  Конец.
[28:17.000 --> 28:19.000]  Вот так.
[28:19.000 --> 28:21.000]  Соответственно, получается, что действительно
[28:21.000 --> 28:23.000]  у нас вот проекция подействует
[28:23.000 --> 28:25.000]  на градиентный спуск с точки
[28:25.000 --> 28:27.000]  иксозвездой. Она просто свернет точку иксозвездой.
[28:27.000 --> 28:29.000]  Вот так вот.
[28:29.000 --> 28:31.000]  Всё. С этим свойствами
[28:31.000 --> 28:33.000]  покончили. Они вроде бы более-менее физичны.
[28:33.000 --> 28:35.000]  То есть, особенно два последних. Два последних
[28:35.000 --> 28:37.000]  нам в принципе и понадобится.
[28:37.000 --> 28:39.000]  Для доказательства сходимости
[28:39.000 --> 28:41.000]  градиентного спуска.
[28:41.000 --> 28:43.000]  Для доказательства сходимости градиентного спуска.
[28:43.000 --> 28:45.000]  Пум-пум-пум.
[28:45.000 --> 28:47.000]  Так.
[28:47.000 --> 28:49.000]  Окей. Поехали
[28:49.000 --> 28:51.000]  подобраться с доказательством градиентного спуска.
[28:51.000 --> 28:53.000]  Здесь я выписал просто
[28:53.000 --> 28:55.000]  ровно то, как мы с вами начинали
[28:55.000 --> 28:57.000]  доказательство обычного градиентного спуска
[28:57.000 --> 28:59.000]  без проекции.
[28:59.000 --> 29:01.000]  Смотрим расстояние
[29:01.000 --> 29:03.000]  между икска.
[29:03.000 --> 29:05.000]  Только тут вверх надо было.
[29:07.000 --> 29:09.000]  Съехало. Ладно.
[29:09.000 --> 29:11.000]  Икска. Смотрим расстояние между икска и
[29:11.000 --> 29:13.000]  иксозвездой. Подставляем
[29:13.000 --> 29:15.000]  икскатой.
[29:15.000 --> 29:17.000]  Выражение для икска плюс один
[29:17.000 --> 29:19.000]  в нашу расстояние
[29:19.000 --> 29:21.000]  до решения.
[29:21.000 --> 29:23.000]  И, соответственно, давайте что-нибудь с ним делать.
[29:23.000 --> 29:25.000]  Давайте сначала
[29:25.000 --> 29:27.000]  воспользуемся нашим
[29:27.000 --> 29:29.000]  последним свойством.
[29:29.000 --> 29:31.000]  Потому что мы знаем, что у нас
[29:31.000 --> 29:33.000]  иксозвездой на самом деле
[29:33.000 --> 29:35.000]  равен просто проекции.
[29:35.000 --> 29:37.000]  Иксозвездой минус градиент.
[29:37.000 --> 29:39.000]  Поэтому здесь я в некотором смысле
[29:39.000 --> 29:41.000]  его заменю.
[29:41.000 --> 29:43.000]  γк
[29:43.000 --> 29:45.000]  fxk
[29:45.000 --> 29:47.000]  проекция
[29:47.000 --> 29:49.000]  минус проекция иксозвездой
[29:49.000 --> 29:51.000]  минус γк
[29:51.000 --> 29:53.000]  f иксозвездой
[29:53.000 --> 29:55.000]  в квадрате.
[29:55.000 --> 29:57.000]  Вот. А теперь
[29:57.000 --> 29:59.000]  чем воспользуемся?
[29:59.000 --> 30:01.000]  Так как у нас тут разность двух проекций.
[30:01.000 --> 30:03.000]  Что можно сделать?
[30:05.000 --> 30:07.000]  Пожалуйста.
[30:07.000 --> 30:09.000]  Липшицевая, но экспансивность
[30:09.000 --> 30:11.000]  этого оператора. То есть то, что он
[30:11.000 --> 30:13.000]  не
[30:13.000 --> 30:15.000]  увеличивает расстояние между точками,
[30:15.000 --> 30:17.000]  которые стоят под проекциями.
[30:17.000 --> 30:19.000]  Ну, я это здесь и выпишу.
[30:19.000 --> 30:21.000]  Соответственно.
[30:31.000 --> 30:33.000]  Ну, смотрите. Уже очень хорошее
[30:33.000 --> 30:35.000]  выражение получается.
[30:35.000 --> 30:37.000]  x как x минус x звездой.
[30:37.000 --> 30:39.000]  Вот. Давайте расписывать
[30:39.000 --> 30:41.000]  как раньше. Как раз расстояние
[30:41.000 --> 30:43.000]  до решения в текущий момент времени.
[30:43.000 --> 30:45.000]  Вот.
[30:45.000 --> 30:47.000]  И плюс
[30:47.000 --> 30:49.000]  что там будет выскакивать дополнительно?
[30:49.000 --> 30:51.000]  xk в квадрате.
[30:51.000 --> 30:53.000]  Тут соответственно выскочит как раз
[30:53.000 --> 30:55.000]  разность градиентов.
[30:55.000 --> 30:57.000]  И это, кстати, даже хорошо. Почему?
[30:57.000 --> 30:59.000]  Потому что, помните, градиент на вспуске
[30:59.000 --> 31:01.000]  у нас просто выскакивал градиент, да?
[31:01.000 --> 31:03.000]  А нам нужно было воспользоваться
[31:03.000 --> 31:05.000]  липшицевостью. Нам как раз нужно было
[31:05.000 --> 31:07.000]  вот такое что-то, да?
[31:07.000 --> 31:09.000]  Ну, мы просто искусственно
[31:09.000 --> 31:11.000]  добавляли этот нулевой градиент
[31:11.000 --> 31:13.000]  просто потому что у нас задача была
[31:13.000 --> 31:15.000]  безусловная. В точке x звездой он был
[31:15.000 --> 31:17.000]  нулевой, поэтому здесь добавили. Здесь мы его в некотором
[31:17.000 --> 31:19.000]  смысле добавили.
[31:19.000 --> 31:21.000]  Он уже не нулевой, но мы его добавили с помощью
[31:21.000 --> 31:23.000]  вот этого свойства проекции.
[31:23.000 --> 31:25.000]  Свойства проекции, что в точке
[31:25.000 --> 31:27.000]  x звездой там все хорошо.
[31:27.000 --> 31:29.000]  Окей. И плюс еще удвоенное скалярное
[31:29.000 --> 31:31.000]  произведение. Там должно
[31:31.000 --> 31:33.000]  остаться, ну, тут только со знака минус.
[31:33.000 --> 31:35.000]  Здесь там еще f от
[31:35.000 --> 31:37.000]  xкт минус
[31:37.000 --> 31:39.000]  x звездой
[31:39.000 --> 31:41.000]  умножить на xкт
[31:41.000 --> 31:43.000]  минус x звездой.
[31:43.000 --> 31:45.000]  Вот.
[31:45.000 --> 31:47.000]  В принципе, уже очень близко
[31:47.000 --> 31:49.000]  к тому, что мы имели в градиентном спуске.
[31:49.000 --> 31:51.000]  Соответственно, вот это
[31:51.000 --> 31:53.000]  мы уже знаем, что надо... Первый кусочек
[31:53.000 --> 31:55.000]  мы знаем, что по липшицевости надо.
[31:55.000 --> 31:57.000]  Вот этот кусочек по липшицевости.
[31:57.000 --> 31:59.000]  Вот что-то вот такое
[31:59.000 --> 32:01.000]  по, соответственно,
[32:01.000 --> 32:03.000]  здесь, соответственно, чего?
[32:03.000 --> 32:05.000]  l гладкость,
[32:05.000 --> 32:07.000]  вот для этого
[32:07.000 --> 32:09.000]  безобразие, мю
[32:09.000 --> 32:11.000]  сильная выпуклость.
[32:11.000 --> 32:13.000]  Единственное, что вот осталось вот это
[32:13.000 --> 32:15.000]  скалярное произведение, неприятное, которое
[32:15.000 --> 32:17.000]  завязано на f от x звездой.
[32:17.000 --> 32:19.000]  Причем оно, видите, оно положительное.
[32:19.000 --> 32:21.000]  Потому что...
[32:21.000 --> 32:23.000]  А?
[32:23.000 --> 32:25.000]  Где-где-где? Потому что я просто раскрыл квадрат.
[32:31.000 --> 32:33.000]  Вот.
[32:33.000 --> 32:35.000]  Сильная выпуклость, вот к этому применению,
[32:35.000 --> 32:37.000]  то, что зелененьким выделил, сильная выпуклость.
[32:37.000 --> 32:39.000]  И это гладкость, через гладкость расписывается.
[32:39.000 --> 32:41.000]  Насталось неприятное скалярное произведение.
[32:41.000 --> 32:43.000]  При этом оно положительное, потому что,
[32:43.000 --> 32:45.000]  видите, минус на минус даст вам плюс.
[32:45.000 --> 32:47.000]  Ну и, соответственно,
[32:47.000 --> 32:49.000]  будет у вас градиент f от x звездой
[32:49.000 --> 32:51.000]  на x ката минус x звездой.
[32:51.000 --> 32:53.000]  А мы знаем, что из условия оптимальности
[32:53.000 --> 32:55.000]  этот член, он положительный.
[32:55.000 --> 32:57.000]  Вот. И нужно бы что-нибудь
[32:57.000 --> 32:59.000]  с ним поделать.
[32:59.000 --> 33:01.000]  На самом деле ничего страшного он вам не даст.
[33:01.000 --> 33:03.000]  Если вы помните, что мы с вами, как мы с вами сейчас...
[33:03.000 --> 33:05.000]  Так.
[33:05.000 --> 33:07.000]  Вот.
[33:07.000 --> 33:09.000]  Дошли до этого. Давайте
[33:09.000 --> 33:11.000]  вы поможете расписать гладкость.
[33:11.000 --> 33:13.000]  Вот здесь вот гладкость, что мы там с вами, как расписывали?
[33:13.000 --> 33:15.000]  Как это можно оценить через гладкость?
[33:15.000 --> 33:17.000]  Что там вылезало?
[33:17.000 --> 33:19.000]  Кто помнит?
[33:23.000 --> 33:25.000]  Что там вылезало из выражения
[33:25.000 --> 33:27.000]  для гладкости?
[33:27.000 --> 33:29.000]  Разность функций и как раз
[33:29.000 --> 33:31.000]  это солярное произведение.
[33:31.000 --> 33:33.000]  Что-то вот такое.
[33:35.000 --> 33:37.000]  x ката минус f от x звездой
[33:39.000 --> 33:41.000]  плюс, соответственно,
[33:43.000 --> 33:45.000]  f от x звездой
[33:51.000 --> 33:53.000]  минус, да, минус x
[33:53.000 --> 33:55.000]  минус x звездой. Вот так.
[33:55.000 --> 33:57.000]  Вот такое вот выражение
[33:57.000 --> 33:59.000]  вылезало там.
[33:59.000 --> 34:01.000]  Но в безусловном случае, в чем мы говорили?
[34:01.000 --> 34:03.000]  Вот этот равен нулю в силу того,
[34:03.000 --> 34:05.000]  что градиент равен нулю.
[34:05.000 --> 34:07.000]  Мы так говорили, просто его уничтожали
[34:09.000 --> 34:11.000]  в безусловном случае.
[34:11.000 --> 34:13.000]  Здесь, соответственно, мы так сделать не можем.
[34:13.000 --> 34:15.000]  Но, как вы понимаете,
[34:15.000 --> 34:17.000]  тут в этом плане
[34:17.000 --> 34:19.000]  все будет довольно хорошо,
[34:19.000 --> 34:21.000]  потому что мы знаем, что вот этот кусочек
[34:21.000 --> 34:23.000]  он у нас
[34:23.000 --> 34:25.000]  положительный.
[34:25.000 --> 34:27.000]  Давайте четко это распишу,
[34:27.000 --> 34:29.000]  как это все можно аккуратно
[34:29.000 --> 34:31.000]  уничтожить. Вот только это не надо стирать.
[34:33.000 --> 34:35.000]  Теперь мы не можем уничтожить как раньше,
[34:35.000 --> 34:37.000]  но давайте это оставим.
[34:37.000 --> 34:39.000]  Во-первых, я знаю, что вот это выражение
[34:39.000 --> 34:41.000]  оно у меня положительное
[34:41.000 --> 34:43.000]  в силу того, что оно как раз оценивает
[34:43.000 --> 34:45.000]  что-то положительное сверху.
[34:45.000 --> 34:47.000]  На самом деле
[34:47.000 --> 34:49.000]  для этого выражения даже есть специальное
[34:49.000 --> 34:51.000]  обозначение, его называют
[34:51.000 --> 34:53.000]  дивергенцией Брегмана в функции
[34:53.000 --> 34:55.000]  порожденной функции f.
[34:55.000 --> 34:57.000]  Это ровно его определение.
[34:57.000 --> 34:59.000]  То есть наберутся две точки
[34:59.000 --> 35:01.000]  x и y, и, соответственно,
[35:01.000 --> 35:03.000]  если записать такое выражение,
[35:03.000 --> 35:05.000]  получается дивергенция Брегмана.
[35:05.000 --> 35:07.000]  Она положительная, ее можно использовать
[35:07.000 --> 35:09.000]  в качестве расстояния. Здесь нам, в принципе,
[35:09.000 --> 35:11.000]  достаточно того, что эта вещь
[35:11.000 --> 35:13.000]  положительная.
[35:13.000 --> 35:15.000]  Как ее использовать в качестве расстояния мы с вами будем
[35:15.000 --> 35:17.000]  рассуждать через две лекции.
[35:17.000 --> 35:19.000]  Пока вот так.
[35:19.000 --> 35:21.000]  Просто у нас есть какое-то выражение, которое
[35:21.000 --> 35:23.000]  положительное, не отрицательное.
[35:23.000 --> 35:25.000]  Дивергенция Брегмана.
[35:25.000 --> 35:27.000]  Смотрите,
[35:27.000 --> 35:29.000]  а теперь давайте вот здесь,
[35:29.000 --> 35:31.000]  для вот этих кусочков,
[35:31.000 --> 35:33.000]  воспользуемся сильной выпуклостью.
[35:33.000 --> 35:35.000]  Сильной выпуклостью.
[35:35.000 --> 35:37.000]  Здесь у меня что-то тогда выскочит.
[35:37.000 --> 35:39.000]  Два гаммака,
[35:39.000 --> 35:41.000]  разной функции,
[35:45.000 --> 35:47.000]  плюс, соответственно,
[35:47.000 --> 35:49.000]  как раз сильная выпуклость.
[35:51.000 --> 35:53.000]  И, соответственно, там еще останется
[35:53.000 --> 35:55.000]  кусок,
[35:55.000 --> 35:57.000]  который зависит
[35:57.000 --> 35:59.000]  от скалярного произведения
[35:59.000 --> 36:01.000]  в x звездой.
[36:01.000 --> 36:03.000]  xk
[36:03.000 --> 36:05.000]  минус x звездой.
[36:07.000 --> 36:09.000]  Вот.
[36:09.000 --> 36:11.000]  И вот здесь вот видно, что я вот это
[36:11.000 --> 36:13.000]  могу переписать в каком виде.
[36:13.000 --> 36:15.000]  Когда я сгруппирую вот это
[36:15.000 --> 36:17.000]  и вот это.
[36:17.000 --> 36:19.000]  Видно, что здесь вылезает тоже
[36:19.000 --> 36:21.000]  дивергенция Брегмана.
[36:21.000 --> 36:23.000]  Видно?
[36:23.000 --> 36:25.000]  Как раз вот тут будет как бы
[36:25.000 --> 36:27.000]  минус, здесь плюс, разные знаки,
[36:27.000 --> 36:29.000]  разность функций.
[36:29.000 --> 36:31.000]  И получается разность функций и минус скалярное
[36:31.000 --> 36:33.000]  произведение. Поэтому здесь просто
[36:33.000 --> 36:35.000]  вылезет дивергенция Брегмана.
[36:35.000 --> 36:37.000]  Дивергенция Брегмана
[36:37.000 --> 36:39.000]  xk
[36:39.000 --> 36:41.000]  x звездой.
[36:41.000 --> 36:43.000]  Ну, и здесь будет
[36:43.000 --> 36:45.000]  все еще здесь.
[36:45.000 --> 36:47.000]  Согласны?
[36:47.000 --> 36:49.000]  Окей.
[36:49.000 --> 36:51.000]  Окей.
[36:51.000 --> 36:53.000]  Вот. Выскочилась дивергенция Брегмана.
[36:53.000 --> 36:55.000]  Выскочила мюшка.
[36:55.000 --> 36:57.000]  Ну, давайте теперь это все аккуратненько
[36:57.000 --> 36:59.000]  схлопнем.
[36:59.000 --> 37:01.000]  Так.
[37:01.000 --> 37:03.000]  Ввели дивергенцию Брегмана.
[37:03.000 --> 37:05.000]  Все, я вот здесь тоже самое сделал,
[37:05.000 --> 37:07.000]  расписал это все аккуратно.
[37:07.000 --> 37:09.000]  И здесь у меня вот вылезло такое выражение.
[37:09.000 --> 37:11.000]  Вот. Ну и что мы там дальше
[37:11.000 --> 37:13.000]  с вами делали, кто помнит?
[37:13.000 --> 37:15.000]  Кто там нужно было сделать, чтобы
[37:15.000 --> 37:17.000]  все хорошо получить?
[37:17.000 --> 37:19.000]  Вот.
[37:19.000 --> 37:21.000]  Да, подбирали γк.
[37:21.000 --> 37:23.000]  Вот мы смотрели вот на это выражение,
[37:23.000 --> 37:25.000]  потому что оно в принципе нам мешает.
[37:25.000 --> 37:27.000]  Вот здесь вот как бы есть расстояние до решения.
[37:27.000 --> 37:29.000]  Здесь есть как бы расстояние до решения на предыдущей
[37:29.000 --> 37:31.000]  итерации. И видно, что они вроде как уменьшаются.
[37:31.000 --> 37:33.000]  Причем там геометрической скоростью.
[37:33.000 --> 37:35.000]  Вот. Нужно подобрать
[37:35.000 --> 37:37.000]  γк, чтобы вот это просто ушло.
[37:37.000 --> 37:39.000]  Вот. Но в силу того, что мы знаем, что
[37:39.000 --> 37:41.000]  дивергенция Брегмана положительная,
[37:41.000 --> 37:43.000]  проблем нет. Мы подбираем γк,
[37:43.000 --> 37:45.000]  ровно так же, как раньше. Берем
[37:45.000 --> 37:47.000]  γк меньше, чем 1 делить на L.
[37:47.000 --> 37:49.000]  Вот. И вот это выражение
[37:49.000 --> 37:51.000]  у нас сразу же уходит.
[37:51.000 --> 37:53.000]  Так. Все.
[37:53.000 --> 37:55.000]  Доказательство остается прежним.
[37:55.000 --> 37:57.000]  Вот. Дальше уже можно
[37:57.000 --> 37:59.000]  шаги ровно те же повторять, какие мы делали
[37:59.000 --> 38:01.000]  градиент на спуске, запускали рекурсы,
[38:01.000 --> 38:03.000]  получали оценку сходимости.
[38:03.000 --> 38:05.000]  Так вот здесь вот нужно было чуть-чуть
[38:05.000 --> 38:07.000]  поиграться в дивергенцию Брегмана.
[38:07.000 --> 38:09.000]  На самом деле, в безусловном случае,
[38:09.000 --> 38:11.000]  мы тоже как бы ее и рассматривали.
[38:11.000 --> 38:13.000]  Просто там градиент равнялся нулю,
[38:13.000 --> 38:15.000]  и дивергенция была в некотором смысле
[38:15.000 --> 38:17.000]  усеченная такая, более приятная,
[38:17.000 --> 38:19.000]  потому что там стояла разность функций.
[38:19.000 --> 38:21.000]  Хорошо. Хорошо.
[38:23.000 --> 38:25.000]  Вот. Соответственно, да,
[38:25.000 --> 38:27.000]  теория именно сходимости для градиентного
[38:27.000 --> 38:29.000]  спуска с проекции и без проекции, они
[38:29.000 --> 38:31.000]  оценки абсолютно одинаковые.
[38:31.000 --> 38:33.000]  То есть сублинейная сходимость
[38:33.000 --> 38:35.000]  в выпуклом случае, гладком,
[38:35.000 --> 38:37.000]  гладком выпуклом случае.
[38:37.000 --> 38:39.000]  А линейная сходимость
[38:39.000 --> 38:41.000]  в
[38:41.000 --> 38:43.000]  сильно выпуклом, гладком случае.
[38:43.000 --> 38:45.000]  Вот. И получается, что на самом деле
[38:45.000 --> 38:47.000]  итерационные и аракульные
[38:47.000 --> 38:49.000]  сложности для
[38:49.000 --> 38:51.000]  градиентного спуска,
[38:51.000 --> 38:53.000]  что с проекцией, что без проекции совпадают.
[38:53.000 --> 38:55.000]  Единственная проблема,
[38:55.000 --> 38:57.000]  что
[38:57.000 --> 38:59.000]  градиентный спуск с проекцией
[38:59.000 --> 39:01.000]  имеет более высокую
[39:01.000 --> 39:03.000]  аналитическую сложность в связи
[39:03.000 --> 39:05.000]  с чем? В связи с тем, что у него есть оператор
[39:05.000 --> 39:07.000]  и он предлагает дополнительные вычисления.
[39:07.000 --> 39:09.000]  Потому что на самом деле
[39:09.000 --> 39:11.000]  решение задачи
[39:11.000 --> 39:13.000]  вот этого arg-minimum проекции
[39:13.000 --> 39:15.000]  это же дополнительная оптимизационная задача.
[39:15.000 --> 39:17.000]  И вам никто не гарантирует, что в общем
[39:17.000 --> 39:19.000]  случае у него есть решение
[39:19.000 --> 39:21.000]  в явном виде.
[39:21.000 --> 39:23.000]  В явном виде нужно, соответственно,
[39:23.000 --> 39:25.000]  это отрешивать дополнительно.
[39:25.000 --> 39:27.000]  Понятно, что для каких-то хороших множеств
[39:27.000 --> 39:29.000]  эти решения есть.
[39:29.000 --> 39:31.000]  Эти решения есть. Ну, например,
[39:31.000 --> 39:33.000]  шарик L2 легко спроецироваться
[39:33.000 --> 39:35.000]  и въевклидывать они друг другу
[39:35.000 --> 39:37.000]  в похожем расстоянии. Это одно и то же
[39:37.000 --> 39:39.000]  расстояние, что в этом шаре используется,
[39:39.000 --> 39:41.000]  что в проекции вы просто в некотором смысле
[39:41.000 --> 39:43.000]  вот находитесь вокруг этого шарика,
[39:43.000 --> 39:45.000]  здесь берете центр, ну и вот сюда приходите.
[39:45.000 --> 39:47.000]  Проекция.
[39:47.000 --> 39:49.000]  Если, соответственно, у вас просто какие-то ограничения
[39:49.000 --> 39:51.000]  типа кубика с двух сторон XI
[39:51.000 --> 39:53.000]  ограничены, ну вы просто
[39:53.000 --> 39:55.000]  если, соответственно, вышли за пределы кубика,
[39:55.000 --> 39:57.000]  ну вот, соответственно, туда вас и вернули.
[39:57.000 --> 39:59.000]  По грани. Ну, для линейных
[39:59.000 --> 40:01.000]  даже ограничений, когда у вас
[40:01.000 --> 40:03.000]  ограничения заданы какой-то системой
[40:03.000 --> 40:05.000]  линейных уравнений, тоже можно на самом деле
[40:05.000 --> 40:07.000]  спроецироваться, но видно, что уже не так
[40:07.000 --> 40:09.000]  дешево. Один раз нужно посчитать обратную
[40:09.000 --> 40:11.000]  матрицу, может, она отраспонирована,
[40:11.000 --> 40:13.000]  но каждую эту рацию нужно будет
[40:13.000 --> 40:15.000]  считать матрично-векторные вычисления.
[40:15.000 --> 40:17.000]  Что тоже
[40:17.000 --> 40:19.000]  ни разу не дешево.
[40:19.000 --> 40:21.000]  Поэтому вообще
[40:21.000 --> 40:23.000]  оператор проекции считается
[40:23.000 --> 40:25.000]  довольно дорогим, то есть
[40:25.000 --> 40:27.000]  для мало чего он существует,
[40:27.000 --> 40:29.000]  именно в явном виде,
[40:29.000 --> 40:31.000]  для некоторых хороших множеств
[40:31.000 --> 40:33.000]  существуют эффективные алгоритмы
[40:33.000 --> 40:35.000]  проекции, которые тоже
[40:35.000 --> 40:37.000]  на самом деле пришлось довольно долго
[40:37.000 --> 40:39.000]  разрабатывать комьюнити,
[40:39.000 --> 40:41.000]  но для многих множество,
[40:41.000 --> 40:43.000]  даже когда есть аналитическое решение
[40:43.000 --> 40:45.000]  и эффективный алгоритм, это дорого,
[40:45.000 --> 40:47.000]  проецироваться дорого.
[40:47.000 --> 40:49.000]  И как уйти, соответственно, от проекции
[40:49.000 --> 40:51.000]  и прийти к чему-то более дешевому,
[40:51.000 --> 40:53.000]  к какому-то альтернативному варианту,
[40:53.000 --> 40:55.000]  на второй половине мы с вами и поговорим.
[40:55.000 --> 40:57.000]  Перерыв. Так, ну что?
[40:57.000 --> 40:59.000]  Давайте продолжать?
[40:59.000 --> 41:01.000]  Давайте продолжать.
[41:03.000 --> 41:05.000]  Так, давайте это.
[41:05.000 --> 41:07.000]  Важное объявление,
[41:07.000 --> 41:09.000]  я еще в телеграмм это тоже напишу.
[41:09.000 --> 41:11.000]  Я уезжаю в командировку,
[41:11.000 --> 41:13.000]  соответственно, следующие две лекции
[41:13.000 --> 41:15.000]  будут в режиме онлайн.
[41:15.000 --> 41:17.000]  Потом у вас контрольная,
[41:17.000 --> 41:19.000]  контрольная,
[41:19.000 --> 41:21.000]  она тоже будет на лекции в лекционное время.
[41:23.000 --> 41:25.000]  Ну, соответственно, ее проведут семинаристы.
[41:25.000 --> 41:27.000]  И вот уже там получается
[41:27.000 --> 41:29.000]  через три недели,
[41:29.000 --> 41:31.000]  то есть три лекционных времени,
[41:31.000 --> 41:33.000]  уже на четвертую неделю я уже буду очна.
[41:33.000 --> 41:35.000]  Следующие две лекции у нас онлайн,
[41:35.000 --> 41:37.000]  потом контрольная на лекции,
[41:37.000 --> 41:39.000]  потом уже возвращаемся в очный режим.
[41:43.000 --> 41:45.000]  Не-не, на лекции и в лекционное время.
[41:45.000 --> 41:47.000]  Контрольная в лекционное время.
[41:49.000 --> 41:51.000]  Ну и, соответственно, это формат контрольная,
[41:51.000 --> 41:53.000]  где-то на 2 балла
[41:53.000 --> 41:55.000]  из трех,
[41:55.000 --> 41:57.000]  где-то суммарно контрольная
[41:57.000 --> 41:59.000]  3,5 балла, на 2 балла из трех
[41:59.000 --> 42:01.000]  это задача уровня 10 минуток.
[42:01.000 --> 42:03.000]  Вот, 10 минуток, 8-10
[42:03.000 --> 42:05.000]  задач уровня 10 минуток.
[42:05.000 --> 42:07.000]  Ну, соответственно, одну 10 минутку вы там решаете,
[42:07.000 --> 42:09.000]  сколько? Две задачи.
[42:09.000 --> 42:11.000]  Ну и, соответственно, если будут 10 задач,
[42:11.000 --> 42:13.000]  то у вас где-то 50 минут, чтобы решить
[42:13.000 --> 42:15.000]  вот эту часть, набрать два балла,
[42:15.000 --> 42:17.000]  плюс еще полтора балла,
[42:17.000 --> 42:19.000]  это задача посложнее.
[42:19.000 --> 42:21.000]  Ну, чтобы, соответственно, у вас там еще минут 40,
[42:21.000 --> 42:23.000]  чтобы решать вот эти задачи
[42:23.000 --> 42:25.000]  и там что-то добрать.
[42:25.000 --> 42:27.000]  Вот, примерно такой формат.
[42:27.000 --> 42:29.000]  Вот, контрольный.
[42:29.000 --> 42:31.000]  Я надеюсь, ну, мне кажется,
[42:31.000 --> 42:33.000]  что это более-менее адекватно,
[42:33.000 --> 42:35.000]  то есть, как раз, если вы умеете решать 10 минут полностью,
[42:35.000 --> 42:37.000]  ну, довольно быстро так,
[42:37.000 --> 42:39.000]  за минуту, там, за час,
[42:39.000 --> 42:41.000]  если 10 минутку умеете решать, то оценку хорошо,
[42:41.000 --> 42:43.000]  а вам за предмет точно можно поставить.
[42:43.000 --> 42:45.000]  Ну, соответственно, 2 балла, поэтому есть.
[42:45.000 --> 42:47.000]  Если хотите что-то решить,
[42:47.000 --> 42:49.000]  ну, показывать, что знаете на отлично,
[42:49.000 --> 42:51.000]  то это сложнее.
[42:51.000 --> 42:53.000]  Поэтому есть время, чтобы и на это
[42:53.000 --> 42:55.000]  порешать.
[42:55.000 --> 42:57.000]  Да, я говорю, как 10 минут, как то,
[42:57.000 --> 42:59.000]  что вы получаете в начале,
[42:59.000 --> 43:01.000]  вот на 2 балла, это ровно вот
[43:01.000 --> 43:03.000]  такого уровня задачи.
[43:07.000 --> 43:09.000]  Кому-то давали в том числе задачи
[43:09.000 --> 43:11.000]  по сходимости методов,
[43:11.000 --> 43:13.000]  ну, не просто там написать
[43:13.000 --> 43:15.000]  сходимость там,
[43:15.000 --> 43:17.000]  итерацию и так далее, или теорему,
[43:17.000 --> 43:19.000]  ну, и немного подумать, там подобрать,
[43:19.000 --> 43:21.000]  если мы подберем шаг вот таким вот образом,
[43:21.000 --> 43:23.000]  какой получится метод.
[43:23.000 --> 43:25.000]  Кому-то писали градиентный спуск,
[43:25.000 --> 43:27.000]  ну, и вы там писали тяжелый шарик, на самом деле,
[43:27.000 --> 43:29.000]  тот же градиентный спуск, просто шаг такой вот.
[43:29.000 --> 43:31.000]  Ну, вот такого рода задачки небольшие.
[43:31.000 --> 43:33.000]  Подоказывать что-то
[43:33.000 --> 43:35.000]  про методы. То, что мы, например,
[43:35.000 --> 43:37.000]  на лекциях не доказывали,
[43:37.000 --> 43:39.000]  но факты такие маленькие, которые
[43:39.000 --> 43:41.000]  в строчку в две делаются.
[43:41.000 --> 43:43.000]  Такое тоже может встретиться.
[43:43.000 --> 43:45.000]  И как в первой части,
[43:45.000 --> 43:47.000]  так и во второй.
[43:51.000 --> 43:53.000]  Я надеюсь, что мы сделаем
[43:53.000 --> 43:55.000]  не два варианта контроля, а три,
[43:55.000 --> 43:57.000]  чтобы у вас было в некотором смысле пример,
[43:57.000 --> 43:59.000]  то есть один мы как пример просто разместим,
[43:59.000 --> 44:01.000]  чтобы вы посмотрели, как это выглядит.
[44:01.000 --> 44:03.000]  Окей, ладно, это по объявлениям,
[44:03.000 --> 44:05.000]  главное тут объявление, это то, что
[44:05.000 --> 44:07.000]  две следующие лекции в режиме онлайн.
[44:07.000 --> 44:09.000]  Так, хорошо, разговаривали
[44:09.000 --> 44:11.000]  про проекции, поняли, что это может быть
[44:11.000 --> 44:13.000]  дорого, вот.
[44:13.000 --> 44:15.000]  По факту проекция ведь это очень простая задача,
[44:15.000 --> 44:17.000]  с одной стороны, это квадратичная задача.
[44:17.000 --> 44:19.000]  Кажется, что
[44:19.000 --> 44:21.000]  легче особо не придумаешь,
[44:21.000 --> 44:23.000]  но что тогда может быть легче, чем квадратичная задача?
[44:25.000 --> 44:27.000]  Линейно, соответственно, да.
[44:27.000 --> 44:29.000]  Если проблема
[44:29.000 --> 44:31.000]  уже есть квадратичная, то давайте перейдем
[44:31.000 --> 44:33.000]  к линейной задаче.
[44:33.000 --> 44:35.000]  Понятно, что мы знаем, что
[44:35.000 --> 44:37.000]  на неограниченном множестве
[44:37.000 --> 44:39.000]  линейной задачи может не иметь решения,
[44:39.000 --> 44:41.000]  просто потому что вы в минус бесконечность свалитесь
[44:41.000 --> 44:43.000]  и все. Вот.
[44:43.000 --> 44:45.000]  И поэтому, конечно, вот такие
[44:45.000 --> 44:47.000]  линейные минимизации рассматривают
[44:47.000 --> 44:49.000]  и на эксе, который ограничен.
[44:49.000 --> 44:51.000]  Ограничен.
[44:51.000 --> 44:53.000]  Вот, чтобы эта линейная минимизация
[44:53.000 --> 44:55.000]  имела хотя бы какое-то конечное
[44:55.000 --> 44:57.000]  решение. Вот.
[44:57.000 --> 44:59.000]  Ну и на основе этой
[44:59.000 --> 45:01.000]  линейной минимизации
[45:01.000 --> 45:03.000]  возникает вопрос. А вообще
[45:03.000 --> 45:05.000]  она легко делается или нет?
[45:05.000 --> 45:07.000]  Легко делается или нет?
[45:07.000 --> 45:09.000]  Для некоторых множеств действительно да, легко.
[45:09.000 --> 45:11.000]  Например, тут приведены L1-шары,
[45:11.000 --> 45:13.000]  вероятность на симплекс, для которых
[45:13.000 --> 45:15.000]  проекцию делать
[45:15.000 --> 45:17.000]  нетривиально. То есть там
[45:17.000 --> 45:19.000]  под этим лежит такая хорошая теория
[45:19.000 --> 45:21.000]  и, соответственно, эффективные
[45:21.000 --> 45:23.000]  алгоритмы для этого всего
[45:23.000 --> 45:25.000]  разрабатывались не так
[45:25.000 --> 45:27.000]  быстро. Соответственно, да.
[45:27.000 --> 45:29.000]  В случае линейной задачи тут
[45:29.000 --> 45:31.000]  довольно просто.
[45:31.000 --> 45:33.000]  Довольно просто. Ну вот
[45:33.000 --> 45:35.000]  давайте на примере какого-нибудь, например,
[45:35.000 --> 45:37.000]  L1-шара
[45:37.000 --> 45:39.000]  и посмотрим, почему
[45:39.000 --> 45:41.000]  решение линейной задачи будет таким,
[45:41.000 --> 45:43.000]  будет таким.
[45:43.000 --> 45:45.000]  Вот. Почему, думайте,
[45:45.000 --> 45:47.000]  давайте порассуждаем, почему
[45:47.000 --> 45:49.000]  решение линейной минимизации
[45:49.000 --> 45:51.000]  для шарика
[45:51.000 --> 45:53.000]  будет вот вот таким
[45:53.000 --> 45:55.000]  или один шар.
[45:57.000 --> 45:59.000]  Ну смотрите, что мы тут делаем.
[45:59.000 --> 46:01.000]  Мы знаем, что у нас компоненты
[46:01.000 --> 46:03.000]  этого шарика в сумме дают
[46:03.000 --> 46:05.000]  меньше единички, да?
[46:05.000 --> 46:07.000]  Вот. А у нас есть вектор,
[46:07.000 --> 46:09.000]  вектор, который, ну вот, наш вектор
[46:09.000 --> 46:11.000]  g, который мы минимизируем,
[46:11.000 --> 46:13.000]  который мы рассматриваем в эту комбинацию,
[46:13.000 --> 46:15.000]  g1 и так далее,
[46:15.000 --> 46:17.000]  gn, ну и мы его скалярно
[46:17.000 --> 46:19.000]  как-то перемножаем с этим
[46:19.000 --> 46:21.000]  вектором s, s1 и так далее,
[46:21.000 --> 46:23.000]  sd.
[46:23.000 --> 46:25.000]  Вот. Хорошо. И мы
[46:25.000 --> 46:27.000]  хотим сделать эту линейную комбинацию
[46:27.000 --> 46:29.000]  как можно меньше, как можно меньше, при этом
[46:29.000 --> 46:31.000]  в некотором смысле можно говорить, что у нас
[46:31.000 --> 46:33.000]  ресурс, который у нас есть в x-ах,
[46:33.000 --> 46:35.000]  он ограничен. Мы ограничены единичкой.
[46:35.000 --> 46:37.000]  Вот. И кажется естественным
[46:37.000 --> 46:39.000]  в некотором смысле вот
[46:39.000 --> 46:41.000]  эту всю единичку пустить
[46:41.000 --> 46:43.000]  самую наименьшую координату по модулю.
[46:43.000 --> 46:45.000]  Да, наибольшую по модулю координату.
[46:45.000 --> 46:47.000]  Мы пускаем ее, например,
[46:47.000 --> 46:49.000]  находим эту наибольшую координату.
[46:49.000 --> 46:51.000]  Вот. Пускаем туда весь
[46:51.000 --> 46:53.000]  наш ресурс s, ну то есть
[46:53.000 --> 46:55.000]  берем как раз s параллельно
[46:55.000 --> 46:57.000]  этому направлению.
[46:57.000 --> 46:59.000]  Вот. Единственное, что нам понятно,
[46:59.000 --> 47:01.000]  нужно взять отрицательный
[47:01.000 --> 47:03.000]  знак g, чтобы у нас как раз был минимум.
[47:03.000 --> 47:05.000]  Потому что если мы просто возьмем это
[47:05.000 --> 47:07.000]  направление и возьмем знак g,
[47:07.000 --> 47:09.000]  то у нас будет как раз максимум.
[47:09.000 --> 47:11.000]  Мы будем параллельны прямо
[47:11.000 --> 47:13.000]  этой координате.
[47:13.000 --> 47:15.000]  Вот. Возьмем с минимумом, будет минус.
[47:15.000 --> 47:17.000]  Вот. Соответственно очень простое решение
[47:17.000 --> 47:19.000]  для l1 шара. Аналогичное рассуждение
[47:19.000 --> 47:21.000]  можно сделать, например, для симплекса.
[47:21.000 --> 47:23.000]  Ну и вот для l бесконечности.
[47:23.000 --> 47:25.000]  l бесконечного шара.
[47:25.000 --> 47:27.000]  Вот. Вот такая вот соответственная
[47:27.000 --> 47:29.000]  идея. То есть вот иногда
[47:29.000 --> 47:31.000]  вот эта линейная минимизация действительно
[47:31.000 --> 47:33.000]  значительно дешевле,
[47:33.000 --> 47:35.000]  чем проекция. Вот. И значительно
[47:35.000 --> 47:37.000]  легче в получении именно результата.
[47:37.000 --> 47:39.000]  Теоретического, аналитического
[47:39.000 --> 47:41.000]  решения. Вот. Но опять же
[47:41.000 --> 47:43.000]  это не панацея. Это не панацея
[47:43.000 --> 47:45.000]  когда-то проекция может быть лучше,
[47:45.000 --> 47:47.000]  когда-то может быть линейная
[47:47.000 --> 47:49.000]  минимизация лучше.
[47:49.000 --> 47:51.000]  Вот.
[47:51.000 --> 47:53.000]  Поэтому это в некотором смысле альтернатива.
[47:53.000 --> 47:55.000]  Но никак не замена методы
[47:55.000 --> 47:57.000]  проекции градиента. Ну и другим
[47:57.000 --> 47:59.000]  методом, который мы будем рассматривать дальше.
[47:59.000 --> 48:01.000]  Вот.
[48:01.000 --> 48:03.000]  Окей.
[48:03.000 --> 48:05.000]  На основе вот этого всего безобразия
[48:05.000 --> 48:07.000]  линейной оптимизации
[48:07.000 --> 48:09.000]  был сделан
[48:09.000 --> 48:11.000]  вот такой вот алгоритм. Алгоритм
[48:11.000 --> 48:13.000]  Франк Вульфа.
[48:13.000 --> 48:15.000]  Алгоритм Франк Вульфа
[48:15.000 --> 48:17.000]  70 лет назад.
[48:17.000 --> 48:19.000]  Вот выглядит следующим образом.
[48:19.000 --> 48:21.000]  Решаем линейную минимизацию,
[48:21.000 --> 48:23.000]  но мы ее решаем для
[48:23.000 --> 48:25.000]  градиента. То есть видите, мы смотрим
[48:25.000 --> 48:27.000]  на градиент.
[48:27.000 --> 48:29.000]  И тут можно на самом деле
[48:29.000 --> 48:31.000]  чуть-чуть, я вот мне это не добавил,
[48:31.000 --> 48:33.000]  но давайте это мы быстренько здесь сразу же
[48:33.000 --> 48:35.000]  проделаем, чтобы понять
[48:35.000 --> 48:37.000]  физику вот этого
[48:37.000 --> 48:39.000]  всего безобразия.
[48:39.000 --> 48:41.000]  Вот что нам вообще даст? Вот это
[48:41.000 --> 48:43.000]  линейная минимизация
[48:43.000 --> 48:45.000]  градиента.
[48:45.000 --> 48:47.000]  Линейная минимизация градиента. Вот.
[48:47.000 --> 48:49.000]  Опять же, в силу того, что аргуминимум
[48:49.000 --> 48:51.000]  это вещь у нас,
[48:51.000 --> 48:53.000]  которая может вбирать
[48:53.000 --> 48:55.000]  у тебя то, что не зависит от s
[48:55.000 --> 48:57.000]  за бесплатно.
[48:57.000 --> 48:59.000]  За бесплатно.
[48:59.000 --> 49:01.000]  Вот. Поэтому, поэтому
[49:01.000 --> 49:03.000]  что я хочу?
[49:03.000 --> 49:05.000]  Что я хочу? Я хочу вот в этот
[49:05.000 --> 49:07.000]  аргуминимум добавить
[49:07.000 --> 49:09.000]  x катой. Вот следующим
[49:09.000 --> 49:11.000]  образом.
[49:11.000 --> 49:13.000]  Понятно, что значение
[49:13.000 --> 49:15.000]  аргуминимума от этого не поменяется.
[49:15.000 --> 49:17.000]  Да?
[49:17.000 --> 49:19.000]  Просто потому что, ну, это скалярное произведение,
[49:19.000 --> 49:21.000]  которое я добавил, оно, ну, никак
[49:21.000 --> 49:23.000]  не зависит от s, поэтому
[49:23.000 --> 49:25.000]  на аргуминимум оно не влияет.
[49:25.000 --> 49:27.000]  Вот. Хорошо.
[49:27.000 --> 49:29.000]  Тогда мы, соответственно, когда
[49:29.000 --> 49:31.000]  решим вот это выражение,
[49:31.000 --> 49:33.000]  когда мы решим вот эту
[49:33.000 --> 49:35.000]  линейную минимизацию,
[49:35.000 --> 49:37.000]  мы получим некоторый
[49:37.000 --> 49:39.000]  вектор s со звездой.
[49:39.000 --> 49:41.000]  И из этого вектора
[49:41.000 --> 49:43.000]  s со звездой мы можем вычесть
[49:43.000 --> 49:45.000]  x каты. Вот.
[49:45.000 --> 49:47.000]  Получается, что вот это,
[49:47.000 --> 49:49.000]  вот это в некотором
[49:49.000 --> 49:51.000]  смысле какое-то направление,
[49:51.000 --> 49:53.000]  которое нам указывается. Ну, давайте вот
[49:53.000 --> 49:55.000]  здесь. Вот у нас есть точка x.
[49:55.000 --> 49:57.000]  Нам, соответственно, говорят,
[49:57.000 --> 49:59.000]  вычислили как-то вот эту
[49:59.000 --> 50:01.000]  линейную минимизацию, получилась
[50:01.000 --> 50:03.000]  получилась
[50:03.000 --> 50:05.000]  какая-то точка. Причем, например,
[50:05.000 --> 50:07.000]  для каких-то, например, симплекса
[50:07.000 --> 50:09.000]  L1 шара мы знаем, что эти
[50:09.000 --> 50:11.000]  точки расположены на границе, причем
[50:11.000 --> 50:13.000]  еще и на углах этих множеств. Там берется
[50:13.000 --> 50:15.000]  просто один из базистных векторов.
[50:15.000 --> 50:17.000]  Вот. И по факту нам говорят,
[50:17.000 --> 50:19.000]  что если мы сделаем шаг по вот этому
[50:19.000 --> 50:21.000]  направлению, которое мы здесь вычислили,
[50:21.000 --> 50:23.000]  например, единичный шаг какой-то,
[50:23.000 --> 50:25.000]  то мы просто перейдем вот этот угол.
[50:25.000 --> 50:27.000]  Просто перейдем вот в этот угол.
[50:29.000 --> 50:31.000]  Вот здесь хорошая картинка как раз
[50:31.000 --> 50:33.000]  говорит о том, что мы что делаем. Мы
[50:33.000 --> 50:35.000]  говорим, что пусть наша задача...
[50:35.000 --> 50:37.000]  Вот. А давайте я еще
[50:37.000 --> 50:39.000]  добавлю дополнительно. Помните, когда мы
[50:39.000 --> 50:41.000]  рассматривали градиентный спуск, мы же
[50:41.000 --> 50:43.000]  рассматривали линейную аппроксимацию,
[50:43.000 --> 50:45.000]  вот такого вида.
[50:47.000 --> 50:49.000]  Вот. И говорили, что давайте мы
[50:49.000 --> 50:51.000]  предположим, что наша функция линейна.
[50:51.000 --> 50:53.000]  Вот. И будем, ну, локально
[50:53.000 --> 50:55.000]  линейна. И соответственно, будем
[50:55.000 --> 50:57.000]  небольшими шажками сдвигаться по градиенту,
[50:57.000 --> 50:59.000]  предполагая, что функция линейна.
[50:59.000 --> 51:01.000]  Франк Вульф в некотором смысле
[51:01.000 --> 51:03.000]  это альтернатива этому
[51:03.000 --> 51:05.000]  всему. Вы как бы вот
[51:05.000 --> 51:07.000]  решая вот эту задачу оптимизации
[51:07.000 --> 51:09.000]  линейную, решаете вот эту
[51:09.000 --> 51:11.000]  же задачу, которая как бы возникает в линейной
[51:11.000 --> 51:13.000]  аппроксимации. Да?
[51:13.000 --> 51:15.000]  Только вы решаете ее агрессивно.
[51:15.000 --> 51:17.000]  Вы говорите, я найду
[51:17.000 --> 51:19.000]  вот такое s на моем ограниченном
[51:19.000 --> 51:21.000]  множестве, предполагая, что
[51:21.000 --> 51:23.000]  моя функция линейна.
[51:23.000 --> 51:25.000]  Ну вот, исходя из того, что вам сказал
[51:25.000 --> 51:27.000]  градиент, вы говорите, что вот тогда
[51:27.000 --> 51:29.000]  моя функция линейна. Вот.
[51:29.000 --> 51:31.000]  И вы минимизируете
[51:31.000 --> 51:33.000]  вашу линейную функцию
[51:33.000 --> 51:35.000]  на множестве
[51:35.000 --> 51:37.000]  ну, вашей оптимизации x.
[51:37.000 --> 51:39.000]  Понятная идея, да? То есть
[51:39.000 --> 51:41.000]  в градиентном спуске маленькие шажки,
[51:41.000 --> 51:43.000]  маленькие шажки
[51:43.000 --> 51:45.000]  по направлению.
[51:45.000 --> 51:47.000]  Здесь, когда вы
[51:47.000 --> 51:49.000]  ищете направление s, вы тоже, как градиент
[51:49.000 --> 51:51.000]  на спуске, смотрите на линейную аппроксимацию.
[51:51.000 --> 51:53.000]  И просто у этой линейной
[51:53.000 --> 51:55.000]  аппроксимации функции
[51:55.000 --> 51:57.000]  ищете минимум
[51:57.000 --> 51:59.000]  исходя из того,
[51:59.000 --> 52:01.000]  ну, исходя из вашего множества x.
[52:01.000 --> 52:03.000]  В силу того, что множество ограничено, минимум, понятно, существует.
[52:03.000 --> 52:05.000]  Вот.
[52:05.000 --> 52:07.000]  Окей? Идея понятна?
[52:07.000 --> 52:09.000]  Вот. Это, соответственно,
[52:09.000 --> 52:11.000]  идея первая. То есть, что вообще
[52:11.000 --> 52:13.000]  означает этот аргуменим? Вы просто
[52:13.000 --> 52:15.000]  минимизировали линейную аппроксимацию.
[52:15.000 --> 52:17.000]  Вот. Но, франк-мульф, понятно,
[52:17.000 --> 52:19.000]  что делает шаги тоже не равные
[52:19.000 --> 52:21.000]  единице, потому что, если вы просто
[52:21.000 --> 52:23.000]  будете делать шаги равные единице, как мы поняли,
[52:23.000 --> 52:25.000]  вы просто будете перемещаться между
[52:25.000 --> 52:27.000]  вот этими точками, которые вы
[52:27.000 --> 52:29.000]  при линейной минимизации
[52:29.000 --> 52:31.000]  находили. Ну, это будут какие-то
[52:31.000 --> 52:33.000]  точки на границе, какие-то уголки вашего
[52:33.000 --> 52:35.000]  множества. Они вам ничего особо не скажут.
[52:35.000 --> 52:37.000]  Вот. Поэтому, смотрите,
[52:37.000 --> 52:39.000]  вот в x
[52:39.000 --> 52:41.000]  запихивается
[52:41.000 --> 52:43.000]  вот этот s, то есть, уголок,
[52:43.000 --> 52:45.000]  который вы нашли, но
[52:45.000 --> 52:47.000]  с уменьшающимся весом.
[52:47.000 --> 52:49.000]  С уменьшающимся весом.
[52:49.000 --> 52:51.000]  И тогда у вас как бы получается что-то вот такое
[52:51.000 --> 52:53.000]  вот гладенькое. Ну, вот сейчас мы это как раз
[52:53.000 --> 52:55.000]  и поймем, что у нас получится. Видите,
[52:55.000 --> 52:57.000]  как раз у нас есть x0.
[52:57.000 --> 52:59.000]  Мы смотрим в сторону
[52:59.000 --> 53:01.000]  уголка. Вот этого как раз у нас
[53:01.000 --> 53:03.000]  вычислился s.
[53:03.000 --> 53:05.000]  Дальше мы делаем небольшое
[53:05.000 --> 53:07.000]  смещение. То есть,
[53:07.000 --> 53:09.000]  в самом жестком случае с шагом 1
[53:09.000 --> 53:11.000]  мы бы сместились тупо в s.
[53:11.000 --> 53:13.000]  Вот. Дальше, из новой точки
[53:13.000 --> 53:15.000]  мы снова делаем линейную минимизацию.
[53:15.000 --> 53:17.000]  Находим новый уголок,
[53:17.000 --> 53:19.000]  который как бы минимизирует
[53:19.000 --> 53:21.000]  нашу функцию, вот уже новую
[53:21.000 --> 53:23.000]  линейную опроксимацию. Вот.
[53:23.000 --> 53:25.000]  И, соответственно, идем
[53:25.000 --> 53:27.000]  в ту сторону. Но, опять же, шаг уменьшается.
[53:27.000 --> 53:29.000]  Поэтому у нас получается какая-то
[53:29.000 --> 53:31.000]  комбинация меньше.
[53:31.000 --> 53:33.000]  Ну и дальше, соответственно, в силу того,
[53:33.000 --> 53:35.000]  что у нас шаг становится меньше, меньше,
[53:35.000 --> 53:37.000]  меньше, меньше, меньше, вот.
[53:37.000 --> 53:39.000]  Точки, вы смотрите все
[53:39.000 --> 53:41.000]  новые и новые уголки, вы получаете
[53:41.000 --> 53:43.000]  некоторую комбинацию вот этих уголков.
[53:43.000 --> 53:45.000]  Вот. А если вы еще глянете
[53:45.000 --> 53:47.000]  на то, как выбирается шаг,
[53:47.000 --> 53:49.000]  что вы можете сказать вообще?
[53:49.000 --> 53:51.000]  На что это похоже?
[53:51.000 --> 53:53.000]  Вот если мы вот так вот на это смотрим выражение.
[53:53.000 --> 53:55.000]  На что это похоже?
[53:59.000 --> 54:01.000]  Выпукла комбинация, но она
[54:01.000 --> 54:03.000]  же где-то еще у вас, скорее всего, встречалась.
[54:05.000 --> 54:07.000]  Как, например, онлайн
[54:07.000 --> 54:09.000]  не считать среднее число?
[54:09.000 --> 54:11.000]  Если я знаю текущее среднее,
[54:11.000 --> 54:13.000]  и мне добавили новое число.
[54:13.000 --> 54:15.000]  Да, если я знаю, что
[54:15.000 --> 54:17.000]  среднее, например, XCAD
[54:17.000 --> 54:19.000]  посчитано по качествам,
[54:19.000 --> 54:21.000]  как и мне прислали новое XCAD
[54:21.000 --> 54:23.000]  плюс один, например.
[54:23.000 --> 54:25.000]  Типа того. То есть, смотрите,
[54:25.000 --> 54:27.000]  у вас есть XCAD, это в некотором смысле
[54:27.000 --> 54:29.000]  среднее, так сказать, среднее
[54:29.000 --> 54:31.000]  тех уголочков, которые вы имели
[54:31.000 --> 54:33.000]  до этого. Каждый раз вам
[54:33.000 --> 54:35.000]  как Вульф, вот эта линейная минимизация,
[54:35.000 --> 54:37.000]  показывает какой-то из уголочков. Говорит, что вот сейчас
[54:37.000 --> 54:39.000]  вот это минимум моей линейной
[54:39.000 --> 54:41.000]  апроксимации. Вы говорите, окей,
[54:41.000 --> 54:43.000]  хорошо, я тебе поверю, давайте я возьму этот уголочек,
[54:43.000 --> 54:45.000]  ну и буду брать средние этих уголочков.
[54:45.000 --> 54:47.000]  То есть, вам, условно, приходит
[54:47.000 --> 54:49.000]  новый уголок, и вы что
[54:49.000 --> 54:51.000]  запихиваете? Если вы
[54:51.000 --> 54:53.000]  прям делали честно среднее, вы должны что там
[54:53.000 --> 54:55.000]  умножить? На k разделить
[54:55.000 --> 54:57.000]  на k плюс один и, соответственно,
[54:57.000 --> 54:59.000]  добавить вот этот новый уголочек
[54:59.000 --> 55:01.000]  с весом k
[55:01.000 --> 55:03.000]  1 делить на k плюс 1.
[55:03.000 --> 55:05.000]  Ну вот здесь же что-то очень похоже и записано,
[55:05.000 --> 55:07.000]  просто чуть-чуть поменен этот
[55:07.000 --> 55:09.000]  коэффициент внизу.
[55:09.000 --> 55:11.000]  То есть, по факту, метод
[55:11.000 --> 55:13.000]  Франко-Вульфа – это агрессивная линейная
[55:13.000 --> 55:15.000]  минимизация, когда каждый раз вы
[55:15.000 --> 55:17.000]  минимизируете вашу локальную функцию. Она вам
[55:17.000 --> 55:19.000]  говорит, что минимум будет достигаться вот на границе
[55:19.000 --> 55:21.000]  вот в этой точке.
[55:21.000 --> 55:23.000]  Вы говорите, окей, да, я приму это к сведению,
[55:23.000 --> 55:25.000]  но буду как бы усреднять
[55:25.000 --> 55:27.000]  те показания, которые
[55:27.000 --> 55:29.000]  у меня получались. И понятно,
[55:29.000 --> 55:31.000]  что если минимум будет где-то близко
[55:31.000 --> 55:33.000]  какому-то, например, одному из
[55:33.000 --> 55:35.000]  уголков, вы на этот уголок будете просто
[55:35.000 --> 55:37.000]  смотреть чаще
[55:37.000 --> 55:39.000]  в линейной минимизации.
[55:39.000 --> 55:41.000]  И соответственно, когда вы это все будете усреднять,
[55:41.000 --> 55:43.000]  усреднять, усреднять, усреднять,
[55:43.000 --> 55:45.000]  этот уголок будет у вас выскакивать чаще
[55:45.000 --> 55:47.000]  и, соответственно, комбинация
[55:47.000 --> 55:49.000]  усредненной, он будет у вас как раз
[55:49.000 --> 55:51.000]  давать включевой вклад, и вы будете потихонечку
[55:51.000 --> 55:53.000]  сходиться к минимуму.
[55:53.000 --> 55:55.000]  Вот эта идея метода Франко-Вульфа.
[55:55.000 --> 55:57.000]  Понятно?
[55:57.000 --> 56:03.200]  Я надеюсь, что да. То есть, вот такой агрессивный взгляд, но все равно такое мягкое усреднение.
[56:06.960 --> 56:13.340]  Окей. Так, давайте тогда доказывать его сходимость. Что-то там я паузу не поставил, поэтому давайте листик создам.
[56:15.960 --> 56:17.960]  Так, листик создам.
[56:18.520 --> 56:24.360]  Докажем давайте сходимость в методе Франко Вульфа для выпуклых задач. Я скажу, почему не для сильных выпуклых концептов.
[56:25.320 --> 56:33.200]  До этого, вроде, всегда сильно выпуклыми задачами работали. Тут будет выпуклое. Давайте гладкость. Гладкость будем писать. Я запишу гладкость.
[56:35.200 --> 56:41.720]  В том виде тоже довольно стандартное определение, которое мы с вами встречали. Это мы его с вами до более того доказывали.
[56:42.360 --> 56:44.360]  Вот. Пишу гладкость.
[56:44.360 --> 56:46.360]  Гладкость.
[56:53.560 --> 56:55.560]  Согласны? Вот.
[56:56.400 --> 56:59.280]  Теперь давайте подставлять итерацию метода Франко Вульфа.
[57:00.120 --> 57:04.240]  Давайте на всякий случай глянем эту итерацию. Что там у нас с вами было?
[57:05.240 --> 57:10.760]  Уголочки, уголочки. Вот. X-каты это комбинация вот такая вот. Комбинация вот такая вот.
[57:10.760 --> 57:12.760]  Вот.
[57:12.960 --> 57:16.200]  Поэтому вот я ее в таком виде перепишу. Это x-k
[57:17.920 --> 57:19.920]  минус гамма-k
[57:21.200 --> 57:22.280]  x-k
[57:22.280 --> 57:24.280]  минус x-k. Пойдет?
[57:25.160 --> 57:30.720]  Чтобы у меня как раз была разность x-k плюс 1 и x-k, она более в более явном виде. Окей?
[57:31.600 --> 57:33.240]  Вот.
[57:33.240 --> 57:36.520]  Поэтому здесь я вот буду записывать вот так.
[57:36.520 --> 57:38.520]  Вот.
[57:38.760 --> 57:41.720]  x-k это минус гамма-k
[57:43.240 --> 57:46.920]  x-k минус x-k. Окей? Вот.
[57:47.840 --> 57:49.840]  Подставляю предыдущее выражение.
[57:51.320 --> 57:54.920]  Что соответственно получаю? Получаю следующее.
[57:55.800 --> 57:57.800]  Следующее.
[57:57.800 --> 57:59.800]  Минус гамма-k
[58:00.000 --> 58:01.000]  f
[58:01.000 --> 58:03.720]  x-k. Здесь соответственно что у меня
[58:03.800 --> 58:07.520]  будет x-k минус x-k.
[58:08.880 --> 58:10.880]  Плюс
[58:10.960 --> 58:12.280]  l
[58:12.280 --> 58:15.320]  гамма-k в квадрате пополам. И здесь будет x-k
[58:16.160 --> 58:18.600]  минус x-k в квадрате в квадрате.
[58:19.560 --> 58:20.760]  Вот.
[58:20.760 --> 58:27.400]  Вот с этим все довольно просто. В силу того, что мы работаем на ограниченном множестве, я веду диаметр этого множества.
[58:28.200 --> 58:31.120]  Диаметр этого множества нашего. Как просто
[58:32.080 --> 58:36.360]  максимум по всем точкам из этого множества, ну расстояние Евклидова.
[58:39.360 --> 58:41.360]  Диаметр в квадрате пусть будет вот так.
[58:41.440 --> 58:46.640]  Соответственно вот это выражение я могу оценить через диаметр. Это выражение могу оценить через диаметр.
[58:47.240 --> 58:49.240]  Будет что-то вот такое.
[58:49.720 --> 58:53.560]  Давайте диаметр пусть будет D у меня сразу. D в квадрате.
[58:54.240 --> 58:55.360]  Вот.
[58:55.360 --> 58:57.360]  Чтобы длина не писать.
[58:57.360 --> 59:04.640]  Интересно посмотреть на скалярное произведение. Интересно посмотреть на скалярное произведение, потому что у меня тут возникает что-то в духе минус f
[59:05.640 --> 59:07.640]  xk
[59:07.680 --> 59:09.680]  xk минус xk.
[59:09.680 --> 59:14.120]  Вот. Я чуть-чуть опять же выделю сейчас вот этот кусочек, который у меня завязан на xk.
[59:15.360 --> 59:19.040]  Вот так вот. И оставлю вот этот кусок.
[59:19.040 --> 59:23.600]  А вот про этот кусочек мы же что-то можем сказать? Что мы про него можем сказать?
[59:25.040 --> 59:27.040]  Ск же как-то получался?
[59:30.080 --> 59:34.080]  То, что слева написано. Но я по-другому чуть-чуть хочу. То есть понятно, что ск это
[59:35.280 --> 59:37.280]  минимум линейной минимизации, да?
[59:38.080 --> 59:43.840]  То есть для любого вектора какого-то x вот это меньше либо равно, чем вот
[59:43.920 --> 59:45.920]  вот что-то вот такое.
[59:46.920 --> 59:47.920]  x
[59:47.920 --> 59:53.720]  где x соответственно должен быть из множества x. Просто потому что ск это он искался как минимум этой задачи.
[59:53.720 --> 59:57.240]  Но я сюда что поставлю? Я сюда поставлю оптимум.
[59:57.960 --> 01:00:01.800]  Просто оптимум. Ну все. Давайте посмотрим теперь, что будет получаться.
[01:00:02.800 --> 01:00:07.040]  Вот. Получится соответственно у меня здесь больше либо равно, чем минус
[01:00:07.760 --> 01:00:09.760]  f xk.
[01:00:09.760 --> 01:00:15.760]  xk минус x звездой. Вот. Как я это могу оценить? Скалярное произведение?
[01:00:21.120 --> 01:00:28.240]  Ну нет, я хочу похитрее. Чего-то помощнее. Что еще есть? Что еще есть скалярное произведение, где возникало?
[01:00:29.080 --> 01:00:35.880]  Гладкость там была. Ну здесь соответственно что можно еще? Выпуклась, да? Выпуклась. Поэтому я по выпуклости его оцениваю вот так.
[01:00:41.040 --> 01:00:43.040]  Здесь просто выпуклась применяется.
[01:00:43.600 --> 01:00:48.480]  Окей. Оценил скалярное произведение. А сейчас увидим, почему мне нужна была эта выпуклость.
[01:00:49.280 --> 01:00:50.280]  Вот.
[01:00:50.280 --> 01:00:51.280]  Вот.
[01:00:51.700 --> 01:00:53.700]  Здесь просто выпуклась применяется.
[01:00:53.920 --> 01:01:01.440]  Окей. Оценил скалярное произведение. А сейчас мы увидим, почему мне нужна была эта выпуклость. Здесь когда я все это подставлю аккуратно наверх?
[01:01:02.140 --> 01:01:04.140]  Вот. У меня вот здесь это выражение болтается.
[01:01:14.340 --> 01:01:16.340]  Что такое?
[01:01:21.280 --> 01:01:30.080]  Ладно, по ходу придется переписывать.
[01:01:30.080 --> 01:01:31.080]  Ладно, я перепишу.
[01:01:31.080 --> 01:01:32.080]  Вот, у меня...
[01:01:32.080 --> 01:01:33.080]  Что такое-то?
[01:01:33.080 --> 01:01:34.080]  Не сел?
[01:01:34.080 --> 01:01:35.080]  Вот не сел.
[01:01:35.080 --> 01:01:36.080]  Ладно.
[01:01:36.080 --> 01:01:37.080]  Так, давайте на слайдах тогда посмотрим.
[01:01:37.080 --> 01:01:38.080]  Вот.
[01:01:38.080 --> 01:01:39.080]  Смотрите.
[01:01:39.080 --> 01:01:40.080]  А что я сделал?
[01:01:40.080 --> 01:01:43.080]  Переписал ровно то же самое, что и я.
[01:01:43.080 --> 01:01:47.040]  Диаметр написал, добавил, дальше по выпуклости это
[01:01:47.040 --> 01:01:48.040]  расписал.
[01:01:48.040 --> 01:01:49.040]  Вот.
[01:01:49.040 --> 01:01:51.360]  То, что мы как раз х со звездой добавили, расписал
[01:01:51.360 --> 01:01:53.720]  по выпуклости, и у меня получилась вот такая вот
[01:01:53.720 --> 01:01:54.720]  разность.
[01:01:54.720 --> 01:01:55.720]  Вот.
[01:01:55.720 --> 01:01:59.440]  Здесь я еще с права и слева вычел по минус х со звездой.
[01:01:59.440 --> 01:02:02.800]  Ну, валидная операция, могу и с права и слева вычесть
[01:02:02.800 --> 01:02:03.800]  просто по часу.
[01:02:03.800 --> 01:02:04.800]  Вот.
[01:02:04.800 --> 01:02:05.800]  Вот.
[01:02:05.800 --> 01:02:06.800]  Вот.
[01:02:06.800 --> 01:02:07.800]  Вот.
[01:02:07.800 --> 01:02:08.800]  Вот.
[01:02:08.800 --> 01:02:09.800]  Вот.
[01:02:09.800 --> 01:02:10.800]  Вот.
[01:02:10.800 --> 01:02:12.560]  Валидная операция, могу и с права и слева вычесть
[01:02:12.560 --> 01:02:13.560]  просто по числу какому-то.
[01:02:13.560 --> 01:02:14.560]  Хорошо.
[01:02:14.560 --> 01:02:15.560]  Ц заменил.
[01:02:15.560 --> 01:02:16.560]  Смотрите, что у меня получилось.
[01:02:16.560 --> 01:02:19.560]  У меня получилось то, что мы, в принципе, получали
[01:02:19.560 --> 01:02:21.560]  учет для градиентного спуска.
[01:02:21.560 --> 01:02:22.560]  Вот.
[01:02:22.560 --> 01:02:23.560]  Но чуть похуже.
[01:02:23.560 --> 01:02:27.400]  Чуть похуже, потому что, видите, у меня как бы есть
[01:02:27.400 --> 01:02:30.640]  разность функций в ка плюс первой точке, разность
[01:02:30.640 --> 01:02:33.520]  функций в ка этой точке, но умноженный коэффициент,
[01:02:33.520 --> 01:02:36.200]  как раз коэффициент вроде как хороший, будет линейная
[01:02:36.200 --> 01:02:37.200]  сходимость.
[01:02:37.200 --> 01:02:40.600]  Но вот то, что болтается дальше, оно не компенсируется.
[01:02:40.600 --> 01:02:43.680]  Потому что мы это просто убивали с помощью подбора
[01:02:43.680 --> 01:02:44.680]  шага.
[01:02:44.680 --> 01:02:46.600]  Оно прямо уходило в ноль оно у нас и становилось
[01:02:46.600 --> 01:02:47.600]  даже отрицательным.
[01:02:47.600 --> 01:02:48.600]  Вот.
[01:02:48.600 --> 01:02:49.960]  Здесь, ну, просто болтается.
[01:02:49.960 --> 01:02:50.960]  Вот.
[01:02:50.960 --> 01:02:53.680]  Поэтому, соответственно, и шаг будет, будет подбираться
[01:02:53.680 --> 01:02:54.680]  как-то хитро.
[01:02:54.680 --> 01:02:57.000]  Но вот главное, что мы получили вот такую рекурренту, что
[01:02:57.000 --> 01:03:02.040]  у нас ка плюс первая зависит от катова линейно, вот так
[01:03:02.040 --> 01:03:05.880]  вот, ну, в некотором смысле линейно, плюс вот этот кусочек,
[01:03:05.880 --> 01:03:07.640]  который у нас болтается, который пропорционален
[01:03:07.640 --> 01:03:08.640]  гамма ка в квадрате.
[01:03:08.640 --> 01:03:12.280]  Каверондаш не одумался, не одумался.
[01:03:12.280 --> 01:03:13.280]  Ладно.
[01:03:13.280 --> 01:03:14.280]  Хорошо.
[01:03:14.280 --> 01:03:15.280]  Смотрите.
[01:03:15.280 --> 01:03:18.280]  Все очень просто дальше будет.
[01:03:18.280 --> 01:03:19.280]  Дальше будет по индукции.
[01:03:19.280 --> 01:03:22.000]  Мы будем доказывать, что справедливо вот такая вот
[01:03:22.000 --> 01:03:23.000]  оценка.
[01:03:23.000 --> 01:03:24.000]  Вот.
[01:03:24.000 --> 01:03:28.000]  На нашу, для нашего метода, на кат и итерации мы можем
[01:03:28.000 --> 01:03:30.560]  гарантировать, что разность по функции будет вот такая
[01:03:30.560 --> 01:03:31.560]  вот.
[01:03:31.560 --> 01:03:32.560]  Вот, вот такая вот.
[01:03:33.240 --> 01:03:34.240]  Ладно.
[01:03:34.240 --> 01:03:35.240]  Так.
[01:03:35.240 --> 01:03:36.240]  Что делаем?
[01:03:36.240 --> 01:03:39.240]  База индукции, понятно, следует автоматически просто
[01:03:39.240 --> 01:03:44.240]  потому, что у нас f от х0 меньше, чем f от х0.
[01:03:44.240 --> 01:03:45.240]  Вот.
[01:03:45.240 --> 01:03:46.240]  Окей.
[01:03:46.240 --> 01:03:49.240]  Дальше, соответственно, что?
[01:03:49.240 --> 01:03:51.560]  Дальше, соответственно, что?
[01:03:51.560 --> 01:03:54.080]  Только четверку вроде надо было вынести за пределы.
[01:03:54.080 --> 01:03:55.080]  Ладно, это не страшно.
[01:03:55.080 --> 01:03:56.080]  Дальше делаем шаг.
[01:03:56.080 --> 01:04:01.080]  Посмотрим, насколько поменяется от х к плюс первого до х катого.
[01:04:01.080 --> 01:04:02.080]  Вот.
[01:04:02.080 --> 01:04:05.080]  Подставляю реально мой шаг в ранг вульфа.
[01:04:05.080 --> 01:04:06.080]  Сюда и сюда.
[01:04:06.080 --> 01:04:09.080]  Соответственно, гамма просто меняю на этот шаг.
[01:04:09.080 --> 01:04:11.080]  Подставляю предположение индукции.
[01:04:11.080 --> 01:04:16.080]  Предположение индукции вместо f от х ката минус f от х
[01:04:16.080 --> 01:04:17.080]  со звездой.
[01:04:17.080 --> 01:04:18.080]  И вот.
[01:04:18.080 --> 01:04:19.080]  Вот.
[01:04:19.080 --> 01:04:20.080]  Вот.
[01:04:20.080 --> 01:04:21.080]  Вот.
[01:04:21.080 --> 01:04:22.080]  Вот.
[01:04:22.080 --> 01:04:29.080]  А дальше сейчас будет просто в некотором смысле манипуляции.
[01:04:29.080 --> 01:04:31.080]  Хочу оценить правую часть.
[01:04:31.080 --> 01:04:36.080]  Показать, что для нее действительно будет выполнено выражение.
[01:04:36.080 --> 01:04:37.080]  Вот.
[01:04:37.080 --> 01:04:41.080]  Это списано с предыдущего слайда.
[01:04:41.080 --> 01:04:45.080]  Только вот, соответственно, вместо c справа подставлено
[01:04:45.080 --> 01:04:50.080]  выражение, которое максимум от 4c f от х0, а минус f от х
[01:04:50.080 --> 01:04:51.080]  со звездой.
[01:04:51.080 --> 01:04:55.080]  Понятно, что c, ну там 4c, потому что здесь двоечка,
[01:04:55.080 --> 01:04:57.080]  она в квадраты возведется, будет все хорошо.
[01:04:57.080 --> 01:04:58.080]  Будет 4c.
[01:04:58.080 --> 01:05:01.080]  Понятно, что 4c меньше либо равно, чем максимум из
[01:05:01.080 --> 01:05:02.080]  4c и разность функций.
[01:05:02.080 --> 01:05:03.080]  Окей.
[01:05:03.080 --> 01:05:05.080]  Соответственно, это здесь и подставлено.
[01:05:05.080 --> 01:05:07.080]  Получается вот такое вот.
[01:05:07.080 --> 01:05:08.080]  Дальше что?
[01:05:08.080 --> 01:05:13.080]  Играю со скобочкой и дохожу до ровно того, что в принципе
[01:05:13.080 --> 01:05:14.080]  и нужно.
[01:05:14.080 --> 01:05:16.080]  Ровно того, что в принципе и нужно, потому что как раз
[01:05:16.080 --> 01:05:20.080]  в знаменателе у меня выскочило k плюс 1 плюс 2.
[01:05:20.080 --> 01:05:21.080]  Вот.
[01:05:21.080 --> 01:05:24.080]  То есть на k плюс 1 итерация это будет справедливо.
[01:05:24.080 --> 01:05:25.080]  Окей?
[01:05:25.080 --> 01:05:26.080]  Вот.
[01:05:26.080 --> 01:05:28.080]  Очень простое доказательство.
[01:05:28.080 --> 01:05:30.080]  Ну, для метафранка Вульфа.
[01:05:30.080 --> 01:05:31.080]  Блин.
[01:05:31.080 --> 01:05:36.080]  Что с ним случилось-то?
[01:05:36.080 --> 01:05:37.080]  Ладно.
[01:05:37.080 --> 01:05:38.080]  Вот.
[01:05:38.080 --> 01:05:43.080]  Теоремка, соответственно, сформулирована.
[01:05:43.080 --> 01:05:44.080]  Получается какая?
[01:05:44.080 --> 01:05:47.080]  Собленейная сходимость 1 делить на k для выпуклой
[01:05:47.080 --> 01:05:48.080]  гладкой задачи.
[01:05:48.080 --> 01:05:49.080]  Вот.
[01:05:49.080 --> 01:05:52.080]  Результат ровно такой же, как для градиентного
[01:05:52.080 --> 01:05:53.080]  спуска.
[01:05:53.080 --> 01:05:54.080]  Да?
[01:05:54.080 --> 01:05:56.080]  То точно такие же результаты для градиентного спуска
[01:05:56.080 --> 01:05:57.080]  у нас были.
[01:05:57.080 --> 01:05:58.080]  Вот.
[01:05:58.080 --> 01:06:02.080]  Единственная проблема в том, что в субленейном случае
[01:06:02.080 --> 01:06:05.080]  в гладком выпуклом случае все окей, действительно
[01:06:05.080 --> 01:06:07.080]  совпадает с градиентным спуском.
[01:06:07.080 --> 01:06:10.080]  Проблема в том, что в сильно выпуклом случае то же самое
[01:06:10.080 --> 01:06:11.080]  получается.
[01:06:11.080 --> 01:06:14.080]  То есть в общем случае, когда у вас выпукло множество
[01:06:14.080 --> 01:06:17.080]  сильно выпуклая функция, вы не получите линейную
[01:06:17.080 --> 01:06:19.080]  скорость сходимости для метода Франко Вульфа.
[01:06:19.080 --> 01:06:21.080]  Ну вот такая специфика.
[01:06:21.080 --> 01:06:25.080]  Причем эта специфика связана именно с тем, что вы используете
[01:06:25.080 --> 01:06:26.080]  линейную минимизацию.
[01:06:26.080 --> 01:06:27.080]  Она тормозит метод.
[01:06:27.080 --> 01:06:31.080]  Она тормозит метод именно с точки зрения оракульных
[01:06:31.080 --> 01:06:32.080]  вызовов.
[01:06:32.080 --> 01:06:33.080]  Вот.
[01:06:33.080 --> 01:06:36.080]  То есть можно решить эту проблему, например, делая
[01:06:36.080 --> 01:06:42.080]  на один вызов градиента несколько оракульных вызовов.
[01:06:42.080 --> 01:06:44.080]  Несколько оракульных вызовов.
[01:06:44.080 --> 01:06:45.080]  Ой, ой, Господи.
[01:06:45.080 --> 01:06:48.080]  На один вызов градиента делать несколько вычислений
[01:06:48.080 --> 01:06:51.080]  линейной минимизации.
[01:06:51.080 --> 01:06:55.080]  И тогда действительно у вас получится, что количество
[01:06:55.080 --> 01:06:58.080]  вызовов градиента будет как у градиентного спуска,
[01:06:58.080 --> 01:06:59.080]  например.
[01:06:59.080 --> 01:07:02.080]  А количество линейных минимизаций будет 1 делить на k.
[01:07:02.080 --> 01:07:05.080]  Потому что именно линейные минимизации тормозят метод
[01:07:05.080 --> 01:07:06.080]  Франко Вульфа.
[01:07:06.080 --> 01:07:09.080]  И именно для них нижняя оценка, ну показывается, что для
[01:07:09.080 --> 01:07:12.080]  метода Франко Вульфа 1 делить на k, это лучше, что можно
[01:07:12.080 --> 01:07:15.080]  достичь именно из-за того, что нужно сделать 1 делить
[01:07:15.080 --> 01:07:16.080]  на k.
[01:07:16.080 --> 01:07:22.080]  Ну, то есть, точнее как, k пропорционально 1 делить
[01:07:22.080 --> 01:07:24.080]  на epsilon линейных минимизаций.
[01:07:24.080 --> 01:07:26.080]  То есть важная деталь, что именно во Франке Вульфе
[01:07:26.080 --> 01:07:29.080]  линейная минимизация делает его медленным.
[01:07:29.080 --> 01:07:32.080]  Ну, в некотором смысле именно с точки зрения оракульных
[01:07:32.080 --> 01:07:33.080]  сложностей.
[01:07:33.080 --> 01:07:35.080]  С арифметической точки зрения это может быть действительно
[01:07:35.080 --> 01:07:38.080]  когда-то лучше, потому что линейная минимизация
[01:07:38.080 --> 01:07:40.080]  довольно дешево стоит.
[01:07:40.080 --> 01:07:44.080]  Но если там линейная минимизация, и при этом приходится часто
[01:07:44.080 --> 01:07:47.080]  считать градиент, хотя можно было считать реже,
[01:07:47.080 --> 01:07:50.080]  это может быть послужить в некотором смысле отторжением,
[01:07:50.080 --> 01:07:52.080]  почему метод Франко Вульфа можно для некоторых задач
[01:07:52.080 --> 01:07:53.080]  не использовать.
[01:07:53.080 --> 01:07:56.080]  Либо пытаться модифицировать, ну и такие модификации
[01:07:56.080 --> 01:08:00.080]  на самом деле в литературе есть, где на 1 вызов градиента
[01:08:00.080 --> 01:08:04.080]  используется подсчет линейной минимизации несколько
[01:08:04.080 --> 01:08:05.080]  раз.
[01:08:05.080 --> 01:08:08.080]  Несколько раз, и там получаются уже правильные оценки.
[01:08:08.080 --> 01:08:09.080]  Вот.
[01:08:09.080 --> 01:08:10.080]  Хорошо.
[01:08:10.080 --> 01:08:11.080]  Все.
[01:08:11.080 --> 01:08:12.080]  На сегодня это все.
[01:08:12.080 --> 01:08:13.080]  Вот.
[01:08:13.080 --> 01:08:14.080]  Вопросы?
[01:08:20.080 --> 01:08:21.080]  Сейчас.
[01:08:21.080 --> 01:08:23.080]  Куда-то, да?
[01:08:23.080 --> 01:08:25.080]  Или предыдущие?
[01:08:25.080 --> 01:08:27.080]  А, С?
[01:08:27.080 --> 01:08:28.080]  Пожалуйста.
[01:08:28.080 --> 01:08:31.080]  Где оно у нас было?
[01:08:31.080 --> 01:08:33.080]  А где я его определил?
[01:08:33.080 --> 01:08:34.080]  Сейчас.
[01:08:34.080 --> 01:08:35.080]  Секундочку.
[01:08:35.080 --> 01:08:36.080]  Во.
[01:08:36.080 --> 01:08:37.080]  Во-во.
[01:08:37.080 --> 01:08:38.080]  А, там еще L.
[01:08:38.080 --> 01:08:39.080]  Там еще L запихано.
[01:08:39.080 --> 01:08:41.080]  Ну вот, короче, вот это выражение, чтобы там просто
[01:08:41.080 --> 01:08:42.080]  С осталось.
[01:08:49.080 --> 01:08:50.080]  Так.
[01:08:50.080 --> 01:08:52.080]  Ну если вопросов нет, тогда всем спасибо.
[01:08:52.080 --> 01:08:54.080]  В следующей неделе еще раз онлайн.

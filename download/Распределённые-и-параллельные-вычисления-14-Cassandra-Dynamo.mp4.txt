[00:00.000 --> 00:15.600]  Так, сегодня мы говорим про систему, которая называется Cassandra. Cassandra — это новое SQL-хранилище
[00:15.600 --> 00:25.400]  данных. В нем можно создавать таблицы. В эти таблицы можно вставлять записи. У нас есть строчки,
[00:25.400 --> 00:32.000]  у нас есть какие-то колонки, у нас есть типы данных. В общем, с виду эта система напоминает базу
[00:32.000 --> 00:40.280]  данных. Но тем не менее, называется она новое SQL, потому что под капотом это вовсе не система
[00:40.280 --> 00:44.480]  подобная, скажем, с Spanner, которую мы разбирали на другом курсе. Это система, которая представляет
[00:44.480 --> 00:49.880]  все еще, разумеется, кивали у хранилища, и поверх этого кивали у хранилища выражена табличная
[00:49.880 --> 00:56.000]  модель со всеми этими типами, схемами, колонками. Но при этом это хранилище, это кивали у хранилища,
[00:56.000 --> 01:02.760]  построено по совершенно иным принципам. А именно, это кивали у хранилища позиционирует себя в
[01:02.760 --> 01:08.920]  терминах капти аремы иначе. Вот не как Spanner, не как HBase. Кстати, у вас был HBase или еще нет?
[01:08.920 --> 01:21.720]  Алло. Ну, я обычно не хожу, но вроде не было HBase. Понятно, трудно сказать. Ну, в общем,
[01:21.720 --> 01:28.960]  хорошо было бы вот эту систему Кассандра и кивали у хранилища, которые в этой системе реализованы,
[01:28.960 --> 01:34.480]  противопоставить другому хранилищу HBase, ну или Spanner, я не знаю. Конечно, противопоставление
[01:34.480 --> 01:40.840]  довольно неудачное будет, правильно сравнивать с Spanner. Потому что вот Кассандра — это open
[01:40.840 --> 01:50.240]  source, реализация принципов дизайна, которые были описаны в 2007 году, кажется, в статье,
[01:50.240 --> 01:58.560]  которую упустил Amazon, назвался Amazon Dynamo. Высоко доступное кивали у хранилища. Но вот его написали
[01:58.560 --> 02:03.360]  в открытом доступе, коз в открытом доступе лежит, поверх этого кивали у хранилища сделали табличные
[02:03.360 --> 02:08.580]  модели. Вот получилась Кассандра. Кассандра, конечно, далеко ушла от изначальной задумки. Она
[02:08.580 --> 02:14.560]  умеет гораздо больше сейчас, но принципы дизайна те же. Так вот, в 2007 году, когда эта статья была
[02:14.560 --> 02:21.280]  написана, в Google уже примерно в то же время написали систему под названием Bigtable. И опять же,
[02:21.280 --> 02:27.760]  она может быть знакома вам в вашем курсе по системе, которая называется HBase. Это опять же
[02:27.760 --> 02:32.600]  open source, реализация конкурента Bigtable. Вот Bigtable и Dynamo — это две системы,
[02:32.600 --> 02:42.400]  которые были прародителями, соответственно, HBase и Kassandra. И две эти системы, Dynamo и Bigtable,
[02:42.400 --> 02:48.800]  выбрали разные буквы, как в теореме. То есть, в случае партишна, Dynamo готова оставаться
[02:48.800 --> 02:55.600]  высокодоступным хранилищем, быть готовым обслужить записи и чтения, а Bigtable выкручен в
[02:55.600 --> 02:59.960]  сторону согласованности. То есть, он не нарушит линейризуемость в этом случае. Разумеется,
[02:59.960 --> 03:05.960]  ведутся системы себя по-разному, дизайны у них очень разные. Ну и вот сегодня моя цель рассказать
[03:05.960 --> 03:12.120]  в первую очередь не как этой системой Kassandra пользоваться, какие там типы и какие таблицы. Это,
[03:12.120 --> 03:16.880]  в конце концов, можно по документации разобрать довольно легко. А про то, по каким принципам эта
[03:16.880 --> 03:24.000]  система построена и каких гарантий от них следует ждать, какого поведения, каковы их возможности,
[03:24.000 --> 03:33.000]  масштабирование, репликация, все вот это. И я буду сегодня рассказывать не про Kassandra,
[03:33.000 --> 03:38.960]  и не про Dynamo, и про то, и про другое, и периодически обозначать различия. То есть,
[03:38.960 --> 03:44.680]  что было сделано в Dynamo, что иначе сделали в Kassandra. Но моя цель рассказать про какой-то
[03:44.680 --> 03:49.720]  общий дизайн, про какие-то общие принципы, которые в основе этого подхода этих двух систем лежат.
[03:49.720 --> 03:56.520]  Но для того, чтобы объяснить, почему дизайн именно такой, нужно как-то его мотивировать. Так вот,
[03:56.520 --> 04:04.640]  давай перенесемся в прошлое, в 2000-е годы, в нулевые, и подумаем, что требовалось Amazon
[04:04.640 --> 04:10.920]  для хранения, для обработки данных. В то время Amazon был еще не облачным провайдером, может быть,
[04:10.920 --> 04:16.880]  уже и был, но, по крайней мере, все было еще не так, как сейчас. Он был не таким большим. Но он был
[04:16.880 --> 04:24.160]  большим интернет-магазином. И задачу, которую ему приходилось решать, это хранение данных,
[04:24.160 --> 04:32.600]  пользователей хранения, в первую очередь, обслуживания покупок в магазине. Если совсем
[04:32.600 --> 04:37.920]  говорить коротко, то Amazon требовалось сделать такую масштабируемую, доступную, отказоостойчивую
[04:37.920 --> 04:43.320]  корзину для товаров, куда пользователи могли добавляться, не знаю, там, в свои книжки, одежду,
[04:43.320 --> 04:49.760]  что-нибудь еще, нажимать на кнопку «купить», и Amazon бы зарабатывал на этом деньги. Вот нужно,
[04:49.760 --> 04:55.160]  чтобы понять дизайн системы Dynamo, нужно отталкиваться от такой задачи. Какие же
[04:55.160 --> 05:02.120]  требования были у Amazon в такой системе хранения? Во-первых, эта система должна легко масштабироваться,
[05:02.120 --> 05:12.320]  потому что нагрузка на магазин, она разная, она разная, не знаю, в течение недели, она разная,
[05:12.320 --> 05:16.880]  в зависимости от того, есть ли сейчас какой-то праздник или нет. Ну, иногда так происходит,
[05:16.880 --> 05:22.640]  что, знаю, там, «черная пятница» или «Рождество» и огромный наплыв пользователей, и нужно как-то
[05:22.640 --> 05:29.600]  увеличить количество машин в системе, чтобы они справлялись с нагрузкой, с рейтом запросов,
[05:29.600 --> 05:35.160]  с количеством покупок просто. Система должна масштабироваться горизонтально, но, естественно,
[05:35.160 --> 05:41.440]  у нее не должно быть какой-то точки, какого-то узкого места, которое бы препятствовало этому
[05:41.440 --> 05:47.120]  масштабированию. Мы хотим, причем не просто масштабироваться, мы хотим масштабироваться очень
[05:47.120 --> 05:54.160]  гибко, то есть мы хотим наращивать свои емкости и вычислительные ресурсы более-менее произвольно,
[05:54.160 --> 05:59.200]  то есть мы можем там в два раза увеличиться, а можем увеличиться на 10%, в зависимости от того,
[05:59.200 --> 06:07.560]  как мы планируем нагрузку на нашу систему. Это первое требование. Система должна быть масштабируемой.
[06:07.560 --> 06:13.880]  Ну, а если мы что-то знаем про дизайн определенных систем, то мы знаем, что строить масштабируемые
[06:13.880 --> 06:22.200]  системы довольно сложно. Причиной, чаще всего, узким местом в масштабировании чаще всего служат
[06:22.200 --> 06:28.920]  некоторые слой координации, некоторые слой, которые отвечают за метаинформацию, за распределение
[06:28.920 --> 06:44.520]  данных. Ну, вот вы видели в курсе системы Hadoop, и там есть name-node, которое... HDFS, все перепутал. HDFS
[06:44.520 --> 06:50.680]  тоже, но сначала про HDFS. У HDFS есть name-node, которое отвечает за хранение метаинформации о том,
[06:50.680 --> 06:57.920]  из каких чанков там файлы состоят, где они лежат. И сама по себе файловая система может масштабироваться
[06:57.920 --> 07:03.320]  на файлы огромного размера, произвольного размера. Мы просто добавляем машины, и диск растет,
[07:03.320 --> 07:08.480]  на котором хранятся все эти данные. Но с другой стороны, растет и количество чанков, из которых
[07:08.480 --> 07:15.400]  состоят файлы, и эта информация может переполнить мастер, name-node. Это некоторое ограничение
[07:15.400 --> 07:24.320]  масштабируемости. Или, скажем, если мы говорим про Bigtable или про его пансорс-реализацию HBase,
[07:24.320 --> 07:29.200]  то это киволюхранилище. Уже это ближе к нашей задаче. Это не файловая система,
[07:29.200 --> 07:34.640]  это киволюхранилище. Но там тоже есть слой координации. Есть некоторый мастер,
[07:34.640 --> 07:41.480]  который распределяет данные между узлами кластера и который знает, кто что обслуживает, где что лежит.
[07:41.480 --> 07:50.920]  Если мы говорим про кавку, то опять у нас был координатор, который должен был снова в себя
[07:50.920 --> 07:58.080]  все вмещать. Он действовал, конечно, аккуратнее. Он хранил данные во внешней системе в звуке
[07:58.080 --> 08:03.400]  перенадежно, отказоустойчиво и мог отказать и телепортироваться на другую физическую машину.
[08:03.400 --> 08:11.280]  Но тем не менее, есть такой координирующий узел. Одним из принципов, на которых строилась система
[08:11.280 --> 08:18.120]  Dynamo, состоял в том, что такого координирующего узла такого центра в системе быть не должно.
[08:18.120 --> 08:26.880]  Для того, чтобы построить масштабируемую систему, мы будем использовать децентрализованный дизайн.
[08:26.880 --> 08:33.080]  Не будет мастера, не будет какой-то точке, который управляет всеми остальными. Мы хотим построить
[08:33.080 --> 08:41.560]  симметричную систему, где все узлы будут одинаковыми и никакой узел не станет узким местом при
[08:41.560 --> 08:47.680]  масштабировании и никакой узел не станет единой точкой отказа, потому что мы хотим обслуживать
[08:47.680 --> 08:52.800]  пользователей. И наша задача приоритетная, чтобы, если человек нажимал на кнопку «добавить товар»,
[08:52.800 --> 08:57.040]  а потом нажимал на кнопку «купить», то его транзакция фиксировалась в хранилище.
[08:57.040 --> 09:10.040]  Итак, первый принцип – это масштабируемость, второй принцип – децентрализованный дизайн.
[09:10.040 --> 09:19.840]  Ну и третий принцип, он все это вместе следует. Мы в терминах CAPTIOREM выбираем букву «a»,
[09:19.840 --> 09:25.960]  то есть мы выбираем доступность. Мы, видимо, не собираемся использовать консенсус, потому что в
[09:25.960 --> 09:31.680]  консенсусе, в случае partition, в меньшей части partition, система наша блокируется на запись. Мы не
[09:31.680 --> 09:38.240]  хотим блокироваться на запись. Мы хотим обслуживать пользователей даже из двух половин, из двух осколков
[09:38.240 --> 09:45.800]  partition. И да, мы понимаем, что если мы отказываемся от консенсуса, то это может привести к некоторой
[09:45.800 --> 09:52.120]  несогласованности данных, которые мы пишем в системе. Но мы на это готовы пойти, потому что все же
[09:52.120 --> 09:58.640]  мы Amazon собираемся хранить в нашей системе не какие-то там счета пользователей, не деньги,
[09:58.640 --> 10:04.480]  а все же их корзину с товарами. Ну и мы готовы себе позволить определенные аномалии. Если мы,
[10:04.480 --> 10:10.480]  скажем, добавим товар в корзину, а потом он вдруг исчезнет, а потом снова появится. Ну если это
[10:10.480 --> 10:15.440]  будет случаться нечасто в каких-то вот таких случаях partition, то мы, в общем-то, готовы это
[10:15.440 --> 10:24.040]  пережить. Главное, чтобы пользователь мог покупать подарки на Рождество. Итак,
[10:24.040 --> 10:30.680]  вот такие принципы, и мы собираемся свой дизайн заточить вот под них.
[10:30.680 --> 10:42.960]  HBase и Bigtable соответственно исповедуют совсем другой подход. Там используется консенсус,
[10:42.960 --> 10:51.040]  там используется некоторый координирующий узел, там есть привычная репликация. Но вот
[10:51.040 --> 11:00.040]  Cassandra от этого всего отказывается. Ну и, в общем, сейчас я расскажу, как именно она устроена.
[11:00.040 --> 11:04.400]  Еще некоторое замечание важное, чтобы правильно спозиционировать свое отношение к всему этому.
[11:04.400 --> 11:15.840]  Cassandra в силу своего дизайна все же заставляет пользователя Cassandra и Dynamo знать про свое
[11:15.840 --> 11:20.520]  внутреннее устройство. Вот мы сейчас будем говорить про то, как Cassandra и Dynamo устроены,
[11:20.520 --> 11:25.680]  не только потому, что полезно знать, по каким принципам такие системы вообще конструируются,
[11:25.680 --> 11:33.400]  а еще и потому, что несмотря на свою табличную модель, несмотря на все эти таблицы, инсерты и
[11:33.400 --> 11:40.240]  типы данных и схемы, все равно пользователь на самом деле не может не знать про то,
[11:40.240 --> 11:45.760]  как Cassandra внутри устроена, как она распределяет данные, как она их реплицирует. Все равно,
[11:45.760 --> 11:50.080]  чтобы понимать, как можно системой корректно пользоваться, какого поведения от нее ожидать,
[11:50.080 --> 12:01.560]  требуется все же знать про внутренний дизайн. Ну что, давай перейдем на доску.
[12:01.560 --> 12:19.160]  И будем говорить сначала про дизайн системы Dynamo.
[12:19.160 --> 12:36.120]  Итак, мы хотим сделать кейвель-украинище с очень скудным, очень скромным API.
[12:36.120 --> 12:50.120]  Bool Eager. На самом деле, сигнатура чуть сложнее, чуть интереснее, но до этого мы еще дойдем.
[12:50.120 --> 12:58.400]  Никаких более сложных операций мы пока не хотим. Да и вообще, если говорить про историю этих
[12:58.400 --> 13:07.080]  систем, то Amazon Dynamo никаких колонок, типов, таблиц и не было. Это было вот именно кейвель-украинище.
[13:07.080 --> 13:13.640]  Вот для Амазона этого было достаточно. Ну конечно, было бы здорово тогда, вот в начале нулевых,
[13:13.640 --> 13:21.440]  иметь какую-то распределенную масштабируемую базу данных, где все были бы все привычные
[13:21.440 --> 13:30.080]  инструменты, таблицы, типы, транзакции, но в 2006 году в начале 2000-х не умели совмещать все вот эти
[13:30.080 --> 13:36.200]  фичи и требования, которые к системе предъявлялись, а именно горизонтальная масштабируемость,
[13:36.200 --> 13:42.720]  децентрализованность, высокая доступность. Вот эти цели и вот эти все фичи были не совместимы,
[13:42.720 --> 13:48.920]  поэтому инженеры Амазона думали, выкинули все, что им не требуется для их задач продуктовой,
[13:48.920 --> 13:57.320]  и вот остановились на кейвель-украинище. Итак, кейвель-украинища, операции Put и Get. Ну и можно
[13:57.320 --> 14:03.840]  начать о реализации, говорить в терминах, ну не знаю, мы делаем хэштаблицу распределенную.
[14:05.840 --> 14:09.760]  Мы можем в ней записать что-то и можем прочитать. Ну и у нас есть набор узлов,
[14:09.760 --> 14:11.920]  на которых эта хэштаблица будет жить.
[14:18.920 --> 14:28.800]  Ну и вопрос, как именно распределить между этими узлами ключи, которые мы собираемся хранить?
[14:28.800 --> 14:36.200]  Вот если думать про кейвель-украинище буквально как про хэштаблицу, то можно применить какой-то
[14:36.200 --> 14:41.960]  понятный дизайн. Вот у нас каждый узел, это же по сути bucket хэштаблицы, он может хранить
[14:41.960 --> 14:50.440]  какой-то набор ключей, но не все. Наша задача каким-то образом распределить все ключи между вот этими
[14:50.440 --> 14:57.080]  bucket. Ну и обычная хэштаблица, самая такая простая, незатейливая, делает это следующим образом.
[14:57.080 --> 15:05.240]  У нас есть число узлов, но здесь это 9. И мы просто ключ, мы каким-то образом эти узлы
[15:05.480 --> 15:15.040]  нумеруем и скажем, что за хранение ключа будет отвечать узел вот такой, хэш от ключа по модулю
[15:15.040 --> 15:27.240]  числа узлов. Можем ли мы так распределить данные между узлами? Хотим ли мы так делать?
[15:27.240 --> 15:40.840]  Ну можно некоторое узло будет слишком большая нагрузка. Почему? Ты про то, что есть какие-то
[15:40.840 --> 15:44.480]  горячие ключи, к которым обращается чаще, но это более-менее неизбежно.
[15:44.480 --> 15:53.000]  Да, давай пока задачу упростим и не будем говорить про отказоустойчивость вообще. Вот наша
[15:53.000 --> 15:57.320]  задача просто распределить данные, распределить ключи между набором узлов.
[15:57.320 --> 16:09.440]  А может из некоторых машин? А переконфигурации мы хотим выполнять?
[16:09.440 --> 16:15.960]  Я же сказал, конечно мы хотим. У нас, не знаю, скоро черная пятница. Число,
[16:16.160 --> 16:24.480]  если вырастет, мы, конечно, хотим добавить новые машины сюда. Вот, не знаю, плюс еще три вот эти.
[16:24.480 --> 16:33.960]  Конечно, мы хотим. Причем мы хотим, чтобы это было делать легко. И у нас машин здесь,
[16:33.960 --> 16:38.920]  может быть, сколько угодно. Там сотни, тысячи. И вот добавляем мы тоже какое-то произвольное
[16:38.920 --> 16:47.520]  количество. Или можем потом отняти. Но такой способ распределения данных, он, конечно же,
[16:47.520 --> 16:54.680]  не годится. Потому что он работает на уровне отдельных ключей независимо. И если вдруг мы
[16:54.680 --> 17:02.160]  добавим, возьмем 9 узлов и добавим 10 узел, то вот это вот отображение ключей в машины,
[17:02.160 --> 17:09.720]  оно прям все целиком поедет. Потому что у нас изменится. И мы должны будем как-то
[17:09.720 --> 17:13.720]  перераспределить все данные, подвигать более-менее данные на всем кластере.
[17:13.720 --> 17:20.480]  Разумеется, если мы добавляем всего лишь одну машину, то мы хотим, чтобы передвигалась какая-то
[17:20.480 --> 17:27.360]  пропорциональная часть данных. Если у нас вот 9 машин и добавляем 10, то мы бы потрогали одну
[17:27.360 --> 17:36.320]  девятую всех данных. Но уж точно не все. Так что такой подход, разумеется, не будет работать. Нам
[17:36.320 --> 17:44.880]  нужно какое-то более сложное, более хитрое отображение. И эта идея называется... Ну как бы
[17:44.880 --> 17:49.640]  мы делали, если бы у нас был какой-то волшебный узел, который бы координировал остальные?
[17:49.640 --> 18:04.840]  Этот узел, он бы просто знал про диапазон всех ключей. Он бы нарезал этот диапазон ключей
[18:04.840 --> 18:13.400]  на какие-то маленькие поддиапазоны, но и каждому узлу поручал свой. Когда у нас появлялся новый
[18:13.400 --> 18:21.400]  узел, то мы бы что-то делили. Центра координации у нас такого нет. У нас есть отдельные узлы,
[18:21.400 --> 18:33.200]  и мы должны придумать какую-то максимально простую процедуру распределения ключей между
[18:33.200 --> 18:46.320]  ними. Эта идея называется consistent hash. Идея из обычных hash таблиц нам не подходит,
[18:46.320 --> 18:50.840]  потому что при добавлении, при изменении состава пластера нам требуется выдвинуть слишком много
[18:50.840 --> 18:59.360]  данных. Но мы можем поступать чуть хитрее. Вот у нас есть ключи, есть некоторая хреш-функция.
[18:59.360 --> 19:16.760]  Вот мы нависуем такое кольцо. Здесь 0, здесь максимальное значение хреш-функции. И скажем,
[19:16.760 --> 19:19.240]  что каждый узел в нашем кластере
[19:25.640 --> 19:27.800]  выбирает себе некоторый токен
[19:32.760 --> 19:39.320]  вот из этого диапазона и приземляется в некоторую точку вот этого кольца.
[19:46.760 --> 19:56.520]  Но это кольцо, разумеется, виртуальное, оно существует только в нашем сознании.
[19:56.520 --> 20:03.240]  Но, тем не менее, каждый узел пластера, на котором развернута наша система динамик,
[20:03.240 --> 20:09.720]  занимает вот иметь такой токен, занимать какую-то позицию. Разумеется, токен нужно хранить надежно,
[20:09.720 --> 20:16.360]  то есть узел добавляется в систему, он генерирует себе случайный токен и вот занимает какое-то
[20:16.360 --> 20:26.280]  место. А дальше, когда пользователи приходят с операцией PUT или GET, то ключ этого пользователя
[20:26.280 --> 20:44.280]  расшируется и опять же попадает в какую-то точку на кольце. И мы скажем, что если идти по кольцу
[20:44.280 --> 20:51.480]  по часовой стрелке, то первый узел, который мы встречаем на этом пути, он и будет отвечать
[20:51.480 --> 21:11.080]  за обслуживание нашего PUT или нашего GET. Идея понятна? Понятно. Вот если какой-то узел вдруг
[21:11.080 --> 21:23.160]  добавляется в наш кластер, вот этот новый узел, то он занимает, на этом кажется, какое-то место
[21:23.160 --> 21:32.800]  и, скажем, дробит какой-то диапазон. Был раньше такой диапазон, в него угодил новый добавляемый
[21:32.800 --> 21:37.960]  узел и в итоге этот диапазон подробился на два. Но все остальные остались такими же.
[21:37.960 --> 21:54.360]  Окей, вопрос. Как схема распределения данных? Чем этот подход не слишком хорош? Какие у него
[21:54.360 --> 22:12.200]  есть минусы? Ну тогда клиент должен знать эти отображения. Кто за что отвечает?
[22:12.200 --> 22:23.800]  Какая нота в какой участок кольца? Клиенту, конечно, знать про это не нужно. Нужно знать
[22:23.800 --> 22:30.280]  самим узла. Что делает клиент? Он приходит с своим PUT или GET на произвольную машину. Эта
[22:30.280 --> 22:37.760]  произвольная машина, мы ее назовем координатором операции, кэширует ключ, ну а дальше внутри
[22:37.760 --> 22:42.520]  системы разбирается, что делать дальше. Вот как именно она это понимает, это хороший вопрос,
[22:42.520 --> 22:49.040]  но в следующей очереди. Сам клиент, конечно, ничего знать не должен, он просто должен знать
[22:49.040 --> 22:53.440]  про какие-то машины, которые входят в состав кластера. Он ходит, не знаю, через какую-то DNS
[22:53.440 --> 22:59.080]  запись попадает в какую-то случайную машину и та его дальше обслуживает. Нужно, чтобы узлы самой
[22:59.080 --> 23:05.040]  системы понимали, что происходит, как устроено это кольцо. Да, от клиентов не требуется. Я говорю
[23:05.040 --> 23:10.800]  даже не про проблемы какие-то, как это написать пока, а про то, какие есть здесь изъяны в смысле
[23:10.800 --> 23:16.560]  распределения данных. Вот будет ли это распределение равномерно?
[23:21.560 --> 23:30.880]  Ну может хэширом кажется, что будет. Ну представь, у нас здесь кайта 64, у нас 2.64 степени различных
[23:30.880 --> 23:36.280]  значений этой хэш-функции и у нас, не знаю, 100 машин.
[23:36.280 --> 23:45.640]  Ну прямо скажем, невеликая вероятность, что вот эти 100 точек на этом гигантском кольце хэшей
[23:45.640 --> 23:52.160]  поделят это кольцо на какие-то с размерной диапазоны. Правда ведь?
[23:52.160 --> 24:03.720]  Не стоит этого ждать. У нас хэшей слишком много, а точек на этом кольце, ну узлов не так много.
[24:03.720 --> 24:08.920]  Ну то есть, если у нас там, не знаю, десятки тысяч, это одна история, ну если у нас их просто десятки
[24:08.920 --> 24:16.040]  или сотни, если мы маленький стартап, то у нас проблемы с распределением ключей.
[24:16.040 --> 24:23.800]  Вот эта проблема, она на самом деле довольно легко решается, решается понятием виртуального узла.
[24:23.800 --> 24:32.840]  Ну то есть, у нас физический узел один, но на вот этом кольце хэшей ему могут соответствовать
[24:32.840 --> 24:33.840]  несколько точек.
[24:46.240 --> 24:50.240]  Картинка, картинка усложняется, но принцип должен быть понятен.
[24:50.240 --> 24:56.320]  Вот у нас есть один физический узел и вот у него есть такие виртуальные,
[25:05.960 --> 25:07.680]  виртуальный экземпляр на этом кольце.
[25:07.680 --> 25:19.640]  То есть, на одном узле сразу у нас несколько набор токенов и чем больше виртуальных узлов мы
[25:19.640 --> 25:27.280]  сделаем, тем, ну у нас увеличивается объем метаинформации, но при этом у нас выравнивается
[25:27.280 --> 25:33.520]  распределение ключей. Вот эти виртуальные узлы, это механизм, который позволяет нам
[25:33.520 --> 25:38.720]  решить еще одну проблему, а именно неоднородность самих узлов.
[25:38.720 --> 25:45.120]  Вот может быть какие-то машины просто более емкие, чем другие, там просто стоят более емкие
[25:45.120 --> 25:50.640]  жесткие диски, дисков больше. Или просто машины, не знаю, там больше, гядер, которые могут
[25:50.640 --> 26:00.840]  обрабатывать данные. С помощью виртуальных узлов мы можем распределять нагрузку между вот этими
[26:00.840 --> 26:05.640]  узлами неравномерно. Если у нас есть разные конфигурации, просто железные, то более емки
[26:05.640 --> 26:11.000]  конфигурация, более емким конфигурациям будет отвечать больше виртуальных узлов на кольце.
[26:11.000 --> 26:22.440]  С каким образом можно более аккуратно распределять данные. Но у нас все равно есть некоторые
[26:22.440 --> 26:26.280]  даты, все этот подход, понятно, он рандомизированный, мы токены выбираем случайно,
[26:26.280 --> 26:34.480]  поэтому какая-то несбалансированность все же возможна. Вот так мы готовы распределять данные.
[26:34.480 --> 26:43.480]  Но как-то справедливо заметил, не совсем понятно, что дальше делать клиенту, потому что эта картинка,
[26:43.480 --> 26:49.480]  она, но только у нас в уме существует, а у каждого узла просто есть набор его токен. Вот какой-то
[26:49.480 --> 26:55.960]  узел получает ут или гет от клиента, и что ему дальше делать? Ему нужно знать вот все эту карту
[26:55.960 --> 27:05.680]  пластера, ему нужно знать про весь кластер, токены, всех других узлов. При этом состав этого пластера
[27:05.680 --> 27:16.960]  он еще и меняется к тому же. Как мы будем решить эту проблему? Кажется это несложно,
[27:16.960 --> 27:24.120]  мы просто будем обмениваться данными со всеми. Что значит обмениваться данными со всеми? Тут
[27:24.120 --> 27:32.800]  нужно все-таки аккуратнее сказать, как именно мы собираемся делать. Поддерживать отображение,
[27:32.800 --> 27:42.080]  когда мы себе токен берем, то отправляем всем. У каждого узла на данный момент есть. Как же им
[27:42.080 --> 27:48.360]  подсинхронизировать всю эту картину мира? Просто мы администратор, мы добавляем в кластер три
[27:48.360 --> 28:06.840]  новых узла. Вообще говоря, они про всех остальных мало что знают, и остальные пока тоже про них
[28:06.840 --> 28:16.760]  ничего не знают, и продолжают обслуживать запросы, но вот как обычно. Мы же не можем просто пойти на
[28:16.760 --> 28:25.680]  месте администратора и поговорить с каждым узлом. Но мы не можем, потому что не все не могут быть
[28:25.680 --> 28:37.040]  доступны прямо сейчас. Мы должны какой-то более устойчивый протокол придумать. Есть. Гвозди протокола
[28:37.040 --> 28:47.760]  называется в общем случае. Как можно распространять некоторое знание, ну или не знаю, по аналогии
[28:47.760 --> 28:56.720]  распространить инфекцию в сети, где ты не знаешь всех участников. Идея такая, что каждый узел
[28:56.720 --> 29:04.680]  знает про какие-то другие узлы. Картина мира каждого узла может быть неполной. Он может не знать про все
[29:04.680 --> 29:10.560]  узлы. Ну а скажем, новички, которые добавляются в кластер, они знают только про наборы некоторых,
[29:10.560 --> 29:22.280]  некоторые выделены под набор узлов, которые называются силами. Про остальных мы ничего не
[29:22.280 --> 29:36.400]  знаем. И для того, чтобы сойтись к одному и тому же представлению о составе кластера, о этой карте,
[29:36.400 --> 29:46.360]  в этом кольце, в фоне действует следующая процедура. Мы из всех наших соседей, из всех узлов,
[29:46.360 --> 29:54.080]  которых мы знаем, выбираем случайный и с ним обмениваем все информации о том, про какие узлы
[29:54.080 --> 30:00.600]  знаем мы и про то, про какие узлы знает он. Но чуть аккуратнее мы обмениваемся даже не узлами.
[30:16.360 --> 30:28.360]  Технические трудности. Вот узлы обмениваются с эктрами дельтами о том, про какие версии,
[30:28.360 --> 30:34.080]  каких узлов они знают. И если вдруг в кластере появляется узел, то он общается с некоторыми
[30:34.080 --> 30:39.960]  сидноудами. Сидноуды узнают про этот новый узел, а дальше, обмениваясь с другими узлами,
[30:39.960 --> 30:47.200]  рассказывают постепенно про вот этих новичков. Ну и если все сделать аккуратно, то примерно за
[30:47.200 --> 30:56.240]  логарифм таких терраций все про всех узнают. То есть у каждого узла будет представление, какие машины
[30:56.240 --> 31:02.960]  с какими токенами в кластере есть, и получая запрос в кутере к клиенту, любая машина может
[31:02.960 --> 31:12.920]  перенаправить клиента в нужное место. Это фоновая процедура, она всегда работает на узге,
[31:12.920 --> 31:21.960]  ну и вот обновляет свое знание о системе. Заметим, что если бы мы строили систему более
[31:21.960 --> 31:26.360]  централизованную, если бы мы использовали дизайн, скажем, похожий на как, если бы мы
[31:26.360 --> 31:31.960]  использовали зукипер, то мы бы делали совершенно иначе. Вот зукипер позволял такие проблемы решить
[31:31.960 --> 31:41.000]  гораздо проще. С укипером можно было бы завести директорию, в которой бы каждый узел системы
[31:41.000 --> 31:49.600]  создавал бы эфемерный узел, эфемерный z-note, в котором были бы написаны его токены. Вот это
[31:49.600 --> 31:56.200]  такой централизованный механизм, через который узлы бы узнавали друг от друга. Просто центральная
[31:56.200 --> 32:03.040]  директория, где отказы устойчивые, туда можно пойти и узнать про скуще состояние кластера. Здесь
[32:03.040 --> 32:10.960]  система децентрализованная, она не хочет зависеть от доступности зукипера, поэтому вот из-за этого
[32:10.960 --> 32:20.800]  выбора у нас вместо зукипера здесь появляется госип. И госип в динамо играет двойную функцию. Во-первых,
[32:20.800 --> 32:27.680]  он обеспечивает обмен информации о составе кластера, о новых узлах и о токенах. А кроме того,
[32:27.680 --> 32:35.360]  в этот же госип встроен механизм детекции избоев. Но мы сейчас перейдем к репликации, нам нужно
[32:35.360 --> 32:42.440]  будет все-таки данные хранить в нескольких копиях. Но вот госип он обеспечивает, он реализует еще и
[32:42.440 --> 32:57.560]  фейнер детектор. Чтобы к нему перейти, нужно поговорить, собственно, про репликации, про то,
[32:57.560 --> 33:06.600]  как быть с отказавшими узлами. Мы сейчас научились что делать, мы научились распределять данные по
[33:06.600 --> 33:14.800]  кластеру, и мы научились распространять эти данные, эту информацию, таблицу роутинга между узлами
[33:14.800 --> 33:20.400]  с помощью госипа. То есть клиент может прийти и узнать про то, кто же ответит ему на запрос. Но вот
[33:20.400 --> 33:26.640]  пока это конкретный узел, и он может отказать, поэтому, конечно же, мы такой конструкции в
[33:26.640 --> 33:32.920]  чистом виде пользоваться не можем. Мы должны хранить копии данных на разных углах. Ну вот давай
[33:32.920 --> 33:46.000]  встроим репликацию в эту схему. Как выбрать набор реплик для ключа? Такие узлы будут являться репликами.
[33:46.000 --> 34:06.000]  Попользуемся тем, что у нас есть кольцо. Скажем, что вот у нас есть ключ, вот его фреш, он попадает
[34:06.000 --> 34:18.120]  сюда точку кольца, и мы возьмём, и скажем, что вот эти три узла, следующие три узла по часовой
[34:18.120 --> 34:27.440]  стрелке будут служить репликами. И, видимо, записать можно аккуратно. Будем считать,
[34:27.440 --> 34:34.280]  что они все хранят копию этого самого ключа. Опять, нам нужно сделать некоторые поправки на
[34:34.280 --> 34:41.320]  реальность. Вот прямо так буквально, наивно делать всё же не стоит. Нам нужно какие-то нюансы учесть.
[34:41.320 --> 34:46.920]  Видно ли, что это за нюансы?
[34:55.520 --> 35:03.480]  Вот на этой картинке... Может быть, это мог бы быть виртуальная узла одного этого?
[35:04.480 --> 35:12.760]  На этой картинке нам повезло, и вот эти три виртуальных узла на кольце, это же всё-таки не машины,
[35:12.760 --> 35:18.840]  но вот этим трём виртуальным узлам соответствуют разные физические узлы, и всё нормально. Но у
[35:18.840 --> 35:25.480]  нас же в системе есть виртуальные узлы, и поэтому следующие три могут быть одними и теми же машинами,
[35:25.480 --> 35:30.280]  поэтому у нас коэффициент репликации фактически снизится. Разумеется, мы должны учитывать
[35:30.280 --> 35:36.040]  виртуальность узлов, и мы должны учитывать всё-таки некоторую топологию нашего кластера,
[35:36.040 --> 35:45.040]  именно про ковернс. То есть мы, конечно же, не хотим, чтобы три наши реплики для ключа находились,
[35:45.040 --> 35:54.200]  скажем, в одной стойке. Принципальная схема такая, но нужно её аккуратно потюдить, чтобы мы не
[35:54.200 --> 36:06.160]  понизили себе случайный коэффициент репликации. Окей, ну вот у нас есть три реплики, и мы, в принципе,
[36:06.160 --> 36:22.680]  готовы, как обычно будем, как мы раньше поступали в таких случаях. У нас есть вот эти три реплики,
[36:22.680 --> 36:27.680]  у нас есть операция пользователя, у нас есть какой-то узел, который стал координатором этой
[36:27.680 --> 36:33.600]  записи или учтения. Ну и как обычно, мы хотим иметь свою доступность, поэтому мы не требуем,
[36:33.600 --> 36:40.000]  чтобы все реплики были доступны на запись, и не все были доступны на чтение. Мы используем обычно
[36:40.000 --> 36:51.680]  для таких случаях клоуна. Если мы пишем, то мы пишем реплики. Мы вот знаем, что это за узлы,
[36:51.680 --> 36:58.440]  мы нашли на нашем кольце решение, и мы, когда пишем, то пишем на какой-то клоун.
[36:58.440 --> 37:05.560]  Когда мы читаем, мы читаем на какой-то клоун.
[37:17.560 --> 37:19.760]  Ну и что нас в такой конструкции беспокоит?
[37:19.760 --> 37:27.440]  Алло, кто-нибудь?
[37:27.440 --> 37:38.680]  Нас беспокоит здесь, если мы хоть что-то знаем про репликацию, нас беспокоит то,
[37:38.680 --> 37:47.600]  что эта конструкция, конечно же, не линейризуема. Ну да, она дает какие-то наивные гарантии. Если у
[37:47.600 --> 37:58.880]  нас запись завершилась до чтения, то, разумеется, на следующее чтение, запись завершилась до начала
[37:58.880 --> 38:02.840]  чтения, то последующее чтение, конечно же, запись увидит, потому что есть члене форму,
[38:02.840 --> 38:09.960]  есть одна реплика, которая эту запись знала, которая входила в форму записи. Но если запись и
[38:09.960 --> 38:18.360]  чтение во времени пересекаются, то, возможно, какие-то странные ситуации, когда мы увидим запись, то не
[38:18.360 --> 38:29.080]  видим. Вот у нас есть три реплики, и запись успела написать что-то пока на одну, то есть запись
[38:29.080 --> 38:35.320]  пока уже началась, но еще не завершилась, а потом мы сделали первое чтение вот с такого набора
[38:35.360 --> 38:45.000]  реплик, второе чтение вот с такого набора. Мы получили такой мигающий ключ, в котором то видно значение, а то не видно.
[38:45.000 --> 38:56.520]  Но мы же уже говорили, что хотели пожертвовать консистенцией. Это правда, я к тому говорю,
[38:56.520 --> 39:04.560]  что Кассандра, если ты читаешь документацию по Кассандре, то там написано, что их подход называется
[39:04.560 --> 39:20.920]  tunable consistency. И tunable тут в том, что ты можешь выбирать значение w и значение e. Ну и пока у тебя
[39:20.920 --> 39:43.880]  их сумма больше, чем нодь не n, конечно, а repetition factor. Тут якобы согласованности меньше. Но тут
[39:43.880 --> 39:50.640]  в документации нужно относиться к этому всему скептически. Вот слово consistency, а на самом
[39:50.640 --> 39:57.720]  деле это не имеет какой-то фиксированной семантики. Это такой зонтик, с которым много конкретных моделей согласованности.
[39:57.720 --> 40:04.000]  Так вот Кассандра, она здесь никакой разумной модели согласованности нам не обещает ни линеризуемости,
[40:04.000 --> 40:11.920]  ни sequential consistency. Ничего, просто такой наивный подход с quorum. Если quorum пересеклись, если
[40:11.920 --> 40:19.120]  операции были уже в порядочном времени, то чтение увидит запись. Если они были конкурентными, то никакой
[40:19.120 --> 40:28.480]  разумной семантики Кассандра пользователю не дает. Что происходит в случае partition?
[40:28.480 --> 40:40.360]  Вот допустим у нас partition вырезал какие-то реплики.
[40:40.360 --> 40:54.120]  Какие-то физические узлы, а вместе с ними какие-то виртуальные, которые входили в наш quorum.
[40:54.120 --> 41:11.120]  В этом случае Кассандра говорит, что она запишет ваши данные, динамо нужно аккуратнее сказать,
[41:11.120 --> 41:17.400]  динамо запишет все-таки ваши данные на V узлов, потому что ваши данные должны быть
[41:17.400 --> 41:24.480]  записаны надежно. Но при этом мы не можем записать их вот на нужный нам quorum,
[41:24.480 --> 41:31.280]  здесь quorum не соберется из двух узлов. Поэтому динамо действует ну так, оптимистично очень.
[41:31.280 --> 41:44.280]  Мы координатор операции пишем не только на узлы, которые должны входить в quorum, а еще и на какие-то
[41:44.280 --> 41:50.280]  другие узлы, если здесь quorum собрать невозможно. Просто какие-то узлы сейчас недоступны по непонятным
[41:50.280 --> 41:58.680]  причинам. В этом случае мы координатор пишем на какие-то дополнительные узлы вот сюда,
[41:58.680 --> 42:08.520]  но говорим этим узлам, что запись, которую они получили, нужно eventually дописать вот сюда.
[42:08.520 --> 42:14.680]  Вот такая незатейливая евристика, она называется hit and off.
[42:14.680 --> 42:30.440]  То есть у нас quorums есть и мы пишем действительно на V узлов всегда,
[42:30.440 --> 42:36.440]  но при этом нет гарантии на самом деле, что quorums у нас пересекутся.
[42:36.440 --> 42:41.680]  Так что мы в одной части partition можно работать с одними узлами, в другой части partition
[42:41.680 --> 42:47.200]  можно работать с другими узлами. Записи у нас всегда фиксируются в системе,
[42:47.200 --> 42:55.520]  они фиксируются надежно на как минимум V узлах. Мы сами значение V выбираем и говорим,
[42:55.520 --> 43:00.400]  что наша запись должна приземлиться на 2 узла в случае того, чтобы отказывать,
[43:00.480 --> 43:11.320]  но при этом пересечение quorums нас уже не спасет, даже если мы сначала записали и после этого только
[43:11.320 --> 43:19.840]  начинаем читать. Как вообще понимать, что какие-то узлы недоступны и нужно писать на другие?
[43:19.840 --> 43:29.480]  Для этого нам нужен детектор сбоев. Детектор сбоев снова использует гостепротокол. Обычно
[43:29.480 --> 43:35.720]  детекторы сбоев реализуются через hard-бита и все очень удобно, когда у нас есть некоторый
[43:35.720 --> 43:42.120]  координатор, который эти hard-биты получает. Есть какой-то фиксированный узел, это может быть
[43:42.120 --> 43:49.520]  узел-координатор, узел name-нода или это может быть зоокипер, которому мы делегировали эту задачу,
[43:49.520 --> 43:55.240]  но пусть даже зоокипер, раз уж мы про него знаем. Так вот, каждый узел создается пефимерную ноду
[43:55.240 --> 44:02.680]  в директории и пока у него открыта сессия, пока он жив, в рамках этой сессии клиент-зоокипер
[44:02.680 --> 44:07.800]  отправляет hard-биты и этот ефемерный узел продолжает жить. Когда клиент умирает, сессия
[44:07.800 --> 44:13.720]  протухает. Когда узел-система умирает, сессия протухает, в директоре исчезает
[44:13.720 --> 44:22.120]  ефемерный z-note и другие узлы об этом узнают просто через подписку-зоокипер. Очень удобный
[44:22.120 --> 44:28.120]  механизм. Вся коммуникация устроена по принципу звездочки, есть один центр и с ним все общаются.
[44:28.120 --> 44:35.720]  Здесь все узлы должны знать про все, потому что координатором записи может быть пока что кто
[44:35.720 --> 44:42.560]  угодно, это не совсем точно, потому что в касандре кто угодно может быть координатором операции,
[44:42.560 --> 44:50.240]  в динаме все-таки нет. Пока можно считать, что кто угодно, поэтому кто угодно должен знать про всех
[44:50.240 --> 44:56.600]  остальных, какие узлы сейчас живы или нет. Но как это сделать? Мы же не будем всем вслать hard-бит,
[44:56.600 --> 45:03.680]  потому что всего у нас в сети десятки тысяч узлов. Вот квадратичную коммуникацию мы себе
[45:03.680 --> 45:11.680]  позволить не можем, поэтому это довольно забавная история. В касандре детектор сбоев встроен в ГОСе
[45:11.680 --> 45:18.760]  протокол и hard-бит мы можем получить напрямую иногда, а можем получить косвенно, когда нам про
[45:18.760 --> 45:24.840]  hard-бит расскажет другой узел. Ну и каждый hard-бит за какое-то количество итераций
[45:24.840 --> 45:32.480]  почти распространяется. Конечно же, по-другому построены тайм-ауты, потому что распределение
[45:32.480 --> 45:39.760]  hard-битов по времени уже другое на существовании этого ГОСЕПа. Но принцип такой же. У нас
[45:39.760 --> 45:47.080]  коммуникация не централизованная, коммуникация при этом не квадратичная, коммуникация промежуточная
[45:47.080 --> 45:54.240]  через ГОСЕП. Ну это такой общий принцип для любой не централизованной системы.
[45:54.240 --> 46:07.160]  Окей. С чем у нас еще здесь сложности есть? Ну, помимо вот этих кворумов и отсутствия согласованности
[46:07.160 --> 46:14.720]  разумных. Ну, если мы работаем с кворумами, то есть пишем не на все реплики, то, видимо,
[46:14.720 --> 46:23.040]  разные узлы могут хранить разные версии значений. И нам нужно каким-то образом эти версии, видимо,
[46:23.040 --> 46:31.560]  упорядочивать. Узлы должны понимать, какие версии более старые, какие более новые. Вот как мы
[46:31.560 --> 46:43.800]  собираемся эту задачу решать. Хранить версии неравное значение. Ну да, само-самое. Откуда мы версии возьмем?
[46:43.800 --> 46:58.160]  Так, нам же клиент присылается в какой-то момент. Ну, клиент, пусть он сам. Ну, клиент же он не один.
[46:58.160 --> 47:04.360]  Представь, ты делаешь покупки, и у тебя там есть какой-то аккаунт, и ты работаешь с ним, скажем,
[47:04.360 --> 47:10.480]  с ноутбука, с телефона, и у тебя одна и та же корзина. И корзина одна, а физических клиентов
[47:10.480 --> 47:17.040]  несколько на разных устройствах. Тут конкарнация, разумеется, может быть. Ну, в конце концов,
[47:17.040 --> 47:23.480]  мы говорим не только про корзины, мы говорим про, вообще, про киевы и ухранилища сейчас. И, разумеется,
[47:23.480 --> 47:27.000]  могут быть клиенты, которые конкурентно работают с одними и теми же ключами.
[47:27.000 --> 47:35.680]  Нет у нас такого клиента, который бы все упорядочивал. У нас разные клиенты могут
[47:35.680 --> 47:42.600]  порождать разные версии для одного и того же ключа. Ну, в общем, возникают задачи версионирования,
[47:42.600 --> 47:49.520]  и вот система Cassandra и Dynamo отличаются тем, как они подходят к этому самому версионированию.
[47:49.520 --> 47:58.480]  Они подходят совершенно по-разному. Cassandra поступает очень просто. Cassandra получает,
[47:58.480 --> 48:05.240]  вот когда приходит клиент, приходится своя операция PUT. Эта операция PUT приземляется,
[48:05.240 --> 48:13.280]  то hash занимает какое-то место на кольце, ну а сама операция PUT, она же попадает просто в
[48:13.280 --> 48:28.440]  какой-то узел, чтобы аккуратно нарисовать, она попадает в какой-то физический узел.
[48:28.440 --> 48:36.920]  Вот, этот узел является координатором, и просто этот координатор в качестве версии
[48:36.920 --> 48:43.400]  записи выбирает свое локальное время. Вот, такая стратегия называется last right wins.
[48:43.400 --> 49:01.840]  Это такой наивный способ, наивный тривиальный способ разрешения конфликтов. Вот, мы клиент отправили
[49:01.840 --> 49:09.800]  операцию GET, какой-то узел ее получил, у него есть карта кластера, он знает, какие узлы отвечают
[49:09.800 --> 49:18.040]  за хранение значения для ключа. Мы к ним идем, собираем какой-то квором на чтение, получаем
[49:18.040 --> 49:25.720]  разные версии и выбираем просто версию, которая по часам старше. Нас не беспокоит то, что эти
[49:25.720 --> 49:32.000]  версии могли быть назначены разными координаторами, у них могли быть не синхронизированные часы,
[49:32.000 --> 49:38.680]  и две записи, которые были, скажем, упорядочены во времени и упорядочно причинностью, они получили
[49:38.680 --> 49:46.440]  противоположные временные метки, и в итоге более ранняя запись перетерла более позднее.
[49:46.440 --> 49:51.400]  У нас Кассандро это устраивает.
[49:51.400 --> 50:07.560]  Некоторая техническая заметка, когда координатор чтения читает с кворома, он может видеть разные
[50:07.560 --> 50:14.520]  версии, и он выбирает старшую версию. Но вот тут можно привести забавную параллель с протоколом
[50:14.520 --> 50:22.200]  ABD, который у нас был на параллельном курсе. Мы тоже читали с кворома по началу и выбирали
[50:22.200 --> 50:28.800]  старшую версию, но для того, чтобы гарантировать, что любое чтение, которое начнется после нашего,
[50:28.800 --> 50:36.880]  гарантированно увидело версию не младше, не старше вернее, мы делали синхронную фазу записи.
[50:36.880 --> 50:42.600]  Вот чтобы не было такого сценария. Чтение сначала увидело, а запись потом не увидело.
[50:42.600 --> 50:48.200]  Вот мы использовали такую дополнительную фазу записи, такой хелпинг в терминах лог-фри,
[50:48.200 --> 50:54.680]  когда мы прочли старшую версию и намазали на квором в случае, если почему-то запись не завершилась.
[50:54.680 --> 51:02.360]  Кассандро тоже это делает, и это называется read-repair.
[51:02.360 --> 51:16.440]  Но при этом делает это ассинхронно. То есть она вам возвращает прочитанные
[51:16.440 --> 51:23.640]  значения со старшим временной метками и в фоне посылает на реплики, которые разошлись,
[51:23.640 --> 51:29.120]  новое значение, чтобы эти реплики перезаписали свою уже устаревшую версию.
[51:32.360 --> 51:46.400]  То есть для доступности у нас есть вот этот hint handoff, для конкуренции у нас есть временные метки
[51:46.400 --> 51:52.320]  и конфликты мы разрешаем с помощью очень простой стратегии lost right weight.
[51:52.320 --> 52:11.120]  Dynamo делала сложнее. В чем проблема такого подхода? В том, что мы на месте координатора просто
[52:11.120 --> 52:16.360]  назначаем в качестве временной метки записи локальное время. В том, что координаторы могут
[52:16.360 --> 52:26.680]  быть разными, часы могут быть не синхронизированы, ну и соответственно причинность, которая была на
[52:26.680 --> 52:32.360]  уровне клиентов, которая реализовалась за пределами системы, она вот этими узлами
[52:32.360 --> 52:38.680]  игнорируется, координаторами игнорируется. Было бы здорово все-таки причинность учитывать.
[52:38.680 --> 52:47.640]  Но а для этого нужно каким-то более сложным образом представлять версии. Вот сейчас у нас версия
[52:47.640 --> 52:53.480]  это просто физическое время, и это физическое время стирает информацию о причинности. Вместо
[52:53.480 --> 52:58.440]  физического времени можно использовать логическое время, в котором эту причинность можно закодировать.
[52:58.440 --> 53:08.040]  А именно мы знаем, что есть понятие happens before, это просто формализация причинности.
[53:08.280 --> 53:19.640]  И есть способ happens before компактно описать в виде векторных временных мет, в виде векторных часов.
[53:19.640 --> 53:32.760]  То есть если у нас есть в системе там n узлов, и на каждом происходит и событие, и в чтении записи,
[53:33.240 --> 53:45.400]  то временная метка события это более-менее вектор, где итая компонента, если у нас есть
[53:47.080 --> 53:54.760]  итый узел, и он обслуживает какой-то пункт, то итая компонента векторных часов – это
[53:55.400 --> 54:03.480]  порядковый номер текущей записи на этом узле, а другие компоненты векторных часов хранят
[54:03.480 --> 54:10.520]  количество событий, количество записей, про количество записей на других узлах,
[54:11.960 --> 54:22.120]  о которых знает данный узел, выполняющий текущую запись. И мы бы могли хранить вместе с каждым
[54:22.120 --> 54:29.480]  значением не скалярную временную метку, не физическое время и не какой-то логический
[54:29.480 --> 54:38.680]  таймстэмп, а векторную временную метку, и тогда бы при чтении, получая с хворума разные версии,
[54:38.680 --> 54:47.240]  мы могли бы их сравнить. В векторных часах есть такая естественная процедура сравнения,
[54:48.040 --> 54:52.600]  они захватывают happens before, happens before – частичный порядок, поэтому мы можем сказать, что, скажем,
[54:52.600 --> 54:59.880]  одно событие предшествовало другому. Если у нас есть один клиент, он сначала записал, получил
[54:59.880 --> 55:05.560]  подтверждение от системы, потом сказал об этом другому клиенту, другой клиент сделал свою
[55:05.560 --> 55:14.040]  запись. Но вот эти две записи могли бы быть упорядочены порядочной причиной, и система могла бы
[55:14.040 --> 55:21.360]  эту причинность захватить, и тогда, в случае сравнения временных меток с разных леплик, могла
[55:21.360 --> 55:27.320]  бы учесть, что новая версия, она просто наследует старую версию, поэтому можно старую версию
[55:27.320 --> 55:35.920]  перезаписать. Или наоборот, если мы вдруг видим, что векторные часы конкурируют, то это означает,
[55:35.920 --> 55:41.360]  что они несравнимы, то есть есть какая-то итая компонента, которая больше в одних часах и житая,
[55:41.360 --> 55:48.280]  которая меньше, то это означает, что события несравнимы, это означает, что события со стороны
[55:48.280 --> 55:54.000]  пользователей не были упорядочены, и никакого разумного способа разрешить конфликт здесь нет.
[55:54.000 --> 56:05.800]  Для того, чтобы эту конструкцию интегрировать в Dynamo, нужна помощь пользователя.
[56:05.800 --> 56:14.160]  Вот, давай посмотрим на статью, там есть пример.
[56:22.160 --> 56:30.440]  Вот D1, D2, D3, D4, D5 – это какие-то версии данных, которые хранятся по ключу.
[56:30.440 --> 56:43.200]  Вот сначала какой-то клиент пишет по ключу, и он порождает первую версию данных,
[56:43.200 --> 56:53.520]  и векторные часы в прошлом году, в прошлом учебном году, весной, мы говорили про векторные часы в
[56:53.520 --> 57:01.440]  контексте потоков, но вот здесь у нас в векторных часах компоненты – это некоторые акторы,
[57:01.440 --> 57:12.240]  которые упорядочивают все свои записи, которые выполняют записи, и вот акторами могут служить
[57:12.240 --> 57:18.160]  разные узлы. Можно считать, что акторы – это клиенты, а можно считать, что акторы – это узлы,
[57:18.160 --> 57:25.960]  координаторы внутри системы. Но вот в этой картинке считается, что координатором является узел,
[57:25.960 --> 57:35.080]  который… узел системы. В смысле, актором, который отвечает за версионирование, за упорядочивание,
[57:35.080 --> 57:42.600]  является узел системы координатора записи. Вот когда какой-то узел Sx обслуживает запись по ключу,
[57:42.600 --> 57:50.720]  то он, собственно, записывает ее на реплике и прикрепляет к этим записям временную метку такую
[57:50.720 --> 57:57.080]  Sx1. То есть, это векторные часы или, как это называется в контексте подобных систем,
[57:57.080 --> 58:06.320]  вектор версий, в котором есть одна компонента – Sx и версия 1. То есть, через Sx была выполнена
[58:06.320 --> 58:13.360]  пока одна запись. Потом была выполнена вторая запись, она тоже прошла через Sx и получила
[58:13.360 --> 58:22.480]  временную метку Sx2. То есть, был узел, который наблюдал порядок двух записей, и вот если вдруг
[58:22.480 --> 58:28.360]  при чтении с квором мы получим вот такую временную метку и такую временную метку, то мы сможем их
[58:28.360 --> 58:40.160]  сравнить. Вот это просто обжаривает это. Мы знаем, что вторая запись, она точно логически следует
[58:40.160 --> 58:45.920]  за первой. Они могут быть логически упорядочно клиентом, могут быть неупорядочно, но на них
[58:45.920 --> 58:55.000]  порядок зафиксировался. А дальше вдруг возникает partition. И в разных частях partition'a происходят
[58:55.000 --> 59:04.040]  две конкурирующие записи. В одной части, эту запись обслуживает узел Sy, в другой части,
[59:04.040 --> 59:12.240]  эту запись обслуживает Sx. И вот у нас появились две новые версии значения, и у них уже две разные,
[59:12.240 --> 59:23.120]  две несравнимые временные метки, два несравнимых вектора версий. А дальше, скажем, partition лечится,
[59:23.120 --> 59:34.520]  и какой-то клиент приходит и читает актуальное значение. И с квором он получает версию вот такие
[59:34.520 --> 59:42.200]  данные, вот с такой версией, и такие данные вот с такой версией. Если бы мы были касандрой,
[59:42.200 --> 59:47.560]  то мы бы в качестве версии получили две временные метки, просто сравнили бы какая из них старшая,
[59:47.560 --> 59:55.160]  какая из них более свежая и выбрали соответствующее значение. Здесь мы наблюдаем, что у нас есть два
[59:55.160 --> 01:00:02.640]  вектора версий, и они несравнимы между собой. Здесь разные акторы занимались обслуживанием
[01:00:02.640 --> 01:00:09.000]  разных записей, и непонятно, как они могли упрядочить, как они упрядочены между собой две эти записи.
[01:00:09.000 --> 01:00:19.800]  В этом случае Dynama действует следующим образом. Она отдаёт решение этой проблемы пользователю,
[01:00:19.800 --> 01:00:28.720]  диригирует пользователю. То есть сама система говорит, что да, я была доступна, я обслуживала
[01:00:28.720 --> 01:00:38.080]  все записи в обеих частях partition, но у меня получились несогласованные версии, и я не
[01:00:38.080 --> 01:00:44.560]  понимаю, как их упорядочить. Если бы у нас был консенсус, у нас был бы сквозной порядок,
[01:00:44.560 --> 01:00:49.680]  но он требовал доступности большинства. Здесь мы от этого отказались, работаем в меньшей части
[01:00:49.680 --> 01:00:56.600]  partition, но при этом у нас получаются несравнимые версии, расходящиеся ветки истории, и пусть теперь
[01:00:56.600 --> 01:01:04.240]  сам клиент занимается тем, что эти ветки как-то мерзят между собой. Он пишет какое-то произвольное
[01:01:04.240 --> 01:01:08.400]  приложение, и вот пусть на уровне логики своего приложения он понимает как данные,
[01:01:08.400 --> 01:01:19.280]  как согласовать две конфликтующие версии. Вопрос тут, правда, в том ещё есть, но помимо этой
[01:01:19.280 --> 01:01:27.440]  проблемы, что само по себе неприятно, есть ещё такой вопрос, а именно кто именно является вот этим
[01:01:27.440 --> 01:01:36.320]  Sx, Sy, Z? Ну то есть, кто увеличивает соответствующие компоненты? Тут есть два варианта, либо это делает
[01:01:36.320 --> 01:01:43.520]  узел системы, либо это делает сам клиент. Но вот если это делает сам клиент, то, ну понятно, то есть
[01:01:43.520 --> 01:01:48.960]  каждый клиент, мы будем ставить его однопоточным, последовательным, и он назначает монотонные метки.
[01:01:48.960 --> 01:02:02.600]  Но скажем, если вы действуете неаккуратно и запускаете каждую запись, там не знаю,
[01:02:02.600 --> 01:02:09.200]  в новом потоке, и в новом потоке создаете новый клиент системы, и получаете такой новый
[01:02:09.200 --> 01:02:15.640]  идентификатор клиентов, фактически, то у вас эти векторы версии будут раздуваться, они будут
[01:02:15.640 --> 01:02:27.040]  расти. Если же вы перенесете нумерацию версий на узлы системы, то тут можно поступить аккуратнее,
[01:02:27.040 --> 01:02:36.280]  а именно сделать так, чтобы все записи одного ключа по возможности, но не то чтобы это можно
[01:02:36.280 --> 01:02:41.240]  было гарантировать, но по крайней мере можно к этому стремиться, чтобы записи одного ключа
[01:02:41.240 --> 01:02:51.640]  обслуживались одной машиной. Вот если мы посмотрим на доску, то вот есть у нас какой-то узел,
[01:02:51.640 --> 01:03:01.200]  который обслуживает запись, и есть, этот узел знает про три реплики ключа, который клиент хочет
[01:03:01.200 --> 01:03:11.520]  записать. Вот, этот узел перенаправляет запись на первую реплику из вот этой группы реплик,
[01:03:11.520 --> 01:03:18.760]  вот первую в смысле расположения на кольцах решений, так чтобы все записи проходили через
[01:03:18.760 --> 01:03:25.360]  одну реплику, и эта реплика их упорядочила. Если вдруг у нас случился partition, то упорядочить
[01:03:25.360 --> 01:03:31.000]  будут разные реплики независимо, у нас появляются разные компоненты, тогда в этом векторе
[01:03:31.000 --> 01:03:46.960]  версий, ну и тогда ветки расходятся. Идея понятна? Понятно. Тут, правда, есть еще один нюанс,
[01:03:46.960 --> 01:03:53.880]  а именно вот такой дизайн, он влияет на описи системы. Вот раньше мы говорили, что у нас есть
[01:03:53.880 --> 01:04:00.680]  просто операция put и операция get, но на самом деле чуть сложнее. Вот я сейчас найду здесь
[01:04:00.680 --> 01:04:18.480]  пример. Когда мы делаем put и get, мы, когда мы делаем get по ключу, мы получаем не просто значение,
[01:04:18.480 --> 01:04:24.760]  которое мы прочли, а мы получаем пару из значения, которое мы прочли, и контекста. Контекст
[01:04:24.760 --> 01:04:30.480]  какой-то непрозрачный, мы его интерпретировать не умеем. И когда мы пишем, по ключу значения,
[01:04:30.960 --> 01:04:38.640]  то мы в API put тоже добавляем этот контекст. То есть как выглядит наша операция обычно? Мы что-то
[01:04:38.640 --> 01:04:46.320]  читаем, смотрим, апдейтим, пишем новую версию. И для того, чтобы между этими версиями сохранялась
[01:04:46.320 --> 01:04:51.560]  преемственность, на смысл, чтобы система понимала, что мы как бы наследовались от какой-то
[01:04:51.560 --> 01:04:59.120]  предшествующей версии, мы эту версию прокидываем клиенту через самый непрозрачный контекст.
[01:04:59.120 --> 01:05:06.840]  Он клиентом не интерпретируется. Он интерпретируется системой, которая получает запись. Вот,
[01:05:06.840 --> 01:05:12.320]  получив put от клиента с таким контекстом, система понимает, что этот контекст – это
[01:05:12.320 --> 01:05:22.360]  вектор версий, и в нем нужно инкрементировать одну из компонентов. Вот ровно так мы передаем
[01:05:22.360 --> 01:05:33.480]  векторные часы. Ну вот, если вспомнить алгоритм векторных часов, можно смотреть википедию, и там
[01:05:33.480 --> 01:05:43.120]  будет картинка. Когда мы отправляем сообщение другому узлу, когда реализуется некоторая причинность,
[01:05:43.120 --> 01:05:50.120]  вот тогда мы прикладываем сообщение в свои векторные часы, и при получении их мержим. Вот узел
[01:05:50.120 --> 01:05:56.400]  системы, узлы системы и клиенты, они вот взаимодействуют через коммуникацию друг с другом,
[01:05:56.400 --> 01:06:03.680]  и через эту коммуникацию они заодно пробрасывают и вот эти векторы версий. Таким образом,
[01:06:03.680 --> 01:06:11.040]  мы пользователя заставляем вот явным образом аннотировать причинность, которая там за пределами
[01:06:11.040 --> 01:06:16.240]  системы реализуется. Если бы система была линейризуема, если бы она использовала консенсус,
[01:06:16.240 --> 01:06:24.360]  то это было бы не нужно, потому что наш консенсус бы давал линейризацию, то есть бы он учитывал
[01:06:24.360 --> 01:06:29.720]  порядок в реальном времени, значит причинность. Вот Кассандра от этого отказывается, поэтому
[01:06:29.720 --> 01:06:37.960]  заставляет пользователя прокидывать контекст своими собственными руками. Ну и конфликт разрешает,
[01:06:37.960 --> 01:06:43.640]  и разрешает конфликты. То есть, с одной стороны, мы получаем высокую доступность, а с другой стороны,
[01:06:43.960 --> 01:06:49.280]  мы получаем более слабую модель согласованности, но вообще непонятно какое просто пересечение
[01:06:49.280 --> 01:06:56.080]  кворумов. А кроме того, у нас немного меняется API, и мы отребуем от пользователей, чтобы он
[01:06:56.080 --> 01:07:06.640]  причинность явно маркировал. Но, кстати, не всё так плохо, точнее, не то чтобы это было плохо,
[01:07:06.640 --> 01:07:15.240]  это скорее дизайн, это намеренно-сознательный выбор. Но в кавке у вас есть альтернативный способ,
[01:07:15.240 --> 01:07:23.120]  если всё-таки вы хотите какой-то упорядоченности, если вы хотите какой-то согласованности, если вы
[01:07:23.120 --> 01:07:28.720]  хотите понимать всё-таки, что происходит, то у вас есть инструмент, который называется Lightweight
[01:07:28.720 --> 01:07:34.720]  Transactions. В динамо его не было, он появился в Кассандре, появился не так давно, и это такой
[01:07:34.720 --> 01:07:42.720]  аналог операции Compare and Set и Compare and Swap. Вы можете сделать insert в таблицу,
[01:07:42.720 --> 01:07:55.320]  если данного ключа ещё не было. Такой атомарный exchange. Или вы можете обновить по ключу какое-то
[01:07:55.320 --> 01:08:09.840]  поле, если предшествующее значение равно чему-то. Это вот уже буквально касс. И здесь всё-таки вот
[01:08:09.840 --> 01:08:16.120]  в этом месте локально для поддержки. Это называется транзакциями, но, конечно, это не в полном смысле
[01:08:16.120 --> 01:08:22.320]  транзакция. Вот эта транзакция, она имеет такой фиксированный вид, и она умеет работать только
[01:08:22.320 --> 01:08:31.320]  с отдельным ключом. То есть такая точечная операция, это операция CASS. И выполнять кросс-шардовые
[01:08:31.320 --> 01:08:37.160]  транзакции, то есть работать с ключами, которые находятся на разных машинах, Кассандр не умеет.
[01:08:37.160 --> 01:08:42.360]  Но всё-таки в пределах одного ключа мы можем достичь вот такой вот линеризуемости, используя
[01:08:42.360 --> 01:08:50.880]  операцию CASS, которая под капотом реализована через некоторую вариацию протокола Paxos.
[01:08:50.880 --> 01:08:58.480]  Вообще говоря, Paxos — это протокол, который реализует то, что называется Write-Once Register,
[01:08:58.480 --> 01:09:04.240]  то есть ничейку памяти, в которой можно один раз записать надёжно. Ну, то есть выигрывать тот,
[01:09:04.240 --> 01:09:10.720]  кто первым это сделал. Но я как-то вот в параллельном курсе на одном из теменах рассказывал,
[01:09:10.720 --> 01:09:16.800]  что на самом деле, если посмотреть на историю пропозалов в Paxos, если представить их в виде
[01:09:16.800 --> 01:09:22.960]  графа, где пропозал цепляется за другое, если он заадоптил значение после фазы prepare,
[01:09:22.960 --> 01:09:29.760]  то там получается такая забавная конструкция, ориентированная на граф без циклов,
[01:09:29.760 --> 01:09:35.640]  где все принятые пропозалы лежат на одной цепи. Ну, то есть, короче говоря, из single to creep
[01:09:35.640 --> 01:09:46.520]  axis можно сделать некоторую историю апдейтов, то есть перезаписываемый регистр. Правда,
[01:09:46.520 --> 01:10:02.800]  и нам важно, что у нас именно операция CAS, потому что, ну вот, когда мы делаем операцию,
[01:10:02.800 --> 01:10:07.560]  мы по сути, ну, если там некоторые детали отбросить, мы по сути выполняем как бы одну
[01:10:07.560 --> 01:10:11.720]  итерацию Paxos. Она может быть успешной, может быть неуспешной, но она может быть неуспешной,
[01:10:11.720 --> 01:10:15.280]  потому что там, не знаю, какие-то тайм-ауты, а она может быть ещё неуспешной, потому что у нас
[01:10:15.280 --> 01:10:27.680]  конкуренция, и наш пропоз прервали, потому что кто другой сделал prepare. Но такой сценарий сам по
[01:10:27.680 --> 01:10:34.160]  себе не означает для этого протокола, что транзакция была неуспешной. Это лишь означает,
[01:10:34.160 --> 01:10:40.040]  что она не понимает, чем она закончилась, её просто перехватили. Но новая транзакция может как бы
[01:10:40.040 --> 01:10:49.280]  зафиксировать значение нашей. Поэтому вот после такой транзакции, вообще говоря, мы можем не знать,
[01:10:49.280 --> 01:10:55.760]  чем дело закончилось, мы можем получить ошибку в духе, ну вот, непонятно. Но в любом случае это
[01:10:55.760 --> 01:11:05.800]  всё безопасно по retry, потому что, ну вот, если как бы if… Понятно, потому что если наша транзакция
[01:11:05.800 --> 01:11:11.360]  всё-таки закоммитировалась, зафиксировалась, то мы увидим, что… То мы, возможно, увидим эффект этого.
[01:11:11.360 --> 01:11:20.760]  У нас операция провалилась, но другая транзакция, закоммитировавшись, закоммитила ещё и нас вместе
[01:11:20.760 --> 01:11:26.640]  с собой. Поэтому мы можем retry нашу транзакцию и не беспокоиться, что она применяет второй раз,
[01:11:26.640 --> 01:11:31.680]  ну, поскольку у нас операция CAS. Ну, конечно, это не совсем общий случай, но всё же, вот там, скажем,
[01:11:31.680 --> 01:11:37.600]  для таких сценариев, где у нас не бывает откатов в сторону NoteXist, это всё будет работать.
[01:11:37.600 --> 01:11:47.760]  В общем, удивительным образом всё-таки Paxos туда встроили, так встроили сбоку немного,
[01:11:47.760 --> 01:11:59.000]  но он там всё же есть. Так что, если нас не беспокоит согласованность, нас беспокоит доступность,
[01:11:59.000 --> 01:12:05.360]  то нас устраивает, то мы используем базовый протокол репликации с кворумами, с однофазными
[01:12:05.360 --> 01:12:12.000]  записями, с асинхронным read-repair'ом, и не имеем разумной гарантии. Но если нам всё-таки где-то
[01:12:12.000 --> 01:12:19.960]  нужно что-то упорядочить, то мы используем всё-таки такой вот локальный Paxos в виде
[01:12:20.800 --> 01:12:27.520]  Ну и последние какие-то замечания общие.
[01:12:27.520 --> 01:12:36.920]  Cassandra из отдельных узлов, вот как эти отдельные узлы хранят данные сами по себе. Ну, это киволе
[01:12:36.920 --> 01:12:42.720]  хранилища, поэтому Cassandra использует снова LSM, то есть ту технику, которую мы уже много раз
[01:12:42.720 --> 01:12:49.760]  наблюдали. Ну и в Cassandra есть ещё одна забавная техника, которую мы видели в контексте блокчейнов,
[01:12:49.760 --> 01:12:58.560]  но вот Cassandra, эта техника полезна, это деревья меркла. Деревья меркла нужны Cassandra на уровне
[01:12:58.560 --> 01:13:07.920]  хранения, когда узел восстанавливается после сбоев. Вот, допустим, его долго не было, и он должен
[01:13:07.920 --> 01:13:14.960]  понять, какие данные ему нужно забрать в других узлов. Просто реплика была выключена некоторое
[01:13:14.960 --> 01:13:22.120]  время, потом её включили, и она должна нагнать состояние. Вот, чтобы не копировать весь жесткий
[01:13:22.120 --> 01:13:29.600]  диск реплики на реплику после её восстановления, реплика, новая реплика может обменяться с другой,
[01:13:29.600 --> 01:13:36.920]  своим представлением о ключах, которые эта реплика хранит в виде дерева меркла,
[01:13:36.920 --> 01:13:45.440]  точнее, в виде верхушки дерева меркла. Реплика всё-таки отвечает за... реплика отвечает за какой-то
[01:13:45.440 --> 01:13:51.320]  набор ключей, мы делим набор ключей на какие-то поддиапазоны, дотриминированным образом, считаем
[01:13:51.320 --> 01:13:57.320]  хэши, потом считаем, потом комбинируем хэши лево-право под дерево, ну и на другую реплику
[01:13:57.320 --> 01:14:05.320]  отправляем верхушку этого дерева. И реплика, получив эту верхушку, сравнивает эту верхушку
[01:14:05.360 --> 01:14:12.400]  своей верхушкой и может локализовать, в каком именно под дереве, в каком поддиапазоне ключей есть
[01:14:12.400 --> 01:14:25.400]  потенциально див и его переотправить на другую машину. То есть тут в биткоине Merkel 3 использовался
[01:14:25.400 --> 01:14:30.720]  для того, чтобы построить такой категорфический сертификат, здесь это всего лишь способ передать
[01:14:30.720 --> 01:14:36.480]  поменьше информации и локализовать разницу, которая накопилась на узлах за время простой одного из них.
[01:14:36.480 --> 01:14:47.360]  Ну что, примерно такой дизайн Cassandra, он не слишком современен, кажется, люди научились делать
[01:14:47.360 --> 01:14:55.440]  масштабируемые системы, надёжные системы, которые ещё и будут согласованы, но тем не менее вот такой
[01:14:55.440 --> 01:15:03.400]  подход существует и его скорее интереснее противопоставить дизайну Bigtable и HBase, потому
[01:15:03.400 --> 01:15:10.000]  что это буквально решение одной и той же задачи, но которые выбрали очень разные противоположные
[01:15:10.000 --> 01:15:15.440]  пути. В одном случае централизованный дизайн и консенсус, в другом случае децентрализованный
[01:15:15.440 --> 01:15:23.760]  дизайн и вот все эти векторы версий, расходящиеся истории, конфликты, разрешение их на стороне
[01:15:23.760 --> 01:15:33.080]  пользователя. Ну, кстати, конфликт, это само по себе не страшно, потому что вполне себе можно
[01:15:33.080 --> 01:15:41.280]  представить структуру данных, которые могут конфликт разрешать. Если мы строим буквально корзину,
[01:15:41.280 --> 01:15:47.320]  то это множество, ну а скажем, разные вариации множеств можно строить так, чтобы конфликт
[01:15:47.320 --> 01:15:52.120]  разрешались автоматически. Ну скажем, если мы во множество только добавляем элемент, то понятно,
[01:15:52.120 --> 01:15:58.680]  что даже если у нас есть конкурирующие апдейты, конфликтующие апдейты, то, точнее, конфликтующие
[01:15:58.680 --> 01:16:02.520]  апдейты не может быть. У нас есть конкурирующие апдейты, то всегда можно смёрзать общие значения,
[01:16:02.520 --> 01:16:06.520]  общий результат, просто взяв победение двух множеств. Если у нас есть множество,
[01:16:06.520 --> 01:16:14.200]  в которое мы можем добавлять и, скажем, один раз удалять, то у нас тоже есть, ну это называется
[01:16:14.200 --> 01:16:20.200]  CRDT, вот способ построить такое множество и сливать конфликты автоматически.
[01:16:20.200 --> 01:16:25.480]  Если же мы можем и добавлять, и удалять произвольное количество раз, ну короче,
[01:16:25.480 --> 01:16:31.200]  тут есть разные вариации, как это можно делать. Какого-то, понятно, общего, максимально
[01:16:31.200 --> 01:16:38.000]  универсального рецепта не существует, но для некоторых под задач, для каких-то отдельных
[01:16:38.000 --> 01:16:47.720]  вариаций такие решения есть. В конце концов, если эту идею развивать, то можно прийти к какому-то
[01:16:47.720 --> 01:16:53.960]  коллаборативному редактированию текста, Google Doc, это вот как раз воплощение потомной структуры
[01:16:53.960 --> 01:17:00.840]  данных в максимально общем виде, там, где у нас уже не множество, не регистры, а целые деревья или
[01:17:00.840 --> 01:17:10.320]  более сложные структуры. Ну в общем, вот это ограничение, эти проблемы со слиянием версий,
[01:17:10.320 --> 01:17:15.320]  по определённой степени разрешаются просто выбором правильной структуры данных,
[01:17:15.320 --> 01:17:23.000]  которые мы реплицируем. Ну или мы доверяемся просто Кассандре и её наивному подходу
[01:17:23.000 --> 01:17:36.080]  сравнения временных меток, полученных из локальных часов. Ну что, есть какие-то вопросы?
[01:17:40.240 --> 01:17:44.600]  Вот кажется, всё довольно просто, довольно наивно, но это всё-таки было придумано уже сколько,
[01:17:44.680 --> 01:17:47.520]  15 лет назад. Человечество с тех пор ушло далеко вперёд.
[01:18:03.800 --> 01:18:11.400]  Ну ладно, тогда спасибо, что заглянул. Счастливо.

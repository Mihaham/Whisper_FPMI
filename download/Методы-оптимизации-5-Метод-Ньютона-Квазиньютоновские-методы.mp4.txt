[00:00.000 --> 00:11.480]  Сегодня, соответственно, продолжаем решать задачу безусловной оптимизации в последний раз.
[00:11.480 --> 00:18.280]  И разбираемся с такими методами, как метод Ньютона, квазиньютоновские методы и все,
[00:18.280 --> 00:28.720]  что с ними связано. Так, хорошо, хорошо. И начинаем мы чуть-чуть необычно. И рассматриваем следующую
[00:28.720 --> 00:37.240]  задачу. Задача у нас вот такая. То есть мы ищем не какой-то минимум функции и так далее, мы ищем точку,
[00:37.240 --> 00:45.000]  в которой значение, причем скалярной функции, равняется нулю. Равняется нулю. Вот такая вот задачка.
[00:45.000 --> 00:51.200]  Связана это с тем, что именно такую задачу в свое время рассматривал Исаак Ньютон, когда придумывал
[00:51.200 --> 00:59.760]  свой метод Ньютона. Соответственно, с нее мы и стартуем. Окей, смотрите в чем идея. Давайте
[00:59.760 --> 01:07.840]  посмотрим на какую-то точку t0. Ну, мы взяли какую-то точку t0. И как-то двигаясь итеративно от этой
[01:07.840 --> 01:15.720]  точки t0, мы хотим прийти к точке t со звездой. В самом лучшем случае, вообще говоря, мы хотим прийти
[01:15.720 --> 01:23.600]  за одну итерацию. Давайте попробуем найти такое дельта t, чтобы у нас вот t0 плюс дельта t в итоге
[01:23.600 --> 01:31.400]  равнялась t со звездой. В худшем случае примерно, а в лучшем случае прям точно. Вот, окей. Давайте
[01:31.400 --> 01:42.840]  тогда разложим нашу функцию в точке t0 плюс дельта t по тейлору. Что у нас тогда получается?
[01:42.840 --> 01:55.240]  Fiat t0 плюс производная точка t0 умножить на дельта t плюс умалая от дельта t. Ну, а дальше что скажем?
[01:55.240 --> 02:03.960]  Скажем следующее. Пусть это будет примерно равно дельта t0 плюс производная t0 на дельта t. Выкинем
[02:03.960 --> 02:16.440]  просто умало. В то же время мы предположили, что вот это у нас примерно t со звездой значение
[02:16.440 --> 02:26.360]  под функцией. А здесь соответственно у нас что будет? Тогда если это значение в точке t со звездой,
[02:26.360 --> 02:32.440]  то здесь у нас можно сказать, что это значение будет равняться примерно нулю. Тогда получается,
[02:32.600 --> 02:41.000]  что у нас, если мы соединим эти два выражения, вот это и вот это, получится, что fiat t0 плюс
[02:41.000 --> 02:49.000]  производная t0 дельта t равняется нулю. Откуда можно выразить значение для дельта t? И оно
[02:49.000 --> 02:56.680]  соответственно будет равно минус значению функции в точке t0 делить на производную точке t0. Вот,
[02:56.680 --> 03:03.680]  и таким вот образом можно получить итеративную схему метода Ньютона, именно в классическом
[03:03.680 --> 03:10.000]  варианте tkt, это tkt, плюс дельта t, а дельта t соответственно вычисляется так, как мы
[03:10.000 --> 03:20.560]  только что обсудили. И от tktova делить на phi штрих от tktova. Вот такой вот метод придуман,
[03:20.560 --> 03:25.240]  соответственно, был в середине 17 века Исаком Ньютоном для решения вот такой вот задачки,
[03:25.240 --> 03:37.400]  или поиска корня функ. Хорошо, на слайдах тоже самое у вас, дельта t и вот метод.
[03:37.400 --> 03:53.800]  Вот, давайте я вас спрошу, я рассказал какую-то интуицию, как до этого метода можно дойти. А насколько
[03:54.040 --> 04:04.120]  интуиция вообще рабочая? Вот, которую я изложил. Что там, какие были в некотором смысле переходы,
[04:04.120 --> 04:15.520]  которые могли быть некорректны? Раз. Два. Ну то есть вообще существование производной,
[04:15.520 --> 04:21.200]  это вещь не очевидна. Ну если производная существует, пусть будет производная существует.
[04:21.200 --> 04:28.160]  Вообще да, ключевой вопрос то, что я очень успешно так сказал, что у мало дельта t это что-то
[04:28.160 --> 04:33.360]  очень маленькое, поэтому там что-то примерно равно и поэтому все хорошо. На самом деле у метода Ньютона
[04:33.360 --> 04:41.280]  в связи с этим есть одна довольно большая проблема. Вот, то что по факту нужно рассматривать точки,
[04:41.280 --> 04:46.000]  которые, ну даже стартовую точку, довольно хорошей окрестности, точки теста звездой,
[04:46.000 --> 04:52.600]  чтобы действительно там умало было реально умалым. Вот, давайте посмотрим на такую функцию. Вот,
[04:52.600 --> 04:58.200]  на такую функцию. Одномерная функция. Какое у нее решение? Быстренько. Ноль. Единственное решение
[04:58.200 --> 05:03.280]  phi от нуля равно нулю только в точке ноль. Понятно, знаменатель всегда положительный,
[05:03.280 --> 05:09.520]  а числитель только ноль в одной точке. Хорошо. Давайте, я уж не буду вас мучать подсчетом
[05:09.520 --> 05:15.760]  производной. Я ее за вас посчитал. Давайте запишем итерацию метода Ньютона для данной задачки.
[05:15.760 --> 05:26.800]  tk t plus 1 равно tk t минус значение функции в точке tk делить на производную функцию в точке tk.
[05:26.800 --> 05:39.200]  Так, tk. Дальше что у нас значение функции в точке tk? Это tk делить на корень из 1 плюс tk в квадрате.
[05:39.200 --> 05:52.960]  Вот. И делим это еще на 1 плюс, ну, 1 делить на 1 плюс t в квадрате, tk t в квадрате в степени
[05:52.960 --> 06:00.160]  3 вторых. Вот. Тут сокращается 1 корешок, как вы видите, да? Вот. И остается соответственно,
[06:00.160 --> 06:14.080]  что tk t минус tk t. Я звук не замутил. Слышу себя. Вот. tk t, а здесь соответственно получается,
[06:14.080 --> 06:22.360]  что tk t 1 плюс tk t в квадрате. Так? Ну и все это раскрыв скобочки, получаем, что у нас
[06:22.360 --> 06:29.720]  на каждой итерации метода Ньютона мы будем приходить вот в точку tk t плюс 1, вот такого вида.
[06:29.720 --> 06:35.880]  Окей. К вам теперь вопрос. Насколько это вообще точка адекватная?
[06:35.880 --> 06:45.800]  То есть, да, смотрите, если у нас все, если tk t нулевое у нас, например, меньше единицы по модулю,
[06:45.800 --> 06:53.760]  так тогда мы соответственно tk t плюс 1 будет тоже по модулю меньше единицы, причем значительно
[06:53.760 --> 06:58.200]  меньше, потому что мы в куб возводим, да, знак поменяется, но вот такой атеративной схемы,
[06:58.200 --> 07:03.600]  мы все ближе и ближе будем подходить к решению. Да? Если, соответственно, у нас t0 по модулю
[07:03.600 --> 07:07.480]  равно единицы, мы просто будем астеллировать плюс-минус единицы. Ну и, соответственно,
[07:07.480 --> 07:14.360]  если t0 у нас больше по модулю единицы, понятно, все это будет успешно, далеко и расходится. Вот.
[07:14.360 --> 07:19.720]  Это вот тот пример того ключевого особенности метода Ньютона, даже того классического,
[07:19.720 --> 07:25.560]  который придумал в свое время Исаак Ньютон, то, что этот метод является локальным. Локальным. Что
[07:25.560 --> 07:30.640]  это значит? Вот до этого мы с вами рассматривали, например, градиентный спуск, который находил нам
[07:30.640 --> 07:36.680]  решение. Да, там, другое задача не такой, там, не ноль мы искали функций, а минимум. Вот. Да,
[07:36.680 --> 07:42.600]  он, мы из какой бы с точки ни стартовали, мы все равно гарантированно доползем до решения. Вот. Здесь
[07:42.600 --> 07:48.400]  сходимость локальна, потому что нужно стартовать из нужной окрестности решение. Стартуете далеко
[07:48.400 --> 07:54.520]  от решения, никуда не сойдетесь. Проблема в некотором смысле да. В некотором смысле да, ну и,
[07:54.520 --> 08:01.080]  соответственно, да, здесь мы отмечаем то, что у нас метод является локальным. Вот. Вот. Окей,
[08:01.080 --> 08:07.240]  теперь давайте подберемся от метода Ньютона, вообще, от классического того, что придумал Исаак Ньютон,
[08:07.240 --> 08:13.160]  к нашей задаче-то минимизации. Мы минимизируем нашу функцию, безусловную, выпуклую, дважды
[08:13.160 --> 08:19.360]  непрерывно дифференцируемую. Вот. На РД, на всем пространстве, нет ограничений никаких. Что,
[08:19.360 --> 08:26.800]  какой ноль мы там у нее искали в свое время? А? Минимум. Мы у нее искали минимум, а ноль чего
[08:26.800 --> 08:33.120]  получается? Ноль градиента, да, то есть мы знаем, что для выпуклых задач необходимым и достаточным
[08:33.120 --> 08:39.240]  условиям минимума является ноль градиента. Вот. И, соответственно, по факту, что мы можем сделать?
[08:39.240 --> 08:47.440]  От задачи, которую рассматривал Ньютон, когда он искал ноль функций, мы можем перейти к задаче уже
[08:47.440 --> 08:56.800]  нашей, когда мы искали, ищем ноль градиента. Ноль градиента. Вот. И тогда метод Ньютона переписывается
[08:56.800 --> 09:06.000]  следующим образом. Ну, в силу того, что у нас теперь уже этот градиент, то надо брать еще
[09:06.000 --> 09:13.320]  производную, это уже гессиан. Вот. Ну и понятно, что там операция 1 делить на производную уже не
[09:13.320 --> 09:18.440]  особо валидна. Нам нужно в некотором смысле эту операцию поменять. Ну вот здесь, соответственно,
[09:18.440 --> 09:25.960]  она меняется на обратную матрицу гессиана. Вот. Вот так вот этот метод выглядит. Сейчас более строго
[09:25.960 --> 09:31.520]  поймем, откуда это тоже берется, но пока это в некотором смысле просто исторический референс,
[09:31.520 --> 09:39.280]  как мы пришли от классического метода Ньютона, который был придуман в 17 веке, к тому методу Ньютона,
[09:39.280 --> 09:46.400]  который нужен нам для задач безусловной оптимизации. Вот. Хорошо. Здесь мы, соответственно, его пишем.
[09:46.400 --> 09:53.360]  Ну вот здесь прямо листинг алгоритма. Метода Ньютона. Вот. Хорошо. Как мы к нему можно еще прийти?
[09:53.360 --> 10:01.000]  Можно вспомнить, что у нас есть опроксимации по Тейлору. Вот. Опять же все можно разложить в ряд.
[10:01.000 --> 10:08.000]  Тут мало как раз не написано, потому что знак уже стоит приближенный. Вот. И в случае градиентного
[10:08.000 --> 10:13.880]  спуска мы с вами работали с опроксимацией первого порядка. Здесь, соответственно, давайте рассмотрим
[10:13.880 --> 10:20.480]  опроксимацию чуть более высокого порядка второго. Вот. И скажем, давайте-ка я минимизирую эту опроксимацию
[10:20.480 --> 10:29.400]  второго порядка. Минимизирую по x. Минимум по x. Минимум по x. Ну, соответственно, что? Когда я беру минимум по
[10:29.400 --> 10:34.640]  x от этой опроксимации, что у меня будет получаться? Я, опять же, воспользуюсь необходимым условием и просто
[10:34.640 --> 10:40.400]  возьму градиент по x. От этого выражения будет получаться градиент от x-катова. Это вылезает
[10:40.400 --> 10:49.440]  из скалярного произведения. Плюс, соответственно, из квадратичного кусочка у меня вылезет f от x-катова
[10:49.440 --> 10:59.040]  гисян на x на минус x-каты. Вот. Здесь я сразу воспользовался тем, что у меня гисян симметричен.
[10:59.040 --> 11:05.720]  Вот. Поэтому там по факту должно было вылезти a транспонированная. Ну, там сумма a и a транспонирована
[11:05.720 --> 11:10.600]  в матрице. На одну вторую на гисян симметричен, поэтому все. Сразу написал так. Соответственно,
[11:10.600 --> 11:17.280]  я ищу 0 и x звездой. Ну и, соответственно, как? В градиентном спуске мы проставляли просто другую
[11:17.280 --> 11:23.360]  точку, а здесь будет x звездой. Это как раз следующая точка. Вот. Ну и отсюда можно выразить как раз
[11:23.360 --> 11:31.040]  x-каты плюс 1, и получится ровно то, что мы хотели, метод Ньютона. Вот. Это более, так скажем,
[11:31.040 --> 11:36.160]  понятное объяснение, откуда берется он. Просто там непонятно, откуда взялся гисян,
[11:36.160 --> 11:40.000]  почему мы его обратили на течение. Понятно, почему именно вот он так возник. Ну вот здесь
[11:40.000 --> 11:49.200]  это становится более понятно. Вот. Получили метод Ньютона. К сожалению, он от своего, так скажем,
[11:49.200 --> 11:55.080]  классического предшественника берет ту же самую проблему локальной сходимости, плюс вбирает
[11:55.080 --> 12:01.960]  себя дополнительно в некотором смысле проблемы, с которыми мы с вами не сталкивались в градиентном
[12:01.960 --> 12:08.600]  спуске. Например, подсчет гисяна. Операция довольно дорогая. Вот. Ну, то есть, иногда и градиент взять
[12:08.600 --> 12:13.320]  сложно, а тут гисян еще нужно считать. Плюс, соответственно, именно с точки зрения арифметических
[12:13.320 --> 12:18.280]  операций вы его посчитали, вы еще нужно его обратить. Вот. Обращение матрицы – вещь довольно дорогая,
[12:18.280 --> 12:23.920]  как мы знаем. То есть, мы до этого как бы избегали этого всего. Вот. Как раз, вот, условно, в методе,
[12:23.920 --> 12:28.760]  когда мы говорили про сопряженный градиент и решали систему, мы как раз пытались отойти от того,
[12:28.760 --> 12:35.560]  чтобы решать эту систему в явном виде, то есть, обращать матрицу. Хорошо. Хорошо. Здесь все
[12:35.560 --> 12:45.440]  записано. То есть, стоимость итерации значительно возрастает. Вот. Обращение гисяна. Такой вопрос. А
[12:45.440 --> 12:50.400]  за сколько итераций, вообще, метод Ньютона сойдется для квадратичной задачи с положительно
[12:50.400 --> 12:55.840]  определенной матрицей? Квадратной положительно определенной матрицы. Вот у нас есть задачка,
[12:55.840 --> 13:03.640]  например, 1 вторая, x транспонированная, ax. Она у нас симметричная, положительно определенная. Вот.
[13:03.640 --> 13:09.600]  Минус bx. Вот. За сколько итераций мы найдем методом Ньютона оптимум вот этой задачки?
[13:09.600 --> 13:21.000]  Во. За одну. Хорошая ответ. Давайте просто посмотрим, что получится. Запишем итерацию. Так.
[13:21.000 --> 13:30.120]  А у нас здесь получится гисян. А что у нас обратный гисян для квадратичной задачи? Симметричной
[13:30.120 --> 13:39.160]  матрицы. Ростоматрица А это гисян. Соответственно, обратно гисян это минус. А в минус 1. Вот. Окей.
[13:39.160 --> 13:50.240]  А градиент? Градиент сколько будет здесь? ax-b. Все правильно. Опять же, тут пользуемся сразу же
[13:50.240 --> 14:01.080]  симметричностью. Ну и тогда чему равно xкат? Быстренько это заметит. А-1, минус 1, b. Просто решение
[14:01.080 --> 14:06.800]  системы. То есть, видите, квадратичная задача начинает сразу подсказывать, где мы вообще
[14:06.800 --> 14:12.840]  находимся. То есть, кажется, что действительно итерация подорожала. Посчитали гисян. Вот. Ну,
[14:12.840 --> 14:17.120]  может быть, это даже, кстати, в случае квадратичной задачи и недорого, потому что матрицу-то мы знаем.
[14:17.120 --> 14:22.760]  Вот. Вот обращение гисяна в данном случае ключевая операция. Но метод при этом сошелся за одну
[14:22.760 --> 14:27.680]  итерацию. Вот. И это подталкивает нас к мысли, что метод Ньютона, который по факту выбирает
[14:27.680 --> 14:33.960]  себе довольно много недостатков, как вычислительных, так и, например, относящихся к сходимости. Вот. У него
[14:33.960 --> 14:39.160]  есть одно замечательное свойство, что он очень быстрый. Очень быстрый при своей стоимости итерации.
[14:39.160 --> 14:48.400]  Вот. Ну, давайте тогда, что? Попробуем показать, насколько он быстрый. Вот. Для этого вводится
[14:48.400 --> 14:53.040]  следующее предположение. Пусть у нас, опять же, задача безусловной оптимизации. Стелевая функция у
[14:53.040 --> 14:59.120]  нас мюсильно выпуклая. Вот. Здесь я, соответственно, записываю критерии второго порядка, с которым вы уже
[14:59.120 --> 15:05.280]  взаимодействовали. Вот. В том числе в домашнем задании. И, соответственно, также я предполагаю,
[15:05.280 --> 15:12.600]  что у нас гисиан является липшицевым. Липшицевым. Вот. Здесь сразу нужно отметить, что в силу того,
[15:12.600 --> 15:16.960]  что теперь мы записываем эту разность, как у нас была до этого градиентов. Была также функция,
[15:16.960 --> 15:21.920]  когда мы говорили про липшицы, то есть функция, там был просто модуль. Вот. А здесь у нас получается
[15:21.920 --> 15:27.720]  уже разность матриц. Вот. И здесь, соответственно, вот эта вторая норма, это, как вы думаете, какая норма?
[15:27.720 --> 15:34.560]  Там не так много вариантов для матриц норм, которые обозначаются второй. Неправильная,
[15:34.560 --> 15:44.640]  нефробениуса. Вот. Евклидова норма. Какую норму индуцируют для матрицы? Какую норму? Какую норму индуцируют?
[15:44.640 --> 15:54.520]  Кто помнит? Нефробениусов, там ты делаешь что? Как бы каль... А! Всё, отлично. Вот. Просто мне
[15:54.520 --> 15:58.920]  интересно было, у вас вообще есть курсы вот такие, которые про нормы матрицы, или только это
[15:58.920 --> 16:07.320]  в функоне у вас есть? Нету, да? Ну вот на функоне только нормы оператора. Да, про матрицы вы не говорите,
[16:07.320 --> 16:13.920]  как сопрягаются. Вот. Ну смотрите, как обычно определяется норма матрицы. То есть вы берёте
[16:13.920 --> 16:21.480]  какую-нибудь векторную норму, берёте и говорите, что она у вас там на шарике. Вот. И берёте с
[16:21.480 --> 16:30.920]  упремом вот такое выражение. На функоне, да? На мотоне. Вот. На мотоне? Нафига вам на мотоне
[16:30.920 --> 16:43.200]  была нужна эта? Ну ладно. Вот. Так. Окей. Соответственно так вот определяется норма матрицы. И в случае,
[16:43.200 --> 16:50.480]  когда вы сюда вбахиваете евклидово-векторную норму, вот, у вас она индуцирует спектральную норму. То
[16:50.480 --> 16:54.560]  есть в принципе вы со спектральной нормой тоже в некотором смысле должны быть знакомы, когда вы
[16:54.560 --> 16:59.360]  с Константу Липши Саэль оценивали в домашнем задании. Ну кто познакомился, кто не познакомился.
[16:59.360 --> 17:04.080]  Но спектральная норма, она на то и спектральная. Вот. Кто знает, что там лежит в спектральной норме,
[17:04.080 --> 17:12.280]  на что она опирается? Может быть, кто знает? На сингулярные числа. Она опирается на сингулярные
[17:12.280 --> 17:18.280]  числа, на собственные значения. Соответственно, если вы берёте спектральную нормницу матрицы А,
[17:18.280 --> 17:23.160]  то она опирается на собственные значения матрицы А транспонированных корешок. Это сингулярные
[17:23.160 --> 17:30.520]  числа для вещественной матрицы. Вот. Ну и в нашем случае, когда у нас, например, гисианы транспонируемые,
[17:30.520 --> 17:35.840]  то есть симметричные, то есть сингулярные значения будут просто вылезать в собственные значения.
[17:35.840 --> 17:44.520]  Вот. Получается, что так. Ну давайте тогда попробуем. Давайте я сразу же спрошу вопрос. Если у меня
[17:44.520 --> 17:49.400]  гисиан ограничен снизу, то есть является положительно определённым, то есть ещё просто ограничен
[17:49.400 --> 17:57.000]  с константа mu на i. Вот. Что можно сказать про его спектральную норму? Ну смотрите,
[17:57.000 --> 18:04.480]  спектральная норма, она опирается на собственные значения. Ну вот, то есть максимум здесь просто
[18:04.480 --> 18:11.240]  спектральной нормы, здесь будет достигаться, будет на максимальном собственном значении? Ага. То есть
[18:11.240 --> 18:17.560]  максимум будет хотя бы mu. Но с другой стороны, он и снизу будет подпираться mu, так? Потому что
[18:17.560 --> 18:23.640]  минимальное собственное значение тоже mu. Поэтому вот здесь вот вы можете сказать, что у вас гисиан,
[18:23.640 --> 18:33.400]  спектральная его норма, она подпирается, ой, просто-просто больше либо равна, чем mu. Так, я понял,
[18:33.400 --> 18:37.560]  то есть это, может быть, по особе нужно будет добавить про нормы матриц, хотя там вроде есть
[18:37.560 --> 18:44.600]  про нормы матриц. Вот. То есть, чтобы это было понятно. Вот. Ну то есть, главное, что тут понять суть,
[18:44.600 --> 18:48.680]  то есть спектральная норма, которая вот здесь вот индуцируется, ифклидовая норма у вас, она
[18:48.680 --> 18:52.360]  зависит от собственных значений, от сингулярных чисел в данном случае, просто собственных значений,
[18:52.360 --> 18:57.240]  поэтому вот у вас спектральная норма гисианы снизу подпирается mu, потому что он положительно
[18:57.240 --> 19:03.960]  полуопределён, причём так, что ещё и с константа mu подпирается. Вот. Окей. Давайте пойдём по
[19:03.960 --> 19:09.280]  сходимости. Всё тут довольно в некотором смысле интуитивно. Как мы с вами доказывали
[19:09.280 --> 19:14.240]  градиентный спуск, просто смотрели, как меняется расстояние до решения в зависимости от номера
[19:14.240 --> 19:21.040]  итерации, да. Поэтому берём cat k плюс первую итерацию. Вот. И подставляем то, что мы как раз в методе
[19:21.040 --> 19:30.440]  Ньютона используем. Вот. А дальше, соответственно, будет не совсем, что ли, как привычно, но мы в
[19:30.440 --> 19:37.200]  некотором смысле такие трюки с вами уже проделывали. Проделывали. Скажите мне, а что вы можете
[19:37.200 --> 19:44.640]  сказать, например, про вот такое вот выражение, если мы вспомним формулу Ньютона-Лебница. Формулу
[19:44.640 --> 19:51.320]  Ньютона-Лебница. Помните, для функций мы такое же проделывали, когда доказывали липчество. Что мы
[19:51.320 --> 20:02.400]  можем сказать? Что это интеграл от нуля до одного, здесь стоит гессиан, tau, x ката минус x
[20:02.400 --> 20:09.760]  со звездой. Вот. Здесь ещё x ката минус x со звездой до tau. Помните точно такой же трюк мы проделывали
[20:09.760 --> 20:15.680]  с значениями функции и градиентами, да? Просто та же формула Ньютона-Лебница, когда вы интегрируете
[20:15.680 --> 20:22.240]  вдоль, когда у вас кривая интегрирование, она запираметризована каким-то. Мы с вами тогда
[20:22.240 --> 20:31.440]  брали отрезочек, здесь тоже мы берем отрезочек от x со звездой до xk. Ровно то же самое, тут ничего
[20:31.440 --> 20:36.080]  такого сверхъестественного нету. Вот. Только единственное, что теперь мы это записали для векторов.
[20:36.080 --> 20:44.280]  Для векторов и, соответственно, для градиентов, и там возник гессиан. Вот. Что мы знаем про x со звездой,
[20:44.280 --> 20:50.680]  про градиент в точке x со звездой. Он равен нулю, так? Тогда вот то, что записано вот здесь вот,
[20:50.680 --> 20:57.640]  это просто выражение для градиента, да? Ну, я, соответственно, вот это подставлю вместо градиента.
[20:57.640 --> 21:12.040]  Давайте я вот так сделаю. Здесь будет, соответственно, минус гессиан в точке xk минус первое, а здесь будет,
[21:12.040 --> 21:13.600]  соответственно, интегральщик наш.
[21:25.600 --> 21:33.960]  Вот. Хорошо. Хорошо. Смотрите, что я еще замечу. Я замечу, что у меня подинтегральное выражение
[21:33.960 --> 21:41.560]  вообще не зависит от x икса звездой. Вот от этого кусочка. От вот этого кусочка. Вот.
[21:41.560 --> 21:57.920]  Поэтому я его могу вынести за пределы интеграла. Так. Сделаю вот так. Скобочка, скобочка, xk минус x
[21:57.920 --> 22:04.840]  со звездой. Вот. Смотрите теперь, что замечу. Замечу, что у меня вот здесь вот болтается как бы одинаковый
[22:04.840 --> 22:15.800]  вектор. Одинаковый вектор. Что, соответственно, можно сделать? Вот. Что можно, соответственно,
[22:15.800 --> 22:21.200]  сделать? Давайте воспользуемся умной единичкой вот тут вот. Умную единичку добавлю. Вот. Чтобы
[22:21.200 --> 22:27.680]  воспроизвести здесь еще как бы минус гисян. Гисян минус первый. Вот. Ну умножить на гисян.
[22:27.680 --> 22:37.920]  Вот. Я тут перед вектором xкт воспроизвожу вот это. Гисян минус первый и гисян в первый. Просто умная
[22:37.920 --> 22:45.480]  единичка. Вот. А дальше у меня все остается. А теперь что я могу сделать, значит? Теперь я
[22:45.480 --> 22:54.680]  могу получается в некотором смысле вот, вот это и вот это вынести за скобки. То есть слева и
[22:54.680 --> 23:03.320]  справа. Вот. И получается, что у меня в скобках будет разница между гисяном в точке xкт минус вот
[23:03.320 --> 23:11.120]  этот интегральчик. Давайте это сделаем. Так. Так. Так. Здесь это сделано. Здесь уже домножено.
[23:11.120 --> 23:19.600]  Сейчас. Где тут у меня? Так. Так. Во. Здесь это сделано. Вот уже здесь вот на этом моменте сделано.
[23:19.600 --> 23:29.200]  Просто вот обозначено за xкт нашу разность гисяна в точке xкт и интеграла. Хорошо. Теперь давайте
[23:29.200 --> 23:39.800]  оценивать разность в евклидовой норме. В евклидовой норме как у нас меняется расстояние. Так. Соответственно,
[23:39.800 --> 23:58.080]  что здесь получается? Будет f xkt минус первая норма gkt. Так. Теперь смотрите. У меня евклидовая
[23:58.080 --> 24:06.480]  норма. Опять же, свойств получается вы не знаете, что можно сделать. Ну, смотрите. Давайте покажем
[24:06.480 --> 24:12.040]  просто довольно такой простой факт. У меня есть матрица что-то типа вот такой евклидовой нормы.
[24:12.040 --> 24:23.000]  Понятно ли то, что это будет вот так вот? Где, соответственно, вот это уже спектральная норма. Вот.
[24:23.000 --> 24:26.560]  Индуцированная норма и, соответственно, евклидовая. То есть спектральная норма, она индуцируется
[24:26.560 --> 24:32.400]  евклидовой нормой. Ну, это понятно из чего? Из определения. Потому что вот это по факту,
[24:32.400 --> 24:44.360]  это же supremum по всем x, которые равны единичке. Давайте я вот здесь x0 поставлю. Вот. А x. Вот. А вот
[24:44.360 --> 24:50.600]  то, что я вынес, это же по факту то, что это значит. Я просто мог бы как вот это записать. Если бы
[24:50.600 --> 24:55.360]  я хотел, например, записать норму по-другому же, определение нормы матрицы, можно вот так вот
[24:55.360 --> 25:06.120]  записать. Можно брать supremum по x, которые по длине, например, как x0. Как норма x0 в квадрате. Вот.
[25:06.120 --> 25:13.760]  Ну и, соответственно, здесь у меня будет ax. Вот. Ну тогда еще нужно будет отнормировать на x0. Вот.
[25:13.760 --> 25:19.440]  Ну и по факту здесь я вот его просто вынес. Вот. Спектральную норму. Сказал, что у меня есть единичка.
[25:19.440 --> 25:24.760]  Отнормировал как бы вектор x. Вектор x его норму вынес. Я оставил под нормой вот здесь
[25:24.760 --> 25:30.240]  единичный вектор. Ну а там по определению спектральной нормы просто получил норму a.
[25:30.240 --> 25:40.240]  Здесь понятно? Я надеюсь, что да. Вот. Ну если еще раз, можете быстренько глянуть потом по особи,
[25:40.240 --> 25:47.760]  там про нормы матриц есть, в том числе их свойства. Вот. Ну вот здесь я еще раз повторю, довольно
[25:47.760 --> 25:57.760]  просто. Здесь что я сделал? Я взял вектор x, разделил его там условно на норму x0 вторую. Вот.
[25:57.760 --> 26:05.760]  Вынес эту норму x0 искусственно. Вот. И здесь у меня получился вектор длины 1 в евклидовой норме. Да.
[26:05.760 --> 26:10.920]  И поэтому здесь я могу просто сказать, что если я возьму suprem по всем этим x, то у меня будет
[26:10.920 --> 26:16.720]  просто норма матрица спектральная по определению. Вот. Ну это хорошее свойство, оно это работает
[26:16.720 --> 26:20.920]  только с евклидовыми нормами. Это работает со всеми нормами, соответственно, когда у вас норма
[26:20.920 --> 26:27.760]  матрицы индуцируется или как вот создается, задается норма вектора. Вот. Соответственно,
[26:27.760 --> 26:46.320]  здесь я делаю то же самое и выношу вот так вот. Вот. Дальше можно с матрицами в данном случае
[26:46.320 --> 26:50.400]  тоже как с числами, когда у нас там произведение двух матриц по норме, это просто меньше либо
[26:50.400 --> 27:08.040]  равно, чем норма произведения. Вот. Смотрите. Теперь что у меня осталось? По факту вот здесь уже
[27:08.040 --> 27:13.560]  все хорошо. Расстояние до решения, расстояние до решения. Мне нужно оценить вот эти два кусочка.
[27:13.560 --> 27:23.160]  Вот. Ну мы что-то уже знаем про матрицу Гесиана. Мы же знаем, что Гесиан у нас как бы положительно
[27:23.160 --> 27:27.720]  определен, более того, ограничен еще снизу константа μ. А что мы тогда можем сказать про
[27:27.720 --> 27:39.160]  обратную матрицу? Чем она ограничена? И как? Один μ сверху, снизу. Ну вот тут она снизу как бы
[27:39.580 --> 27:47.560]  ограничена. А тут сверху будет все правильно. Ну смотрите, вы обращаете, то есть условно.
[27:47.560 --> 27:53.400]  Смотрите, на примере диагональной матрицы. Вот. Или здесь, о, здесь даже видно, вы домножите
[27:53.400 --> 27:59.680]  сейчас на матрицу Гесиана. Нет, смотрите, вот здесь просто домножите это на матрицу Гесиана.
[27:59.680 --> 28:05.840]  Вот. Правую левую часть вот этого выражения. Правую левую часть этого выражения домножите на
[28:05.840 --> 28:10.820]  матрицу 10. А что получится тогда? У вас здесь получится единичная, меньше
[28:10.820 --> 28:18.720]  либо равна, чем 1 на µ, 10. µ домножаете, нам домножаете на µ, получаете то, что было
[28:18.720 --> 28:22.320]  до этого. Согласны?
[28:23.160 --> 28:29.360]  Вот, тогда смотрите, в силу того, что у вас вот это нам уже не нужно, нам главное,
[28:29.360 --> 28:35.220]  что мы знаем вот это. Вот, тогда смотрите, вы знаете, что у вас матрица сверху
[28:35.220 --> 28:39.220]  ограничена, собственные значения у нее ограничены, 1 делить на µ, поэтому и
[28:39.220 --> 28:43.900]  спектральная норма тоже ограничена, 1 делить на µ.
[28:47.100 --> 28:52.900]  Вот, ну и все, тогда получается вот следующая оценка. Осталось только по факту
[28:52.900 --> 28:59.180]  здесь разобраться со спектральной нормой вот этого безобразия.
[28:59.180 --> 29:05.500]  Жкт, то есть то, что я ввел, это как разность вот этого десианов точки к и интеграла.
[29:05.500 --> 29:09.820]  Вот, ну давайте сейчас с ними будем разбираться, здесь я в принципе просто с µ поигрался,
[29:09.820 --> 29:18.180]  то же самое сделал. Бум-бум-бум-бум. Все, теперь с жк разбираемся. Там странички нету,
[29:18.180 --> 29:25.620]  свободные давайте я создам ее, чтобы расписать это. Так, добавлю страничку. Так,
[29:25.620 --> 29:32.620]  жкт, норма спектральная, это что у нас есть? По факту гисиан в xk там, просто
[29:32.620 --> 29:43.460]  потому как я определил. Вот, на гисиан в точке x звездой плюс tau, xk минус x звездой и здесь
[29:43.460 --> 29:51.820]  d tau. Вот, дальше смотрите, я внесу вот этот гисиан под интеграл, потому что гисиан
[29:51.820 --> 29:56.820]  вообще не зависит от tau. Вот, и проинтегрировав от 0 до 1, я просто получу его же.
[30:12.540 --> 30:20.460]  Так, хорошо. Дальше опять же стандартный трюк, который мы уже с вами применяли, когда опять же
[30:20.460 --> 30:29.460]  говорили про липшицевость градиентов. Вот, норма интеграла, то есть, ну, модуль интеграла меньше
[30:29.460 --> 30:39.500]  чем, получается, модуль от суммы, вот, меньше, чем сумма модулей. Вот, поэтому я норму просто
[30:39.500 --> 30:58.580]  занесу под интеграл и оценю сверху. Ну, а теперь что? Что можно применить?
[30:58.580 --> 31:12.340]  Нет, зачем треугольник? Липшицевость гисиана. Липшицевость гисиана. Здесь вылезет что? m к
[31:12.340 --> 31:18.620]  вылезет и вылезут соответственно, сейчас скажу, что там вылезет. 1 минус tau, да, 1 минус tau вроде вылезет.
[31:18.620 --> 31:36.540]  x kt по норме минус x звездой по норме d tau. Вроде так. Ну, вроде да. Вот, так, тогда можно за пределы интеграла
[31:36.540 --> 31:45.180]  вынести m xk минус x звездой в квадрате. Интеграл останется от нуля до единицы, тот, который, в принципе,
[31:45.180 --> 31:53.940]  уже берущийся. Так, ну что, сколько там получается у этого интеграла? Одна, вторая, все правильно.
[31:53.940 --> 32:09.900]  Все, оценили. jkt оценено. Вот, хорошо. Закроем. И переходим на следующую страничку. Здесь, соответственно,
[32:09.900 --> 32:18.620]  у нас как раз оценил jkt. Вот оно. Ну и подставил. Получилось вот такое вот выражение. Вот. И, на самом
[32:18.620 --> 32:27.100]  деле, это все. Это уже теорема сходимости метода Ньютона за одну итерацию. Вот. Особенности. Смотрите.
[32:27.100 --> 32:34.500]  Степень разная. Степень разная. Вот. Ключево. Плюс коэффициент еще здесь. Давайте рассуждать,
[32:34.500 --> 32:40.260]  что мы в итоге получили. Вот. Сходится, вот судя по этой оценке, сходится ли этот метод всегда?
[32:43.140 --> 32:45.700]  Если за одну итерацию, мы можем гарантировать вот такое.
[32:45.700 --> 33:04.980]  Хотим, чтобы, например, было что-то вот такое, да? Вот. Тогда мы что можем гарантировать? Тогда
[33:04.980 --> 33:10.340]  мы вот в этой оценке можем затереть вот это, затереть одну из степеней и здесь затереть вот
[33:10.340 --> 33:18.540]  нестрогий знак, да? Сделать его, ой, нестрогий знак, делать его строгим. Вот. Все. Соответственно,
[33:18.540 --> 33:22.740]  вот так вот. Когда мы вот попали в такую окрестность, тогда мы действительно можем
[33:22.740 --> 33:27.500]  гарантировать. Вот. И каждую итерацию мы будем приближаться, приближаться, приближаться. Иначе,
[33:27.500 --> 33:32.420]  понятно, никаких гарантий нет. Это вот ровно то, что, в принципе, есть и в классическом методе Ньютона,
[33:32.420 --> 33:38.740]  там, который сам Ньютон придумал. Локальная сходимость. Вот. Но при этом, что можно сказать про именно
[33:38.740 --> 33:44.060]  скорость сходимости, она прям бешена. Если мы уже залетели в решение, то там уже все хорошо. Ну,
[33:44.060 --> 33:50.580]  давайте подставим, например, условно m равно 2, mu равно единички. Ну и вот стартовая точка пусть у меня
[33:50.580 --> 33:55.780]  на расстоянии 1 и 2. Тогда что я могу гарантировать? Что следующая точка у меня на расстоянии 1 и 4.
[33:55.780 --> 34:04.900]  А следующая, лавинообразная, уже на расстоянии 1 и 4 в квадрате, да? И так далее, и так далее,
[34:04.900 --> 34:09.800]  и так далее. Это квадратичная скорость сходимости. Вот эквивалент ее то есть там рассматривали
[34:09.800 --> 34:13.780]  с вами другое определение. Но вот это очень быстро, это квадратичная скорость сходимости
[34:13.780 --> 34:19.900]  по факту у нас курсен методов быстрее не будет. Это квадратичная скорость сходимости. То есть,
[34:19.900 --> 34:25.460]  когда вы попали в решение, когда вы попали в окрестность решения. Сваливаетесь, вы в него
[34:25.460 --> 34:31.020]  начинаете очень-очень быстро. Там, за несколько итераций, вы там прям уже найдете точность там,
[34:31.020 --> 34:36.420]  чуть ли не на 10, минус 10 и так далее. Вот. Это быстрый метод, но опять же проблема
[34:36.420 --> 34:40.260]  есть в локальной сходимости. Ну и в дороге возне итерации.
[34:40.260 --> 34:45.360]  Окей, давайте быстренько порассуждаем, как можно порешать вообще проблему с
[34:45.360 --> 34:49.340]  локальной сходимостью. Вот. Я на самом деле немного удивлен, что когда я вообще
[34:49.340 --> 34:52.740]  расписывал интуицию метода Ньютона, когда я вот оставлял это квадратичную
[34:52.740 --> 34:57.060]  аппроксимацию, это же не совсем то, что мы всегда рассматривали в градиентном
[34:57.060 --> 35:00.340]  спуске. Когда мы смотрим на градиентный спуск, что вы там помните, когда мы
[35:00.340 --> 35:04.540]  рассматривали его итерацию, например, когда мы рассматривали, как он доказывается,
[35:04.540 --> 35:09.100]  какие там были такие, что ли, ключевые идеи.
[35:11.940 --> 35:15.900]  Что есть, например, в градиентном спуске, чего нет в методе Ньютона, который мы
[35:15.900 --> 35:23.060]  рассмотрели? Ну или гладкость тут можно добавить, а скорее вот, ну вот на сам
[35:23.060 --> 35:29.580]  метод посмотрите, что есть тут, чего нет тут и есть там. Шаг, конечно.
[35:29.580 --> 35:35.100]  Потому что мы же как раз, когда говорили про линейную аппроксимацию, когда мы
[35:35.100 --> 35:37.780]  говорили про градиентный спуск, мы говорим, линейная аппроксимация, выходить далеко
[35:37.780 --> 35:44.460]  нельзя, улетим. А здесь, соответственно, да, что-то мы про шаг забыли, то есть,
[35:44.460 --> 35:49.020]  ну вот есть идея, что просто можно добавлять шаг. На самом деле, это не самая крутая
[35:49.020 --> 35:54.380]  идея, вот. Сильно она не фиксит метод Ньютона, но может в некотором смысле
[35:54.380 --> 36:00.540]  точно увеличить радиус сходимости, это раз. Плюс, если вы будете очень умно
[36:00.540 --> 36:06.340]  выбирать шаг, вот, действительно, как-то можно метод сделать более рабастым, с точки
[36:06.340 --> 36:09.980]  зрениями, на окрестности сходимости. В прошлый раз, вы помните, какую-то идею
[36:09.980 --> 36:14.780]  обсуждали выборы шага? Не просто какую-то там константину, как можно было подбирать,
[36:14.780 --> 36:20.300]  что у нас было в методе сопряженных градиентов. Вот что-то вот такое, помните?
[36:20.300 --> 36:26.940]  Гамма-к, давайте тут альфа обозначу. Нет, давайте гамма-к пусть все равно останется в предыдущем
[36:26.940 --> 36:31.820]  складе гамма. Пкта, да, то есть у нас было направление, ну так вот оно направление-то
[36:31.820 --> 36:37.940]  с минусом. Пкта, вот, направление, которое нам дает метод Ньютона, подбирайте гамма,
[36:37.940 --> 36:43.700]  ну и попытайтесь, соответственно, что-то найти. Вопрос, найдете ли вы это гамма, конечно, вот,
[36:43.700 --> 36:48.180]  для некоторых задач. То есть, как бы там, конечно, полностью от локальной сходимости
[36:48.180 --> 36:54.300]  таким способом не избавиться. Вот, тут не всегда эта гамма-к найдется, там нужно, или если линейным
[36:54.300 --> 36:58.340]  поиском, непонятно, какой отрезок брать. Можно добавлять до каких-то условий, которые вы будете
[36:58.340 --> 37:04.420]  разбирать на семинаре, но в общем все равно тут есть как бы в некотором смысле игра. Вот, еще вспоминайте,
[37:04.420 --> 37:11.980]  что было, когда мы с вами доказывали сходимость градиентного спуска. Кому-то даже на семинаре
[37:11.980 --> 37:20.820]  такую задачу давали про то, как подбирать шаг в градиентном спуске. Если помните, из каких
[37:20.820 --> 37:25.180]  вообще соображений подбирается вот оптимальный шаг, например, в градиентном спуске?
[37:25.180 --> 37:37.580]  Ну, помните, как выглядела картинка для линейной апроксимации? И вот,
[37:37.580 --> 37:44.900]  типа того, да, у нас, помните, было как? У нас была как бы, вот сама функция как-то выглядит,
[37:44.900 --> 37:52.380]  и была у нее верхняя апроксимация через l, да, вот, и была нижняя апроксимация через mu, ну, или там не было
[37:52.380 --> 37:58.020]  ее вообще, была там линейная просто апроксимация, если mu нету. Вот, и шаг-то там подбирался как? Чтобы
[37:58.020 --> 38:05.220]  минимизировать что? Я вам, помню, рассказывал, забыли, наверное, вот, минимизировать вот эту
[38:05.220 --> 38:10.020]  параболу, потому что она как бы говорит, насколько функция вообще быстро может меняться. Вот это
[38:10.020 --> 38:16.020]  константа l, видно, что она растет быстрее, чем сама функция, вот, и поэтому, как бы исходя из этой
[38:16.020 --> 38:20.300]  параболы, вы можете делать какие-то выводы, насколько большим шаг вы можете брать, то есть в худшем случае
[38:20.300 --> 38:26.260]  у вас функция реально может только сильно скакать, вот, сильно расти, вот, ну и, соответственно, да,
[38:26.260 --> 38:31.300]  мы хотим, чтобы мы как бы в некотором смысле не перескакивали через ветви параболы, а наоборот,
[38:31.300 --> 38:37.660]  как-то хорошо сходились. Ну и самый такой хороший вариант, это просто взять и упасть в минимум этой
[38:37.660 --> 38:44.140]  параболы, да? Да, он не отражает до конца свойства функции, минимум может быть реально ниже, вот, но
[38:44.140 --> 38:48.940]  делая так вот итеративно, мы приближаемся, мы приближаемся, то есть каждый раз восстанавливаю эту
[38:48.940 --> 38:55.020]  параболу, вот, и поэтому шаг градиентного спуска, там вот, градиентный спуск с шагом 1 делить на l, это
[38:55.020 --> 39:00.780]  более чем хороший выбор. Есть выбор и получше, вот, как показывает теория, но вот шаг 1 делить на l,
[39:00.780 --> 39:08.700]  в выпуклом случае он как бы вообще не улучшаем, вот, соответственно, да, тут направление, соответственно,
[39:08.700 --> 39:15.340]  мы с вами поговорили, вот, как раз минимизировали вот эту параболу по иксу, вот, и это было эквивалент
[39:15.340 --> 39:22.900]  тому, что мы просто делаем шаг градиентного спуска с 1 делить на l, вот, а здесь, вы помните,
[39:22.900 --> 39:27.780]  когда я же, когда вам метод Newton это показал, я сказал, что вот вам, как бы, место линейной
[39:27.780 --> 39:33.860]  аппроксимации квадратичной, вот, только я все малое, а оно может быть и нифига не умалое,
[39:33.860 --> 39:41.300]  забыл, вот, ну и получил какой-то метод, но и здесь такая же мысль, если вот это гарантированно работает,
[39:41.300 --> 39:47.540]  если мы как бы ограничили функцию сверху, знаем, что она растет не быстрее, чем вот это, вот, и тогда
[39:47.540 --> 39:56.380]  вот, подбирая такой шаг, будет все сходиться, вот, то здесь аналогичная идея, вот, причем вот, как вы
[39:56.380 --> 40:01.820]  помните, вот, то, что записано под argmin в первом случае, оно ведь реально мажорирует функцию,
[40:01.820 --> 40:09.140]  когда у вас l гладкий градиент, вот, здесь же это как, это по факту argmin по иксу, это же тоже
[40:09.140 --> 40:15.180]  самое, что minimum как бы по иксу, то есть, вот так вот, вы можете записать вот такое свойство, это вот как
[40:15.180 --> 40:21.300]  раз просто гладкость, липшица из градиента, вот, и оказывается, это тоже справедливо, вторая строчка
[40:21.300 --> 40:28.460]  тоже справедлива, когда у вас липшица в гессиан, вот, и вот идея как раз в том, чтобы минимизировать,
[40:28.460 --> 40:35.220]  делать шаг ньютона, но вот добавляя вот эту вот дополнительную, дополнительный член, который как
[40:35.220 --> 40:40.340]  раз вылезает из-за того, что вы говорите, что у вас липшица в гессиан, поэтому сверху вы можете это
[40:40.340 --> 40:47.940]  ограничить, вот, такой метод называется кубический метод ньютона за авторством Полика и Нестерова,
[40:47.940 --> 40:54.740]  15 лет назад, вот, соответственно, в домашнем задании, в бонусной, ну, там, частью с треугольничком,
[40:54.740 --> 40:59.820]  у вас он есть, в части с треугольничком у вас он есть, можно с ним поиграться, вот, как он работает,
[40:59.820 --> 41:07.420]  соответственно, вот, вот такие способы, как можно фиксить локальную сходимость метода ньютона, но,
[41:07.420 --> 41:13.140]  смотрите, вообще глобальная, какая ключевая проблема метода ньютона при этом остается, это подсчет
[41:13.140 --> 41:23.340]  гессиана, ну и его обращение, возникает мысль, давайте-ка сделаем перерыв и потом перейдем к следующей,
[41:23.340 --> 41:29.940]  как раз к теме, как мы будем избегать подсчет гессианов, все, давайте перерыв 5 минут, так, ну,
[41:29.940 --> 41:37.580]  ладно, давайте продолжать, продолжать, вот, про что теперь пойдет речь, опять же, поняли, что вроде
[41:37.580 --> 41:46.100]  как локальность, ну, она, конечно, остается, но частично решаема, вот, но подсчет гессиана и его
[41:46.100 --> 41:53.280]  обращение, это не решаемая проблема для метода ньютона, в некотором смысле, вот, поэтому давайте
[41:53.280 --> 41:58.340]  запишем, опять же, итерацию, вот, похожую на метод ньютона, где вот матрица есть, а некоторые
[41:58.340 --> 42:09.620]  матрица ашкаты, то есть в случае метода ньютона здесь встает обратный гессиан, ну, а мы зададимся
[42:09.620 --> 42:18.500]  целью, давайте-ка этот обратный гессиан, ну, как-то дешево, что ли, считать, вот, или как-то дешево
[42:18.500 --> 42:27.500]  опроксимировать, вот, смотрите, в чем идея будет, идея будет следующим, вот, смотрите, давайте опять же
[42:27.500 --> 42:35.140]  запишем нашу формулу Тейлора, которую мы уже с вами несколько раз сегодня писали, например, в таком
[42:35.140 --> 43:00.660]  виде, для градиента, для градиента, вот так вот, 1, xk-xk+, 1, плюс-малая от xk-xk+, 1, вот, ну, и как уже привычно
[43:00.660 --> 43:09.260]  нам делать, чтобы получить какую-то интуицию, давайте, опять же, я уберу малое, вот, и скажу, что это
[43:09.260 --> 43:16.620]  примерно равно, вот, тогда, тогда, смотрите, какое свойство, в некотором смысле, гессиана, мы из этого
[43:16.620 --> 43:29.860]  можем вытащить, f гессиан в точке k+, 1, в минус первой, это получается, что градиент f от xk+, 1,
[43:29.860 --> 43:43.860]  минус градиент f от xk, равно xk+, 1, минус xk, вот, просто из строчки выше выразил, ну, то есть,
[43:43.860 --> 43:54.900]  сделал так, чтобы у меня появилась гессиан в минус первой, вот, это в некотором смысле то
[43:54.900 --> 44:01.260]  свойство гессиана, которым он обладает, если мы пишем какую-то связь этого гессиана и, например,
[44:01.260 --> 44:07.340]  градиентов, вот, ну, и смотрите, соответственно, из этого уравнения я могу что сказать, ну, давайте,
[44:07.340 --> 44:13.620]  пусть у нас будет, что вот эта матрица hк, которая у нас как бы будет заменять гессиан, и мы от нее
[44:13.620 --> 44:25.060]  потребуем, чтобы было выполнено вот это свойство, потребуем это свойство, почему нет, потребовали,
[44:25.060 --> 44:35.580]  вот, а какие еще свойства гессианы есть довольно простенькие, например, симметричность, тоже можно
[44:35.580 --> 44:42.540]  потребовать симметричность, вот, ну, окей, какие-то есть два уравнения, соответственно, я их тут еще и
[44:42.540 --> 44:51.920]  выписываю, сейчас выпишу их, вот, вот, оно, здесь вводится нотация sк, это разница х, вот, ук, это
[44:51.920 --> 44:56.620]  разница градиентов, ну, это классическая нотация для этого класса методов, который мы сейчас будем
[44:56.620 --> 45:01.020]  рассматривать, поэтому, ну, она для удобства рассматривается просто потому, что сейчас выражения будут
[45:01.020 --> 45:07.220]  довольно длинные и громоздкие, поэтому, чуть-чуть их уменьшаем вот таким образом, за счет sк и yк,
[45:07.220 --> 45:13.980]  у это разница градиентов, с разница х, записали уравнение, которое, как бы, вот, хотим, чтобы оно
[45:13.980 --> 45:20.940]  для нашего hкт, которое мы будем считать, должно выполняться, вот, хорошо, ну, вот это вот это
[45:20.940 --> 45:25.700]  уравнение называется квази-нютоновским уравнением, вот, плюс, соответственно, еще добавляем
[45:25.700 --> 45:34.060]  симметричность, окей, все хорошо, вроде как, скт можем вычислить, yкт можем вычислить,
[45:34.060 --> 45:41.420]  плюс у нас есть ограничение, что матрица симметрична, получилось вот такое вот уравнение
[45:41.420 --> 45:50.340]  на матрицу hкт, плюс один, решается, нет, и сколько решений вообще имеет,
[45:50.340 --> 46:01.340]  чтобы прям симметрично, можно даже бессимметрично пока сказать, про это забудем, сказать,
[46:01.340 --> 46:14.100]  ну, что решается, вот это, или нет, ну, что там нажимаем, на yкт транспонирован,
[46:14.100 --> 46:20.940]  наделим на что, на норму, что получится, что получится,
[46:20.940 --> 46:35.500]  а что мне оттуда как найти, это будет h, ну, хорошо, вариант, а вообще, в принципе, это, я думаю,
[46:35.500 --> 46:45.140]  рабочий вариант, так, смотрите, сколько уравнений в этой системе, ну, не k, размеры задачи сколько,
[46:45.140 --> 46:53.100]  den, ну, пусть размеры задачи d, поэтому у нас в этой системе d уравнений, а неизвестных сколько,
[46:53.100 --> 47:02.820]  ну, если там транспонированность не добавлять, то d в квадрате, вот, если транспонированность,
[47:02.820 --> 47:07.940]  там нужно главную диагональ и одну оставить, верхнюю, например, часть матрицы, а вторая,
[47:07.940 --> 47:13.620]  там она как бы достраивается, но в любом случае, у вас тут о, d в квадрате неизвестных, да,
[47:13.620 --> 47:22.060]  получается, что это уравнение, в принципе, имеет бесконечное число решений, вот, и как, соответственно,
[47:22.060 --> 47:31.380]  подбирать теперь что-то, непонятно, нужно что-то требовать еще, нужно что-то требовать еще,
[47:31.380 --> 47:41.500]  кроме квазинютовского уравнения и симметричности, вот, давайте первая идея, потребуем, ну, вот,
[47:41.500 --> 47:46.780]  как в прошлый раз, довольно дешевый апдейт, то есть, пусть у нас hкаты обновляется
[47:46.780 --> 47:53.060]  атеративно, причем я прошу, чтобы она обновлялась довольно дешево, то есть, смотрите, вот,
[47:53.060 --> 47:58.460]  чтобы посчитать вот это, где пока mu — это какой-то скаляр, q — это какой-то вектор,
[47:58.460 --> 48:03.420]  пока неизвестный нам, вот, чтобы обновить h, нам сколько нужно, например, на рифматических
[48:03.420 --> 48:10.140]  операций, чтобы два вектора перемножить, но внешним образом, то есть, получится матрица,
[48:10.140 --> 48:16.940]  матрица получится, d квадрат, d квадрат, то есть, здесь получится o от d в квадрате, то есть,
[48:16.940 --> 48:22.420]  каждое обновление матрицы h будет занимать o от d в квадрате, вот, теперь наша задача понять,
[48:22.420 --> 48:30.740]  какими мы вообще можем взять вот эти значения qкаты и muкаты, ну, для этого, в этом нам поможет,
[48:30.740 --> 48:41.060]  как ни странно, квазинюттонское уравнение, давайте домножим на y, вот, из квазинюттонского
[48:41.060 --> 48:49.100]  уравнения мы знаем, что вот это, соответственно, у нас skt, вот, с другой стороны, мы можем подставить
[48:49.100 --> 49:03.860]  выражение для hкт, hкт плюс 1 это hkt ykt плюс muqk qk транспонированное yk, вот, а заметим,
[49:03.860 --> 49:11.340]  что вот это что, что с размерностью у этого безобразия, это скаляр, это просто число,
[49:11.340 --> 49:22.860]  поэтому, смотрите, что сделаю, я вот запишу вот так вот, muк, этот скаляр сюда впишу, транспонировано
[49:22.860 --> 49:35.260]  yк, так, и здесь останется qк, то есть, вот, вот это у меня скаляр, скаляр, и остался вектор, вот,
[49:35.260 --> 49:44.460]  этот вектор в свою очередь равен skt минус hкт yкт, вот, вектор, вектор, и слева вектор, и справа вектор,
[49:44.460 --> 49:52.900]  что можем сказать про них, если они отличаются по факту только на множители, они коллиниарны,
[49:52.900 --> 50:01.860]  получается вектор q и вектор sk минус hk yk, они коллиниарны, вот, поэтому, в силу того,
[50:01.860 --> 50:08.820]  что всё равно с помощью mu и всё равно дополнительной q, я могу в некотором смысле нормировать мой вектор q,
[50:08.820 --> 50:14.100]  как бы тут в знаменателе просто вектора, в числителе он как бы вот у меня возникает,
[50:14.100 --> 50:22.620]  поэтому давайте я просто положу qкт равна skt минус hкт yкт, вот, то есть, получается,
[50:22.620 --> 50:27.460]  что вот этот коэффициент, который здесь, он равен 1, вот, отсюда сразу же выражается mu,
[50:27.460 --> 50:34.060]  это qкт и транспонированная yкт, всё, мы нашли q, как обновлять q и, соответственно,
[50:34.060 --> 50:43.340]  как считать mu, вот, получается метод, вот, квази-нютоновский метод, первый квази-нютоновский
[50:43.340 --> 50:49.460]  метод, так называемый одноранговый, одноранговый update, просто потому, что вот мы добавляем кусочек,
[50:49.460 --> 50:53.060]  у этой матрицы будет rank 1, потому что это просто комбинация из одного и того же вектора,
[50:53.060 --> 50:58.220]  там на свои же коэффициенты, вот, так называемый одноранговый update, квази-нютоновский метод,
[50:58.220 --> 51:09.140]  он же метод Бройдена, метод Бройдена, всё, здесь он записан, вот, всё записано,
[51:09.140 --> 51:15.580]  меня уже в явном виде, как матрица h обновляется, вот, мы поняли, что довольно дешево это получается
[51:15.580 --> 51:24.340]  такой одноранговый update стоимостью от d в квадрате, вот, но, опять же, как вы понимаете, в силу того,
[51:24.340 --> 51:29.420]  что система, которая из квази-нютоновских уравнений, которые мы задали, даже с симметричной матрицей,
[51:29.420 --> 51:34.460]  имеет множество решений, вот, и это в некотором смысле простор для творчества, в том числе для
[51:34.460 --> 51:42.460]  исследователей сейчас, вот, но в некотором смысле есть такой прям железобетонный квази-нютоновский
[51:42.460 --> 51:49.540]  метод, который пока работает лучше всего, вот, и не придумали, как его можно улучшить, в общем,
[51:49.540 --> 51:57.180]  в случае в некотором смысле, прям хорош на практике, вот, и в теории тоже, вот, смотрите, та идея,
[51:57.180 --> 52:03.100]  которая почему-то изложена в статье про него и в книгах, когда про него рассказывают, излагают вот
[52:03.100 --> 52:10.060]  эту идею, что давайте-ка мы будем смотреть на задачу следующим образом, вот, поиска матрицы
[52:10.060 --> 52:15.900]  h-катая плюс 1 в следующем вопрос, пусть это будет в некотором смысле задача минимизации с
[52:15.900 --> 52:21.020]  ограничениями, ограничение это квази-нютоновского уравнения и симметричность, плюс я потребую,
[52:21.020 --> 52:28.980]  чтобы у меня матрица h, которую я ищу, была близка к моей матрице h-катой в некоторой норме, причем
[52:28.980 --> 52:34.820]  норма здесь может быть любая, то есть, как бы в зависимости того, какую вы норму поставите, у вас
[52:34.820 --> 52:39.780]  будет новый квази-нютоновский метод и, соответственно, решение вот это r-g минимум будет меняться. Для каких-то,
[52:39.780 --> 52:46.020]  понятно, норм решения будет в явном виде, для каких-то будет не в явном виде, ну, вот, оказывается,
[52:46.020 --> 52:54.140]  что если рассмотреть норму, вот, вот здесь норма, это норма пробеньюся, причем необычная, а взвешенная,
[52:54.140 --> 53:01.060]  где веса должны удовлетворять вот такому вот уравнению, то вот можно получить, что решение вот
[53:01.060 --> 53:07.700]  этой задачи минимизации, это вот это. Я, если честно, вот, когда на это смотрю, ну, я не понимаю,
[53:07.700 --> 53:14.620]  как это берется, откуда это берется, вот. Сейчас посмотрим, вот, не знаю, вот, в книжках рассказывают,
[53:14.620 --> 53:20.220]  что это вот так, вот такая интуиция, если люди реально дошли, то вот четыре автора как раз BFGS,
[53:20.220 --> 53:26.220]  это четыре мужика, четыре фамилии, по фамилии метод называется BFGS, вот, как, если они вот реально
[53:26.220 --> 53:32.220]  вот оттуда дошли вот до сюда реально через матрицу взвешенную, ну, респект, вот, потому что мне не
[53:32.220 --> 53:37.020]  интуитивно, как вот это прямо придумать еще, чтобы взять матрицу такую хитрую, видимо, они искали
[53:37.020 --> 53:43.220]  в явном виде решение, чтобы выписывалось, вот, и я оказался такой матрицей, но вообще, вообще,
[53:43.220 --> 53:50.260]  я думаю, что можно объяснить и вот так, вот. Смотрите, на самом деле, квази-ньютоновское уравнение
[53:50.260 --> 53:54.980]  можно записать не только для матрицы H, а для некоторого смысла для ее обратной, ну, это как
[53:54.980 --> 54:00.620]  для гессиана и обратного гессиана, то есть, H это у нас как раз уже обратный гессиан, оно от матрицы B,
[54:00.620 --> 54:08.140]  пусть будет что-то типа нашей аппроксимации через прямой гессиан, ну, честный гессиан без обратности,
[54:08.140 --> 54:14.900]  ну, то есть, там нужно просто H у нас стояло здесь, поэтому, если B, то надо ее перенести как раз к S-комп,
[54:14.900 --> 54:21.860]  ну, то есть, просто обратить, вот, как бы гессиан. Получается вот такое вот уравнение, и для него,
[54:21.860 --> 54:27.420]  например, тоже можно записать одноранговый апдейт, ровно также искать в виде miukate, kukate,
[54:27.420 --> 54:34.020]  на kukate транспонированные, вот, и получится вот такое вот уравнение. Выражение это одноранговый
[54:34.020 --> 54:40.060]  апдейт для матрицы, ну, вот, для, соответственно, уже матрицы B. Вот, матрица B сама по себе не особо нам
[54:40.060 --> 54:47.500]  интересна, потому что ее нужно обращать, вот. А другой вопрос, если вы это можете сделать в явном
[54:47.500 --> 54:53.060]  виде, а не обращать численно, то это уже другое, более интересная тема. Вот, смотрите, что я вот хочу
[54:53.060 --> 54:58.220]  про эту матрицу сказать. Видно, что в одноранговом апдейте, смотрите, какие комбинации встречаются,
[54:58.220 --> 55:09.460]  ykate, ykate на ykate транспонированы, и bkate-skate на bkate-skate транспонированы. Ну, именно,
[55:09.460 --> 55:13.020]  с точки зрения матрицы на векторном вычислении, то, что стоит в знаменателе, это просто число,
[55:13.020 --> 55:18.900]  вот. А сверху вот такое вот получается. Я смотрю, что здесь вот возникает, перемножаю вот эти,
[55:18.900 --> 55:23.500]  перемножаю вот эти, возникает вот такое. Там еще будут вот эти, как бы, скалярные произведения,
[55:23.500 --> 55:32.100]  ну, удвоенные, вот. Но, в принципе, это, ну, как бы, кросс-произведения y на bkate-skate. Но, в принципе,
[55:32.100 --> 55:38.860]  вот, как бы, кусочки-то вот такие. Ну, и возникает мысль, давайте я тогда буду в таком виде искать
[55:38.860 --> 55:47.060]  update. Как бы, не одноранговый, а двухранговый. Раз вектор, два вектор. Прогласны? Вот. Ну, почему
[55:47.060 --> 55:54.100]  нет? Давайте попробуем. Как бы, это же в некотором смысле творческий процесс. Вот. ykate-1 и ykate-2
[55:54.100 --> 55:59.060]  подбираются. Опять же, вы просто пользуетесь квази-нютонским уравнением и домножаете на
[55:59.060 --> 56:06.100]  ykate. Вот. Подставляете, там получается вот здесь вот у вас будет коэффициент один, здесь будет
[56:06.100 --> 56:11.500]  другой, его можно спокойно найти. Опять же, ровно так же, как мы делали в sr1. И получится вот такой вот
[56:11.500 --> 56:22.780]  update. Ну, как-то выглядит, опять же, я говорю, bkate, оно не особо нужно, если вы не умеете его обращать.
[56:22.780 --> 56:30.260]  Но, оказывается, вот, bkate в таком виде можно задешево обратить. В явном виде это можно обратить.
[56:30.260 --> 56:37.500]  И вот, вот такое bkate в обратном виде, ну, если его обратить, в явном виде, там есть специальная формула,
[56:37.500 --> 56:43.980]  формула, формула Шермана Мориса Вудбери, вот, для обращения матрицы. Оно дает то, что как раз мы
[56:43.980 --> 56:52.860]  выписывали для bfgs. Вот. Я не знаю, вот мне кажется, вот так bfgs получить легче. То есть, написать для
[56:52.860 --> 56:58.580]  bkato, а потом, ну, вот, формула, конечно, не самая тривиальная. Вот. У меня есть доказательства, как это
[56:58.580 --> 57:04.740]  все переписывается, bkato через hkate, hkate через bkate, вот, как это обращается, вот это выражение
[57:04.740 --> 57:10.900]  получается нужное. Вот. Но мне кажется, вот так интуитивнее. Интуитивнее понять, почему bfgs
[57:10.900 --> 57:15.340]  получается вот таким, чем рассматривать взвешенную матрицу. Возможно, через взвешенную матрицу что-то
[57:15.340 --> 57:20.900]  похоже получается. Там нужно просто правильно порассуждать. Вот. Для меня вот это просто проще.
[57:20.900 --> 57:27.420]  Хорошо. Смотрите, а такой вообще вопрос. Я вот выписал апдейд для bfgs здесь еще раз,
[57:27.420 --> 57:34.020]  а он вообще дешевый или нет? Просто для sr1 мы сказали, что он стоит oad в квадрате. А здесь,
[57:34.020 --> 57:43.260]  как я вижу, матрица, матрица, матрица. Матрица на матрицу на матрицу дорого. Вот. Это же
[57:43.260 --> 57:49.460]  перемножение матрицы, это как раз что? oad в кубе. Что в итоге? Дорого это или дешево? Вот это ладно,
[57:49.460 --> 57:57.780]  это понятно. Это как раз вектор на вектор oad в квадрате. А здесь что? Вот. Когда мы матрицы
[57:57.780 --> 58:07.820]  перемножаем, там дорого или нет? Да в кубе уже плохо, потому что это обращение гисиана. Это
[58:07.820 --> 58:13.140]  хочется избежать, потому что мы как раз шли за тем, чтобы в том числе решать проблемы обращения
[58:13.140 --> 58:20.340]  гисиана. Смотрите на явный вид. Давайте посмотрим, если мы раскроем. Там вылезет ашката сначала и
[58:20.340 --> 58:29.620]  вылезет также вот такие произведения. Например, вот такое. Ну так на него же можно чуть по-другому
[58:29.620 --> 58:40.380]  посмотреть. Так. Вектор на матрицу. oad в квадрате. Дальше снова что получилось? Вектор на вектор.
[58:40.380 --> 58:53.460]  Ну получается oad в квадрате. То есть, если правильно перемножать, не сначала создавать матрицу из вот
[58:53.460 --> 59:00.340]  этого. И здесь получается у вас матрица d в квадрате на d в квадрате. Ее придется перемножать с матрицей
[59:00.340 --> 59:05.660]  d в квадрате на d в квадрате. А сначала вы берете вектор на матрицу, делаете это z o d в квадрате,
[59:05.660 --> 59:10.780]  получаете новый вектор. И два вектора нужно перемножить внешним образом, получаете o d в квадрате
[59:10.780 --> 59:18.820]  снова. Все. То есть, вся идея тут такая. Тут потому что возникают ровно такие же произведения матрицы
[59:18.820 --> 59:25.180]  на вот эту матрицу. Ну и возникает еще вот это. Ну вот это вообще одноранговый, это тоже o d в квадрате.
[59:25.180 --> 59:30.180]  Вот. Вся суть. То есть, главное вот эту формулу чуть немного для себя переписать и сделать ее более
[59:30.180 --> 59:36.460]  выгодной с точки зрения вычислений. Вот. Тогда окажется, что тут действительно o d в квадрате операции.
[59:36.460 --> 59:43.420]  Вот. Когда мы вообще говорим про инициализацию матрицы h, то оказывается можно брать ее просто
[59:43.420 --> 59:50.660]  единичной. Вот. Можно брать ее просто единичной. Работает и так. Вот. И работает хорошо. Есть более
[59:50.660 --> 59:59.700]  какие-то евристики. Брать ее похитрее. Вот. Но это как бы в некотором смысле не дает особо
[59:59.700 --> 01:00:04.700]  каких-то сильных улучшений. На начальных витерациях чуть лучше, но потом все более-менее выравнивается.
[01:00:04.700 --> 01:00:10.700]  Вот. То есть, единичная матрица и так хороший вариант. Просто берете h0 единичной. Дальше уже исходя
[01:00:10.700 --> 01:00:18.780]  из того, что получается, там какие градиенты, какие точки, считаете уже новые апдейты. Вот. Вот такая
[01:00:18.780 --> 01:00:24.500]  соответственная идея квазий ньютоновских методов. Что мы соответственно про них можем сказать в целом?
[01:00:24.500 --> 01:00:31.780]  Не считаем гисиан и не делаем обращение гисиана. То есть, сложности с подсчетом гисиана устранена,
[01:00:31.780 --> 01:00:40.100]  и устранена проблема с тем, что есть арифметическая операция, которая требует от d в кубе вычислений. Вот.
[01:00:40.100 --> 01:00:47.860]  Стало значительно дешевле. Вот. Более того, причем это вот финальный результат вот в этой области,
[01:00:47.860 --> 01:00:53.740]  финальный результат в этой области, это результат последних пяти лет. Антон Родоманов, Юрий Нестеров,
[01:00:53.740 --> 01:00:58.460]  Юрий Евгениевич Нестеров, про сходимость квазий ньютоновских методов. Вот. В оригинальной статье тоже
[01:00:58.460 --> 01:01:04.820]  была сходимость Power of G jest депутат. Что есть сверх Ethereum hoch rankine. Вот. Ну, вот финальный такой и прям
[01:01:04.820 --> 01:01:10.900]  финальная точка в этом вопросе, вот скорее... скорее всего это финальная точка. Redamans
[01:01:10.900 --> 01:01:15.540]  Нестеров глобально – сверх Ethereum hoch rank. То есть в отличие от методов Ньют Bentans, квазий
[01:01:15.540 --> 01:01:22.200]  ньютоновские методы сходятся глобально независимо от точки старта. Вот. Но, сверх
[01:01:22.200 --> 01:01:26.240]  сверхлинейно. Сверхлинейно – это чуть медленнее, чем, как вы помните из оценок,
[01:01:26.240 --> 01:01:33.360]  это медленнее, чем квадратично. Но на практике, на выпуклых, на выпуклых
[01:01:33.360 --> 01:01:37.540]  задачках, все говорим про выпуклые. На самом деле, если мы говорим про
[01:01:37.540 --> 01:01:42.800]  не выпуклые задачи и квазий-нютонские методы, и вообще метод Ньютона, запускать
[01:01:42.800 --> 01:01:46.240]  можно на не выпуклых задачах, в том числе для нейронных сетей. Я иногда вижу такие
[01:01:46.240 --> 01:01:51.640]  статьи, квазий-нютонский метод для нейронных сетей. Есть своя специфика, не для
[01:01:51.640 --> 01:01:56.980]  всего работают, потому что вообще вот эти методы очень хорошо находят точку
[01:01:56.980 --> 01:02:00.000]  минимум. Как вы понимаете, ньютонский метод быстро сваливается в минимум,
[01:02:00.000 --> 01:02:04.480]  причем точно. Квазий-нютонский на самом деле тоже. Чуть медленнее, но на
[01:02:04.480 --> 01:02:07.920]  практике очень близко по скорости к ньютону сваливаются к минимуму за
[01:02:07.920 --> 01:02:12.360]  несколько десятков итераций. И в случае каких-нибудь нейронных сетей,
[01:02:12.360 --> 01:02:17.680]  где у вас этих локальных минимумов много, свалиться в него и потом не вылезти из
[01:02:17.680 --> 01:02:22.920]  него, довольно плохая перспектива. Поэтому вот часто квазий-нютонские методы
[01:02:22.920 --> 01:02:28.680]  тупо застревают в каком-то локальном минимуме для нейронной сети. А нам же
[01:02:28.680 --> 01:02:33.160]  нужен какой-то хороший глобальный минимум, который, да, он где-то там чуть ниже,
[01:02:33.160 --> 01:02:37.200]  эта чашка расположена, но вот он просто для него не доходит до того, что сваливается.
[01:02:37.200 --> 01:02:42.920]  Там нужно как-то дополнительно это исправлять, делать какие-то процедуры, там
[01:02:42.920 --> 01:02:47.440]  выталкивать эту точку, либо брать какую-то стахастику, либо делать старт из
[01:02:47.440 --> 01:02:52.200]  разных точек. В общем, дополнительно геморрой, для выпуклых задач классно все
[01:02:52.200 --> 01:02:57.160]  работа, для не выпуклых там уже сложнее, вот именно для нейросетей вот прям неприятно.
[01:02:57.160 --> 01:03:05.320]  Для некоторых не выпуклых тоже классно. Вот. Смотрите, квазий-нютоновские методы
[01:03:05.320 --> 01:03:09.280]  по факту считают только градиенты, да, и для них доказана сверхлинейная
[01:03:09.280 --> 01:03:13.600]  сходимость, а для метода Нестерова она только линейная. Мы с вами что-то говорили,
[01:03:13.600 --> 01:03:19.760]  что метод Нестера вроде как оптимален среди методов, которые считают градиент.
[01:03:19.760 --> 01:03:34.600]  В чем я вас обманул? Там или сейчас? Да, смотрите, здесь, когда мы вычисляли нижние оценки для
[01:03:34.600 --> 01:03:41.000]  класса методов, которых Нестеров оптимален, это соответственно вы можете брать только линейную
[01:03:41.000 --> 01:03:46.640]  комбинацию источек и градиентов. Здесь же, когда вы работаете с квазий-нютоновским методом,
[01:03:46.640 --> 01:03:53.760]  вы берете произведения векторов, в том числе как скалярные произведения, так и внешние
[01:03:53.760 --> 01:03:59.800]  произведения, вот, то есть из матрицы в них превращаете. Такие операции были запрещены в том
[01:03:59.800 --> 01:04:04.440]  классе задач, в которых Нестеров оптимален, и поэтому, соответственно, квазий-нютоновские методы,
[01:04:04.440 --> 01:04:16.520]  они его могут обогнать. Могут обогнать, потому что класс методов, в которых, соответственно,
[01:04:16.520 --> 01:04:20.880]  Нестеров оптимален, квазий-нютоновские методы просто туда не входят. Квазий-нютоновские методы
[01:04:20.880 --> 01:04:31.400]  просто туда не входят. Так, неразрешенные векторные произведения, соответственно, метод Ньютона очень
[01:04:31.400 --> 01:04:36.440]  хорош, как метод именно дорешивателя, то есть доползли до окрестности решения, потом взяли пару
[01:04:36.440 --> 01:04:42.160]  итераций Ньютона, запустили и упали там очень хорошо глубоко. Квазий-нютоновские методы тоже
[01:04:42.160 --> 01:04:48.440]  хороши, как дорешиватели, потому что все равно стоимость итераций отличается от метода градиентного
[01:04:48.440 --> 01:04:54.160]  спуска просто хотя бы из-за арифметических операций. Но при этом его можно использовать как бы
[01:04:54.160 --> 01:05:00.240]  просто для старта, сразу как стартовый метод, потому что есть глобальная сходимость. А с какими-то
[01:05:00.240 --> 01:05:04.480]  дополнительными техниками квазий-нютоновских методов, например, как ограниченная память,
[01:05:04.480 --> 01:05:09.920]  потому что вот, например, в тех итерациях, которых я писал, у вас h, она обновлялась
[01:05:09.920 --> 01:05:17.360]  постоянно итеративно, используя предыдущую h. Ну h себе копит в некотором смысле старые какие-то
[01:05:17.360 --> 01:05:22.960]  разность градиентов, старую разность точек, а это старые свойства функций. Вы могли уйти
[01:05:22.960 --> 01:05:28.720]  далеко от этих точек, поэтому тянуть эти свойства функции не имеет смысла. Не имеет смысла
[01:05:28.720 --> 01:05:37.320]  просто потому что это старые какие-то свойства и, возможно, текущие локальные свойства градиента,
[01:05:37.320 --> 01:05:43.040]  в том числе и выпуклости какой-то, квадратичности, они отличаются. Поэтому стоит опираться только на
[01:05:43.040 --> 01:05:49.480]  что-то близкое. Поэтому есть методы квазий-нютоновские, которые так называемые с ограниченной памятью,
[01:05:49.480 --> 01:05:56.120]  с ограниченной памятью. И они в некотором смысле уничтожают все, что было до этого. Плюс этих методов
[01:05:56.120 --> 01:06:03.920]  составляют в том, что они еще и не требуют хранения матрицы. Потому что вот здесь вот до этого
[01:06:03.920 --> 01:06:09.680]  всего безобразия нам нужно было хранить матрицу h и каждый раз все пересчитывали. Вот, когда вы в
[01:06:09.680 --> 01:06:14.280]  некотором смысле храните какой-то набор векторов, из которых матрицу h можно восстанавливать,
[01:06:14.280 --> 01:06:19.320]  это может быть дешевле. Это может быть дешевле. Ну и вот, соответственно, в связи с этим появилась,
[01:06:19.320 --> 01:06:25.520]  так скажем, l limited версия BFGS метода, с которой вам и нужно разобраться в домашнем задании.
[01:06:25.520 --> 01:06:36.480]  Хорошо. Смотрите, теперь обсудим такой вопрос. Я решил это тоже рассказать здесь. Возможно,
[01:06:36.480 --> 01:06:41.320]  исторически это возникло чуть-чуть из других идей, но с этим это тоже ложится. На самом деле,
[01:06:41.320 --> 01:06:46.600]  не только квазий-нютоновский вариант подходит на тот случай, когда мы хотим использовать какую-то
[01:06:46.600 --> 01:06:56.000]  матрицу A вместо матрицы гесса в методе ньютона. Можно, например, там брать какую-то константную
[01:06:56.000 --> 01:07:01.360]  матрицу. Это называется preconditioning, часто используется. В чем там может помочь,
[01:07:01.360 --> 01:07:06.600]  но каких-то глобальных улучшений, таких именно теоретических не дает. На практике может чуть-чуть
[01:07:06.600 --> 01:07:15.080]  помочь. Но есть более интересные варианты, чем можно заменить гессианы. Можно заменить,
[01:07:15.080 --> 01:07:21.320]  например, вот такого-то проксимации. В чем ее суть? Смотрите, мы берем градиент. У нас
[01:07:21.320 --> 01:07:27.640]  как бы вот тут вот вылезает какое-то произведение. Градиент на вектор, а потом мы покомпонентно
[01:07:27.640 --> 01:07:33.560]  домножаем его на этот же вектор. А скажите мне, насколько дорого вообще будет посчитать, а тут
[01:07:33.560 --> 01:07:39.120]  гессиан, я извиняюсь, тут ошибка, тут опечатка. Насколько дорого, как вы думаете, будет посчитать
[01:07:39.120 --> 01:07:46.800]  вот этот гессиан на вектор? Кажется, что это просто подсчет гессиана, умножишь на вектор, но это
[01:07:46.800 --> 01:07:56.280]  дорого значит. Если гессиан дорогой, для нейронных сетей невозможно посчитать дешево. Но можно что
[01:07:56.280 --> 01:08:06.840]  сделать? Какие есть идеи? Нет. Гессиан на вектор это же что-то хорошее, смотрите. Вот смотрите,
[01:08:06.840 --> 01:08:13.320]  у меня есть градиент. Я посчитал просто градиент. А если я градиент умножу на вектор скалярно,
[01:08:13.320 --> 01:08:20.880]  это что будет? Число. Если снова посчитать теперь градиент от вот этого числа, это же тоже
[01:08:20.880 --> 01:08:26.040]  эквивалентно просто взять градиент от скалярной функции. Это уже не гессиан. То есть вот посчитать
[01:08:26.040 --> 01:08:35.360]  вот это на самом деле это два раза взять градиент. Просто второй градиент он хитрый. Ну и за это,
[01:08:35.360 --> 01:08:38.960]  конечно, вы платите тем, что это на самом деле не гессиан, а гессиан умножить на какой-то
[01:08:38.960 --> 01:08:46.040]  вектор, на который вы домножили градиент в свое время. Кстати, вот такая функциональ реализован
[01:08:46.040 --> 01:08:51.400]  в петерче. То есть вы можете реально вот это посчитать почти за бесплатно. То есть граф этих
[01:08:51.400 --> 01:08:58.640]  вычислений сохраняет, вы можете домножить на вектор и прогнать еще раз. А зачем домножение на
[01:08:58.640 --> 01:09:04.800]  УК здесь? Ну давайте разберемся. То есть я тут в принципе рассказываю, что можно сделать. На УК тут
[01:09:04.800 --> 01:09:10.240]  по компонентно, потому что вот это вектор, это тоже вектор. Вы получаете вот здесь вот вектор и вы
[01:09:10.240 --> 01:09:16.400]  его выстраиваете в диагональ матрицы. То есть у вас получается диагональная матрица, где нули. А по
[01:09:16.400 --> 01:09:24.720]  центру будут какие-то числа. Вопрос. Что вы можете сказать, например, про мотожидание вот этой
[01:09:24.720 --> 01:09:31.360]  диагональной матрицы? Вот. Если у нас компоненты вектора УК генерируются как минус один и один,
[01:09:31.360 --> 01:09:43.440]  с вероятностью одна вторая. Независимо. Что там будет? Вот вы гессиан домножаете на вектор, у вас
[01:09:43.440 --> 01:09:50.680]  получается первая, там диагональная компонента первая умножается на У1, дальше вторая компонент
[01:09:50.680 --> 01:09:57.640]  на У2, дальше У3 и так далее. Когда вы домножите на УК еще раз по компонентно, что у вас будет стоять
[01:09:57.640 --> 01:10:06.360]  там в векторе итогового? У вас будет стоять компонента гессиана первая на У1 в квадрате,
[01:10:06.360 --> 01:10:17.480]  дальше компоненты гессиана в первой строке второго столбца на У1, У2 и так далее. Согласны? Вот. Что
[01:10:17.480 --> 01:10:29.880]  вы можете сказать про мотожидание таких вещей? Вот. Почему ноль? У1 в квадрате это сколько? Это
[01:10:29.880 --> 01:10:36.080]  всегда один, вот это всегда один, а вот это по мотожиданию сколько? Вот это уже ноль. Получается,
[01:10:36.080 --> 01:10:46.440]  что по мотожиданию в декатам будут всегда стоять диагональные элементы гессиан. Ну вот это лучше,
[01:10:46.440 --> 01:10:52.840]  чем можно добиться, просто получить диагональные элементы гессиана. Вот из такого вот. И это прямо
[01:10:52.840 --> 01:10:58.440]  по мотожиданию. При этом вы не знаете, что происходит в реальности, потому что это в среднем хорошо все,
[01:10:58.440 --> 01:11:06.440]  а так у вас происходят разбросы. Поэтому когда работают вот с этим, тут я еще модуль забыл,
[01:11:06.440 --> 01:11:14.560]  модуль, вот, модуль, то вот предлагается вот такая вот схема, вот такая вот схема, в некотором смысле
[01:11:14.560 --> 01:11:22.240]  итеративно обновлять быкаты тоже, вот, и добавлять это в декаты. Вот. Это помогает чем? Это помогает
[01:11:22.240 --> 01:11:26.920]  бороться со стахастикой, потому что когда вы предполагаете, что, например, ну пусть у меня гессиан
[01:11:26.920 --> 01:11:31.760]  меняется не сильно, поэтому когда я вот так вот суммирую декаты и в этом в быкатах их
[01:11:31.760 --> 01:11:37.880]  аккумулирую, то, соответственно, кажется, что вот это сильные разбросы из-за того, что у меня вот
[01:11:37.880 --> 01:11:43.800]  этот укат как-то генерировался случайно, они в некотором смысле утихают, утихают и становится
[01:11:43.800 --> 01:11:49.560]  легче. Но есть и обратный эффект, который тоже полезен, вот, в силу того, что опять же у меня
[01:11:49.560 --> 01:11:57.840]  локальные свойства гессиана меняются, меняются, вот, я не хочу особо помнить, что у меня происходило
[01:11:57.840 --> 01:12:03.840]  там миллионы терраций назад, сто тысяч терраций назад, вот, и в связи с тем, что вы здесь домножаете
[01:12:03.840 --> 01:12:10.160]  на какой-то коэффициент, который меньше единицы, у вас старое все схлопывается, старое схлопывается,
[01:12:10.160 --> 01:12:17.440]  вот. Получается такой хороший эффект. С одной стороны, боремся с астахастикой и как бы не в
[01:12:17.440 --> 01:12:22.840]  тупую используем просто диагональ гессиана, вот, с другой стороны, как бы и предысторию вовремя
[01:12:22.840 --> 01:12:32.120]  успеваем вычищать, вот. Окей, есть еще идея, соответственно, так называемые суперпопулярные
[01:12:32.120 --> 01:12:39.960]  методы RMSProp и метод Adam, вот. Там тоже используется как раз матрица диагональная, которая обращается
[01:12:39.960 --> 01:12:51.480]  вот. Вот, соответственно, только я тут чуть-чуть накосиполил, вот так вот надо, квадраты, вот.
[01:12:51.480 --> 01:12:58.320]  Квадраты тут нужны. Смотрите, RMSProp был придуман, тут важно, что квадраты, потому что у вас тут
[01:12:58.320 --> 01:13:05.600]  градиенты в квадрате, вот. То есть, тут берется просто вектор градиентов, вектор градиентов, вот,
[01:13:05.600 --> 01:13:12.040]  и, опять же, по компоненту они умножаются друг на друга, вот, и выставляются на диагональ, вот,
[01:13:12.040 --> 01:13:19.080]  на диагональ. Вопрос, а что вообще могут сказать хорошего градиента? Что вообще могут сказать хорошего
[01:13:19.080 --> 01:13:23.960]  градиента? Ну, в случае, например, когда у нас квадратичная задача диагональная, у нас там,
[01:13:23.960 --> 01:13:30.920]  например, что-то в духе x1 в квадрате плюс 100x2 в квадрате. Стартуем, например, с точки 1,1, да,
[01:13:30.920 --> 01:13:36.400]  тогда, в принципе, градиенты нам действительно много скажут о том, как выглядит, как бы,
[01:13:36.400 --> 01:13:42.920]  гессиан задач. То есть, тут будет, что получается, 2 и 200, да, в градиентах. Вы когда их там просуммируете,
[01:13:42.920 --> 01:13:47.720]  ой, вы там перемножите, у вас получится как раз, условно, что-то хорошее. Вот, возьмете корень,
[01:13:47.720 --> 01:13:55.080]  у вас это реально будет равно гессиану, вот, поэтому это может работать, вот. Почему это работает в
[01:13:55.080 --> 01:14:01.080]  реальности, мало, ну, никто не знает. То есть, доказательства сходимости RMS-пропы и Адама,
[01:14:01.080 --> 01:14:07.040]  чтобы они побили, например, сходимость градиентного спуска, такого нет. Вот, такого нет. То есть,
[01:14:07.040 --> 01:14:12.000]  почему вот это все работает, до конца непонятно. С квазий Ньютона все понятно, там доказано это все.
[01:14:12.000 --> 01:14:18.480]  Здесь ничего не доказано, просто какие-то еврестики. Эти подходы, они проще, чем квазий Ньютоновский,
[01:14:18.480 --> 01:14:23.720]  видно же. Просто нужно там градиенты перемножить по компонентно, вот, и как-то сложить с предыдущей
[01:14:23.720 --> 01:14:31.840]  матрицей. Хорошо, не проблема, вот. Но при этом это хорошо работает для тех же неровных сетей, вот,
[01:14:31.840 --> 01:14:37.000]  и там нет такой проблемы, что вы застреваете в локальных минимумах, вот. Ну, просто потому,
[01:14:37.000 --> 01:14:43.080]  что метод стал проще, он не так быстро сходится, вот, и получается. Но при этом вот именно вот эта
[01:14:43.080 --> 01:14:49.600]  структура, что вы домножаете на эту матрицу вот здесь, вот, она помогает, она помогает, метод
[01:14:49.600 --> 01:14:54.680]  сходится лучше. До конца никто не понимает, почему. Вот, какие-то идеи в духе того, что давайте
[01:14:54.680 --> 01:15:00.680]  рассмотрим квадротичную задачу, посмотрим на ее градиент. В градиенте будут 200, 2 200. Теперь
[01:15:00.680 --> 01:15:08.200]  давайте выстроим это в диагональную матрицу, тут получится 2 200 тоже, матрицу B. Вот, но это
[01:15:08.200 --> 01:15:15.000]  пропорционально там условно гессиану, это реально гессиан, вот. Ну и вот, поэтому хорошо может быть.
[01:15:15.000 --> 01:15:21.280]  А почему это в общем случае хорошо, непонятно. Отличие Адама от RMSProp вообще небольшое, то есть
[01:15:21.280 --> 01:15:28.920]  у RMSProp фиксирован коэффициент, с которым вы как бы эти матрицы складываете. У Адама он меняется
[01:15:28.920 --> 01:15:34.280]  в зависимости от номера террации, стремится он тоже к бета какому-то, вот. На начальных террациях
[01:15:34.280 --> 01:15:40.280]  он ведет себя чуть по-другому, вот, и вы в некотором смысле меньше доверяете вот этим начальным
[01:15:40.280 --> 01:15:45.880]  аппроксимациям гессиану. То, что вы используете в качестве вот этой матрицы предобрабочика,
[01:15:45.880 --> 01:15:53.120]  или вот то, что методы Ньюта вместо гессиана. Вот. Вот такая вот идея. Возможно, то есть изначально
[01:15:53.120 --> 01:15:57.760]  авторы, наверное, вот Адама и RMSProp они мотивировались чуть другим вопросом, ну то есть чуть другой
[01:15:57.760 --> 01:16:02.760]  техникой. Но мы когда будем просто хастику разговаривать, тоже я и с той стороны к этому всему
[01:16:02.760 --> 01:16:08.040]  вернусь, потому что методы суперпопулярны, там статья Адама это уже десятки тысяч цитирований,
[01:16:08.040 --> 01:16:13.640]  хотя в статье содержится теоретическая ошибка, сходимость там неверная, вот. И на самом деле шаг
[01:16:13.640 --> 01:16:21.480]  от Адама, от RMSProp до Адама он совсем небольшой, поменять только коэффициент БКТ. Но статья
[01:16:21.480 --> 01:16:29.840]  суперпопулярна и метод является одним из ключевых методов решения обучения нейронных сетей. Вот.
[01:16:29.840 --> 01:16:35.480]  Соответственно, как-то так, вот. Опять же про исчерпывающую теорию Адама никто ничего не
[01:16:35.480 --> 01:16:40.760]  знает. Вот. Какие-то предположения есть, но они успешно через несколько лет опровергаются
[01:16:40.760 --> 01:16:46.560]  оптимизационным комьюнити уже другими ребятами. Всё. Всё, спасибо. Спасибо, на сегодня всё.

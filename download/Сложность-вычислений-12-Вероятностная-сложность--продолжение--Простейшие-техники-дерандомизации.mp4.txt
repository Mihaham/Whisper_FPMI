[00:00.000 --> 00:11.280]  Сегодня я хочу продолжить рассказ про вероянстое вычисление. Надеюсь, я немножко успею поговорить
[00:11.280 --> 00:16.480]  про дарандомизацию, то есть как вообще избавиться от вероятностей, по крайней мере в некоторых случаях.
[00:16.480 --> 00:24.640]  Что про вероятность надо сказать? Во-первых, нужно поговорить про амплификацию.
[00:24.640 --> 00:35.000]  Амплификация или уменьшение ошибки.
[00:45.000 --> 00:51.640]  Проще всего поговорить для класса RP. Я напомню класс RP с односторонней ошибкой.
[00:51.640 --> 01:13.640]  Это означает следующее, что существует пальномиальный алгоритм такой, что если х лежит в А,
[01:13.640 --> 01:26.920]  то тогда вероятность того, что этот алгоритм, скажем, м от хр, выдаст единицу,
[01:26.920 --> 01:44.920]  значит, эта вероятность больше либо равна 1 и 2, значит, если х не лежит в А, то тогда вероятность того, что м от хр
[01:44.920 --> 02:02.920]  равно 1, значит, это будет равно нулю. Значит, вот это односторонняя ошибка. То есть если сказали да, то точно ответ да.
[02:02.920 --> 02:12.920]  Если сказали нет, то непонятно. Может быть, на самом деле нет, а может быть, так ошибка произошла.
[02:12.920 --> 02:39.920]  Так вот утверждение следующее, что вместо 1 и 2 можно поставить любой порог, вместо 1 и 2 можно поставить любую границу на некоторых диапазонах.
[02:39.920 --> 03:02.920]  Можно поставить любую границу от единицы делить на n в степени c, где n это длина х.
[03:03.920 --> 03:18.920]  Значит, до 1 минус, а тут будет 1 делить на 2 в степени n, степени d.
[03:18.920 --> 03:30.920]  Вот, то есть вот этот разрыв может быть от обратного полинома до 1 минус обратного экспонента.
[03:30.920 --> 03:42.920]  Ну, значит, сильнее расширить уже нельзя, значит, если здесь мы доведем до единицы, то фактически можно любой r взять,
[03:42.920 --> 03:50.920]  если я его подставить, то это будет p, а если мы здесь сузим и сделаем, скажем, обратную экспоненту, то будет np.
[03:50.920 --> 04:02.920]  Если тут будет 1 длить на 2 в степени длина сертификата, в степени длины r, то это будет в точности np.
[04:02.920 --> 04:10.920]  Но в этом диапазоне все будет одно и то же. Здесь в том числе будут любые константы между 0 и единицей.
[04:10.920 --> 04:18.920]  Так, значит, как это делается? Ну, нужно повторять много раз доказательства.
[04:18.920 --> 04:46.920]  Можно повторять много раз и брать дизьюнцию ответов.
[04:46.920 --> 05:02.920]  Значит, смотрите, если на самом деле ответ нет, то это будет дизьюнция нулей.
[05:02.920 --> 05:28.920]  Если х не принадлежит А, то будет дизьюнция нулей. Ну, то есть ноль получится.
[05:28.920 --> 05:38.920]  Ну, в смысле вероятность единицы будет нулем.
[05:38.920 --> 05:57.920]  Вот, если х лежит в А, то при к независимых запусках...
[05:57.920 --> 06:13.920]  Значит, вероятность нуля будет 1 минус епсилон в катой степени.
[06:13.920 --> 06:17.920]  Ну, диапазон как раз то, что мы вместо 1 и 2 подставляем.
[06:17.920 --> 06:21.920]  Тут, чтобы дизьюнция была нулем, нужно, чтобы каждый раз был ноль.
[06:21.920 --> 06:27.920]  Соответственно, вероятность единицы больше либо рано, чем вот эта епсилон, значит нуля не больше, чем 1 минус епсилон.
[06:27.920 --> 06:31.920]  Ну, тут а тоже не больше, чем 1 минус епсилон.
[06:31.920 --> 06:37.920]  Ну, а теперь вспоминаем матан, первый семестр, пределы.
[06:37.920 --> 06:51.920]  Если епсилон это 1 делить на n степени c, а k это n степени c плюс d, то тогда как раз и получится.
[06:51.920 --> 07:09.920]  Тогда как раз вот это 1 минус епсилон в катой степени будет примерно равно, чем e в степени минус n степени d.
[07:09.920 --> 07:21.920]  Ей это больше 2, соответственно, это будет больше, чем 2 в степени минус n в d, а это как раз то, что нам нужно.
[07:21.920 --> 07:37.920]  То есть меньше, наоборот, нам и нужно меньше, чтобы вот эта вероятность нуля была маленькая.
[07:37.920 --> 07:45.920]  Ну вот, так и получается. Так и получается, что вот от такого епсилона пришли к вот такому.
[07:45.920 --> 07:49.920]  Так, ничего, понятно?
[07:49.920 --> 07:57.920]  Это для односторонней ошибки это простое рассуждение, для двусторонней немножко посложнее.
[07:57.920 --> 08:09.920]  Так, давайте я тут напишу, напомню. Двусторонняя ошибка это BPP.
[08:09.920 --> 08:29.920]  То есть тут получается, что если x лежит в A, то тогда вероятность того, что m от xr равно единице будет больше, чем 2 третьих.
[08:29.920 --> 08:49.920]  Вот, соответственно, если x не лежит в A, то тогда вероятность того, что m от xr равно единице тут будет меньше либо равно, чем 1 треть.
[08:49.920 --> 09:03.920]  Вот, ну и вот тут, если тут альфа, а тут бета, то тогда утверждение такое, что класс не поменяется для всех альфа и бета.
[09:04.920 --> 09:10.920]  Класс. Один и тот же.
[09:11.920 --> 09:20.920]  Значит, для всех альфа, бета, таких что?
[09:20.920 --> 09:32.920]  Значит, тут соответственно альфа будет больше, чем 1 делить на 2 в степени D.
[09:32.920 --> 09:46.920]  Значит, бета будет больше, чем альфа плюс 1 делить на 1 в степени C, и бета меньше, чем 1 минус 1 делить на 2 в степени D.
[09:46.920 --> 09:59.920]  Да, то есть как бы по краям между альфа и бета и краями должна быть обратная экспонента, а вот между альфа и бета должен быть обратный полином.
[10:03.920 --> 10:07.920]  Так, ну что, утверждение понятно?
[10:11.920 --> 10:15.920]  Значит, идея доказательства такая.
[10:15.920 --> 10:37.920]  Идея доказательства заключается в том, что мы точно так же запускаем много раз, и если это 1 треть и 2 треть, то голосуем по большинству,
[10:37.920 --> 10:44.920]  а если произвольна альфа и бета, то сравниваем выборочные средние со средним арисметическим.
[10:45.920 --> 11:03.920]  Значит, запускаем K раз и сравниваем долю единиц с альфа плюс бета пополам.
[11:08.920 --> 11:16.920]  Дальше есть некоторая вероятностная выкладка, которую я хотел бы пропустить.
[11:16.920 --> 11:20.920]  Так, а у вас прям червьер был с CPT?
[11:20.920 --> 11:21.920]  Да.
[11:21.920 --> 11:25.920]  А слов типа не раз в Черново вам не говорили или говорили?
[11:25.920 --> 11:26.920]  У нас на семинаре было.
[11:26.920 --> 11:27.920]  О, на семинаре было.
[11:27.920 --> 11:34.920]  Хорошо, сейчас, на семинаре по вероятности или по сложности?
[11:34.920 --> 11:35.920]  По вероятности.
[11:35.920 --> 11:38.920]  По сложности, а то мы общались, что будет использоваться.
[11:38.920 --> 11:40.920]  Будет использоваться, да.
[11:49.920 --> 11:52.920]  Ой, наоборот, альфа и бета наоборот нужно.
[11:55.920 --> 11:58.920]  Альфа нижняя граница, да, а бета верхняя.
[12:06.920 --> 12:07.920]  Вот так.
[12:07.920 --> 12:13.920]  Но, в общем, я не буду прям выкладку делать, я нарисую картинку.
[12:19.920 --> 12:24.920]  Знаете, смотрите, чему нас учит CPT?
[12:24.920 --> 12:31.920]  К тому, что если мы много раз вот такую вот бернуливскую случайную величину независимо запускаем,
[12:31.920 --> 12:37.920]  да, и берем средняя, то это будет колоколообразная функция.
[12:37.920 --> 12:42.920]  Соответственно, есть как бы пороги вот альфа и бета,
[12:42.920 --> 12:48.920]  а еще есть настоящая средняя, которая либо левее альфы, либо правее бета.
[12:48.920 --> 12:53.920]  То есть вот какое-то, вот здесь вот будет какое-то настоящее среднее.
[12:53.920 --> 13:01.920]  Соответственно, тут будет какой-то колокол, который как-то вот так вот будет идти.
[13:02.920 --> 13:05.920]  Ну а вот здесь вот альфа плюс бета пополам.
[13:08.920 --> 13:15.920]  Соответственно, если мы попали вот в этот правый хвост, то тут будет ошибка.
[13:23.920 --> 13:29.920]  И нам нужно доказать, что эта ошибка может быть сделана экспедициально маленькой.
[13:32.920 --> 13:43.920]  Но тут дело в следующем, что ширина этого колокола будет порядка 1 делить на корень из числа повторов.
[13:43.920 --> 13:51.920]  На число повторов у нас k, то есть вот тут будет порядка 1 делить на корень из k.
[13:53.920 --> 14:02.920]  А вот эта вот штука от альфы до альфы плюс бета пополам, она у нас хотя бы 1 длительная степень С.
[14:12.920 --> 14:16.920]  Ну и, соответственно, ошибка, поскольку это нормальное распределение,
[14:16.920 --> 14:23.920]  то ошибка будет экспедициально малой от того, сколько раз ширина этой сигны уложится.
[14:24.920 --> 14:31.920]  Но если она вот настолько экспедициально малой должна быть, то, соответственно, должно порядка n в степени D раз уложиться.
[14:33.920 --> 14:38.920]  Ну как раз получается корень из k типа n в степени С.
[14:46.920 --> 14:56.920]  Так. Ну вообще там какое-то многочлено получается.
[15:05.920 --> 15:08.920]  Наверное должно быть 2C плюс D, что ли.
[15:08.920 --> 15:18.920]  Ну ладно, в общем, короче говоря, вот это k должно быть полиномом от m.
[15:18.920 --> 15:24.920]  А каким именно это как раз получается из точных выкладок с неразрешением Чернова?
[15:24.920 --> 15:27.920]  Да, собственно, почему нам CPT недостаточно?
[15:27.920 --> 15:32.920]  Потому что она именно предельная теорема, а нам нужно конкретное полиномиальное число повторов.
[15:32.920 --> 15:39.920]  И вот поэтому нужно неразрешение Чернова, что именно такого числа повторов хватит, чтобы этот график был достаточно близок,
[15:39.920 --> 15:44.920]  чтобы можно было сделать вывод о том, что ошибка маленькая.
[15:44.920 --> 15:50.920]  Так, ну что, интуиция понятна?
[15:50.920 --> 15:57.920]  Вот, хорошо. Ну, значит, выкладки либо для самостоятельного изучения, либо насчет интуиции.
[15:57.920 --> 16:00.920]  Вот, потому что дальше я хочу успеть поприменять это.
[16:00.920 --> 16:02.920]  Теперь какая идея?
[16:02.920 --> 16:06.920]  Идея, что ошибка может быть сделана экспоненциально маленькой,
[16:06.920 --> 16:12.920]  и в том числе это может быть экспонента больше, чем размер входа.
[16:12.920 --> 16:20.920]  То есть знаменатель в экспоненте не показатель, в смысле.
[16:20.920 --> 16:27.920]  Нет, показатель, да, показатель в знаменателе в экспоненте может быть больше, чем длина входа.
[16:27.920 --> 16:32.920]  Вот, это очень важное соображение.
[16:32.920 --> 16:34.920]  Это очень важное соображение.
[16:34.920 --> 16:39.920]  Сейчас на его основе я расскажу две теоремы,
[16:39.920 --> 16:44.920]  которых, в принципе, можно считать дарандомизацией.
[16:44.920 --> 16:50.920]  То есть как превратить вероятностные алгоритмы во что-то детерминированное,
[16:50.920 --> 16:55.920]  но, правда, не в П, но во что-то похожее.
[16:55.920 --> 17:02.920]  Значит, одна теорема связывает BPP и неравномерную сложность,
[17:02.920 --> 17:07.920]  а вторая теорема связывает BPP и полинарную рафхю.
[17:07.920 --> 17:12.920]  Сейчас, значит, с этим разберемся.
[17:19.920 --> 17:24.920]  Первая называется теорема Эйдлмана.
[17:24.920 --> 17:27.920]  Значит, если вы знаете про криптосистему RSA,
[17:27.920 --> 17:33.920]  то Эйдлман как раз буква A в RSA,
[17:33.920 --> 17:37.920]  он на букву A начинается.
[17:37.920 --> 17:38.920]  Вот.
[17:38.920 --> 17:39.920]  Вот.
[17:39.920 --> 17:40.920]  Вот.
[17:40.920 --> 17:41.920]  Вот.
[17:41.920 --> 17:42.920]  Вот.
[17:42.920 --> 17:43.920]  Вот.
[17:43.920 --> 17:44.920]  Вот.
[17:44.920 --> 17:45.920]  Вот.
[17:45.920 --> 17:46.920]  Вот.
[17:46.920 --> 17:47.920]  Вот.
[17:47.920 --> 17:48.920]  Вот.
[17:49.920 --> 17:53.920]  Он на букву A начинается.
[17:53.920 --> 17:54.920]  Вот.
[17:54.920 --> 17:55.920]  Так.
[17:55.920 --> 18:04.920]  Значит, теорема такая, что BPP вложена в П слеш поле.
[18:04.920 --> 18:11.920]  То есть можно заменить неравномерный вероятностный алгоритм
[18:12.920 --> 18:17.920]  на неравномерный детерминированный.
[18:23.920 --> 18:31.920]  Равномерный вероятностный алгоритм можно заменить на неравномерный детерминированный.
[18:42.920 --> 18:43.920]  Так.
[18:43.920 --> 18:46.920]  Значит, как мы это делаем?
[18:46.920 --> 18:54.920]  Ну, первым делом сделаем ошибку меньше, чем 2 в степени −m.
[18:57.920 --> 18:58.920]  Так.
[18:58.920 --> 19:00.920]  Доказательства.
[19:00.920 --> 19:12.920]  Сделаем вероятность ошибки меньше, чем 1 делить на 2 в степени −m.
[19:12.920 --> 19:17.920]  Значит, здесь n — это длина x.
[19:17.920 --> 19:20.920]  Ну, что значит вероятность ошибки?
[19:20.920 --> 19:22.920]  Ну, значит, дали неправильный ответ.
[19:22.920 --> 19:27.920]  То есть это как бы альфа или 1-бета.
[19:27.920 --> 19:32.920]  Значит, если правильный ответ 0, то альфа будет вероятность дать ответ 1.
[19:32.920 --> 19:39.920]  Если правильный ответ 1, то 1-бета — это будет дать вероятность ответ 0.
[19:41.920 --> 19:42.920]  Вот.
[19:42.920 --> 19:45.920]  Значит, соответственно, ошибку мы сделаем вот такую.
[19:45.920 --> 19:46.920]  Что это значит?
[19:46.920 --> 20:04.920]  Значит, это означает, что существует такое r, что для любого x, значит, если длина x равно n,
[20:04.920 --> 20:07.920]  то тогда этот алгоритт дает правильный ответ.
[20:07.920 --> 20:12.920]  Значит, тогда получается, что v от xr равно 1.
[20:12.920 --> 20:18.920]  Тогда и только тогда, значит, когда там x лежит va.
[20:22.920 --> 20:30.920]  Да, то есть есть один конкретный, случайно, r, который дает правильный ответ ко всем x.
[20:30.920 --> 20:32.920]  Почему?
[20:32.920 --> 20:40.920]  Ну, это простая вероятность наценка, что каждый конкретный x меньше, чем вот столько забраковывает разных r.
[20:40.920 --> 20:44.920]  Всего x как раз столько, соответственно, суммарно, все x меньше единицы.
[20:44.920 --> 20:47.920]  Забракуют, значит, будет не забракованный.
[20:47.920 --> 20:50.920]  Но и вот этот не забракованный мы и возьмем.
[20:53.920 --> 20:54.920]  Так.
[20:54.920 --> 20:57.920]  Ничего, понятно соображение?
[20:57.920 --> 21:02.920]  Вот. Ну а после этого мы пользуемся тем, что это не равномерно.
[21:02.920 --> 21:10.920]  Получается, что v от xr преобразуем в схему.
[21:10.920 --> 21:14.920]  V от xr преобразуется в схему.
[21:14.920 --> 21:23.920]  Это аналогично тому, как было p вложено в p-slash-поле.
[21:27.920 --> 21:28.920]  Вот.
[21:28.920 --> 21:30.920]  Но это будет схема от xr.
[21:30.920 --> 21:34.920]  Дальше r просто можно, как говорят, запаять в схему.
[21:34.920 --> 21:45.920]  А r можно зафиксировать вот таким образом.
[21:45.920 --> 21:50.920]  Значит, соответственно, получится схема от x.
[21:56.920 --> 22:00.920]  Ну и размер будет пальномиален, потому что v пальномиально.
[22:00.920 --> 22:03.920]  Но от фиксации вообще размер не меняется.
[22:04.920 --> 22:05.920]  Вот.
[22:05.920 --> 22:06.920]  Так.
[22:06.920 --> 22:07.920]  Ну чего?
[22:07.920 --> 22:08.920]  Понятно?
[22:08.920 --> 22:21.920]  Так, в общем, не очень сложное рассуждение.
[22:21.920 --> 22:23.920]  Сейчас следующая теремия посложнее будет.
[22:23.920 --> 22:27.920]  Кинуть, может, вопросы.
[22:34.920 --> 22:35.920]  Так.
[22:35.920 --> 22:48.920]  Значит, следующая теорема про вложенность в пальномиальную
[22:48.920 --> 22:49.920]  иерархию.
[22:49.920 --> 22:50.920]  Вот.
[22:50.920 --> 22:54.920]  Это, в общем, тоже некоторое избавление от вероятности,
[22:54.920 --> 22:56.920]  но ценой недотерминизма.
[22:56.920 --> 23:17.920]  Тут, в принципе, даже не умеет доказывать, что
[23:17.920 --> 23:27.920]  p меньше, чем pn exp даже, или что-то даже большее.
[23:27.920 --> 23:35.920]  Хотя верят, что, наверное, bpp равно p, но не умеет отделять
[23:35.920 --> 23:42.920]  от очень больших классов, да, еще больше, чем exp.
[23:42.920 --> 23:43.920]  Вот.
[23:43.920 --> 23:47.920]  Ну, зато если окажется, что bpp равно exp, тогда будет,
[23:47.920 --> 23:48.920]  что p не равно np.
[23:48.920 --> 23:50.920]  Да, это еще один такой способ.
[23:50.920 --> 23:55.920]  Иронический способ, конечно, p не равно np, но, скорее всего,
[23:55.920 --> 23:58.920]  так не получится, просто потому что утверждение.
[23:58.920 --> 24:01.920]  Ну, и утверждение, из которого следует p не равно np, скорее
[24:01.920 --> 24:02.920]  всего, неверно.
[24:02.920 --> 24:03.920]  Так.
[24:03.920 --> 24:04.920]  Хорошо.
[24:04.920 --> 24:09.920]  Давайте напишу теорема.
[24:09.920 --> 24:10.920]  Так.
[24:10.920 --> 24:18.920]  Я ее атрибутирую тремя именами gacha, cipsa, relautemana.
[24:18.920 --> 24:32.920]  Значит, bpp выложено в σ2-пальномиальное, в пересечение
[24:32.920 --> 24:35.920]  p2-пальномиальное.
[24:35.920 --> 24:36.920]  Вот.
[24:36.920 --> 24:43.920]  Там такая была история, cipsa делал доклад на конференции,
[24:43.920 --> 24:48.920]  и он доказал, что bpp вложено в ph на каком-то более высоком
[24:48.920 --> 24:49.920]  уровне.
[24:49.920 --> 24:52.920]  Ну, там, наверное, можно посмотреть, на каком.
[24:52.920 --> 24:53.920]  Вот.
[24:53.920 --> 25:00.920]  А gacha был среди слушателей, и он сказал, что да, это все
[25:00.920 --> 25:03.920]  нормально, но можно упростить и показать, что на втором
[25:03.920 --> 25:04.920]  уровне.
[25:04.920 --> 25:05.920]  Вот.
[25:05.920 --> 25:09.920]  И там что-то cipsa ему предлагал совместную статью, а gacha
[25:09.920 --> 25:11.920]  говорит, да нет, вообще все очевидно.
[25:11.920 --> 25:16.920]  И типа, ну, с другой стороны, это там ваш результат.
[25:16.920 --> 25:21.920]  В общем, в итоге lautemana там как-то тоже все послушал
[25:21.920 --> 25:26.920]  и еще упростил доказательства.
[25:26.920 --> 25:29.920]  В общем, тот конец, который я буду рассказывать, придумал
[25:29.920 --> 25:35.920]  lautemana по мотивам достижения gacha и cipsa.
[25:35.920 --> 25:42.920]  О, это давно, это какие-то...
[25:42.920 --> 25:43.920]  конец 70-х, наверное.
[25:43.920 --> 25:50.920]  Так, нет, ну я могу в книжке посмотреть, там есть ссылка.
[25:50.920 --> 26:10.920]  Ну ладно, давайте будем доказывать.
[26:10.920 --> 26:16.920]  Так, сначала я нарисую метафорическую картинку.
[26:16.920 --> 26:22.920]  Значит, сначала мы, конечно же, произведем амплификацию,
[26:22.920 --> 26:27.920]  то есть у нас будет ошибка маленькая.
[26:27.920 --> 26:32.920]  Соответственно, я буду там рисовать множество, скажем s,
[26:32.920 --> 26:40.920]  это множество таких r, что m от xr равно 1.
[26:40.920 --> 26:46.920]  Соответственно, вот будет две ситуации.
[26:46.920 --> 26:52.920]  Одна ситуация, это если x лежит ва,
[26:52.920 --> 27:00.920]  значит, тогда это множество, оно какое-то вот такое вот большое будет.
[27:00.920 --> 27:03.920]  Почти всего пространства занимает.
[27:03.920 --> 27:07.920]  Квадратик это все r, а вот это множество кривое,
[27:07.920 --> 27:12.920]  это, соответственно, те r, которые, в общем, s.
[27:12.920 --> 27:15.920]  Вот это вот s.
[27:15.920 --> 27:23.920]  Если x лежит ва, значит, если x не лежит ва,
[27:23.920 --> 27:34.920]  то тут, соответственно, это s, это будет какая-то крошечная часть всего пространства.
[27:34.920 --> 27:41.920]  Дальше геометрическая интуиция, про которую мы дальше будем доказывать,
[27:41.920 --> 27:45.920]  что она работает и в нашем пространстве тоже, потому что в нашем пространстве были в куб,
[27:45.920 --> 27:49.920]  а вовсе не в квадрат, как я здесь нарисовал.
[27:49.920 --> 27:54.920]  Тем не менее, я наглядно нарисую, что происходит.
[27:54.920 --> 27:57.920]  Попробую, по крайней мере.
[27:57.920 --> 28:03.920]  Идея такая, что если мы вот это большое множество немножко подвигаем во все стороны,
[28:03.920 --> 28:13.920]  то оно там будет каким-то вот таким вот, вот так вот сдвинется.
[28:13.920 --> 28:19.920]  Этот угол закроет, потом мы его сдвинем сюда,
[28:19.920 --> 28:25.920]  да, но там как-то вот, соответственно,
[28:25.920 --> 28:37.920]  ой, нет, так, это я не то нарисовал, вот так оно должно быть.
[28:37.920 --> 28:40.920]  Этот угол закроет.
[28:40.920 --> 28:47.920]  Ну и, соответственно, здесь тоже, я думаю, что это уже не очень будет видно,
[28:47.920 --> 28:59.920]  но как-то вот так вот я части нарисую, и, соответственно, вот сюда вот.
[28:59.920 --> 29:05.920]  В общем, таким небольшим числом сдвигов этого множества можно прямо все пространство покрыть.
[29:05.920 --> 29:18.920]  А вот эту вот штучку, если вы сдвинете, то она даже если все копии будут не перекрываться,
[29:18.920 --> 29:25.920]  то все равно оно все не покроет, потому что она маленькая.
[29:25.920 --> 29:31.920]  Ну и тогда утверждается, что вот это и будет разделяющее свойство.
[29:31.920 --> 29:36.920]  Значит, существует набор сдвигов такой, что любая точка будет покрыта.
[29:39.920 --> 29:56.920]  Значит, существует набор сдвигов такой, что любая точка покрыта одним из двигов.
[30:01.920 --> 30:09.920]  Ну ладно, не совсем грамотно.
[30:09.920 --> 30:19.920]  Одним из образов, образом при одном из двигов исходного множества, если полностью.
[30:19.920 --> 30:22.920]  Вот так.
[30:22.920 --> 30:28.920]  Ну хорошо, значит, это то, что мы хотим.
[30:28.920 --> 30:38.920]  Ну а теперь нужно доказать, что это действительно можно сделать,
[30:38.920 --> 30:45.920]  что в нашем булевом кубе интуиция из Евклидова квадрата сработает.
[30:45.920 --> 30:50.920]  Вы написали топор сигнала.
[30:50.920 --> 30:52.920]  Это правда, да.
[30:52.920 --> 31:00.920]  Но дело в том, что BPP за счет симметрии между 1 и 3, 2 и 3, 7,
[31:00.920 --> 31:03.920]  BPP будет замкнут относительно дополнения.
[31:03.920 --> 31:08.920]  Так что если мы вложили в sigma2, то тогда его дополнение в ложится в p2,
[31:08.920 --> 31:12.920]  дополнение к BPP, то есть множество, не дополнение, а co-класс,
[31:12.920 --> 31:16.920]  множество дополнений к языкам с BPP, это будет тоже BPP,
[31:16.920 --> 31:23.920]  и с другой стороны ложится в p2.
[31:23.920 --> 31:25.920]  Так, понятно, да?
[31:25.920 --> 31:35.920]  Вот.
[31:35.920 --> 31:45.920]  Так, ну хорошо, значит, как будем доказывать, что в булевом кубе все работает?
[31:45.920 --> 31:54.920]  Так, а слышали ли вы про вероянностный метод в комбинаторике?
[31:54.920 --> 31:57.920]  Дискретно в анализе в первом семестре, да?
[31:57.920 --> 31:59.920]  Или сейчас вы тоже ходите?
[31:59.920 --> 32:04.920]  Мы не ходим.
[32:04.920 --> 32:07.920]  В третьем семестре был, да, очень хорошо.
[32:07.920 --> 32:10.920]  Вообщем, здесь будет как раз он.
[32:10.920 --> 32:26.920]  Вероянностный метод заключается в том, что мы возьмем случайные сдвиги,
[32:26.920 --> 32:41.920]  и докажем, что вероятность отрицания будет меньше единицы.
[32:41.920 --> 32:51.920]  Ну, следовательно, существуют сдвиги, для которых утверждение верно.
[32:51.920 --> 32:57.920]  Так, и нужно еще оценить, чтобы их количество было не очень большим,
[32:57.920 --> 32:59.920]  а инпульны мякоть.
[32:59.920 --> 33:05.920]  Ну, в общем-то, это очень важно.
[33:05.920 --> 33:10.920]  Так, и нужно еще оценить, чтобы их количество было не очень большим,
[33:10.920 --> 33:13.920]  а инпульны мякоть.
[33:13.920 --> 33:17.920]  Ну, а сейчас это само все получится.
[33:17.920 --> 33:24.920]  Так, смотрите, пусть этих сдвигов К штук.
[33:24.920 --> 33:28.920]  Пусть сдвигов К.
[33:28.920 --> 33:34.920]  Значит, тогда чего мы хотим?
[33:34.920 --> 33:49.920]  Мы оценим вероятность того, что существует такой х.
[33:49.920 --> 33:53.920]  Так, давайте их как-нибудь обозначим.
[33:53.920 --> 33:56.920]  Значит, эти сдвиги будут shift.
[33:56.920 --> 34:00.920]  С1, С2 и так далее, СКТ.
[34:00.920 --> 34:08.920]  Это просто случайные вектора такой же длины, как R.
[34:08.920 --> 34:18.920]  Значит, случайные вектора длины R.
[34:18.920 --> 34:23.920]  Ну, давайте закончим рассуждение, и потом сделаем перерыв.
[34:23.920 --> 34:26.920]  Так, что мы хотим?
[34:26.920 --> 34:32.920]  Мы хотим, что какой-то...
[34:32.920 --> 34:38.920]  Так, нет, не х, а R.
[34:38.920 --> 34:44.920]  Существует такой плохой R, что мы его как не сдвинем,
[34:44.920 --> 34:50.920]  потому что все попали вне того множества, которое дает единицу.
[34:50.920 --> 34:53.920]  Значит, существует R.
[34:53.920 --> 35:02.920]  А тут будет, что V от х и R плюс S1 равно 0.
[35:02.920 --> 35:04.920]  И так далее.
[35:04.920 --> 35:11.920]  И V от х и R плюс СКТ.
[35:11.920 --> 35:17.920]  СКТ равно 0.
[35:17.920 --> 35:23.920]  Так, при том, что вообще этих R будет...
[35:23.920 --> 35:26.920]  Ну, почти все.
[35:26.920 --> 35:33.920]  Значит, при этом...
[35:33.920 --> 35:40.920]  Количество таких R, что V от х и R равно единице,
[35:40.920 --> 35:44.920]  поделить на 2 в степени длина R,
[35:44.920 --> 35:51.920]  это у нас будет больше, чем 1 делить на 2 в степене.
[35:51.920 --> 36:01.920]  Наверное, столько хватит, где на длина х.
[36:01.920 --> 36:08.920]  Мы хотим подобрать параметры, чтобы это было меньше единицы.
[36:08.920 --> 36:12.920]  Как мы это будем оценивать?
[36:12.920 --> 36:20.920]  Значит, смотрите.
[36:20.920 --> 36:24.920]  Во-первых, тут квантов существования по R,
[36:24.920 --> 36:27.920]  то есть это объединение событий,
[36:27.920 --> 36:35.920]  это будет меньше либо равно, чем просто количество разных R.
[36:35.920 --> 36:41.920]  Значит, 2 в степени R умножить на вероятность для конкретного R.
[36:41.920 --> 36:46.920]  Значит, это на самом деле одинаковая будет вероятность.
[36:46.920 --> 36:51.920]  Умножить на вероятность того же самого,
[36:51.920 --> 36:53.920]  только без квантов существования.
[36:53.920 --> 36:59.920]  Тут V от х, R плюс S1 равно 0, и так далее.
[36:59.920 --> 37:06.920]  И V от х, R плюс S ка т равно 0.
[37:13.920 --> 37:18.920]  А здесь у меня уже конъюнция независимых событий.
[37:18.920 --> 37:22.920]  Давайте напишем.
[37:22.920 --> 37:26.920]  Они случайные, равномерные, независимые.
[37:26.920 --> 37:32.920]  Соответственно, это уже будет в точности равняться.
[37:32.920 --> 37:47.920]  Тут будет 2 в степени длина R умножить на вероятность для конкретного V от х, R плюс.
[37:47.920 --> 37:55.920]  R плюс S и T равно 0 в ка т степени.
[37:55.920 --> 37:59.920]  Ну а какая вероятность?
[37:59.920 --> 38:03.920]  R тут фиксирована, S и T случайные.
[38:03.920 --> 38:11.920]  Соответственно, нужно попасть в дополнение к этим.
[38:11.920 --> 38:19.920]  Понятно, что если R фиксирована, то R ксс и T тоже равномерно распределено.
[38:19.920 --> 38:23.920]  И нужно попасть не в это множество.
[38:23.920 --> 38:37.920]  То есть это будет у нас меньше, чем 2 в степени длина R умножить на 1 делить на 2 в степени n в ка т степени.
[38:37.920 --> 38:42.920]  И это должно быть меньше единицы.
[38:42.920 --> 38:47.920]  Я напоминаю, это должно быть.
[38:47.920 --> 38:53.920]  Ну, вроде ясно, какой ка нужно взять.
[38:53.920 --> 39:04.920]  Должно быть, что ка на n у нас должно быть больше, чем длина R.
[39:04.920 --> 39:09.920]  Ну, то есть ка больше, чем длина R делить на n.
[39:09.920 --> 39:11.920]  Но это полином.
[39:11.920 --> 39:19.920]  Это полином, потому что длина R сама полином.
[39:19.920 --> 39:23.920]  Ну и в принципе все. На этом все схлопывается.
[39:23.920 --> 39:25.920]  Ка мы выбираем.
[39:25.920 --> 39:28.920]  Получаем, что вероятность меньше единицы.
[39:28.920 --> 39:31.920]  То есть это то, что нам нужно, чтобы было последнее верно.
[39:31.920 --> 39:35.920]  Значит, мы получили вероятность.
[39:35.920 --> 39:43.920]  Значит, мы можем выбрать так, чтобы такого R не существовало.
[39:43.920 --> 39:48.920]  А это означает, что любое R покрыто.
[39:48.920 --> 39:51.920]  И, соответственно, вот эта формула верна.
[39:51.920 --> 39:54.920]  Давайте перепишу теперь в этих обозначениях.
[39:54.920 --> 40:01.920]  Значит, в итоге существует такой набор s1 и т. д. s ка т.
[40:01.920 --> 40:11.920]  Для любого R получается V от X R плюс s1, что равно единице.
[40:11.920 --> 40:21.920]  Или, и т. д., или V от X R сор с ка т. равно единице.
[40:21.920 --> 40:26.920]  Да, ну и последнее, что еще нужно сказать, что если на самом деле ответ нет,
[40:26.920 --> 40:28.920]  то этих сдвигов не хватит.
[40:28.920 --> 40:31.920]  Но это с большим запасом выполнено.
[40:31.920 --> 40:55.920]  Если настоящий ответ нет, то число вот таких R, таких, для которых вот это верно,
[40:55.920 --> 41:15.920]  будет меньше либо равно, чем ка умножить на 1 делить на 2 в степени n.
[41:15.920 --> 41:21.920]  Но это, конечно, если n не слишком маленькая,
[41:21.920 --> 41:26.920]  а k это полином, то это будет меньше единицы.
[41:26.920 --> 41:30.920]  То есть все R покрыть не получится.
[41:33.920 --> 41:38.920]  Получается, что для любого R это не верно.
[41:38.920 --> 41:51.920]  Так, ну теперь уж мне кажется точно все.
[41:51.920 --> 41:54.920]  Вот эту формулу для σ2 мы доказали.
[41:54.920 --> 41:57.920]  π2 получается приходом к дополнению.
[41:57.920 --> 42:00.920]  Значит, соответственно, если настоящий ответ да, то эта формула верна.
[42:00.920 --> 42:02.920]  Если настоящий ответ то не верна.
[42:02.920 --> 42:04.920]  Вроде все получилось.
[42:04.920 --> 42:07.920]  Так, ну что, киньте вопросы.
[42:13.920 --> 42:15.920]  Ну вот, отсюда получается ироническое следствие.
[42:15.920 --> 42:17.920]  Давайте я прямо напишу.
[42:17.920 --> 42:25.920]  Такое ироническое следствие, что если вдруг bpp равно exp,
[42:25.920 --> 42:28.920]  то тогда p не равно np.
[42:28.920 --> 42:39.920]  Да, потому что если еще и p равно np, то p равно ph,
[42:39.920 --> 42:45.920]  а bpp в ph, и тогда exp в ph, и exp равно p,
[42:45.920 --> 42:49.920]  а это претворяет четыре время оба иерархия.
[42:53.920 --> 42:57.920]  Ну, опять же, маловероятно, что именно таким образом
[42:57.920 --> 42:59.920]  будет доказано, что p не равно np.
[42:59.920 --> 43:02.920]  Скорее всего, посылка не верна.
[43:05.920 --> 43:08.920]  Так, ну хорошо, давайте сейчас перерыв небольшой сделаем.
[43:08.920 --> 43:13.920]  И поговорим после перерыва про дерандомизацию.
[43:15.920 --> 43:22.920]  Значит, по крайней мере, в каких случаях можно избавиться от вероятности вообще.
[43:28.920 --> 43:30.920]  Значит, дерандомизация.
[43:42.920 --> 43:44.920]  Значит, дерандомизация.
[43:48.920 --> 43:55.920]  Так, ну, давайте сейчас на простом примере покажу минимум один способ.
[43:55.920 --> 43:58.920]  А так, не знаю, может быть, мы и второй успеем.
[44:03.920 --> 44:09.920]  Значит, первый способ называется метод условных мата ожиданий.
[44:21.920 --> 44:23.920]  Значит, метод условных мата ожиданий.
[44:25.920 --> 44:27.920]  Но пример будет такой.
[44:27.920 --> 44:30.920]  Значит, задача MaxCut.
[44:32.920 --> 44:34.920]  Задача MaxCut.
[44:34.920 --> 44:36.920]  Максимальный MaxCut.
[44:36.920 --> 44:39.920]  Да, не MinCut, а MaxCut.
[44:43.920 --> 44:48.920]  Ну, например, есть группа людей, некоторые из них по какой-то причине
[44:48.920 --> 44:51.920]  не хотят находиться друг с другом в одном автобусе.
[44:51.920 --> 44:55.920]  И вопрос, как их рассадить на дварных автобусах,
[44:55.920 --> 45:00.920]  чтобы как можно больше этих нежелательных связей было разорвано.
[45:02.920 --> 45:04.920]  Вот эта задача MaxCut.
[45:07.920 --> 45:11.920]  Значит, терна задачи поиска это следующее.
[45:12.920 --> 45:22.920]  Значит, по графу G, который есть набор вершины-рёбер,
[45:22.920 --> 45:25.920]  найти представление...
[45:31.920 --> 45:39.920]  Найти представление V, которое есть S, не связанное с T.
[45:39.920 --> 45:48.920]  Значит, такое, что количество рёбер у V из E, таких, что у в S,
[45:48.920 --> 45:53.920]  а В в T, это чтобы было максимально.
[45:55.920 --> 46:00.920]  Значит, если мы ставим вопрос о том, можно ли найти такой разрез,
[46:00.920 --> 46:04.920]  чтобы это количество было больше липрона, чем K,
[46:04.920 --> 46:07.920]  то это будет NP-полная задача.
[46:10.920 --> 46:17.920]  Вот, значит, это NP-полная задача.
[46:19.920 --> 46:26.920]  Однако её можно апроксимировать вероятностно.
[46:29.920 --> 46:32.920]  Значит, вероятностная апроксимация.
[46:32.920 --> 46:40.920]  Просто каждая вершина равновероятно Vs и Vt.
[46:42.920 --> 46:55.920]  Значит, каждая вершина равновероятно Vs и Vt.
[46:56.920 --> 47:01.920]  Тогда у нас каждое ребро разрежется с вероятностью 1-2.
[47:09.920 --> 47:19.920]  Каждое ребро разрежется с вероятностью 1-2.
[47:19.920 --> 47:28.920]  Таким образом, средний размер разреза – это одна вторая от всех рёбер,
[47:28.920 --> 47:32.920]  что больше либо равно 1-2 от максимума.
[47:32.920 --> 47:52.920]  Средний размер разреза будет больше либо равен 1-2 от общего числа рёбер,
[47:52.920 --> 47:57.920]  что, конечно, будет больше, чем 1-2 от максимального.
[47:58.920 --> 48:03.920]  Ну а это, получается, решение в среднем.
[48:03.920 --> 48:09.920]  А вопрос, можно ли с такой же точностью апроксимировать детерминированно?
[48:09.920 --> 48:14.920]  Я не знаю, может, кто-нибудь из вас уже в уме сообразил, как.
[48:14.920 --> 48:18.920]  Я это буду рассказывать для демонстрации метода.
[48:18.920 --> 48:36.920]  Вопрос, как апроксимировать детерминированно с той же точностью.
[48:48.920 --> 49:04.920]  Смотрите, идея такая, что пусть у нас есть случайная величина,
[49:04.920 --> 49:14.920]  значит, кси ит – это будет, скажем, единица, если v иt лежит в s,
[49:14.920 --> 49:19.920]  значит, и ноль, если v иt лежит в t.
[49:19.920 --> 49:28.920]  И мы будем определять, да, ну и скажем, что пусть там cat,
[49:28.920 --> 49:35.920]  значит, от кси1 и так далее ксиенная, значит, это размер разреза.
[49:46.920 --> 49:52.920]  Что мы выясним? Мы выясним, что мат ожидания этого размера – это 1-2.
[49:52.920 --> 49:57.920]  Ну а дальше смотрите, значит, мат ожидания
[50:01.920 --> 50:10.920]  разреза, размера разреза, это с одной стороны равняется 1-2 просто,
[50:10.920 --> 50:17.920]  а с другой стороны – это полусумма, ну давайте я вот так напишу подробно,
[50:17.920 --> 50:20.920]  значит, это почему условные мат ожидания.
[50:20.920 --> 50:31.920]  Значит, cat от кси1 и так далее кси n при условии, что кси1 равно 0,
[50:31.920 --> 50:39.920]  значит, и плюс мат ожидания cat от кси1 и так далее кси n при условии,
[50:39.920 --> 50:43.920]  что кси1 равно единица, и пополам.
[50:47.920 --> 50:51.920]  Так, эта запись понятна?
[50:51.920 --> 50:58.920]  Ну как бы мы разбили границу пространства на две равномерно половины,
[50:58.920 --> 51:03.920]  в середине сначала по каждой из них, а потом взяли середине от всего.
[51:03.920 --> 51:07.920]  Но что это означает? Это означает, что одна из этих штук будет больше
[51:07.920 --> 51:11.920]  либо равно 1-2.
[51:11.920 --> 51:27.920]  Следовательно, одна из этих оценок будет больше либо равно 1-2.
[51:27.920 --> 51:32.920]  Ну и вот так мы кси1 зафиксируем, и дальше будем продолжать по индукции.
[51:32.920 --> 51:38.920]  То есть каждый раз у нас какое-то количество кси и так зафиксировано,
[51:38.920 --> 51:44.920]  и мы среднее выбираем так, чтобы условного мат ожидания не уменьшилось.
[51:44.920 --> 52:00.920]  Значит, зафиксируем кси1 и продолжим аналогично.
[52:00.920 --> 52:19.920]  То есть каждая кси и плюс первая фиксируется так, чтобы условный мат ожидания не уменьшилось.
[52:19.920 --> 52:26.920]  И это очень правильный вопрос.
[52:26.920 --> 52:30.920]  Действительно, чтобы это работало, нужно уметь вычислять.
[52:30.920 --> 52:33.920]  И это, собственно, ограничение метода.
[52:33.920 --> 52:39.920]  Это работает, если мы умеем вычислять все эти условно-мат ожидания.
[52:39.920 --> 52:52.920]  Это работает, если можно вычислить условно-мат ожидания.
[52:52.920 --> 52:55.920]  Но в данном случае это действительно можно.
[52:55.920 --> 52:57.920]  Значит, как это вычислять?
[53:01.920 --> 53:05.920]  Между какими фиксированными?
[53:25.920 --> 53:35.920]  Ну да, совершенно верно.
[53:35.920 --> 53:39.920]  Да, пересчитывается быстро.
[53:39.920 --> 53:44.920]  Так, ну что, можно не записывать?
[53:44.920 --> 53:54.920]  А теперь скажите, если это теперь все раскрутить, то что это за алгоритм получится?
[53:54.920 --> 54:09.920]  Да, это получается жадный алгоритм, когда мы просто каждую новую вершину отправляем так, чтобы разрезать хотя бы половину ребер, идущих от нее, в уже разминченные.
[54:14.920 --> 54:18.920]  Ну и там как бы очевидно, что одну вторую мы так разрежем.
[54:18.920 --> 54:23.920]  Это не самый лучший алгоритм из известных, там есть более хитрый.
[54:23.920 --> 54:30.920]  Там какое-то рациональное число, которое примерно, что-то типа 0.878 или что-то такое.
[54:37.920 --> 54:41.920]  Можно более умно действовать и большую долю гарантированно разрезать.
[54:41.920 --> 54:47.920]  То есть это будет доля от всех ребер исследовать на приближении к максимуму.
[54:48.920 --> 55:02.920]  Но еще сильнее приближать не умеют, и для какого-то уровня приближения уже доказано, что это НП трудное.
[55:09.920 --> 55:15.920]  Но есть другой пример, в котором так просто уже не обойдешься.
[55:15.920 --> 55:19.920]  Простой же алгоритм дает худшую оценку.
[55:19.920 --> 55:22.920]  Это пример МАКС-3-САД.
[55:25.920 --> 55:32.920]  МАКС-3-САД это дана 3 KNF.
[55:33.920 --> 55:46.920]  Дана 3 KNF, причем по-честному 3 KNF, то есть там в каждой скобке ровно 3 литерала с разными переменными.
[55:46.920 --> 56:00.920]  В каждой скобке 3 литерала с разными переменными.
[56:03.920 --> 56:08.920]  И нужно максимизировать число истинных скобок.
[56:09.920 --> 56:22.920]  Нужно максимизировать число истинных скобок.
[56:22.920 --> 56:41.920]  Смотрите, совсем наивный же один алгоритм дает приближение тоже 1-2.
[56:41.920 --> 56:53.920]  Мы рассматриваем какую-нибудь переменную P1 и рассматриваем все скобки, которые ее содержат.
[56:53.920 --> 57:02.920]  Смотрим, что больше с P1 или с отрицанием P1, берем соответствующие значения.
[57:02.920 --> 57:08.920]  И вот эти скобки с P1 вообще больше не рассматриваем, мы половину из них выполнили.
[57:08.920 --> 57:23.920]  Дальше если немножко подумать, то ясно, что это совсем не очевидно, что те, которые не выполнены, зачем выкидывать.
[57:23.920 --> 57:28.920]  Можно их попробовать еще выполнить на следующем.
[57:28.920 --> 57:33.920]  Чтобы одна вторая, мы прям выкидываем.
[57:33.920 --> 57:42.920]  Давайте не будем выкидывать, а просто эту переменную только вычеркиваем из скобки, а само ее оставляем.
[57:42.920 --> 57:48.920]  И если есть следующие переменные, то мы их тоже рассматриваем.
[57:48.920 --> 57:51.920]  Так что получится?
[57:51.920 --> 57:54.920]  Точно это не хуже.
[57:54.920 --> 57:58.920]  Не, не хуже-то это да, но лучше на самом деле получится.
[57:58.920 --> 58:10.920]  Получится, просто гарантия будет выше, так мало ли, может там всюду есть P1, и она сразу на первом этаге, на первом шаге все выполнены станут.
[58:10.920 --> 58:13.920]  Так строго лучше не получится.
[58:13.920 --> 58:17.920]  Вообщем гарантия будет вместо 1 и 2 будет 3 четверти.
[58:17.920 --> 58:24.920]  Потому что смотрите, чтобы у нас скобка стала ложной, нужно, чтобы все три литерала стали ложными.
[58:24.920 --> 58:31.920]  И когда у нас один литерал становится ложным, то какая-то другая скобка нова становится истинной.
[58:31.920 --> 58:35.920]  И так на каждую ложную у нас приходится хотя бы 3 истинных.
[58:35.920 --> 58:39.920]  Но это значит, что истинные хотя бы 3 четверти.
[58:39.920 --> 58:44.920]  Так, да?
[58:44.920 --> 58:47.920]  Во, да, совершенно верно.
[58:47.920 --> 58:52.920]  Если вот этим методом делать, то выяснилось, что можно еще лучше.
[58:52.920 --> 59:10.920]  Потому что если взять случайные значения всех переменных, то сколько у нас скобок-то выполнится в среднем?
[59:10.920 --> 59:13.920]  7 восьмых.
[59:13.920 --> 59:19.920]  В среднем 7 восьмых, что больше, чем 3 четверти, которые у нас получились.
[59:19.920 --> 59:25.920]  Действительно, чтобы получить 7 восьмых детерминированно, нужно увеличивать веса.
[59:25.920 --> 59:28.920]  То есть нужно не только оставлять скобки, но еще увеличивать их вес.
[59:28.920 --> 59:35.920]  Сейчас, это в среднем для конкретного трясата?
[59:35.920 --> 59:38.920]  Для любого конкретного трясата, да.
[59:38.920 --> 59:46.920]  Для любого конкретного трясата, но только вот с этим условием, что там в каждой скобке ровно 3 разных литерала.
[59:46.920 --> 59:48.920]  И от разных переменных.
[59:48.920 --> 59:50.920]  Тогда в среднем будет точно 7 восьмых.
[59:50.920 --> 59:54.920]  Мне не нравится, что там же события зависимые у нас.
[59:54.920 --> 01:00:00.920]  Так мы же суммируем, не перемножаем.
[01:00:00.920 --> 01:00:07.920]  Мотождание суммы равно сумму от ожидания даже для зависимых.
[01:00:07.920 --> 01:00:13.920]  То есть у нас у каждой одной скобки 7 восьмых, и мы эти 7 восьмых складываем.
[01:00:13.920 --> 01:00:17.920]  Они, конечно, зависим, но это нам не важно.
[01:00:23.920 --> 01:00:24.920]  Хорошо.
[01:00:24.920 --> 01:00:29.920]  В общем, соответственно, такие наивные подходы нам 7 восьмых не дали.
[01:00:29.920 --> 01:00:34.920]  А что получилось 7 восьмых, нужно именно вот этим условным от ожидания считать.
[01:00:34.920 --> 01:00:54.920]  И тут получается, что если у нас какая-то дизъюнкт, то тут получается вероятность истинного.
[01:00:54.920 --> 01:01:04.920]  Это будет 1, если есть истина.
[01:01:04.920 --> 01:01:10.920]  Соответственно, 7 восьмых, если 3 неопределенных.
[01:01:10.920 --> 01:01:18.920]  Дальше 3 четверти, если 2 неопределенных, одна ложная.
[01:01:18.920 --> 01:01:26.920]  Одна вторая, если одна неопределенная, и две ложных.
[01:01:26.920 --> 01:01:33.920]  И, соответственно, ноль, если три ложных.
[01:01:33.920 --> 01:01:42.920]  И отсюда получается, что у такого должен быть вес в два раза больше, а у такого в четыре раза больше.
[01:01:42.920 --> 01:01:47.920]  Почему? Потому что это 7 восьмых, оно перейдет либо в один, либо в три четвертых.
[01:01:47.920 --> 01:01:52.920]  Потом добавится или вычис ts одна восьмая, а три четвертых придет в один или в одну вторую, то есть тут добавится или вычисцerte одна четвертая.
[01:01:52.920 --> 01:01:58.920]  Одна вторая перейдет в единицу или в ноль, то есть добавится или вычисто reflectingventa одна вторая.
[01:01:58.920 --> 01:02:03.920]  Ну вот поэтому соответственно, эта фанада в 4 раза больше чем в одно восьмое.
[01:02:03.920 --> 01:02:10.680]  поэтому вес 4, а здесь вес 2. Дотабинированный алгоритм получается как наш второй,
[01:02:10.680 --> 01:02:18.640]  только когда мы сравниваем, кого больше п или отрицание п, то мы берем скобки,
[01:02:18.640 --> 01:02:23.600]  в которых еще 3 с весом 1, скобки, в которых 2 с весом 2, скобки, в которых 1 с весом 4.
[01:02:23.600 --> 01:02:40.000]  Вот здесь уже теорема, что лучше нельзя, если 7 восьмых плюс епсилон и епсилон константы,
[01:02:40.000 --> 01:02:52.080]  то это уже NP трудное. Но это сложная теорема. Это, кстати, в следующем году на спецкурсе
[01:02:52.080 --> 01:02:57.920]  я буду рассказывать, почему 7 восьмых плюс епсилон уже NP трудное.
[01:02:57.920 --> 01:03:07.280]  Ну что, у нас еще 10 минут, можно еще один сюжет рассмотреть. Давайте на том же самом
[01:03:07.280 --> 01:03:18.120]  примере про MaxCAD другой подход, который, кстати, этот же подход позволяет еще на один вопрос
[01:03:18.120 --> 01:03:25.720]  ответить, связанный с драндомизацией. Ну это как бы такая драндомизация по максимуму,
[01:03:25.720 --> 01:03:34.200]  когда мы хотим вообще от вероятности избавиться. А может быть, ну как бы такая более эффективная,
[01:03:34.200 --> 01:03:39.720]  чтобы алгоритм был все еще вероятностный, но использовал меньше случайных битов.
[01:03:39.720 --> 01:03:54.400]  Вот, и можно поставить вопрос. Ну, например...
[01:04:09.720 --> 01:04:24.400]  Сейчас, чего-чего, в чем вопрос?
[01:04:24.400 --> 01:04:32.200]  А, ну это правда, да. Не-не-не, сейчас, подождите, мы хотим 7 восьмых плюс епсилон не от числа скобок,
[01:04:32.200 --> 01:04:39.680]  а от максимально возможного числа одновременно истинных. Да, это вы, конечно, правы, что больше
[01:04:39.680 --> 01:04:47.400]  7 восьмих числа скобок может не получиться, но мы хотим 7 восьмых плюс епсилон от максимального
[01:04:47.400 --> 01:04:54.040]  количества одновременно истинных. Так, в общем, смотрите, вот если вспомнить, что у нас было в самом
[01:04:54.040 --> 01:04:59.880]  начале лекции, то мы уменьшали ошибку и делали это за счет повторения того же самого алгоритма
[01:04:59.880 --> 01:05:07.840]  каждый раз со свежими случайными битами. И у нас получалось, что ошибка становилась экспоненциально
[01:05:07.840 --> 01:05:16.880]  маленькой, но при этом число случайных битов умножалось на число запусков, то есть как раз
[01:05:16.880 --> 01:05:24.200]  на то, что у нас было в этом самом, в показателе экспонентов знаменателя. Ну и вопрос,
[01:05:24.200 --> 01:05:31.360]  можно ли сделать как-то лучше? Вот, оказывается, можно, и один из способов это использовать
[01:05:31.360 --> 01:05:37.440]  попарную независимость. И вот дарандомизацию МАКСК тоже можно сделать через попарную независимость.
[01:05:37.440 --> 01:05:58.880]  Значит, дарандомизация через попарную независимость.
[01:05:58.880 --> 01:06:17.400]  Ну, значит, смотрите, вот в этом рассуждении, в рассуждении про то, что случайный разрез имеет
[01:06:17.400 --> 01:06:25.760]  размер 1 на 2, мы используем только попарную независимость вот этих ксиитах. Нам достаточно
[01:06:25.760 --> 01:06:46.680]  нам достаточно попарной независимости ксиитах для утверждения о том, что мат ожидания
[01:06:46.680 --> 01:07:06.560]  того, что размер разреза будет равен 1 и 2. Ну понятно почему, потому что у ребра только два конца,
[01:07:06.560 --> 01:07:13.320]  и, соответственно, если для концов ребра ксиита и ксижита и независимая, то они совпадут
[01:07:13.360 --> 01:07:20.560]  всё равно всегда на 1 и и2, и нет всё равно еще на 1 и 2. Вот, а дальше есть, смотрите, есть такой
[01:07:20.560 --> 01:07:26.140]  способ за счет маленького числа истинно-случайных величин получить большое число попарной
[01:07:26.140 --> 01:07:35.480]  из маленького числа не зависимых случайных величин, то есть совокупностей независимых,
[01:07:35.480 --> 01:07:41.600]  получить большое число попарно независимых. Значит, а именно, смотрите, а вот такой подход,
[01:07:41.600 --> 01:08:01.160]  Пусть, скажем, у1, у2 и так далее, уLt это у нас независимые в совокупности случайная величина.
[01:08:01.160 --> 01:08:16.200]  Тогда мы рассмотрим вот такой набор. Во-первых, их всех оставим. Дальше возьмем, да, они из 0,1.
[01:08:16.200 --> 01:08:24.760]  Значит, рассмотрим их всех, независимые в совокупности и равномерно распределенные.
[01:08:24.760 --> 01:08:41.120]  Так, значит, возьмем их всех, дальше возьмем все попарные суммы по модулю 2, значит, у1 к
[01:08:41.120 --> 01:08:52.160]  4, у2, у1 к 4, у3 и так далее, у1 к 4, уL, у2 к 4, у3 и так далее, вплоть до уL-1 к 4, уL.
[01:08:52.160 --> 01:09:03.160]  Значит, дальше все потроечные суммы у1 к 4, у2 к 4, у3 и так далее, и вплоть до к 4 их всех.
[01:09:03.160 --> 01:09:25.880]  Значит, это будут попарно независимые и тоже равномерно распределенные случайная величина.
[01:09:25.880 --> 01:09:33.800]  Так, ну почему равномерно распределенные? Понятно, да, как бы мы их ссорим, они были независимы,
[01:09:33.800 --> 01:09:41.880]  значит, будут равномерные. Ну а попарно независимые, вот почему, значит, ясно, что уже по тройкам они не будут независимы,
[01:09:41.880 --> 01:09:50.880]  у1, у2 и у1 к 4, у2 вполне себе зависимы. Но пока мы берем только две, то, смотрите, есть какая-то одна сумма,
[01:09:51.880 --> 01:10:01.880]  есть какая-то другая сумма. И тогда есть хотя бы одна переменная такая, что она в одной сумме есть,
[01:10:01.880 --> 01:10:12.880]  а тут, соответственно, нет у ИТ. И тогда, смотрите, одно из определений независимости,
[01:10:12.880 --> 01:10:20.880]  что условное распределение одной величины условно на вторую совпадает с безусловным.
[01:10:20.880 --> 01:10:38.880]  Тогда получается, что если мы зафиксируем все переменные кроме уИТ, значит, зафиксируем все переменные кроме уИТ,
[01:10:38.880 --> 01:10:46.880]  значит, тогда тут получается фиксированное значение, а тут все еще равномерно-случайное.
[01:10:46.880 --> 01:10:57.880]  Значит, за счет одной уИТ аксор будет случайным, равномерно-случайная.
[01:10:57.880 --> 01:11:06.880]  Но это и означает, условное распределение будет равномерно-случайным, безусловное тоже равномерно-случайным, значит, они попарно независимы.
[01:11:06.880 --> 01:11:17.880]  При условии, что вторая фиксирована. Но тут еще более дробное разбиение, то есть то, что вторая фиксирована,
[01:11:17.880 --> 01:11:24.880]  я разбиваю на то, из чего конкретно она сложилась, и все равно получается, что у ИТ будет равномерно-случайная,
[01:11:24.880 --> 01:11:31.880]  даже на более мелких событиях, условно, тоже будет равномерно-случайная.
[01:11:31.880 --> 01:11:38.880]  Согласны, что они все попарно независимы?
[01:11:38.880 --> 01:12:03.880]  Получается, что у нас получилось, что l независимых совокупностей мы превратили в 2 в степени l минус 1 попарно независимых.
[01:12:03.880 --> 01:12:19.880]  Хорошо. Таким образом, получилось, что это ожидание по так построенным, именно для таких,
[01:12:19.880 --> 01:12:31.880]  кси1 и т.д., и ксиn, которые как раз вот эти, кси1 вот это, ксиn вот это. У нас будет равно 1 и 2.
[01:12:31.880 --> 01:12:41.880]  Дальше еще одна мысль осталась, что вот это мат ожидания можно прямо явно посчитать за понимание время.
[01:12:41.880 --> 01:12:59.880]  Да, значит, это можно не его посчитать, а это будет как бы усреднение, то, что оно усредняет, можно посчитать.
[01:12:59.880 --> 01:13:19.880]  А, ну ладно, я тебя так оставлю, значит, можно посчитать все значения для всех, точнее сейчас, значения для всех наборов у1 и т.д. уlt.
[01:13:19.880 --> 01:13:31.880]  Ну и выбрать из них максимальное.
[01:13:31.880 --> 01:13:42.880]  Да, почему? Потому что всего этих наборов, всего наборов 2 в степени l.
[01:13:42.880 --> 01:13:47.880]  Но 2 в степени l, это у нас примерно то же самое, что и n.
[01:13:47.880 --> 01:13:59.880]  Что получается? Получается, что мы перебираем все наборы вот такой длины, из них вот по этому правилу составляем кси и т.
[01:13:59.880 --> 01:14:05.880]  По кси и т.д. считаем размер разреза и находим самую большую из того, что посчитали.
[01:14:05.880 --> 01:14:14.880]  И поскольку средняя не больше максимума, то этот максимум будет хотя бы 1 вторая.
[01:14:14.880 --> 01:14:23.880]  Не самый простолкрито для этой задачи, но это опять же демонстрация метода.
[01:14:23.880 --> 01:14:33.880]  Ну чего, понятно? Да, перебор по подможеству, при том, что по этому подможеству средняя такое же, как по всему множеству.
[01:14:33.880 --> 01:14:39.880]  И получается, что этот перебор мы можем сделать.
[01:14:39.880 --> 01:14:50.880]  Значит, у нас есть исходный короткий вектор длины l, он булливый, мы перебираем все возможные булливые векторы длины l.
[01:14:50.880 --> 01:14:55.880]  L это логарифмен, грубо говоря.
[01:14:55.880 --> 01:15:01.880]  Потому что для конкретного вектора у мы делаем вектор кси вот по этим вот формулам.
[01:15:01.880 --> 01:15:07.880]  То есть по всем ненулевым маскам мы берем под сумму, и это будет соответствующий кси и т.
[01:15:07.880 --> 01:15:15.880]  И потом вот это кси и т.д. понимаем, в какую часть графа мы поместили эту вершину.
[01:15:15.880 --> 01:15:20.880]  И вот у нас получается два в степени разреза, в которых примерно m.
[01:15:20.880 --> 01:15:27.880]  И из этих разрезов средняя такая же, как средняя по всем, то есть в среднем половина.
[01:15:27.880 --> 01:15:31.880]  И мы вот по этому подможеству можем выбрать максимальный.
[01:15:31.880 --> 01:15:44.880]  Ну типа того, да.
[01:15:44.880 --> 01:15:55.880]  А зато она какой-то другой там гарантированно больше резит, а в среднем это все сократится.
[01:15:55.880 --> 01:16:00.880]  Ну вот, да, действительно такая вот идея сокращения перебора.
[01:16:01.880 --> 01:16:09.880]  Так, ну вот.
[01:16:09.880 --> 01:16:13.880]  Так, ну идем все. Спасибо.

[00:00.000 --> 00:11.480]  В прошлый раз мы с вами такое утверждение сформулировали, что при передаче классической
[00:11.480 --> 00:17.560]  информации через закодированное в квантовые носители возникает такая граница Холева. То есть
[00:17.560 --> 00:23.120]  взаимная информация между классическим входом, классическим выходом не превосходит вот такой
[00:23.600 --> 00:31.480]  средней энтропии по ансамблю минус среднее состояние ансамблю минус средней энтропии
[00:31.480 --> 00:39.200]  состояний входящих в этот ансамбль. Для доказательства мы с вами две леммы сформулировали.
[00:39.200 --> 00:52.520]  Лемма одна заключается в том, что относительно энтропии она обладает свойством монотонности,
[00:52.520 --> 01:06.920]  и у нас была интерпретация этого свойства, поскольку относительно энтропии квантовая имеет
[01:06.920 --> 01:12.960]  смысл расстояния, хотя она не является расстоянием, потому что она несимметрична. Но под действием
[01:12.960 --> 01:23.280]  некоторого отображения, которое есть квантовый канал, у вас происходит образ вот этих двух точек,
[01:23.280 --> 01:30.720]  он куда-то вот сюда перейдет. Вот это относительно энтропии не увеличивается. Само доказательство
[01:30.720 --> 01:37.240]  довольно сложное, в книжке Холева есть, но с физической точки зрения понятно. Ну и вам как
[01:37.240 --> 01:43.680]  математикам тоже. Вот это вот это сжимающее отображение, фи, оно сжимает, то есть относительная
[01:43.680 --> 01:50.920]  энтропия не увеличивается. И вторая была, которая является частным случаем, вот этой первой,
[01:50.920 --> 01:58.040]  когда это отображение есть просто взятие частичного следа. Взятие частичного следа это
[01:58.040 --> 02:04.920]  тоже канал, просто разменность входа больше, чем разменность выхода. Получается тогда вот
[02:04.920 --> 02:18.080]  свойство такое СРОАБ относительно энтропии Сигма Б больше либо равно, чем СРОАРОБ. Здесь в
[02:18.080 --> 02:27.080]  качестве этого фи выступает взятие частичного следа по Б. Так, что еще мы с вами сделали в прошлый
[02:27.080 --> 02:33.320]  раз? Мы вспомогательную конструкцию ввели. Вот эти буквы Х из входного алфавита,
[02:33.320 --> 02:41.840]  мы с вами как интерпретировали? Мы ввели вспомогательное пространство, размерность
[02:41.840 --> 02:49.840]  которого равна размеру алфавита. Это у нас был вспомогательный пространство HP Preparation. Вот его
[02:49.840 --> 03:00.120]  размерность есть размерность алфавита. Что еще мы с вами сделали? Мы с вами аналогичную
[03:00.120 --> 03:08.440]  процедуру для Y сделали. То есть мы ввели вспомогательное пространство HM, M от английского
[03:08.440 --> 03:22.520]  measurement. И это была у вас разменность выходного алфавита. И рассмотрели такое эффективное
[03:22.520 --> 03:33.920]  состояние R, P, Q, M такого вида. Сумма по Х, по Х это ваша вероятность, с которой буквы возникают.
[03:33.920 --> 03:40.760]  Вот теперь ket вектора Х это элементы вот этого вспомогательного пространства. И они
[03:40.760 --> 03:49.200]  ортогональные друг другу, ортонормированные. Дальше идет ρх, те состояния, в которые вы
[03:49.200 --> 04:01.160]  кодируете. И дальше идет 00. Это пока еще вы не провели измерение. Вот такой вот объект мы с
[04:01.160 --> 04:07.920]  вами посмотрели. Тут 00 тензорно умножается на некоторый объект. Мы сначала с этим объектом
[04:07.920 --> 04:21.160]  повозились и увидели с вами вот такую вот штуку, что энтропия Ро, П, Ку относительно
[04:21.160 --> 04:35.860]  энтропии Ро, П, Ку. И Ро, П, тензорно Ро, Ку. Правильно пишут хоть. Она у вас чему равна? Она как
[04:35.860 --> 04:45.040]  раз равна вот правой части в утверждении, которую мы хотим доказать. То есть это С, Ро среднее,
[04:45.040 --> 04:59.440]  минус сумма по Х. Вот это мы с вами сделали. В том мы сказали. Тут видите, буковка М здесь просто
[04:59.440 --> 05:05.880]  получается тензорномножение вот таким. Поэтому та величина, которая здесь стоит относительно
[05:05.880 --> 05:17.080]  энтропии, она вся еще равна вот таком величине Р, П, Ку, М. А тут надо нам придумать, куда добавить это.
[05:17.080 --> 05:33.080]  Ку, М. Я это М могу приписать или к П, или к Ку. Вот надо тут правильно приписать. Если вдруг ошибемся,
[05:33.080 --> 05:40.280]  то потом поправим. Вот здесь в принципе мы и остановились. Дальше был набросок. В чем он
[05:40.280 --> 05:49.160]  заключался? Он заключался в том, чтобы теперь применить измерение. То есть записать некоторую
[05:49.160 --> 05:56.000]  операцию, которая даст результат измерения. Значит включаем измерение в рассмотрение.
[05:56.000 --> 06:13.200]  Включаем измерение в рассмотрение. Вот смотрите, как я это сделаю. Я опишу вот это отображение ФИ,
[06:13.200 --> 06:18.920]  которое потом буду использовать в лейме 1. Вот в таком виде оно будет на вход брать
[06:18.920 --> 06:31.640]  некоторый оператор. На выходе выдавать мне вот такое вот выражение. То есть я сейчас написал
[06:31.640 --> 06:41.320]  представление Крауса для канала ФИ. А сами операторы Крауса будут выглядеть вот таким вот образом.
[06:41.320 --> 06:51.720]  На часть, которая связана с приготовлением, никак не влияем. Тут единичный оператор. На квантовую
[06:51.720 --> 07:05.160]  часть мы действуем корнем из EY. Это корень из того POV элемента, который в измерении сидит. А здесь
[07:05.160 --> 07:19.320]  будет такой оператор сдвига некоторый. Сейчас объясню чуть такое. Сумма по х, х. Ну ладно,
[07:19.320 --> 07:23.480]  х не очень хорошо. Давайте другой элемент, потому что это с выходным алфавитом связано. Давайте
[07:23.480 --> 07:38.120]  Y'. А тут будет Y'Y'. Ну плюсик, как надо понимать, плюсик по модулю размерности выхода выходного
[07:38.120 --> 07:59.560]  алфавита. Mod, Dim, H, M. Так, конструкция понятна пока? Значит, вот это что за матрица у последнего
[07:59.560 --> 08:08.200]  члена? Это просто сдвиг на Y. То есть это матрица перестановок. Если вы в стандартном байсе
[08:08.200 --> 08:19.000]  запишете, это матрица перестановок. Понятно? Она унитарная. Унитарная. Значит, проверяем свойство,
[08:19.000 --> 08:24.040]  нам же нужно, чтобы это было не просто вполне положительное отображение, которое мы сразу видим,
[08:24.040 --> 08:28.840]  что является таким вот этой формой, но чтобы еще след сохранялся. То есть нам нужно проверить
[08:28.840 --> 08:39.360]  условия, что сумма по Y, АY, крестик АY есть единичный оператор. Вот что нам нужно сделать.
[08:39.360 --> 08:55.680]  Теперь как мы это видим? Это единичный оператор где? В PQM. Ну понятно, для P единичка так и осталась.
[08:55.680 --> 09:05.040]  EY, корень из EY, умножить на корень из EY, будет EY. А сумма всех EY есть единичный оператор. Почему?
[09:05.040 --> 09:13.800]  Потому что это измерение, ее VEM-элементы суммируются в единичный оператор. Поэтому на втором месте
[09:13.800 --> 09:21.600]  тоже будет единичный. А на последнем всегда единичный, потому что здесь вы умножаете матрицу на
[09:21.600 --> 09:33.040]  свою унитарную, на свою эрмитовосопряженную. Получается единичный оператор. Понятно? Значит это канал, это канал.
[09:33.040 --> 09:46.080]  А значит для него верно вот это вот первое свойство. Давайте теперь этот канал применим к нашему
[09:46.080 --> 09:56.800]  состоянию вот этому PQM здесь и применим к вот этому тензорному произведению справа. Понимаете?
[09:56.800 --> 10:16.720]  Применяем теперь phi к этим величинам. Так, как действует phi на вот это RO-PQM?
[10:16.720 --> 10:26.280]  С классической частью, с приготовлением ничего не происходит. Просто переписываем ее и все.
[10:26.280 --> 10:35.140]  Потому что там действует единичный оператор. Дальше. С квантовой частью что происходит?
[10:35.140 --> 10:43.200]  Слева умножается на корень из EY, справа домножается на корень из EY. Напомню, что EY-эрмитовый,
[10:43.200 --> 10:49.400]  поэтому крестик не ставь. Что произойдет с этим нулем? Этот ноль просто заменится на Y и все.
[10:49.400 --> 11:04.320]  Это и есть этот Y. Результат измерения. Просто теперь вы его представили в виде некоторого
[11:04.320 --> 11:13.600]  объекта во вспомогательном Гильбертом пространстве. Ну в принципе этот объект
[11:13.600 --> 11:23.880]  целиком можно обозначить RO-PQM, чтобы не писать phi все время. Значит, с левой частью мы знаем,
[11:23.880 --> 11:31.560]  как поступить. Теперь нужно к правой части применить вот это все. Значит, если я phi,
[11:31.560 --> 11:48.040]  подействую на RO-QM. Что здесь произойдет? Но RO-P не поменяется. Так и останется.
[11:48.040 --> 11:59.960]  С квантовой частью что произойдет? У вас изначально вот эта вот часть как записывалась. Давайте
[11:59.960 --> 12:12.040]  вспомним. Это была сумма по X, PX, RO-X, тензор на 0,0. Вот как она записывалась. RO-QM. Вот в прошлый
[12:12.040 --> 12:21.680]  раз это делали. И теперь применяем этот phi. Значит, здесь опять корни из EY возникнут.
[12:21.680 --> 12:38.600]  А это есть среднее состояние ансамбли. Кстати, можно коротко так написать. Ну ладно, уже написал.
[12:38.600 --> 12:53.520]  Вот это все. Это есть корень из EY, RO среднее, корень из EY. Сообразили.
[12:53.520 --> 13:16.480]  Теперь. Сейчас. Правильный вопрос. EY должны остаться. EY должны остаться. Совершенно верно. Спасибо.
[13:23.520 --> 13:36.920]  Спасибо. Исправили. Что результат измерения не фиксирован уже. В принципе, мы первую лему теперь
[13:36.920 --> 13:44.960]  используем. Говорим, что граница Холева, которую мы получили вот здесь подстановкой RO-P-QM, а тут вот
[13:44.960 --> 14:02.080]  этого выражения. Заменяется на некоторое другое. Давайте я запишу последовательность операции,
[14:02.080 --> 14:13.840]  которую мы делаем. Значит, вот начнем с этой границы Холева. S, RO среднее, минус сумма Px, Px, S, ROx. Мы с вами
[14:13.840 --> 14:29.280]  помним, что это взаимная информация RO-P-QM, RO-P, RO-QM. Теперь мы применяем нашу лему и конкретный
[14:29.280 --> 14:39.320]  вид канала phi применяем. Что мы получаем? Мы получаем S. Вот здесь будет стоять тогда RO-P-QM.
[14:39.320 --> 15:03.680]  Давайте я перепишу сумма Pxy, Px, xx, корень из ey, rox, корень из ey, yy. Это мы переписали первое
[15:03.680 --> 15:10.040]  выражение в этой относительной энтропии, а второе выражение в относительной энтропии это будет что?
[15:10.040 --> 15:26.760]  RO-P. RO-P это вот такая величина сумма Px, Px, xx. Первая часть тензорное произведение. Теперь вторая
[15:26.760 --> 15:47.040]  часть QM. Вот так вот ее запишем, как корень из ey, RO среднее, корень из ey и еще yy, но сумма по y, yy.
[15:47.040 --> 15:57.280]  Так вы тогда себе, если здесь мы исправили сумму по y, тогда здесь тоже она останется.
[15:57.280 --> 16:04.880]  Закрываю скобочки для этой относительной энтропии. Видите, до двух палочек первой
[16:04.880 --> 16:12.600]  выражения, после двух палочек второй выражения. Теперь пользуемся второй леммой. Теперь я могу
[16:12.600 --> 16:20.240]  взять частичный след по подсистеме Q, по квантовой. Выкинуть квантовую систему вообще из рассмотрения.
[16:20.240 --> 16:29.880]  Беру частичный след по Q, относительная энтропия может только уменьшиться. Что я должен сделать?
[16:29.880 --> 16:37.840]  Я должен взять след по вот этим вот частям. По вот этим частям я должен взять след. Вы видите,
[16:37.840 --> 16:45.000]  что здесь получится. Здесь получится, смотрите какая величина, напишу под этим неравенством,
[16:45.000 --> 16:57.760]  след от корень ey, rox, корень из ey. Под знаком следа можно переставлять. Получится ey, rox,
[16:57.760 --> 17:06.360]  а это есть вероятность получить исход y при условии, что на входе была букой. То есть получится
[17:06.720 --> 17:13.040]  условная вероятность. Ну и смотрим, что тогда получится. Будет относительная энтропия каких
[17:13.040 --> 17:31.400]  величин. Значит здесь будет сумма по x, по y, по x. Дальше стоит py при условии x, xx, yx. Первая
[17:31.400 --> 17:43.320]  часть упростилась. Вторая часть. Что это такое? Вторая часть. Здесь тогда будет тоже вот эта
[17:43.320 --> 18:01.720]  вот условная вероятность. Уже со средним состоянием вот этого ансамбля. Это мне
[18:01.720 --> 18:22.480]  нужно еще подумать, что это такое. Ну давайте я пока просто напишу. Сейчас сообразим. Сумма
[18:22.480 --> 18:38.680]  по y, след ey, rox средняя и yy. Теперь нам нужно получившуюся величину упростить далее. С первым
[18:38.680 --> 18:46.680]  членом все понятно. Смотрите, умножаем px на условную вероятность pyx, получаем просто вероятность
[18:46.680 --> 18:56.840]  pxy. Понятно? По определению условной вероятности. То есть здесь у нас получится с вами что за объект.
[18:56.840 --> 19:06.640]  Сумма по x, по y, pxy, xy и y. То есть это будет как раз-таки хороший объект в плане совместного
[19:06.640 --> 19:17.840]  распределения вероятностей x и y. А вот что будет в правой части? Давайте посмотрим. Тут я не
[19:17.840 --> 19:43.440]  очень понимаю, что такое будет ey, rox средняя? Ну вот вы имеете в виду объединить эту сумму.
[19:43.440 --> 19:54.400]  Так давайте запишем.
[19:54.400 --> 20:18.240]  Сейчас сообразим. Это сумма по x, px, py при условии x. Вот так вот это все понятно?
[20:18.240 --> 20:32.000]  Не понял. Это будет, значит, вот это все вместе. Это есть pxy, сумма по x дает маргинальное распределение.
[20:32.000 --> 20:42.560]  Значит, все, вторая часть тоже приняла очень хороший вид, очень красивый. Смотрите, первая
[20:42.560 --> 20:48.560]  часть уже записана в том виде, который нам нужен. Это диагональная матрица, на диагонали которой
[20:48.560 --> 21:03.600]  стоят px. А второе выражение приобрело тоже красивый вид. Это сумма по y, py, yy. Теперь смотрите, все
[21:03.600 --> 21:11.960]  матрицы, которые сюда входят, они все диагональные. Все диагональные. Вот это py умножается на yy,
[21:11.960 --> 21:23.240]  yy ортонормированные, xy тоже ортонормированные. Это получается классический аналог квантовой
[21:23.240 --> 21:33.880]  относительной тропии. То есть это расхождение Кульбека Лейблера. У этих двух величий. А что это такое?
[21:33.880 --> 21:43.960]  Здесь ставим знак равно для всего выражения с. Вот я здесь его продолжаю. И это все есть не что иное,
[21:43.960 --> 21:53.360]  как просто расхождение Кульбека Лейблера для каких распределений? С одной стороны у вас стоит
[21:53.360 --> 22:10.520]  распределение pxy xy, а с другой стороны у вас стоит произведение распределений px и py. Не знаю,
[22:10.520 --> 22:22.400]  как это нарисовать красиво. Вот это? Это просто распределение вероятности. Вот когда я возьму
[22:22.560 --> 22:29.640]  набор таких вероятностей с элементами, перечислив элементарные исходы x и y, то я получу распределение
[22:29.640 --> 22:38.680]  вероятности. Расхождение Кульбека Лейблера оно для распределения вероятности делается. Вот одно
[22:38.680 --> 22:43.280]  распределение вероятности, вот другое. Произведение маргинальных, напишу так,
[22:43.280 --> 22:57.160]  произведение маргинальных распределений вероятности. И это есть не что иное, как hx
[22:57.160 --> 23:17.360]  плюс hy минус hxy. То есть та самая взаимная информация. Так, последний шаг, понятен?
[23:17.360 --> 23:40.200]  Екатерина, вопрос. И таким образом мы с вами закончили доказывать границу Холева. Смотрите,
[23:40.200 --> 23:46.880]  с чего мы начинали? Мы начинали с вот этого выражения, а закончили взаимной информации
[23:46.880 --> 23:52.880]  между x и y. То есть здесь присутствуют только классические объекты, вот в чем его нетривиальность.
[23:52.880 --> 23:59.840]  Тут классические объекты. Распределение букв на входе и распределение результатов измерения на
[23:59.840 --> 24:10.800]  выходе. А здесь присутствуют только квантовые объекты. Ро x, те операторы плотности, в которые
[24:10.800 --> 24:16.640]  мы кодируем буквы. Ро среднее, среднее состояние ансамбля. От исходного, от исходной классики
[24:16.640 --> 24:23.680]  остались только вот эти вот px, вероятности букв находит. Вот в чем его нетривиальность.
[24:23.680 --> 24:35.240]  Это означает, как следствие из границы Холева, что из одного кубита вы можете максимум извлечь
[24:35.240 --> 24:51.920]  сколько бит информации. Один бит. Следствие, если размерность вашего, вашей квантовой системы
[24:51.920 --> 25:06.320]  равняется d, то смотрите, что получается. s, ro, среднее, минус сумма по x, по x, s, ro, x.
[25:06.320 --> 25:14.000]  Интропия не отрицательна, поэтому все это меньше и равной, чем s, ro, среднее. А энтропия любого
[25:14.000 --> 25:24.520]  состояния в демерном пространстве не превышает логарифмы d. Вот и получается, что хотя вы в кубит
[25:24.520 --> 25:30.920]  можете закодировать сколько угодно информации, а извлечь вы можете не больше, чем один бит. Если
[25:30.920 --> 25:39.280]  у вас d-уровневая система, то не больше, чем логарифмы d. Вот такое вот нетривиальное утверждение.
[25:44.000 --> 25:56.800]  Если состояние вот здесь вот ro, x чистые, то тогда вы получите здесь s, ro, x, 0 и достигнете вот этой
[25:56.800 --> 26:04.440]  вот величины. Теперь немножко по обозначениям. От чего зависит вот эта величина? Она зависит от
[26:04.440 --> 26:12.400]  задания ансамбля, поэтому ее еще называют Hi-величиной ансамбля. Давайте такое. Hi-величина,
[26:12.400 --> 26:21.080]  она определяется как буква Hi, на вход ей вы подаете ансамбль. Что такое ансамбль? Это совокупность
[26:21.080 --> 26:28.760]  вероятностей p и x, которыми буквы возникают, и ro и x, те квантовые состояния, в которые вы их
[26:28.760 --> 26:43.160]  кодируете. Hi-величина этого ансамбля по определению есть вот эта s, ro средняя минус сумма px, px, s, ro, x.
[26:43.160 --> 26:55.200]  Давайте теперь информационный смысл придадим этой процедуре. У нас была классическая буква,
[26:55.200 --> 27:01.680]  мы ее закодировали в ro, x, потом мы ее направим на измерительное устройство, но мы можем
[27:01.680 --> 27:16.920]  трактовать, что мы пропустили это ro, x через тождественный канал, канал без шума. Потом мы что
[27:16.920 --> 27:26.160]  с вами сделали? Взяли и измерили те состояния, которые к нам пришли с помощью некоторого QVM,
[27:26.160 --> 27:34.640]  получили выход классический y. Получается, что то количество информации, которую можно передать
[27:34.640 --> 27:41.560]  через канал без шумный, не превосходит вот этой величины Холева, а она в свою очередь не
[27:41.560 --> 27:49.880]  превосходит вот этот логарифм D. А если вы в качестве этого ансамбля что сделаете? Возьмете
[27:49.880 --> 28:04.000]  ну например пусть у вас алфавит тоже демерный, то тогда вы можете вот это ro среднее сделать как
[28:04.000 --> 28:19.240]  1 на D единичный оператор, сейчас напишу, 1 делить на D, сумма по х, где здесь уже алфавит демерный. И получится,
[28:19.240 --> 28:26.920]  что вы тогда через канал без шума можете передавать максимум вот это вот логарифм D.
[28:26.920 --> 28:43.960]  Не понял вопрос еще. Чтобы получить вот это ансамбль для того случая, чтобы Х величина равнялась
[28:43.960 --> 28:57.200]  логарифм D. Сами вот эти ro х это в этом случае просто xx. Спасибо за вопрос,
[28:57.200 --> 29:06.400]  то я не четко сформулировал. То есть если шума нет, то все прекрасно. Теперь вы можете модифицировать
[29:06.400 --> 29:14.880]  эту задачу и сказать, я заменю тождественный канал на канал с шумом. Что тогда у меня получится?
[29:14.880 --> 29:20.320]  У меня получится тогда передача классических сообщений через квантовые каналы зашумленные.
[29:20.320 --> 29:34.320]  Так, у вас не возникает вопросов, где здесь типичность, потому что в классике были какие-то
[29:34.320 --> 29:45.000]  типичные слова. Здесь тоже это понятие можно ввести. То есть здесь будут типичные
[29:45.000 --> 29:55.640]  подпространства и будут условно типичные подпространства. Но я не знаю, насколько это вам
[29:55.640 --> 30:04.280]  интересно. Стоит об этом говорить или лучше двинуться вперед. Сейчас я посмотрю по времени. 33
[30:04.280 --> 30:12.280]  минуты прошло уже. Давайте до перерыва я чуть-чуть проговорю про вот эти типичности. Откуда они
[30:12.280 --> 30:24.480]  возникают? Вот смотрите, что мы сделали. Мы буквом поставили соответствие матрице плотности. Если мы
[30:24.480 --> 30:36.240]  возьмем какое-то слово, то ему в соответствии будет какая матрица плотность? РОХ1, тензорный и так далее, РОХn.
[30:36.240 --> 30:49.000]  Теперь вероятность слова у нас чему равнялась? Это вероятность Х1 и так далее, вероятность Хn
[30:49.000 --> 30:58.320]  перемножая. А это можно назвать РОВ, матрица плотности слова, которая отвечает этому слову.
[30:58.320 --> 31:09.840]  Значит, если я умножу ПВ на РОВ и просуммирую по вот этим словам, то что я получу? Это получу я
[31:09.840 --> 31:18.320]  тогда матрицу плотности среднего слова, правильно? И как она будет выглядеть? Вы видите, что здесь
[31:18.320 --> 31:35.600]  отдельно расцепится суммирование по Х1, ПХ1, РОХ1 и так далее, а здесь будет сумма по Хn, ПХn, РОХn.
[31:35.600 --> 31:44.640]  А все эти суммы, это есть просто среднее состояние ансамбля. То есть получается, что матрица плотности
[31:44.640 --> 31:51.920]  среднего слова это произведение nt-тензорная степень, вот такой вот значок еще используют,
[31:51.920 --> 32:02.240]  nt-тензорная степень РОСредняя. Это будет у вас РОВСредняя. А теперь вы можете посмотреть на этот
[32:02.240 --> 32:09.640]  оператор. У РОСреднего есть спектральное разложение. Что такое спектральное разложение? Вы берете
[32:09.640 --> 32:19.400]  сумму по каким-то там к, лямбда кт, а тут ψкт и ψкт, это собственные векторы, нормированные у этого
[32:19.400 --> 32:28.400]  РОСреднего, лямбда кт и собственное значение. И вам нужна тензорная степень. Что такое спектральное
[32:28.400 --> 32:38.840]  разложение? Теперь давайте эту n-тензорную степень смотреть. Что это такое? Это будет
[32:38.840 --> 32:49.600]  сумма, можно здесь ее так написать, k1 и так далее kn, будет лямбда k1 и так далее лямбда kn,
[32:49.600 --> 32:59.560]  ну и дальше будут всякие выражения длинные типа ψk1, тензорную умножить на ψk2 и так далее,
[32:59.560 --> 33:10.520]  ψkn, это будет вот ket-вектор некоторый длинный, ну и в обратную сторону, ψk1, бра-векторы пошли,
[33:10.520 --> 33:25.160]  ψkn. Так, теперь смотрите, вот эти лямбда это распределение вероятностей тоже по k. Теперь
[33:25.160 --> 33:32.920]  вы когда берете это произведение, вот у вас получается, вот смотрите, если произведение вот
[33:32.920 --> 33:42.880]  этих лямбд лежит в некотором, сейчас сформулирую, вот смотрите на эти лямбда, как раньше смотрели на
[33:42.880 --> 33:56.320]  p от x, то есть вот эти распределения у вас будут либо типичные, либо нетипичные вот эти вот вероятностей.
[33:56.320 --> 34:11.760]  Не очень понятно. А с чем надо сравнивать?
[34:11.760 --> 34:26.880]  Да, здесь вероятности вот такие, но если вы посчитаете энтропию среднего состояния ансамбля,
[34:26.880 --> 34:33.920]  что это такое? Это вот как раз таки энтропия вот этого распределения лямбда-ката, это просто есть
[34:33.920 --> 34:45.400]  минус сумма по k, лямбда-ката, алгорифм лямбда-ката. И получается, что типичные вклады вот в эту сумму,
[34:45.400 --> 34:55.800]  типичные вклады в сумму, это какие? Это те, для которых произведение вот этих лямбда-катах,
[34:55.800 --> 35:12.760]  давайте k1, лямбда-kn, оно зажато между 2 в степени минус, вот это n, s, ρ, средняя, тут будет у нас еще
[35:12.760 --> 35:24.520]  минус какая-то дельта, а тут будет 2 в степени минус n, s, ρ, средняя, плюс дельта. Вот теперь лямбда
[35:24.520 --> 35:31.320]  играет роль. У нас была раньше вероятность слова, а теперь произведение вот этих лямб, вместо вероятности слова.
[35:31.320 --> 35:43.240]  И получается, что вот эти вот, если вы возьмете Psi с вот этим мультииндексом k большое, k большое
[35:43.240 --> 35:55.160]  это вот этот мультииндекс, то тогда типичное подпространство, это что такое? Ну или можно
[35:55.160 --> 36:07.160]  сказать дельта типичное подпространство. Это вот такая вот штука, это будет у вас, ну давайте назовем
[36:07.160 --> 36:16.400]  его h, n, дельта, потому что зависит от того, какую дельту возьмете, и это будет линейная оболочка
[36:16.400 --> 36:32.240]  чего? Вот таких Psi-катах. Каких? Таких, что вот эти лямбда-каты зажаты вот в этом диапазоне. Таких,
[36:32.240 --> 36:43.760]  что выполнена вот эта звездочка. То есть вот где типичные возникают всякие объекты. Типичное
[36:43.760 --> 36:50.840]  подпространство можно ввести, а когда мы смотрим фиксированное слово и куда оно может перейти,
[36:50.840 --> 37:01.080]  то мы получим с вами условно типичные подпространства. Но я не буду в это угубляться,
[37:01.080 --> 37:08.640]  в книжке Холева про это все написано. То есть идеология, которая была в классике, переносится
[37:08.640 --> 37:20.080]  на квантовый случай. Ну конечно же, там нужно больше внимания уделять этим объектам. Еще вот
[37:20.080 --> 37:26.160]  таким образом, когда вы передаете классическую информацию с помощью квантовых объектов,
[37:26.160 --> 37:33.760]  вам нужно использовать в качестве кодирования вот такие вот объекты из типичного подпространства.
[37:33.760 --> 37:40.560]  Но это еще называется сжатие шумахера. То есть вот Шеннон показывал, как сжимать информацию,
[37:40.560 --> 37:48.680]  кодируя в типичные слова. А шумахер показал, как можно, так скажем, сжимать квантовую
[37:48.680 --> 37:56.120]  информацию, кодируя вот в эти типичные подпространства. Ну все, не буду дальше
[37:56.120 --> 38:05.360]  углубляться в эту тему. В книжке Холева все есть. Делаем перерыв. Так, вот картинку стираю, хотя зря.
[38:05.360 --> 38:11.480]  Теперь смотрите, что мы будем с вами делать. Мы сейчас будем смотреть сценарии передачи
[38:11.600 --> 38:20.760]  классической информации через квантовый канал. Их на самом деле существует несколько.
[38:20.760 --> 38:28.440]  И мы сейчас увидим отличия от классики. Вот в чем оно будет проявляться.
[38:28.440 --> 38:44.320]  Значит, вот если использовать, давайте так, сценарий 1 передачи, сценарий 1. Если вы каждую
[38:44.320 --> 38:51.480]  букву кодируете в матрицу плотности, а потом каждую матрицу плотности измеряете,
[38:51.480 --> 39:02.880]  то вы реализуете самую простую схему, которая возможна. То есть вы, закодировав сообщение,
[39:02.880 --> 39:10.840]  давайте я нарисую вот эти вот кружочки. Это будет матрица плотности. Вот пусть это ro x1,
[39:10.840 --> 39:27.040]  это ro xn-1, ro xn. Вы закодировали какое-то слово w равное x1 и так далее xn в квантовом виде.
[39:27.040 --> 39:35.880]  Потом вы отправляете его уже через какой-то канал, который может шум привносить. Здесь я тоже напишу
[39:35.880 --> 39:41.460]  буквку phi, но это phi отличается от того, что было в доказательстве. Понятно? Это phi показывает
[39:41.460 --> 39:56.640]  уже шум в канале связи. Значит, что к вам придет? К вам придут зашумленные версии вот этих матриц
[39:56.640 --> 40:03.160]  плотности. Помните, в прошлом семестре смотрели образ шара Блоха под действием какого-то шума,
[40:03.160 --> 40:11.880]  диполяризующий или дефазирующий. Здесь зашумленные версии этих объектов к вам приходит. Здесь к вам
[40:11.880 --> 40:22.560]  приходит phi, ro x1 и так далее. Здесь к вам приходит phi, ro xn. Что при этом подразумевается?
[40:22.560 --> 40:32.560]  Подразумевается, что вот этот канал phi без памяти, отсутствуют эффекты памяти. То есть предыдущие
[40:32.560 --> 40:42.960]  сообщения не влияют на то, как этот канал действует на следующий. Значит, что дальше вы
[40:42.960 --> 40:50.760]  можете делать? Вы можете измерить каждый из этих кубит по отдельности. Это будет индивидуальное
[40:50.760 --> 41:00.000]  измерение каждого кубита. У вас какие-то классические исходы здесь получатся. Здесь
[41:00.000 --> 41:07.560]  будет какой-то y1, здесь будет какой-то yn. В принципе, вы ничем не ограничены и можете
[41:07.560 --> 41:17.680]  дальше постобработать эти результаты. Классическая постобработка не запрещается. То есть что такое
[41:17.680 --> 41:22.560]  классическая постобработка? Это некоторая стахастическая матрица, которая говорит,
[41:22.560 --> 41:29.040]  как перенаправить эти y. Можно какие-то объединить, можно какие-то с вероятностями
[41:29.040 --> 41:34.880]  направить в одну сторону, в другую сторону. Вот здесь вот классическая постобработка, это просто
[41:34.880 --> 41:44.640]  некоторая стахастическая матрица. Это вы сами можете делать, если надо. Если не надо, можете
[41:44.640 --> 41:51.680]  ничего не делать. Это будет просто стахастическая матрица в виде единичной матрицы. На выходе вы,
[41:51.680 --> 42:00.240]  давайте я результат, который на выходе получается, напишу просто какой-то y большой, без индекса.
[42:00.240 --> 42:11.640]  Результат классической постобработки. То есть в этом сценарии 1 первые два момента главных.
[42:11.640 --> 42:20.240]  Первый это то, что состояние, которое, если вы хотите кодировать слово, они будут у вас
[42:20.240 --> 42:28.440]  записываться в виде вот этих тензорных произведений. Первый момент. А второй момент заключается в том,
[42:28.440 --> 42:40.760]  что POV ам элементы на выходе е, у. Это будет что такое? Сумма по вот этим y1 и так далее, y, n.
[42:40.760 --> 42:49.840]  Тут какая-то стахастическая матрица. Давайте я напишу p, y, y1 и так далее, y, n. Вот это
[42:49.840 --> 42:55.920]  стахастическая матрица. А здесь индивидуальные измерения. Индивидуальные измерения, они для каждого кубита
[42:55.920 --> 43:12.880]  могут быть разными, могут быть одинаковыми, е, y1 тензорно, е, y, n. Что это такое? Мы задали POV ам элементы.
[43:12.880 --> 43:25.720]  В этом случае, что вы можете делать? Вы можете только подбирать вероятности p, x и те буквы,
[43:25.720 --> 43:31.840]  в которые вы кодируете, ρ, х. То есть вот что вы можете варировать. Вот это мы можем варировать,
[43:31.840 --> 43:38.720]  то есть вы можете варировать ансамбль. Можем изменять вот эти величины.
[43:38.720 --> 43:52.880]  Смотрите, что в этом случае вы получите. Если вы найдете взаимную информацию между x и y и
[43:52.880 --> 44:05.960]  возьмете Supreme по вот этому ансамблям, которые вы используете, то в этом конкретном случае,
[44:05.960 --> 44:17.520]  давайте здесь это подчеркнем, что здесь у вас вероятность получить y какой-то катой,
[44:17.520 --> 44:34.320]  при условии, что на входе был x, это есть след е, y катова с phi, который действует на ρ, х. Вот где канал
[44:34.320 --> 44:41.040]  сидит phi. Канал phi сидит вот здесь, в этих условных вероятностях. Если вы теперь возьмете
[44:41.040 --> 44:48.480]  этот Supreme, то вы получите так называемую c11 пропускную способность канала phi.
[44:48.480 --> 45:02.560]  В самом деле мы варьируем ансамбль, значит p, x пропадает, ρ, х пропадает. Что остается?
[45:02.560 --> 45:13.240]  Остается только канал phi. А, и что мы еще? Я не дописал, извиняюсь. В Supreme мы еще дописываем
[45:13.240 --> 45:24.200]  вот эти вот е, y. То есть я могу варьировать эту стокастическую матрицу, могу варьировать вот
[45:24.200 --> 45:38.280]  эти вот p, y элементы. Значит остается только свойство канала phi. И это называется c11 пропускная
[45:38.280 --> 45:50.960]  способность. Это свойство канала, характеристика канала. Вот одна из задачек задания как раз таки на
[45:50.960 --> 45:58.560]  это и нацелено, когда у вас всего две буквы в алфавите, и вот эти состояния ρ чистые, но они
[45:58.560 --> 46:08.200]  зафиксированы там. Теперь смотрите, что можно в этом сценарии поменять, чтобы получить что-то
[46:08.200 --> 46:22.200]  получше. Да-да. Нет, нет, в этом и есть смысл, то есть смотрите, в этом случае, правильно,
[46:22.200 --> 46:35.600]  спасибо за вопрос, в этом случае вот это c11 совпадает с классической пропускной способностью,
[46:35.600 --> 46:45.520]  ну как это сказать, с шенненовской, с c шенненовской. Но здесь еще дополнительная у вас есть оптимизация
[46:45.520 --> 46:55.960]  по вот этим измерениям. У шеннена сразу какая-то буква на выходе получается. С шенненовской, где,
[46:55.960 --> 47:01.840]  вот смотрите, что нужно в шенненовской пропускной способности сделать. Нужно заменить вот это p,
[47:01.840 --> 47:11.720]  y при условии x, но вот эту формулу, заменяете p, y при условии x на эту формулу, получаете
[47:11.720 --> 47:24.320]  то же самое, что для квантового канала, или что для классического канала, извините. Но это самый
[47:24.320 --> 47:30.880]  простой сценарий, который можно представить, поэтому не удивительно, что здесь получается полное
[47:30.880 --> 47:41.000]  совпадение с классическим. Вот смотрите, это когда вот эти два условия выполнены, то есть слова
[47:41.000 --> 47:48.720]  кодируем в индивидуальные, в тензорные произведения индивидуальных матриц плотности, а измеряем
[47:48.720 --> 48:07.840]  каждую частицу по отдельности. В сценарии 2 он что подразумевает? Вот смотрите, мы же знаем,
[48:07.840 --> 48:18.800]  что существуют перепутанные состояния, поэтому вы можете условия 1 изменить и сказать, что
[48:18.800 --> 48:25.480]  ro-w не обязана быть вот таким вот тензорным произведением, вот это же факторизованное
[48:25.480 --> 48:32.080]  состояние, а вы можете в какое-то коррелированное состояние, например, в перепутанное состояние
[48:32.080 --> 48:41.760]  кодируем. То есть взять слово и ему в соответствие поставить уже некоторое, в общем случае перепутанное
[48:41.760 --> 48:56.200]  состояние многих частиц. То есть если вы теперь ro-w представите вот в таком виде, который не
[48:56.200 --> 49:06.680]  сводится к тензорным произведению индивидуальных букв, перешлете каждую из частичек, которые
[49:06.680 --> 49:17.400]  составляет это состояние через квантовый канал phi, то вы получите зашумленную версию. Если шум
[49:17.400 --> 49:28.920]  не очень сильный, то даже и квантовые корреляции могут остаться. То есть вот этими волнами я
[49:28.920 --> 49:40.280]  пытаюсь показать, что состояние не имеет вид тензорного произведения. Но измеряете, если вы
[49:40.280 --> 49:56.360]  по-прежнему каждую частицу индивидуально с возможной постобработкой, то вы получите то же самое,
[49:56.360 --> 50:10.600]  к сожалению. То есть supremum уже теперь по каким величинам? Вы уже будете брать вероятности слов
[50:10.600 --> 50:26.800]  ro-w, e-y, но они имеют e-y по-прежнему вот этот вот вид 2. То есть давайте я напишу, что это индивидуальные
[50:26.800 --> 50:39.560]  измерения. От чего? От вот этой x, y, но под x теперь подразумеваются уже расширенные выражения. Это
[50:39.560 --> 50:50.880]  будет так называемая c-бесконечность 1-пропускная способность канала phi. Бесконечность показывает,
[50:50.880 --> 51:00.520]  что вы в кодировании можете допускать блочность, вот это блочное кодирование. И размер блока не
[51:00.520 --> 51:07.840]  ограничен, то есть он в принципе размер блока может стремиться бесконечно. Так,
[51:07.840 --> 51:16.720]  теперь почему я говорю, что выигрыша здесь нет? Значит, есть такое утверждение c-бесконечность
[51:16.720 --> 51:32.680]  1 совпадает с c-1-1. Значит, на чем оно основано? Поскольку это есть, будьте здоровы, модификация
[51:32.680 --> 51:41.760]  шенненовской пропускной способности, а для шенненовской пропускной способности и для
[51:41.760 --> 51:52.080]  взаимной информации есть некоторое свойство. Так, мне бы его еще вспомнить.
[51:52.080 --> 52:07.800]  Вот здесь я могу ошибиться, то есть это надо смотреть книжку Холева.
[52:22.080 --> 52:29.120]  Тут где-то может условность стоять, я вот не помню, но это уже для доказательства этого утверждения.
[52:29.120 --> 52:37.360]  При этом надо мне сделать еще некоторую оговорочку. Когда мы считаем вот эту взаимную
[52:37.360 --> 52:46.320]  информацию, мы считаем ее в битах в пересчете на вот количество символов, которые используется.
[52:46.320 --> 52:56.480]  Смотрите, что получается, что здесь в этом сценарии перепутанность никак не помогает,
[52:56.480 --> 53:07.440]  если вы измеряете индивидуально. Теперь следующий сценарий, вот он как раз уже нетривиальный.
[53:07.440 --> 53:28.560]  С Бесконечность это не меньше, чем СН. Сценарий 3. В нём, в этом сценарии вы как раз таки
[53:28.560 --> 53:35.440]  свойства 1 оставляете, то есть кодируете каждую букву по отдельности. А что меняете?
[53:35.440 --> 53:42.760]  Меняете вид измерений. Вы не накладываете теперь условия, что вы каждую частицу по
[53:42.760 --> 53:48.320]  отдельности измеряете, а вы можете подождать, пока они к вам придут, и измерить их целиком, коллективно.
[53:48.320 --> 54:02.320]  То есть рисуем такую картинку. В состоянии на входе у вас кодируют отдельные буквы,
[54:02.320 --> 54:18.080]  потом они проходят через канал без памяти, получаем зашумленные версии этих букв. Но измерять
[54:18.080 --> 54:30.160]  мы их все будем, коллективное измерение. Почему здесь может быть выигрыш? За счет того,
[54:30.160 --> 54:38.520]  что у вас есть условно-типичные подпространства, и вы можете измерять вот в этих условно-типичных,
[54:38.520 --> 54:47.640]  то есть брать проекторы на эти условно-типичные подпространства. Что значит коллективное измерение?
[54:47.640 --> 55:00.280]  Значит у вас общего вида POVM просто для n-частиц на выходе. В этом случае вот этот
[55:00.280 --> 55:10.440]  supremum и x, y, что вы опять можете варьировать? Вы можете варьировать ансамбль, но е,
[55:10.440 --> 55:19.800]  у теперь у вас общего вида без ограничений, общего вида. Вы получите так называемую
[55:19.800 --> 55:30.480]  c1-бесконечность, пропускную способность канала phi, и эта c1-бесконечность, пропускная способность,
[55:30.480 --> 55:50.160]  она равна chi величине. Чего? Какого ансамбля? Давайте supremum надо написать. Supremum по ансамблю
[55:50.160 --> 56:00.480]  только px rho x, chi величина, ансамбль тот же самый, а вот здесь стоит phi действуя на rho x.
[56:00.480 --> 56:14.760]  Так, не очень мелко, видите, что пишу. Ну что такое chi величина? Это энтропия среднего,
[56:14.760 --> 56:24.960]  минус средней энтропии. Получается, что вот эта c1-бесконечность задается другой формулой уже,
[56:24.960 --> 56:35.720]  граница Холева достигается вот в этом пределе, когда длина слова становится бесконечной. Достигается
[56:35.720 --> 56:49.480]  граница Холева, это результат 96-го года. Холева и независимо Вестмарленд и Шумахер,
[56:49.480 --> 57:05.040]  который я уже упоминал, но не тот, который в формуле 1. Они чуть попозже, но по-другому доказывали,
[57:05.040 --> 57:12.400]  по-моему 97-го года, кстати. Вот, поэтому этот результат про это равенство еще называется теоремой
[57:12.400 --> 57:21.480]  Холева Вестмарленда Шумахера. Вот что он говорит. То есть коллективные измерения могут помочь.
[57:21.480 --> 57:29.040]  Почему? Потому что вот эта величина уже может быть больше, есть такие примеры, когда она больше,
[57:29.040 --> 57:40.000]  чем c1-1, который в свою очередь совпадает с c-бесконечностью. То есть если вы делаете
[57:40.000 --> 57:45.760]  коллективные измерения, то вы можете получить результат получше.
[57:45.760 --> 57:58.720]  C-шенона, она же определяется, вот классическая пропускная способность классического канала,
[57:58.720 --> 58:06.920]  определяется только стахастической матрицей переходов. А в качестве этой стахастической
[58:06.920 --> 58:17.320]  матрицы переходов берем вот такую формулу, правило Борона. Вероятность получить исход при y,
[58:17.320 --> 58:35.560]  при условии, что на входе было вот такое состояние. Да-да-да, но на практике это обычно выполнено.
[58:35.560 --> 58:58.360]  Да, кстати, на практике. По распределениям на входе. А здесь вы берете распределение не
[58:58.360 --> 59:08.520]  только px, но еще тех состояний, в которые вы канируете, и еще тех измерений, которые вы используете.
[59:08.520 --> 59:16.520]  Ну поэтому нетривиальный результат, что вот эти е и у, вот здесь формуле справа уже не участвует,
[59:16.520 --> 59:24.120]  они ушли оттуда. Так, теперь у нас совсем чуть-чуть времени, наверное, остается. Сейчас я посмотрю,
[59:24.120 --> 59:35.600]  сколько. Да, немножко. Будем двигаться уже в 21 век, потому что это еще прошлый. Четвертый сценарий.
[59:35.600 --> 59:52.840]  Передача информации. Вы теперь можете использовать блокчное кодирование. Сейчас объясню, в чем его
[59:52.840 --> 59:59.920]  суть. Я уже чуть-чуть в сценарии 2 здесь говорил, но очень кратко, поскольку там нет выигрыш.
[59:59.920 --> 01:00:14.120]  Вот смотрите, что происходит, если вы будете два, две частицы рассматривать как одно целое,
[01:00:14.120 --> 01:00:24.680]  но пропускать их каждую через один и тот же канал phi. Вы можете эту картинку, вот эту картинку,
[01:00:24.680 --> 01:00:32.120]  развернуть и представить в тождественном виде вот так вот. Смотрите, я поворачиваю на 90 градусов
[01:00:32.120 --> 01:00:46.960]  свою систему, но тогда каждый из этих частиц пройдет через канал phi. Просто нарисовал
[01:00:46.960 --> 01:00:53.200]  эквивалентную схему. Каждый из частиц проходит через канал phi. На выходе какое-то состояние
[01:00:53.200 --> 01:00:59.400]  получится. Если шума нет, то то же самое, если шум есть, то может оно перестать быть перепутанным,
[01:00:59.800 --> 01:01:06.600]  Теперь как нам написать вот этот весь канал целиком? Ведь это же тоже некоторое преобразование.
[01:01:06.600 --> 01:01:20.160]  Пишут его очень просто. Все в порядке у нас тут. Phi tensor на 2 степени. Потому что phi действует в
[01:01:20.160 --> 01:01:26.200]  первом пространстве Гильбертовом, phi действует во втором одинаково, phi tensor на 2. Теперь вы
[01:01:26.200 --> 01:01:33.640]  можете что делать? В качестве своих букв брать уже вот такие вот состояния двух частиц. Это
[01:01:33.640 --> 01:01:41.640]  будет блокченоэкодирование. Выбирать вероятность появления таких букв. При этом вы что получите?
[01:01:41.640 --> 01:01:58.720]  Вы получите в таком сценарии C. Давайте напишем 2 бесконечности. Если вы измерение общего вида
[01:01:58.720 --> 01:02:05.680]  делаете. Сейчас нарисую. Вот измерение общего вида. Это означает, что здесь вы делаете измерение
[01:02:05.680 --> 01:02:19.920]  общего вида. Вот так. Здесь вы, варьируя вот это pVM и вот эти состояния, получите C2 бесконечности.
[01:02:20.480 --> 01:02:30.440]  Как написать эту C2 бесконечности в виде формулы? Это будет у вас, смотрите что. Сейчас напишем
[01:02:30.440 --> 01:02:46.400]  аккуратненько. C1 бесконечности для phi tensor на 2. Пока сейчас понятно. Здесь я напишу просто
[01:02:46.400 --> 01:02:57.560]  канал phi. Оставьте еще место. Сейчас чуть подправим эту формулу. C2 бесконечности phi равно C1
[01:02:57.560 --> 01:03:10.080]  бесконечности phi tensor на 2. Но вы хотите считать эту пропускную способность по количеству частиц,
[01:03:10.080 --> 01:03:17.400]  которых вы посылаете через канал. То есть считать реальные посылки через канал. А здесь вы посылаете
[01:03:17.400 --> 01:03:23.560]  один раз, второй раз. То есть у вас две посылки через канал идут. Вот эту величину нужно поделить
[01:03:23.560 --> 01:03:51.640]  нам. Думаем пока я стираю. Эдуард будет задавать вопрос. Объясню на формуле. Вот представьте,
[01:03:51.640 --> 01:04:00.280]  что шума нет. Phi тождественный канал. Какая у нас там была формула для вот этого C1 бесконечности?
[01:04:00.280 --> 01:04:12.040]  Это была phi-величина, ее максимум. У phi-величины максимум мы с вами вспоминали. Это логарифма
[01:04:12.040 --> 01:04:19.320]  размерности пространства. А в каком пространстве действует phi tensor на 2? Он берет на вход
[01:04:19.320 --> 01:04:26.080]  операторы в тензорном произведении Гильбертова пространства и переводит их в операторы такие.
[01:04:26.080 --> 01:04:41.960]  У h размерность D. И получается, что если ваш канал phi тождественный, если phi равняется тождественному
[01:04:41.960 --> 01:04:55.920]  каналу, то вы получите логарифм размерности этого пространства. А это есть D квадрат. Это два логарифма D.
[01:04:55.920 --> 01:05:07.440]  Понимаете? Откуда двойка? Из-за того, что размерность пространства увеличилась.
[01:05:07.440 --> 01:05:20.320]  Поэтому если я хочу считать по отношению к исходным частицам, сколько пересылок было сделано
[01:05:20.320 --> 01:05:28.560]  через канал, то я должен поделить на вот эту степень, которая стоит вот здесь в степени канал.
[01:05:28.560 --> 01:05:40.880]  Понятно? Вот из-за чего это двойка. Теперь вы можете сказать, а давайте-ка я рассмотрю предел
[01:05:40.880 --> 01:06:01.640]  При n, стремящемся к бесконечности, 1 делить на n, c1 бесконечность, phi тензорно n. И эту
[01:06:01.640 --> 01:06:15.160]  величину назову c бесконечность бесконечности. И это есть истинная пропускная способность.
[01:06:15.160 --> 01:06:29.800]  Давайте напишем классическую пропускную способность квантового канала.
[01:06:29.800 --> 01:06:47.960]  Понятно? То есть вы допускаете блочное кодирование неограниченной длины. Вот это вот максимум из того,
[01:06:47.960 --> 01:06:55.840]  что вы можете выжать для передачи классической информации используя кодирование в квантовое
[01:06:55.840 --> 01:07:09.600]  состояние. Теперь смотрите, как думали люди в 20 и 21 веке. Давайте найдем пример такого канала,
[01:07:09.600 --> 01:07:19.840]  для которого будет вот эта величина больше, чем просто c1 бесконечность. Вот так. Короче,
[01:07:19.840 --> 01:07:28.920]  чтобы сценарий 4 был лучше, чем сценарий 3. Понятно? Чтобы блочное кодирование помогало. Искали-искали.
[01:07:28.920 --> 01:07:40.000]  Но смотрите, здесь-то вы в сценарии 3 кодируете в индивидуальные состояния, а в сценарии 4 вы
[01:07:40.000 --> 01:07:48.320]  можете кодировать в перепутанные состояния. Вопрос заключается в том, помогает ли это перепутанность
[01:07:48.320 --> 01:08:06.280]  на входе для того, чтобы передавать больше информации. Искали-искали. Сначала поняли,
[01:08:06.280 --> 01:08:13.120]  что не помогает для определенного класса каналов, которые называются там деполяризующие каналы
[01:08:13.120 --> 01:08:20.480]  произвольного размера. Потом сказали, а у нас же компьютер есть, давайте в машине загрузим, пусть она
[01:08:20.480 --> 01:08:27.320]  решает эту оптимизационную задачу. Нам не нужно вот этот прям предел, давайте какое-то n возьмем,
[01:08:27.320 --> 01:08:33.760]  ну например вот эту двойку. Если покажем, что c2 бесконечность больше, чем c1 бесконечность,
[01:08:33.760 --> 01:08:42.280]  ухи, то уже хорошо. То есть перепутанное из двух частиц помогает. Искали на компьютере, не нашли.
[01:08:42.280 --> 01:08:49.720]  Для кубитных каналов. И возникла гипотеза. Гипотеза агитивности.
[01:08:49.720 --> 01:09:12.160]  Заключается она в том, что c, phi, tensor, другой любой канал возьмите, phi. Так, давайте теперь
[01:09:12.160 --> 01:09:29.520]  индекс поставим. Значит 1 бесконечность равно c1 бесконечность, phi, плюс c1 бесконечность, phi.
[01:09:29.520 --> 01:09:37.840]  Si это канал. Если в качестве этого psi вставите phi, то получите просто здесь 2 c1 бесконечность,
[01:09:37.840 --> 01:09:54.240]  и вот эту формулу. Там будет раньше тогда. Если она верна, то тогда у вас получится,
[01:09:54.240 --> 01:10:07.000]  что cn бесконечность, phi, tensor на n будет 1 делить на n, c1 бесконечность, phi, tensor на n,
[01:10:07.000 --> 01:10:19.160]  и это будет просто c1 бесконечность, phi. Вот. Значит, сейчас посмотрю еще время. Там интересная
[01:10:19.160 --> 01:10:25.560]  история. Вот мало времени. Про те каналы, для которых она выполняется, поговорим в следующий раз.
[01:10:25.560 --> 01:10:40.240]  Но что самое интересное? Самое интересное заключается в том, что в 2009 году Hastings все-таки
[01:10:40.240 --> 01:10:58.400]  нашел Hastings. Нашел конструкцию, для которой c2 бесконечность, phi, больше, чем c1 бесконечность,
[01:10:58.400 --> 01:11:16.360]  то есть перепутанность на входе помогает передавать больше информации. Про эту конструкцию стоит
[01:11:16.360 --> 01:11:24.280]  поговорить отдельно. Вот сколько успеем до звонка, столько поговорим. Как она была сделана? Эта
[01:11:24.280 --> 01:11:44.280]  конструкция вероятностная. То есть с вероятностью 1 существует канал phi такой что. Но сам канал
[01:11:44.280 --> 01:11:56.280]  представлен трудящимся небом. Значит, где существуют такие каналы, вернее, какие конструкции?
[01:11:56.280 --> 01:12:03.960]  Размерность пространства должна быть большой. Число операторов Крауса должно быть большим,
[01:12:03.960 --> 01:12:18.280]  но приблизительно в корень, если я правильно помню, из размерности пространства. Дальше вы
[01:12:18.280 --> 01:12:32.680]  пользуетесь некоторым аппаратом теории вероятности. Этот канал он random unitary. Сейчас напишу формулу
[01:12:32.680 --> 01:12:46.000]  для него. То есть вид этого канала, как он строится, он известен. То есть это 1 делить на n, сумма по i от
[01:12:46.000 --> 01:13:04.800]  доводу n, u, i, t, rho, u, i, t, крестик, где u, i, t это унитарные преобразования. Как вы понимаете,
[01:13:04.800 --> 01:13:17.680]  количество этих уиток будет определять ранг Крауса. n это ранг Крауса, а вот размерность самого
[01:13:17.680 --> 01:13:26.480]  пространства Гильбертова это другая величина d. Вам нужно и d делать большим, и n делать большим.
[01:13:26.480 --> 01:13:33.000]  То есть и n во много раз больше единицы, и d во много раз больше единицы. Но между ними есть
[01:13:33.000 --> 01:13:44.520]  определенная связь. И вот используя такую конструкцию, Хассингс смог показать, что с вероятностью
[01:13:44.520 --> 01:13:52.560]  один существует такой канал, для которого это нарушается. Если кто-то представит канал в явном
[01:13:52.560 --> 01:14:05.040]  виде, то он будет молодец, конечно. Хорошая статья будет. Это была в nature physics. В общем,
[01:14:05.040 --> 01:14:13.880]  гипотеза аддитивности опровергнута, но в явном виде написать канал phi я вам не могу. Задачи
[01:14:13.880 --> 01:14:19.280]  для будущих поколений. Все, спасибо, встретимся через неделю.

[00:00.000 --> 00:11.240]  Итак, внезапно начинает прошлого занятия, мы с вами уже знакомы чуть-чуть совсем капельку с
[00:11.240 --> 00:16.320]  PyTorch, поэтому на PyTorch мы с вами будем обращать внимание регулярно, и, собственно, ваша третья
[00:16.320 --> 00:23.040]  домашка, которая вам сегодня будет доступна, будет также зависеть от PyTorch. По факту мы сейчас
[00:23.040 --> 00:29.120]  начнем делать вашу третью домашку. В каком смысле? Сейчас мы с вами SVM потыкаем палочкой просто в коде
[00:29.120 --> 00:35.200]  и реализуем его. Он за вас реализован, разберем на PyTorch, а затем дома вам нужно будет
[00:35.200 --> 00:41.000]  краткий ядро реализовать, чтобы с ним было понятие. И, на всякий случай, я бы еще хотел обратить ваше внимание
[00:41.000 --> 00:49.240]  какое-то конкретное ядро. По-моему, там вам RBF-ядро радиальных байдсовых функций надо будет
[00:49.240 --> 00:56.520]  сделать. Что я здесь хотел бы сказать дополнительно. Во время перерыва, как раз-таки, ко мне тут
[00:56.520 --> 01:01.560]  коллеги ваши подходили с вопросом на всякий случай. То есть, еще раз, собственно, функция называется
[01:01.560 --> 01:06.720]  ядром, возвращаемся к SVM. Если она представима вот в таком виде, при некотором отображении из
[01:06.720 --> 01:11.880]  исходного пространства в какое-то целевое гильбертовое пространство, то есть пространство
[01:11.880 --> 01:16.920]  со скалярным произведением. При этом вот этот самый переход нам зачастую-то не очень важен. Почему?
[01:16.920 --> 01:22.760]  Потому что нам не нужен сам переход, нам нужно подходящее ядро. И про это еще в 909 году была
[01:22.800 --> 01:28.040]  доказана теорияма Мерсера, которая, кратцкий говорит, что функция является ядром тогда и только
[01:28.040 --> 01:32.960]  тогда, когда она симметрична и не отрицательно определенна. То есть любые симметричные не
[01:32.960 --> 01:37.420]  отрицательно определенные функции, которые могут быть представлены, то есть функция двух векторов,
[01:39.420 --> 01:45.560]  могут являться ядром. Поэтому нам не обязательно представлять какое именно отображение в целевое
[01:45.560 --> 01:49.880]  пространство есть, нам это не столь важно. Нам важно, чтоб мы знаем какое ядро. Ядро ввести
[01:49.880 --> 01:54.480]  проще, чем перейти в кутеновый пространство. Понятно?
[02:01.960 --> 02:04.960]  Что?
[02:04.960 --> 02:22.000]  Да, по-моему, все-таки оно должно быть гильбертовым. Давайте, там-там-там.
[02:22.000 --> 02:34.440]  Слушайте, по-моему, все-таки полнота там тоже должна быть. Я, скажу так, с Вэмом я
[02:34.440 --> 02:41.200]  пользовался последний раз в году, в 2018, теорему эту читал вот сейчас, так что я лучше это потом
[02:41.200 --> 02:46.160]  почитаю, видимо, дома, и вам, на всякий случай, потом скажу. Как видите, как бы, я сам не все наизусть
[02:46.160 --> 02:52.480]  помню, это нормально. Многие вещи, они просто-напросто за неиспользованием начинают чуть-чуть отваливаться.
[02:52.480 --> 02:59.960]  Но, на всякий случай, вот пример ядер, которые работают. Константа – это ядро. Бесполезное,
[02:59.960 --> 03:05.400]  но ядро. Произведение любых двух ядер тоже является ядром, потому что это две
[03:05.400 --> 03:11.200]  неотрицательные функции. Для любой функции, собственно, вот такой, это тоже ядро. Ну и так
[03:11.200 --> 03:17.480]  далее. Любая линейная комбинация работает. Фурье образ можно также взять. Почему нет преобразования
[03:17.480 --> 03:24.400]  фурье? Привет! Но по факту, вот этим всем занимаются в крайне редких случаях. На практике,
[03:24.400 --> 03:29.600]  я лично, вот именно, какую-то подбору ядра через фурье образы за всю жизнь не видел ни разу вообще.
[03:29.600 --> 03:36.480]  Так что думаю, это не совсем то, что на практике применяется. Ладно, давайте тогда сюда вернемся.
[03:36.480 --> 03:44.880]  Вам нормально видно, или побольше сделать? Нормально? Хорошо. Ну, давайте запилим маленький
[03:44.880 --> 03:50.480]  SVM непосредственно на Pytorch. На всякий случай звук, видео, все идет. Правильно, товарищ оператор?
[03:50.480 --> 03:59.120]  Все, спасибо. Внизу там зеленая штука прыгает, что я говорю. Хорошо. Давайте построим какую-нибудь
[03:59.120 --> 04:04.520]  выборочку, которая линейно-разделимая. Проиллюстрируем все то, что мы с вами разбирали.
[04:04.520 --> 04:18.640]  Сейчас немножко проснется. О, выборочка появилась. Мы с вами пользуемся скалерным,
[04:18.640 --> 04:25.480]  как мы и заявляли, в основном, чтобы картинку рисовать. Вот у нас выборка из двух кучек. Кучка
[04:25.480 --> 04:39.280]  желтых и кучка... В нынешних реалиях, я предпочту изменить. Симап? Не, слушайте, никаких этих, но...
[04:39.280 --> 04:55.040]  Образование вне политики, я так скажу. По крайней мере постараемся. Вот. Да и любой, на самом деле,
[04:55.040 --> 05:01.520]  выберите. Вы можете ввести любой, он у вас выпадет с ошибкой со словами. Вы не правы,
[05:01.520 --> 05:09.840]  есть вот такие. Очень удобно. О, кстати, есть очень классный этот. Отум, кажется, он называется.
[05:14.760 --> 05:15.800]  Красный на улице осень.
[05:15.800 --> 05:25.560]  Вот. Я думаю, сейчас проблем у нас никаких нет, я надеюсь. Короче, вот у нас две кучки. Давайте
[05:25.560 --> 05:34.440]  реализуем ровно вот эту самую функцию и будем краски ее минимизировать. Хорошо? Ее можно решать
[05:34.440 --> 05:41.240]  с помощью либо субгренитов методов, либо с помощью решения двойственной задачи оптимизации,
[05:41.240 --> 05:48.160]  привет, собственно, решение задач с ограничениями. И здесь мы краски вместо подсчета вот этого ядра,
[05:48.160 --> 05:51.960]  который по умолчанию просто-напросто скалярное произведение, мы с вами можем подобрать любое
[05:51.960 --> 06:01.440]  другое ядро, которое мы с вами только что обсуждали. Согласны? Хорошо. Как оттуда сюда перейти,
[06:01.440 --> 06:07.720]  смотрите, на всякий случай, даже с использованием ядра вы можете перейти точно также к двойственной
[06:07.720 --> 06:13.080]  задаче, просто-напросто в оригинальной форме, и решать ее градиентными методами. Это не совсем
[06:13.080 --> 06:18.600]  тривиально сделать. В общем случае, об этом целая статья написана, на нее можно сослаться,
[06:18.600 --> 06:25.400]  ее можно почитать. PageNotFound замечательно. Нельзя почитать, найдем новую ссылку, раньше работала.
[06:25.400 --> 06:31.080]  Возможно, она не работает с наших айпишников по, опять же, всем понятным причинам. Но ничего.
[06:31.080 --> 06:37.520]  Давайте реализуем СВМ, но используя СГД, то есть без решения двойственных задач.
[06:37.520 --> 06:45.880]  Во-первых, тут у нас сидит hingeLoss. Давайте реализуем hingeLoss, нам нужно посчитать hingeLoss.
[06:45.880 --> 06:53.200]  Как мы можем это сделать? На вход нам пришли лейблы. Лейблы — это истинные значения. И
[06:53.680 --> 07:02.680]  скоры. Что такое скор? Я думаю, все помнят. Я не помню. А чего?
[07:05.680 --> 07:07.840]  Простите, говорите, пожалуйста, погромче, вас не слышно.
[07:15.960 --> 07:19.600]  Все, тишина. Ну ладно.
[07:23.200 --> 07:29.640]  Пришел у нас батч объектов. Скоро, видимо, это то, что наша моделька предсказала.
[07:29.640 --> 07:40.920]  Соответственно, нам нужно с вами посчитать, во-первых, кого? Scores на labels и, соответственно,
[07:40.920 --> 07:48.000]  1 минус вот эта штука. И здесь у нас, я не помню, какого они типа, но пусть будет есть такая функция,
[07:48.000 --> 08:03.160]  например, torch clamp, кажется, называется. Да, clamp в нужный нам range. Ну вот, давайте проверим,
[08:03.160 --> 08:14.400]  соответственно, 1, 2, 3, 4, 5. Соответственно, будем его отображать в 2, 3.
[08:14.400 --> 08:41.760]  Ну хорошо. Вот как работает clamp, то есть он все что нужно нам обрезает. Мы совершенно спокойно
[08:41.760 --> 08:50.240]  можем сделать так, минус 1, минус 2, 3, 4, 5 и обрезать все то, что у нас краски меньше нуля.
[08:53.640 --> 08:59.440]  Видите, все отрицательные числа заменены на 0, все положительные остаются. Так что
[08:59.440 --> 09:12.560]  можем сделать вот так. Torch clamp, 1 минус Scores на labels, обрезаем нулю. Ну как-то так. В принципе, работает.
[09:12.560 --> 09:33.040]  Вот. Ну и разве что в данном случае, раз у нас кинжалоз, давайте еще это будем тогда усреднять.
[09:33.040 --> 09:51.000]  Вот пожалуйста, вот наш с вами. Что? А где норма? Так да, смотрите, первая часть это кинжалоз,
[09:51.000 --> 09:59.840]  вторая это норма. Вот нам пока только кинжалоз нужен. Все, вот мы его по идее посчитали. Ну
[09:59.840 --> 10:06.320]  собственно, вот у нас внутри сидит ядро. Ядро в простейшем случае линейное. У вас 2 массивчика,
[10:06.320 --> 10:12.920]  собственно, вам нужно перемножить их друг на друга. Скалярное произведение по парной посчитать.
[10:13.760 --> 10:15.520]  Хорошо? Как нам это сделать?
[10:23.600 --> 10:33.880]  Так, и чего? Кого умножаем? Получается Torch matmul. Наверное, x1 на кого? На x2.
[10:33.880 --> 10:43.640]  Ну у вас же матрица по идее, а? Нет, matmul это как раз таки скалярное произведение. Смотрите,
[10:43.640 --> 10:48.600]  у вас выборка подна две. В общем случае, вы можете ядро считать не на паре объектов, а сразу на
[10:48.600 --> 10:58.640]  двух подвыборках. Все по парной посчитать. Вот у вас x1, собственно, это что? Это array,
[10:58.640 --> 11:06.560]  короче, первая подвыборка, это вторая. По идее, как-то вот так должно работать.
[11:15.040 --> 11:19.160]  Так, бла-бла-бла, бла-бла-бла.
[11:19.160 --> 11:27.280]  Редикц корс, короче, один момент.
[11:27.280 --> 11:51.200]  Я с вашего позволения сейчас не буду изобретать велосипед, потому что сейчас кажется, что я
[11:51.200 --> 12:01.040]  что-нибудь напутал. Ну тут у нас все абсолютно правильно. x1 на x2t тоже правильно. Дальше я все,
[12:01.040 --> 12:13.400]  пожалуй, и копирую. От греха подальше. Я просто не тот ноутбук открыл, поэтому в нем пришлось
[12:13.400 --> 12:25.520]  что-то кудить. Вот смотрите, собственно, вот наше ядро. Пока что просто скалярное. Мы и пишем
[12:25.520 --> 12:32.200]  скалярное произведение. Один вектор на второй. И поехало теперь тело нашего свм. Во-первых,
[12:32.200 --> 12:36.320]  в конструкторе нам ничего не надо. У нас есть learning rate для нашего градиентного спуска.
[12:36.320 --> 12:41.280]  Количество эпох опять же для него. Размер батча, на котором будем это считать. Лямбда. Лямбда
[12:41.280 --> 12:48.880]  это краска. Та величина, которая у нас отвечает за коэффициент регуляризации. Кто у нас там дальше?
[12:48.880 --> 12:55.880]  Ядро. Если у нас никакого ядра не задано, то мы используем линейное. Или то ядро,
[12:55.880 --> 12:59.360]  которое вы сдадите в домашней краске, можете сдать другое ядро. Verbal просто,
[12:59.360 --> 13:03.040]  чтобы он нам что-то отдавал. И fit от flag, что мы его уже обучали или нет.
[13:03.040 --> 13:12.680]  И метод fit, соответственно. Вот у нас опять x, y. Привет, sklearn, лайк. У нас с вами есть беты.
[13:12.680 --> 13:19.800]  Это кто? Это у нас краски виса. И свободный член. Оптимизатор у нас будет классический sgd. И в принципе
[13:19.800 --> 13:25.720]  здесь обысказать нечего. Что тут хочется сказать? Во-первых, несмотря на то, что свм мы с вами решаем
[13:25.720 --> 13:29.720]  градиентным образом, мы с вами не будем считать градиент по всей выборке. Это логично. У нас
[13:29.720 --> 13:36.000]  выборка может быть размером миллион. Мы не хотим по всему миллиону объектов это дело считать.
[13:36.000 --> 13:42.920]  Поэтому что мы с вами делаем? Мы берем какую-то случайную под выборку. Вот она у нас. И
[13:42.920 --> 13:51.400]  соответственно теперь мы с вами считаем ядро между ними. И вот у нас краски kbatch на вектор весов,
[13:51.400 --> 13:57.200]  плюс свободный член. Вот наше с вами линейное отображение. У нас с вами линейный классикатор,
[13:57.200 --> 14:06.520]  как вы помните. И теперь соответственно наш лос это что? Это наше предсказание. Это наш хинж-лос.
[14:06.520 --> 14:12.520]  Вот он есть. Плюс как раз таки норма нашего вектора весов.
[14:12.520 --> 14:31.840]  В одну строчку, чтобы он вам делал случайно под выборку. Да, но обычно это пишут прямо в камере.
[14:31.840 --> 14:36.760]  Ну вот так. Обычно это если вам это нужно где-то в вашем коде, то вы пишете эту функцию в три
[14:36.760 --> 14:41.600]  строчки сами и потом ее используете. Баскалерни по-моему ее нет. Если она есть, я ее не пользуюсь,
[14:41.600 --> 14:49.040]  поэтому я ее не знаю. Ну короче мы с вами сделали ровно то, что у нас здесь написано. Вот мы с
[14:49.040 --> 14:55.440]  вами по сути переформулировали, точнее воспользовались наработками вот этой статьи. Я найду где новая
[14:55.440 --> 15:06.320]  ссылка на нее. Вот собственно наш новый f от x, краски beta на kx. Вот мы с вами исключительно
[15:06.320 --> 15:15.160]  вот это переписали в виде нашего только что кода. Вот наша соответственно хинж-лос, а вот наша
[15:15.160 --> 15:18.960]  вторая норма вектора весов. Тут только ядро у нас еще появляется, потому что мы в другое пространство
[15:18.960 --> 15:27.160]  перешли. Ну и соответственно предсказываем мы его абсолютно в лоб. Вот наша по сути линейная
[15:27.160 --> 15:32.120]  моделька. Если у нас соответственно что-то положительное, то это один класс, что-то
[15:32.120 --> 15:40.080]  отрицательное, это другой класс. Вот можем его построить. Пожалуйста, лос у нас какой-то есть,
[15:40.080 --> 15:48.480]  не знаю, что он значит, он с попугаек, но при этом accuracy у нас 100%. Но логично, что у нас все просто,
[15:48.480 --> 15:52.440]  у нас линейно-разделимые выборки, svm гарантированно должен находить решение для линейно-разделимой
[15:52.440 --> 15:58.320]  выборки, то все понятно. Но при этом вы совершенно спокойно можете попытаться воспользоваться и
[15:58.320 --> 16:06.960]  линейно-неразделимой выборкой. Ну, например, давайте здесь. Что сделаем? Число эпох. Смотрите,
[16:06.960 --> 16:13.600]  что такое эпоха на всякий случай. Когда говорят про обучение медным градиентам спуска наших
[16:13.600 --> 16:19.400]  моделей, непонятно, сколько раз проходить градиентам. То, что вы можете менять размер бача,
[16:19.400 --> 16:24.520]  вы можете менять количество шагов, непонятно, что это такое. Велиткое понятие эпоха. Что такое эпоха?
[16:24.520 --> 16:31.000]  Одна эпоха — это количество парагонов, за которые мы покрываем столько объектов, сколько было во
[16:31.000 --> 16:35.640]  всей обучающей выборке. Так как у нас бачи, как правило, выделяются случайным образом, вы некоторые
[16:35.640 --> 16:39.720]  объекты возьмете несколько раз, некоторые вообще не возьмете, но в среднем вы покрыли всю выборку.
[16:39.720 --> 16:45.840]  То есть, если у вас бач размером 100, а выборка из тысячи элементов, за 10 проходов у вас пройдет
[16:45.840 --> 16:52.520]  одна эпоха. Понятно? То есть, просто это гораздо проще, чем каждый раз специфицировать и размер
[16:52.520 --> 16:59.000]  бача и количеству шагов. Так вы говорите, одна эпоха — значит, вы прошлись по условным, сколько объектов у вас было.
[16:59.000 --> 17:04.800]  Как правило, при обучении сложных моделей число эпоха считается десятками сотен. Это нормально.
[17:04.800 --> 17:13.280]  Давайте попробуем теперь сделать ситуацию чуть похуже. Сделаем пересекающимся наше множество.
[17:13.280 --> 17:19.680]  Вот, видите, теперь они у нас накладываются друг на друга. При этом у нас выборка перестала быть
[17:19.680 --> 17:27.080]  линией наразделимой, но нам абсолютно ничего не мешает точно также обращиться к своему. Как видите,
[17:27.080 --> 17:33.280]  0.94 он нас показал результат, и мы с ним совершенно спокойно можем с вами смотреть. Более того,
[17:33.280 --> 17:42.960]  мы с вами можем посмотреть на нашу модулу Betas. Вот те самые веса, которые наша модель выучила.
[17:42.960 --> 17:50.120]  Ну а теперь, собственно, это было зачем. Вот, можно на самом деле реализовать. Я понимаю, что мы с вами
[17:50.120 --> 17:54.680]  вроде как тут за 20 минут посмотрели на какой-то огромный кусок кода. Почему я его не разбираю построчно?
[17:54.680 --> 17:59.600]  Во-первых, разбирать код на семинаре скучно. Во-вторых, вам в любом случае придется разобраться в его
[17:59.600 --> 18:05.400]  интерфейсах в домашке, потому что вам придется для него ядро написать. Тут, в принципе, все абсолютно
[18:05.400 --> 18:09.400]  примененно. Мы взяли формулы, которые написаны выше из статьи, которые являются, собственно,
[18:09.400 --> 18:16.400]  градиентным видом для задачи своема, и их реализовали в коде. Все. Давайте теперь возьмем из
[18:16.400 --> 18:36.240]  escalern готовый svm и посмотрим, как он себя ведет. Давай. Что значит большая константа? Помните,
[18:36.240 --> 18:42.280]  у нас в формуле там 1 делить на 2c, правильно? Так что, если константа 1 на 10 десятой, то,
[18:42.280 --> 18:46.920]  просто-напросто, мы говорим, что 1 делить на c равен нулю. Круг говоря, на регулизацию мы забили.
[18:46.920 --> 18:50.600]  Ну или у нас ядро зависло, что тоже может быть.
[18:59.560 --> 19:04.280]  Уважаемый, проснись. Ну, ему плохо.
[19:06.240 --> 19:22.920]  Эскалерн, эскалерн. Мы же в тебя верили.
[19:28.280 --> 19:30.080]  У меня она влияет на то, что у меня все зависло.
[19:30.080 --> 19:52.320]  О, пожалуйста, все, построилось. Смотрите, тут еще у нас удобная функция есть,
[19:52.320 --> 19:57.280]  которая просто-напросто рисует поверхность, возразляющая для вашего классикатора. Логично,
[19:57.280 --> 20:02.200]  что она работает только для двумерного случая. Вот, как оно у нас выглядит. Смотрите,
[20:02.200 --> 20:08.440]  вот у вас константа какая-то одна, вы говорите, она не влияет. Ну, давайте попробуем поменять.
[20:08.440 --> 20:12.480]  Вот я взял 10 во второй, давайте я возьму c равно просто единиц.
[20:17.280 --> 20:24.880]  Чет визуально ничего не поменялся, правда. Много опорных векторов.
[20:27.280 --> 20:29.440]  Ну, давайте поглядим.
[20:57.280 --> 21:15.400]  Смотрите, вот если у нас будет линейно-разделимая выборка, вот у нас получилось все, что мы хотели
[21:15.400 --> 21:19.880]  увидеть. Вот наша полоса. Причем, заметьте, сколько у нас будет опорных векторов, мы на них
[21:19.880 --> 21:25.200]  можем явно посмотреть. Так как у нас выборка линейно-разделимая, у нас всего два барахных
[21:25.200 --> 21:31.680]  вектора. Крайний объект из одной точки и крайний объект из другой точки и с другой кучки. Согласны?
[21:31.680 --> 21:37.400]  Если, соответственно, мы с вами выборку сделаем более шумной, они как-то будут пересекаться,
[21:37.400 --> 21:51.520]  пусть меня там, не знаю, два, то, соответственно, они все еще не пересекаются, все еще должно быть два.
[21:51.520 --> 22:00.800]  Три опорных вектора, они, видимо, на одинаковом расстоянии оказались. Окей, случилось. Кластер std 4.
[22:13.120 --> 22:17.640]  Теперь у нас опорных векторов стало больше. Если c сделать,
[22:22.320 --> 22:23.200]  гораздо меньше.
[22:29.120 --> 22:34.560]  Вот, смотрите, я константу уменьшил, то есть сила регуляризации стала больше, полоса стала шире.
[22:34.560 --> 22:39.840]  Так что нет, она влияет. Если вы c просто увеличиваете от ста до миллиона, у вас она в знаменателе,
[22:39.840 --> 22:43.640]  еще раз напоминаю, если у вас знаменатель был 1 сотый или 1 миллион, это в принципе там уже
[22:43.640 --> 22:48.280]  умалая относительно функции потерь хинжелоса. Богично, что у вас гиберплоскость особо не меняется.
[22:48.280 --> 22:53.520]  Чем ближе вы будете c приближать к нулю, тем больше у вас будет сила регуляризации.
[22:53.520 --> 22:58.240]  Вот, есть, соответственно, опорных векторов, у нас теперь будет сильно больше.
[22:58.240 --> 23:04.720]  Ну и давайте теперь посмотрим на разные ядра. Вот простейший случай, линия неразделимой выборки.
[23:04.720 --> 23:09.240]  У вас концентрические окружности. Вы можете как угодно пытаться провести вашу прямую,
[23:09.240 --> 23:14.760]  ничего у вас не получится, потому что выборка линии неразделимая. Но при этом вот вам какая-нибудь
[23:14.760 --> 23:25.960]  функция, например, вот у нас ядро радиальное, можем к нему обратиться. Вот что у вас получается в итоге.
[23:25.960 --> 23:32.320]  Смотрите, радиальные базисные функции, вы применили краски преобразования к вашим данным,
[23:32.320 --> 23:37.960]  вот по сути ваше ядро, и вы можете увидеть, что теперь у вас все гораздо проще. При этом заметите,
[23:37.960 --> 23:44.200]  опять же, если вы сделаете константу поменьше, например, там просто один,
[23:48.200 --> 23:56.840]  ну ладно, это, видимо, надо рядом просто нарисовать. Вот один случай. Вот, у вас видите,
[23:56.840 --> 24:02.400]  гиперплоскость, у меня она вообще всех в себя включила, вы границы даже не видите на экране. То есть,
[24:02.400 --> 24:07.360]  у вас одна граница, на самом деле, где-то там в точке, другая по контуру. Или, что то же самое,
[24:07.360 --> 24:18.440]  вы возьмете константу большую, 1g3, например, и вот для него построите график. Видите,
[24:18.440 --> 24:23.600]  если соответственно константа большая, то вы видите, что у вас лишь часть объектов попала
[24:23.600 --> 24:30.640]  в разделяющую полушь, потому что она достаточно узкая. Понятно? Вопросы, пожелания, комментарии
[24:30.640 --> 24:36.160]  здесь есть? Это радиальные базисные функции, это ядро вот с такого вида.
[24:38.160 --> 24:40.880]  Но мы с вами только что его в слайдах разглядывали.
[24:40.880 --> 24:49.320]  Где-то там, sklearn, rbf, rnl.
[24:58.320 --> 24:58.640]  Нет.
[24:58.640 --> 25:10.440]  Хоть одну формулу он нам покажет? Нет, не покажет. Ладно, пойдемте вот сюда.
[25:28.640 --> 25:55.920]  Вот rbf, как выглядит. Вот оно. Так, окей, хорошо.
[25:58.640 --> 26:18.480]  А, вот здесь вы имеете в виду? Проще всего, наверное, поглядеть. Давайте попробуем. Вот rbf,
[26:18.480 --> 26:30.360]  вот наша x, y. Она здесь. У нас, наверное, константа какая-то очень маленькая. Пусть
[26:30.360 --> 26:38.520]  будет константа вообще единицы. Ну вот она вам, по сути, выделила. Вот у вас, опять же,
[26:38.520 --> 26:44.000]  разделяющая поверхность. Она, на самом деле, я подозреваю, что это замкнутая все равно штука,
[26:44.000 --> 26:48.160]  она должна быть замкнутая по определению. Но при этом вот у вас краски, множество точек,
[26:48.160 --> 26:51.720]  которые попали в полосу. Опять же, видите, полоса у вас здесь уже не совсем понятна.
[26:51.720 --> 27:01.200]  Рbf в среднем лучше, чем… Ну, во-первых, ее считать все-таки дороже, у вас экспоненты
[27:01.200 --> 27:11.560]  каждый раз считать. Во-вторых, верно ли, что она в среднем лучше? Если у вас выборка
[27:11.560 --> 27:16.080]  линейно-разделимая, наверное, хуже она не сделает. Но при этом линейно-разделимая выборка
[27:16.080 --> 27:20.720]  это детский сад. Если выборка какая-нибудь сложная, то она может вам что-нибудь очень странного
[27:20.720 --> 27:25.720]  сделать. Ну, например, сейчас, давайте проверим. Воскалерни все еще есть? To moons.
[27:25.720 --> 27:42.200]  Так вот, make moons. Давайте попробуем эти нарисовать две луны. Скажем так. Make moons.
[27:46.080 --> 28:11.080]  У меня там параметры. Samples, shuffle, noise, random state. Фактор нам нужен.
[28:16.080 --> 28:20.680]  Вот у нас, например, классический датсет, который показывает, как линейно-разделимый выборок из
[28:20.680 --> 28:25.920]  которой все модельки страдают. Давайте попробуем запихать сюда тот же самый rbf и посмотреть,
[28:25.920 --> 28:34.960]  что с ним произойдет. Слушайте, а неплохо. Да, rbf вообще хорошее ядром. Я не смог его сходу сломать.
[28:39.960 --> 28:44.880]  Ну, слушайте, неспроста, но является, наверное, одним из наиболее таких широко применимых на
[28:44.880 --> 28:49.800]  практике. У него, собственно, единственная проблема в том, что его правда дорого считать. Если вы
[28:49.800 --> 28:54.680]  посмотрите на имплементацию, вам придется, по сути, под капотом посчитать по парамескалерной
[28:54.680 --> 28:59.680]  произведении всех объектов между собой. Ну, точнее, ядро. А если у вас много объектов,
[28:59.680 --> 29:06.480]  вам еще и экспонента, то этого считать будет дороговато. Ну, вообще, оно меня прям... А?
[29:06.480 --> 29:14.880]  А, кстати, да. Давайте-ка еще возьмем, собственно, что? Давайте-ка сюда шума добавим,
[29:14.880 --> 29:19.760]  потому что здесь у нас, по сути, в спрямляющем пространстве вборка линейноразделима. А это,
[29:19.760 --> 29:27.160]  на самом деле, больно хорошо. Давайте-ка его пробуем. Что-то у нас шума маловато.
[29:27.160 --> 29:32.480]  Что-то у нас шума многовато.
[29:37.120 --> 29:44.760]  Я все еще не вижу этих лун вообще. Я пытаюсь подобрать, сколько нам его надо.
[29:53.720 --> 29:57.640]  Ну вот, что-то у нас там это уже... Давайте, 0,2, 0,25 будет.
[30:00.240 --> 30:05.760]  Вот какие-то взаимопроникающие кривульны у нас остались. Ну, давайте попробуем опять РБФ
[30:05.760 --> 30:14.960]  на него натравить. Неплохо. Честно признаюсь, неплохо. А если регуляризацию сделать побольше?
[30:14.960 --> 30:29.480]  Шикарное ядро. А? Нет, ну тогда у вас просто будет случайная выборка. Ну ладно,
[30:29.480 --> 30:33.600]  месиво так месиво. Обращайтесь, вот вам месиво. Хотя все еще видно, что справа,
[30:33.720 --> 30:44.280]  словно больше желтых, а слева больше красных. Не, ну а что? Он пытался.
[30:50.280 --> 30:57.040]  Ну и, собственно, на самом деле все то, что дальше, мы с вами уже руками чуть-чуть поковыряли.
[30:57.040 --> 31:04.720]  А именно, вот наша линия разделимая выборка, можно менять блобы. Вот С большой, С маленький.
[31:04.720 --> 31:10.920]  Соответственно, чем больше С, тем уже полоса. Опять же, это сделано исключительно в качестве
[31:10.920 --> 31:15.520]  реверанса оригинальной статье, где С стоял из знаменателя. Всем сейчас привычнее, конечно,
[31:15.520 --> 31:21.520]  что константа регуляризационная линейно влияет на степень регуляризации, а не наоборот, обратным
[31:21.520 --> 31:27.520]  образом. Поэтому в комментарии, на самом деле, рекомендация будет классическая. Читайте доки,
[31:27.520 --> 31:31.920]  потому что в доках написано, за что отвечает константа. А иногда бывает, вроде как, очевидно,
[31:31.920 --> 31:36.480]  что чем больше С, тем больше регуляризация. На деле нет, потому что так исторически сложилось,
[31:36.480 --> 31:45.720]  например. Будьте осторожны. Вот. Тут еще какие-то вопросы, комментарии, пожелания есть? Нет. Хорошо,
[31:45.720 --> 31:53.040]  я тогда предлагаю сходить во второй ноутбук. Только я пометую сразу о том, что у нас с вами всего
[31:53.040 --> 31:59.320]  25 минут. Предлагаю открыть решенную версию, разве что мне придется чуть-чуть поменять пути.
[31:59.320 --> 32:13.400]  СВД, practice, MLMID становится MLCourse, соответственно, 22FB, а, нет, 21FB.
[32:13.400 --> 32:35.520]  О, оно даже работает. Смотрите, что я тогда могу с этим сделать. Давайте я его тогда
[32:35.520 --> 32:42.080]  сейчас ad-hoc добавлю, а потом уже починю нормальной репозиторией.
[32:59.600 --> 33:04.920]  Так, смотрите, можете тогда сразу зайти сюда, вот у вас появится Solve, в нем можете жанкнуть,
[33:04.920 --> 33:12.080]  попадете вот сюда. Здесь разве что тоже нам придется ad-hoc поменять MLMID на MLCourse.
[33:12.080 --> 33:21.040]  А? Почему? Вам не помогло? Может, мне тоже не поможет.
[33:35.920 --> 33:46.960]  Так, у меня вроде все загрузилось. Смотрите, опять же, придется это дело переименовать.
[33:46.960 --> 34:06.840]  Так, на всякий случай, у кого не получилось эту штуку загрузить, в смысле, к палике?
[34:16.960 --> 34:33.040]  Магия. Чего-чего? Не, я в 22-м вроде поменял. Все работает, слава богу.
[34:33.040 --> 34:40.800]  Короче, смотрите, вот у нас картинка есть. Помним, что картинка это матрица, причем у нас картинка
[34:40.800 --> 34:46.920]  цветная, поэтому у нас с вами три, на самом деле, канала. У нас три матрицы картинка. У нас канал,
[34:46.920 --> 34:57.280]  отвечающий за красный цвет, синий цвет и зеленый цвет. РГБ. Мы с вами можем избавиться от среднего,
[34:57.280 --> 35:02.200]  как вам угодно. Давайте попробуем опять нарисовать.
[35:11.600 --> 35:15.360]  ПЛТ, Ким Шоу, Пэйс.
[35:32.520 --> 35:34.400]  Что-то, мне кажется, здесь опечатка.
[35:40.800 --> 35:42.800]  Что-то, мне кажется, здесь опечатка.
[36:11.800 --> 36:23.720]  Потому что это ось 2, это кто? Это ось каналов, то есть у нас с вами для каждой матрицы подельность
[36:23.720 --> 36:38.920]  вычисляется средняя. Что с тобой происходит?
[36:38.920 --> 37:04.400]  А, что? У вас нормально получается. Вот нормальная картинка.
[37:08.920 --> 37:20.040]  По идее, сейчас она должна точно так же. А, сейчас, погодите-ка,
[37:20.040 --> 37:31.000]  минус это поделить на STD. А, понятно, нам надо не на STD меня делить, простите, а на 255.
[37:31.000 --> 37:46.520]  А, сейчас, 255. Сейчас все скажу. Я сам немножко запутался.
[37:52.360 --> 37:55.720]  Это нам нафиг не надо.
[38:31.000 --> 38:38.000]  Ладно, хорошо убедил, я ни фига ничего не слышу.
[38:52.560 --> 38:53.240]  Согласен.
[40:31.000 --> 40:43.000]  Я пытался понять, что происходит, я наконец-то понял. Короче, вернусь назад, я зачем-то начал
[40:43.000 --> 40:49.120]  нормировать изображение, в данном случае нам напрочь не надо. Вот наша с вами изначальная
[40:49.120 --> 40:57.080]  картинка, вот она. Мы ее разбиваем на три канала с вами. У нас три канала, соответственно,
[40:57.080 --> 41:06.040]  нулевой, первый, второй, RGB, соответственно. И после этого мы с вами можем посчитать их,
[41:06.040 --> 41:11.960]  построить в качестве, как сказать, для каждой матрички по отдельности. Куда она все пропала?
[41:27.080 --> 41:44.200]  Давай. Делали, делали, нарисовали, убрали напрочь. Вот наши с вами, соответственно, три канала RGB.
[41:44.200 --> 41:50.640]  Можем короткие их попытаться нарисовать теперь, как это у нас выглядит для сингулярных значений,
[41:50.640 --> 41:54.680]  то есть для каждого канала, каждой матрички абсолютно по отдельности, как оно себя ведет.
[41:54.680 --> 42:00.360]  Вот, смотрите, у нас сингулярные значения, они отсортированы в порядке убывания, причем здесь
[42:00.360 --> 42:05.920]  у вас шкала логарифмическая. Как видите, всего у нас значений порядка, сколько там, чуть меньше,
[42:05.920 --> 42:15.480]  400. На самом деле, откуда взялась размерность 400? Я думаю, примерно понятно откуда. Вот она у нас,
[42:15.480 --> 42:22.040]  385. Это у нас меньшее размерство, что у нас матрица, ранга 385. Больше, чем ранга 385,
[42:22.040 --> 42:27.240]  мы с вами по логичным причинам получить не можем, согласны? Троковый и столцовый ранг матрицы
[42:27.240 --> 42:37.200]  совпадают. Соответственно, мы с вами раскладываем матрицу по K компонентам от 0 до 350. И видим,
[42:37.200 --> 42:43.000]  что у нас в среднем в районе 350 как раз такие сингулярные значения падают от порядка 1 до
[42:43.000 --> 42:49.000]  порядка 10-10, то есть там ничего больше важного не остается. Соответственно, по идее мы для того,
[42:49.000 --> 42:59.840]  чтобы хранить наше изображение о матричке, можем использовать не 385 строк, а 350 и уже почти
[42:59.840 --> 43:04.600]  ничего не потерять. Или использовать на самом деле еще сильно меньше и терять какую-то часть
[43:04.600 --> 43:10.720]  информации. Собственно, давайте к это и сделаем. У нас для этого как раз есть функция compress. Что
[43:10.720 --> 43:16.480]  такое compress? Мы просто-напросто берем и выкидываем часть информации из вот этого нашего латентного
[43:16.480 --> 43:24.720]  представления. То есть 1 и K элементов мы берем. Как это происходит? Как вы видите, чистой воды вот
[43:24.720 --> 43:33.800]  здесь мы с вами берем 1 и K элементов, все остальное мы с вами выкидываем. Вот у нас 1 и K векторов для
[43:33.800 --> 43:39.760]  наших матриц поворота артагональных, 1 и K компонент, 1 и K векторов для артагональной матрицы,
[43:39.760 --> 43:44.320]  так для всех трех каналов. Ну и давайте посмотрим, что у нас там происходит.
[43:58.960 --> 44:04.360]  Вот, смотрите, вот 350 компонент, все работает как надо, мы ничего не потеряли. Логично,
[44:04.360 --> 44:08.960]  мы по графику выше видели, что 350 компонент нам примерно хватает для каждого из цветов.
[44:08.960 --> 44:16.760]  Вот на 300 компонент начинают появляться какие-то шумы. Вот 250, еще хуже, 200, 150,
[44:16.760 --> 44:26.920]  150, уже совсем какое-то, 20. Вот у нас одна, одна главный компонент. На всякий случай,
[44:26.920 --> 44:31.120]  почему у нас с одной главной компонентой все равно какие-то вертикальные горизонтальные линии
[44:31.120 --> 44:42.800]  появились? Ваши доводы. Чего? Три даже матрицы у нас есть, раз. И два, у нас с вами эта одна
[44:42.800 --> 44:46.840]  компонента в себя включает все равно линейную комбинацию какой-то всех предыдущих компонентов.
[44:46.840 --> 44:50.800]  Поэтому, когда мы обратно разворачиваемся, мы для каждой компонент на самом деле что-то находим.
[44:50.800 --> 44:58.040]  Вот. И заметьте, коллеги, здесь можно обнаружить достаточно любопытную вещь. У нас, в принципе,
[44:58.040 --> 45:03.960]  по каждому из трех каналов деградация, скажем так, была практически одинаковая. Как вы думаете,
[45:03.960 --> 45:08.800]  почему? Но у нас же с вами три матрицы вообще независимо полностью разбиваются. Для красного,
[45:08.800 --> 45:15.520]  зеленого и синего канала. Как вы думаете, почему у нас качество почти везде падает равномерно по
[45:15.520 --> 45:24.080]  всем трем каналам? Мы везде используем к одинаковым и есть, на самом деле, вторая причина,
[45:24.080 --> 45:29.360]  которая достаточно простая. У нас картинка-то практически черно-белая. У нас либо все три
[45:29.360 --> 45:34.080]  компонента одновременно, все три канала одновременно 255 максимальное значение,
[45:34.080 --> 45:39.480]  либо одновременно 0 черное. У нас почти везде черно и белое. У нас вот не белое,
[45:39.480 --> 45:45.800]  только кусок галстука и membase.com. Древние времена. Да.
[45:45.800 --> 46:08.440]  Ну, я так скажу, что сжатием без потери-то назвать можно, только если вы откидываете.
[46:08.440 --> 46:27.960]  Вам нужно тогда промежуточное представление сохранять. То есть, смотрите, вот у вас при
[46:27.960 --> 46:36.200]  построении, вот вы построили свд. Видите? И здесь мы с вами полные матрицы. Здесь мы с
[46:36.200 --> 46:42.120]  вами отбрасываем все до катего минора. По сути, если вот здесь мы с вами сохраним только первые
[46:42.120 --> 46:49.160]  к-векторов отсюда, первые к-векторов отсюда и первые к-векторов отсюда, вот тогда у вас объем
[46:49.160 --> 47:01.640]  занимаемый будет меньше. Картинка хранит вам, по сути, маску. Это у вас какое-то будет внутреннее
[47:01.640 --> 47:04.960]  представление, чтобы получить картинку, вам потом их придется между собой перемножить.
[47:04.960 --> 47:21.880]  По сути, когда вы открываете картинку на компе, у вас также прогоняется рандомная
[47:21.880 --> 47:28.800]  программа, которая умеет, например, в кодеке JPEG. Существующие программы, скорее всего, в это не умеют,
[47:28.800 --> 47:33.680]  потому что так картинки обычно не сжимают, это сильно менее эффективно, чем, опять же, тот же самый JPEG
[47:33.680 --> 47:39.560]  или lossless JPEG и так далее. То есть это, скажем так, картинки так сжимать плохая идея, на самом деле,
[47:39.560 --> 47:45.720]  по достаточно простой причине, потому что здесь мы с вами работаем именно что с матрицей. Матрица у нас,
[47:45.720 --> 47:49.760]  в общем случае, поддается такому разложению. Картинка она, на самом деле, обладает достаточно
[47:49.760 --> 47:54.800]  большим количеством свойств специфичным именно картинки. Например, у вас есть свойства локальности,
[47:54.800 --> 47:59.760]  что пиксель зависит от соседних пикселей сильно. В матрице, в общем случае, такого нет, у вас может
[47:59.760 --> 48:08.520]  что угодно быть. Ну и соответственно, можно тоже самое попытаться сделать с какой-нибудь другой картинкой.
[48:08.520 --> 48:14.160]  Например, здесь у нас, по-моему, в качестве картинки используется фотография МФТИ. Вот она тут
[48:14.160 --> 48:21.680]  сейчас будет несколько. Надо было перед тем, как я вам начал отвечать. Ну что-то лень как-то ждать.
[48:21.680 --> 48:31.640]  Ну хотя почему бы не увеличить? Пусть раньше у нас будет по 10 компонентов добавлять.
[48:31.640 --> 48:38.040]  Вот вместо двух минут стало 20 секунд, даже меньше.
[48:52.680 --> 48:59.040]  Странный хвост. А, кажется, я знаю, почему у нас странный хвост.
[48:59.040 --> 49:22.160]  Сейчас я пытаюсь понять, пофиксил ли я. Дайте мне один момент.
[49:29.160 --> 49:41.160]  Во, все.
[49:41.160 --> 49:52.160]  Сейчас.
[49:59.160 --> 50:07.160]  Норм. Этого минус этого.
[50:07.160 --> 50:15.160]  Какой-то маркобисие.
[50:29.160 --> 50:47.160]  Так, слушайте, окей. Я не понимаю. Тут что-то явно с нормировкой не то. Я не понимаю, почему вот этот хвост себя так ведет.
[50:47.160 --> 50:55.160]  Но, собственно, что можно увидеть? У нас с вами относительно ошибка падает. Ошибка восстановления изображения.
[50:55.280 --> 51:02.280]  Видите, относительно общего, скажем так. Что происходит? Мы с вами по-пиксельно считаем, насколько
[51:02.280 --> 51:07.280]  восстановлено изображение отличается от того, что было изначально. И нормируемся просто-напросто,
[51:07.280 --> 51:12.280]  так как у нас с вами по-пиксельно, кругово говоря, квадратичное отклонение, мы также нормируемся на
[51:12.280 --> 51:17.280]  изначальное, по сути, стд изначальное изображение по-пиксельно. Получается, что у нас ошибка
[51:17.280 --> 51:24.080]  примерно на 320 компонентах сваливается примерно в 0, как видите, для изображения. При этом
[51:24.080 --> 51:30.280]  логично предположить, что при увеличении ранга нашего латентного представления линейно растет
[51:30.280 --> 51:34.480]  изнимаемое пространство, потому что у нас просто-напросто ранг матрицы растет линейно. Количество
[51:34.480 --> 51:40.200]  векторов, количество нулевых элементов диагональных растет линейно. По сути, отсюда тоже можно видеть,
[51:40.200 --> 51:44.960]  что в какой-то момент у нас есть какой-то оптимум поименной эффективности. Другой стороны, не очень
[51:44.960 --> 51:51.200]  понятно, насколько полезно вообще так делать для сжатия изображений. Опять же, это не алгоритм
[51:51.200 --> 51:55.680]  сжатия изображений, которые используются во всяких кодеках. Почему мы это здесь добавили? Причина
[51:55.680 --> 52:01.120]  простая. Показывать это на произвольных матрицах, будь то какие-то датасеты без обучения
[52:01.120 --> 52:06.320]  соответствующих моделей, странно и долго. Если это показывать просто на произвольных матрицах с
[52:06.320 --> 52:10.920]  точки зрения, смотрите, мы СВД построили, то вам эта матрица как ничего не значил, так ничего не
[52:10.920 --> 52:16.360]  значит. Матричка как картинка при восстановлении мы глазами можно посмотреть и понять, что стало хуже,
[52:16.360 --> 52:32.000]  стало лучше. Понятно? Вот, можно сравнить два изображения и между собой их сравнить,
[52:32.000 --> 52:37.640]  посмотрев насколько разница восстановления одного и другого, потому что у них априори, скажем так,
[52:37.640 --> 52:42.760]  структура разная. Здесь много мелких деталей, здесь наоборот больше каких-то однородных цветов и так
[52:42.760 --> 52:53.440]  далее. Давайте попробуем их, например, сжать на 100 компонент оба. Заметьте, в фистех вроде как
[52:53.440 --> 52:59.320]  почти не пострадал, вот здесь только сверху слева какие-то артефакты появились, и вот здесь справа
[52:59.320 --> 53:04.040]  в основном почти полное восстановление, то есть визуально. При этом правая картинка, как вы видите,
[53:04.040 --> 53:24.680]  гораздо больше пострадало, в ней гораздо больше деталей утеряно. Я не понял. Да, конечно, вот они.
[53:24.680 --> 53:34.680]  То есть получается, что у нас с вами количество компонент 100, для фистеха плюс минус достаточно,
[53:34.680 --> 53:40.680]  для вот этой фотографии уже похуже. Или можно попытаться на ту же самую ошибку, их свести,
[53:40.680 --> 54:03.560]  но не хочется минуту ждать. K-лист. Ладно, начал уже, пускай считает. Пока оно думает,
[54:03.560 --> 54:17.840]  может у вас какие-то вопросы есть? Наверное, пока оно думает, я вас постараюсь чуть-чуть
[54:17.840 --> 54:24.440]  упокоить. Как правило, на вот этой лекции происходит отвал внимания у некоторой части потока,
[54:24.440 --> 54:28.640]  потому что, хоть Матан непонятно пошел с двойственной задачей, вообще ничего непонятно,
[54:28.640 --> 54:34.520]  электро еще и тоже про это упоминает, зачастую без каких-то деталей. Почему я не зарываюсь,
[54:34.520 --> 54:37.520]  например, в детали двойственной задачи? Потому что есть 30 процентов аудитории,
[54:37.520 --> 54:41.480]  которые, наверное, от этого поймут лучше, и 70, которые окончательно отвалятся и ничего не поймут
[54:41.480 --> 54:47.920]  вообще. Я приношу свои извинения, но про двойственные задачи, правда, надо было тогда на курсе по оптам
[54:47.920 --> 54:53.280]  нормально разбираться. Что я могу сказать? Те, кто как раз хочет почитать, у нас есть в доп-материалах,
[54:53.280 --> 54:58.280]  у нас есть ссылочка на доп-материалы в репозитории, надеюсь, вы их уже видели,
[54:58.280 --> 55:07.360]  если нет, то тут целая пачка всего. Нет, есть ссылки, которые прямо сейчас рекомендованы,
[55:07.360 --> 55:14.400]  а, на магазин? Нет, ну погодите, там везде практически есть ссылки на открытые материалы,
[55:14.400 --> 55:20.720]  просто есть книжка на английском языке, от автора выложенной на гитхаб, ее перевели на русский,
[55:20.720 --> 55:24.880]  и переводчики, они за это хотят денег, а не от издательства, поэтому на русском языке ее в открытом
[55:24.880 --> 55:31.280]  доступе нет, на английском она есть. Ну вот, смотрите, например, 100page, machine learning book,
[55:31.280 --> 55:44.520]  ее продают на амазоне, но при этом автор ее положил на гитхаб, потому что, короче, как он сам у себя в
[55:44.520 --> 55:49.680]  влоге писал, у него была политика read first, buy later, поэтому она была доступна. Видимо,
[55:49.680 --> 55:53.080]  ссылки регулярно протухают, ну что ж, будем ревьюить, или кидайте нам какие-нибудь эти,
[55:53.080 --> 55:59.760]  а? Ну да, ну или его вообще забанили по каким-то причинам, я, честно говоря, не знаю. Давайте
[55:59.760 --> 56:08.640]  проверим вообще, у него репозиторий живой. Что только что произошло?
[56:08.640 --> 56:23.440]  Я не очень понял, что сейчас произошло, если честно. У меня вылетели все вкладки.
[56:23.440 --> 56:36.360]  Так, еще раз едем сюда. Вот наши доп-материалы, вот наш гитхаб.
[56:36.360 --> 56:51.920]  Ладно, предположим. Понять не имею, что стало с репозиторием, раньше там была книжка. Ну,
[56:51.960 --> 56:55.760]  найдем тогда другую версию. Или, правда, у нее поменялась линцензия, теперь ее нельзя
[56:55.760 --> 57:03.760]  сначала прочитать, потом купить. Но, тем не менее, смотрите, собственно, есть такой рукописный
[57:03.760 --> 57:08.400]  учебник, во-первых, от ваших предшественников, во-вторых, есть методичка Воронцова. Воронцов
[57:08.400 --> 57:16.440]  замечательно краски здесь расписал про метод топорных векторов, поэтому крайне рекомендую всем
[57:16.440 --> 57:23.200]  тем, кто жаждет двойственных задач и всего прочего, это дело прочитать. Собственно, вот до седы мы с
[57:23.200 --> 57:28.360]  вами дошли формально, а дальше начинается вот эта красота, которую я вам лишь декларативно указал,
[57:28.360 --> 57:36.360]  хотя по факту она тоже доказывается. Окей? Вот. В остальном же, смотрите, коллеги, еще раз,
[57:36.360 --> 57:41.720]  сегодня у вас появится еще аж две домашки, не пугайтесь, потому что одна на автопроверку, вторая
[57:41.720 --> 57:47.600]  надолго, аж на месяц вперед, это лабо. Но рекомендую не забивать на лабо, она вам пригодится, скажем так,
[57:47.600 --> 57:52.800]  решенная ближе к концу месяца, потому что потом за один день ее решать не очень удобно. И лабо
[57:52.800 --> 57:57.440]  это как раз то задание, которое позволяет вам по шагам уже пройти через пайплайн подготовки данных,
[57:57.440 --> 58:02.600]  проверки разных гипотез, решение задачи и так далее. То есть вот эти маленькие домашки, они про то,
[58:02.600 --> 58:08.000]  чтобы вам походить и головой подумать, лабо это про то, как решить реальные задачи. Ну а в остальном
[58:08.000 --> 58:10.360]  дерзайте знать. До новых встреч!

[00:00.000 --> 00:16.200]  вот звонок да так давайте начнем с того чего не успели в прошлый раз давайте вот какой
[00:16.200 --> 00:22.720]  рассмотрим вопрос значит мы с вами в свое время установили что сумма случайных величин
[00:23.720 --> 00:29.640]  то есть можно их складывать вопрос как складывать это же там не 2 плюс 2 прибавить
[00:29.640 --> 00:38.680]  получившееся случайная величина собственно что нам про нее неизвестно случайно величина
[00:38.680 --> 00:43.680]  это элемент ну такого специального вероятностного пространства множество
[00:43.680 --> 00:50.040]  исходов известная это r1 сигма алгебра известная это барелевская неизвестна только функции
[00:50.040 --> 00:56.000]  распределения то есть для того чтобы определить сумму случайных величин нужно определить ее
[00:56.000 --> 01:04.920]  функцию распределения давайте это рассмотрим но сначала общая как бы да соображение пусть
[01:04.920 --> 01:17.680]  это равно кси 1 плюс кси 2 функция распределения это в точке у это вероятность того что кси 1
[01:17.680 --> 01:27.720]  плюс кси 2 меньше у а вот дальше давайте предположим что кси 1 и кси 2 непрерывные
[01:27.720 --> 01:31.720]  случайные величины это собственно самый ну что называется важный случай то есть
[01:31.720 --> 01:36.520]  у них есть совместная плотность тогда вот эта вероятность представима в виде двойного
[01:36.520 --> 01:53.720]  интеграла по области x 1 плюс x 2 меньше у f кси 1 кси 2 x 1 x 2 d x 1 d x 2 через функцию плотности
[01:53.720 --> 02:05.840]  вероятности да распишем ну и давайте сделаем замену переменных y 1 равно x 1 y 2 равно x 1
[02:05.840 --> 02:19.800]  плюс x 2 модуль и кабиана такого преобразования чему равен единица как видоизменяются области
[02:19.920 --> 02:38.040]  изначальная область интегрирования x 1 x 2 вот она превращается и грек один и грек два
[02:38.040 --> 02:59.320]  вот в такую поэтому делаем замену переменных получаем интеграл по y 2 от минус бесконечности
[02:59.320 --> 03:19.040]  до у интеграл f кси 1 кси 2 x 1 x 2 д ой прошу прощения x 1 это y 1 а вот x 2 это y 2
[03:19.040 --> 03:32.160]  минус x 1 то есть y 2 минус y 1 y 2 минус y 1 d y 1 и берется ну да в области написал здесь от
[03:32.160 --> 03:37.640]  минус бесконечности до бесконечности минус бесконечности до бесконечности
[03:37.640 --> 03:52.800]  д д y 2 вот дальше это функция распределения давайте функцию плотности посчитаем для
[03:52.800 --> 04:00.920]  этого возьмем производную по у слева получим f это от у а справа получим интегрирование
[04:00.920 --> 04:13.720]  по верхнему пределу интеграл минус бесконечности до бесконечности f кси 1 кси 2 y 1 у минус
[04:13.720 --> 04:25.280]  y 1 d y 1 ну и чисто для красоты y 1 заменим на z на другую букву просто ну и получим
[04:25.280 --> 04:37.160]  вот такую формулу я ее вот здесь напишу пусть у нас здесь побудет f кси 1 плюс кси 2 от у равно
[04:37.160 --> 04:54.920]  интеграл от минус бесконечности до бесконечности f кси 1 кси 2 а нет z у минус z по d z
[04:54.920 --> 05:03.200]  ну вот это вообще формула свертки так она в целом вам знакомы должна быть да
[05:03.200 --> 05:17.840]  это функция плотности вот этого вектора кси 1 кси 2 значит если у нас абсолютно непрерывное
[05:17.840 --> 05:32.680]  распределение то функция плотности случайного вектора это производные его функции распределения
[05:32.800 --> 05:52.280]  dx 1 dx к но это очевидно по моему вот значит вот эта функция свертки пусть пока повисит хорошим
[05:52.280 --> 06:00.480]  примером на использование этой функции является решение такой задачи пусть у вас есть n штук
[06:00.520 --> 06:07.020]  независимых случайных величин независимых случайных величин да про независимость
[06:07.020 --> 06:14.260]  мы уже говорили все из нормального распределения все из равномерного распределения 0 1 все из
[06:14.260 --> 06:23.320]  равномерного распределения 0 1 тогда вероятность того что ксе 1 плюс так далее плюс кси n окажется
[06:23.320 --> 06:32.720]  меньше некого x равно x в степени n делить на n факториал формула верна для x от нуля до единицы
[06:32.720 --> 06:49.000]  ну а это что такое это объем энмерного симплекса с ребром x да что симплекс это множество точек
[06:49.000 --> 07:00.600]  вида x1 плюс и так далее плюс xn меньше некого у ну x1 и так далее xn больше равно нуля ну
[07:00.600 --> 07:10.960]  можно больше такой в трехмерном случае пирамида в четырехмерном вот симплекс что не незнакомое
[07:10.960 --> 07:21.120]  слово ну вот я там я его то на него сослался потому что думал что вы знаете объем симплекса но если
[07:21.120 --> 07:27.680]  вы его не знаете тогда как бы вам этот результат ни о чем не говорит но тем не менее канонический
[07:27.680 --> 07:38.800]  пример использования функции формулы свертки но вот один такой который всегда приводится так это
[07:38.800 --> 07:55.840]  значит по поводу распределение суммы так ну мы с вами на прошлой лекции так подробно разобрали
[07:55.840 --> 08:04.680]  подробно разобрали функцию распределения случайного вектора но определение дали
[08:04.680 --> 08:11.120]  понятное определили функцию распределения свойство ее из учили ну теперь надо хоть
[08:11.120 --> 08:18.560]  какую-нибудь одну хоть какой-нибудь один случайный вектор так себе для примеров вести да ну и давайте
[08:18.560 --> 08:24.040]  в качестве примера мы с вами из случайных векторов рассмотрим два это так называемое
[08:24.040 --> 08:29.320]  полиномиальное распределение и нормальное распределение нормальный случайный вектор давайте
[08:29.320 --> 08:34.640]  Давайте начнём с полинамиального распределения.
[08:34.640 --> 08:43.280]  Оно определяется такими параметрами k штук положительных
[08:43.280 --> 08:49.120]  чисел п-житая и числом n, причём сумма всех п-житых
[08:49.120 --> 08:50.240]  равна единице.
[08:50.240 --> 09:02.320]  Это полинаминальное распределение для k-мерного вектора.
[09:02.320 --> 09:08.920]  x1, xk дискретные случайные величины, принимающие значение
[09:08.920 --> 09:11.640]  из натурального ряда, и 0 ещё могут принимать.
[09:12.520 --> 09:16.200]  Вот такой это область определения, так сказать.
[09:16.200 --> 09:26.360]  И определяется это дискретный случайный вектор следующими
[09:26.360 --> 09:27.840]  вероятностями.
[09:27.840 --> 09:35.680]  Вероятность того, что k-житая равно некому n-житому, g
[09:35.680 --> 09:45.080]  равно от единицы до k, равно n-факториал, n1-факториал,
[09:45.080 --> 09:52.880]  так далее, nk-факториал, умножить на p1 в степени n1, умножить
[09:52.880 --> 10:00.080]  и так далее, на pk в степени nk, где сумма всех nk равно
[10:00.080 --> 10:07.280]  0, это определение.
[10:07.280 --> 10:11.480]  Для дискретной случайной величины определили вероятность
[10:11.480 --> 10:13.960]  того, что случайный вектор принимает именно такое
[10:13.960 --> 10:14.960]  значение.
[10:14.960 --> 10:18.280]  В принципе, как бы всё понятно, но мы когда какой-то
[10:18.280 --> 10:25.280]  вводим случайную величину, а я произнёс, но не написал.
[10:25.280 --> 10:28.080]  Нет, конечно, p-житая строго больше нуля.
[10:28.080 --> 10:35.080]  Но мы когда вводим какую-то случайную величину, я привожу
[10:35.080 --> 10:38.560]  некий эксперимент, мысленный эксперимент, который ей
[10:38.560 --> 10:39.560]  соответствует.
[10:39.560 --> 10:42.640]  Здесь давайте поступим следующим образом.
[10:42.640 --> 10:50.040]  Возьмём отрезок 0,1 и разобьём его на отрезочки.
[10:50.040 --> 10:59.480]  Один длины p1, второй длины p2, последние длины pk.
[10:59.480 --> 11:03.800]  И начнём на удачу бросать точку на этот отрезок 0,1.
[11:03.800 --> 11:05.480]  Бросаем точку на удачу.
[11:05.480 --> 11:13.520]  Делаем это n раз незавихимо, значит эта точка может куда-то
[11:13.520 --> 11:19.160]  попадать, n раз мы это делаем, и сколько-то точек попадут,
[11:19.280 --> 11:24.640]  назову это вот в этот ящик, это n1, вот столько во второй
[11:24.640 --> 11:28.960]  ящик n2, в кат и ящик nk точек попадёт.
[11:28.960 --> 11:34.080]  Вот этот набор и представляет из себя случайную величину,
[11:34.080 --> 11:35.640]  имеющую полинаминальное распределение.
[11:35.640 --> 11:44.120]  Ну это один из, можно рассматривать ящики и бросать шарики,
[11:44.120 --> 11:47.640]  можно бросать точки на отрезок специальным образом
[11:47.640 --> 11:48.640]  поделённый.
[11:49.120 --> 11:53.080]  Вот такому мысленному эксперименту соответствует полинаминальная
[11:53.080 --> 11:59.880]  случайная величина, вероятность вектора с компонентами n1, n2, nk
[11:59.880 --> 12:04.640]  задаётся вот такой формулой.
[12:04.640 --> 12:07.360]  Мы знаем случайную величину, которая является частным
[12:07.360 --> 12:09.800]  случаем полинаминального распределения.
[12:09.800 --> 12:15.880]  Кто подскажет, что из-за случайной величины?
[12:15.880 --> 12:23.680]  Полинаминальное распределение соответствует k равно 2, если
[12:23.680 --> 12:28.440]  взять k равно 2, то полинаминальное распределение превращается
[12:28.440 --> 12:30.200]  в бинаминальное распределение.
[12:30.200 --> 12:35.000]  Так, ну вот давайте мы этот пример как бы отложим
[12:35.000 --> 12:36.000]  в памяти.
[12:36.000 --> 12:55.600]  Это пример дискретного случайного вектора.
[12:55.600 --> 12:59.960]  Следующий объект, с которым нам надо познакомиться,
[12:59.960 --> 13:05.120]  это условные функции распределения для случайного вектора.
[13:05.520 --> 13:10.880]  Но сначала чуть-чуть напомню, к чему мы пришли в конце
[13:10.880 --> 13:12.360]  прошлой лекции.
[13:12.360 --> 13:24.680]  Мы определили свойство независимости k штук случайных величин
[13:24.680 --> 13:27.080]  компонент вектора.
[13:27.160 --> 13:34.000]  Таким образом, функция распределения распадается
[13:34.000 --> 13:48.440]  в произведение функции распределения k, от 1 до k.
[13:48.440 --> 13:51.400]  Распадается в произведение функции распределения
[13:51.480 --> 14:00.840]  И, что не менее важно, это то же самое, как, или точнее
[14:00.840 --> 14:03.080]  говоря, это свойствие эквивалентно вот такому.
[14:03.080 --> 14:09.000]  Вероятность того, что ксиджитэ принадлежит некому боджитому
[14:09.000 --> 14:14.760]  g равно от 1 до k, равно произведению вероятности
[14:14.760 --> 14:18.840]  того, что ксиджитэ принадлежит боджитому.
[14:18.840 --> 14:29.000]  А вот в качестве боджитого, напомню, это важно, боджитая
[14:29.000 --> 14:30.640]  это может быть.
[14:30.640 --> 14:33.040]  И все это эквивалентное определение.
[14:33.040 --> 14:39.600]  Полуинтервал от житая боджитая, элемент бареллевской
[14:39.600 --> 14:44.440]  алгебры, элемент бареллевской сигма алгебры.
[14:44.440 --> 14:52.520]  Всякое свойство с любым из этих элементов, если
[14:52.520 --> 14:55.760]  оно выглядит так, означает независимость и означает
[14:55.760 --> 14:56.760]  вот это свойство.
[14:56.760 --> 14:58.880]  Вот на этом мы вчера закончили.
[14:58.880 --> 15:02.680]  Ну теперь в практической плоскости давайте поймем,
[15:02.680 --> 15:06.080]  ну просто запишем один раз, не будем к этому возвращаться.
[15:06.080 --> 15:13.680]  Для дискретных случайных величин DSV, свойство независимости
[15:13.680 --> 15:25.320]  можно записать в таком виде, ксиджитэ равно x с джитому
[15:25.320 --> 15:28.400]  одновременно для всех компонент.
[15:28.400 --> 15:31.600]  Эта вероятность равна произведению вероятности.
[15:31.600 --> 15:43.920]  А для непрерывных случайных величин это можно записать
[15:43.920 --> 15:44.920]  в виде.
[15:44.920 --> 15:54.560]  Функция плотности вектора x1, xкт распадается в произведение
[15:54.560 --> 16:02.240]  функций плотностей, ксиджитэ, xджитэ, g равно от единицы до
[16:02.240 --> 16:07.600]  k.
[16:07.600 --> 16:12.000]  Практически так можно и таким пользоваться определением.
[16:12.000 --> 16:15.480]  Это следствие вот этих определений.
[16:15.480 --> 16:16.480]  Эквивалентное.
[16:17.320 --> 16:24.040]  Так, ну и теперь давайте еще рассмотрим один объект
[16:24.040 --> 16:26.880]  в части функции распределения.
[16:26.880 --> 16:30.440]  Это так называемая условная функция распределения.
[16:30.440 --> 16:34.160]  Ну по названию догадываетесь, что это связано с условными
[16:34.160 --> 16:37.040]  вероятностями, но так и есть.
[16:37.040 --> 16:43.760]  Напомню, что для случайной величины кси функция распределения
[16:43.840 --> 16:48.080]  вероятность того, что кси меньше х, ну можно рассмотреть
[16:48.080 --> 16:50.720]  и условную функцию распределения.
[16:50.720 --> 16:56.680]  Значит, вот здесь напишу чуть подробнее, хотя мы уже
[16:56.680 --> 17:01.600]  так не делаем, но может быть сейчас это будет полезно.
[17:01.600 --> 17:04.160]  Это вероятностная мера тех омега, что кси от омега
[17:04.160 --> 17:05.160]  меньше х.
[17:05.160 --> 17:09.520]  Ну и можно ввести какую-нибудь условную вероятность относительно
[17:09.520 --> 17:10.520]  события.
[17:11.360 --> 17:13.360]  Условную функцию.
[17:13.360 --> 17:19.600]  Это будет условная вероятность омега таких, что кси от омега
[17:19.600 --> 17:23.240]  меньше х при условии, что омега принадлежит некому
[17:23.240 --> 17:28.440]  а событию из сигма-алгебры.
[17:28.440 --> 17:32.160]  Условная вероятность – это вероятность со всеми свойствами,
[17:32.160 --> 17:35.880]  поэтому условная функция распределения – это полноценная
[17:35.880 --> 17:38.400]  функция распределения.
[17:38.400 --> 17:40.360]  Чему это равно?
[17:40.360 --> 17:46.360]  По формуле условной вероятности это равно вероятности таких
[17:46.360 --> 17:51.400]  омега, что, во-первых, кси от омега меньше х, и одновременно
[17:51.400 --> 17:59.480]  с этим омега принадлежит а, и поделить это на вероятность
[17:59.480 --> 18:04.360]  того, что омега принадлежит а.
[18:04.360 --> 18:09.680]  Значит, собственно, и всё.
[18:09.680 --> 18:16.360]  Если знаменатель не равен нулю, то никаких проблем
[18:16.360 --> 18:19.200]  ни методологически ни в использовании нету.
[18:19.200 --> 18:22.160]  Можно пользоваться условной функцией распределения
[18:22.160 --> 18:24.040]  так же, как любой другой.
[18:24.040 --> 18:28.360]  Более того, иногда эти формулы вполне так презентабельно
[18:28.360 --> 18:34.120]  выглядят, например, f кси при условии это, х при условии
[18:34.120 --> 18:35.120]  у.
[18:35.120 --> 18:42.520]  Если мы под этим будем понимать вероятность того, что кси
[18:42.520 --> 18:48.920]  меньше х при условии, что это меньше у, это две случайные
[18:48.920 --> 18:54.640]  личины кси и это, то, воспользуясь формулой полной вероятности,
[18:54.640 --> 18:56.600]  мы это можем переписать в виде.
[18:56.840 --> 19:03.200]  Функция распределения вектора кси это делить на функцию
[19:03.200 --> 19:05.000]  распределения это.
[19:05.000 --> 19:09.240]  Ну, конечно, для тех у, где знаменатель не равен нулю.
[19:09.240 --> 19:11.040]  Вот такая вполне красивая формула.
[19:11.040 --> 19:14.080]  Ну, и они могут быть всякие другие.
[19:14.080 --> 19:26.000]  Но, как сказать, практика привела к тому, что необходимо
[19:26.000 --> 19:30.080]  создать какую-то интерпретацию в случае, когда вероятность
[19:30.080 --> 19:34.160]  вот этого события ара равна нулю, вероятность события
[19:34.160 --> 19:37.440]  ара равна нулю, ну, что приходит в голову.
[19:37.440 --> 19:50.520]  Так, вот здесь сотру.
[19:50.520 --> 19:53.080]  Надо какой-то предельный переход сделать.
[19:53.160 --> 19:57.560]  Например, построить систему вложенных множеств, вероятность
[19:57.560 --> 20:01.440]  каждого из которых не равна нулю, так, чтобы их пересечение
[20:01.440 --> 20:07.360]  в счетном числе равнялось а, да, то есть, давайте попробуем
[20:07.360 --> 20:10.440]  обойти эту проблему, построим вот такую вложенную систему
[20:10.440 --> 20:20.680]  множеств, такую, что p, a, n равно a.
[20:20.680 --> 20:28.560]  То есть, каждого из a, n не равна нулю, и тогда вот,
[20:28.560 --> 20:31.760]  ну, не буду всю формулу, наверное, переписывать,
[20:31.760 --> 20:35.440]  просто напишу, а нет, пожалуй, придется.
[20:35.440 --> 20:42.680]  Значит, вот это xi, х при условии a, n, точнее говоря, при условии
[20:42.680 --> 20:50.480]  a мы запишем как предел, когда n стремится к бесконечности,
[20:50.480 --> 20:58.040]  вероятности ω такие, что xi от ω меньше х и одновременно
[20:58.040 --> 21:05.360]  с этим ω принадлежит a, n, делит на вероятность ω принадлежит
[21:05.360 --> 21:08.360]  a, n.
[21:09.040 --> 21:12.560]  Ну, если такой предел существует, то мы, собственно, и назовем
[21:12.560 --> 21:18.160]  это вот условной вероятностью, когда вероятность а равна
[21:18.160 --> 21:19.160]  нулю.
[21:19.160 --> 21:29.960]  Ну, выглядит разумно, нет, идея неплохая, но проблема
[21:29.960 --> 21:33.400]  в том, что, к сожалению, этот предел зависит от того,
[21:33.400 --> 21:34.880]  как мы построим вот эти множества.
[21:34.880 --> 21:37.760]  То есть вот так не пройдет.
[21:37.760 --> 21:41.640]  Что же нам делать?
[21:41.640 --> 21:47.000]  Ну, мы должны обрисовать какие-то частные случаи,
[21:47.000 --> 21:49.520]  в которых этот предельный переход корректен.
[21:49.520 --> 21:53.920]  Идея та же, но только, грубо говоря, надо множество задать,
[21:53.920 --> 21:59.160]  явно, вот эти а, n, и тогда предел не будет зависеть,
[21:59.160 --> 22:01.680]  или мы стремимся к тому, чтобы он не зависел, и тогда
[22:01.680 --> 22:04.760]  можно считать, что мы определили условную функцию
[22:04.760 --> 22:05.760]  распределения.
[22:05.760 --> 22:13.760]  Ну, и давайте это для, собственно, того случая, когда это не
[22:13.760 --> 22:14.760]  получается.
[22:14.760 --> 22:18.760]  Что я имею в виду?
[22:18.760 --> 22:24.440]  Если вот эта, например, дискретная случайная уличина,
[22:24.440 --> 22:29.120]  то, в принципе, все получается, потому что нет смысла брать
[22:29.120 --> 22:31.080]  это, вероятность которых равна нулю.
[22:31.080 --> 22:33.560]  То есть вероятность этих событий не равно нулю, и
[22:33.560 --> 22:37.880]  если условие задано на дискретной случайной величине,
[22:37.880 --> 22:39.560]  то тогда там все работает.
[22:39.560 --> 22:43.960]  То есть тот случай, когда не получается заведомо,
[22:43.960 --> 22:51.280]  это когда, как это сказать, случайная величина задающая
[22:51.280 --> 22:55.040]  условия непрерывна, тогда вероятность того, что она
[22:55.040 --> 22:58.160]  равна какому-то значению, всегда равна нулю, то есть
[22:58.160 --> 22:59.160]  никак не обойдешь.
[22:59.160 --> 23:06.200]  Поэтому давайте мы пройдем этот путь, считая, что
[23:06.200 --> 23:10.000]  кси и это непрерывные случайные величины, то есть у них есть
[23:10.000 --> 23:11.720]  совместная плотность.
[23:11.720 --> 23:14.960]  Ну и давайте выпишем вот, обозначим так же, потому
[23:14.960 --> 23:24.360]  что тут обозначения, как сказать, нет такого стандартизированного.
[23:25.360 --> 23:30.560]  Таким, ну не похожим, а совпадающим с тем, но только по-другому
[23:30.560 --> 23:31.560]  теперь определим.
[23:31.560 --> 23:40.680]  Это есть у нас вероятность того, что кси меньше х, одновременно
[23:40.680 --> 23:46.960]  с этим это равно y, вот события нулевой меры, делить на
[23:46.960 --> 23:51.320]  вероятность того, что это равно y.
[23:51.320 --> 23:55.000]  Должно так быть, но здесь деление на 0, поэтому давайте
[23:55.000 --> 23:58.400]  сделаем предельный переход, вот таким образом.
[23:58.400 --> 24:04.560]  Это предел, при n, стремящемся к бесконечности, вероятности
[24:04.560 --> 24:10.240]  того, что кси меньше х, а это принадлежит некому d
[24:10.240 --> 24:14.760]  дельта n, делить на вероятность того, что это принадлежит
[24:14.760 --> 24:15.760]  дельта n.
[24:16.360 --> 24:21.120]  А дельта n, это у нас вот такой полуинтервалчик
[24:21.120 --> 24:30.800]  а n, bn такой, что а n возрастая сходится к y, а bn убывая сходится
[24:30.800 --> 24:31.800]  к y.
[24:31.800 --> 24:38.000]  Вот такой переход рассмотрим.
[24:38.000 --> 24:40.960]  Давайте распишем, что это такое через плотности.
[24:40.960 --> 24:44.520]  Это предел, при n, стремящемся к бесконечности, ну и могу
[24:44.520 --> 24:46.040]  так записать.
[24:46.040 --> 24:58.160]  Интеграл по дельта n, по dy, интеграл от минус бесконечности
[24:58.160 --> 25:12.960]  до х, f, кси, это, ну в, возьму, в, y, по dv.
[25:12.960 --> 25:16.720]  Вероятность вот такого события, а в знаменателе
[25:16.720 --> 25:28.040]  стоит интеграл по дельта n, f, это от y dy.
[25:28.040 --> 25:30.480]  Вот этот переход уже можно сделать корректно.
[25:30.480 --> 25:53.240]  Вот сюда напишу.
[25:53.240 --> 25:57.840]  Я напишу сразу ответ, надеюсь, что вас не затруднит.
[25:57.840 --> 26:12.320]  Это интеграл от минус бесконечности до х, f, кси, это в, y, делить,
[26:12.320 --> 26:23.960]  это все под интегралом, делить на f, это y dv.
[26:23.960 --> 26:25.720]  Ну согласны, да?
[26:25.720 --> 26:28.720]  Вот этот интеграл, как функция y, представим значение
[26:28.720 --> 26:33.080]  на, там, в точке в этом интервале, на длину плюсу малой длины
[26:33.080 --> 26:35.760]  интервала, в общем, переходим к пределу, получаем такую
[26:35.760 --> 26:36.760]  штуку.
[26:36.760 --> 26:37.760]  Ну и теперь сравниваем.
[26:37.760 --> 26:41.360]  Это функция распределения, это ее представление через
[26:41.360 --> 26:42.360]  интеграл.
[26:42.360 --> 26:48.680]  Это что означает, что эта функция абсолютно непрерывна,
[26:48.680 --> 26:53.680]  раз имеет такое представление, и поэтому можно получить,
[26:53.680 --> 27:04.520]  я это вот здесь запишу, функцию плотности.
[27:04.520 --> 27:18.560]  Продиференцировав обе стороны по х, мы получим функцию плотности,
[27:18.560 --> 27:22.600]  или ту функцию, с которой формально можно поступать
[27:22.600 --> 27:26.280]  как с условной функцией плотности.
[27:26.280 --> 27:33.080]  Вот эта вот функция плотности означает, что, означает
[27:33.080 --> 27:37.240]  вероятность того, что случайная величина кси попадет в
[27:37.240 --> 27:40.360]  какой-то дифференциал в окрестности точки х при
[27:40.360 --> 27:46.040]  условии, что случайная величина это равна y.
[27:46.040 --> 27:50.880]  Вот с этой функцией, которая, ну как бы так сказать, строго
[27:50.880 --> 27:55.200]  говоря, там не определена, поскольку условие имеет
[27:55.200 --> 28:00.480]  нулевую вероятностную меру, но с ней можно оперировать,
[28:00.480 --> 28:03.960]  если в качестве ее взять вот такую вот функцию.
[28:03.960 --> 28:09.960]  И вот эта вот вещь, которой как бы все пользуются, условная
[28:09.960 --> 28:10.960]  функция распределения.
[28:10.960 --> 28:20.640]  Так, про функции распределения вектора вроде все, и мы тогда
[28:20.960 --> 28:27.600]  переходим к другой, ну такой фундаментальной, можно
[28:27.600 --> 28:28.600]  сказать, теме.
[28:28.600 --> 28:40.280]  Ну, наверное, и так, и так можно вас поймут.
[28:40.280 --> 28:48.360]  Значит, ну, функция плотности вероятности, если полное
[28:48.360 --> 28:52.480]  название, условная функция плотности вероятности.
[28:52.480 --> 28:58.120]  Ну, собственно, мы изначально называли вот эту f, которая
[28:58.120 --> 29:04.360]  ядро интеграла от минус бесконечности до x, f dx равно функции распределения,
[29:04.360 --> 29:08.080]  мы называли функцией плотности вероятности, но я тогда
[29:08.080 --> 29:11.400]  сказал, что иногда говорят, просто функция плотности,
[29:11.400 --> 29:15.080]  понимают, что речь идет о вероятности, а не о материале
[29:15.120 --> 29:17.640]  каком-то, да, так все, там тоже есть функция плотности,
[29:17.640 --> 29:18.760]  РО, да, называется.
[29:18.760 --> 29:25.960]  Вот, значит, ну, а как это, перестановки здесь слов для,
[29:25.960 --> 29:29.560]  ну, наверное, так сказать, там ничего не меняют, по
[29:29.560 --> 29:33.920]  разному можно, лишь бы вас понимали, о чем вы говорите.
[29:33.920 --> 29:40.760]  Так, значит, следующий, ну, прямо трудно переоценить,
[29:40.760 --> 29:44.360]  по важности объект, который мы с вами введем, это
[29:44.360 --> 29:48.560]  математическое ожидание, случайные величины, это
[29:48.560 --> 29:57.160]  числовая характеристика, обозначается, ну, традиционно,
[29:57.160 --> 30:04.400]  екси, математическое ожидание, ну, экспект, ожидаемая величина,
[30:04.400 --> 30:14.640]  и по определению, это есть интеграл Либега по множеству
[30:14.640 --> 30:16.160]  элементарных исходов.
[30:16.160 --> 30:25.880]  Скажите вам, как привычней, pdОмега или dpОмега, а, ну,
[30:25.880 --> 30:31.760]  неважно как бы, нет, просто иногда пишут dp в скобочках
[30:31.760 --> 30:37.840]  Омега, а иногда п в скобочках dОмега, ну, если вам все равно,
[30:37.840 --> 30:39.840]  я буду так писать.
[30:39.840 --> 30:47.000]  Значит, вот, математическое ожидание екси, еще для математического
[30:47.000 --> 30:57.200]  ожидания используют обозначение мкси, это ровно тот же объект,
[30:57.200 --> 31:02.520]  тоже математическое ожидание, но называют его mкси.
[31:02.520 --> 31:05.680]  Вот здесь я хочу сказать, что это тот случай, когда
[31:05.680 --> 31:11.560]  это не просто два названия эквивалентных, на самом деле
[31:11.560 --> 31:15.280]  это не случайно, что один екси, а второй мкси, это
[31:15.280 --> 31:18.840]  мы поймем в конце лекции, и, собственно, это и покажет
[31:18.840 --> 31:24.040]  нам, так сказать, важность этой числовой характеристики,
[31:24.040 --> 31:28.480]  то есть это эквивалентные названия, я напишу вот так
[31:28.480 --> 31:33.880]  mкси равно екси, в том смысле, что это одно и то же, ну,
[31:33.880 --> 31:38.360]  и должен вам сказать так это, пафосно, что это, по
[31:38.360 --> 31:42.000]  сути дела, второй закон Ньютона теории вероятности.
[31:42.000 --> 31:48.680]  mкси равно екси, тут, конечно, определенный художественный
[31:48.680 --> 31:53.720]  образ, но к концу лекции вы поймете, что я имею в виду.
[31:53.720 --> 31:58.240]  Вот, значит, вот такой вот объект, математическое
[31:58.240 --> 31:59.240]  ожидание.
[31:59.240 --> 32:00.240]  Ну, что мы про это можем сказать?
[32:00.240 --> 32:03.520]  Во-первых, это число, это уже не случайная влечина,
[32:03.520 --> 32:05.320]  это число.
[32:05.320 --> 32:09.280]  Как любой интеграл Либега, у него есть некие универсальные
[32:09.280 --> 32:10.280]  свойства.
[32:10.280 --> 32:14.480]  Ну, давайте первое, пусть у нас кси от омега тождесть
[32:14.480 --> 32:19.600]  на равно с, то есть константа, выраженная случайно влечена.
[32:19.600 --> 32:23.520]  Чему равном от ожидания такой случайной влечины?
[32:24.520 --> 32:26.520]  A?
[32:26.520 --> 32:28.520]  C?
[32:28.520 --> 32:31.520]  Ну, собственно, сюда поставьте C.
[32:31.520 --> 32:40.520]  Значит, второе свойство, которое есть, пусть, ну,
[32:40.520 --> 32:43.520]  или можно в первом же свойстве, речь идет о каких-то
[32:43.520 --> 32:45.520]  конкретных случайных величинах.
[32:45.520 --> 32:49.520]  Давайте пусть кси от омега у нас, это индикаторная
[32:49.520 --> 32:51.520]  функция множества A.
[32:51.520 --> 32:55.520]  То есть функция, которая равна единице, когда омега
[32:55.520 --> 32:59.520]  принадлежит A и нулю, когда омега не принадлежит A.
[32:59.520 --> 33:02.520]  Математическое ожидание такой случайной влечины,
[33:02.520 --> 33:04.520]  чему равно?
[33:04.520 --> 33:08.520]  Мера множества A, которую мы в теории вероятности
[33:08.520 --> 33:10.520]  обозначаем вот так.
[33:10.520 --> 33:14.520]  В общей теории меры мю или лямбда, мы обозначаем
[33:15.520 --> 33:19.520]  Вот, значит, следующее свойство
[33:19.520 --> 33:23.520]  случая интеграла Либега.
[33:23.520 --> 33:32.520]  Математическое ожидание A кси, A константа, чему равно?
[33:32.520 --> 33:41.520]  Во-первых, математическое ожидание кси плюс это,
[33:41.520 --> 33:43.520]  чему равно?
[33:44.520 --> 33:47.520]  Математическому ожиданию кси плюс математическое
[33:47.520 --> 33:49.520]  ожидание это.
[33:49.520 --> 33:52.520]  Свойства линейности интеграла Либега.
[33:52.520 --> 33:54.520]  Интеграл Либега такими свойствами обладает,
[33:54.520 --> 33:58.520]  поэтому и мат ожидания, которая есть интеграл Либега,
[33:58.520 --> 34:00.520]  тоже ими обладает.
[34:00.520 --> 34:03.520]  Так, следующее свойство, которое мы отметим.
[34:03.520 --> 34:07.520]  Давайте рассмотрим две случайные величины кси1 и кси2
[34:07.520 --> 34:10.520]  вот с такими свойствами.
[34:10.520 --> 34:15.520]  Вероятностная мера Омега таких, что кси1 от Омега
[34:15.520 --> 34:20.520]  равно кси2 от Омега равна единице.
[34:22.520 --> 34:26.520]  Вербально это говорят, кси1 равно кси2 с вероятностью
[34:26.520 --> 34:32.520]  единица, или кси1 равно кси2 почти, наверное.
[34:32.520 --> 34:36.520]  В функциональном анализе употребляется почти всюду,
[34:36.520 --> 34:39.520]  а в теории вероятности почти, наверное, поскольку мы
[34:39.520 --> 34:43.520]  здесь имеем дело то ли да, то ли нет, случайное событие,
[34:43.520 --> 34:46.520]  поэтому не почти всюду, а почти, наверное.
[34:46.520 --> 34:51.520]  В общем, либо говорят, что эти величины различаются
[34:51.520 --> 34:54.520]  на множестве меры ноль, естественно.
[34:54.520 --> 34:58.520]  И если кси1 и кси2 совпадают с друг с другом,
[34:58.520 --> 35:01.520]  почти, наверное, то вот эти два числа равны.
[35:01.520 --> 35:06.520]  Е кси1 равно е кси2.
[35:06.520 --> 35:10.520]  Это следствие того, что подинтегральная функция
[35:10.520 --> 35:14.520]  в интеграле Либега, будучи изменена на множестве меры ноль,
[35:14.520 --> 35:16.520]  не изменяет интеграл Либега.
[35:16.520 --> 35:20.520]  Еще одно свойство ответимо интеграла Либега.
[35:20.520 --> 35:25.520]  Пусть у нас теперь две функции, но только вот такие.
[35:25.520 --> 35:31.520]  Кси1 от Омега меньше ли равно кси2 от Омега?
[35:31.520 --> 35:33.520]  Равно единице.
[35:33.520 --> 35:37.520]  То есть почти, наверное, случайная величина кси2
[35:37.520 --> 35:40.520]  мажорирует случайную величину кси1.
[35:40.520 --> 35:43.520]  Тогда по свойствам интеграла Либега
[35:43.520 --> 35:46.520]  математическое ожидание кси1,
[35:46.520 --> 35:52.520]  число вот это, меньше числа математическое ожидание кси2.
[35:54.520 --> 36:00.520]  Вот, кстати, где свойство линейности,
[36:00.520 --> 36:02.520]  допишите еще вот такое свойство.
[36:03.520 --> 36:07.520]  Математическое ожидание кси,
[36:07.520 --> 36:10.520]  минус мат ожидания кси, чему равно?
[36:14.520 --> 36:16.520]  Нулю.
[36:16.520 --> 36:18.520]  Вот я вот тут вот, вот так вот.
[36:18.520 --> 36:20.520]  Нулю.
[36:20.520 --> 36:24.520]  Просто мы такой конструкцией частенько будем пользоваться.
[36:24.520 --> 36:27.520]  Давайте запомним, что, кстати, она равна нулю.
[36:28.520 --> 36:32.520]  Значит, такие вот свойства,
[36:32.520 --> 36:34.520]  это все как бы такие,
[36:34.520 --> 36:36.520]  совсем пока все просто.
[36:36.520 --> 36:38.520]  Ну и наконец,
[36:38.520 --> 36:41.520]  еще одно свойство интеграла Либега,
[36:41.520 --> 36:45.520]  которое мы уже как-то так
[36:45.520 --> 36:49.520]  позволит нам продвинуться дальше.
[36:57.520 --> 36:59.520]  Давайте рассмотрим
[36:59.520 --> 37:01.520]  конечную систему множеств.
[37:01.520 --> 37:05.520]  ajt j равно от 1 до n.
[37:05.520 --> 37:07.520]  Это покрытие, то есть
[37:07.520 --> 37:10.520]  объединение всех ajt равно
[37:10.520 --> 37:12.520]  ω,
[37:12.520 --> 37:17.520]  а it ajt пересечения равно пустому множеству.
[37:18.520 --> 37:20.520]  Ну да, и все.
[37:20.520 --> 37:22.520]  Тогда
[37:22.520 --> 37:24.520]  введем вот такую
[37:24.520 --> 37:26.520]  функцию,
[37:26.520 --> 37:28.520]  Sn атомига,
[37:28.520 --> 37:30.520]  которая равна сумме
[37:30.520 --> 37:32.520]  какие-то xjt
[37:32.520 --> 37:36.520]  на индикаторную функцию ajt.
[37:36.520 --> 37:40.520]  g равно от 1 до n.
[37:40.520 --> 37:42.520]  Ну, давайте, так сказать,
[37:42.520 --> 37:44.520]  для строгости
[37:44.520 --> 37:48.520]  напишем, что все xjt больше нуля.
[37:48.520 --> 37:50.520]  Должна быть знакомая вам конструкция.
[37:50.520 --> 37:52.520]  Это что такое?
[37:52.520 --> 37:54.520]  В терминах интеграла Либега.
[37:54.520 --> 37:56.520]  Это простая функция.
[37:56.520 --> 37:58.520]  В терминах интеграла Либега
[37:58.520 --> 38:00.520]  это простая функция.
[38:00.520 --> 38:02.520]  И в теории Либега
[38:02.520 --> 38:04.520]  математическое ожидание такой функции
[38:04.520 --> 38:06.520]  задается аксиоматически.
[38:06.520 --> 38:09.520]  А именно, математическое ожидание
[38:09.520 --> 38:10.520]  Sn
[38:10.520 --> 38:12.520]  больше 0,
[38:12.520 --> 38:14.520]  чем
[38:14.520 --> 38:18.520]  а именно, математическое ожидание Sn
[38:18.520 --> 38:20.520]  равно сумме
[38:20.520 --> 38:22.520]  xjt
[38:22.520 --> 38:26.520]  на вероятность ajt.
[38:26.520 --> 38:30.520]  g равно от 1 до n.
[38:30.520 --> 38:32.520]  Правильно, да?
[38:32.520 --> 38:34.520]  Но давайте посмотрим,
[38:34.520 --> 38:37.520]  это важный методологический такой момент.
[38:37.520 --> 38:39.520]  Давайте посмотрим, что это за функция
[38:39.520 --> 38:41.520]  с точки зрения
[38:41.520 --> 38:43.520]  ну, нашей теории.
[38:43.520 --> 38:45.520]  С точки зрения случайных величин.
[38:45.520 --> 38:47.520]  Какая это случайная величина?
[38:47.520 --> 38:49.520]  Это дискретная случайная величина,
[38:49.520 --> 38:51.520]  потому что она принимает
[38:51.520 --> 38:53.520]  ну, вот эти значения xjt
[38:53.520 --> 38:55.520]  с вероятностями ajt.
[38:55.520 --> 38:57.520]  Но
[38:59.520 --> 39:01.520]  для того, чтобы случайная величина
[39:01.520 --> 39:03.520]  приняла значение xjt,
[39:03.520 --> 39:05.520]  значит, у нас должно произойти
[39:05.520 --> 39:07.520]  событие с вероятностью ajt.
[39:07.520 --> 39:09.520]  И вот я сейчас напишу
[39:09.520 --> 39:11.520]  и поясню.
[39:11.520 --> 39:13.520]  Я перепишу так,
[39:13.520 --> 39:15.520]  xjt
[39:15.520 --> 39:17.520]  на вероятность того,
[39:17.520 --> 39:19.520]  что Sn
[39:19.520 --> 39:21.520]  равно xjt.
[39:23.520 --> 39:25.520]  Правильно, да?
[39:25.520 --> 39:27.520]  Если Sn
[39:27.520 --> 39:29.520]  равно xjt,
[39:29.520 --> 39:31.520]  значит, вот эта равна 1,
[39:31.520 --> 39:33.520]  она только одна может равна 1.
[39:33.520 --> 39:35.520]  Ну и значит,
[39:35.520 --> 39:37.520]  что вероятность ajt
[39:37.520 --> 39:39.520]  это вероятность того, что Sn
[39:39.520 --> 39:41.520]  равно xjt. Правильно?
[39:45.520 --> 39:47.520]  Ну и что получается?
[39:47.520 --> 39:49.520]  Получается, что
[39:49.520 --> 39:51.520]  математическое ожидание дискретной
[39:51.520 --> 39:53.520]  случайной величины
[39:53.520 --> 39:55.520]  может быть нами выражено
[39:55.520 --> 39:57.520]  только
[39:57.520 --> 39:59.520]  на основании, ну скажем так,
[39:59.520 --> 40:01.520]  функции распределения этой случайной
[40:01.520 --> 40:03.520]  величины, да?
[40:03.520 --> 40:05.520]  Смотрите, здесь нет омега, здесь нет
[40:05.520 --> 40:07.520]  исходно вероятностного пространства.
[40:07.520 --> 40:09.520]  Все удачно срослось.
[40:09.520 --> 40:11.520]  Интеграл либега,
[40:11.520 --> 40:13.520]  который, вообще говоря, берется
[40:13.520 --> 40:15.520]  по исходному вероятностному пространству,
[40:15.520 --> 40:17.520]  ну, по крайней мере, для дискретных
[40:17.520 --> 40:19.520]  случайных величин, оказалось,
[40:19.520 --> 40:21.520]  что он выражается через функцию
[40:21.520 --> 40:23.520]  распределения этой дискретной случайной величины.
[40:23.520 --> 40:25.520]  Нам не нужно исходное
[40:25.520 --> 40:27.520]  вероятностное пространство. И это
[40:27.520 --> 40:29.520]  революционный скачок для нас.
[40:29.520 --> 40:31.520]  Так, отдыхайте и продолжим.
[40:33.520 --> 40:35.520]  Продолжаем.
[40:37.520 --> 40:39.520]  Итак,
[40:39.520 --> 40:41.520]  у нас все так удачно срослось,
[40:41.520 --> 40:43.520]  что мы можем теперь
[40:43.520 --> 40:45.520]  вычислять математические ожидания
[40:45.520 --> 40:47.520]  дискретных
[40:47.520 --> 40:49.520]  случайных величин,
[40:49.520 --> 40:51.520]  как бы,
[40:51.520 --> 40:53.520]  зная только
[40:53.520 --> 40:55.520]  вероятности того, что
[40:55.520 --> 40:57.520]  случайная величина приняла какое-то
[40:57.520 --> 40:59.520]  конкретное значение.
[40:59.520 --> 41:01.520]  Вот набор вот этих вероятностей,
[41:01.520 --> 41:03.520]  которые, ну, естественно, так сказать,
[41:03.520 --> 41:05.520]  и создают функцию распределения,
[41:05.520 --> 41:07.520]  с функцией распределения.
[41:07.520 --> 41:09.520]  Давайте
[41:09.520 --> 41:11.520]  теперь этим воспользуемся
[41:11.520 --> 41:13.520]  и найдем
[41:13.520 --> 41:15.520]  математическое ожидание
[41:15.520 --> 41:17.520]  такой дискретной случайной
[41:17.520 --> 41:19.520]  величины, как Бернулевская.
[41:19.520 --> 41:21.520]  У нее, как мы помним, параметр P.
[41:21.520 --> 41:23.520]  Чему оно равно?
[41:23.520 --> 41:25.520]  Мат ожидания.
[41:25.520 --> 41:27.520]  Мы должны вот по этой формуле взять
[41:27.520 --> 41:29.520]  значение, которое она принимает,
[41:29.520 --> 41:31.520]  и множеная вероятность.
[41:31.520 --> 41:33.520]  Бернулевская случайная величина
[41:33.520 --> 41:35.520]  принимает значение 0
[41:35.520 --> 41:37.520]  с вероятностью Q
[41:37.520 --> 41:39.520]  и значение 1
[41:39.520 --> 41:41.520]  с вероятностью P.
[41:41.520 --> 41:43.520]  То есть получается P.
[41:43.520 --> 41:45.520]  Математическое ожидание
[41:45.520 --> 41:47.520]  Бернулевской случайной величины равно P.
[41:47.520 --> 41:49.520]  Ну, на самом деле
[41:49.520 --> 41:51.520]  это еще следует
[41:51.520 --> 41:53.520]  вот отсюда. Мы уже это, собственно,
[41:53.520 --> 41:55.520]  сделали, потому что индикаторная
[41:55.520 --> 41:57.520]  функция, это
[41:57.520 --> 41:59.520]  там, Бернулевская случайная
[41:59.520 --> 42:01.520]  величина.
[42:01.520 --> 42:03.520]  Только заданная еще через исходное
[42:03.520 --> 42:05.520]  вероятностное пространство.
[42:05.520 --> 42:07.520]  А теперь давайте поймем,
[42:07.520 --> 42:09.520]  чему равно математическое
[42:09.520 --> 42:11.520]  ожидание
[42:11.520 --> 42:13.520]  биномиальной случайной величины,
[42:13.520 --> 42:15.520]  у которой два параметра
[42:15.520 --> 42:17.520]  N и P. Согласно
[42:17.520 --> 42:19.520]  нашей формуле, это равно
[42:19.520 --> 42:21.520]  сумма
[42:21.520 --> 42:23.520]  значения, которое принимает
[42:23.520 --> 42:25.520]  случайная величина K,
[42:25.520 --> 42:27.520]  на вероятность того, что она примет это значение,
[42:27.520 --> 42:29.520]  это C из N по K,
[42:29.520 --> 42:31.520]  P в степени K,
[42:31.520 --> 42:33.520]  Q в степени N минус K.
[42:33.520 --> 42:35.520]  K изменяется
[42:35.520 --> 42:37.520]  от нуля до N.
[42:37.520 --> 42:39.520]  Вот
[42:39.520 --> 42:41.520]  чему это равно, с одной стороны.
[42:41.520 --> 42:43.520]  С другой стороны,
[42:43.520 --> 42:45.520]  это равно математическому
[42:45.520 --> 42:47.520]  ожиданию
[42:47.520 --> 42:49.520]  суммы в количестве
[42:49.520 --> 42:51.520]  N штук, каких случайных
[42:51.520 --> 42:53.520]  величин?
[42:53.520 --> 42:55.520]  Бернулевских.
[42:59.520 --> 43:01.520]  Помните, да,
[43:01.520 --> 43:03.520]  когда мы вводили
[43:03.520 --> 43:05.520]  биномиальную случайную величину,
[43:05.520 --> 43:07.520]  мы говорили, что это на самом деле сумма
[43:07.520 --> 43:09.520]  Бернулевских. Мат ожидания
[43:09.520 --> 43:11.520]  суммы равно сумме мат ожиданий,
[43:11.520 --> 43:13.520]  и получается,
[43:13.520 --> 43:15.520]  что это равно N на P.
[43:15.520 --> 43:17.520]  То есть,
[43:17.520 --> 43:19.520]  вот эта вот сумма
[43:19.520 --> 43:21.520]  равна N,
[43:21.520 --> 43:23.520]  П.
[43:23.520 --> 43:25.520]  Обошлись без суммирования,
[43:25.520 --> 43:27.520]  воспользовались свойством линейности.
[43:27.520 --> 43:29.520]  Ну, надо сказать,
[43:29.520 --> 43:31.520]  что все наши канонические
[43:31.520 --> 43:33.520]  дискретные случайные величины,
[43:33.520 --> 43:35.520]  они все положительные,
[43:35.520 --> 43:37.520]  и поэтому вот это условие
[43:37.520 --> 43:39.520]  выполняется автоматически.
[43:39.520 --> 43:41.520]  Но формула верна, конечно,
[43:41.520 --> 43:43.520]  не только для случая,
[43:43.520 --> 43:45.520]  когда exit больше нуля,
[43:45.520 --> 43:47.520]  потому что, как вы помните,
[43:47.520 --> 43:49.520]  в теории Лебега просто не такая
[43:49.520 --> 43:51.520]  логика. Сначала вводится для положительных
[43:51.520 --> 43:53.520]  функций, потом произвольная
[43:53.520 --> 43:55.520]  функция разбивается на сумму
[43:55.520 --> 43:57.520]  положительный и отрицательный.
[43:57.520 --> 43:59.520]  Отрицательный берется с минусом,
[43:59.520 --> 44:01.520]  получается положительная,
[44:01.520 --> 44:03.520]  считаются два интеграла Лебега,
[44:03.520 --> 44:05.520]  и один вычитается из другого.
[44:05.520 --> 44:07.520]  Поэтому это не принципиально,
[44:07.520 --> 44:09.520]  просто не буду это все проделывать.
[44:09.520 --> 44:11.520]  То есть, математическое ожидание
[44:11.520 --> 44:13.520]  дискретной случайной величины
[44:13.520 --> 44:15.520]  всегда определяется вот такой формулой,
[44:15.520 --> 44:17.520]  независимо от того
[44:17.520 --> 44:19.520]  положительные вот эти exit
[44:19.520 --> 44:21.520]  или отрицательные.
[44:21.520 --> 44:23.520]  Нет вопросов, все понятно.
[44:23.520 --> 44:25.520]  Ну давайте теперь
[44:27.520 --> 44:29.520]  давайте теперь
[44:31.520 --> 44:33.520]  ну чего-то
[44:33.520 --> 44:35.520]  попробуем получить
[44:35.520 --> 44:37.520]  с этими мат ожиданиями
[44:37.520 --> 44:39.520]  какой-то прок извлечь.
[44:39.520 --> 44:41.520]  По крайней мере, может быть, понять к чему это все,
[44:41.520 --> 44:43.520]  как они связаны,
[44:43.520 --> 44:45.520]  что из чего следует.
[44:45.520 --> 44:47.520]  Для этого давайте рассмотрим
[44:47.520 --> 44:49.520]  вот такую конструкцию
[44:49.520 --> 44:51.520]  математическое ожидание
[44:51.520 --> 44:53.520]  КС минус А в степени К.
[44:57.520 --> 44:59.520]  Вот такое мат ожидание называется
[44:59.520 --> 45:01.520]  кратым моментом
[45:01.520 --> 45:03.520]  случайной величины КС
[45:03.520 --> 45:05.520]  относительно А.
[45:05.520 --> 45:07.520]  Числа А.
[45:07.520 --> 45:09.520]  Кратым моментом
[45:09.520 --> 45:11.520]  случайной величины КС
[45:11.520 --> 45:13.520]  относительно числа А.
[45:13.520 --> 45:15.520]  Значит,
[45:15.520 --> 45:17.520]  ну, это
[45:17.520 --> 45:19.520]  что называется, так сказать, общий вид,
[45:19.520 --> 45:21.520]  рассматриваются два частных случая.
[45:21.520 --> 45:23.520]  А равно нулю
[45:23.520 --> 45:25.520]  и тогда это
[45:25.520 --> 45:27.520]  ЕКСИ-катая
[45:27.520 --> 45:29.520]  и называется
[45:29.520 --> 45:31.520]  кратым начальным моментом.
[45:31.520 --> 45:33.520]  Вот это уже расхожий термин.
[45:33.520 --> 45:35.520]  Катый начальный момент.
[45:37.520 --> 45:39.520]  И второй случай, который, так сказать,
[45:39.520 --> 45:41.520]  рассматривается, это
[45:41.520 --> 45:43.520]  А равно мат ожидания КС.
[45:43.520 --> 45:45.520]  Тогда это
[45:45.520 --> 45:47.520]  получается КС минус мат ожидания
[45:47.520 --> 45:49.520]  КС в степени К.
[45:49.520 --> 45:51.520]  И вот такое мат ожидание
[45:51.520 --> 45:53.520]  называется катым
[45:53.520 --> 45:55.520]  центральным моментом.
[45:55.520 --> 45:57.520]  Вот это катый начальный момент,
[45:57.520 --> 45:59.520]  катый центральный момент.
[46:05.520 --> 46:07.520]  Первый начальный момент
[46:07.520 --> 46:09.520]  это что в наших терминах?
[46:11.520 --> 46:13.520]  Это, собственно, мат ожидания ЕКСИ.
[46:13.520 --> 46:15.520]  Первый центральный
[46:15.520 --> 46:17.520]  момент чему равен?
[46:17.520 --> 46:19.520]  Нулю.
[46:19.520 --> 46:21.520]  А вот для второго центрального момента
[46:25.520 --> 46:27.520]  есть свое название
[46:27.520 --> 46:29.520]  и называется это дисперсии
[46:29.520 --> 46:31.520]  случайной увеличены КС.
[46:33.520 --> 46:35.520]  Ну, еще традиционно
[46:35.520 --> 46:37.520]  используют обозначение
[46:37.520 --> 46:39.520]  Сигма квадрат и тогда Сигма
[46:39.520 --> 46:41.520]  называется средне квадратичным
[46:41.520 --> 46:43.520]  отклонением. Средне квадратичное
[46:43.520 --> 46:45.520]  отклонение.
[46:49.520 --> 46:51.520]  Ну, просто
[46:51.520 --> 46:53.520]  обращу внимание.
[46:53.520 --> 46:55.520]  Мы тут берем
[46:55.520 --> 46:57.520]  мат ожидания, применяем к некой
[46:57.520 --> 46:59.520]  ну, другой случайной
[46:59.520 --> 47:01.520]  увеличении КС, да?
[47:01.520 --> 47:03.520]  Ну и просто надо понять, определение
[47:03.520 --> 47:05.520]  как-нибудь меняется. Ну нет, определение
[47:05.520 --> 47:07.520]  конечно не меняется.
[47:07.520 --> 47:09.520]  Мат ожидания ФИ от КС,
[47:09.520 --> 47:11.520]  если ФИ баррельская функция, то есть
[47:11.520 --> 47:13.520]  случайно увеличена. Но это тот же самый
[47:13.520 --> 47:15.520]  интеграл Лебега, только будет
[47:15.520 --> 47:17.520]  ФИ КСИ от
[47:17.520 --> 47:19.520]  Омега ПДОмега.
[47:21.520 --> 47:23.520]  Здесь с определением
[47:23.520 --> 47:25.520]  все понятно.
[47:25.520 --> 47:27.520]  Вот.
[47:27.520 --> 47:29.520]  Значит,
[47:31.520 --> 47:33.520]  теперь давайте
[47:33.520 --> 47:35.520]  чего сделаем?
[47:35.520 --> 47:37.520]  Давайте, поскольку мы ввели дисперсию,
[47:37.520 --> 47:39.520]  мы получим ее некоторые
[47:39.520 --> 47:41.520]  свойства.
[47:41.520 --> 47:43.520]  Некоторые свойства дисперсии.
[47:43.520 --> 47:45.520]  Ну первое свойство,
[47:45.520 --> 47:47.520]  математическое ожидание
[47:47.520 --> 47:49.520]  дисперсии,
[47:49.520 --> 47:51.520]  то есть дисперсия равна
[47:51.520 --> 47:53.520]  математическому ожиданию КС
[47:53.520 --> 47:55.520]  квадрат, минус мат ожидания
[47:55.520 --> 47:57.520]  КС в квадрате.
[47:57.520 --> 47:59.520]  Не надо выводить, да?
[47:59.520 --> 48:01.520]  Вы знаете.
[48:01.520 --> 48:03.520]  Так, значит,
[48:03.520 --> 48:05.520]  второе свойство дисперсии,
[48:05.520 --> 48:07.520]  дисперсия
[48:07.520 --> 48:09.520]  АКСИ плюс
[48:09.520 --> 48:11.520]  В, ну давайте
[48:11.520 --> 48:13.520]  коротенько
[48:13.520 --> 48:15.520]  сделаем один раз.
[48:15.520 --> 48:17.520]  Математическое ожидание АКСИ
[48:17.520 --> 48:19.520]  плюс В,
[48:19.520 --> 48:21.520]  минус мат ожидания
[48:21.520 --> 48:23.520]  АКСИ плюс В
[48:23.520 --> 48:25.520]  в квадрате
[48:25.520 --> 48:27.520]  равно.
[48:27.520 --> 48:29.520]  По линейности раскрываем
[48:29.520 --> 48:31.520]  В сокращается, А выносится
[48:31.520 --> 48:33.520]  за скобки. Получается
[48:33.520 --> 48:35.520]  математическое ожидание
[48:35.520 --> 48:37.520]  А,
[48:37.520 --> 48:39.520]  вот тут вот КСИ
[48:39.520 --> 48:41.520]  центрированное.
[48:41.520 --> 48:43.520]  Это обозначение, которое
[48:43.520 --> 48:45.520]  рекомендую вам знать.
[48:45.520 --> 48:47.520]  Случайная влечина с кружочком сверху,
[48:47.520 --> 48:49.520]  это случайная
[48:49.520 --> 48:51.520]  влечина минус ее мат ожидания.
[48:51.520 --> 48:53.520]  Центрированная случайная влечина
[48:53.520 --> 48:55.520]  называют. Значит,
[48:55.520 --> 48:57.520]  АКСИ
[48:57.520 --> 48:59.520]  центрированная в квадрате
[48:59.520 --> 49:01.520]  по свойству мат ожидания это равно
[49:01.520 --> 49:03.520]  А квадрат,
[49:03.520 --> 49:05.520]  а мат ожидания
[49:05.520 --> 49:07.520]  центрированной случайной влечины
[49:07.520 --> 49:09.520]  в квадрате это, собственно, дисперсия есть.
[49:09.520 --> 49:11.520]  А квадрат дисперсия КСИ.
[49:11.520 --> 49:13.520]  Значит, вот такое свойство.
[49:13.520 --> 49:15.520]  Дисперсия АКСИ плюс В
[49:15.520 --> 49:17.520]  равно А квадрат дисперсии КСИ.
[49:17.520 --> 49:19.520]  Давайте пока
[49:19.520 --> 49:21.520]  эти два свойства
[49:21.520 --> 49:23.520]  возьмем.
[49:23.520 --> 49:25.520]  Думаю, тут уже можно стирать.
[49:27.520 --> 49:29.520]  Вот только второй закон Ньютона
[49:29.520 --> 49:31.520]  ставлю здесь.
[49:33.520 --> 49:35.520]  Значит, следующий
[49:35.520 --> 49:37.520]  объект,
[49:37.520 --> 49:39.520]  который мы введем,
[49:39.520 --> 49:41.520]  используя понятие математического ожидания,
[49:41.520 --> 49:43.520]  это кавариация
[49:43.520 --> 49:45.520]  двух случайных величин.
[49:45.520 --> 49:47.520]  Кавариация
[49:47.520 --> 49:49.520]  АКСИ это
[49:49.520 --> 49:51.520]  по определению
[49:51.520 --> 49:53.520]  это есть
[49:53.520 --> 49:55.520]  математическая
[49:55.520 --> 49:57.520]  влечина.
[49:57.520 --> 49:59.520]  Кавариация АКСИ
[49:59.520 --> 50:01.520]  это по определению
[50:01.520 --> 50:03.520]  это есть математическое ожидание
[50:03.520 --> 50:05.520]  КСИ
[50:05.520 --> 50:07.520]  минус мат ожидания КСИ
[50:07.520 --> 50:09.520]  умножить на это
[50:09.520 --> 50:11.520]  минус мат ожидания это.
[50:15.520 --> 50:17.520]  Ну, пока не очень понятно, зачем она нам.
[50:17.520 --> 50:19.520]  Ну, дальше станет понятно.
[50:19.520 --> 50:21.520]  А пока просто давайте отметим такие
[50:21.520 --> 50:23.520]  свойства. Первое свойство
[50:23.520 --> 50:25.520]  кавариация
[50:25.520 --> 50:27.520]  КСИ это
[50:27.520 --> 50:29.520]  равна
[50:29.520 --> 50:31.520]  математическому ожиданию КСИ
[50:31.520 --> 50:33.520]  умножить на это
[50:33.520 --> 50:35.520]  минус мат ожидания КСИ
[50:35.520 --> 50:37.520]  умножить на мат ожидания это.
[50:37.520 --> 50:39.520]  Ну,
[50:39.520 --> 50:41.520]  не делаю, собственно,
[50:41.520 --> 50:43.520]  просто раскрыть скобки, воспользоваться
[50:43.520 --> 50:45.520]  линейностью.
[50:45.520 --> 50:47.520]  Второе,
[50:47.520 --> 50:49.520]  вот второе уже специфичное, как бы,
[50:49.520 --> 50:51.520]  для нашей теории,
[50:51.520 --> 50:53.520]  кавариация случайной величины
[50:53.520 --> 50:55.520]  сама с собой, чему равна?
[50:55.520 --> 50:57.520]  На
[50:57.520 --> 50:59.520]  определение смотрим.
[51:01.520 --> 51:03.520]  Дисперсия.
[51:05.520 --> 51:07.520]  То есть кавариация сама с собой
[51:07.520 --> 51:09.520]  это дисперсия.
[51:09.520 --> 51:11.520]  И третье свойство, которое
[51:11.520 --> 51:13.520]  мы сейчас выделим,
[51:13.520 --> 51:15.520]  значит,
[51:15.520 --> 51:17.520]  вот, выпишу его,
[51:17.520 --> 51:19.520]  кавариация КСИ это
[51:19.520 --> 51:21.520]  по определению
[51:21.520 --> 51:23.520]  этот интеграл
[51:23.520 --> 51:25.520]  КСИ от Омега минус ЕКСИ
[51:27.520 --> 51:29.520]  умножить на это
[51:29.520 --> 51:31.520]  от Омега
[51:31.520 --> 51:33.520]  минус ЕЭто
[51:33.520 --> 51:35.520]  и ПДОмега.
[51:37.520 --> 51:39.520]  Вот здесь поставлю квадрат,
[51:39.520 --> 51:41.520]  тогда вот это все в квадрате.
[51:43.520 --> 51:45.520]  И по неравенству к ошибке
[51:45.520 --> 51:47.520]  Буниковского, это меньше
[51:47.520 --> 51:49.520]  или равно,
[51:49.520 --> 51:51.520]  чем интеграл
[51:51.520 --> 51:53.520]  КСИ от Омега
[51:53.520 --> 51:55.520]  минус ЕКСИ
[51:55.520 --> 51:57.520]  в квадрате
[51:57.520 --> 51:59.520]  ПДОмега
[51:59.520 --> 52:01.520]  умножить
[52:01.520 --> 52:03.520]  на интеграл
[52:03.520 --> 52:05.520]  ЭТО от Омега
[52:05.520 --> 52:07.520]  минус
[52:07.520 --> 52:09.520]  мат ожидания
[52:09.520 --> 52:11.520]  ЭТО в квадрате
[52:11.520 --> 52:13.520]  ПДОмега.
[52:13.520 --> 52:15.520]  То есть равно дисперсии
[52:15.520 --> 52:17.520]  КСИ умножить
[52:17.520 --> 52:19.520]  на дисперсию ЭТО.
[52:19.520 --> 52:21.520]  Вот это свойство, которое
[52:21.520 --> 52:23.520]  мы как бы
[52:23.520 --> 52:25.520]  положим в копилку.
[52:25.520 --> 52:27.520]  Ковариация в квадрате
[52:27.520 --> 52:29.520]  случайных величин
[52:29.520 --> 52:31.520]  меньше или равно произведению их дисперсии.
[52:35.520 --> 52:37.520]  Дальше, что еще мы можем
[52:37.520 --> 52:39.520]  получить?
[52:49.520 --> 52:51.520]  Давайте определим класс
[52:51.520 --> 52:53.520]  случайных величин
[52:53.520 --> 52:55.520]  с вероятностью 1
[52:55.520 --> 52:57.520]  больше 0,
[52:57.520 --> 52:59.520]  больше равно 0.
[52:59.520 --> 53:01.520]  А если мы
[53:01.520 --> 53:03.520]  определим класс
[53:03.520 --> 53:05.520]  случайных величин
[53:05.520 --> 53:07.520]  в квадрате
[53:07.520 --> 53:09.520]  ПДОмега,
[53:09.520 --> 53:11.520]  то мы можем
[53:11.520 --> 53:13.520]  определить класс
[53:13.520 --> 53:15.520]  случайных величин
[53:15.520 --> 53:17.520]  с вероятностью 1
[53:17.520 --> 53:26.760]  или равно нуля? Ну, таким естественным образом. То есть рассмотрим случайные
[53:26.760 --> 53:33.840]  величины с вероятностью единицы не отрицательные. Тогда для таких случайных
[53:33.840 --> 53:37.600]  величин имеет место такое неравенство.
[53:37.600 --> 53:44.880]  кси меньше или равно епсилон, больше или равно епсилон, кси больше или равно
[53:44.880 --> 53:50.040]  епсилон, меньше или равно математическому ожиданию кси делить на
[53:50.040 --> 53:56.680]  епсилон. Знакома вам такая формула? Ну, в каком-то виде, да, так сказать, вы
[53:56.680 --> 54:06.760]  там ее получали. Как она называется? Неравенство Маркова, да. Я почему
[54:06.760 --> 54:14.040]  уточняю, потому что иногда называют неравенством Чебышева. И, в принципе, может
[54:14.040 --> 54:18.280]  это отчасти справедливо, поскольку Марков ученик Чебышева, они как бы так
[54:18.280 --> 54:24.880]  перепутаны у них работой, и там не поймешь, так сказать, там кто. Вот, но вроде бы вот
[54:24.880 --> 54:29.600]  так вот, вот это конкретно неравенство, правильно называть неравенством Маркова.
[54:29.600 --> 54:38.840]  Ну, давайте его докажем. Вероятностные меры омега такие, что кси от омега больше
[54:38.840 --> 54:44.280]  равно епсилон, это на самом деле вероятность индикаторного события кси от
[54:44.280 --> 54:52.880]  омега больше равно епсилон. Поэтому это интеграл, индикаторная функция по
[54:52.880 --> 55:04.200]  множеству кси от омега больше равно епсилон от омега, pd омега. Ну а теперь заметим,
[55:04.200 --> 55:12.800]  что для любого омега кси от омега делить на
[55:12.800 --> 55:29.600]  епсилон больше или равно индикаторной функции соответствующей. Согласны, да? Там, где
[55:29.600 --> 55:34.960]  индикаторная функция равна единице при тех омега, значит при тех омега кси от
[55:34.960 --> 55:40.440]  омега больше равна епсилон, но отношение, соответственно, больше единицы. А там, где
[55:40.440 --> 55:45.560]  индикаторная функция равна нулю, кси-то от омега положительная, не отрицательная,
[55:45.560 --> 55:52.080]  поэтому эта штука не отрицательная. Ну а дальше, значит, меньше или равно по
[55:52.080 --> 56:05.800]  свойствам интеграла Либега единица на епсилон, интеграл по омега, кси от омега, pd омега. То есть
[56:05.800 --> 56:17.320]  мат ожидания кси делить на епсилон. Вот такое немудренное доказательство. Ну и какой-то
[56:17.320 --> 56:23.080]  содержательный первый вывод, который из него можно сделать, состоит в следующем.
[56:36.800 --> 56:42.520]  А, ну, кстати, неравенство Маркова строгое в том смысле, что можно привести пример,
[56:42.520 --> 56:50.400]  когда именно больше или равно, именно равно мат ожидания кси на епсилон. То есть неулучаемая,
[56:50.400 --> 56:57.440]  так вообще говоря. Вот, давайте в качестве положительной случайной величины возьмем
[56:57.440 --> 57:04.920]  случайную величину кси минус мат ожидания кси в квадрате. Ну она, понятное дело, больше равна нуля.
[57:04.920 --> 57:13.840]  И для нее неравенство Маркова кси минус мат ожидания кси в квадрате. Больше ли равно
[57:13.840 --> 57:20.720]  епсилон? Ну мы возьмем епсилон в квадрат, имеем право. С одной стороны, это вероятность того,
[57:20.720 --> 57:28.040]  что модуль кси минус мат ожидания кси больше или равно епсилон. А с другой стороны, по неравенству
[57:28.040 --> 57:37.840]  Маркова меньше или равно мат ожидания вот этого выражения, которое из дисперсии кси делить на
[57:37.840 --> 57:52.520]  епсилон квадрат. Я когда говорил о дисперсии, забыл сказать очевидный факт, что для константы
[57:52.520 --> 58:00.840]  дисперсия равна нулю. Это так сказать очевидный факт. Вот из этого неравенства следует и в обратную
[58:00.840 --> 58:09.800]  сторону. Если дисперсия какой-то случайной величины равна нулю, то она совпадает со своим мат ожиданием,
[58:09.800 --> 58:19.760]  поскольку это для любого епсилона верно. Ну и, собственно, вот это, этот факт, или точнее говоря,
[58:19.760 --> 58:25.600]  вот это неравенство, вот это неравенство уже называют неравенством Чебышева бывает. Вот это
[58:25.600 --> 58:33.360]  неравенство устанавливает связь между мерой отклонения случайной величины от ее мат ожидания
[58:33.360 --> 58:41.000]  и дисперсией. Если дисперсия очень маленькая, то с очень большой вероятностью случайная величина
[58:41.000 --> 58:49.320]  сконцентрирована в окрестности своего мат ожидания. Ну а если дисперсия равна нулю, значит случайная
[58:49.320 --> 58:57.440]  величина просто с вероятностью единицы равна своему мат ожиданию. Является константой. Вот, значит,
[58:57.440 --> 59:05.320]  ну вот как бы первое какое-то понимание, для чего нам нужны вот эти числовые характеристики.
[59:05.320 --> 59:07.920]  Теперь мы должны двинуться дальше.
[59:17.440 --> 59:20.240]  Ну ладно, вернусь потом.
[59:26.920 --> 59:34.520]  На чем мы остановились? Мы остановились на том, что для дискретных случайных величин мы умеем
[59:34.520 --> 59:40.760]  вычислять мат ожидания на основании функции распределения, ну и дальше с этими от ожиданиями
[59:40.760 --> 59:47.480]  делать, ну, то, чего там у нас получается. Но у нас остался вопрос, что делать с непрерывными
[59:47.480 --> 59:54.360]  случайными величинами. Для того, чтобы на него ответить, давайте вернемся к функции СН и от
[59:54.360 --> 01:00:07.960]  Омега, но только мы ее чуть-чуть видоизменим так. Х житая, а индикаторная функция будет
[01:00:07.960 --> 01:00:17.000]  не какого-то произвольного множества житая, а вот такое. Омега такие что, кси от Омега,
[01:00:17.000 --> 01:00:22.800]  то есть мы взяли какую-то случайную величину кси от Омега, вот она у нас есть, и для нее
[01:00:22.800 --> 01:00:31.680]  строим вот такой класс функций. Меньше х жи плюс один, больше ли равно х житого.
[01:00:31.680 --> 01:00:47.760]  Ну, ж равно от единицы пусть будет до n минус один. То есть, это частный случай вот тех простых
[01:00:47.760 --> 01:00:55.960]  функций, которые мы с вами ввели, для которых определено мат ожидания, частный в том смысле,
[01:00:55.960 --> 01:01:03.040]  что мы вот так специфичным образом, так сказать, взяли эти множества аитые. Вот таким образом.
[01:01:03.040 --> 01:01:09.080]  Ну, честно говоря, это она пока не до конца определена. Не понятно, чему она равна,
[01:01:09.080 --> 01:01:16.440]  когда Омега такие, когда кси от Омега меньше х один и больше равно х жи плюс один. Мы можем
[01:01:16.520 --> 01:01:29.800]  определить, значит, сн от Омега равно нулю, если кси от Омега меньше х один и сн от Омега равно
[01:01:29.800 --> 01:01:39.040]  х н, если кси от Омега больше равно х н. Так вот, чтобы была определена эта функция. Это тоже
[01:01:39.040 --> 01:01:47.800]  простая функция в терминах интеграла Либега. Ну, еще раз просто скажу, мы, понимая, как с этим
[01:01:47.800 --> 01:01:54.560]  поступить, не говорим, что все х т больше нуля. Но единственное, что мы их здесь упорядочим. Ну,
[01:01:54.560 --> 01:02:05.200]  чтобы вот это было корректно. Они у нас х один меньше х и х два, так далее, меньше х. Ведем вот такой
[01:02:05.200 --> 01:02:14.760]  класс функций. Это тоже простые функции. Но, как в теории Либега доказывается, для любой измеримой
[01:02:14.760 --> 01:02:25.880]  функции кси от Омега, вот в этом классе существует последовательность функций сн от Омега, которые,
[01:02:25.880 --> 01:02:36.960]  возрастая, поточечно сходится кси от Омега. Был такой результат у вас? В этом классе мы можем
[01:02:36.960 --> 01:02:47.200]  найти вот такую последовательность. Еще у нас есть теорема Леви, которая говорит, что интеграл Либега,
[01:02:47.200 --> 01:02:55.600]  то бишь, математическое ожидание кси, которое равно интегралу Либега кси от Омега, ПДОмега,
[01:02:55.600 --> 01:03:08.120]  Омега, которое равно предел сн от Омега. Ну, поскольку мы так подобрали сн от Омега,
[01:03:08.120 --> 01:03:19.040]  что ее предел равен кси от Омега. Значит, ПДОмега, Омега. И, собственно, это пока жонглирование
[01:03:19.040 --> 01:03:29.200]  определениями. А, собственно, содержание теоремы, вот оно, это равно пределу, когда n стремится к
[01:03:29.200 --> 01:03:42.200]  бесконечности. Интеграл Либега сн от Омега, ПДОмега. Теорема Леви. В современной теории,
[01:03:42.200 --> 01:03:49.400]  когда интеграл Либега вводят, там вводят интеграл от произвольной функции как супремум по всем
[01:03:49.400 --> 01:03:59.840]  функциям меньше. И тогда, и тогда теорема, вот этот результат является теоремой Леви. Но сам Либег
[01:03:59.840 --> 01:04:07.280]  вот так водил свой интеграл. Это у него было определение. Ну, просто как бы со временем более
[01:04:07.280 --> 01:04:13.960]  технологично, так как делают сейчас. Но, тем не менее, такой результат есть. Что отсюда следует
[01:04:13.960 --> 01:04:22.640]  для нас, помня о том, что сн от Омега это дискретная случайная величина. Во-первых, мы получили алгоритм,
[01:04:22.640 --> 01:04:29.200]  механизм, как мы можем посчитать математическое ожидание произвольной случайной величины кси. Вот
[01:04:29.200 --> 01:04:37.440]  таким образом. Потому что вот это в наших терминах это математическое ожидание сн. Надо взять
[01:04:37.440 --> 01:04:45.160]  пределы сн, предел дискретных случайных величин, ну, правильным образом построенных. И тогда мы
[01:04:45.160 --> 01:04:52.840]  получим математическое ожидание произвольной случайной величины. Вот. Это первое, что, значит,
[01:04:52.840 --> 01:05:01.960]  мы отсюда извлечем. И второй такой, как бы сказать, методологический факт. Большинство свойств
[01:05:01.960 --> 01:05:08.520]  мат ожиданий, которые верны для дискретных случайных величин, будут верны и для непрерывных
[01:05:08.520 --> 01:05:14.360]  случайных величин. То, что можно сделать предельным переходом. Почти все можно. Ну, какие-то есть
[01:05:14.360 --> 01:05:20.760]  случаи, когда нельзя. И мы, кстати, с ними столкнемся с одним случаем. Но, в принципе,
[01:05:20.760 --> 01:05:29.600]  большинство свойств можно как бы доказать только для дискретных случайных величин. А для
[01:05:29.600 --> 01:05:37.800]  непрерывных это будет следовать из предельного перехода. Значит, вот. Следующим шагом. А,
[01:05:37.800 --> 01:05:51.040]  ну, собственно, да. Вот мы получили, что мот ожидания кси это есть предел вот таких сн. Или,
[01:05:51.040 --> 01:05:57.000]  точнее говоря, предел мот ожиданий сн. А что представляет из себя мот ожиданий сн?
[01:05:57.000 --> 01:06:20.880]  Что представляет из себя мот ожиданий сн? А мот ожиданий сн
[01:06:20.880 --> 01:06:31.840]  равно сумма хж на вероятность вот этого множества. А вероятность этого множества что такое?
[01:06:31.840 --> 01:06:44.080]  Просто по какому-то счастливейшему для нас течению обстоятельств это f кси от хg плюс 1
[01:06:44.080 --> 01:07:00.960]  минус f кси от хg. Правильно? Оказывается, что математическое ожидание той дискретной случайной
[01:07:00.960 --> 01:07:06.360]  величины, предел которой даст нам математическое ожидание непрерывной случайной величины,
[01:07:06.360 --> 01:07:14.640]  мы его можем тоже вычислить исходя только из его функции распределения. Только из его
[01:07:14.640 --> 01:07:22.000]  функции распределения. Ну, и если мы там n устремляем к нулю, то, как мы уже с вами остановились,
[01:07:22.000 --> 01:07:34.000]  с одной стороны это сходится к мот ожиданию кси. Ну, это так как схематическое, да. А с другой
[01:07:34.000 --> 01:07:41.280]  стороны это сходится, ну, точнее говоря, к мат ожиданию кси. Но поскольку вот это на что похоже
[01:07:41.280 --> 01:07:52.080]  так сильно? На конечную интегральную сумму, да. То вот эту штуку мы будем обозначать,
[01:07:52.080 --> 01:08:03.320]  вводим обозначение х df кси от х уже по области х от минус бесконечности до бесконечности. Вот
[01:08:03.320 --> 01:08:10.800]  этот вот интеграл называется интегралом Либега Стилтиса. Интеграл Либега Стилтиса.
[01:08:10.800 --> 01:08:17.600]  Вообще-то или Стилтьеса. Вообще-то Стилтьес этот интеграл вводил как бы из других неких
[01:08:17.600 --> 01:08:27.160]  соображений, но тут он хорошо прижился. Никакой особой как бы он там глубокого смысла не несет,
[01:08:27.160 --> 01:08:35.240]  это тот же самый интеграл Либега. Ну, просто, так сказать, мера, на которой производится интегрирование,
[01:08:35.240 --> 01:08:44.920]  задается функцией распределения. И его такая практическая ценность состоит в том, что для
[01:08:44.920 --> 01:08:53.680]  дискретных случайных величин он превращается в сумму икджита на вероятность того, что кси равно
[01:08:53.680 --> 01:09:02.680]  икджита. А для непрерывных случайных величин, глядя вот на эту формулу до перехода,
[01:09:02.680 --> 01:09:14.080]  он превращается в интеграл х, плотность, функция плотности f кси от х dx минус бесконечности до
[01:09:14.080 --> 01:09:26.440]  бесконечности. Вот удобный формализм, который позволяет не говорить, сделаем ли дискретного,
[01:09:26.440 --> 01:09:34.480]  сделаем ли непрерывного, сделаем ли интеграла Стилтьеса, Либега Стилтьеса. Вот это вот в свой
[01:09:34.480 --> 01:09:43.120]  багаж, так сказать, положите. Давайте я вот здесь вот эту выпишу формулу общую, что мат ожидания кси равно
[01:09:43.120 --> 01:10:07.200]  х df кси от х. Тоже нам полезно знать. Так, ну ни у кого нет какого-то ощущения такой незаконченности.
[01:10:07.200 --> 01:10:18.960]  Ну иногда мне студенты задают правильный вопрос. Они говорят, вот если мат ожидания кси вот
[01:10:18.960 --> 01:10:23.760]  представляется в виде интеграла Стилтьеса, все понятно. А давайте рассмотрим случайную
[01:10:23.760 --> 01:10:37.960]  личину это, которая есть фи от кси. Тогда мат ожидания это как бы по аналогии это фи от х df кси от х,
[01:10:37.960 --> 01:10:45.640]  но с другой стороны у случайной личины это своя же функция распределения и абсолютно по
[01:10:45.640 --> 01:10:59.840]  определению мат ожидания это равно интегралу у df это от у. А эти величины вообще совпадают.
[01:10:59.840 --> 01:11:09.400]  Корректно ли наше определение? Ну честно говоря, конечно, ну я бы был очень удивлен,
[01:11:09.400 --> 01:11:15.480]  если бы получались разные результаты, потому что ну здесь как бы такие довольно, так сказать,
[01:11:15.480 --> 01:11:21.000]  там безобидные переходы. Вот и негде спрятаться. Но тем не менее формально вопрос такой возникает.
[01:11:21.000 --> 01:11:30.200]  Ну и давайте мы это один раз сделаем. Для дискретных случайных величин.
[01:11:30.200 --> 01:11:44.680]  Или другими словами, для простых функций в смысле интеграла Либега. Ну давайте пишем,
[01:11:44.680 --> 01:11:51.400]  значит пусть это у нас случайная величина, которая определяется набором вот таких вероятностей,
[01:11:51.400 --> 01:12:01.800]  это равно у gt, а кси случайная величина дискретная, которая определяется набором вероятностей,
[01:12:01.800 --> 01:12:12.440]  кси равно хк. Ну в счетном количестве в общем случае. Ну и давайте напишем мат ожидания это.
[01:12:12.440 --> 01:12:26.280]  По определению это сумма у gt на вероятность того, что это равна у gt. Сумма по g равно сумма по g,
[01:12:26.280 --> 01:12:37.560]  у gt. А вот эту вероятность мы представим вот в таком виде. Сумма вероятностей того,
[01:12:37.560 --> 01:12:52.600]  что кси равно хк, где суммирование по таким индексам k, что фи от хк равно у gt. Правильно,
[01:12:52.600 --> 01:13:02.480]  да? Это равняется у gt, этот это на самом деле фи от кси. Для того, чтобы она равнялась у gt,
[01:13:02.480 --> 01:13:11.880]  вот эти хк каким-то значением должны равняться. Может одному, может несколько. Вот значит равно
[01:13:11.880 --> 01:13:26.160]  сумма по g, а здесь я напишу сумму пока вот по таким же самым фи от хк равно у gt, фи от хк
[01:13:26.160 --> 01:13:37.760]  на вероятность того, что кси равно хк. Правильно, да? Вот это у gt вношу в эту сумму,
[01:13:37.760 --> 01:13:43.440]  поскольку оно для этих слагаемых всегда одно и то же. Ну и заменяю на фи хк,
[01:13:43.440 --> 01:13:51.720]  которое внутри этой суммы равно у gt. Ну и видно, что это просто перестановка членов вот такого ряда.
[01:13:51.720 --> 01:14:04.800]  Сумма фи хк на вероятность того, что кси равно хк или в наших обозначениях математическое ожидание
[01:14:04.800 --> 01:14:15.080]  фи от кси. Математическое ожидание фи от кси. Я напомню вам, что интеграл Лебега обладает тем
[01:14:15.080 --> 01:14:21.160]  свойством, что интеграл от функции и интеграл от модуля функции одновременно существует или
[01:14:21.160 --> 01:14:28.040]  не существует. Поэтому те ряды, то есть если мат ожидания существует, это означает, что соответствующие
[01:14:28.040 --> 01:14:33.720]  ряды для дискретных случайных величин не просто сходятся, а сходятся абсолютно. А в абсолютно
[01:14:33.720 --> 01:14:39.400]  сходящемся ряде можно как угодно переставлять его члены. Вот, собственно, чем мы воспользовались.
[01:14:39.400 --> 01:14:48.400]  Значит, корректность мы подтвердили. Значит, следующий важный факт. Правильно, я стер второй
[01:14:48.400 --> 01:15:00.720]  закон нитона. Мы до него не дойдем сегодня. Вот, значит, следующий важный факт. Это нам надо понять.
[01:15:00.720 --> 01:15:09.040]  Значит, мат ожидания суммы равно сумме мат ожиданий всегда. А вот мат ожидания произведения,
[01:15:09.040 --> 01:15:21.080]  когда равно мат ожиданию кси умножить на мат ожидания это. Ну, на самом деле вопрос как не совсем,
[01:15:21.080 --> 01:15:28.360]  как это, многогранный. Но, тем не менее, мы сейчас докажем, что это свойство выполнено для независимых
[01:15:28.360 --> 01:15:34.720]  случайных величин. Независимых случайных величин. И сделаем это тоже для дискретных случайных
[01:15:34.720 --> 01:15:47.000]  величин. Значит, кси определяется вероятностями кси равно хкт. Это определяется вероятностями
[01:15:47.000 --> 01:15:58.280]  это равно ygt. И z, которая есть кси на это случайная величина, она определяется какими-то вероятностями
[01:15:58.280 --> 01:16:13.720]  z равно zlt. Ну и давайте писать. Мат ожидания z равно сумма zlt на вероятность того, что z
[01:16:13.720 --> 01:16:29.000]  равно zlt. Сумма по l. Слушайте, я вот опущу, надеюсь, что, ну, напишу и как-то там поясню на
[01:16:29.000 --> 01:16:35.800]  словах, если будет непонятно. На самом деле это ничто иное, как двойная сумма по k и по i, по g,
[01:16:35.800 --> 01:16:51.040]  хкт на ygt на вероятность того, что кси равно хкт и одновременно с этим это равно ygt. Вот это понятно?
[01:16:51.040 --> 01:17:03.640]  То есть, ну, если какой-то g, точнее, какой-то z, значит он представим в виде суммы. Ну вот,
[01:17:03.640 --> 01:17:09.840]  по той же логике мы можем выбрать все такие пары, записать вот эту вероятность как сумму
[01:17:09.840 --> 01:17:16.720]  вероятностей, внести сюда, в общем, ну, пройти ту же логику и получить вот такую штуку. Но поскольку
[01:17:16.720 --> 01:17:23.200]  кси и это независимо, то вот эта вероятность распадается в произведение вероятностей. Это
[01:17:23.200 --> 01:17:30.160]  свойство, на которое мы обращали внимание. И тогда все это просто распадается в произведение двух
[01:17:30.160 --> 01:17:41.600]  сумм хкт на вероятность того, что кси равно хкт, умножить на сумму ygt на вероятность того,
[01:17:41.600 --> 01:17:52.960]  что это равно ygt. То есть, математическое ожидание кси умножить на математическое ожидание это.
[01:17:52.960 --> 01:18:01.080]  Вот такое, ну, важное свойство. То есть, математическое ожидание произведения
[01:18:01.080 --> 01:18:07.280]  случайных величин, независимых случайных величин равно произведению математических ожиданий.
[01:18:07.280 --> 01:18:17.760]  Ну и давайте тут некий промежуточный итог подведем, связанный с независимостью.
[01:18:17.760 --> 01:18:32.360]  Ну, во-первых, напоминаю, что кавариация двух случайных величин равна мат ожидания кси
[01:18:32.360 --> 01:18:42.200]  это минус мат ожидания кси умножить на мат ожидания это. Смотрим вот сюда. И тогда получается,
[01:18:42.360 --> 01:18:57.120]  что возникают вот такие связи. Кси это независимо. Тогда математическое ожидание кси на это равно
[01:18:57.120 --> 01:19:04.520]  математическому ожиданию кси умножить на мат ожидания это. Это утверждение в обе стороны
[01:19:04.520 --> 01:19:13.160]  эквивалентно тому, что кавариация кси это равно нулю. Ну и если кси это независимое,
[01:19:13.160 --> 01:19:25.800]  то кавариация, опять же, так сказать, равна нулю. Обратное не верно. То есть, если кавариация равна
[01:19:25.800 --> 01:19:35.640]  нулю, то не следует независимость кси и это. Но есть один чрезвычайно важный истерический
[01:19:35.640 --> 01:19:43.240]  с практической точки зрения пример, когда из некоррелированности компонент случайного вектора
[01:19:43.240 --> 01:19:51.440]  следует их независимость. Это нормальный случайный вектор. Мы не успеем сегодня им заняться,
[01:19:51.440 --> 01:20:10.080]  но там продолжим в следующий раз. Ну и в следующий раз докажем второй закон Ньютона
[01:20:10.080 --> 01:20:16.440]  для теории вероятности. Все коллеги, давайте отдыхайте. Спасибо.

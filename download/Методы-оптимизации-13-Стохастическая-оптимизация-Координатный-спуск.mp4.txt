[00:00.000 --> 00:10.640]  Начнем. Что сегодня, соответственно, про что сегодня разговариваем. Во-первых,
[00:10.640 --> 00:18.320]  продолжаем обсуждать стахастическую оптимизацию. Снова вернемся к методам редукции дисперсии,
[00:18.320 --> 00:24.440]  потому что в прошлый раз посмотрели только на них с точки зрения каких-то идей и интуиций. Сегодня
[00:24.440 --> 00:30.000]  уже строго покажем, что это действительно все безобразие работает. На второй части лекции
[00:30.000 --> 00:38.840]  поговорим про координатный спуск. Здесь я вам просто напоминаю, что у нас было в прошлый раз.
[00:38.840 --> 00:45.120]  Рассматривали с вами постановку вида конечной суммы. Рассматривали, на самом деле, не только ее,
[00:45.120 --> 00:50.520]  но более общие постановки стахастические. Но как раз во второй части лекции, когда говорили
[00:50.520 --> 00:54.640]  про методы редукции дисперсии, остановились именно на вот такой вот постановке, когда у нас
[00:54.640 --> 01:00.040]  целевая функция имеет вид конечной суммы. Полный градиент функций считать дорого,
[01:00.040 --> 01:07.840]  поэтому мы считаем только градиент по какому-то слагаемому. Эта слагаемая часто берется просто
[01:07.840 --> 01:14.000]  случайно и равномерно. То есть, генерируются какой-то индексы, который как раз нам и говорит,
[01:14.000 --> 01:19.840]  какой мы слагаемый выберем на данный момент. И в среднем, как вы понимаете, как мы с вами уже
[01:19.840 --> 01:25.280]  показывали, это будет действительно просто такой вот честный градиент. Но опять же, с дисперсией этого
[01:25.280 --> 01:30.840]  всего безобразия никто вам не гарантирует, что с ней будет все хорошо. Она может быть довольно большой,
[01:30.840 --> 01:36.320]  довольно большой. В связи с этим именно для задач конечной суммы придумали методы редукции
[01:36.320 --> 01:44.440]  дисперсии, редукции дисперсии, о которых мы уже с вами поговорили. Идея у них была такая,
[01:44.440 --> 01:50.400]  что нам нужно использовать какой-то стахистический градиент g. И суть этого стахистического градиента
[01:50.400 --> 01:57.800]  такая, что он стремится к честному значению функции f в точке х со звездой, которая стремится к нулю,
[01:57.800 --> 02:05.120]  если у нас, соответственно, процесс решения задачи стремится к х со звездой. И тогда все хорошо,
[02:05.120 --> 02:12.840]  тогда проблем с этим стахистическим градиентом не должно быть. В частности, мы с вами привели
[02:12.840 --> 02:25.360]  пример, это метод сага. Напоминаю его интуицию, то, что здесь мы запоминаем градиенты по бочам,
[02:25.360 --> 02:33.760]  которые посчитали. То есть посчитали градиент f и того, сохранили его в переменной y, y и t. И вот
[02:33.760 --> 02:39.000]  тогда в итоге у нас в этих y, точнее в усредненном значении, будет копиться такой усредненный
[02:39.000 --> 02:44.720]  градиент. Это все обсуждали, почему это хорошо. Сегодня, соответственно, докажем, почему это
[02:44.720 --> 02:50.860]  действительно хорошо с точки зрения теории. Но интуиция была понятна уже в прошлый раз. Говорили
[02:50.860 --> 02:57.200]  про это, то, что мы сохраняем то, что у нас есть. Здесь у нас в y в итоге сохранится такой запаздывающий
[02:57.200 --> 03:04.360]  градиент. В среднем это будет честный градиент и, соответственно, будет как раз вот это замечательное
[03:04.360 --> 03:09.440]  свойство. Так как у нас в итоге, если x ката стремится к созвездой, то в y у нас значение
[03:09.440 --> 03:15.720]  будет стремиться к значению градиента конкретного f житого в точке x и звездой. И тогда вот средний
[03:15.720 --> 03:22.320]  y будет стремиться к градиенту в точке x и звездой, что стремится к нулю. Тогда стахистический градиент,
[03:22.320 --> 03:28.200]  который мы используем, тоже будет стремиться к нулю, что хорошо и у нас пропадут асталляции.
[03:28.200 --> 03:35.000]  Проблема с памятью мы тоже обсуждали, но давайте теперь подоказываем. Все это безобразие, потому что
[03:35.000 --> 03:43.440]  пока мы все это делали в некотором смысле голословно, а хочется сделать это по-честному. Мы предполагали
[03:43.440 --> 03:49.120]  то, что у нас есть сходимость x как со звездой, но кто же знает, потому что метод настолько плохой,
[03:49.120 --> 03:57.120]  что он этого не гарантирует, поэтому вся его физика идет крахом. Ну давайте посмотрим,
[03:57.120 --> 04:03.760]  что там получается. Опять же начинаем супер уже знакомо. Расписываем, как у нас ведет себя
[04:03.760 --> 04:12.160]  расстояние до решения. Потом подставляем сам метод и получаем что-то вот такое.
[04:12.280 --> 04:26.520]  Дальше вы мне подсказываете, что мы делаем, чтобы как-то анализировать стахастику. Что делаем,
[04:28.520 --> 04:33.160]  чтобы сейчас со стахастикой работать. Пока непонятно, что с стахастикой делать. Что надо
[04:33.160 --> 04:39.000]  сделать-то? От ожидания. Берем условное математическое ожидание. Можно было полное,
[04:39.000 --> 04:46.520]  но по-хорошему лучше условное, которое как раз у нас зафиксирует всю предысторию,
[04:46.520 --> 04:51.600]  которая происходила до текущей итерации. Как мы с вами это уже помним, на случайной величины
[04:51.600 --> 04:57.400]  в данном случае будет xкат и плюс один, поэтому на него это условное мат. ожидание будет действовать
[04:57.400 --> 05:07.080]  дальше. Вот эта скобочка у нас первая. С xкат случайной с точки зрения этого математического
[05:07.080 --> 05:15.320]  ожидания не будет, поэтому я его сразу не накидываю, просто ее переписываю. Дальше xкат это
[05:15.320 --> 05:20.160]  тоже случайная величина, потому что она как раз генерируется на текущей итерации, поэтому на нее
[05:20.160 --> 05:26.040]  условное мат. ожидания действует. А то, что стоит второе в скалярном произведении, это как раз не
[05:26.040 --> 05:30.560]  случайная величина, поэтому я сразу условное мат. ожидание занес под скалярное произведение.
[05:30.560 --> 05:36.880]  Дальше соответственно так. Опять же xкат у нас случайная величина, поэтому норма xкат в
[05:36.880 --> 05:46.320]  квадрате тоже нужно промат. ожидать. Окей, ну давайте я здесь еще сразу же занесу вот этот нашумный 0,
[05:46.320 --> 05:55.040]  f от x звездой. Градиент f от x звездой. Ну понятно, в оптимуме наша вся целевая функция дает 0,
[05:55.040 --> 06:01.800]  так как мы решаем с вами безусловную задачу. Окей, с чем там надо обычно разбираться? Первое,
[06:01.800 --> 06:08.560]  это нужно разобраться с мат. ожиданием xкат условным. Ну я на самом деле ответ уже написал на предыдущем
[06:08.560 --> 06:14.560]  слайде. Ну давайте как раз покажем, что это будет несмещённой вещью. Окей, давайте разбираться,
[06:14.560 --> 06:22.920]  просто подставляем xкат, что там у нас было. Кстати, ну давайте тогда, раз уж заговорили
[06:22.920 --> 06:30.200]  при этом мат. ожидания, предлагается тогда немного посмотреть вообще на сам алгоритм. И вроде как,
[06:30.200 --> 06:37.000]  когда мы говорили про алгоритм, кажется, что вот как раз в yкат у нас в этой сумме yкат
[06:37.000 --> 06:44.360]  усреднённых и в некотором смысле копится запаздывающий градиент. Поэтому такая вот более-менее
[06:44.360 --> 06:52.280]  натуральная идея это просто менять yкат. То есть менять yкат и вот эту сумму yкат использовать
[06:52.280 --> 06:58.200]  как градиент, по которому я буду шагать. То есть делать не вот так, как здесь, а конкретно,
[06:58.200 --> 07:05.600]  ну например, вот так. То есть такая самая банальная идея, то есть не банальная, но приходящая на ум
[07:05.600 --> 07:11.560]  после того, как я вот воспользовался этой идеей с сагой, что сохранять мои запаздывающие градиенты,
[07:11.560 --> 07:18.240]  которые мне я считал на прошлой террации. Вот давайте я здесь выпишу. Идея, что, возможно,
[07:18.240 --> 07:22.920]  я как раз в качестве jkат буду просто использовать что-то в духе вот такого.
[07:22.920 --> 07:31.440]  yk plus 1 g, то есть суммировать по всем g и усреднять. Ну или по-другому это можно переписать вот так вот.
[07:31.440 --> 07:40.480]  Это вроде как y из прошлой террации, просто один из y у меня обновился, конкретно тот,
[07:40.480 --> 07:46.680]  который у меня является случайным. Поэтому вот здесь можно выписать что-то вот такое. То есть
[07:46.680 --> 08:01.480]  я вычитаю y k и k и вместо него добавляю честный градиент по вот этому сэмплу. Так? Понятная идея,
[08:01.480 --> 08:08.480]  да? То есть разница на самом деле только в том, что вот я вот здесь, вот этого нет в сага. Вот этого
[08:08.480 --> 08:14.880]  нет в сага. И вообще изначально мне казалось, когда я сам просто пытался разобраться в доказательствах,
[08:14.880 --> 08:21.360]  почему не берутся jkato в качестве, ну вот такое jkato, которое я здесь написал, в качестве градиента.
[08:21.360 --> 08:29.360]  Вот. И на самом деле это, честного ответа на этот вопрос нет. Я на него отвечу сейчас просто потому,
[08:29.360 --> 08:34.720]  что давайте посмотрим, что вот с таким jkato, давайте я стильдой обозначу, jkato стильдой,
[08:34.720 --> 08:39.320]  что у него происходит по математическому ожиданию. Вот. Ну давайте проверим,
[08:39.320 --> 08:46.800]  что у него происходит по математическому ожиданию. Давайте jkato с тильдой, xkato.
[08:46.800 --> 08:52.840]  Условное математическое ожидание. Ну давайте мы отожидаем. Понятно, что здесь случайно
[08:52.840 --> 08:58.400]  величиной будет только ikt с точки зрения этого условного отожидания, поэтому я по ней и буду
[08:58.400 --> 09:10.640]  отожидать. f ikt xkato – y kato ikt. Ну и сразу за пределы условного отожидания вынесу вот эту сумму y.
[09:10.640 --> 09:17.280]  Вот. Ну и чему будет равна математическое ожидание, кто понимает вот это вот условное,
[09:17.280 --> 09:20.680]  когда мы пробежимся по всем возможным значениям ик?
[09:20.680 --> 09:33.440]  y мы обновляем следующим образом. Вот мы выбираем, ну тут на самом деле уже y вот,
[09:33.440 --> 09:39.560]  ну а я вот здесь и вот так вот выпишу. То есть вот по факту здесь я вот так вот просто добавляю
[09:39.560 --> 09:46.920]  вот этот алгоритм n-ку везде. Вот. Чтобы по факту не пересчитывать заново как-то g по-другому,
[09:46.920 --> 09:53.520]  а просто средний y – это у меня то, что есть. То есть я вот когда-то посчитал y, обновил его,
[09:53.520 --> 09:59.120]  а дальше просто делаю вот так вот. Следующий y у меня опять же обновится, я просто вот средний
[09:59.120 --> 10:04.080]  буду использовать. Без вот этого дополнительного, без этой дополнительной манипуляции, которая
[10:04.080 --> 10:11.600]  написана здесь. Вот. То есть jkato – это будет просто средний y. Понятно, y-ки обновляются на этой же
[10:11.600 --> 10:16.040]  итерации и на следующей итерации получается у меня будет что-то вот такое уже новое. Единственное,
[10:16.040 --> 10:22.920]  что я почему добавил сюда вот это вот в таком виде. Ну я типа y обновил заранее. То есть я вот так
[10:22.920 --> 10:30.040]  вот сделал, чтобы у меня в текущем y хотя бы была итерация о текущей точке k. Потому что в таком
[10:30.040 --> 10:34.520]  виде это будет запаздывающая какая-то вещь. Потому что в jkato, вот если я так без этой
[10:34.520 --> 10:41.280]  стрелочки сделаю, то в y-ках будет храниться градиент максимум в точке xk-1. Вот. Ну в том виде,
[10:41.280 --> 10:45.840]  в котором изначально было понятно все хорошо, потому что там был градиент в точке xk. Ну вот
[10:45.840 --> 10:50.600]  если я так сделаю, я поэтому немного это теряю и поэтому лучше y-ки обновить сначала, а потом
[10:50.600 --> 10:58.280]  посчитать средний. Вот так вот он обновляется. Ну в той версии, которую я сейчас хочу предложить.
[10:58.280 --> 11:02.760]  Вот. Чему будет равно математическое ожидание? Тут на самом деле ничего сложного нет. Нужно просто
[11:02.760 --> 11:13.120]  посмотреть, как ведет себя и. Какие она значения принимает? От 1 до n с вероятностью 1 на n, так?
[11:13.120 --> 11:19.160]  Поэтому здесь вот это 1 на n она вытащится, потому что она здесь была. Вот. А здесь она и останется.
[11:19.160 --> 11:24.560]  Еще вот эта вероятность. А это просто коэффициент, который был. Первое 1 на n это коэффициент,
[11:24.560 --> 11:36.120]  который внутри был. И тогда здесь будет fj-yjkato. Вот. Сумма по j. Вот. И что получается-то? Что-то хорошее
[11:36.120 --> 11:41.880]  или нет с точки зрения, если мы сейчас все это схлопнем? Да на самом деле нет. Потому что, видите,
[11:41.880 --> 11:48.480]  в чем проблема? Тут выскочит тоже средний y только с дополнительным множителем 1 делить на n. 1 делить
[11:48.480 --> 11:55.360]  на n. Он неприятный. И вот поэтому вот это по условному от ожидания не равно f от xk. Вот. Опять же,
[11:55.360 --> 12:02.400]  я не говорю, что это плохо. Вот. Просто нам до этого вроде как нравилось, что по условному от ожидания
[12:02.400 --> 12:08.760]  градиент бы хотелось, чтобы он был не смещенным. Но в том же методе сара не так. Вот. Там нет такого
[12:08.760 --> 12:14.480]  свойства. И за счет этого, кстати, он работает даже лучше. Но здесь, соответственно, понятно,
[12:14.480 --> 12:21.360]  что авторы метода, да, и мы с вами сейчас делаем так, что просто подгоним. Это все безобразие. И вот эту
[12:21.360 --> 12:28.120]  1 на n отсюда и уберем. 1 на n отсюда и уберем. Поэтому она испарится здесь, испарится здесь. И тогда у вас
[12:28.120 --> 12:35.440]  как раз по условному от ожидания будет все хорошо. У вас y схлопнутся между собой и взаимуничтожится,
[12:35.440 --> 12:40.360]  останется просто средний градиент и все будет хорошо. Вот. Поэтому здесь по условному от ожидания
[12:40.360 --> 12:45.440]  действительно получается честный градиент. Вот. Но для этого, соответственно, нужна вот такая вот
[12:45.440 --> 12:50.880]  манипуляция. То есть чуть-чуть метод нужно все же было модифицировать. И здесь вот не просто считать
[12:50.880 --> 12:57.320]  средний y, а вот так вот. То есть с еще домножением тогда вот будет несмещенность. Тогда будет
[12:57.320 --> 13:03.000]  несмещенность, при этом сами y обновляются просто градиентами. Вот. Понятная идея, как получается
[13:03.000 --> 13:08.320]  несмещенность здесь. Как раз обсудили это, не просто голословно посмотрели, а обсудили, как можно это
[13:08.320 --> 13:13.320]  чуть поварировать. Чуть поварировать, и никто на самом деле не говорит, что вот этот вариант
[13:13.320 --> 13:19.560]  лучший. Потому что вы в некотором смысле таким образом все равно чуть-чуть увеличиваете дисперсию.
[13:19.560 --> 13:26.960]  То есть средний y, возможно, был бы лучше. Потому что так у вас есть какой-то fy sample, вы его берете
[13:26.960 --> 13:33.640]  с множителем больше, чем вроде как 1 на n, и значит увеличивать и вклад его дисперсии. Вот. Никто
[13:33.640 --> 13:37.560]  вам не гарантирует, что этот вклад будет хороший. Но, опять же, с точки зрения теоретического анализа,
[13:37.560 --> 13:42.560]  который мы сделаем сейчас, хотелось бы иметь что-то среднее. Ну, чтобы по мотожданию, условному,
[13:42.560 --> 13:49.280]  все было хорошо. Поэтому мы останавливаемся на таком варианте. Окей. Так, теперь надо разобраться
[13:49.280 --> 14:01.800]  теперь уже со второй нормой, где я сразу, как я напоминаю, добавил градиент со звездой. Вот. Вот тут
[14:01.800 --> 14:09.280]  как раз начнется более интересная вещь. Ну, поэтому давайте разбираться. Вот. Опять же, выписываю просто
[14:09.280 --> 14:30.320]  значение ж, как я ее обновляю. У меня здесь градиент f и k, x и k. Так, y, k и k плюс средний y.
[14:30.320 --> 14:47.840]  Так. Ну, давайте здесь я сразу же занесу в это все безобразие умный ноль. Умный ноль. Следующим
[14:47.840 --> 14:58.240]  образом. Следующим образом. Разнесу чуть-чуть. Здесь вычту f и k, t, x со звездой и добавлю сразу
[14:58.240 --> 15:03.480]  же его. Понятно, что мне, как мы с вами уже много раз обсуждали, градиент по какому-то бачу в оптимуме
[15:03.480 --> 15:09.440]  меня обязательно равен нулю. Но опять же, я же его вычитаю и добавляю, поэтому ничего страшного не
[15:09.440 --> 15:20.560]  произойдет. Вот. Окей. Дальше. Кошибуниковский шварц. Кабаша. И получается что-то вот такое. У меня
[15:20.560 --> 15:31.640]  в одной скобочке будет f и k, t, x и k, t минус f и k, t, x со звездой. Что в принципе выглядит уже
[15:31.640 --> 15:37.280]  адекватно, потому что как раз разница градиентов по гладкости и выпуклости мы это можем расписать.
[15:37.280 --> 15:43.360]  Вот. Через разные функции. Но меня больше не будет сейчас интересовать вторая скобка. Вот. И вы мне
[15:43.360 --> 15:53.920]  подскажите, а что вы в ней такого интересного увидите. Я еще выпишу и k, t, x со звездой минус y и k, t, k
[15:53.920 --> 16:03.840]  минус вот этот давайте вот так вот 1 на n плюс y, k, g и вот здесь давайте вот градиент в x со звездой я
[16:03.840 --> 16:14.480]  вот тоже распишу через сумму. Вот. Это в некотором смысле такая небольшая подсказка. Интересно,
[16:14.480 --> 16:19.960]  что вот происходит с этой скобкой. Давайте вот быстренько на нее глянем. Кто что видит в ней? Что-то
[16:19.960 --> 16:25.960]  интересное в ней есть? Как это связано? Может в ней вот эти два слагаемых. То есть вот этот,
[16:25.960 --> 16:31.000]  вот это слагаемое и вот это. Есть ли какая-то между ними взаимосвязь?
[16:31.000 --> 16:39.400]  У них мотождания совпадают, но второе это вообще не случайная величина. Вот это вообще не случайная
[16:39.400 --> 16:46.600]  величина с точки зрения условного математического ожидания, которое мы ввели. Вот. Да, это дисперсия
[16:46.600 --> 16:52.040]  на самом деле записана. То есть вот это случайная величина относительно того математического
[16:52.040 --> 16:57.640]  ожидания, которое мы с вами записываем. То есть тут вот есть это и kt, которое по факту, ну давайте
[16:57.640 --> 17:03.080]  я вот так вот так и запишу. Xi случайная величина, ну случайный вектор в данном случае. А это получается
[17:03.080 --> 17:10.560]  как раз математическое ожидание этого случайного вектора здесь записано. И получается, что здесь
[17:10.560 --> 17:17.360]  у меня записано выражение. Xi минус математическое ожидание Xi. Ну условное, понятно, математическое
[17:17.360 --> 17:25.840]  ожидание в квадрате. Вот так вот. Ну и получается, что это действительно дисперсия. Дисперсия.
[17:25.840 --> 17:32.360]  Поэтому это все безобразие. Можно оценить как второй момент, потому что второй момент у нас,
[17:32.360 --> 17:44.200]  понятно, больше либо равен, чем дисперсия. Вот. Отлично. То есть здесь мы более-менее разобрались,
[17:44.200 --> 17:55.680]  что произошло. Так вот, давайте тогда перепишем это все. А, ну по второму тоже двойка. КБШ-то вот
[17:55.680 --> 18:03.560]  оно. Там у меня сумма вот здесь. Я вот это. Сейчас давайте. Вот это. Это у меня типа A. Вот это у меня B.
[18:03.560 --> 18:09.640]  Ну я A плюс B в квадрате расписываю как 2A в квадрате плюс 2B в квадрате. Вторая вот эту двоечку я
[18:09.640 --> 18:18.720]  забыл. Вот. Все, а вот так вот получается. Понятно, да? Ну все. Супер. Так, тогда у меня здесь остается
[18:18.720 --> 18:30.000]  разница градиентов. Xкт и kt со звездой. Математическое ожидание. И здесь опять же это двоечка,
[18:30.000 --> 18:41.560]  только уже стало полегче, потому что грохнули этот еще дополнительный хвост. Вот. И тут, в принципе,
[18:41.560 --> 18:47.960]  все хорошо. Осталось только, как раз, опять же, условно проматожидать. Потому что, опять же, у нас
[18:47.960 --> 18:53.040]  единственная случайная величина, которая осталась здесь, это и kt. Вот. И на самом деле только она
[18:53.040 --> 18:58.360]  является случайной. Поэтому, когда я вот это все условно теперь проматожидаю, у меня просто вылезет
[18:58.360 --> 19:04.560]  сумма. Вот. 1 на n в данном случае это, опять же, вероятность того, что у меня выражение, которое
[19:04.560 --> 19:12.800]  написано под мотожиданием, это, ну, например, градиент f итого xкатого минус градиент f итого
[19:12.800 --> 19:26.520]  х со звездой в квадрате в квадрате. Вот. И внизу то же самое. 1 на n сумма yk итый минус f итый х со звездой.
[19:26.520 --> 19:35.640]  Согласны? Вот. То есть, вот такое вот выражение, вот такая вот получилась оценка. В принципе,
[19:35.640 --> 19:41.600]  выглядит неплохо. Почему? Потому что вот это мы с этим мы с вами умеем работать. Это будет что-то
[19:41.600 --> 19:47.440]  в духе. Сейчас мы по гладкости это распишем. А это, ну, сейчас чуть-чуть обсудим, что это за безобразие.
[19:47.440 --> 19:53.920]  То есть, давайте бам-пам-пам. Это все сделали. Вот. К этому, а нет, я даже еще не проматожидал. Вот
[19:53.920 --> 20:04.240]  здесь я проматожидал. А, и здесь я сразу же расписал это все безобразие по гладкости. По гладкости. Вот.
[20:04.240 --> 20:09.800]  Ну, там у меня проматожидали еще, а здесь я еще расписываю это все по гладкости. То есть,
[20:09.800 --> 20:15.700]  у меня разница градиентов теперь расписывается вот так вот. Через дивергенцию Брегмана. И кто
[20:15.700 --> 20:22.720]  понимает, почему вот дальше вылезает просто разница самих функций? Есть понимание, почему?
[20:22.720 --> 20:32.960]  Вроде как еще было скалярное произведение. Где скалярное произведение? Ой, я описался здесь,
[20:32.960 --> 20:37.720]  поэтому. Ну, где скалярное произведение? Вот. Если там Х со звездой должно быть,
[20:37.720 --> 20:44.240]  Х со звездой там должно быть. Ну, оно ноль, потому что у вас суммирование по И идет, вы вносите
[20:44.240 --> 20:49.360]  сумму под скалярное произведение, суммируете эти градиенты. Градиент f правильную нулю,
[20:49.360 --> 20:54.640]  поэтому у вас останется просто сама функция. Вот. И получается следующее. Вот. Тут собрали
[20:54.640 --> 21:00.400]  результаты. И вот это по мотожиданию все хорошо. Это просто честный градиент. А вот эта дисперсия
[21:00.400 --> 21:05.840]  вот эта оценивается следующим образом. Вот это как, ну, то, что у нас обычно возникало в градиентном
[21:05.840 --> 21:11.600]  спуске. Только у нас обычно возникало с двоечкой. Но тоже разница по функции была. Вот. Окей,
[21:11.600 --> 21:17.440]  это хорошо. Ну, четверкой. Ладно, не страшно, разберемся четверкой. Вот. Шаг просто нужно будет
[21:17.440 --> 21:23.880]  взять поменьше, потому что только на это влияет. А второй кусочек, ну, вот он такой как раз непонятный,
[21:23.880 --> 21:29.560]  но на самом деле это ровно то, что мы хотели. Помните, когда мы обсуждали интуицию, получается,
[21:29.560 --> 21:39.960]  что если у меня y, k, t, it будут стремиться к градиенту f, f, y со звездой, то все хорошо. Как
[21:39.960 --> 21:44.240]  раз у меня дисперсия будет стремиться к нулю. Вот. И это мы здесь как бы в некотором смысле
[21:44.240 --> 21:48.360]  физически получили. То есть более формально, получили вот это замечательное свойство. То,
[21:48.360 --> 21:54.880]  что у нас стремление y, k, t к it будет действительно нам уменьшать дисперсию стахастического градиента.
[21:54.880 --> 21:59.720]  У нас же не дисперсию, а второй момент вот этот. Второй момент. Ну, на самом деле вот просто вот это
[21:59.720 --> 22:04.360]  отвечает за честный градиент, поэтому вот это скорее отвечает действительно за дисперсию. То есть
[22:04.360 --> 22:11.080]  это бы вылезло, если бы там g, k, t расписали вот так вот. Вот. Это бы вот это. Вот это скорее
[22:11.920 --> 22:18.040]  бы ушло в дисперсию. Вот это бы ушло в дисперсию. Вот. А тот первый кусочек,
[22:18.040 --> 22:24.840]  который бы мы там расписали вот так вот, он бы ушел вот сюда. Поэтому вот это как раз у нас дисперсия.
[22:24.840 --> 22:30.240]  И здесь, как вы понимаете, она действительно может стремиться к нулю, если y ведут себя правильно.
[22:30.240 --> 22:35.960]  Если y ведут себя правильно, ну давайте понимать. То есть пока опять же мы до конца не дошли,
[22:35.960 --> 22:43.800]  но какую-то физику уже получили. Вот. Собрали вместе, получили вот такое безобразие. Здесь,
[22:43.800 --> 22:47.680]  как опять же правильный подбор шага, как вы знаете, нам может дать классный результат,
[22:47.680 --> 22:55.160]  но все еще непонятно, что делать вот с этой фигней, потому что физика хорошая, но уничтожить
[22:55.160 --> 23:01.640]  формально не можем. Чтобы с ней бороться, нужно будет доказать еще один дополнительный факт. Еще
[23:01.640 --> 23:09.920]  один дополнительный факт. Давайте попробуем разобраться, как ведет себя вот тогда вот эта
[23:09.920 --> 23:16.280]  последовательность от итерации к итерации. Приближается ли она к градиентам х звездой или нет.
[23:16.280 --> 23:23.080]  Вот. Ну надо разбираться, надо разбираться. Поэтому буду вести, буду разбираться, как вот себя ведет
[23:23.080 --> 23:30.480]  вот это безобразие. То есть накаплюсь первой итерации, смотрю на вот то, что у меня вылезло там в каты
[23:30.480 --> 23:36.800]  итерации. Давайте попробуем оценить. Опять же, возьму условно-математическое ожидание по каты
[23:36.800 --> 23:45.760]  итерации. По каты итерации. Занесу это условно-математическое ожидание под знак суммы, потому что есть
[23:45.760 --> 23:55.040]  линейность. Вот она всегда есть, она никуда не пропадет. Вот. Теперь у меня к вам вопрос. Теперь
[23:55.040 --> 23:59.920]  у меня к вам вопрос, потому что сейчас, до этого мы вообще никак не пользовались тем, как меняется
[23:59.920 --> 24:05.440]  от итерации к итерации у. Вот. А теперь этим придется воспользоваться, потому что до этого у нас
[24:05.440 --> 24:10.120]  был просто y-кат, и там у него просто индекс был случайный. Там от ожидания по индексу, и оно, на
[24:10.120 --> 24:15.960]  самом деле, хоть y, хоть z, напиши. Неважно, это все равно получится что-то там среднее. А теперь важно,
[24:15.960 --> 24:23.880]  как мы им меняем y. И это сыграет роль. y-кат плюс 1. Какие значения он может принимать? И с какой
[24:23.880 --> 24:32.720]  вероятностью? Сколько у него вообще вариантов? Все правильно. То есть у y-кат плюс 1 и всего два варианта.
[24:32.720 --> 24:40.920]  Либо градиент, если нам повезло. И если не повезло, тогда просто с предыдущей итерацией мы его забираем.
[24:40.920 --> 24:49.760]  Поэтому здесь, когда мы условно-математическое ожидание, у меня что выскочит? С вероятностью 1-1
[24:49.760 --> 24:59.040]  делить на n. Это просто y-кат, который заберется с прошлой итерацией. Давайте вот так поставлю скобочки.
[24:59.040 --> 25:06.720]  А с вероятностью 1 на n, у меня ровно то, что нам уже подсказали, выскочит очень хорошая вещь. Это вот
[25:06.720 --> 25:19.680]  как раз разница f и t, x и kt, f и t, x и звездой. Вот. Отлично. Отлично. Тогда смотрите, что получается.
[25:19.680 --> 25:36.000]  Выписываю следующим образом. 1-1 делить на n. Здесь у меня будет тумма y и x и звездой.
[25:36.720 --> 25:50.960]  А здесь у меня как раз останется 1 делить на n, умножить на 1 делить на n, f и t, x и kt, минус f и t, градиент f и t, x и звездой.
[25:50.960 --> 25:57.600]  Как расписывать вот это? Мы только что с вами разобрались вот эту сумму. Поэтому я здесь сразу напишу, давайте
[25:57.600 --> 26:09.040]  2l, f от xk, минус f от x и звездой. По гладкости и выпуклости, дальше скалярное произведение убьется.
[26:09.040 --> 26:16.080]  Окей. Смотрите, что теперь классно получилось. Вот это у меня в некотором смысле какая-то последовательность sigma k плюс 1.
[26:16.080 --> 26:22.240]  За которой мы как бы понимаем, какая у нее физика. Если она будет сходиться к нулю, то все с ней хорошо.
[26:22.240 --> 26:30.800]  Значит там дисперсия будет сходиться к нулю, а y стремятся к f от x звездой. Вот. Так это же она у меня тоже, только на кат и итерации.
[26:30.800 --> 26:48.800]  На кат и итерации получается, что я получил что-то в духе sigma k плюс 1, по условному математическому ожиданию, меньше либо равна, чем sigma k, плюс какая-то добавка.
[26:48.800 --> 27:01.040]  2r делить на n, f, xk, минус f от x звездой. Согласны? Вот. Смотрите, получается такое что-ли занимательное свойство.
[27:01.040 --> 27:09.440]  Здесь это доказано. Вот. Смотрите, что в итоге-то. У меня есть сходимость по x. Здесь я сразу накидываю полным от ожидания.
[27:09.440 --> 27:17.440]  Сходимость по x, вот она. Вот. С этим мы вроде как умеем бороться. Правильный гамма меньше чего-то уничтожит это.
[27:18.080 --> 27:27.280]  Здесь, опять же, с дисперсией не знали, что делать. Не знали. Вот. Но про эту дисперсию я знаю то, что она мне сходится, то есть вот тут тоже линейная сходимость.
[27:27.280 --> 27:40.240]  Линейная сходимость. И здесь линейная сходимость. Вот. Ну и здесь опять же возникает неприятный член, который вот здесь, мы вроде как уже умели уничтожать.
[27:41.040 --> 27:47.040]  Надо получается как-то вот эти две рекурренты, которые по факту у меня вроде как есть сходимость.
[27:47.040 --> 27:53.040]  По x линейная, просто чуть-чуть что-то ей мешает до полного счастья.
[27:53.040 --> 28:03.040]  Плюс у меня есть опять же сходимость линейная по вот этим y. По вот этим y тоже что-то мешает до полного счастья ее сделать прям по-настоящему линейной.
[28:03.840 --> 28:15.840]  Вот. Но видно, что они друг другу могут в некотором смысле помочь. То есть вот это хотелось бы уничтожить, а это умеет уничтожать вот здесь, вот правильным подбором шага, потому что там есть вот это выражение с минусом.
[28:15.840 --> 28:25.840]  Ну а это хотелось бы тоже как-то поуничтожать здесь, но вот это возникает здесь. И вот здесь как бы есть знак минус, который нам может помочь это уничтожить.
[28:25.840 --> 28:32.640]  Поэтому вот эти две рекурренты предлагается просто сложить между собой, сложить между собой с правильным коэффициентом.
[28:32.640 --> 28:42.640]  Вот. Коэффициент m гамма в квадрате. m гамма в квадрате, и я складываю две рекурренты, которые у меня получились.
[28:42.640 --> 28:51.640]  Вот. Вот это линейный кусочек у меня сходимости был. Вот это у меня вылезло из второй рекурренты на y.
[28:51.640 --> 28:59.440]  Вот. А это у меня вылезло из первой рекурренты, где у меня как раз болтался неприкаядный кусочек гамма в квадрате, два гамма в квадрате.
[28:59.440 --> 29:05.440]  Вот. m-ка тут как раз у него сокращается. Вот. Он у меня вылез из той рекурренты.
[29:05.440 --> 29:14.440]  Плюс как раз то, что я хотел. Вот сюда у меня залетело в скобочку вот это значение fx ката минус f от x звездой,
[29:14.440 --> 29:24.240]  который ровно как мы знаем у нас больше либо равно нуля. В скобочку залетел дополнительный кусочек, который мы как раз сейчас с вами и за нули.
[29:24.240 --> 29:32.240]  Который мы как раз с вами сейчас и за нули. Ну точнее вот эту скобочку просто сделаем меньше либо равной, больше либо равной нуля.
[29:32.240 --> 29:41.240]  Вот. Окей. Какую m-ку взять, ваше предположение? Какую m-ку взять? Вообще имеет смысл какую брать m-ку?
[29:41.240 --> 29:46.240]  Какие мысли? Что там?
[29:53.240 --> 30:00.240]  Ну там мы же это с помощью шага еще сможем сделать. То есть скорее там вот шаг подбирается исходя из m-ки.
[30:00.240 --> 30:07.240]  Вот. Вот интересно скорее с точки зрения m, потому что вот только вот это слагаемое зависит только от m.
[30:07.240 --> 30:14.240]  Потому что m мы можем варьировать, но вот это слагаемое зависит только от m. Что мы от него хотим?
[30:14.240 --> 30:17.240]  Что мы от него хотим?
[30:22.240 --> 30:34.240]  Больше нуля это хорошо, но оно в любом случае будет больше скорее всего нуля. То есть тут единица, n-ка это понятно больше единицы, поэтому она будет больше нуля.
[30:34.240 --> 30:43.240]  Вот. Что от него-то хочется? Мы же вроде обсуждали, то есть у нас есть линейная сходимость по x, есть линейная сходимость по y.
[30:46.240 --> 30:57.240]  Хороший вариант, хороший вариант, но подобрать будет довольно сложно. Это правда, это лучший вариант, это сделать так, чтобы вот эти как бы линейные сходимости вот это и вот это вообще совпали.
[30:57.240 --> 31:03.240]  То есть подобрать m-ку так, чтобы прям вот это можно было сделать, но вообще это даже не всегда возможно.
[31:03.240 --> 31:12.240]  Хотя вроде как, если m-ку брать довольно, ну ладно, может и возможно всегда, можно проще просто сделать. Я хочу, чтобы у меня линейная сходимость сохранилась, так?
[31:12.240 --> 31:22.240]  Вот. Поэтому я вот хочу, чтобы у меня вот здесь вот, вот из этой скобки вылезло что-то, что меньше единицы. Что меньше единицы.
[31:22.240 --> 31:30.240]  Вот. И за счет m-ки я понятно это могу сделать. Беру довольно большую m, и ну она не сильно повлияет на то, что произойдет.
[31:30.240 --> 31:36.240]  Давайте я возьму m равную 4n. 4n. Пока непонятно, почему я такую беру.
[31:36.240 --> 31:42.240]  Вот. Ну все довольно просто будет, во-первых, потому что у меня линейная сходимость сохранится. Здесь вылезет 1 девять на 2n.
[31:42.240 --> 31:54.240]  Вот. Ну почему, например, нельзя взять больше, обсудим чуть попозже. Потому что кажется, что чем больше m я возьму вот здесь вот, тем лучше у меня будет линейная сходимость по вот этому кусочку.
[31:54.240 --> 31:59.240]  Но посмотрим, почему нельзя. Почему я взял такой, и этого достаточно.
[31:59.240 --> 32:07.240]  Тогда у меня вот здесь вот там все сократится, вылезет конкретно 6 гамма л. Мы возьмем гамма меньше, чем 1 делить на 6 л.
[32:07.240 --> 32:14.240]  И это все у нас уйдет. Останется только линейная сходимость. Две линейные сходимости.
[32:14.240 --> 32:22.240]  Но я могу их вот так вот склопнуть через максимум. Понятно, что я могу сверху оценить вот эти два множителя как максимум из этих двух множителей.
[32:22.240 --> 32:27.240]  И тогда смотрите, что у меня получилось. У меня получилась вот такая вот функция в k плюс первой.
[32:27.240 --> 32:31.240]  В k плюс первой точке. И эта же функция в k этой точке.
[32:31.240 --> 32:37.240]  Ну и вот так получился такой вот хитрый критерий сходимости, состоящий из двух кусочков. Из двух кусочков.
[32:37.240 --> 32:41.240]  И по нему можно измерять сходимость, запускать рекурсию и получать результат.
[32:41.240 --> 32:50.240]  Это здесь у меня и отражено. Как, соответственно, получить сходимость для метода saga.
[32:50.240 --> 32:57.240]  Здесь, соответственно, да. У меня берется ката-итерация.
[32:57.240 --> 33:05.240]  Ну, запускается рекурсия. Здесь остается, соответственно, вот эта функция, которая у нас используется в качестве сходимости в нулевой точке.
[33:05.240 --> 33:12.240]  Здесь k в k этой точке. И все хорошо. Все хорошо. То есть вот вообще эта функция довольно хорошо отражает физику.
[33:12.240 --> 33:16.240]  То есть она дает нам понимание, что у нас есть сходимость и по x, и по y.
[33:16.240 --> 33:22.240]  То есть и y стремятся вот к этому значению. К f и тому x звездой. Причем линейно. Вот.
[33:22.240 --> 33:28.240]  И x. А почему так? Потому что на самом деле у меня v-ката состоит из двух неотрицательных частей.
[33:28.240 --> 33:38.240]  И если у меня v-ката стремится к нулю, то у меня и любая его из его частей, которая по факту меньше либо равна этому v-кату, тоже будет стремиться к нулю.
[33:38.240 --> 33:43.240]  Вот. Как и расстояние по x, так и расстояние по вот этим y до градиентов.
[33:43.240 --> 33:47.240]  Вот такой вот результат для метода saga.
[33:47.240 --> 33:54.240]  Единственный вот вопрос, опять же, возвращаясь к тому, почему я не взял довольно большую m, потому что вроде как мог.
[33:54.240 --> 33:59.240]  Вот. А почему не взял?
[33:59.240 --> 34:08.240]  Да. Потому что на самом деле вот тут вот, вот тут вот в оценке сходимости, в оценке сходимости, вот это и есть m.
[34:08.240 --> 34:13.240]  Вот это и есть m. И как вы видите, правая часть от этого всего безобразия зависит.
[34:13.240 --> 34:21.240]  В левой части я, да, могу написать что-то в духе, что у меня здесь всегда будет x-ката минус x звездой в квадрате.
[34:21.240 --> 34:24.240]  Вот. И вроде как зависимости от m-ки нет.
[34:24.240 --> 34:27.240]  Но вот здесь зависимость от m-ки выскочит.
[34:27.240 --> 34:37.240]  Зависимость от m-ки выскочит. И как вы понимаете, тут как раз, если вы y инициализируете нулями, то есть изначально память для текущего градиента там,
[34:37.240 --> 34:50.240]  по sample e, вы зададите нулем, то у вас как раз будет m-ка увеличивать множитель, который зависит от как раз вот этих норм градиента x звездой.
[34:50.240 --> 34:54.240]  Вот. Берете большую m-ку, соответственно, у вас сходимость в этом плане портится.
[34:54.240 --> 35:01.240]  Но m-ка, видите, как видите, такой инструмент. Инструмент именно подбора результатов, чтобы сходимость была красивая.
[35:01.240 --> 35:07.240]  При этом, как вы видите, ну там m-ку порядка n брать и стоит.
[35:07.240 --> 35:13.240]  Потому что вот здесь вот по линейной сходимости она у вас была изначально 1 минус n, 1 делить на n.
[35:13.240 --> 35:19.240]  Ну, понятно, что это какая-то линейная сходимость, там, зависящая от n-ки.
[35:19.240 --> 35:26.240]  И то, что здесь появилась двойка, в принципе, с точки зрения онотации, в которой мы обычно записываем результаты, не особо важно.
[35:26.240 --> 35:31.240]  Поэтому чуть-чуть попортили, но не страшно. То есть m-ку главное не взяли особо большой.
[35:31.240 --> 35:38.240]  Вот. Получается здесь вот такая вот игра. Взять m-ку небольшой, но при этом еще не испортить эту линейную сходимость.
[35:39.240 --> 35:45.240]  Окей. Из этого всего безобразия можно получить, понятно, сходимость метода,
[35:45.240 --> 35:50.240]  которую мы с вами, в принципе, уже видели. Это итерационная сложность.
[35:50.240 --> 36:00.240]  Она, понятно, зависит от обоих множителей. Если вы там подставите шаг 6L, который максимальный, и у вас останется вот так вот.
[36:00.240 --> 36:05.240]  Какой-то из этих множителей, понятно, преобладает. И в зависимости от этого у вас получается результат.
[36:05.240 --> 36:12.240]  То есть по-хорошему здесь нужно бы написать максимум от 2n 6L делить на mu.
[36:12.240 --> 36:15.240]  6L делить на mu, logarithm 1 делить на epsilon.
[36:15.240 --> 36:22.240]  Ну так как мы записываем результат с точки зрения онотации, то у вас здесь получается,
[36:22.240 --> 36:29.240]  что вы максимум можете раскрыть просто как сумму, и от констант, которые численные, можете просто избавиться.
[36:30.240 --> 36:33.240]  Понятно, что вот это вот так вот.
[36:33.240 --> 36:38.240]  На двойку, ладно, припишу, а потом, когда в онотацию запихивать, вообще все пропадет.
[36:41.240 --> 36:44.240]  Как-то так, как-то так. То есть вот такой вот результат.
[36:44.240 --> 36:54.240]  Видели уже в прошлый раз, обсуждали то, что этот результат на самом деле лучше, чем сходимость градиентного спуска,
[36:54.240 --> 37:00.240]  потому что для градиентного спуска итерационная сложность на самом деле вот L делить на mu.
[37:00.240 --> 37:06.240]  Итерационная сложность, но, как вы понимаете, в градиентном спуске нам нужно считать полный градиент по всем бочам.
[37:06.240 --> 37:14.240]  По всем бочам, то есть он будет в N раз тяжеловеснее, чем подсчет градиента по одному сэмплу.
[37:14.240 --> 37:19.240]  По одному сэмплу, ну либо там если делаем бочирование, то будет чуть больше, но не страшно.
[37:19.240 --> 37:25.240]  Вот, то есть вот эта N, N часто не играет роль, потому что градиентного спуска с точки зрения вычислений,
[37:25.240 --> 37:31.240]  именно градиентов здесь будет N, а там получается у нас N плюс L делить на mu,
[37:31.240 --> 37:34.240]  потому что для градиентного спуска можно даже вот так оценку записать.
[37:34.240 --> 37:38.240]  И понятно, что какая из них будет лучше.
[37:41.240 --> 37:47.240]  Окей, окей. Все, получили опять же результат, напомнили то, что у нас это все действительно хорошо работает.
[37:47.240 --> 37:54.240]  Для метода SVRG, для метода SAR доказательства очень похожие, то есть главное суть этой техники в том,
[37:54.240 --> 37:58.240]  что сконструировать критерий сходимости не из одного кусочка, а сразу из нескольких.
[37:58.240 --> 38:04.240]  Такое мы в принципе видели, когда, а, ну, метод Нестеров я по-другому доказывал.
[38:04.240 --> 38:09.240]  Ну, то есть в методе Нестеру на самом деле также критерий сходимости, он состоит из двух кусочков.
[38:09.240 --> 38:15.240]  Исходимости по функции и исходимости по аргументу, вот, с правильными коэффициентами подобранной.
[38:15.240 --> 38:20.240]  Вот, и здесь то же самое, то есть тут вводится две последовательности, которые обе как бы сходятся,
[38:20.240 --> 38:24.240]  но с небольшими помехами, но они друг друга в этом плане компенсируют,
[38:24.240 --> 38:29.240]  и в итоге получается хорошая линейная сходимость в отличие от обычного HDD.
[38:29.240 --> 38:36.240]  Вот, окей, тогда в первой части все, перерыв, ну что, продолжаем, продолжаем.
[38:36.240 --> 38:44.240]  Вот, теперь чуть-чуть опять же сдвинемся в другую сторону.
[38:44.240 --> 38:47.240]  Ну, продолжаем как бы рассуждать про статистические методы.
[38:47.240 --> 38:54.240]  И вот, когда мы говорили про методы редукции дисперсии, ну, там, не только про них конкретно,
[38:54.240 --> 39:01.240]  мы сейчас говорили о том, что мы просто вместо подсчета полного градиента по всей обучающей выборке,
[39:01.240 --> 39:07.240]  например, считаем градиент по отдельным сэмплам, по отдельным сэмплам, вот.
[39:07.240 --> 39:12.240]  Вот, а если с точки зрения какой-то матрицы, говорить с точки зрения матрицы A,
[39:12.240 --> 39:18.240]  которая по факту у нас из этих сэмплов и состоит, то есть они вот тут в строке записаны эти сэмплы,
[39:18.240 --> 39:22.240]  A1, там A2 и так далее, вот, что это означает?
[39:22.240 --> 39:26.240]  Это означает, что мы по факту выбираем какие-то строки из этой матрицы
[39:26.240 --> 39:30.240]  и используем только их при подсчете градиента, вот.
[39:30.240 --> 39:35.240]  И как раз стахастика состоит в том, что мы выбираем нужные строки, вот,
[39:35.240 --> 39:37.240]  именно в матрице данных.
[39:37.240 --> 39:44.240]  Ну а какой более-менее противоположный, как бы, стахастический смысл можно вложить в метод?
[39:44.240 --> 39:49.240]  Вот, какие еще могут быть идеи, если не строки, то что?
[39:49.240 --> 39:53.240]  Столбцы, понятно. Вот, а за что тогда отвечают столбцы?
[39:53.240 --> 39:59.240]  Если строка это просто у нас сэмпл, вот, то столбец с матрицей данными, то что?
[39:59.240 --> 40:01.240]  Координаты это признак, да, то есть
[40:01.240 --> 40:05.240]  конкретная координата оптимизационной перемены – это конкретный признак.
[40:05.240 --> 40:07.240]  То есть получается, что…
[40:07.240 --> 40:12.240]  с одной стороны, когда мы говорим про задачу
[40:12.240 --> 40:15.240]  стахастической оптимизации до этого, мы просто выбирали сами строки,
[40:15.240 --> 40:18.240]  выбирали конкретные сэмплы, вот,
[40:18.240 --> 40:21.240]  противоположный вариант, давайте брать не сэмплы,
[40:21.240 --> 40:25.240]  давайте брать конкретные признаки, брать конкретные координаты.
[40:25.240 --> 40:29.500]  Ну и на самом деле с точки зрения вообще квадратичной задачи все эти методы
[40:29.500 --> 40:35.740]  редукции дисперсии часто ну не особо эффективны в плане вычислений, потому что
[40:35.740 --> 40:41.900]  опять же у вас есть матрица A, вы записываете вот эту задачу в виде вот такой вот
[40:41.900 --> 40:46.940]  вот такой вот и что вы делаете? Вы можете на самом деле переписать ее даже вот так
[40:46.940 --> 40:52.780]  x транспонирована, A транспонирована, A x минус A транспонирована, B
[40:52.780 --> 40:57.460]  целевую функцию и на самом деле вам достаточно один раз только возвести мат...
[40:57.460 --> 41:01.580]  умножить матрицу на матрицу, вот, а дальше умножать вектор x для подсчета
[41:01.580 --> 41:07.020]  градиента уже на вот эту матрицу размера D на D, вот. Понятно, что в более сложных
[41:07.020 --> 41:11.960]  задачах кипологистической регрессии так вам просто не проканает, вот. Там уже нужно
[41:11.960 --> 41:16.480]  считать каждый раз что-то отдельно, вот. Поэтому как бы здесь сразу же возникает
[41:16.480 --> 41:20.700]  такая сразу же естественная идея, как мы говорили про признаки, то есть выбор
[41:20.700 --> 41:25.020]  каких-то признаков. А если у меня есть задача, вида, ну давайте вот так вот
[41:25.020 --> 41:30.100]  запишу, градиент у меня какой-то моей там целевой функции g равен какой-то
[41:30.100 --> 41:39.780]  матрице B на x, B на x там, ну, минус C, например, B на x минус C, вот. Как мы поняли,
[41:39.780 --> 41:45.980]  мы должны выбрать какой-то отдельный признак, но в данном случае даже проще,
[41:45.980 --> 41:53.620]  даже проще, давайте будем брать просто координату, координату этого вектора,
[41:53.620 --> 41:59.540]  координату этого вектора, то есть считать не полный градиент, не там перемножать
[41:59.540 --> 42:05.060]  полную матрицу на вектор x, а перемножать получается опять же строку, ну, потому что
[42:05.060 --> 42:09.660]  здесь мы уже рассматриваем матрицу B, которая в некотором смысле с матрицей A
[42:09.660 --> 42:17.940]  связана не тождественно, вот, вы вместо полной матрицы B, вместо полной матрицы B хотите
[42:17.940 --> 42:22.620]  посчитать градиент вот этой вашей функции по какой-то случайной координате, по какой-то
[42:22.620 --> 42:28.700]  случайной координате, и тогда вам нужно просто вот эту строку и катую строку перемножить на
[42:28.700 --> 42:36.500]  вектор x. Это значительно дешевле, потому что перемножая всю матрицу на вектор x, вам нужно
[42:36.500 --> 42:48.660]  сделать D в квадрате операций, то в случае, когда вы считаете только одну координату этого
[42:48.660 --> 42:53.900]  градиента, вам нужно сделать D операции, то есть такая процедура в D может быть и две раз дешевле,
[42:53.900 --> 43:00.420]  когда вы вместо полного градиента считаете градиент по координате. Как это в том числе связано
[43:00.420 --> 43:06.260]  с задачей машинного обучения? Для квадратичных задач, для логистической регрессии там
[43:06.260 --> 43:13.420]  может быть довольно просто, то есть считать градиент по конкретному признаку, то есть производную
[43:13.420 --> 43:18.180]  просто по этому признаку. Там может быть явная формула, она может быть действительно дешевле по
[43:18.180 --> 43:27.020]  сравнению с тем, что вы вычисляете полный градиент. Часто в реальных задачах вы не просто не можете
[43:27.020 --> 43:33.460]  посчитать градиент, не производный по координатам, вы просто имеете доступ только к информации
[43:33.460 --> 43:37.940]  нулевого порядка. Используя информацию нулевого порядка, вы на самом деле можете просто
[43:37.940 --> 43:42.260]  апроксимировать производную по направлению. Производную по направлению, только здесь,
[43:42.260 --> 43:47.740]  я вот так напишу, производную по направлению. Какое-то направление E у вас случайно задано,
[43:47.740 --> 43:53.220]  но это вы из мотонализа еще знаете, что через конечную разность его можно просто апроксимировать.
[43:53.220 --> 43:58.540]  Понятно, если вы стремите tau к нулю, то у вас может быть что-то действительно хорошее в этой
[43:58.540 --> 44:04.180]  апроксимации и получится. То есть вот такого рода координатные стахастические методы,
[44:04.180 --> 44:09.540]  они возникают не только из-за того, что вам хочется что-то дешевле считать, но из-за того,
[44:09.540 --> 44:18.780]  что вам хочется просто выжить как-то из информации нулевого порядка, информацию типа градиента,
[44:18.780 --> 44:24.980]  ну через конечной разности, например, это рабочая схема. Подробно про вот именно
[44:24.980 --> 44:30.540]  оптимизацию нулевого порядка, когда у вас градиент производно по направлению апроксимируется
[44:30.540 --> 44:35.980]  через конечные разности, вам расскажет Маргарита на семинаре. Сегодня мы этого касаться не будем,
[44:35.980 --> 44:42.980]  ну тут скорее мы просто сегодня поговорим про координатный метод, когда вы вместо подсчета
[44:42.980 --> 44:49.100]  полного градиента считаете одну из его координат. Одну из его координат, по каким причинам так
[44:49.100 --> 44:56.540]  происходит? Ради удешевления информации, ради удешевления, ой только тут я видимо из саги это все
[44:56.540 --> 45:04.620]  переписал, тут память не нужна, вот ради удешевления вычислений или просто потому что не до жира быть бы
[45:04.620 --> 45:11.980]  живым, вот нам сейчас не важно. Окей, считаем градиент по координате, посчитали вот производную,
[45:11.980 --> 45:18.980]  чтобы у меня получился градиент я здесь домножаю еще на вектор, ну точнее вектор размерности D,
[45:18.980 --> 45:23.940]  я здесь домножаю еще на базисный вектор соответствующий, на самом деле можно было не
[45:23.940 --> 45:28.460]  домножать, потому что по факту я же обдейчу только одну координату и мог бы здесь как раз
[45:28.460 --> 45:34.940]  апдэйтить её, и поэтому в том числе координатные методы и дешевле, в том числе даже в виде финального
[45:34.940 --> 45:41.120]  апдэйта мне нужно поменять только одну координату, а не полный вектор. Но я здесь записал так,
[45:41.120 --> 45:45.320]  потому что это вещь такая более формальная, мне скорее для доказательства так удобнее.
[45:45.320 --> 45:50.920]  Вот, а я зачем-то здесь еще приписал вот эту D, вот этот множитель D,
[45:50.920 --> 45:55.240]  который у меня соответствует размерности задачи. Зачем?
[45:55.240 --> 46:02.080]  Кто может уже сразу понимает, зачем здесь эта D? Я вроде выбрал случайную координату и все.
[46:02.080 --> 46:05.000]  Каким бы чем?
[46:05.000 --> 46:34.920]  Ну, на самом деле мы, опять же, боремся за не смещенными.
[46:35.280 --> 46:42.440]  Ну, давайте сейчас мы на это посмотрим. Вот, понятно, что вы физику уже более-менее понимаете,
[46:42.440 --> 46:46.360]  зачем это все нужно, что как бы в скалярном произведении дата возникает одна координата
[46:46.360 --> 46:50.360]  вместо всех. Вот, и это нужно как бы ее чуть-чуть подправить.
[46:50.360 --> 46:57.960]  Окей, делаем все то же самое, как делали раньше. Делаем шаг градиентного спуска,
[46:57.960 --> 47:01.760]  смотрим, как меняется расстояние. Ну, тут уже подставляем то, что мы используем в качестве
[47:01.880 --> 47:06.940]  нашего стахастического градиента. И понятно, нам нужно будет по условному
[47:06.940 --> 47:12.480]  математическому ожиданию это все безобразие оценить. Ну, давайте попробуем оценить это
[47:12.480 --> 47:15.460]  безобразие по условному математическому ожиданию.
[47:19.000 --> 47:23.180]  Так, к чему равно? К чему равно? Кто понимает, как посчитать такое условно
[47:23.180 --> 47:28.160]  математическое ожидание, если координаты у меня выбираются равномерно.
[47:28.160 --> 47:30.840]  Чему равно это условно-математическое ожидание?
[47:32.840 --> 47:35.000]  Давайте, я D вынесу сразу за него.
[47:35.000 --> 47:36.500]  Какие у меня значения принимает E?
[47:36.500 --> 47:39.500]  От 1 до D, поэтому я так и запишу.
[47:39.500 --> 47:42.500]  G пробегает значение от 1 до D.
[47:42.500 --> 47:45.000]  Вероятность выбора из координатов равномерна,
[47:45.000 --> 47:47.000]  так как у нас все это выбирается,
[47:47.000 --> 47:48.500]  мы просто один делим на D.
[47:48.500 --> 47:51.500]  И здесь, соответственно, пробегают значения F,
[47:51.500 --> 47:57.500]  K, T, G, E, G.
[47:57.840 --> 48:01.340]  И чему равна эта безобразие?
[48:01.340 --> 48:02.340]  Кто понимает?
[48:04.340 --> 48:05.340]  Кто понимает?
[48:05.340 --> 48:08.340]  Давайте, я D сокращу и выпишу вот это.
[48:11.340 --> 48:13.340]  Все правильно, это просто градиент.
[48:13.340 --> 48:16.340]  Мы каждый из координат домножаем
[48:16.340 --> 48:18.840]  на соответствующий базисный вектор.
[48:18.840 --> 48:20.840]  И в итоге получается просто градиент.
[48:20.840 --> 48:22.340]  Все правильно.
[48:22.340 --> 48:24.840]  И как вы видите, без домножения на D,
[48:24.840 --> 48:27.840]  которое я изначально здесь приписал,
[48:27.840 --> 48:30.840]  которое здесь возникло, понятно, и здесь возникло,
[48:30.840 --> 48:33.840]  у вас здесь был бы градиент, но только деленный на D.
[48:33.840 --> 48:37.840]  Вот и вся история.
[48:37.840 --> 48:39.840]  Почему это домножение нужно?
[48:39.840 --> 48:41.840]  Опять же, для несмещенности,
[48:41.840 --> 48:44.840]  просто потому что мы уже привыкли делать несмещенный анализ.
[48:44.840 --> 48:46.840]  На самом деле это не обязательно.
[48:46.840 --> 48:49.840]  Можно было, опять же, оставить без этой D.
[48:49.840 --> 48:51.840]  В некотором смысле сейчас мы поймем,
[48:51.840 --> 48:53.840]  что мы перегоняем ее из одного места в другое.
[48:53.840 --> 48:55.840]  Здесь мы ее добавили,
[48:55.840 --> 48:57.840]  но придется ее в некотором смысле
[48:57.840 --> 48:59.840]  и компенсировать в другое место.
[48:59.840 --> 49:01.840]  Окей.
[49:01.840 --> 49:03.840]  С первым моментом разобрались.
[49:03.840 --> 49:05.840]  Теперь второй момент.
[49:15.840 --> 49:17.840]  Вот так.
[49:17.840 --> 49:19.840]  Здесь что получается?
[49:19.840 --> 49:21.840]  Здесь что получается?
[49:21.840 --> 49:23.840]  Так.
[49:23.840 --> 49:25.840]  Так, кто понимает?
[49:25.840 --> 49:27.840]  Опять же, в условном от ожидания
[49:27.840 --> 49:31.840]  и kt пробегает значение от 1 до D,
[49:31.840 --> 49:33.840]  все аккуратненько выписываем.
[49:33.840 --> 49:35.840]  D квадрат выскакивает из-под нормы.
[49:35.840 --> 49:37.840]  Вероятность – это 1 делить на D.
[49:37.840 --> 49:39.840]  Опять же.
[49:39.840 --> 49:41.840]  И значение, по которому пробегаем,
[49:41.840 --> 49:43.840]  это у нас просто f,
[49:43.840 --> 49:45.840]  xk,
[49:45.840 --> 49:47.840]  и…
[49:47.840 --> 49:49.840]  ну, здесь давайте g поставлю.
[49:49.840 --> 49:51.840]  g от 1 до D,
[49:51.840 --> 49:53.840]  e g,
[49:53.840 --> 49:55.840]  в квадрате в квадрате.
[49:55.840 --> 49:57.840]  Окей, давайте я D вынесу.
[49:57.840 --> 49:59.840]  У меня одна D сократится.
[49:59.840 --> 50:01.840]  Получается, останется что-то вот
[50:01.840 --> 50:03.840]  в таком виде.
[50:03.840 --> 50:05.840]  Окей, сейчас давайте выпишем.
[50:05.840 --> 50:07.840]  Так.
[50:07.840 --> 50:09.840]  f g e g
[50:09.840 --> 50:11.840]  в квадрате.
[50:11.840 --> 50:13.840]  Чему равна эта норма?
[50:13.840 --> 50:15.840]  Внимает.
[50:15.840 --> 50:17.840]  Точнее, даже сумма этих норм.
[50:17.840 --> 50:19.840]  Просто норм и градиент.
[50:19.840 --> 50:21.840]  Потому что, опять же,
[50:21.840 --> 50:23.840]  когда вы берете норму в квадрате
[50:23.840 --> 50:25.840]  от вектора,
[50:25.840 --> 50:27.840]  то есть за норму вообще можно вынести
[50:27.840 --> 50:29.840]  вот эту координату.
[50:29.840 --> 50:31.840]  Вот эту координату можно вынести за норму,
[50:31.840 --> 50:33.840]  потому что это скаляр,
[50:33.840 --> 50:35.840]  производная по координате.
[50:35.840 --> 50:37.840]  Потому что скаляр можно вынести
[50:37.840 --> 50:39.840]  за пределы нормы в квадрате.
[50:39.840 --> 50:41.840]  Норма базисного вектора
[50:41.840 --> 50:43.840]  равна просто единице.
[50:43.840 --> 50:45.840]  Поэтому здесь у вас будет просто
[50:45.840 --> 50:47.840]  сумма квадратов координата.
[50:47.840 --> 50:49.840]  Это есть просто норма.
[50:49.840 --> 50:51.840]  d
[50:51.840 --> 50:53.840]  норма f от xk
[50:53.840 --> 50:55.840]  в квадрате.
[50:55.840 --> 50:57.840]  Вот. Получилось так, что
[50:57.840 --> 50:59.840]  что у нас здесь?
[50:59.840 --> 51:01.840]  Там-пам-пам.
[51:01.840 --> 51:03.840]  По мотожданию все хорошо.
[51:03.840 --> 51:05.840]  В дисперсе получили
[51:05.840 --> 51:07.840]  увеличение
[51:07.840 --> 51:09.840]  градиента. Увеличение градиента
[51:09.840 --> 51:11.840]  в d раз.
[51:11.840 --> 51:13.840]  Увеличение нормы градиента
[51:13.840 --> 51:15.840]  в d раз.
[51:15.840 --> 51:17.840]  Поэтому, когда мы это все безобразие
[51:17.840 --> 51:19.840]  подставляем, у нас получается то, что
[51:19.840 --> 51:21.840]  мы обычно имели в градиентном спуске
[51:21.840 --> 51:23.840]  расстояние до решения предыдущей точки,
[51:23.840 --> 51:25.840]  скалярное произведение, плюс
[51:25.840 --> 51:27.840]  норма градиента в квадрате
[51:27.840 --> 51:29.840]  на гамма в квадрате. Единственное, что здесь
[51:29.840 --> 51:31.840]  добавилась еще это дополнительная d,
[51:31.840 --> 51:33.840]  которая
[51:33.840 --> 51:35.840]  сейчас увидим, где она дает
[51:35.840 --> 51:37.840]  свой импакт фактор.
[51:37.840 --> 51:39.840]  Расписываем по выпуклости, по гладкости.
[51:39.840 --> 51:41.840]  Все получается ровно, как обычно.
[51:41.840 --> 51:43.840]  Здесь линейная сходимость.
[51:43.840 --> 51:45.840]  Здесь тоже у вас сначала
[51:45.840 --> 51:47.840]  вылезает с минусом разность функций,
[51:47.840 --> 51:49.840]  которая из скалярного произведения
[51:49.840 --> 51:51.840]  выскочила из сильной выпуклости.
[51:51.840 --> 51:53.840]  А дальше вы по гладкости расписываете
[51:53.840 --> 51:55.840]  норму градиента,
[51:55.840 --> 51:57.840]  добавляя туда f и x звездой,
[51:57.840 --> 51:59.840]  и у вас выскакивает, вроде бы,
[51:59.840 --> 52:01.840]  все хорошо, гамма, l.
[52:01.840 --> 52:03.840]  Но здесь выскакивает еще дополнительная d,
[52:03.840 --> 52:05.840]  потому что она у нас возникла
[52:05.840 --> 52:07.840]  из-за того, что
[52:07.840 --> 52:09.840]  брали мы от ожидания.
[52:09.840 --> 52:11.840]  Выскочила эта d.
[52:11.840 --> 52:13.840]  Ну и, как вы понимаете,
[52:13.840 --> 52:15.840]  шаг теперь какой?
[52:15.840 --> 52:17.840]  Он не 1 делить на l, он теперь должен быть
[52:17.840 --> 52:19.840]  меньше, чем d делить на l.
[52:19.840 --> 52:21.840]  В некотором смысле у вас
[52:21.840 --> 52:23.840]  произошла компенсация.
[52:23.840 --> 52:25.840]  Вы домножили
[52:25.840 --> 52:27.840]  градиент на d,
[52:27.840 --> 52:29.840]  чтобы он был не смещенным.
[52:29.840 --> 52:31.840]  Но в шаге
[52:31.840 --> 52:33.840]  вы в итоге
[52:33.840 --> 52:35.840]  на d разделили.
[52:35.840 --> 52:37.840]  То есть в итоге у вас что получилось?
[52:37.840 --> 52:39.840]  У вас
[52:39.840 --> 52:41.840]  итоговая сумма
[52:41.840 --> 52:43.840]  получилась такая, что у вас
[52:43.840 --> 52:45.840]  умножение на d, деление d на d
[52:45.840 --> 52:47.840]  никакого фактора от этой дешки нет.
[52:47.840 --> 52:49.840]  А почему так? Почему так вообще происходит?
[52:49.840 --> 52:51.840]  Потому что в некоторых крайних
[52:51.840 --> 52:53.840]  случаях может так
[52:53.840 --> 52:55.840]  случиться, что
[52:55.840 --> 52:57.840]  ну
[52:57.840 --> 52:59.840]  вы
[52:59.840 --> 53:01.840]  координатным методом по факту
[53:01.840 --> 53:03.840]  вы цепляете
[53:03.840 --> 53:05.840]  полный градиент.
[53:05.840 --> 53:07.840]  Пусть у вас есть какая-то совсем простенькая задача.
[53:07.840 --> 53:09.840]  x1 в квадрате
[53:09.840 --> 53:11.840]  плюс x2 в квадрате вы минимизируете.
[53:11.840 --> 53:13.840]  И вдруг вам задали
[53:13.840 --> 53:15.840]  стартовую точку, например, 0.1.
[53:15.840 --> 53:17.840]  Стартовую точку 0.1.
[53:17.840 --> 53:19.840]  И, как вы понимаете,
[53:19.840 --> 53:21.840]  то, что по второй переменной вы по факту
[53:21.840 --> 53:23.840]  все уже минимизировали.
[53:23.840 --> 53:25.840]  И у вас
[53:27.840 --> 53:29.840]  то, что сейчас будет записано,
[53:29.840 --> 53:31.840]  градиент такой вот,
[53:31.840 --> 53:33.840]  и вы из него начинаете
[53:33.840 --> 53:35.840]  выдергивать координаты.
[53:35.840 --> 53:37.840]  Вы начинаете из него выдергивать координаты.
[53:39.840 --> 53:41.840]  Выбирай иногда 0 и
[53:41.840 --> 53:43.840]  иногда полную координату.
[53:43.840 --> 53:45.840]  Если, опять же, по этой стратегии,
[53:45.840 --> 53:47.840]  которую мы с вами обговаривали, вам нужно домножать
[53:47.840 --> 53:49.840]  на d, ну можем даже
[53:49.840 --> 53:51.840]  для наглядности задачу
[53:51.840 --> 53:53.840]  супер увеличить до размерности d,
[53:53.840 --> 53:55.840]  и здесь стартовать из всех
[53:55.840 --> 53:57.840]  нулей, кроме дешки, тогда вы
[53:57.840 --> 53:59.840]  умножаете на d.
[53:59.840 --> 54:01.840]  Вы же что делаете, когда вы выбираете
[54:01.840 --> 54:03.840]  нулевые координаты?
[54:03.840 --> 54:05.840]  У вас нулевой градиент и ничего не происходит.
[54:05.840 --> 54:07.840]  А когда вы выбираете
[54:07.840 --> 54:09.840]  первую координату, просто случайно,
[54:09.840 --> 54:11.840]  вы на самом деле посчитали
[54:11.840 --> 54:13.840]  честный градиент. Честный градиент
[54:13.840 --> 54:15.840]  в текущей точке.
[54:15.840 --> 54:17.840]  Потому что вот так у вас устроена
[54:17.840 --> 54:19.840]  стартовая точка, так вам
[54:19.840 --> 54:21.840]  в некотором смысле
[54:21.840 --> 54:23.840]  подфартило, с другой стороны, не подфартило.
[54:23.840 --> 54:25.840]  То есть вы реальный
[54:25.840 --> 54:27.840]  градиент умножили на d,
[54:27.840 --> 54:29.840]  но этого же нельзя делать,
[54:29.840 --> 54:31.840]  ну просто так, за бесплатно.
[54:31.840 --> 54:33.840]  Мы знаем, что в градиентном спуске,
[54:33.840 --> 54:35.840]  по факту вы сейчас будете пытаться сделать
[54:35.840 --> 54:37.840]  шаг обычного градиентного спуска, так делать
[54:37.840 --> 54:39.840]  нельзя. Увеличение шага,
[54:39.840 --> 54:41.840]  воспределенный шаг 2
[54:41.840 --> 54:43.840]  делить на l, а вы тут
[54:43.840 --> 54:45.840]  пытаетесь увеличить
[54:45.840 --> 54:47.840]  размер градиента в d
[54:47.840 --> 54:49.840]  раз, то есть запихать в шаг
[54:49.840 --> 54:51.840]  дополнительную дешку, ее нужно компенсировать.
[54:51.840 --> 54:53.840]  И она, понятно, будет у вас компенсирована
[54:53.840 --> 54:55.840]  в шаге.
[54:55.840 --> 54:57.840]  Вот. Ровно из-за
[54:57.840 --> 54:59.840]  этой ситуации.
[54:59.840 --> 55:01.840]  Из-за этой ситуации, потому что
[55:01.840 --> 55:03.840]  в некоторых частных случаях
[55:03.840 --> 55:05.840]  вызов координаты, выбор просто
[55:05.840 --> 55:07.840]  координат эквивалентен, по факту подсчета
[55:07.840 --> 55:09.840]  полного градиента.
[55:09.840 --> 55:11.840]  И поэтому
[55:11.840 --> 55:13.840]  увеличение градиента в d раз
[55:13.840 --> 55:15.840]  сразу же увлечет уменьшение
[55:15.840 --> 55:17.840]  шага в d раз.
[55:17.840 --> 55:19.840]  Вся идея. Поэтому вот здесь
[55:19.840 --> 55:21.840]  появляется компенсация
[55:21.840 --> 55:23.840]  в виде d делить на l.
[55:23.840 --> 55:25.840]  Окей. Ну, тогда, раз
[55:25.840 --> 55:27.840]  появилась компенсация, мы можем убить
[55:27.840 --> 55:29.840]  вот это все безобразие, получить
[55:29.840 --> 55:31.840]  линейную сходимость, которую, опять же,
[55:31.840 --> 55:33.840]  мы любим, знаем, и
[55:33.840 --> 55:35.840]  все. Дальше
[55:35.840 --> 55:37.840]  опять берем полным от ожидания, пользуемся
[55:37.840 --> 55:39.840]  tower property, получаем честную линейную
[55:39.840 --> 55:41.840]  сходимость. Единственно, что выглядит
[55:41.840 --> 55:43.840]  вообще как будто для градиентного спуска.
[55:43.840 --> 55:45.840]  Но, опять же, разница только в том,
[55:45.840 --> 55:47.840]  что теперь в шаг закинуто константа
[55:47.840 --> 55:49.840]  d. И когда вы будете вычислять
[55:49.840 --> 55:51.840]  реально уже результат для сходимости,
[55:51.840 --> 55:53.840]  больше чем 1 делить на d вы подставить
[55:53.840 --> 55:55.840]  не сможете. И поэтому здесь у вас
[55:55.840 --> 55:57.840]  возникнет что-то вот такое.
[55:57.840 --> 55:59.840]  Вот такой вот множитель.
[55:59.840 --> 56:01.840]  И когда вы будете это переводить уже
[56:01.840 --> 56:03.840]  в итерационную сложность,
[56:03.840 --> 56:05.840]  в итерационную сложность, в итерационную сложность
[56:05.840 --> 56:07.840]  у вас будет d делить на mu l.
[56:07.840 --> 56:09.840]  logarithm
[56:09.840 --> 56:11.840]  1 делить на epsilon.
[56:11.840 --> 56:13.840]  1 делить на epsilon.
[56:13.840 --> 56:15.840]  То есть получается, как она соотносится
[56:15.840 --> 56:17.840]  со сложностью градиентного спуска.
[56:17.840 --> 56:19.840]  В d раз больше, в d раз хуже.
[56:19.840 --> 56:21.840]  Опять же, вот этот пример,
[56:21.840 --> 56:23.840]  который мы с вами рассматривали,
[56:23.840 --> 56:25.840]  x1 в квадрате, x2 в квадрате
[56:25.840 --> 56:27.840]  и так далее,
[56:27.840 --> 56:29.840]  xd в квадрате
[56:29.840 --> 56:31.840]  и стартовая точка
[56:31.840 --> 56:33.840]  1000.
[56:33.840 --> 56:35.840]  По факту же вам, чтобы
[56:35.840 --> 56:37.840]  хорошо минимизировать эту функцию
[56:37.840 --> 56:39.840]  градиентным спуском, нужно
[56:39.840 --> 56:41.840]  один раз посчитать градиент
[56:41.840 --> 56:43.840]  по первой компоненте.
[56:43.840 --> 56:45.840]  Ну или несколько раз в зависимости от того,
[56:45.840 --> 56:47.840]  какой вы возьмете шаг, но если правильно
[56:47.840 --> 56:49.840]  подберете шаг, то вообще один раз и вы сразу же
[56:49.840 --> 56:51.840]  упадете в оптимум.
[56:51.840 --> 56:53.840]  Но возникает проблема
[56:53.840 --> 56:55.840]  в силу того, что во-первых, одну проблему
[56:55.840 --> 56:57.840]  мы решили. Мы компенсировали
[56:57.840 --> 56:59.840]  эту d, которая дает несмещенность
[56:59.840 --> 57:01.840]  в шаге, но это не страшно.
[57:01.840 --> 57:03.840]  То есть теперь у нас будет получаться по факту шаг
[57:03.840 --> 57:05.840]  градиентного спуска, когда мы будем выбирать
[57:05.840 --> 57:07.840]  нужную координату в градиенте.
[57:07.840 --> 57:09.840]  Но эта координата
[57:09.840 --> 57:11.840]  теперь, к сожалению, будет выпадать нам
[57:11.840 --> 57:13.840]  необязательно
[57:13.840 --> 57:15.840]  сразу, потому что мы выбираем одну из d координат
[57:15.840 --> 57:17.840]  и во всех остальных
[57:17.840 --> 57:19.840]  координатах у вас просто ноль
[57:19.840 --> 57:21.840]  градиента, вы никуда не двигаетесь.
[57:21.840 --> 57:23.840]  Ну и как вы понимаете,
[57:23.840 --> 57:25.840]  это что? Это просто
[57:25.840 --> 57:27.840]  геометрическая случайная величина.
[57:27.840 --> 57:29.840]  Вам нужно выбрать
[57:29.840 --> 57:31.840]  то есть вы подбрасываете монетку, вероятность
[57:31.840 --> 57:33.840]  выпадения успеха 1 на d.
[57:33.840 --> 57:35.840]  Понятно, что количество
[57:35.840 --> 57:37.840]  подбросов, чтобы у вас появился
[57:37.840 --> 57:39.840]  успех, чтобы вы выбили
[57:39.840 --> 57:41.840]  нужную вам координату, он будет
[57:41.840 --> 57:43.840]  пропорционален d.
[57:43.840 --> 57:45.840]  И вот, к сожалению,
[57:45.840 --> 57:47.840]  эта d она здесь и возникает.
[57:47.840 --> 57:49.840]  То есть даже вот такая совсем простая задача
[57:49.840 --> 57:51.840]  вам говорит о том, что
[57:51.840 --> 57:53.840]  вот эта проблема
[57:53.840 --> 57:55.840]  координатного метода,
[57:55.840 --> 57:57.840]  увеличение количества итераций на d,
[57:59.840 --> 58:01.840]  она не решается.
[58:01.840 --> 58:03.840]  Она не решается, если вы не введете
[58:03.840 --> 58:05.840]  какие-то дополнительные знания о задачи.
[58:05.840 --> 58:07.840]  Понятно, что
[58:07.840 --> 58:09.840]  если вы знаете, что во всех точках
[58:09.840 --> 58:11.840]  с оптиум кроме нужной точки
[58:11.840 --> 58:13.840]  вы сразу же в нее ударите,
[58:13.840 --> 58:15.840]  в некотором смысле сместите вероятность
[58:15.840 --> 58:17.840]  просто в нужное место
[58:17.840 --> 58:19.840]  и все будет хорошо.
[58:19.840 --> 58:21.840]  Но вы же этого изначально не знаете,
[58:21.840 --> 58:23.840]  вы делаете какой-то solver, который более-менее
[58:23.840 --> 58:25.840]  универсально работает.
[58:25.840 --> 58:27.840]  Поэтому в общем случае ничего хорошего.
[58:27.840 --> 58:29.840]  Ничего хорошего.
[58:29.840 --> 58:31.840]  Окей.
[58:31.840 --> 58:33.840]  Что еще?
[58:33.840 --> 58:35.840]  А в градиентном слуске это как соотносится
[58:35.840 --> 58:37.840]  в сложности?
[58:37.840 --> 58:39.840]  Будет то же самое.
[58:39.840 --> 58:41.840]  Если мы предполагаем, что вычисление
[58:41.840 --> 58:43.840]  не обязательно,
[58:43.840 --> 58:45.840]  что может произойти,
[58:45.840 --> 58:47.840]  что градиентный спуск почисляет полный градиент.
[58:47.840 --> 58:49.840]  Здесь мы считаем одну из производных
[58:49.840 --> 58:51.840]  по направлению и кажется, что
[58:51.840 --> 58:53.840]  в хорошем случае это в d раз дешевле.
[58:53.840 --> 58:55.840]  Понятно, в общем случае это не совсем так.
[58:55.840 --> 58:57.840]  Часто бывает это
[58:57.840 --> 58:59.840]  не в d раз дешевле,
[58:59.840 --> 59:01.840]  чуть меньше, чем в d раз дешевле.
[59:01.840 --> 59:03.840]  В хорошем случае это в d раз дешевле,
[59:03.840 --> 59:05.840]  но как мы видим количество итераций
[59:05.840 --> 59:07.840]  в d раз возрастает.
[59:07.840 --> 59:09.840]  Здесь в d раз выиграли
[59:09.840 --> 59:11.840]  с точки зрения вычисления градиента,
[59:11.840 --> 59:13.840]  но в d раз проиграли с точки зрения
[59:13.840 --> 59:15.840]  итераций.
[59:15.840 --> 59:17.840]  К сожалению, в отличие от методов редукции дисперсии,
[59:17.840 --> 59:19.840]  координатные методы
[59:19.840 --> 59:21.840]  с точки зрения аракулной сложности
[59:21.840 --> 59:23.840]  почета производных
[59:23.840 --> 59:25.840]  по направлению, по координате,
[59:25.840 --> 59:27.840]  они не улучшаемы.
[59:27.840 --> 59:29.840]  Как мы видим, на простой задаче это
[59:29.840 --> 59:31.840]  подтверждается.
[59:31.840 --> 59:33.840]  Вот такая вот беда.
[59:33.840 --> 59:35.840]  Но опять же можно
[59:35.840 --> 59:37.840]  в некотором смысле эти свойства
[59:37.840 --> 59:39.840]  чуть улучшить,
[59:39.840 --> 59:41.840]  когда вы знаете какую-то
[59:41.840 --> 59:43.840]  специфику задачи.
[59:43.840 --> 59:45.840]  Знаете, например, что
[59:45.840 --> 59:47.840]  константа липшицы отличается
[59:47.840 --> 59:49.840]  по разным из направлений.
[59:49.840 --> 59:51.840]  Но это опять же такая специфика,
[59:51.840 --> 59:53.840]  которая, как мы знаем, может улучшить
[59:53.840 --> 59:55.840]  оценку, просто потому что
[59:55.840 --> 59:57.840]  задача в некотором классе сужается,
[59:57.840 --> 59:59.840]  поэтому исходимость может стать быстрее.
[59:59.840 --> 01:00:01.840]  Это ровно о том, что
[01:00:01.840 --> 01:00:03.840]  когда мы с вами говорили, что
[01:00:03.840 --> 01:00:05.840]  есть, например,
[01:00:05.840 --> 01:00:07.840]  выпуклые задачи, для них есть
[01:00:07.840 --> 01:00:09.840]  одна оценка, сублинейная исходимость для
[01:00:09.840 --> 01:00:11.840]  градиентного спуска. Если вы говорите,
[01:00:11.840 --> 01:00:13.840]  что задача не просто выпуклая, а сильно выпуклая,
[01:00:13.840 --> 01:00:15.840]  у вас появляется уже линейная исходимость,
[01:00:15.840 --> 01:00:17.840]  но вы сузили класс задач,
[01:00:17.840 --> 01:00:19.840]  который вы умеете решать.
[01:00:19.840 --> 01:00:21.840]  Здесь то же самое, добавляете какие-то дополнительные
[01:00:21.840 --> 01:00:23.840]  хорошие свойства, понятно, могут быть какие-то
[01:00:23.840 --> 01:00:25.840]  и улучшения.
[01:00:25.840 --> 01:00:27.840]  На самом деле это все, что я хотел
[01:00:27.840 --> 01:00:29.840]  сегодня рассказать, но время у нас осталось,
[01:00:29.840 --> 01:00:31.840]  поэтому давайте чуть-чуть еще
[01:00:31.840 --> 01:00:33.840]  пообсуждаем тот
[01:00:33.840 --> 01:00:35.840]  подход, а почему у меня, кстати,
[01:00:35.840 --> 01:00:37.840]  слайды не пролистываются.
[01:00:37.840 --> 01:00:39.840]  Тут я просто говорю, что на самом деле
[01:00:39.840 --> 01:00:41.840]  координатный метод хорошо себя проявляет на практике,
[01:00:41.840 --> 01:00:43.840]  потому что уменьшение
[01:00:43.840 --> 01:00:45.840]  шага не обязательно делать
[01:00:45.840 --> 01:00:47.840]  в ДРС.
[01:00:47.840 --> 01:00:49.840]  Понятно, можно
[01:00:49.840 --> 01:00:51.840]  это все обобщить на так называемый
[01:00:51.840 --> 01:00:53.840]  блочно-координатный случай, когда
[01:00:53.840 --> 01:00:55.840]  вы считаете производную
[01:00:55.840 --> 01:00:57.840]  шагу не по одному направлению, а по сразу
[01:00:57.840 --> 01:00:59.840]  нескольким направлениям по бачу направлений.
[01:00:59.840 --> 01:01:01.840]  Для задач большой
[01:01:01.840 --> 01:01:03.840]  размерности это
[01:01:03.840 --> 01:01:05.840]  такой хороший рабочий способ.
[01:01:05.840 --> 01:01:07.840]  И понятно, что это все безобразие, можно
[01:01:07.840 --> 01:01:09.840]  ускорить с помощью моментума.
[01:01:09.840 --> 01:01:11.840]  Метод чуть более хитрый будет, там нужно добавить
[01:01:11.840 --> 01:01:13.840]  второй моментум.
[01:01:13.840 --> 01:01:15.840]  Но, в принципе, это все ускоряется
[01:01:15.840 --> 01:01:17.840]  и получается тоже на самом деле
[01:01:17.840 --> 01:01:19.840]  оценка такая, что по сравнению с
[01:01:19.840 --> 01:01:21.840]  Нестером вы ничего не выигрываете,
[01:01:21.840 --> 01:01:23.840]  то есть там тоже в ДРС больше по количеству
[01:01:23.840 --> 01:01:25.840]  вот такая вот проблема, к сожалению,
[01:01:25.840 --> 01:01:27.840]  у координатных методов она
[01:01:27.840 --> 01:01:29.840]  есть.
[01:01:29.840 --> 01:01:31.840]  На самом деле у вас был проект про координатный
[01:01:31.840 --> 01:01:33.840]  метод и редукцию дисперсии в координатных
[01:01:33.840 --> 01:01:35.840]  методах.
[01:01:35.840 --> 01:01:37.840]  Но раз у нас время осталось, давайте про нее и поговорим.
[01:01:37.840 --> 01:01:39.840]  Потому что, опять же,
[01:01:39.840 --> 01:01:41.840]  координатный метод,
[01:01:41.840 --> 01:01:43.840]  он похож на SGD.
[01:01:43.840 --> 01:01:45.840]  В SGD выбрали бач, пошли
[01:01:45.840 --> 01:01:47.840]  по бачу, здесь выбрали координат
[01:01:47.840 --> 01:01:49.840]  и пошли по координате.
[01:01:49.840 --> 01:01:51.840]  И возникает такая же естественная
[01:01:51.840 --> 01:01:53.840]  идея, как
[01:01:53.840 --> 01:01:55.840]  в методе SEGA.
[01:01:55.840 --> 01:01:57.840]  В методе SEGA, раз
[01:01:57.840 --> 01:01:59.840]  вы уж пошли
[01:01:59.840 --> 01:02:01.840]  по какой-то координате,
[01:02:01.840 --> 01:02:03.840]  посчитали производной по этой координате,
[01:02:03.840 --> 01:02:05.840]  давайте сохраним ее.
[01:02:05.840 --> 01:02:07.840]  Давайте ее сохраним.
[01:02:07.840 --> 01:02:09.840]  И тогда что получается?
[01:02:09.840 --> 01:02:11.840]  Давайте опять же введу
[01:02:11.840 --> 01:02:13.840]  некоторый вектор y, в данном случае он
[01:02:13.840 --> 01:02:15.840]  будет просто размерности d.
[01:02:15.840 --> 01:02:17.840]  Размерности d.
[01:02:17.840 --> 01:02:19.840]  И что я буду y сохранять?
[01:02:19.840 --> 01:02:21.840]  Я специализирую этот вектор нулями.
[01:02:21.840 --> 01:02:23.840]  Просто нулевой вектор
[01:02:23.840 --> 01:02:25.840]  какой-то у меня будет.
[01:02:25.840 --> 01:02:27.840]  И что будет происходить? Давайте я
[01:02:27.840 --> 01:02:29.840]  выбираю какую-то одну из координат
[01:02:29.840 --> 01:02:31.840]  равномерно и
[01:02:31.840 --> 01:02:33.840]  независимо
[01:02:33.840 --> 01:02:35.840]  от предыдущих итераций.
[01:02:35.840 --> 01:02:37.840]  Считаю производную по этой координате.
[01:02:37.840 --> 01:02:39.840]  Давайте вот так. Я ее записываю.
[01:02:41.840 --> 01:02:43.840]  И вектор y
[01:02:43.840 --> 01:02:45.840]  обновляюсь соответствующим образом.
[01:02:45.840 --> 01:02:47.840]  Я беру y.
[01:02:47.840 --> 01:02:49.840]  Давайте
[01:02:49.840 --> 01:02:51.840]  k плюс первый
[01:02:51.840 --> 01:02:53.840]  равен y к этому.
[01:02:53.840 --> 01:02:55.840]  И
[01:02:55.840 --> 01:02:57.840]  из старого y я забираю
[01:02:57.840 --> 01:02:59.840]  ту координату, которая у меня была.
[01:02:59.840 --> 01:03:01.840]  Та координата, которая у меня хранилась.
[01:03:01.840 --> 01:03:03.840]  И записываю вместо нее ту координату,
[01:03:03.840 --> 01:03:05.840]  которую я посчитал.
[01:03:09.840 --> 01:03:11.840]  Ровно та же самая похожая идея
[01:03:11.840 --> 01:03:13.840]  на сагу.
[01:03:13.840 --> 01:03:15.840]  Метод называется SEGA.
[01:03:15.840 --> 01:03:17.840]  Потому что
[01:03:17.840 --> 01:03:19.840]  идея
[01:03:19.840 --> 01:03:21.840]  довольно эквивалентна.
[01:03:21.840 --> 01:03:23.840]  Понятно, что там были бы
[01:03:23.840 --> 01:03:25.840]  координаты.
[01:03:25.840 --> 01:03:27.840]  И вы векторе y
[01:03:27.840 --> 01:03:29.840]  сохраняете те координаты,
[01:03:29.840 --> 01:03:31.840]  которые вы когда-то посчитали.
[01:03:31.840 --> 01:03:33.840]  Дальше опять же
[01:03:33.840 --> 01:03:35.840]  тут два варианта.
[01:03:35.840 --> 01:03:37.840]  Два варианта оказывается,
[01:03:37.840 --> 01:03:39.840]  что можно, например,
[01:03:39.840 --> 01:03:41.840]  так и делать шаг
[01:03:41.840 --> 01:03:43.840]  по y.
[01:03:45.840 --> 01:03:47.840]  Это, кстати, на самом деле
[01:03:47.840 --> 01:03:49.840]  не SEGA.
[01:03:49.840 --> 01:03:51.840]  Это такого рода метод
[01:03:51.840 --> 01:03:53.840]  казался
[01:03:53.840 --> 01:03:55.840]  довольно плохим.
[01:03:55.840 --> 01:03:57.840]  То, что я вам говорил про SEGA.
[01:03:57.840 --> 01:03:59.840]  Средний y,
[01:03:59.840 --> 01:04:01.840]  который хранится в SEGA,
[01:04:01.840 --> 01:04:03.840]  не обязательно плохой.
[01:04:03.840 --> 01:04:05.840]  Потому что
[01:04:05.840 --> 01:04:07.840]  в методе
[01:04:07.840 --> 01:04:09.840]  мне сделал студент Андрей.
[01:04:09.840 --> 01:04:11.840]  Он попробовал
[01:04:11.840 --> 01:04:13.840]  в классической SEGA
[01:04:13.840 --> 01:04:15.840]  взять
[01:04:15.840 --> 01:04:17.840]  вот этот y, который мы получили.
[01:04:17.840 --> 01:04:19.840]  Назвал этот метод Ягуар.
[01:04:19.840 --> 01:04:21.840]  И оказывается, что в таком варианте
[01:04:21.840 --> 01:04:23.840]  он работает даже лучше
[01:04:23.840 --> 01:04:25.840]  на практике для некоторых задач.
[01:04:25.840 --> 01:04:27.840]  И более того,
[01:04:27.840 --> 01:04:29.840]  в теории для невыпуклых задач,
[01:04:29.840 --> 01:04:31.840]  для методов типа
[01:04:31.840 --> 01:04:33.840]  Франко Вульфа тоже дают лучше оценки, чем SEGA.
[01:04:33.840 --> 01:04:35.840]  Проблема только в том,
[01:04:35.840 --> 01:04:37.840]  что вот здесь вот этот y,
[01:04:37.840 --> 01:04:39.840]  как вы понимаете,
[01:04:39.840 --> 01:04:41.840]  по математическому ожиданию,
[01:04:41.840 --> 01:04:43.840]  не дает честный градиент.
[01:04:43.840 --> 01:04:45.840]  Не дает честный градиент – не дает.
[01:04:45.840 --> 01:04:47.840]  Но, как оказалось,
[01:04:47.840 --> 01:04:49.840]  это не страшно.
[01:04:49.840 --> 01:04:51.840]  Это то, про что я говорил,
[01:04:51.840 --> 01:04:53.840]  в том числе, когда мы говорили про SEGA.
[01:04:53.840 --> 01:04:55.840]  Не обязательно вот тот y,
[01:04:55.840 --> 01:04:57.840]  который, та самая банальная идея
[01:04:57.840 --> 01:04:59.840]  использовать просто
[01:04:59.840 --> 01:05:01.840]  средний y как градиент,
[01:05:01.840 --> 01:05:03.840]  она не обязательно страшная.
[01:05:03.840 --> 01:05:05.840]  И никто не говорит о том,
[01:05:05.840 --> 01:05:07.840]  что нужно использовать
[01:05:07.840 --> 01:05:09.840]  обязательно какую-то
[01:05:09.840 --> 01:05:11.840]  несмещенную оценку градиента.
[01:05:11.840 --> 01:05:13.840]  Исходный алгоритм SEGA действительно
[01:05:13.840 --> 01:05:15.840]  использует несмещенную оценку градиента.
[01:05:15.840 --> 01:05:17.840]  Для этого там делается
[01:05:17.840 --> 01:05:19.840]  следующая манипуляция.
[01:05:19.840 --> 01:05:21.840]  Используется домножение на D.
[01:05:21.840 --> 01:05:23.840]  Это ровно то,
[01:05:23.840 --> 01:05:25.840]  что мы видели и в Саге.
[01:05:25.840 --> 01:05:27.840]  Домножаете на дополнительный
[01:05:27.840 --> 01:05:29.840]  множитель, там был N,
[01:05:29.840 --> 01:05:31.840]  здесь, соответственно, у нас D.
[01:05:31.840 --> 01:05:33.840]  Вот это SEGA.
[01:05:33.840 --> 01:05:35.840]  Это SEGA.
[01:05:35.840 --> 01:05:37.840]  И там, соответственно, делается шаг
[01:05:37.840 --> 01:05:39.840]  xk плюс 1
[01:05:39.840 --> 01:05:41.840]  равно xk
[01:05:41.840 --> 01:05:43.840]  минус гамма zhk.
[01:05:43.840 --> 01:05:45.840]  Давайте вот так.
[01:05:45.840 --> 01:05:47.840]  Запишем.
[01:05:47.840 --> 01:05:49.840]  Это две вещи, это SEGA.
[01:05:49.840 --> 01:05:51.840]  В Ugoar такого не делается,
[01:05:51.840 --> 01:05:53.840]  метод даже становится проще,
[01:05:53.840 --> 01:05:55.840]  не нужно вычислять какое-то дополнительное zhkt.
[01:05:55.840 --> 01:05:57.840]  И оказывается,
[01:05:57.840 --> 01:05:59.840]  что в некоторых случаях
[01:05:59.840 --> 01:06:01.840]  он еще и оценки исходимости дает лучше.
[01:06:01.840 --> 01:06:03.840]  Поэтому ровно то, что я говорил про SEGA.
[01:06:03.840 --> 01:06:05.840]  Не обязательно там использовать
[01:06:05.840 --> 01:06:07.840]  несмещенную оценку.
[01:06:07.840 --> 01:06:09.840]  Никто не говорит, что несмещенная оценка
[01:06:09.840 --> 01:06:11.840]  это всегда хорошо.
[01:06:11.840 --> 01:06:13.840]  Опять же, на статистике, я думаю, вам это тоже рассказали,
[01:06:13.840 --> 01:06:15.840]  что несмещенность
[01:06:15.840 --> 01:06:17.840]  это, конечно, хорошее свойство,
[01:06:17.840 --> 01:06:19.840]  но это не панацея.
[01:06:19.840 --> 01:06:21.840]  Давайте можно на всякий случай
[01:06:21.840 --> 01:06:23.840]  проверить, что у SEGA это будет
[01:06:23.840 --> 01:06:25.840]  несмещенная оценка.
[01:06:25.840 --> 01:06:27.840]  Понятно, у Ugoar тогда это будет
[01:06:27.840 --> 01:06:29.840]  уже смещенная оценка, но проблем в этом нет.
[01:06:29.840 --> 01:06:31.840]  Проверяем.
[01:06:31.840 --> 01:06:33.840]  Берем, опять же, условное математическое ожидание.
[01:06:33.840 --> 01:06:35.840]  И грек в этом условном математическом ожидании
[01:06:35.840 --> 01:06:37.840]  никак не фигурирует, потому что
[01:06:37.840 --> 01:06:39.840]  он взят с прошлой террации,
[01:06:39.840 --> 01:06:41.840]  поэтому это условное математическое ожидание действует
[01:06:41.840 --> 01:06:43.840]  только на IKT, на наш выбор координаты.
[01:06:43.840 --> 01:06:45.840]  Действуем тогда
[01:06:45.840 --> 01:06:47.840]  ею.
[01:06:51.840 --> 01:06:53.840]  Условно математическим ожиданием.
[01:06:57.840 --> 01:06:59.840]  Вот.
[01:06:59.840 --> 01:07:01.840]  Ну и когда подействуем условно математическим
[01:07:01.840 --> 01:07:03.840]  ожиданием, что у нас вылезет, кто понимает
[01:07:03.840 --> 01:07:05.840]  быстренько, из условного математического
[01:07:05.840 --> 01:07:07.840]  ожидания.
[01:07:07.840 --> 01:07:09.840]  Опять же, сумма.
[01:07:09.840 --> 01:07:11.840]  Ну давайте
[01:07:11.840 --> 01:07:13.840]  я вот так буду записывать, так как мы каждую координату
[01:07:13.840 --> 01:07:15.840]  с вероятностью 1 на D эта вероятность
[01:07:15.840 --> 01:07:17.840]  вылезает. Понятно, она сейчас
[01:07:17.840 --> 01:07:19.840]  сократится сразу же.
[01:07:19.840 --> 01:07:21.840]  И здесь у нас что будет вылезать?
[01:07:21.840 --> 01:07:23.840]  y, it, j,
[01:07:23.840 --> 01:07:25.840]  j, индекс,
[01:07:25.840 --> 01:07:27.840]  d, e, j,
[01:07:27.840 --> 01:07:29.840]  базисный вектор, или давайте базисный вектор
[01:07:29.840 --> 01:07:31.840]  я даже поставлю.
[01:07:31.840 --> 01:07:33.840]  За пределы я вот так вот сделаю.
[01:07:33.840 --> 01:07:35.840]  f, x, k, t,
[01:07:35.840 --> 01:07:37.840]  j,
[01:07:37.840 --> 01:07:39.840]  e, j.
[01:07:39.840 --> 01:07:41.840]  Вот.
[01:07:41.840 --> 01:07:43.840]  Все сократилось. Понятно, что когда вы сейчас
[01:07:43.840 --> 01:07:45.840]  просумируете вот эти все y,
[01:07:45.840 --> 01:07:47.840]  y, j, t умножить на e, j, t,
[01:07:47.840 --> 01:07:49.840]  это будет просто полный вектор
[01:07:49.840 --> 01:07:51.840]  y, и тогда у вас здесь останется
[01:07:51.840 --> 01:07:53.840]  честный градиент f от x, k.
[01:07:53.840 --> 01:07:55.840]  Вот. Это не выполняется
[01:07:55.840 --> 01:07:57.840]  для Егора, как вы понимаете, в силу того,
[01:07:57.840 --> 01:07:59.840]  что там нет этого домножения на d.
[01:07:59.840 --> 01:08:01.840]  Ну, не страшно, не страшно.
[01:08:01.840 --> 01:08:03.840]  Вот. И аналогичным
[01:08:03.840 --> 01:08:05.840]  образом здесь делается
[01:08:05.840 --> 01:08:07.840]  техника редукции дисперсии.
[01:08:07.840 --> 01:08:09.840]  То есть, также в доказательстве вы
[01:08:09.840 --> 01:08:11.840]  используете
[01:08:11.840 --> 01:08:13.840]  дополнительную
[01:08:13.840 --> 01:08:15.840]  последовательность, дополнительную
[01:08:15.840 --> 01:08:17.840]  последовательность. Единственное, что
[01:08:17.840 --> 01:08:19.840]  здесь это вроде как
[01:08:19.840 --> 01:08:21.840]  техника редукции дисперсии сильно и не нужна.
[01:08:21.840 --> 01:08:23.840]  Потому что, когда вы выбираете координаты,
[01:08:23.840 --> 01:08:25.840]  как мы поняли,
[01:08:25.840 --> 01:08:27.840]  это не страшно,
[01:08:27.840 --> 01:08:29.840]  потому что у вас
[01:08:29.840 --> 01:08:31.840]  градиент
[01:08:31.840 --> 01:08:33.840]  стремится к нулю.
[01:08:33.840 --> 01:08:35.840]  Но даже если вы выберете у него какую-то случайную координату,
[01:08:35.840 --> 01:08:37.840]  она тоже стремится к нулю, потому что весь вектор
[01:08:37.840 --> 01:08:39.840]  стремится к нулю, значит и координата стремится к нулю.
[01:08:39.840 --> 01:08:41.840]  Координаты стремятся к нулю.
[01:08:41.840 --> 01:08:43.840]  Поэтому дисперсия
[01:08:43.840 --> 01:08:45.840]  того, что происходит вообще
[01:08:45.840 --> 01:08:47.840]  в координатном методе, она не так страшна.
[01:08:47.840 --> 01:08:49.840]  То есть, она действительно
[01:08:49.840 --> 01:08:51.840]  стремится к нулю, поэтому variance reduction
[01:08:51.840 --> 01:08:53.840]  в координатном методе не нужен.
[01:08:53.840 --> 01:08:55.840]  Но почему бы его не сделать?
[01:08:55.840 --> 01:08:57.840]  Просто потому, что это какие-то хорошие свойства
[01:08:57.840 --> 01:08:59.840]  памяти, которые у вас есть.
[01:08:59.840 --> 01:09:01.840]  Вы запоминаете старые координаты
[01:09:01.840 --> 01:09:03.840]  и говорите, что их переиспользуете.
[01:09:05.840 --> 01:09:07.840]  Значит, возможно вносите
[01:09:07.840 --> 01:09:09.840]  что-то хорошее в метод, который у вас есть.
[01:09:09.840 --> 01:09:11.840]  Оценки сходимости
[01:09:11.840 --> 01:09:13.840]  у Сеги ровно такие же,
[01:09:13.840 --> 01:09:15.840]  как у обычного координатного спуска.
[01:09:15.840 --> 01:09:17.840]  В LZD никакой панацеи тут не происходит.
[01:09:17.840 --> 01:09:19.840]  Опять же, потому что
[01:09:19.840 --> 01:09:21.840]  такая вот задача
[01:09:21.840 --> 01:09:23.840]  можно рассмотреть в нашу неприятную
[01:09:23.840 --> 01:09:25.840]  квадратичную задачу с неприятной стартовой точкой.
[01:09:25.840 --> 01:09:27.840]  То, что вы запомнили,
[01:09:27.840 --> 01:09:29.840]  что происходит в других координатах,
[01:09:29.840 --> 01:09:31.840]  где и так были все нули,
[01:09:31.840 --> 01:09:33.840]  ничего хорошего, ничего плохого вам это не даст.
[01:09:33.840 --> 01:09:35.840]  Поэтому вам нужна
[01:09:35.840 --> 01:09:37.840]  конкретная координата.
[01:09:37.840 --> 01:09:39.840]  Все вырубилось, да?
[01:09:41.840 --> 01:09:43.840]  Ну ладно, это, наверное, знак.
[01:09:43.840 --> 01:09:45.840]  Это, наверное, знак.
[01:09:45.840 --> 01:09:47.840]  Сега, соответственно,
[01:09:47.840 --> 01:09:49.840]  не лучше, не хуже.
[01:09:49.840 --> 01:09:51.840]  Но, опять же,
[01:09:51.840 --> 01:09:53.840]  выбор координат, он, возможно, не только
[01:09:53.840 --> 01:09:55.840]  в нераспределенном случае.
[01:09:55.840 --> 01:09:57.840]  На следующей лекции, на последней лекции
[01:09:57.840 --> 01:09:59.840]  мы с вами поговорим про
[01:09:59.840 --> 01:10:01.840]  так называемые
[01:10:01.840 --> 01:10:03.840]  распределенные задачи обучения.
[01:10:03.840 --> 01:10:05.840]  И там нам тоже во многом
[01:10:05.840 --> 01:10:07.840]  будут пригождаться методы, которые
[01:10:07.840 --> 01:10:09.840]  выбирают координаты. Но там у них физика уже другая.
[01:10:09.840 --> 01:10:11.840]  Там борьба идет не за подсчет
[01:10:11.840 --> 01:10:13.840]  градиентов.
[01:10:13.840 --> 01:10:15.840]  Включился?
[01:10:17.840 --> 01:10:19.840]  Что с ним происходит, непонятно.
[01:10:21.840 --> 01:10:23.840]  Там идет борьба не за подсчет
[01:10:23.840 --> 01:10:25.840]  градиентов, ну и за них, на самом деле,
[01:10:25.840 --> 01:10:27.840]  тоже нет. Там больше идет борьба за то,
[01:10:27.840 --> 01:10:29.840]  чтобы пересылать меньше, поэтому вы выбираете
[01:10:29.840 --> 01:10:31.840]  координат. То есть вы считаете
[01:10:31.840 --> 01:10:33.840]  полный градиент, но используете
[01:10:33.840 --> 01:10:35.840]  координатный метод просто потому, что хотите
[01:10:35.840 --> 01:10:37.840]  переслать меньше информации,
[01:10:37.840 --> 01:10:39.840]  вместо полного вектора переслать
[01:10:39.840 --> 01:10:41.840]  1% координат.
[01:10:41.840 --> 01:10:43.840]  И вот там уже
[01:10:43.840 --> 01:10:45.840]  возникнет своя специфика,
[01:10:45.840 --> 01:10:47.840]  которая будет неприятной,
[01:10:47.840 --> 01:10:49.840]  с которой мы с вами, соответственно, будем бороться.
[01:10:49.840 --> 01:10:51.840]  И там уже какие-то техники редукции
[01:10:51.840 --> 01:10:53.840]  дисперсии для координатных методов.
[01:10:53.840 --> 01:10:55.840]  Ну там мы уже будем называть их методами
[01:10:55.840 --> 01:10:57.840]  сжатия.
[01:10:57.840 --> 01:10:59.840]  Там они
[01:10:59.840 --> 01:11:01.840]  понадобятся, там они понадобятся.
[01:11:01.840 --> 01:11:03.840]  Что с ним?
[01:11:03.840 --> 01:11:05.840]  Там они понадобятся,
[01:11:05.840 --> 01:11:07.840]  поэтому жду на следующую
[01:11:07.840 --> 01:11:09.840]  лекцию. Она нам будет такая, скорее, уже обзорная.
[01:11:09.840 --> 01:11:11.840]  Доказательств я там
[01:11:11.840 --> 01:11:13.840]  уже показывать не буду, просто скорее
[01:11:13.840 --> 01:11:15.840]  пройдусь по идеям распределенных методов,
[01:11:15.840 --> 01:11:17.840]  которые не
[01:11:17.840 --> 01:11:19.840]  затронуты будут, например,
[01:11:19.840 --> 01:11:21.840]  вот в пятницу-субботу.
[01:11:21.840 --> 01:11:23.840]  Просто так пройдемся, поговорим.
[01:11:23.840 --> 01:11:25.840]  Понятно, что в программе этого всего в колокве
[01:11:25.840 --> 01:11:27.840]  и монету. Познакомимся,
[01:11:27.840 --> 01:11:29.840]  ну и на этом закончим курс лекции.
[01:11:29.840 --> 01:11:31.840]  А на сегодня все, всем всем спасибо.

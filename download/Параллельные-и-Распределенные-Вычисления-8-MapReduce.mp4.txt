[00:00.000 --> 00:12.300]  Хорошо, давайте продолжать тогда. Вспомним особенности HDFS, потому что мы сейчас будем
[00:12.300 --> 00:16.120]  работать с вычислениями на больших данных, и надо сначала вспомнить, как мы их храним.
[00:16.120 --> 00:20.800]  То есть HDFS это, по сути, с точки зрения пользователя обычная файловая система,
[00:20.800 --> 00:28.080]  но у нее такие особенности. Во-первых, она распределенная, во-вторых, из-за того,
[00:28.080 --> 00:32.160]  что она распределенная, мы устойчивы к отказам, потому что у нас есть копии, есть реплики,
[00:32.160 --> 00:39.840]  и мы используем подход write once, read many. Вспоминаем почему, потому что когда мы читаем,
[00:39.840 --> 00:48.040]  мы читаем в два этапа, и это происходит не очень медленно, а когда мы пишем, мы используем синхронную
[00:48.040 --> 00:53.960]  репликацию, то есть пока все не запишется, пока на все ноды мы все копии не запишем, мы не закончим,
[00:53.960 --> 00:59.320]  и запись она может работать очень долго, поэтому стараемся лишний раз не писать и больше читать.
[00:59.320 --> 01:07.440]  Точнее, что касается этой записи, мы стараемся писать редко, но по многу.
[01:07.440 --> 01:16.280]  Ну и теперь MapReduce. MapReduce такой, как мы его знаем, он появился в 2004 году, вышла статья
[01:16.280 --> 01:24.800]  от двух сотрудников Google, то есть в этом году MapReduce исполняется 20 лет. По этой статье вы можете
[01:24.800 --> 01:30.040]  прочитать математическое обоснование, почему он так работает, и он используется не только в больших
[01:30.040 --> 01:37.200]  данных, а и в питоне. Вот кто знает, есть мап и Reduce раньше был во втором питоне, в третьем питоне его
[01:37.200 --> 01:43.920]  убрали. Поэтому вот MapReduce используется очень много где, и не только в питоне, в других языках
[01:43.920 --> 01:50.960]  программирования. Но мы с вами будем говорить про то, как используется MapReduce в Hadoop. А в Hadoop
[01:50.960 --> 01:56.160]  он используется так, мы работаем с парами типа ключ значения, и эти пары проходят несколько этапов обработки.
[01:56.160 --> 02:03.760]  Первый этап, можно назвать даже нулевой этап, это чтение, когда мы данные просто считываем,
[02:03.760 --> 02:11.480]  и формируем из них пары, в любом случае формируем пары, даже если мы считываем какой-нибудь там
[02:11.480 --> 02:17.840]  какие-нибудь объекты или просто строки, мы все равно делаем пары, это просто значит, что мы поставим
[02:17.840 --> 02:27.840]  рядом null, и у нас будет два значения. Потом стадия Map, на вход подается одна пара, и на выходе мы
[02:27.840 --> 02:34.280]  получаем несколько пар, все это зависит от того, как мы напишем код, может быть несколько пар,
[02:34.280 --> 02:40.840]  может быть одна пара, может быть не одной. Дальше идет стадия Shelf and Sort, когда мы полученные вот
[02:40.840 --> 02:47.680]  эти пары группируем по ключам, и у нас получается уже не одна пара, получается группа, у которой одинаковый
[02:47.680 --> 02:54.800]  ключ, один и тот же. Ладно, идем дальше. В общем, после стадии Sort у нас получаются группы, и с этими
[02:54.800 --> 03:00.640]  группами работает редьюзер. Редьюзер, в отличие от маппера, он на вход получает не одну пару, а целую
[03:00.640 --> 03:08.400]  группу пар, у которых одинаковый ключ. И на выходе мы получаем еще другие пары, у которых, может быть,
[03:08.400 --> 03:14.840]  ключ будет совпадать, может быть, он будет каким-то другим, потому что и мап и редьюз это то, что пишут
[03:14.840 --> 03:21.280]  разработчики сами. И что мы там напишем, как бы заранее система MapReduce не знает.
[03:21.280 --> 03:28.840]  Чтобы было понятнее, как это все работает, вот все из вас знают питон, я думаю. Скажите,
[03:28.840 --> 03:33.040]  как отработает первый кусочек кода и второй кусочек кода, что будет в результате?
[03:33.040 --> 03:51.360]  Не совсем. Ты, видимо, забыл, как range работает с питон, и он начинается с нуля. То есть range 5
[03:51.360 --> 04:01.240]  это будет 0, 1, 2, 3, 4. Поэтому map будет нам давать не от двойки до пятерки, а от единицы до пятерки,
[04:01.240 --> 04:16.200]  и редьюз тоже от нуля до четырех это будет 10. 5 не входит. Это парадигма, с помощью которой мы
[04:16.200 --> 04:21.000]  можем написать алгоритм какой-то, который будет обрабатывать большие данные. То есть mapReduce это
[04:21.000 --> 04:26.800]  не какой-то алгоритм, который мы вот отдельно изучаем, это не какая-нибудь еще одна сортировка или
[04:26.800 --> 04:34.560]  BFS. А это целая парадигма, на основе которой разработан фреймворк, и в рамках этого фреймворка мы
[04:34.560 --> 04:47.360]  будем работать. Что она должна делать? Она должна получать на вход какие-то данные, их как-то
[04:47.360 --> 04:52.680]  преобразовывать распределенно, и на выход выдавать другие данные. Это может быть все что угодно.
[04:52.680 --> 04:58.280]  Мы будем работать в основном с текстом, потому что текст можно сразу увидеть, что было, что стало,
[04:58.280 --> 05:04.000]  но это могут быть вообще любые данные, любые объекты. Да, любое преобразование.
[05:04.000 --> 05:12.360]  Слева сверху.
[05:12.360 --> 05:26.040]  Да, мы независимо обрабатываем. Да, это агрегация. То есть мы независимо обрабатываем кусочки,
[05:26.040 --> 05:33.840]  а в Reduce мы тоже независимо обрабатываем кусочки на самом деле, но они просто больше. Ну и насчет
[05:33.840 --> 05:39.560]  того, что любое преобразование можно сделать на MapReduce, ну вообще да, любое, но некоторые
[05:39.560 --> 05:50.560]  преобразования будут делаться очень больно. За счет того, что мы не можем выйти за рамки
[05:50.560 --> 05:57.600]  MapReduce, то есть вот что бы вы не хотели сделать, вам придется написать MapReduce. И как вы видите,
[05:57.600 --> 06:03.880]  вот на входе у нас пары, они однородные. Видите, K1, V1. То есть должны быть однородные данные. А теперь
[06:03.880 --> 06:10.440]  мы смотрим, у нас есть простая операция join, где уже таблички две и данные у них разные. Что делать
[06:10.440 --> 06:17.440]  вот в такой схеме, если мы хотим реализовать join с разными данными? Я вам это объясню, будет больно.
[06:17.440 --> 06:30.000]  Ну и давайте теперь посмотрим немного на практике, как это работает на каких-то уже более
[06:30.000 --> 06:35.440]  смысленных задачах. И как обычно, если кто-то уже изучал Hadoop или MapReduce раньше, то вы, наверное,
[06:35.440 --> 06:41.760]  знаете, что самый первый пример, с которого начинают изучать MapReduce, где-либо это WordCount. То
[06:41.760 --> 06:48.920]  есть мы считаем, сколько раз слово встречается в тексте. И вот у нас есть такой текст, всего одна
[06:48.920 --> 06:55.400]  фраза. Давайте представим, что эту фразу мы взяли и размножили в 100-500 тысяч раз и получилось много
[06:55.400 --> 07:01.240]  терабайт. И вот будем с помощью MapReduce обрабатывать этот текст, который внезапно стал очень большим.
[07:01.240 --> 07:09.400]  Вот из того, что я показал на схеме раньше, подумайте, какой может быть мап здесь.
[07:09.400 --> 07:18.720]  На единицу мапим, да. То есть нам нужно взять, разбить данные по словам. И раз нам нужно понять,
[07:18.720 --> 07:23.640]  сколько раз каждое слово встречается, то это вот сколько этот подсчет мы будем делать на
[07:23.640 --> 07:28.920]  редьюсере. И наша задача для редьюсера данные подготовить. То есть на мапере мы их готовим,
[07:28.920 --> 07:36.280]  это всякий там препроцессинг делаем, всякие фильтры. На редьюсе мы считаем. Поэтому на мапе мы
[07:36.280 --> 07:44.920]  создадим пары типа слова единичка, потом их постартируем и потом посчитаем. Получится вот примерно
[07:44.920 --> 07:59.760]  такая схема. Теперь давайте посмотрим, как это делается в коде. Для этого я сейчас возьму пример,
[07:59.760 --> 08:07.000]  который у нас лежит на кластере. Это кусочек книги Tolkien на Hobbit. И мы попробуем на нем запустить
[08:07.000 --> 08:19.440]  мапперы и посмотреть, что будет. Сильмориль он тоже будет позже, просто если успеем дойти.
[08:19.440 --> 08:35.200]  Давайте возьму какой-нибудь материал.
[08:49.440 --> 09:00.880]  Вот у нас такой маленький кусочек текста. Как вы видите, он уже обработан. То есть тут нет ни знаков
[09:00.880 --> 09:08.400]  припинания, никаких скобочек, все почищено. И мы вот с помощью мапредьюса попробуем посчитать
[09:08.400 --> 09:19.240]  вордкаунт по этому тексту. Сначала мы будем делать маппер. Давайте посмотрим на код этого маппера и
[09:19.240 --> 09:28.960]  посмотрим, что он вообще делает. Мы это потом будем встраивать в Hadoop. Для начала, чтобы вы
[09:28.960 --> 09:35.920]  поняли примерно, что это такое, Hadoop это большой фреймворк, в котором можно писать на джаве,
[09:35.920 --> 09:41.520]  прямо используя API этого фреймворка. Но у нас тут, я так понимаю, джавист, вот один джавист,
[09:41.520 --> 09:52.840]  он точно здесь есть. А кто еще очень любит джаву? Поэтому раз никто, значит мы будем писать на
[09:52.840 --> 09:59.720]  питоне в основном. Питон в Hadoop работает так. Так как API питона появилось в Hadoop чуть позже,
[09:59.720 --> 10:05.920]  это по сути даже не API, а некоторый такой сервис, который называется Hadoop Streaming. Это значит,
[10:05.920 --> 10:12.600]  что нам как пользователям выдается готовая Hadoop программа, и мы в нее встраиваемся вот такими
[10:12.600 --> 10:18.320]  скриптами. То есть мы берем не свою программу и туда вскапливаем свои мапперы, редьюсеры,
[10:18.320 --> 10:25.600]  еще всякие другие элементы. И мы будем сразу писать вот этот код, который будет встраиваться в
[10:25.600 --> 10:30.480]  Hadoop. Как встроить в Hadoop какой-то код? На самом деле очень просто. Все что нужно,
[10:30.480 --> 10:35.480]  это чтобы этот код умел читать из консоли и писать тоже в консоль. То есть самые простые
[10:35.480 --> 10:40.480]  программки, которые вы все писали на первом курсе. И вот этот сервис, да сейчас отвечу,
[10:40.480 --> 10:45.520]  этот сервис Hadoop Streaming, он позволяет писать код на любом языке программирования. То есть теперь
[10:45.520 --> 10:51.600]  вам, чтобы писать на Hadoop, как вы позже увидите, вам не надо будет знать джаву. Хотя я бы очень
[10:51.800 --> 10:59.600]  советовал писать на Hadoop и на джаве. Он просто как бы будет понятнее. Но в целом под Hadoop можно
[10:59.600 --> 11:05.320]  писать на питоне, на перле я видел код, который на плюсах писали под Hadoop. Но единственное,
[11:05.320 --> 11:10.080]  что файл, который мы подаем в Hadoop, он должен быть исполняемый. Поэтому если это плюсы,
[11:10.080 --> 11:15.480]  то надо сначала скомпилировать и подать уже бинарник в Hadoop. Какой вопрос?
[11:15.480 --> 11:28.600]  Да, мы пишем на питоне. Ну, скажем так, Hadoop, он не очень быстро работает,
[11:28.600 --> 11:37.280]  но он может переваливать большие куски и большие объемы данных. То есть если сравнивать Hadoop с
[11:37.280 --> 11:44.600]  какими-нибудь другими системами, которые были до Hadoop, или с каким-нибудь там Pandas, с чем Hadoop
[11:44.600 --> 11:50.720]  часто сравнивают, то Pandas просто ограничен тем, что он не умеет работать на нескольких машинках
[11:50.720 --> 12:02.360]  одновременно, а Hadoop умеет. Ну, если люди плюсы не знают, что делать. В компании нету людей,
[12:02.360 --> 12:16.800]  которые знают плюсы. Это правда. Если мы говорим про большие петабайты данных,
[12:16.800 --> 12:22.760]  и вы постепенно начнете понимать, как работает Hadoop, на самом деле там очень много накладных
[12:22.760 --> 12:29.000]  расходов на вот это все распределение, балансировку, выделение ресурсов. И то,
[12:29.000 --> 12:33.280]  что мы напишем вот этот небольшой код на плюсах или на питоне, это не сильно важно.
[12:33.280 --> 12:46.880]  А в случае Java, как я уже говорил, Hadoop написан на Java, поэтому у него есть
[12:46.880 --> 12:53.320]  нативный API, и можно сразу писать на Java без использования стриминга. Правда я видел
[12:53.320 --> 12:59.600]  код, когда в стриминге пишут на Java. Отдельный вопрос, зачем так делать, но это инфраструктурой
[12:59.600 --> 13:05.960]  продиктовано было, когда поверх Hadoop делалась оболочка, которая работает на стриминге. Туда
[13:05.960 --> 13:12.080]  приходил человек, который очень любит Java, писал на Java и подсовывал туда Java файлы. Какие файлы
[13:12.080 --> 13:22.040]  подсовывать в стриминг? Bytecode, file.class. Давайте посмотрим, что наш маппер делает. Он читает
[13:22.040 --> 13:29.400]  с потока ввода stdin. Дальше мы получаем строчки, сплитим их по словам и подаем пары слов
[13:29.400 --> 13:34.960]  единичка. Все очень просто. Давайте посмотрим, как это работает. Пока что без Hadoop, а просто как
[13:34.960 --> 14:03.160]  это работает в консоли. Вот у нас файл. Вот мы запустили маппер. Мы просто передали данные
[14:03.160 --> 14:20.120]  через pipe и получили слово единичка. Вот этот код, который на прессе. Давайте еще побольше.
[14:20.120 --> 14:38.720]  А синий это просто путь. Вот у нас что получилось. Теперь давайте посмотрим на редьюсер. Если мы
[14:38.720 --> 14:44.600]  работаем в Hadoop, то у нас отработает маппер, после этого данные передадутся на стадию сорт,
[14:44.600 --> 14:52.600]  они там сгруппируются и после этого мы запустим вот этот редьюсер. А теперь вы сразу увидите
[14:52.600 --> 14:59.080]  минусы написания кода не на джаве. На семинаре вам покажут коды и на джаве, и не на джаве у вас
[14:59.080 --> 15:04.080]  будет возможность сравнить, но просто вот то, что я рассказывал в начале занятия. Редьюсер работает
[15:04.080 --> 15:10.560]  с группами. Вот есть группа, запускается редьюсер и ее обрабатывает. Тут мы не видим группы, тут мы
[15:10.560 --> 15:17.400]  видим, что у нас опять есть файл, который читает из stdin и начинает вот эти вот пары слова единичка
[15:17.400 --> 15:25.080]  последовательно обрабатывать. То есть нам выдан кусок данных и нам нужно самим отслеживать,
[15:25.080 --> 15:30.760]  где одна группа заканчивается, где начинается вторая. Кто может показать в коде, в каком месте мы
[15:30.760 --> 15:42.280]  этим занимаемся. Не, можно на плюсах. Вот между разницей между написанием на питоне и написанием
[15:42.280 --> 15:49.240]  на плюсах никакой нету. Вы можете домашние коды на семинарах делать на плюсах. Вообще не проблема.
[15:49.240 --> 16:03.480]  Мы сможем проверить, инфраструктура сможет съесть и джаву, и питон, и плюсы. На джаве оно как-то
[16:03.480 --> 16:10.440]  понятнее для тех, кто джаву уже знает. Лично мой опыт. А для тех, кто джавой не хочет заниматься,
[16:10.440 --> 16:18.200]  там уже нет разницы питон, плюсы или что-то еще. Кто может сказать в каком месте мы занимаемся тем,
[16:18.200 --> 16:33.600]  что отслеживаем изменения группы. Где трай? А где там изменения группы? Там континью. Где трай,
[16:33.600 --> 16:39.160]  что мы делаем? Мы считаем строчку, проверяем, что ключи значения валидные. Если они у нас не
[16:39.160 --> 17:03.640]  валидные, значит мы просто пропускаем эту строчку. Какой алгоритм у нас идет? На вход
[17:03.640 --> 17:11.320]  приходят пары слова единичка, мы их получаем и накапливаем счетчик. Вот то, что там видите,
[17:11.320 --> 17:22.760]  word sum равно нулю. И мы вот этот word sum накапливаем. Потом в какой-то момент у нас изменяется ключ,
[17:22.760 --> 17:29.640]  то есть было слово ходуб, ходуб, ходуб, ходуб. Слова ходуб приходили, после этого пришло слово джава.
[17:29.640 --> 17:38.480]  Это значит, что у нас закончилась группа и мы начали работать с другой группой. Мы не сортировали,
[17:38.480 --> 17:49.840]  это ходуб отсортировал. Если мы вот эти коды подаем в ходуб, то у нас есть... Да, ходуб сам
[17:49.840 --> 18:06.640]  сортирует между маппером и редьюсером. Сейчас мы просто не подключили ходуб, мы запустили маппер
[18:06.640 --> 18:16.880]  без ходуба. То есть мы чуть позже научимся, как это встроить в ходуб. Сейчас мы просто рассматриваем
[18:16.880 --> 18:22.120]  этот код, запускаем, убеждаемся, что он работает, а следующий шаг мы научимся запускать это дело в ходубе.
[18:22.120 --> 18:35.800]  Да, мы не подключили ходуб, поэтому мы делаем вот так.
[18:46.880 --> 19:00.440]  Команда sort? Я вас еще больше обрадую или расстрою, есть команда board-counter,
[19:00.440 --> 19:09.160]  называется wc. Да, через wc можно было подать, но в какой-то момент у нас бы просто оперативка
[19:09.160 --> 19:15.920]  кончилась, если бы здесь был не маленький кусочек текста, а вся википедия, которую мы
[19:15.920 --> 19:21.560]  выгрузили просто и сохранили где-то в hdfs. Слов мало, нам-то надо все это прочитать,
[19:21.560 --> 19:35.680]  нам надо все это прогнать через маппер и потом через редьюсер. Ну конечно. Да, ну там как бы мы
[19:35.680 --> 19:43.360]  через через pipe работаем, поэтому вариантов нет. Ну опять же ты можешь написать код, который будет
[19:43.360 --> 19:48.040]  сохранять это файл периодически, но это уже надо код писать. Зачем его писать, если у нас для этого
[19:48.040 --> 19:56.800]  есть ходуб. Вот, поэтому вот с таким выводом мы работаем и дальше на этом вот деле запускаем редьюсер.
[20:06.680 --> 20:17.080]  Ну да, конечно. Ну опять же, как ты напишешь, но факт остается фактом, тебе нужно весь аутпут
[20:17.080 --> 20:28.400]  где-то сохранить. Нет, тебе все равно нужно будет его хранить в оперативке, это может быть какой-то
[20:28.400 --> 20:33.760]  буфер, там что-то еще, но на диск у тебя не будет ничего писаться само без твоего участия.
[20:33.760 --> 20:52.240]  То есть ты хочешь подключиться к этому буферу и как-то его читать по частям. Ну окей, а если у тебя
[20:52.240 --> 21:00.640]  просто диска не будет хватать, что ты тогда будешь делать? Окей, поэтому давайте посмотрим.
[21:00.640 --> 21:07.920]  Действительно, вот здесь, если мы посмотрим на вот эти ифы, то в какой-то момент группа у нас
[21:07.920 --> 21:13.320]  меняется. Что мы делаем, когда изменилась группа? Мы обнуляем счетчик, мы выводим результат,
[21:13.320 --> 21:18.760]  который у нас был принтом и переходим к следующей группе. И вот так по циклу мы обрабатываем
[21:18.760 --> 21:24.800]  все данные, пока они не закончатся. Вопрос, а зачем нужно что-то после цикла? Вот видите, цикл 4
[21:24.800 --> 21:31.560]  закончился, и вот этот вот в конце две строчки, кусочек этот. Последнюю группу так-то выведет,
[21:31.560 --> 21:41.560]  в плане у вас же триер сейчас на то, чтобы выводить результаты? Изменение, да. А для последней такой? Все
[21:41.560 --> 21:47.840]  правильно, то есть если мы... ну понятно, что последнюю группу мы не успели отследить, когда она
[21:47.840 --> 21:53.600]  поменялась, потому что она не поменялась. Она выводилась, и потом данные кончились. Поэтому нам
[21:53.600 --> 22:01.600]  надо последнюю группу вывести вот этим последним IF. Теперь давайте соберем наш пазл и запустим
[22:01.600 --> 22:21.320]  MapReduce. Все еще без Hadoop, она уже целиком. Вот, редьюсер добавили. Давайте сделаем head 10 строчек.
[22:21.320 --> 22:27.160]  Что у нас получилось? Ну пока ничего не понятно. Какие-то слова и какие-то цифры. Давайте отсортируем.
[22:27.160 --> 22:35.400]  И сортировать мы будем теперь не по первому полю, а по второму, потому что нам нужно... ну если мы
[22:35.400 --> 22:41.000]  считаем в WorldCount, нам, наверное, интереснее всего понять, какие слова встретились чаще всего. Не
[22:41.000 --> 22:48.960]  просто вывести какие-то рандомные слова и их каунты. Поэтому сортируем по второму ключу. В обратном
[22:48.960 --> 23:01.360]  порядке... Чего? Ну можно и так, в принципе. Я просто привык работать с sort с ключиками, потому что в Hadoop
[23:01.360 --> 23:11.960]  тоже есть sort с ключиками. Там свой есть sort, в котором сделали такие же ключики. Давайте уберу head.
[23:18.960 --> 23:33.080]  Ожидаемо, что у нас чаще всего встречаются все предлоги, союзы и прочие стоп-слова. Если немножко
[23:33.080 --> 23:42.920]  увеличить наш head, то мы увидим, что дальше после стоп-слова идут какие-то осмысленные слова,
[23:42.920 --> 23:49.000]  что там болото вышли, вылезли, что-то такое. В общем, мы видим, что книга, видимо, какая-то про
[23:49.000 --> 23:53.920]  путешествие. Вот мы даже не читая книгу, определили, о чем она примерно говорит.
[23:53.920 --> 24:17.320]  Вот такая у нас получилась штука. И кажется, что никакой Hadoop нам не нужен. Мы можем вот такие
[24:17.320 --> 24:23.440]  pipeline писать, и все будет хорошо. Но у нас в какой-то момент кончится оперативка. Если мы даже
[24:23.440 --> 24:27.960]  с этим как-то справимся, у нас в какой-то момент кончится диск на машинке. Придется параллелить.
[24:27.960 --> 24:32.840]  Можно писать на MPI и параллелить руками. Вы уже все почувствовали, что такое MPI,
[24:32.840 --> 24:38.080]  как можно параллелить процессор руками. И, наверное, не очень хочется это делать.
[24:38.080 --> 24:45.200]  Почему? Тебе нравится MPI?
[24:45.200 --> 24:56.480]  Книжка ничего не занимает, а библиотека книжек уже занимает.
[24:56.480 --> 25:10.400]  Поэтому нам придется все-таки использовать Hadoop и какую-то вот такую схему придумывать.
[25:10.400 --> 25:17.720]  Давайте посмотрим, как она работает, как устроена обработка данных уже в Hadoop. На вход у нас
[25:17.720 --> 25:25.520]  подаются теперь данные не с файла, а с блоков. Вот у нас блоки серые с левой стороны. Дальше
[25:25.520 --> 25:31.800]  запускается маппер, и каждый блок независимо обрабатывается своими мапперами. Причем могут
[25:31.800 --> 25:37.640]  быть даже несколько мапперов. После этого запускается стадия Shuffle and Sort. Данные передаются,
[25:37.640 --> 25:42.920]  пока не будем особо разбираться, как они внутри передаются, так чтобы на редюсер они пришли
[25:42.920 --> 25:51.920]  с одинаковыми ключами, но на редюсер они приходят с одинаковыми ключами. Мы видим K1, K2, K3. Три
[25:51.920 --> 25:59.600]  блока ключей и два редюсера. Тут важно понимать, что у нас никогда не может быть такого, чтобы один
[25:59.600 --> 26:07.080]  и тот же ключ попал на редюсер 1 и на редюсер 3, например. Все одинаковые ключи будут на
[26:07.080 --> 26:16.480]  одном редюсере находиться. Ну и дальше запускается редюсер, и выдается результат уже в HDFS.
[26:16.480 --> 26:26.960]  Если какого-то ключа очень много, то это все плохо? Все плохо, мы упадем. Если половина ключей, то это один и тот же ключ?
[26:26.960 --> 26:33.440]  Это называется несбалансированными данными, когда у нас действительно по какому-то ключу перекос.
[26:33.440 --> 26:45.960]  Допустим, мы можем какую-нибудь предметную область взять, например, анализ зарплаты IT-специалистов
[26:45.960 --> 26:53.480]  по городам России. Где-то в топе будет Москва, Питер, то есть это большие блоки с большим
[26:53.480 --> 26:58.960]  количеством данных по ключам. И такие данные называются несбалансированными, когда у нас процентов
[26:58.960 --> 27:08.200]  70-80 находятся на одном ключе. Тогда это все приходит на один редюсер, не помещается, и мы падаем.
[27:08.200 --> 27:19.960]  Что в этом случае делать, как вы думаете? Не так. Это от нас не зависит, то есть Hadoop
[27:19.960 --> 27:26.400]  группирует данные по ключам сам. Мы написали маппер, идеально хорошо оптимизированный редюсер тоже,
[27:26.400 --> 27:31.040]  но для того, чтобы работал редюсер, надо, чтобы на него подалась группа с одинаковыми ключами.
[27:31.040 --> 27:51.920]  А как мы это сделаем, если у нас... Что такое в двух местах? Двух местах это значит вообще
[27:51.920 --> 27:56.480]  на разных серверах где-нибудь. На этих разных серверах запускаются редюсеры.
[27:56.480 --> 28:08.880]  Да не в этом дело. Дело в том, что у нас редюсер это один шаг. То есть если у нас данные хранятся
[28:08.880 --> 28:14.840]  на разных серверах с одинаковыми ключами, мы их там поредюсим, а потом нам их надо собрать вместе.
[28:14.840 --> 28:24.800]  Как мы это сделаем? У нас кончилась стадия редюс. Для того, чтобы что-то еще доделать после
[28:24.800 --> 28:31.720]  стадии редюс, нам надо запускать новый мапредюс. То есть на самом деле, если мы посмотрим на реальные
[28:31.720 --> 28:37.440]  всякие программы, которые в компаниях пишут на Hadoop, на Spark, то там будет целая цепочка.
[28:37.440 --> 28:42.520]  И вот один этот мапредюс, который мы сейчас разбираем, это только вершинка графа. Этот граф
[28:42.600 --> 28:49.480]  отчисления может быть очень большой на десятки-сотни таких мапредюсов. Но каждый новый мапредюс это
[28:49.480 --> 28:55.840]  большие затраты по ресурсам. Потому что куда мы пишем данные после того, как редюс закончился? На
[28:55.840 --> 29:03.480]  диск. Маппер должен с этого диска считать. Потом, когда мы данные сортируем и группируем, это все
[29:03.480 --> 29:08.920]  опять происходит через диск, как вы увидите. То есть мы постоянно работаем с диском. Плюс нам нужны
[29:08.920 --> 29:15.680]  постоянно ресурсы на то, чтобы поддерживать джаву и все, что с ней связано, всякие там джава хип и прочий
[29:15.680 --> 29:25.040]  горбач коллектор. Поэтому лишнюю джобу все-таки делать не хочется, лишний мапредюс. Поэтому делаем
[29:25.040 --> 29:31.480]  так, чтобы мы могли за один такт средюсить данные с одинаковыми ключами. Итак, если у нас все-таки не
[29:31.480 --> 29:38.880]  хватает памяти и ресурсов редюсера, чтобы этот ключ съесть, что можно сделать?
[29:38.880 --> 29:50.160]  То есть как-то его дополнить чем-то. Да, вот это и называется подсаливание, когда мы берем ключ и
[29:50.160 --> 29:57.800]  добавляем к нему, например, какую-то рандомную соль. У нас будет K1-1, K1-2 и ходу будет думать,
[29:57.800 --> 30:03.480]  что это разные ключи. Можно сделать это как-то более умным. Если вот я говорил про Москву,
[30:03.480 --> 30:09.480]  то можно Москву разбить на регионы какие-нибудь и добавить еще это к ключу.
[30:09.480 --> 30:23.000]  Такую проблему как-будто можно увидеть только если глазами. Запустить, увидеть, что все сломалось,
[30:23.000 --> 30:26.760]  потому что данные не сбалансированы, понять, что надо что-то там...
[30:26.760 --> 30:35.880]  Ну, это задача дата-инженера. Просто увидеть это заранее можно всякими иллюристическими методами,
[30:35.880 --> 30:41.280]  вплоть до того, что ты понимаешь, что у тебя предметная область такая, что, наверное, в основном
[30:41.280 --> 30:47.960]  там будет Москва присутствовать. У тебя будет 80% данных, ключ равно Москва. Значит, мы эту Москву как-то
[30:47.960 --> 31:08.120]  дробим еще. Я думаю, что такого фреймворка, который вообще всегда будет работать стабильно,
[31:08.120 --> 31:15.120]  вы, наверное, даже не найдете. То есть вот если брать так поверхностно самые известные такие
[31:15.120 --> 31:20.880]  фреймворки, то вот у ходу побывает проблема с ключами и с тем, что он в целом не очень быстро работает.
[31:20.880 --> 31:26.960]  У Spark'а бывает проблема с тем, что он течет по памяти и не хватает оперативки, начинает падать.
[31:26.960 --> 31:32.160]  У Hive'а бывает проблема с тем, что он не очень оптимально превращает SQL в MapReduce,
[31:32.160 --> 31:36.960]  но мы это все тоже будем разбирать. Поэтому у каждого такого фреймворка есть проблемы,
[31:36.960 --> 31:41.080]  надо просто знать особенности и как-то с ними справляться.
[31:41.080 --> 31:53.520]  Вот здесь еще справа на слайде написано блок неравного блоку в HDFS. Что это значит?
[31:53.520 --> 32:00.120]  Вот давайте вспомним, как HDFS разбивает данные по блоку, на что он обращает внимание при разбиении.
[32:00.120 --> 32:17.680]  Ни на что. Все молчат, потому что ни на что, только на объем. Да, конец оптимизировать,
[32:17.680 --> 32:21.960]  но тоже потому что там объем, мы не хотим хранить маленький блок. То есть мы смотрим
[32:21.960 --> 32:27.400]  только на количество байт и больше ни на что. А теперь у нас каждый блок идет на один какой-то
[32:27.400 --> 32:33.080]  маппер отдельный. Давайте представим, что мы обрабатываем музыку или видео. Все, наверное,
[32:33.080 --> 32:40.480]  сталкивались с таким, что вы записываете какое-то видео и у вас в конце там запись прерывается,
[32:40.480 --> 32:46.640]  вы последнюю секунду не успели записать, но из-за этого весь файл, который у вас может быть 2 гига,
[32:46.640 --> 32:55.120]  видео не читается. Сталкивались с таким? Те, кто снимает и монтирует, я думаю, точно сталкивались.
[32:55.120 --> 33:02.800]  Поэтому если мы будем класть файлы в HDFS, разбивать их с точностью до объема, то окажется,
[33:02.800 --> 33:08.520]  что у нас вот это разбиение, оно с концом файла совпадать не будет. И вот мы первый блок считали,
[33:08.520 --> 33:15.040]  а там ни начала ни конца файла нет. Мы его не можем вообще никак, ну ничего с ним сделать,
[33:15.040 --> 33:21.400]  с этим куском, который мы прочитали. С другим блоком точно так же. Что можно сделать в такой
[33:21.400 --> 33:30.640]  ситуации, как вы думаете? То есть когда мы файлы кладли в HDFS и потом читали, просто HDFS,
[33:30.640 --> 33:37.320]  DFS-GET, скачали файл, было все хорошо, потому что при чтении мы все блоки считали, они собрались
[33:37.320 --> 33:43.200]  опять в кучку и у нас получился целый файл, как он был. А здесь так не получится, здесь каждый блок
[33:43.200 --> 33:49.040]  передается на один отдельный маппер, они находятся в разных процессах, на разных машинках,
[33:49.040 --> 33:54.200]  когда они, ну они уже никогда не встретятся, эти кусочки файлов. Нам придется их обрабатывать
[33:54.200 --> 34:11.440]  независимо. Что можно в такой ситуации сделать? Мы возьмем файл, добавим чисто какой-то мишанины
[34:11.440 --> 34:17.880]  в конец, и за счет этого будет чуть больше. А если у нас слишком маленький блок, то есть файл больше,
[34:17.880 --> 34:32.280]  чем блок, что тогда делать? Давайте очень много данных в конец. Нам не хватает примерно вот этого,
[34:32.280 --> 34:49.680]  что у нас есть. Вот смотрите, вы сохранили в HDFS наши лекции по курсу. Вот у нас тут первая,
[34:49.680 --> 34:58.080]  вот у нас тут вторая, вот у нас тут третья, вот вы это сохранили в HDFS. А HDFS у нас бьет данные по
[34:58.080 --> 35:05.360]  блокам. И здесь он разбил вот так, здесь вот так, здесь вот так, и вот как-то вот это не совпало.
[35:05.360 --> 35:16.800]  Что делать? Мы понесли вот этот блок на маппер, а считать мы это дело не можем, потому что оно не
[35:16.800 --> 35:22.520]  закончилось. Здесь вообще все плохо, мы получается вот этот кусочек не можем прочитать, и вот этот
[35:22.520 --> 35:44.600]  кусочек не можем прочитать. Да, у нас будет много маленьких блоков, потому что вот эта штука,
[35:44.600 --> 36:12.920]  это будет отдельный блок. Вот это тоже. Это если файлик меньше, чем блок, а если больше?
[36:14.600 --> 36:25.800]  Мы можем, но это придется делать руками, то есть не хочется руками делать. Вот выше вас сидит человек,
[36:25.800 --> 36:35.320]  которому не нравится руками делать. Маппер берет на вход какие-то данные, независимо обрабатывает
[36:35.320 --> 36:52.480]  их и передает дальше. Ну грубо говоря, блоками HDFS, 64-128 мегабайт, не килобайт. В ходу есть такая
[36:52.480 --> 37:03.560]  штука, как вот в скобочках написано, называется она Split. То есть вот главная мысль то, что разделение
[37:03.560 --> 37:09.840]  не должно испортить формат, чтобы мы эти файлы вообще могли прочитать как-то. Поэтому когда мы с
[37:09.840 --> 37:20.800]  помощью маппера читаем данные из HDFS, мы обращаем внимание на формат данных. То есть мы читаем не
[37:20.800 --> 37:26.880]  блок, а мы читаем Split. И в ходу можно специальным образом с помощью специальных аргументов подать
[37:26.880 --> 37:35.400]  формат данных, с которым мы работаем. Это может быть видео какое-нибудь, архивы. Если брать некоторые
[37:35.400 --> 37:41.600]  архивы, то там тоже структура данных блочная. И если мы вот этот блок пополам разрубим, то мы его не
[37:41.600 --> 37:46.880]  сможем прочитать ни на одном маппере, ни на втором. И нам надо вот эту границу сдвигать. Видите,
[37:46.880 --> 37:52.160]  такую вот схемку, типа того, что я на доске нарисовал. Внизу у нас блок boundary, то есть то, как HDFS
[37:52.160 --> 37:56.360]  разбьет данные по блокам. А вверху у нас сплиты.
[37:56.360 --> 38:20.600]  Смотрите, мы не можем разбивать на блоке как хотим, мы можем задать размер с плита. Размер блока мы не
[38:20.600 --> 38:25.800]  можем с помощью MapReduce изменить вообще, мы можем задать размер с плита. Что в этом случае будет
[38:25.800 --> 38:42.280]  делать мап? Размер плита это тот объем данных, с которым будет работать маппер. То есть блок
[38:42.280 --> 38:50.600]  плюс еще какой-то кусочек или минус какой-то кусочек. И в этом случае можно будет подстроиться под
[38:50.600 --> 39:06.760]  вот эту вот границу. То есть вот смотрите, у нас какой-то вот архив здесь, TAR BZ2 или что-нибудь еще,
[39:06.760 --> 39:14.000]  и нам нужно обязательно эти маленькие блоки синие не порезать. И вот сплиты позволяют эти блоки не
[39:14.000 --> 39:27.000]  порезать. Если будет что-то очень большое, то мы сможем размер сплита задать больше,
[39:27.000 --> 39:35.880]  если там уже мы упремся в какой-нибудь лимит, тогда придется все-таки руками данные процестить. То
[39:35.880 --> 39:43.040]  есть придется упаковывать их в пакеты нужного нам размера. В принципе, это умеют архиваторы,
[39:43.040 --> 39:49.960]  я знаю, что всякие архиваторы еще лет 15 назад имели такую функцию, что можно было большой архив
[39:49.960 --> 40:07.400]  разбить на какие-то пакеты, которые потом можно было восстановить. Здесь какие-то вопросы есть,
[40:07.400 --> 40:12.760]  просто дальше уже будем разбирать с вами, как работает сорт между мапом и редьюсом.
[40:29.480 --> 40:34.960]  Теперь давайте посмотрим, как мап редьюс технически работает более подробно. В конце я вам покажу
[40:34.960 --> 40:44.840]  схему, если успеем, там она еще более страшная, то есть как сам ходу работает внутри. И вот получается,
[40:44.840 --> 40:52.000]  что мы считали input-split, это вот тот самый блок с поправкой на формат файла. Мы его считали,
[40:52.000 --> 40:59.520]  добавили в маппер, маппер его обработал и данные сложил куда? В буфер. Но буфер может переполниться.
[40:59.520 --> 41:07.040]  Что мы тогда делаем? Тогда мы сбрасываем данные на диск. И вот если вы посмотрите на четвертый вот
[41:07.040 --> 41:12.680]  этот элемент в схеме, то вы увидите много кусочков данных на диске. То есть маппер работает, пишет
[41:12.680 --> 41:17.320]  данные в буфер, буфер переполняется, пишется на диск. Снова буфер переполняется, пишется на диск.
[41:17.320 --> 41:28.160]  И так происходит много раз. Дальше мы вот эти маленькие кусочки слопываем в один и одновременно мы уже
[41:28.160 --> 41:37.000]  здесь начинаем стадию сорта, то есть мы проводим группировку. Вот до этого момента какие-то вопросы
[41:37.000 --> 41:55.840]  есть? Или пока ничего не понятно? А вам? А какой вопрос есть?
[41:58.160 --> 42:07.360]  Ну да, вообще понять зачем и что там происходит.
[42:07.360 --> 42:23.760]  То есть самое главное то, что так как маппер мы пишем сами, то Hadoop никак предсказать не может,
[42:23.760 --> 42:30.680]  что у нас из этого маппера в итоге выйдет, какого размера будут данные. То есть на вход
[42:30.680 --> 42:36.360]  мы подали один сплит, его размер мы знаем и предсказать можем. Что будет на выходе мы не знаем,
[42:36.360 --> 42:44.760]  поэтому нам надо пойду смотреть механизм, сохранять данные сколько бы их не было. Поэтому вот буфер,
[42:44.760 --> 42:51.000]  который переполняется и сбрасывается на диск, и в итоге мы получаем вот такой вот файл. Дальше у нас
[42:51.000 --> 42:59.320]  запускается такая штука как partitioner. Это еще один процесс, который занимается распределением
[42:59.320 --> 43:10.840]  вот этих вот данных по юсерам. То есть вот вы видите один файл, в котором уже данные сгруппированы
[43:10.840 --> 43:16.760]  по ключам. Дальше мы запускаем partitioner, который по умолчанию работает вот с такой формулой.
[43:21.000 --> 43:33.680]  Вот, то есть берется хэш от ключа и делим по модулю на количество редьюсеров. В ответе получаем
[43:33.680 --> 43:40.680]  число и это число будет индекс редьюсера, то есть индекс той машинки, куда мы данные отправим. Вот
[43:40.680 --> 43:48.960]  вы видите стрелочки, красные, зеленые. Что это значит? Это значит, что вот этот кусочек пошел на первый
[43:48.960 --> 43:56.120]  редьюсер, плюс на первый редьюсер пошли данные еще откуда-то извне. И точно так же вот эта первая
[43:56.120 --> 44:07.080]  машинка отправила данные куда-то еще в другой редьюсер. Это уже следующий этап, потому что вот
[44:07.080 --> 44:14.040]  мы эти все данные отправили на машинке, и они на машинке хранятся в виде чего? В виде маленьких
[44:14.040 --> 44:20.320]  файлов. И нам перед тем, как с ними работать дальше, их надо собрать вместе. Поэтому вот merge.
[44:20.320 --> 44:35.040]  Ну так да, вот то, что слева мы с вами только что проговаривали, что когда у нас буфер переполняется,
[44:35.040 --> 44:39.400]  мы скидываем данные на диск, и мы это делаем много раз, у нас получаются маленькие файлы.
[44:39.400 --> 44:46.440]  Потом нам нужно их собрать в один, попутно сгруппировать. После этого мы их разносим по
[44:46.440 --> 44:51.400]  разным редьюсерам, стрелочками, и у нас ситуация повторяется. Опять маленькие файлы, которые опять
[44:51.400 --> 45:04.120]  нужно собрать вместе. В виде файлов на диске. В виде просто файлов на диске.
[45:04.120 --> 45:12.880]  Другой файл. В каком виде, что именно под видом понимается? Формат какой или что?
[45:12.880 --> 45:26.240]  Что изменилось после мапа? Данные прошли через маппер. Например, мы считали строчки и сделали пары
[45:26.240 --> 45:29.680]  слова единичка. То есть преобразование какое-то получилось. Данные другие.
[45:29.680 --> 45:42.680]  То есть у тебя функция изменилась к каждому элементарному объекту, а потом все что дальше мы отскидываем правильно по разному.
[45:42.680 --> 45:54.840]  Ну условно мы на вход подаем массив какой-то, и считаем в нем максимум минимум, получается пара
[45:54.840 --> 46:00.680]  максимум минимум. Вот уже другие данные. Все что угодно. Мы маппер и редьюсер пишем сами,
[46:00.680 --> 46:12.920]  поэтому там может быть все что хочешь. Конечно. Тоже забегая немного вперед, у нас есть распорядитель
[46:12.920 --> 46:20.400]  ресурсов, который называется Ярн. Он выделяет контейнер. Это определенный набор ядер процессоров.
[46:20.400 --> 46:32.200]  Ярн контейнер это набор ресурсов, на которых крутится Java. И это не очень
[46:32.200 --> 46:37.480]  большой набор ресурсов. На любом сервере этих контейнеров может быть несколько десятков.
[46:37.480 --> 46:58.040]  Очень много. Это сколько, например? Ну вот объем одного ярн контейнера это где-то там 2-3-5 гигов.
[46:58.040 --> 47:04.440]  То есть вот примерно такой набор. На машинке может быть например 250 гигов оперативки, а в контейнере
[47:04.440 --> 47:16.520]  может быть 5 гигов оперативки. Да, у нас на кластере это 1-1,5-2 где-то так. В жизни бывает 5-10.
[47:16.520 --> 47:28.800]  Мы склеиваем в несколько файликов, как ты видишь два файла.
[47:28.800 --> 47:50.760]  Один ключ. Несколько разных ключей в одном файле. В принципе это может быть, но нам даже не надо
[47:50.760 --> 48:04.480]  так глубоко залазить, потому что мы работаем с директориями. Если пишем на Java, то да. Если пишем на
[48:04.480 --> 48:11.160]  Python, то надо будет. Мы будем считать, что нам выдали некую группу данных, и данные эти отсортированы.
[48:11.160 --> 48:17.800]  То есть ключ приходит к 1, к 1, к 1. Если пришел к 2, то к 1 больше не встретится. Вот такая вот штука.
[48:20.760 --> 48:38.520]  Почему? Если они отсортированы по хэшу, то хэш для одинаковых ключей выдаст одинаковое значение.
[48:38.520 --> 48:55.080]  Зачем проверить? Ты к тому, что в ходу может быть плохой хэш? Плохая хэш-функция? У вас какие-то
[48:55.080 --> 49:08.720]  вопросы? Давайте это после пары все-таки. Вот, можете посмотреть. Вот у нас данные были в буфере,
[49:08.720 --> 49:19.040]  потом попали сюда, и мы их собрали здесь, в итоговом файле. И вот как работает partitioner.
[49:19.040 --> 49:27.600]  Тоже. Вот у нас есть несколько кусочков. Мы их проводим через функцию вот эту хэш от K,
[49:27.600 --> 49:45.960]  и получаем уже сгруппированные данные на reducer. Теперь давайте посмотрим, что делает ходуб,
[49:45.960 --> 49:51.560]  если что-то падает, и как он вообще с точки зрения отказаустойчивости себя ведет. И тут надо
[49:51.560 --> 49:58.240]  сначала договориться о терминах. Если мы имеем дело с большой программой, в которой много job,
[49:58.240 --> 50:05.200]  друг за другом идущих, то это называется application. Собственно, один такт от начала мапа до конца
[50:05.200 --> 50:14.080]  reducer это называется job. Один синий квадратик, один mapper или один reducer называется task.
[50:14.080 --> 50:20.600]  Если будете читать русскоязычную литературу, там job и task переводят странно и перемешивают
[50:20.600 --> 50:29.240]  между собой, поэтому вот еще одна причина, чтобы ходуб читать на английском. И у task может быть
[50:29.240 --> 50:35.760]  попытка, может быть несколько попыток, несколько отемтов. Давайте посмотрим, что происходит,
[50:35.760 --> 50:42.480]  если происходят отказы, и вообще какие могут быть в ходубе отказы. Что может произойти такого,
[50:42.640 --> 50:51.680]  чтобы случился отказ? Ну, name-надо упало, и это все сразу ничего работать не будет. Я имею
[50:51.680 --> 50:57.920]  в виду отказ на стороне вот этих задачек, что может произойти. Переполнились по памяти,
[50:57.920 --> 51:12.240]  закончился диск, отвалилась дата нода. Обработчик, ну код некорректный просто. Мы написали плохой код,
[51:12.240 --> 51:29.360]  но это в итоге тоже какая-то часть нод просто отвалилась. Ну вот что происходит,
[51:29.360 --> 51:35.280]  если у нас отвалилась какая-то нода одна или какая-то часть нод. Каждая нода, она шлет
[51:35.280 --> 51:40.840]  хардбиты в name-ноду, и если хардбиты мы не получаем какое-то время, то мы считаем,
[51:40.840 --> 51:45.560]  что эта нода упала и начинаем пересчитывать на другой. То есть мы посылаем следующую попытку.
[51:45.560 --> 51:54.720]  Благо у нас есть репликация, то есть мы стараемся запускать мапперы там, где лежат данные. Вот лежат
[51:54.720 --> 52:01.120]  данные на ноде номер один, мы запускаем там маппер. Если нода номер один упала, то мы запускаем
[52:01.120 --> 52:06.040]  маппер на ноде номер десять, где лежит реплика этих же данных, которые надо обработать.
[52:06.040 --> 52:21.760]  Да нет, потому что данные лежат на диске, а мапперы работают в оперативке.
[52:21.760 --> 52:31.480]  Опять же подходы есть разные. Сейчас есть разные подходы к облачным вычислением,
[52:31.480 --> 52:38.520]  когда у нас эти сервисы разделяются, и отдельно у нас компьют, отдельно у нас дата, дата ноды.
[52:38.520 --> 52:44.440]  Вот можно посмотреть там Яндекс.Облако, у них есть такой сервис, который называется Яндекс.Датапрок.
[52:44.440 --> 52:50.440]  У них отдельно можно сделать дата ноды, отдельно компьют ноды. С одной стороны это хорошо, что никто
[52:50.440 --> 52:55.480]  ни у кого ничего не отжирает, с другой стороны это сеть. То есть теперь нам нужно данные по сети
[52:55.480 --> 53:10.280]  передавать. Мы все-таки пишем на диск, это правда, и здесь нужно правильно настроить сам ходу бкластер,
[53:10.280 --> 53:16.680]  чтобы на дата ноде, если там еще могут быть мапперы, то на дата ноде мы не давали возможность
[53:16.680 --> 53:25.960]  занять все 100% ресурсов. Обычно вот как у нас настроен кластер, например, если 85% занято,
[53:25.960 --> 53:34.800]  он начинает уже там пинговать, ципать аллерты, что что-то не так. Поэтому перезапускаются попытки,
[53:34.800 --> 53:41.800]  и вот вы правильно сказали, что сбоем можно считать и то, что наш код упал. Хадуб, он вообще это не
[53:41.800 --> 53:49.960]  разделяет. То есть если код, который мы запустили, ответа не выдает, то хадубу все равно,
[53:49.960 --> 53:55.800]  это нода упала или это код упал, он просто думает, что проблема на его стороне и начинает перезапускать.
[53:55.800 --> 54:00.240]  Поэтому когда вы будете в домашке писать неправильный код, вы будете наблюдать то,
[54:00.240 --> 54:06.000]  что его запускает хадуб несколько раз на разных нодах. В случае нашего кластера это четыре.
[54:06.000 --> 54:09.320]  Четыре раза запустились, получили ошибку, упали.
[54:09.320 --> 54:22.240]  У нас храбрая программа, она может что угодно делать. В теории она может работать слишком долго.
[54:22.240 --> 54:22.880]  Да.
[54:22.880 --> 54:30.360]  Но можно же вести какое-то адекватное ограничение, что на каком-то дреме дамы программа должна работать слишком долго.
[54:30.360 --> 54:34.400]  Не должна работать слишком долго? Да, такое ограничение.
[54:34.400 --> 54:37.840]  Есть ли в ходу перепланировки сетев?
[54:37.840 --> 54:44.480]  Конечно есть. Причем и по времени, и по ресурсам. Когда мы будем ярн разбирать,
[54:44.480 --> 54:50.640]  вам на семинарах более подробно расскажут, но в целом вот у нас есть ярн-контейнер.
[54:50.640 --> 54:56.840]  Когда нам не хватает ресурсов, мы идем к главному ресурс-менеджеру и говорим,
[54:56.840 --> 55:01.880]  мы хотим еще ресурсы. Ресурс-менеджер нам дает еще ресурсы, дает, дает, дает.
[55:01.880 --> 55:07.480]  Какое-то время контейнер растет, после этого ресурс-менеджер говорит все, и контейнер убивается.
[55:11.480 --> 55:18.560]  Точно так же, если он у нас работает очень долго, не шлет хорбиты, то вылазит ошибка,
[55:18.560 --> 55:24.440]  типа weight-output-threads, то есть мы не дождались никакого ответа, и контейнер тоже убивается.
[55:24.440 --> 55:28.680]  Что значит не шлет хорбиты? В какой момент шлет?
[55:28.680 --> 55:39.640]  Хорбиты шлются, то есть внутри ходупа есть такой специальный процесс, который считает,
[55:39.640 --> 55:47.320]  сколько данных мы прочитали, сколько данных мы записали. И если у нас в какое-то время перестает
[55:47.320 --> 55:51.480]  меняться количество прочитанных и записанных данных, то хорбитов нет,
[55:51.480 --> 55:54.640]  но и что-то пошло не так, и ходуп понимает, что, наверное, все зависло.
[55:54.640 --> 56:02.600]  Он начинает перезапускать на других нодах, и на самом деле это помогает иногда. Почему?
[56:02.600 --> 56:08.200]  Потому что может быть такое, что наш код, например, использует библиотеку Panda с какой-нибудь
[56:08.200 --> 56:13.480]  специфической версией, а не очень добросовестный админ поставил эту библиотеку только на некоторых
[56:13.480 --> 56:19.440]  нодах, на остальных обновить не успел, и вот мы перезапустились и где-то рандомно
[56:19.440 --> 56:22.160]  наскочили на правильную библиотеку и, возможно, отработали.
[56:22.160 --> 56:38.040]  Он сначала бьет на блоки, потом разделяет по нодам. То есть когда мы перезапускаем код на другой
[56:38.040 --> 56:46.480]  ноде, мы перезапускаем там, где будет точная копия этого блока. Ну сплиты тоже, соответственно,
[56:46.480 --> 57:02.960]  точная копия этого сплита. Давайте подведем небольшой итог промежуточный. То есть мы работаем
[57:02.960 --> 57:09.600]  с парами ключ значения. На мапе мы поэлементно обрабатываем данные, дальше группируем,
[57:09.600 --> 57:16.760]  сортируем по ключам и на reduce обрабатываем группы. Если говорить, чтобы было понятнее в
[57:16.760 --> 57:26.720]  терминах SQL, то мап — это какая-нибудь такая функция, для которой не нужны соседние записи в
[57:26.720 --> 57:32.320]  таблице. То есть вот у нас табличку считаем, считываем, если мы подключаем условия where,
[57:32.320 --> 57:39.040]  field 1 меньше 5, то никакие другие строчки нас не интересует. Мы просто смотрим,
[57:39.040 --> 57:45.320]  подошла строчка или не подошла. Точно так же с селектом. Вот это все маппер. Если мы говорим
[57:45.320 --> 58:00.280]  про джойны, агрегации, всякие оконки, вот это все уже редьюсеры. Ну и теперь с точки зрения того,
[58:00.280 --> 58:06.120]  где какие процессы работают, мы запускаем программу. Программу мы запускаем на клиентской машине.
[58:06.120 --> 58:13.760]  Дальше, когда она попадает в ходу, происходит несколько форков и соответственно мапперы и
[58:13.760 --> 58:19.840]  редьюсеры запускаются на нодах. Нам их надо как-то докатить до нод, поэтому мы эти коды,
[58:19.840 --> 58:25.880]  если мы пишем на стриминге, то вот эти Python файлики мы должны докатить до нод, а собственно
[58:25.880 --> 58:33.680]  положить в распределенный кэш. Распределенный кэш это не HDFS, это дополнительная такая папка на
[58:33.680 --> 58:43.000]  датанодах, где эти файлы хранятся, и мы можем прямо оттуда их запустить. Вот запускаем мапперы,
[58:43.000 --> 58:49.040]  запускаем редьюсеры и обратите внимание, что промежуточные данные, которые между мапперами
[58:49.040 --> 58:54.800]  и редьюсерами, они хранятся не в HDFS, а просто на диске. Почему, как вы думаете,
[58:54.800 --> 59:05.800]  почему не в HDFS? HDFS как бы надежнее. Ну да, мы будем копировать с нодом на ноды,
[59:05.800 --> 59:12.000]  но почему мы не хотим сохранить эти данные в HDFS для большей надежности? Там есть репликации,
[59:12.000 --> 59:20.640]  отказа устойчивости. На неймноде считаем результаты?
[59:20.640 --> 59:38.800]  Ну вообще не будет ли странно, что мы оперируем с данными HDFS и результаты тоже в HDFS?
[59:38.800 --> 59:46.880]  Ну мы оперируем данными на ноутбуке, на диске и кладем данные на этот же диск,
[59:46.880 --> 59:59.840]  ничего странного не будет. Мы же покладем другую папку, какая разница? Да, зачем? Потому что
[59:59.840 --> 01:00:07.440]  скорость. Вы сказали про временные данные, действительно вы правы, нам эти данные нужны
[01:00:07.440 --> 01:00:13.600]  только до конца работы джобы. Какой смысл нам их сначала класть в HDFS, ждать, пока write once,
[01:00:13.600 --> 01:00:18.600]  read many, пока репликация дойдет, потом опять читать и снова ждать, когда просто можно
[01:00:18.600 --> 01:00:32.120]  с диска на диск перекинуть. Теперь стриминг. Чтобы вам не сильно скучать, мы скоро посмотрим на пример,
[01:00:32.120 --> 01:00:39.040]  больше будет на семинарах, но я вам хотя бы покажу как в принципе это все работает. Пока посмотрим на
[01:00:39.040 --> 01:00:45.600]  стриминг. То есть Hadoop написан на джаве, у него есть хорошая нативная java API, но как я уже сказал,
[01:00:45.600 --> 01:00:53.240]  если мы не хотим писать на джаве никакую, то приходится работать вот с такой вот штукой. То есть у
[01:00:53.240 --> 01:01:00.520]  нас имеется готовая Hadoop программа, куда нам нужно подкинуть вот эти вот мапперы и редьюсеры. С одной
[01:01:00.520 --> 01:01:05.840]  стороны мы получаем полную свободу действий, то есть Hadoop нам выкидывает кусок данных и говорит
[01:01:05.840 --> 01:01:13.320]  обрабатываете вы их чем хотите. Любой исполняемый файл, вы туда можете, любой бинарник, лишь бы
[01:01:13.320 --> 01:01:20.960]  операционка ваша могла его выполнить. Вот это все что надо, но и помимо этого надо, чтобы он умел
[01:01:20.960 --> 01:01:37.520]  писать, читать в консоли. В Hadoop есть специальный аргумент default separator, можно его указать,
[01:01:37.520 --> 01:01:44.120]  это может быть точка запятой там для CSV файлов всяких. По умолчанию это tab, ну и соответственно
[01:01:44.120 --> 01:01:52.200]  можно указать еще и формат хранения файлов и в этом формате этот default separator сохранить.
[01:02:06.200 --> 01:02:12.400]  Да все что угодно может быть, например есть библиотека, которая анализирует звуковые файлы и
[01:02:12.400 --> 01:02:17.440]  можно сделать так, что разделителем будет соответствующий звук какой-нибудь там хлопок,
[01:02:17.440 --> 01:02:26.960]  можно и так сделать. По умолчанию в Hadoop это tab и для нашего курса хватит таба, если вы захотите
[01:02:26.960 --> 01:02:31.640]  там какие-нибудь в своей научке данные попроцессить, у вас скорее всего будет
[01:02:31.640 --> 01:02:45.160]  какие-нибудь CSV, то там вот точка запятой поставите. Что? Ключом что ли? Это разделитель
[01:02:45.160 --> 01:02:58.200]  между ключом и значением, а так ключом может быть что угодно. Все что хочешь, это просто объекты,
[01:02:58.200 --> 01:03:12.080]  джевовые. В итоге вот Hadoop стриминг выглядит он вот так. Сейчас мы это все запускаем,
[01:03:12.080 --> 01:03:16.840]  посмотрим, но вы по крайней мере сможете сказать всем своим коллегам, что Hadoop очень простой,
[01:03:16.840 --> 01:03:21.880]  потому что запускается он с помощью одной команды, вот это вот одна команда. То есть вам чтобы
[01:03:21.880 --> 01:03:26.680]  запустить MapReducer, вам нужно по сути вызвать одну команду, ну и туда поставить mapper,
[01:03:26.680 --> 01:03:35.280]  reducer, все эти настройки, команда все равно одна. Давайте посмотрим, что в этой команде есть.
[01:03:35.280 --> 01:03:44.160]  Для примера запуска давайте разбираться пошагово и заодно будет понятно как и на джаве,
[01:03:44.160 --> 01:03:51.720]  потому что это на самом деле тоже на джаве. Там джар есть, просто он не наш. То есть кто умеет
[01:03:51.720 --> 01:03:56.920]  писать на джаве пишет свои джарники, а кто не умеет писать на джаве, пользуется чужими джарниками
[01:03:56.920 --> 01:04:03.600]  и их настраивает. Если грубо говоря, то вот так все работает. Поэтому вот есть yarn, наш
[01:04:03.600 --> 01:04:10.000]  распределитель ресурсов, команда yarn-jar, туда мы подсовываем джарник, соответственно свой или тот,
[01:04:10.000 --> 01:04:18.760]  который вошел в поставку Hadoop и создаем параметры. Название джабы, количество редюсеров, всякие там
[01:04:18.760 --> 01:04:24.440]  маперы, редюсеры, настройки, дополнительные файлы, которые мы туда прикрепляем, обязательно пути
[01:04:24.440 --> 01:04:31.640]  для входа и для выхода и вот получается вот такая у нас штука, вот такая команда. Сейчас мы попробуем
[01:04:31.640 --> 01:04:52.720]  ее запустить. Вот так она у нас выглядит и сейчас конкретно эту программу будем
[01:04:52.720 --> 01:05:00.080]  запускать. Видите, мы с вами написали маппер, написали редюсер и сейчас мы их просто сюда
[01:05:00.080 --> 01:05:23.120]  поселили и запустим. Давайте посмотрим. Есть, ну и запускаем. Что мы видим?
[01:05:23.120 --> 01:05:38.720]  Вот смотрите, мы видим number of splits равно 2. То есть это значит, что у нас два блока на входе.
[01:05:38.720 --> 01:05:53.720]  У нас сейчас есть другой курс, где очень активно ребята сдают сегодня домашки,
[01:05:53.720 --> 01:05:56.640]  поэтому может быть какое-то время оно будет тупить.
[01:06:08.720 --> 01:06:17.320]  Можно пока воспользоваться вот этим.
[01:06:17.320 --> 01:06:44.000]  Да, тут у нас сейчас 16-16, поэтому прямо сейчас мы, к сожалению, не увидим, как задачка работает.
[01:06:44.000 --> 01:06:58.480]  Какая разница? Это же хадуба, то есть каждая таска хадуба,
[01:06:58.480 --> 01:07:17.560]  для таких случаев, когда у нас кластер перегружен или когда мы просто не хотим ждать,
[01:07:17.560 --> 01:07:27.720]  пока Java все это будет запускаться, есть псевдораспределенный запуск. Вот если вы посмотрите на этот код,
[01:07:27.720 --> 01:07:36.200]  что поменялось по сравнению с тем, что я показывал на слайде? У нас поменялось вот этот конфиг,
[01:07:36.200 --> 01:07:42.640]  если вы там видите, переменная конфиг, которую мы везде подставляли. То есть это специальная
[01:07:42.640 --> 01:07:47.840]  настройка, которая позволяет запустить хадуб в псевдораспределенном режиме. Это значит,
[01:07:47.840 --> 01:07:54.160]  что он как бы распределенный, но все работает на одной ноде, конкретно на клиенте. Там
[01:07:54.160 --> 01:08:02.560]  запускаются и инстанс-неймноты, и инстанс всех этих обработчиков, и все это работает без участия
[01:08:02.560 --> 01:08:11.320]  большого глобального хадуба. Он разворачивает так называемый мини-кластер хадуб, то есть это урезанная
[01:08:11.320 --> 01:08:19.360]  версия. В этом случае, чтобы воспользоваться такой штукой, нам нужно скачать с хадуба
[01:08:19.360 --> 01:08:23.920]  сэмпл данных, поместить его на диск, потому что да, псевдораспределенный режим работает,
[01:08:23.920 --> 01:08:30.160]  в отличие от большого хадуба, он работает не с HDFS, а с диском, просто с диском на этой машинке.
[01:08:30.160 --> 01:08:44.760]  Вот видите, тут написано input in. Что вот это за in? А вот он. Вот у нас in, папка. Давайте посмотрим,
[01:08:44.760 --> 01:08:51.440]  что в этом in есть. Есть, есть кусочек, довольно большой кусочек википедии.
[01:08:51.440 --> 01:09:12.840]  Ну и вот собственно у нас тут все посчиталось, и мы видим, мы тут видим разные счетчики,
[01:09:12.840 --> 01:09:19.880]  вот можно полистать этот лог и посмотреть, что у нас было прочитано, вот столько байт записано,
[01:09:19.880 --> 01:09:30.960]  вот столько, вот read-write. Вот commit-memory, used-memory, то есть вот в этих логах можно подробнее
[01:09:30.960 --> 01:09:34.920]  посмотреть, сколько памяти было выделено, сколько было заиспользовано, сколько было
[01:09:34.920 --> 01:09:57.040]  утилизировано памяти, все это можно здесь увидеть. Есть ли какие-нибудь вопросы? Чего? А,
[01:09:57.040 --> 01:10:06.520]  результат будет в папке out. Нет, почему сломалось? Мы его даже увидели, этот результат, потому что
[01:10:06.520 --> 01:10:16.920]  в команде есть, то есть в нашем файле есть команда HDFS dfs-cat. Вот такой вот результат. Просто слова
[01:10:16.920 --> 01:10:22.640]  у нас не почищенные конкретно вот в этих данных, они не почищены от всяких там точек запятых и так
[01:10:22.640 --> 01:10:29.480]  далее, поэтому вот такой вот мусор, но все равно мы посчитали количество по каждому слову. Тут
[01:10:29.480 --> 01:10:46.240]  какая-то даже не по слову, а по каждому токену. А топ нужно дополнительно отсортировать, топ нужно
[01:10:46.240 --> 01:10:52.000]  дополнительно отсортировать, то есть когда я вам показывал команду, да здесь ее нет, потому что
[01:10:52.000 --> 01:11:01.240]  это еще одна джоба. Вам на семинаре покажут подробнее, вам нужно сделать сорт после редьюса. Для этого
[01:11:01.240 --> 01:11:08.720]  нужно запустить еще одну джобу, которая будет как бы с пустым маппером, с пустым редьюсером,
[01:11:08.720 --> 01:11:29.240]  но там будет сорт, то есть это следующий шаг уже. Да, вот что касается вывода, вот аутдир,
[01:11:29.240 --> 01:11:34.760]  вот здесь она будет сохраняться.
[01:11:34.760 --> 01:11:58.000]  Какие-нибудь вопросы по этой части есть?
[01:12:04.760 --> 01:12:21.960]  Тогда нам осталось совсем чуть-чуть, а именно нам надо понять, для чего ходуб мы не можем
[01:12:21.960 --> 01:12:28.920]  использовать. Во-первых, ходуб у нас постоянно работает с диском, поэтому там, где нужна какая-то
[01:12:28.920 --> 01:12:40.200]  реал-тайм обработка данных, ходуб там не подойдет. Во-вторых, почему он не масштабируемый?
[01:12:40.200 --> 01:13:03.400]  Можем, почему? Можем это сделать? АВС у тебя работает с достаточно однородными нодами,
[01:13:03.400 --> 01:13:12.360]  там должна стоять одинаковая операционка, должны быть плюс-минус одинаковые наборы ядер и параций
[01:13:12.360 --> 01:13:19.040]  всего остального. Вот, например, С3-хранилище так работает, все должно быть однородное. В этом случае
[01:13:19.040 --> 01:13:27.640]  ходуб может съесть любые машинки, например, это могут быть машинки на старых двухядерных процах,
[01:13:27.640 --> 01:13:32.280]  и рядом могут стоять какие-нибудь более современные машинки, и все это может жить в одном ходуб-кластере.
[01:13:32.280 --> 01:13:47.600]  Достаточно часто, если вы работаете в не очень большой компании, где в больших компаниях там,
[01:13:47.600 --> 01:13:52.920]  понятно, все достаточно стандартизовано, все унифицировано, у вас есть огромные ЦОДы,
[01:13:52.920 --> 01:13:59.440]  там сотни тысяч одинаковых машин, если вы работаете в небольшой компании, которая, тем не менее,
[01:13:59.440 --> 01:14:07.440]  занимается большими данными, то вам нужна такая система, которая будет поддерживать любую
[01:14:07.440 --> 01:14:19.000]  инфраструктуру. Если все одинаковое, да, можно воспользоваться ходубом, тоже нормально. Но для
[01:14:19.000 --> 01:14:25.680]  реалтайма ходуб не будет нормально работать, потому что, во-первых, все хранится на диске, во-вторых,
[01:14:25.680 --> 01:14:35.120]  нам нужно каждый раз запускать вот эти вот виртуальные машины Java, потом их останавливать,
[01:14:35.120 --> 01:14:43.880]  тоже на это тратится время. Да, ходуб не предназначен просто в то время, когда ходуб создавался,
[01:14:43.880 --> 01:14:50.360]  запроса на реалтайм-обработку бигдейта вообще не было. То есть тогда достаточно было просто
[01:14:50.360 --> 01:14:54.560]  обрабатывать большие данные, которые другие системы обрабатывать вообще не могли.
[01:14:54.560 --> 01:15:08.560]  Для реалтайма у нас есть всякие аналоги, которые работают в похожей парадигме,
[01:15:08.560 --> 01:15:12.360]  это, например, Spark или Flink для реалтайма, может быть, кто-то слышал.
[01:15:12.360 --> 01:15:26.640]  Ходуб очень востребован как хранилище, то есть вот чистый MapReduce, который запускает
[01:15:26.640 --> 01:15:34.200]  маппер и редьюсеры, у него сохранилась такая некая ниша, как у ассемблера. Есть языки более
[01:15:34.200 --> 01:15:39.360]  высокоуровневые, есть низкоуровневый ассемблер. У ходуба точно так же, есть высокоуровневый,
[01:15:39.360 --> 01:15:45.840]  намного более приятный Spark, Flink, Hive. Ходуб в этом смысле менее приятный, потому что,
[01:15:45.840 --> 01:15:52.680]  чтобы посчитать простейший wordcount, нам нужно написать два файла на Python и вот эту большую
[01:15:52.680 --> 01:16:00.480]  команду на Bash. В Spark в этом случае мы напишем 5 лямбдочек и у нас все будет работать. Поэтому,
[01:16:00.480 --> 01:16:06.800]  как обработка данных, ходуб не так популярен, но с другой стороны, на ходубе можно сделать то,
[01:16:06.800 --> 01:16:14.800]  что нельзя сделать на Spark. Например, тут уже зависит от инфраструктуры. Sparkу нужно много памяти,
[01:16:14.800 --> 01:16:20.760]  он данные обрабатывает в памяти, хранит в памяти. Если у нас памяти мало или если у нас
[01:16:20.760 --> 01:16:28.240]  разнородная инфраструктура, то здесь ходуб будет работать лучше. И еще мы с вами будем разбирать
[01:16:28.240 --> 01:16:33.840]  в следующий раз элементы Shaft and Sort. Есть маппер, есть редьюсер, между ними Shaft and Sort.
[01:16:33.840 --> 01:16:40.720]  Пока это для нас достаточно серый ящик, мы разберем в следующий раз более подробно,
[01:16:40.720 --> 01:16:46.160]  что там внутри находится, и ходуб позволяет лучше, чем Spark, залезть внутрь, прям покопаться,
[01:16:46.160 --> 01:16:56.520]  как сортировка происходит. Если говорить про файловую систему HDFS, она сейчас практически везде есть.
[01:16:56.520 --> 01:17:10.080]  Все, что нам осталось на сегодня, это посмотреть на вот это. Мы этим будем заниматься в следующий раз,
[01:17:10.080 --> 01:17:16.760]  пока просто посмотрите. Это реальная схема того, как работает ходуб. Выглядит она ужасно,
[01:17:16.760 --> 01:17:21.720]  но вот маппер наш вот здесь, редьюсер наш вот здесь. То есть вы просто можете увидеть,
[01:17:21.720 --> 01:17:38.680]  как много всякой работы ходуб за нас делает. В домашней нам надо будет писать только маппер и
[01:17:38.680 --> 01:17:43.800]  редьюсер. Может быть немного подкрутить параметрами сортировку, там уже будете
[01:17:43.800 --> 01:17:49.240]  на семинарах смотреть. Вот этот весь ужас, который посередине написан, вам писать не нужно,
[01:17:49.240 --> 01:17:54.640]  да и вообще те, кто работают с ходубом, они очень редко что-то переписывают из вот этого под себя.
[01:17:54.640 --> 01:18:03.760]  Пишут свои компараторы для того, чтобы можно было задать алгоритм, как мы сравниваем объекты при
[01:18:03.760 --> 01:18:08.960]  сортировке. Пишут свои партишнеры, пишут свои файловые форматы. Ну вот, пожалуй, и все.
[01:18:08.960 --> 01:18:23.520]  Там какие-то вопросы есть про сбер, но в сбере, насколько я знаю, довольно много ходубов разных,
[01:18:23.520 --> 01:18:26.760]  там одновременно разные версии живут и все это очень интересно.
[01:18:26.760 --> 01:18:46.280]  У них есть СДП, достаточно много статей про СДП, почитайте. И даже можете пойти на
[01:18:46.280 --> 01:18:54.000]  ходуб админ с метап, тоже погуглите, что это такое, там достаточно много докладов про СДП, сбер датаплатформ.
[01:18:54.000 --> 01:19:05.320]  На последнем слайде такая инструкция. Она старая, ей почти 10 лет, но здесь подробно
[01:19:05.320 --> 01:19:12.200]  описано с пояснениями, как писать на ходуб стриминги свои коды и что куда ставить и почему.
[01:19:12.200 --> 01:19:23.200]  У меня на этом все, есть ли какие-то вопросы? Тогда всем спасибо и до следующего раза.

[00:00.000 --> 00:11.240]  вот соответственно сегодня продолжаем разговор про то что у нас происходит методах оптимизации
[00:11.240 --> 00:18.840]  прошлый раз мы немного в некотором смысле поразвлекались что ли какие-то совсем простенькие
[00:18.840 --> 00:26.760]  примеры посмотрели сегодня будем уже погружаться в сам предмет те в те в ту терминологию которая
[00:26.760 --> 00:37.440]  нам понадобится в рамках курса вот соответственно да так окей напоминаю что мы с вами решаем вот
[00:37.440 --> 00:43.520]  такую задачу оптимизации и в самом простом случае она у нас безусловная то есть то множество по
[00:43.520 --> 00:48.160]  которым по которым мы хотим найти минимум оно просто соответствует всему вашему пространству
[00:48.760 --> 00:56.320]  но никак не ограничено ну и соответственно водится следующее определение вот точка
[00:56.320 --> 01:04.640]  их у нас называется локальный минимум функции f на на rd если существует некоторый шарик в данном
[01:04.640 --> 01:10.400]  случае вот шарик определенно так вот у нас вокруг этой точке радиус этого шарика r
[01:10.400 --> 01:21.520]  и следует на любой точке из этого шарика f от x звездой меньше чем f от y соответственно
[01:21.520 --> 01:28.800]  понятную в этом шарике на иксе достигается минимальное значение вот мы соответственно
[01:28.800 --> 01:35.040]  глобальный минимум точке их что глобально глобальный минимум на rd если у нас соответственно вот это
[01:35.040 --> 01:40.720]  вот неравенство выполняется для любой точке и из всего множество то есть какой бы мы
[01:40.720 --> 01:47.960]  играть не взяли у нас я по 30 но тут все понятно вот здорово ну то есть принципе вот это вот эти
[01:47.960 --> 01:55.520]  все определения которые даны здесь можно обобщить и на случай когда у нас рассматривается не
[01:55.520 --> 02:01.800]  безусловная задача условно на каком-то множестве x вот вам достаточно соответственно пересечь
[02:01.800 --> 02:07.240]  вот здесь вот написать x вот и вот здесь вот этот шарик пересесть множеством
[02:07.240 --> 02:15.080]  шарики и для всех игр которые содержатся ваше множество x по которым вы оптимизируете в
[02:15.080 --> 02:18.600]  шарике у вас соответственно должно быть выполняться условие ну и во втором случае
[02:18.600 --> 02:25.440]  аналогично здесь у вас становится множество x ну и здесь соответственно множество x вот тут
[02:25.440 --> 02:31.400]  все более-менее просто какие-то такие совсем базовые определения ok давайте соответственно
[02:31.400 --> 02:36.760]  докажем такой факт который вы скорее всего знаете уже с математического анализа что
[02:36.760 --> 02:44.000]  соответственно в точке которая является локальным минимумом у вас дифференцируемая функция
[02:44.000 --> 02:50.400]  принимает значение градиента данных в случае так как у нас она зависит от вертера аргументов
[02:51.040 --> 02:58.960]  вот окей давайте докажем давайте пойдем от противного и предположим что в оптиме
[02:58.960 --> 03:06.960]  назначение не 0 ну и разложим ряд в нашей окрестности локального минимума вот этой
[03:06.960 --> 03:14.200]  точке x звездой соответственно ряд здесь выписан то есть нулевой член член первого порядка
[03:14.200 --> 03:21.800]  вот и соответственно о малая которая выписана вот здесь то есть при стремлении x
[03:21.800 --> 03:29.920]  x звездой у вас о малое вот это вот расстояние x минус x звездой нормировано на реальный
[03:29.920 --> 03:33.760]  расстояние x звездой стремиться к нулю ну то есть это по факту просто векторное
[03:33.760 --> 03:40.360]  определение малого объективного нормы вот ну и что соответственно мы делаем давайте попробуем
[03:40.360 --> 03:45.600]  доказать что действительно от противного что если у нас градиент в оптиме локальный минимум не
[03:45.600 --> 03:52.080]  равен нулю то это неверно и для этого давайте вот рассмотрим точку вот такого вида x волной
[03:52.080 --> 04:00.800]  которая представляет собой x звездой минус какой-то коэффициент лямбда скалярный умножить на наш
[04:00.800 --> 04:06.280]  градиент который не нулевой я напоминаю вот первая цель которую хочется поставить по-первых
[04:06.280 --> 04:13.800]  попасть нашей точке и точка икса звездой в окрестность где у нас точка икса звездой
[04:13.800 --> 04:19.760]  является локальный минимум вот икс икс тильда при достаточно малом лямбда понятно попадет в
[04:19.760 --> 04:26.000]  окрестность потому что вектор градиента у нас зафиксирован вот шарик мы попадем такое лямбда
[04:26.000 --> 04:36.800]  понятно подберется вот окей тогда соответственно что тогда с одной стороны у нас значения в ф с тильдой
[04:36.800 --> 04:41.960]  должно быть больше либо равно чем в ф икса звездой просто по определению локального минимума мы
[04:41.960 --> 04:47.280]  подобрали как раз окрестность так чтобы мы попали туда и в итоге нашли какое-то противоречие плюс
[04:47.280 --> 04:54.960]  должна быть выполнена вот эта вот опроксимация которую мы записали в терминах о малая вот ну
[04:54.960 --> 05:01.680]  здесь соответственно во второй строчке просто подставляю явный вид x стильдой вот здесь вот я
[05:01.680 --> 05:07.520]  его просто выписываю здесь соответственно я уж подставляю что у нас икс стильдой имеет конкретный
[05:07.520 --> 05:14.640]  вид икса звездой минус лямбда градиенты ф икса звездой ну вот здесь соответственно появляется вот
[05:14.640 --> 05:21.560]  этот член плюс вот этот член вот понятно что в данном случае уже норма градиента только тут
[05:21.560 --> 05:29.040]  да двоечку забыл вот норма градиент в данном случае это просто константа некоторая и умалые терминах
[05:29.040 --> 05:35.640]  умала мы говорим уже с точки зрения лямбда вот окей ну и соответственно да лямбда мы можем
[05:35.640 --> 05:42.560]  подобрать так поварировать так посмотреть посмотреть в том числе на константы градиента что у нас
[05:42.560 --> 05:49.600]  умалое от лямбда вот это ведет себя там меньше либо равно чем лямбда пополам и множество норма
[05:49.600 --> 05:56.200]  градиентов в квадрате вот просто по определению мало что у нас умала от лямбда и оно и при
[05:56.200 --> 06:01.480]  достаточно малых лямбда будет меньше либо равно чем лямба пополам и множество какую-то константу вот
[06:01.480 --> 06:10.280]  все ну и тогда я подставляю что вот у меня вот здесь вот эта вещь ограничена вот таким вот значением
[06:10.280 --> 06:16.040]  лямбда пополам на норму градиента в квадрате и получаешь вот следующее выражение вот в силу
[06:16.040 --> 06:21.080]  того что я подбирал лямбда достаточно маленьким но больше нуля в силу того что я предположил что
[06:21.080 --> 06:27.800]  у меня норма градиента не равна нулю я получаю что значение в экстильдой меньше строго меньше
[06:27.800 --> 06:34.640]  чем значение в точке икса звездой мы уж получаем соответственно насчет противоречия вот с тем что
[06:34.640 --> 06:42.080]  у нас и икса звездой это локальный получили соответственно условия оптимальностей самое
[06:42.080 --> 06:47.640]  одно из самых базовых условий оптимальности в оптимизации вот но вообще наша цель ведь находить
[06:47.640 --> 06:52.640]  какие-то глобальные минимумы ну застряли мы в каком-то локальном минимуме это же не особо решение
[06:52.640 --> 06:58.880]  задачки которая нам нужна вот цель находить какие-то хорошие решения глобальные минимумы вот и на
[06:58.880 --> 07:05.680]  прошлой лекции в принципе стало понятно что эти глобальные минимумы вот довольно сложно находить
[07:05.680 --> 07:14.240]  даже добавляя какие-то предположения на функцию все равно получается что лучше чем какой-то перебор
[07:14.240 --> 07:20.480]  у нас ничего и не работает ну что довольно плохо поэтому как я и обещал сегодня мы будем разбираться
[07:20.480 --> 07:26.200]  с предположениями которые нам помогут обойти эти проблемы вот и первый такой из таких предположений
[07:26.200 --> 07:33.480]  это выпуклость выпуклость функции соответственно выпуклость функции в rd вот пусть нам до нас
[07:33.480 --> 07:39.360]  соответственно неперерывно дифференцируема функция f будем говорить что она является выпуклой если
[07:39.360 --> 07:46.400]  для любых на всем rd вот если у нас для любых x и y из rd выполнено следующее соотношение вот
[07:46.400 --> 07:52.080]  некоторое соотношение которое нам пока не понятно что вообще значит вот часто пройдем дальше и
[07:52.080 --> 07:57.000]  посмотрим на картинку вот но смысл такой то что на самом деле вот тут я сразу записал
[07:57.000 --> 08:03.280]  определение опираюсь на то что функция у нас дифференцируема на самом деле как вы понимаете
[08:03.280 --> 08:08.360]  выпуклые функции могут быть не обязательно дифференцируемы мы сейчас это поймем потому
[08:08.360 --> 08:14.240]  что есть еще одно определение выпуклых функций и вы его будете разбирать на семинарах точнее
[08:14.240 --> 08:19.320]  более подробно разбирать как первое так и второе определение даже скорее начнете со второго на
[08:19.320 --> 08:25.520]  семинарах посмотрите примеры выпуклых функций посмотрите как можно проверять функции на выпуклость
[08:25.520 --> 08:30.560]  как по первому определению как по второму так и по каким-то дополнительным фактам вот но второе
[08:30.560 --> 08:35.000]  определение гласит оно вообще уже не требует никакой дифференцируемости оно просто гласит то
[08:35.000 --> 08:41.920]  что у нас значение функции f вот такой вот точке лямбда x плюс 1 минус лямбда y что называется
[08:41.920 --> 08:47.360]  выпуклой комбинации двух точек потому что лямбда берется от нуля до единицы ну получается у нас
[08:47.360 --> 08:53.960]  есть две точки x и y и мы берем какую-то из этих точек на отрезке соединяющих эти две точки вот и
[08:53.960 --> 09:01.280]  оказывается что для любых значений вот это и в точке значение функции в точке на отрезке вот
[09:01.280 --> 09:07.720]  такая вот комбинация двух значений тоже выпуклая комбинация двух значений должна лежать выше вот
[09:07.720 --> 09:13.080]  ну давайте посмотрим немного на физику обоих определений в лекциях мне скорее будет нужна
[09:13.080 --> 09:19.400]  физика первого определения вот ну и соответственно картинка я вам ее нарисовал вот это пока не нужно
[09:19.400 --> 09:25.080]  это мы дальше обсудим а смысл соответственно выпуклости означает в том что вот у вас есть
[09:25.080 --> 09:34.160]  ваша целевая функция здесь она соответственно выделена такой линии без пунктирчиков непрерывистой
[09:34.160 --> 09:39.520]  линии вот а выпуклость означает что вы проводите в некотором смысле линейную опроксимацию этой
[09:39.520 --> 09:46.120]  функции по градиенту то есть вы берете значение в точке x данном случае просто f от x и линейно
[09:46.120 --> 09:53.080]  ее опроксимируете исходя из того как выглядит градиент вот если вот функция лежит я реально
[09:53.080 --> 09:59.200]  целевая функция лежит выше вот этой линейной опроксимации то тогда она выпукла вот это
[09:59.200 --> 10:07.080]  свойство нам соответственно понадобится больше на лекции второе свойство которое я выписал оно
[10:07.080 --> 10:12.840]  в принципе тоже хорошее и интересно и часто оно пригодается для проверки функции на выпуклость
[10:12.840 --> 10:19.680]  полуопределения вот оно означает что вот у вас есть два значения f от x и 2 и 2 значение f от y и
[10:19.680 --> 10:26.760]  вам говорится о том что отрезочек которые соединяют вот эти две точки соответственно
[10:26.760 --> 10:35.240]  точку x и f от x и точку y и f от y он лежит выше чем сама функция ваша выпукла вот тоже хорошее в
[10:35.240 --> 10:43.480]  принципе физическое свойство вот соответственно да вот но в случае когда у нас функция дифференцируема
[10:43.480 --> 10:48.280]  вот эти два определения эквивалентны соответственно вторая понятно более общая в связи с тем что оно
[10:48.280 --> 10:53.840]  справедливо не только для дифференцируемых функций например какие вы знаете сами просты
[10:53.840 --> 11:01.080]  недифференцируемых функций модуль понятно вот он выглядит вот так вот является ли он выпуклым да
[11:01.080 --> 11:06.040]  является просто по вот этому физическому смыслу который и второму определению который я вам сказал
[11:06.040 --> 11:12.680]  вот какие бы вы отрезки не соединяли они будут лежать выше чем график функции вот об этом вы
[11:12.680 --> 11:16.640]  соответственно подробнее уже на семинарах поговорите вот сейчас мы вам главное вот это
[11:16.640 --> 11:22.080]  физическое свойство которое то что у вас линейная опроксимация всегда подпирает снизу вашу целевую
[11:22.080 --> 11:29.280]  функцию вот хорошо аналогичное определение только сильная выпуклость сильная выпуклость теперь
[11:29.280 --> 11:34.880]  у вас добавляется еще вот такой вот дополнительный дополнительный член ми пополам х минус
[11:34.880 --> 11:45.120]  игрек в квадрате вот и аналогично можно вести ровно такое же определение для как раз случае
[11:45.120 --> 11:51.800]  когда у вас уже не дифференцируема функции вот соответственно вот так вот два определения чуть
[11:51.800 --> 11:58.920]  она стала в некотором смысле не знаю сложнее но на самом деле вот вот это выпуклость сильная выпуклась
[11:58.920 --> 12:04.880]  дает более хорошие свойства для функции а именно на картиночке это выглядит следующим образом то
[12:04.880 --> 12:10.440]  есть теперь у вас подпирается не вот этой линейной просто опроксимации не вот этой линейной
[12:10.440 --> 12:17.560]  опроксимации а теперь вы говорите то что у вас функция растет но и растет быстрее может расти
[12:17.560 --> 12:21.920]  быстрее чем квадрат то есть вот тут вот я уже подпираю чем-то более качественные то есть
[12:21.920 --> 12:27.040]  квадратичные функции дополнительно то есть до этого у меня было просто линейная ну и она как-то
[12:27.040 --> 12:33.160]  себя ведет рядом с этой линейной функции то есть она может там и вообще и вести себя как линейная
[12:33.160 --> 12:38.480]  функция потому что там если мы реально рассматриваем а здесь вот вас еще подпирается дополнительно
[12:38.480 --> 12:46.640]  квадратом вот что как мы увидим дает лучшее свойство как и для каких-то гарантий нахождения
[12:46.640 --> 12:53.440]  решения так и работы методов вот вот такие два свойства выпуклости сильная выпуклость вот
[12:53.440 --> 13:00.840]  хорошо тогда давайте докажем следующую теоремку довольно простую пусть она выпукла непрерывно
[13:00.840 --> 13:09.840]  дифференцирована функция вот и у нас есть некая точка икса звездой что соответственно там у
[13:09.840 --> 13:15.920]  нас значение градиента равно нулю не дописал значение градиента равно нулю тогда у нас
[13:15.920 --> 13:22.800]  икса звездой это не просто какая-то точка это глобальный минимум выпуклых функций что довольно
[13:22.800 --> 13:27.840]  хорошо то есть это уже какие-то гарантии что если я нашел стационарную точку то я сразу же нашел
[13:27.840 --> 13:35.280]  глобальный минимум вот окей тут все просто просто делаем по определению записываем определение
[13:35.280 --> 13:41.600]  выпуклой функции и все то есть тут просто нужно немного подставить точки которые вам нужны то есть
[13:42.340 --> 13:48.820]  и точку икс ну и соответственно силу того что это у этого скалярно произведение обращается в
[13:48.820 --> 13:53.920]  0 потому что градиент равен нулю тогда у вас это просто значение от икса звездой и получается
[13:53.920 --> 14:00.840]  что ехал тикс больше либо равно ф от икса звездой что по определению вам сразу же говорит о том
[14:00.840 --> 14:09.320]  что для любой точке икс у вас ф от икса звезды меньше либо равно и значение это хорошо классное
[14:09.320 --> 14:14.480]  свойства, приближаемся уже к чему-то явно хорошему. В обратную сторону,
[14:14.480 --> 14:17.840]  понятно, мы и доказывали, соответственно, получается у вас критерий. Глобально минимум
[14:17.840 --> 14:23.600]  значения градиента равно нулю. Значение градиента равно нулю. Это глобально минимум
[14:23.600 --> 14:31.600]  для выпуклой функции. Отлично. Теперь определение выпуклого множества,
[14:31.600 --> 14:35.960]  потому что, в принципе, мы будем решать не только условные задачи, но и
[14:35.960 --> 14:39.080]  безусловные. Сегодня мы, конечно, больше сконцентрируемся на безусловных
[14:39.080 --> 14:42.960]  задачах, то есть на задачах, которые нужно, где нужно оптимизировать функцию на
[14:42.960 --> 14:49.200]  Rd. Ну и, в принципе, безусловные задачи, мы дальше уже их будем касаться,
[14:49.200 --> 14:54.440]  то есть через две лекции уже пойдут безусловные задачи. Ну и, соответственно,
[14:54.440 --> 14:58.920]  чтобы их тоже уметь решать, нужно вводить множество, на которых мы будем пытаться
[14:58.920 --> 15:03.080]  решать. В данном случае, опять же, вводится выпукло множество. Вводится оно
[15:03.080 --> 15:07.760]  следующим образом. У нас множество выпукла. Если мы берем две точки и отрезок,
[15:07.760 --> 15:14.160]  соединяющие эти две точки, тоже будет лежать в этом множестве. Подробно вы про
[15:14.160 --> 15:17.360]  выпукло множество будете говорить уже на следующем семинаре, то есть не на этой
[15:17.360 --> 15:23.240]  неделе, а на следующем. Сейчас вы закончите обучаться взять у производных,
[15:23.240 --> 15:27.000]  там и градиентов, десианов, а потом, соответственно, перейдете как раз уже к
[15:27.000 --> 15:33.680]  понятию выпуклостью, выпуклым множеством, выпуклым функциям. Вот. Слайд назад, пожалуйста.
[15:33.680 --> 15:43.040]  Если что, слайды есть в чате. Еще один слайд назад. Да, да, смотрите, я же прям в
[15:43.040 --> 15:49.760]  теореме прописываю, непрерывно дифференцируемо. Да, да, да, я же здесь тоже написал, что
[15:49.760 --> 15:59.400]  равно нулю, описался здесь вот, вот. Равно нулю, равно нулю, тут не дописал. Вот. Вот.
[15:59.400 --> 16:04.760]  Если что, слайды я выложил в чат, чтобы вы в формулировке теореме, если что, видели. Вот.
[16:04.760 --> 16:09.160]  Мы сразу быстренькие вопросы. Какое множество выпуклое, какое множество не выпуклое?
[16:09.160 --> 16:20.240]  Да, первое, третье у нас, соответственно, понятно выпуклое, первое, третье вот, а второе нет. Второе
[16:20.240 --> 16:24.040]  нет, ну понятно в силу того, что ровно по физическому смыслу определения выпуклого
[16:24.040 --> 16:31.440]  множества две точки, соединяя лежащие во множестве, должны содержать и отрезок,
[16:31.440 --> 16:41.560]  который тоже должен принадлежать этому множеству. Вот. Тут какой-то бак, вот. Можно, в принципе,
[16:41.560 --> 16:49.800]  рассмотреть и более этот. Сложный вариант, но это скорее так, примеры. Вот. Можно, можно поиграться,
[16:49.800 --> 16:55.640]  там где-то убирать границы, где-то не убирать границы. Нас сейчас нет, там главное просто понять
[16:55.640 --> 17:02.200]  суть, что у нас есть отрезок, он должен лежать там. Поиграть и с границами на семинарах. Вот.
[17:02.200 --> 17:08.760]  Окей. Смотрите, на самом деле, определение выпуклой функции и сильно выпуклой функции можно обобщить
[17:08.760 --> 17:15.960]  и не просто на R, ну а сказать, что она выпукла просто на каком-то множестве. Но при этом почему-то
[17:15.960 --> 17:22.600]  нужно всегда добавлять, что множество, на котором вы хотите задать выпуклость, оно выпукло. Вот. Как вы
[17:22.600 --> 17:33.960]  думаете, зачем? Ну да, то есть помните, в чем сам суть и того, и другого определения выпуклости,
[17:33.960 --> 17:40.640]  особенно вот того, который более такой наглядный, что у вас отрезок, соединяющий точки x, f от x,
[17:40.640 --> 17:49.440]  и соответственно y, f от y, он должен лежать выше, чем график функции f и y. Вы же здесь проверяете
[17:49.440 --> 17:55.440]  любые точки вот этого множества, да? Вот. В принципе, вы должны уметь проверять любые точки. И если вы
[17:55.440 --> 18:00.440]  рассматриваете уже не выпуклое множество, вы можете взять и вырезать какой-то кусок отсюда. И здесь
[18:00.440 --> 18:05.880]  с функцией может происходить все, что угодно. Вот. Как вы при этом что-то там собрались минимизировать,
[18:05.880 --> 18:12.280]  вот. Как вы будете там методами в том числе обходить вот эти ямы, ну никто не знает. Соответственно,
[18:12.280 --> 18:18.200]  понятно, что хочется вот такие эффекты выбирать, и поэтому если мы задаем выпуклость уже не на rd,
[18:18.200 --> 18:23.640]  а rd, как вы понимаете, это выпуклое множество, просто потому что оно содержит все векторы,
[18:23.640 --> 18:29.280]  соответственно, и любые отрезки. Вот. То, соответственно, да, можно обобщить при понятии
[18:29.280 --> 18:35.120]  на какое-то любое выпуклое множество. Окей. Здесь я соответственно ввожу еще одно оптимальное
[18:35.120 --> 18:44.880]  условие, когда мы минимизируем функцию не на всем r, а на каком-то выпуклом множестве x. Вот. Ну и,
[18:44.880 --> 18:51.280]  соответственно, условие оптимальности выглядит следующим образом. Доказывать я его сегодня не
[18:51.280 --> 18:55.920]  буду, потому что, в принципе, оно нам не понадобится на сегодняшней лекции. Доказательство его уже будет
[18:55.920 --> 19:02.840]  в пособии. Вот. Давайте просто посмотрим на физический смысл. Вот. Физический смысл по факту означает то,
[19:02.840 --> 19:09.200]  что у вас градиент указывает внутри вашего выпуклого множества. Вот. То есть любой вектор
[19:09.200 --> 19:15.120]  градиента и как бы вектор, соединяющий какую-то точку множества и ту оптимальную точку,
[19:15.120 --> 19:22.200]  которая является решением, они должны между собой иметь острый угол, потому что, в принципе,
[19:22.200 --> 19:27.320]  как раз вам градиент указывает на рост функции, антиградиент вам указывает на то, куда функция
[19:27.320 --> 19:33.560]  убывает. Вот. И понятно, что если вам направление убывания, оно вдруг окажется внутри множества,
[19:33.560 --> 19:39.920]  вы туда и будете убывать. Вот. Значит, вы не нашли ни оптимум. Вот. А так, ну, соответственно, да. То
[19:39.920 --> 19:45.400]  есть, получается, вы лежите на границе и как бы не посмотрели, куда градиент, в какую бы сторону вы
[19:45.400 --> 19:51.360]  там этот вектор не прочертили, который с точкой x, у вас оказывается, что угол между градиентом и
[19:51.360 --> 19:56.960]  этим вектором, он острый. Вот. Значит, что в направлении множества, у вас идут только увеличения.
[19:56.960 --> 20:03.000]  Более формально доказательство, соответственно, будет в пособии и, соответственно, возможно,
[20:03.000 --> 20:09.440]  мы докажем, оно довольно простое, не уверен, что нам понадобится через две лекции даже, возможно,
[20:09.440 --> 20:15.840]  просто в пособии будет. Вот. Окей. Давайте еще чуть-чуть поговорим о том, что мы можем сказать о
[20:16.440 --> 20:20.720]  минимумах выпуклых функций, то есть мы поняли, что если мы нашли значение градиента района нулю,
[20:20.720 --> 20:27.680]  то сразу же у нас глобальный минимум. А вообще вопрос,
[20:27.680 --> 20:33.140]  существует ли локальный минимум выпуклых функций, вот? Ответ — нет. Ответ — нет. Локальных
[20:33.140 --> 20:38.080]  минимумов выпуклых функций не существует. Вот. Если, соответственно, локальный минимум,
[20:38.080 --> 20:41.680]  если мы нашли локальный минимум выпуклых функций, он сразу же является глобальным,
[20:41.680 --> 20:47.720]  что значительно лучше. Докажем это следующим образом. Пусть мы нашли
[20:47.720 --> 20:53.320]  какой-то локальный минимум, рассматриваем вот такую выпуклую комбинацию нашего
[20:53.320 --> 20:59.960]  локального минимума и какой-то точке x из нашего множества x. Что вы можете
[20:59.960 --> 21:06.600]  сказать о точке x, которая здесь у вас написана, x лямбда, что вы можете о ней
[21:06.600 --> 21:11.480]  сказать в силу того, что как мы ее определили.
[21:12.840 --> 21:21.280]  Супер! Почему? Потому что это выпуклая комбинация, x у нас в x, x с звездой тоже в x,
[21:21.280 --> 21:26.120]  просто потому что это решение в этом множестве. Да, мы знаем, что у нас x
[21:26.120 --> 21:33.400]  со звездой, или здесь надо приписать звездочку, она у нас лежит в x.
[21:33.400 --> 21:42.320]  Соответственно, давайте подберем лямбда достаточно малым, чтобы у нас x лямбда
[21:42.320 --> 21:48.440]  попадало не просто во множество x, а попадало еще в ту окрестность, где у нас x
[21:48.440 --> 21:53.280]  со звездой является локальным минимумом. Опять же, в силу того, что у вас вектор x
[21:53.280 --> 21:56.560]  какой-то фиксированный в данном случае, несмотря на то, что вы это брали как
[21:56.560 --> 22:00.200]  произвольную точку для любого, x вы такую лямбду подберете, чтобы вы взяли и
[22:00.200 --> 22:04.800]  попали в окрестность, где у вас x со звездой является локальным минимумом.
[22:04.800 --> 22:13.800]  А не должно быть наоборот, что лямбда уходит в единицу, а он уходит в единицу?
[22:13.800 --> 22:21.600]  А, да, тут скорее лямбда, наоборот, близким к единице. Если бы наоборот написал вот так,
[22:21.600 --> 22:26.720]  вот сюда лямбда, тут один минус лямбда, тогда мало, да, спасибо. То есть понятно,
[22:26.720 --> 22:30.600]  все равно суть, варьируем лямбда так, чтобы у нас в итоге эти две функции,
[22:30.600 --> 22:35.760]  точка x лямбда попала в окрестность, где у нас x со звездой экстрем. Дальше что
[22:35.760 --> 22:40.560]  делаю? Пользуюсь определением, во-первых, локального минимума, то есть у нас
[22:40.560 --> 22:45.360]  значение в x со звездой меньше, чем значение в x лямбда. А дальше пользуюсь
[22:45.360 --> 22:49.720]  выпуклостью, потому что x лямбда это выпуклая комбинация точек. И соответственно
[22:49.720 --> 22:57.920]  что у меня получается? Лямбда f от x и 1 минус лямбда f от x со звездой, так? Вот. Ну и что я
[22:57.920 --> 23:09.240]  из этого могу? Какой вывод сделать? Какой я могу отсюда сделать вывод? Все видят, да, то есть я
[23:09.240 --> 23:15.880]  просто из правой, из левой части вычту 1 минус лямбда f от x со звездой, да. И что у меня
[23:15.880 --> 23:20.520]  останется тогда там с коэффициентом лямбда, но по факту останется то, что f от x меньше либо
[23:20.520 --> 23:26.720]  равно f от x со звездой. Согласны? Супер. Вот. Но в силу того, что мы x брали произвольным,
[23:26.720 --> 23:32.160]  получается, что для любой точки x у нас это выполнено. Значит x со звездой это просто глобальный
[23:32.160 --> 23:40.320]  минимум. Окей. Как мы с вами уже обсуждали, в случае выпуклых функций у вас множество может
[23:40.320 --> 23:46.760]  как содержать решение. Вот. Может оно быть, и решение может быть не единственным. Мы смотрели
[23:46.760 --> 23:55.600]  на функцию x минус y, ну там x1 минус x2 в квадрате. Вот. А может вообще не содержать решение. Помним
[23:55.600 --> 24:00.240]  тоже линейную функцию, которая является, как вы понимаете, выпуклой. В силу опять же определения,
[24:00.240 --> 24:05.880]  понятно, что линейную функцию снизу подпирает линейная функция. Вот. Линейная функция у вас
[24:06.340 --> 24:10.600]  выпуклая, но она всем r, она у вас решение не содержит. Вот. Поэтому выпуклые функции,
[24:10.600 --> 24:17.000]  могут как иметь нулевое множество решений пусто, как так, и в соответствии далеко не
[24:17.000 --> 24:22.860]  уникальное решение. Вот. Но можно доказать, что множество решений для выпуклой задачи,
[24:22.860 --> 24:26.840]  то есть с выпуклой множеством, с выпуклой функцией, тоже является выпуклым множеством.
[24:26.840 --> 24:33.140]  По определению, пустое множество и множество из одной точки
[24:33.140 --> 24:34.140]  понятно выпукло.
[24:34.140 --> 24:37.400]  Первое, это скорее так договоренно считать, что пустое множество
[24:37.400 --> 24:38.400]  выпукло.
[24:38.400 --> 24:40.680]  А второе, понятно, у вас одна точка, вторую точку
[24:40.680 --> 24:43.120]  просто оттуда не возьмете, отрезок содержит одну
[24:43.120 --> 24:44.120]  точку.
[24:44.120 --> 24:46.440]  Интереснее более случаи, когда у вас две точки в
[24:46.440 --> 24:47.440]  этом множестве.
[24:47.440 --> 24:49.520]  Ну и соответственно, что нужно сделать, рассмотреть
[24:49.520 --> 24:53.920]  правильно выпуклую комбинацию этих двух точек.
[24:53.920 --> 24:57.000]  Ну и посмотрим, что там происходит с этими двумя
[24:57.000 --> 25:00.440]  точками в этой выпуклой комбинации.
[25:00.440 --> 25:05.120]  Опять же, в силу выпуклости я знаю, что у меня вот это
[25:05.120 --> 25:08.800]  значение, в этом значении оно меньше, чем оптимальное
[25:08.800 --> 25:09.800]  значение.
[25:09.800 --> 25:13.640]  Просто в силу определения того, что у меня х1 со звездой,
[25:13.640 --> 25:17.280]  х2 со звездой, это какие-то решения.
[25:17.280 --> 25:22.600]  Соответственно, на отрезке значения, либо больше, либо
[25:22.600 --> 25:23.600]  равно.
[25:23.840 --> 25:26.540]  А дальше я расписываю по выпуклости и получаю
[25:26.540 --> 25:28.600]  вот такую комбинацию двух значений.
[25:30.600 --> 25:34.120]  Я же знаю, что у меня значение в х1 со звездой и в х2 со звездой
[25:34.120 --> 25:38.480]  это просто оптимальное значение f со звездой, ну и f со звездой.
[25:38.480 --> 25:40.720]  Ну все, получаю f со звездой.
[25:40.720 --> 25:41.720]  Ну и что получается?
[25:41.720 --> 25:45.140]  Что у меня вот это значение f от х лямбда со звездой
[25:45.140 --> 25:46.920]  подперто с двух сторон.
[25:46.920 --> 25:48.440]  F со звездой.
[25:48.440 --> 25:52.780]  Получается, что оно равно f со звездой.
[25:52.780 --> 25:58.460]  Получается, что любая точечка, любая выпуклая комбинация двух решений также
[25:58.460 --> 26:03.260]  лежит во множестве решений. Получается, что множество решений у нас действительно
[26:03.260 --> 26:11.960]  выпукло. Хорошо. Ну и соответственно, хороший факт то, что в сильно выпуклом
[26:11.960 --> 26:17.660]  случае у вас множество решений состоит ровно из одной точки.
[26:17.660 --> 26:22.620]  Нет, тут в данном случае я пока утверждаю, что если оно вообще решение есть, то
[26:22.620 --> 26:27.180]  оно состоит из одной точки. Ну и соответственно, доказываем от противного.
[26:27.180 --> 26:31.100]  Пусть у нас два решения, которые не равны друг другу. Рассматриваем какую-то
[26:31.100 --> 26:35.660]  выпуклую комбинацию. Причем эта выпуклая комбинация берется так, чтобы у вас
[26:35.660 --> 26:40.340]  она не совпадала ни с одним из крайних значений, просто какая-то точка на
[26:40.340 --> 26:48.500]  отрезке. Вот. И рассматриваю, что происходит с этой функцией нашей функции f в этой
[26:48.500 --> 26:53.620]  точке. То есть она, понятно, больше либо равна, чем значение оптимальное. Дальше я
[26:53.620 --> 26:59.140]  использую определение сильной выпуклости. Видите, тут добавился вот этот кусочек,
[26:59.140 --> 27:05.380]  который из определения сильной выпуклости. Ну а дальше что? Дальше что? Я говорю то,
[27:05.380 --> 27:09.540]  что у меня значение f от x со звездой 11 со звездой, f от x со звездой 2 со звездой,
[27:09.540 --> 27:15.460]  а не эквивалент на f со звездой. Выписал. Остался вот этот кусочек, который, как вы понимаете,
[27:15.460 --> 27:24.180]  какой? Вот этот кусочек что про него стоит? Ну с минусом он будет отрицательным. То есть лямбда
[27:24.180 --> 27:29.780]  мы взяли неравную единицу нулю. Взяли специально так, чтобы были точки. И точки по предположению x
[27:29.780 --> 27:34.260]  11 со звездой и x 2 со звездой между собой не равны. Мы предположили, что решение не единственное.
[27:34.260 --> 27:39.220]  Поэтому вот то, что я выделил, оно отрицательное. Получается вы из оптимального значения вычислили
[27:39.220 --> 27:45.380]  отрицательное, и у вас получается противоречие f от x со звездой меньше, чем f от x со звездой,
[27:45.380 --> 27:51.820]  потому что вычтено что-то явно отрицательное. Все, окей, тогда получили противоречие. Значит,
[27:51.820 --> 27:56.980]  точка всего одна, если она вообще существует. Оказывается, можно и доказать, опять же,
[27:56.980 --> 28:02.260]  будет в конспекте, что такая точка вообще единственна. То есть не просто она одна,
[28:02.260 --> 28:10.260]  она обязательно еще и существует. Доказательства тут довольно тоже интуитивные. В силу того,
[28:10.260 --> 28:16.740]  что у вас, как мы помним в определении сильно выпуклая функция, она у вас подпирается внизу
[28:16.740 --> 28:22.820]  параболы. Давайте я верну на картинку. В выпуклом случае у вас может постоянно вот эта линейная
[28:22.820 --> 28:29.180]  функция опускаться, а в сильно выпуклом случае у вас это парабола, хорошая парабола, которая вас
[28:29.180 --> 28:33.580]  ограничивает. И вы знаете, что у вас функция ограничена этой параболой. То есть там есть
[28:33.580 --> 28:41.420]  ограничение снизу, а значит и будет достижение. Более формально будет в конспекте. Все мы сегодня
[28:41.420 --> 28:48.260]  реально не успеем, потому что нам нужно успеть еще и самый первый метод рассмотреть оптимизации.
[28:48.260 --> 29:03.220]  Окей, так здесь посмотрели, доказали. Хорошо. Дальше еще два факта про сильную выпуклость.
[29:03.220 --> 29:08.740]  Первый нам скорее понадобится для доказательства, и доказывается он довольно тривиально из
[29:08.740 --> 29:17.340]  определения. Раз факт, то есть дифференцируемая функция у нас выпукла, если выполнено вот такое
[29:17.340 --> 29:24.580]  вот соотношение. Это эквалентное определение тому, что у нас было до этого. Можно использовать такое
[29:24.580 --> 29:31.140]  определение, если удобно. Вот мы и в доказательстве в том числе и воспользуемся в дальнейшем. Опять же
[29:31.140 --> 29:36.860]  доказательства в конспекте. Второй кусочек, это вот тоже довольно важный факт, так называемый
[29:36.860 --> 29:43.700]  критерий сильного выпуклости. Для дважды непрерывно дифференцируемая функция. Функция у вас
[29:43.700 --> 29:49.460]  соответственно является сильно выпуклой, если ее гессиан является положительно определенным.
[29:49.460 --> 29:57.340]  Ой, ой, ой, ой, ой, ой, ой, ой, ой, ой, ой, ой, ой, ой. Да, да, да, я извиняюсь.
[29:57.340 --> 30:05.260]  Дозы петто, ой да, Господи, что-то у меня, я поплыл в это определение конкретно.
[30:05.260 --> 30:11.740]  вот так оно должно выглядеть вот я поплыл да здесь в определении на всякий
[30:11.740 --> 30:16.060]  случай зафиксируйте себе или там что вот я понятно конспект как я это поправлю
[30:16.060 --> 30:29.860]  вот вот мю вот оно мю вот я же говорю я тут это конкретно поплыл в двух местах
[30:29.860 --> 30:33.900]  смотреть это положительная определенность положительная
[30:33.900 --> 30:36.700]  определенность то есть у вас матрица гесса должна быть положительно
[30:36.700 --> 30:44.980]  определена вот это критерий и он вам понадобится в домашнем задании вот опять
[30:44.980 --> 30:49.660]  же сегодня уже не успеем его доказать пособие будет он вам скорее вот нужен
[30:49.660 --> 30:55.380]  для проверки некоторых свойств функций в домашнем задании вот я думаю там будет
[30:55.380 --> 30:59.860]  понятно где вам этот критерий может пригодиться вот поэтому мы сегодня на
[30:59.860 --> 31:05.540]  него и смотрим вот чтобы вы его уже знали и могли использовать в своих
[31:05.540 --> 31:11.860]  целях вот окей так все могу перелистывать
[31:11.860 --> 31:20.340]  супер вот так тут как раз про это говорится продезе 2 определение которое
[31:20.340 --> 31:24.540]  нам понадобится это гладкость это гладкость выпуклость это вот по одну
[31:24.540 --> 31:27.980]  сторону то что мы поговорили то что у нас функция ограничена а гладкость это как
[31:27.980 --> 31:33.300]  то не странно будет в другую сторону но сначала это нужно будет понять вот пусть
[31:33.300 --> 31:37.620]  дана опять же у нас непрерывно дифференцируема функция вот давайте
[31:37.620 --> 31:44.140]  пусть будет сразу тоже r вот тогда у нас функция является гладкой
[31:44.140 --> 31:51.020]  л гладкой либо можно эквивалентно говорить именит и липшицев градиент если
[31:51.020 --> 31:56.900]  выполнено вот следующие условия вот то есть вот такого условия некоторые на
[31:56.900 --> 32:01.860]  резкость изменения градиента вот у нас было в прошлый раз условия на
[32:01.860 --> 32:06.740]  резкость изменения функции липшица функции теперь у нас липшицевость
[32:06.740 --> 32:14.820]  непрерывность градиента вот непрерывность градиента вот давайте
[32:14.820 --> 32:18.740]  попробуем разобраться связи с физическим смыслом тут я просто даю
[32:18.740 --> 32:22.340]  некоторую ремарку то что гладкость на самом деле можно может определяться не
[32:22.340 --> 32:28.820]  только во второй норме но и там в любой другой вот нам это сейчас долго еще не
[32:28.820 --> 32:33.060]  понадобится но вот можно про это знать вот давайте докажем вот такое
[32:33.060 --> 32:37.140]  замечательное свойство для гладких функций вот почему я в том числе не
[32:37.140 --> 32:41.220]  доказывал эти там положительная определенность гессиана просто потому что
[32:41.220 --> 32:46.180]  доказательство тех фактов похоже вот на то что я сейчас буду делать поэтому это
[32:46.180 --> 32:51.740]  особо и то есть как бы технику мы поймем потом уже это как бы по аналогии в
[32:51.740 --> 32:55.740]  некотором смысле доделывается ну то есть там главное сейчас вот этот один факт
[32:55.740 --> 33:01.260]  вот окей я что делаю я беру формулу ньютона лебница вы на нее сейчас
[33:01.260 --> 33:06.460]  внимательно смотрите на верхнюю строчку и говорите понимаете вы ее или нет что
[33:06.460 --> 33:15.580]  я написал тут можно сказать что вы не понимаете понимаете ли вот верхнюю
[33:15.660 --> 33:24.260]  строчку да вы что все понимают производная по направлению все правильно то
[33:24.260 --> 33:34.540]  есть смотрите что там что произошло верхней формуле это скорее это ваш третий
[33:34.540 --> 33:43.540]  семестр третий семестр когда вам соответственно определяли определяли
[33:43.540 --> 33:55.180]  интеграл по кривой а б и это соответственно чему равнялась просто
[33:55.180 --> 34:02.860]  значению f в точке rb минус значение в точке r в точке a где r это некоторая кривая
[34:02.860 --> 34:07.420]  вдоль которой вы интегрировали ее можно как-то запереметризовать в данном случае
[34:07.420 --> 34:16.100]  она запереметризована параметром того так вот вот какая у меня кривая у меня
[34:16.100 --> 34:24.620]  кривая довольно простая точка x плюс соответственно направляющий вектор y
[34:24.620 --> 34:30.540]  минус x который как раз и отвечает за то в какую точку я попаду то есть там я
[34:30.540 --> 34:37.900]  варьирую от нуля до единицы соответственно в нуле я в иксе в единице я в игреке то есть я
[34:37.900 --> 34:44.060]  запереметризовал мою кривую как раз вот начальная точка x и вектором направления y минус
[34:44.060 --> 34:50.060]  x все у меня есть кривая r по которой вдоль которой я интегрирую справедливо формула ньютона
[34:50.060 --> 34:57.020]  лебница также вроде как вы скорее всего мотонализе должны были доказывать то что такого рода
[34:57.020 --> 35:03.500]  интеграл и когда у вас есть функция потенциал так называемый есть ее градиент то есть вот то что
[35:03.500 --> 35:07.900]  у вас записано это есть просто градиент некоторого потенциала то такой интеграл
[35:07.900 --> 35:12.820]  вас вообще не зависит вдоль какой кривой вы там интегрируете вот такой факт у вас вроде как был
[35:12.820 --> 35:17.300]  вот но нам он в принципе даже особо и не важен сейчас главное что вот эта формула ньютона лебница
[35:17.300 --> 35:28.820]  она справедливо справедливо вот теперь понятно откуда взялась но я надеюсь туда вот дальше
[35:28.820 --> 35:38.540]  дальше вот во второй строчке что я делаю нет смотрите кривая так как задается если я
[35:38.540 --> 35:42.860]  поделю на модуле у меня будет единичный вектор а мне нужно попасть из точки x точку y
[35:42.860 --> 35:52.860]  на единичный вектор не смотрите у вас же вот в этой формуле вот так вот если и дальше вы еще
[35:52.860 --> 36:12.260]  будете раскрывать у вас так ну давайте выпишем производная так так выписали
[36:12.860 --> 36:23.360]  у
[36:23.360 --> 36:40.780]  y минус x вот так вот детал все так а ну давайте рассуждать нужно ли поделить на y минус x вот
[36:40.780 --> 36:48.940]  у вас же кривая как задается вот с этой формулы вы согласны вот с этой формулы согласны
[36:48.940 --> 36:58.220]  нижний смотреть а хорошо тогда у нас есть кривая в точке соответственно rb и ра но в данном
[36:58.220 --> 37:07.660]  случае rb это просто точка y ра это точка x так правильно вот ну и соответственно вот у
[37:07.660 --> 37:14.380]  меня получается f от x минус f от y ой f от y минус f от x вот ну и я задаю как вот этот вектор вы
[37:14.380 --> 37:20.580]  хотите его отнормировать еще то есть у меня как раз вот здесь точка y здесь точка x я задаю как
[37:20.580 --> 37:30.340]  x tau y минус x вот что будет когда я продиференсирую возьму как раз дифференциал rt у меня чему
[37:30.340 --> 37:43.460]  равен дифференциал rt чему равен вот здесь вот если на это смотрю y минус x надо tau вроде
[37:43.460 --> 37:49.380]  все правильно то есть нормировки никакой не вижу нормировка нас какой-то единичный вектор
[37:49.380 --> 38:00.300]  переведет то есть я не попаду в точку y я попаду на расстояние 1 от точки x направление точки y так
[38:00.300 --> 38:10.060]  окей или нет смотрите то есть у нас же что у нас есть кривая точка x точка y задается вот
[38:10.060 --> 38:17.020]  таким вот уравнением точка условно b которая там вот в этой общей формуле это y точка a это x так
[38:17.020 --> 38:25.340]  я соответственно вот выписываю f от y минус f от x соответственно если я вот здесь вот отнормирую
[38:25.340 --> 38:35.380]  на x минус y по модулю я же не попаду в точку y просто если от tau буду варьировать от нуля до
[38:35.380 --> 38:40.620]  единицы вы можете это нормировку вот у меня вот здесь вот эта нормировка она как бы зашита сюда
[38:40.620 --> 38:48.380]  вы можете вот разделить на x минус y но тогда вот здесь вот и родить вот этот x минус y чтобы
[38:48.380 --> 38:59.380]  вы как раз попали в игре окей всем понятно я надеюсь туда вот так дальше вторая строчка
[38:59.380 --> 39:07.340]  ничего сложного просто вытащил вот это которая в принципе не зависит от tau и здесь я про
[39:07.340 --> 39:13.220]  интегрировал по tau потому что тут никакой зависимости от tau в этом скалярном произведении
[39:13.220 --> 39:16.860]  нет поэтому интеграл от нуля до единицы просто будет давать единицу умножить это скалярное
[39:16.860 --> 39:25.140]  произведение дальше дальше ну я счет получил раз два вот это я сгруппировал поставил сюда
[39:25.140 --> 39:32.360]  модули еще сгруппировал и поставил модуле вот дальше просто равенство и модуль от интеграла
[39:32.360 --> 39:40.260]  модуль от интеграла дальше что интеграл же то получается мне что модуль суммы ну понятно что
[39:40.260 --> 39:47.740]  что сумма модулей больше, больше либо равна, чем модуль суммы. Согласны? Вот. Оценили. Занесли,
[39:47.740 --> 39:57.620]  соответственно, модуль под интеграл. Хорошо. Это прям хорошо. Вот. Занесли модуль под интеграл.
[39:57.620 --> 40:02.220]  Дальше, что я делаю? У меня есть скалярное произведение. Кстати, я надеюсь, все понимают,
[40:02.220 --> 40:07.460]  что вот эти скобочки – это скалярное произведение. Вот. Просто для меня шок был,
[40:07.460 --> 40:14.540]  оказывается. Раньше же вы обозначали типа круглыми скобочками. Я уже привык вот к этим. Вот.
[40:14.540 --> 40:22.420]  И кто-то спрашивает у меня иногда. Вот. Окей. Смотрите. Модуль. У меня стоит как бы разность
[40:22.420 --> 40:28.860]  градиентов. И еще стоит разность аргументов. Ну, вот разность градиентов она как бы как раз
[40:28.860 --> 40:36.820]  намекает на то, чтобы хотелось бы воспользоваться гладкостью по определению. Вот. Для этого я применяю
[40:36.820 --> 40:45.460]  КБШ. Вот. Коши-Бунюковский-Шварц. В векторном виде. Все знают, все помнят. Или хотя бы слышали.
[40:45.460 --> 41:00.140]  Аб меньше либо равно, чем… Просто аб. Согласны, да? Все. Коши-Бунюковский-Шварц. Ничего тут опять
[41:00.140 --> 41:08.540]  сверхъестественного нету. Вот. Вытащилась у меня норма разности градиентов. Вытащилась разность
[41:08.540 --> 41:20.300]  аргументов. Супер. Вот. Пользуюсь гладкостью для вот этого безобразия. И выношу коэффициент.
[41:20.300 --> 41:32.020]  Тут как раз выскочит у меня L у-х в квадрате, потому что вот здесь еще есть у-х. Так? Вот. А под интегралом
[41:32.020 --> 41:39.380]  останется только tau. Вот. Но вот с этим интегралом мы, конечно, справимся уже. Вот. И успешно
[41:39.380 --> 41:50.540]  справляемся. Вот. Все. Вопросы? В принципе, вот те свойства, которые я не доказывал,
[41:50.540 --> 41:58.420]  доказываются похожим образом. Вот. Поэтому я показал только вот это. Вот. Хорошо. Получили
[41:58.420 --> 42:04.700]  замечательное свойство, которое мы с вами доказали. Анонсировали до этого. Вот. Оказывается,
[42:04.700 --> 42:11.740]  для выпуклой функции справедливо оно же, но еще и вот этот модуль можно раскрыть просто по
[42:11.740 --> 42:18.700]  определению выпуклости. Вот. До этого я поставил модуль, но по определению выпуклости подмодульное
[42:18.700 --> 42:23.940]  выражение больше нуля. Вы модуль раскрываете и просто записываете. Опять же, ничего такого
[42:23.940 --> 42:29.140]  сверхъестественного нету. Первое свойство. На самом деле оно хорошо отражает физику. Вы уже на
[42:29.140 --> 42:33.780]  предыдущих картинках это все, в принципе, видели. Сейчас давайте вам картинку покажу. Это заранее.
[42:33.780 --> 42:43.500]  Первое свойство отражает физику. Гладкости. Вот. Выпуклость. Ограничение снизу линейной функции.
[42:43.500 --> 42:49.660]  Сильно выпуклость. Ограничение снизу не просто линейной функции, а какой-то еще и дополнительно
[42:49.660 --> 42:58.340]  подпертый квадратичной. Вот. Гладкость. Это уже ограничение сверху. Тоже квадратичной функции. Вот.
[42:58.340 --> 43:06.540]  Ну и получается, что вот как раз в сильно выпуклом случае у вас по факту функция как бы в некотором
[43:06.540 --> 43:13.460]  смысле подпирается двумя параболами и как-то между ними себя ведет. Вот это все. Вся физика того,
[43:13.460 --> 43:21.980]  какого рода класс задач рассматривается. В выпуклом случае, соответственно, подпирается не параболой,
[43:21.980 --> 43:29.060]  но у него там коэффициент квадратичный равен нулю. Вот. Второе свойство. Оно нам скорее понадобится для...
[43:29.060 --> 43:39.820]  Так, пара 12. 10, да? Вроде нормально. Вот. Второе свойство. Оно нам понадобится как раз больше для
[43:39.820 --> 43:47.820]  доказательств, чтобы бороться как раз с разностью градиентов. Первое оно скорее про физику,
[43:47.820 --> 43:55.180]  про физику того, что происходит. Второе свойство про, скорее, какое-то техническое свойство,
[43:55.180 --> 44:01.340]  которое понадобится больше для доказательств. Вот. Хорошо. Тут просто про первый факт сказано,
[44:01.340 --> 44:09.300]  мы это обсудили. Вот. Давайте про второй. Смотрите, чтобы доказать второй факт, рассмотрим вот такую
[44:09.300 --> 44:15.740]  вот не совсем обычную функцию. Какую вспомогательную функцию phi, которая представляет собой f от y,
[44:15.740 --> 44:19.820]  и вот такой вот градиент умножить на вектор y. Градиент при этом фиксированный,
[44:19.820 --> 44:27.180]  точка x фиксирована. Вопрос. А вот является ли это вообще функция гладкой, с какой-то константой
[44:27.180 --> 44:40.860]  липшица. Вот. И выпуклой. Как вы думаете? Я вот ее рассмотрел зачем-то. Вообще она выпуклая и гладкая или нет?
[44:40.860 --> 44:53.180]  Ну что нужно делать, чтобы проверить? У нас не так много вариантов. Вот. Давайте по
[44:53.180 --> 45:09.980]  определению лупанем. Вот. Так. Что там в градиенте у phi будет стоять? Давайте только тут точки,
[45:09.980 --> 45:16.660]  пусть будут, чтобы x не употреблять, я везде y поставлю. Что там в градиенте у phi будет? Расскажите
[45:16.660 --> 45:34.260]  мне. Люди, которые сегодня будут писать. Отлично. Что еще? А у скалярного произведения
[45:34.260 --> 45:42.300]  какой градиент? При фиксированном векторе. У какого скалярного произведения будет градиент
[45:42.300 --> 45:55.220]  при фиксированном f от x? Градиент f от x. Сколько у скалярного произведения будет? Вот у этой
[45:55.220 --> 46:04.700]  просто функции. Давайте продиференцируем функцию a от x одномерно. Сколько будет градиент? Сколько? А. А здесь сколько?
[46:04.700 --> 46:15.900]  Левая часть. Все правильно. Я f от x. По-моему, это базовая вещь, которую вам должны были
[46:15.900 --> 46:22.860]  упомянуть на семинаре. Градиент, условно, линейные функции, это просто вот кусочек градиента. Вот.
[46:22.860 --> 46:28.020]  Это же по определению просто показывается. В разности расписывать, у вас градиент остается. Вот.
[46:28.020 --> 46:39.540]  Что? И вот здесь у вас что будет? Соответственно, минус f от x и плюс f от x. Один у вас вылез
[46:39.540 --> 46:47.900]  с минусом, когда брали градиент по phi y1, а второй с плюсом вылез, когда брали по y.
[46:47.900 --> 46:55.620]  Потому что он не меняется, он сокращается. Правда, он сокращается. Вот. Он сокращается. И у вас остается
[46:55.620 --> 47:01.980]  просто что? Что константа липчется, эквивалент на какой константе липчется? Для функции f.
[47:01.980 --> 47:21.240]  В первом? Тут можно и равенство поставить, я согласен. Все. Ну в принципе было же верно. А в
[47:21.240 --> 47:32.040]  первом взяли градиент. Взяли градиент. Градиент phi от y равен чему? Градиент f от y минус
[47:32.040 --> 47:41.800]  градиент f от x. Все. Мы это подставили туда. Все. Хорошо. То есть мы нашли константу липчется.
[47:41.800 --> 47:48.800]  Нашли константу липчется. Вот. Является ли эта функция выпуклой? Да, является. Я уж не буду это
[47:48.800 --> 47:55.200]  проверять. Тоже проверяется по определению. Вот. Градиент вы уже научились брать. Вот вы знаете
[47:55.200 --> 48:03.200]  вот этот градиент. Его можно подставить в определение функций. И все будет нормально. Вот. Все. Тут я
[48:03.200 --> 48:08.560]  пишу как раз константа липчеца совпадает. Просто проверяем по определению. Я вот тогда уж надеюсь,
[48:08.560 --> 48:12.960]  что вы это будете проверять по определению как упражнение. Потому что если вот тут уже какие-то
[48:12.960 --> 48:18.840]  вопросы с градиентом возникли у довольно простой функции. Вот. То нужно что-то дома тоже попробовать
[48:18.840 --> 48:25.040]  порешать. Вот. Тем более вас сегодня что-то спросят. Как посчитать градиент? Вот дадут вам линейную
[48:25.040 --> 48:35.400]  функцию, а вы так о. Также что можно заметить? Смотрите. А y со звездой равный x, это минимум,
[48:35.400 --> 48:46.280]  оказывается, нашей функции phi. А почему? Почему? Градиент равен нулю. Все правильно. Градиент
[48:46.280 --> 48:58.000]  в этой точке y со звездой равный, соответственно, f от y со звездой минус f от x равен нулю. Если
[48:58.000 --> 49:07.040]  я действительно подставлю сюда x, будет 0. Все получается хорошо. Вот. Как раз все правильно.
[49:07.040 --> 49:13.880]  Градиент равен нулю. Воспользуемся первым утверждением. Раз мы его с вами доказали. Давайте им
[49:13.880 --> 49:20.920]  воспользуемся. Вот оно первое утверждение про гладкость. Вот. Но тут я подставляю довольно хитрое.
[49:20.920 --> 49:26.400]  То есть вот тут y это вот для первого утверждения. Здесь x это тоже для первого утверждения.
[49:26.400 --> 49:31.280]  Подставляю вот соответственно y для первого утверждения вот таким, x вот таким. Ну и соответственно
[49:31.280 --> 49:38.120]  вместо функции f я подставляю теперь уже мою функцию phi. Вот. Ну и все. То есть вот раз f от y это,
[49:38.120 --> 49:45.960]  вот это f от x. Дальше скалярное произведение выписывается. Разность между y и x она понятна.
[49:45.960 --> 49:53.920]  Она будет просто минус 1 делить на l градиентов от y. Вот. Ну и здесь все. Вот этот кусочек. Зная
[49:53.920 --> 49:59.200]  константу липшица. Я его здесь выписываю. Окей? Все понятно вроде, да? Не должно быть.
[49:59.200 --> 50:12.600]  Так. Вопрос есть или нет здесь? Вот. А теперь смотрите. Я вот в этом скалярном произведении
[50:12.600 --> 50:23.800]  что могу найти? Вот в этом скалярном произведении. Что это? Один и тот же вектор на себя. Это модуль,
[50:23.800 --> 50:30.160]  да, конечно. Только с коэффициентом 1 делить на l. Так. Вот. Поэтому в правой части тоже что-то
[50:30.160 --> 50:37.240]  стоит похожее. Коэффициентики можно поубирать. Ну там соответственно 1 делить на 2 l. Тут будет l,
[50:37.240 --> 50:45.160]  остается 1 делить на 2 l. Все. Все. Просто перестановочка. Получилась перестановочка. Я
[50:45.160 --> 50:50.600]  выписал то, что получается. Здесь вот это значение в моей точке y, которое я взял. Это мы то,
[50:50.600 --> 50:55.360]  что получили на предыдущем слайде. Но я говорю то, что это меньше, чем значение в оптимуме.
[50:55.360 --> 51:01.720]  Понятно. Потому что какая тут произвольная точка, а тут значение в целом оптимуме. А здесь вот x это
[51:01.720 --> 51:12.840]  значение. Поняли, что оптимум это x. Все. Супер. Тогда можно просто подставить вместо phi вот в эти два
[51:12.840 --> 51:20.080]  неравенства реальное значение. Чему же это phi равно? И получить то, что в принципе мы и хотели.
[51:20.080 --> 51:28.120]  То, что мы и хотели. Вот. Супер. Вот такое вот свойство. Я тут переписываю уже в том виде,
[51:28.120 --> 51:34.880]  в котором я его сформулировал в теореме. Как им пользоваться, мы уже увидим дальше. Вот.
[51:34.880 --> 51:45.120]  Сейчас? Ну если у вас получается. Супер. Вот. Никто не успевает. Помедленнее.
[51:45.120 --> 52:00.560]  Хорошо. Хорошо. Давайте помедленнее пойдем. Вот. Так. Ну я вроде комментирую то, что там происходит,
[52:00.560 --> 52:09.840]  куда мы что передвигаем. Вот. Вот можете проверить вот этот переход, что я все переставил тут верно. Вот.
[52:15.120 --> 52:26.640]  Я могу хоть эти выкладки делать сам рукой, потом они у вас просто окажутся в готовом виде. Давайте
[52:26.640 --> 52:34.440]  делать так в следующий раз. Я буду рукой писать. Вот. Давайте так тогда. В следующий раз давайте так.
[52:34.440 --> 52:45.120]  Вот. Хорошо. Соответственно получилось свойство, которое понадобится нам в дальнейшем. Вот. Тут у
[52:45.120 --> 52:50.800]  меня конечно был вопрос, а где мы воспользовались выпуклостью? Потому что вроде как вот в этом
[52:50.800 --> 52:56.280]  неравенстве, когда я делал переход, я пользовался только тем, что мы в принципе доказывали в
[52:56.280 --> 53:00.880]  произвольном случае. Когда доказывал вот это для гладкости, я ставил модуль и там не было,
[53:00.880 --> 53:09.160]  никак не использовалось то, что функция выпукла. Это потом мы его раскрыли положительно. Все. Супер. Мы
[53:09.160 --> 53:14.720]  пользовались тем, что градиент равен нулю, и поэтому мы говорили, что это минимум. Глобальный
[53:14.720 --> 53:24.760]  минимум функций. Вот. Вот здесь мы и пользовались выпуклостью. Вот. Хорошо. Так. Вот здесь есть ответик
[53:24.760 --> 53:31.360]  на этот вопросчик. Физический смысл, который мы с вами обсудили. То, что у нас гладкость
[53:31.360 --> 53:36.040]  ограничивает сверху. Вот. Ну и здесь соответственно сильная выпуклость и то же самое. То есть у нас
[53:36.040 --> 53:44.080]  есть ограничение снизу, сильная выпуклость и ограничение сверху через это. Вот. Окей. Переходим.
[53:44.080 --> 53:52.320]  Наверное главной ключевой темой сегодня это градиентный спуск. Метод, который в принципе зародился.
[53:52.320 --> 53:55.880]  Первый метод оптимизации, который вообще появился, придумал его Коши для решения
[53:55.880 --> 54:01.520]  соответственно системы линейных равнений. Это мы немного с вами обсуждали в прошлый раз. Вот.
[54:01.520 --> 54:09.240]  Рассматривается. Безусловная задача оптимизации множество оптимизации РД. Нет никаких ограничений
[54:09.240 --> 54:16.680]  всего пространства. Вот. В чем идея? В чем идея? Как мы знаем, что у нас производный или градиент
[54:16.680 --> 54:22.520]  просто указывают в направлении возрастания функции. Соответственно антипроизводный или
[54:22.520 --> 54:29.360]  антиградиент указывают, будьте здоровы, в направлении убывания функции. Вот. Ну и соответственно
[54:29.360 --> 54:34.720]  возникает мысль. Если нам градиент локально говорит, куда функция убывает, ну давайте я пойду вдоль
[54:34.720 --> 54:39.280]  градиента. Ну вот соответственно это и есть реализация. То есть у нас есть какая-то текущая
[54:39.280 --> 54:44.840]  точка, мы считаем в ней градиент, делаем какой-то шаг вдоль этого градиента, получаем новую точку.
[54:44.840 --> 54:51.760]  Вся идея. Очень просто, очень понятно. Вот. А теперь нужно понять немного смысл. Давайте
[54:51.760 --> 55:00.680]  на всякий случай вы мне скажете, куда указывает градиент в точке x1. Перпендикулярно это прямой.
[55:00.680 --> 55:10.640]  Сюда или сюда? Вот здесь оптимум у нас. Наружу. То есть функция увеличивается сюда. Вот. Здесь как
[55:10.640 --> 55:15.880]  раз изображены линии уровня, что вот здесь функция меньше, здесь больше становится. Увеличение
[55:15.880 --> 55:21.880]  функции идет вот сюда. Вот. Соответственно градиент у нас указывает наружу. Вот. А антиградиент как
[55:21.880 --> 55:29.760]  раз наоборот внутрь. Вот вся и суть. Используя какие-то локальные свойства толкаться к решению. Вот.
[55:29.760 --> 55:36.200]  Вот направление роста. Соответственно в направлении убывания мы и пойдем. Вот. Возникает вопрос, а
[55:36.200 --> 55:42.560]  зачем вообще в градиентном спуске нужен шаг? Вот у нас есть какая-то функция, есть стартовая точка.
[55:42.560 --> 55:49.280]  Я зачем-то в градиентном спуске добавлял шаг. Например, функция квадратичная x2. Понятно,
[55:49.280 --> 55:55.720]  у меня там производная будет просто x. Ну давайте по ней будем ходить. Там x-гамма kx. Вот. Конечно,
[55:55.720 --> 55:59.680]  понятно, что если взять гамму равной единичке, мы сразу в оптимум придем. Ну что будет,
[55:59.680 --> 56:06.440]  если мы будем менять гамму от маленьких до больших значений? Да. Если гамма маленькая,
[56:06.440 --> 56:12.120]  мы будем как-то медленно-медленно вот так потихонечку шаг за сожжечком спускаться. Это вроде как долго.
[56:12.120 --> 56:21.800]  Вот. Вроде как долго. Если мы возьмем гамма довольно большим, то есть мы можем как-то за
[56:21.800 --> 56:28.000]  несколько шагов, а то и за один шаг дойти до решения. Вот. Мы можем взять гамма довольно большим,
[56:28.000 --> 56:34.960]  что мы будем в некотором смысле скакать вот так вот из разных кусков параболу. Так. А может
[56:34.960 --> 56:40.920]  произойти вообще ужасная ситуация, что мы настолько доверились градиентному спуску,
[56:40.920 --> 56:46.040]  мы настолько доверились градиенту, что он нам верно отражает какие-то локальные свойства функций.
[56:46.040 --> 56:53.280]  Ну и предположили, что эти локальные свойства хорошо апроксимируют всю функцию, что взяли точку,
[56:53.280 --> 57:00.080]  взяли огромный шаг, ну и начали вот так вот шагать. Понятно, вы разойдетесь. Вот. Видите,
[57:00.080 --> 57:04.680]  в чем суть? То есть суть градиентного спуска, что вы по факту рассматриваете эту апроксимацию,
[57:04.680 --> 57:12.120]  рассматриваете апроксимацию первого порядка вокруг вот точки. Вот. Соответственно,
[57:12.120 --> 57:19.680]  у вас здесь градиент будет стоять, здесь будет стоять расстояние. Вот. Апроксимация первого
[57:19.680 --> 57:24.640]  порядка может быть нехорошей. То есть да, для каких-то функций она может быть близка к реальности,
[57:24.640 --> 57:29.680]  для каких-то, ну вы понимаете, что функция ведет себя лучше. Вот. Или наоборот, хуже, чем вот эта
[57:29.680 --> 57:36.360]  апроксимация. Поэтому нужно уже учитывать, что вот это локальные свойства градиента, они имеют
[57:36.360 --> 57:44.920]  свойства меняться вообще с точкой. Поэтому большие шаги могут вас ввести совсем далеко. Вот. Ну и давайте
[57:44.920 --> 57:52.400]  попробуем доказать сходимость градиентного спуска. Вот. Ну и давайте я вот тут уже буду руками
[57:52.400 --> 57:59.400]  попробуем доказывать. Тут что-то я уже выписал. Вот. Но тут в принципе все понятно будет. Я не
[57:59.400 --> 58:05.200]  показываю результат, потому что мы пока, ну я вам в прошлый раз показывал теоремы, где мы по факту
[58:05.200 --> 58:10.200]  там оценки какие-то получали уже. Зачем их сразу показывать, если по факту результат становится
[58:10.200 --> 58:14.720]  понятен уже после того, как мы что-то доказали. Здесь что хочется сделать? Давайте рассматривать
[58:14.720 --> 58:22.400]  гладкие и мюсельно выпуклые задачи. Знаем, что оптимум уникален. Так. Ну и что? Посмотрим, насколько
[58:22.400 --> 58:30.880]  мы к нему приближаемся. Вот. Каждую итерацию. Что я делаю? Я подставляю шаг градиентного спуска.
[58:30.880 --> 58:38.480]  Окей? Все. Пока здесь ничего такого. Дальше я раскрываю этот квадрат. Раскрываю этот квадрат.
[58:38.480 --> 58:47.360]  Раз-два. Уходит в одну скобку. Вот этот кусочек уходит в другую скобку. Плюс удвоенное произведение.
[58:47.360 --> 58:54.280]  Все. Тут ничего такого сверхъестественного. И у меня к вам вопрос. А что мы будем делать дальше?
[58:54.280 --> 59:02.800]  Что мы будем делать дальше? Вот. Пока я сделал какие-то самые простые манипуляции. Что у нас есть,
[59:02.800 --> 59:09.360]  чтобы можно было что-то пооценивать. Я же не зря предположил гладкость и сильную выпуклость.
[59:09.360 --> 59:18.080]  Вот. Давайте как-то попробуем их использовать. Конечно. Давайте попробуем неравенство применить.
[59:18.080 --> 59:26.080]  Вот. Смотрите. Норма градиента. Как оценим? Что у нас было в гладкости? Какие может быть свойства
[59:26.080 --> 59:30.440]  в гладкости? Какие может быть свойства в выпуклости? Сильные выпуклости были, чтобы это можно было
[59:30.440 --> 59:41.560]  оценить. Норма градиента. Где мы встречали с вами норму градиента в квадрате? Через константу
[59:41.560 --> 59:47.640]  сильной выпуклости. Где мы встречали норму градиента, чтобы можно было сверху оценить? Мы же
[59:47.640 --> 59:53.440]  хотим сверху оценить, насколько мы вообще хорошо приближаемся. Где мы в гладкости встречали?
[59:53.440 --> 01:00:01.760]  Помните, как раз у нас было вот так. Вот. Но здесь у нас точка одна. А вторую где взять?
[01:00:01.760 --> 01:00:09.480]  Ноль. Правильно. Не зря же мы это условие оптимальности сегодня обговорили. Вот.
[01:00:09.480 --> 01:00:25.800]  Давайте добавим этот умный ноль сюда. Так. Хорошо. И с этим мы уже понимаем, как действовать. Так.
[01:00:25.800 --> 01:00:36.520]  Я взял и расписал по определению. L гладкости. Согласны? Вот. С этим кусочком что-то тоже нужно
[01:00:36.520 --> 01:00:49.480]  делать. А что? Вот L гладкость мы уже пользовали. Надо бы сильную выпуклость использовать. Давайте я
[01:00:49.480 --> 01:01:04.880]  вот здесь делаю так. Я тоже добавлю умный ноль. Вот. Так. Умный ноль добавлен. И помните, у нас было
[01:01:04.880 --> 01:01:10.960]  такое свойство. Я вам упоминал в дыхе того, что нам оно понадобится в доказательствах. Не доказывал.
[01:01:10.960 --> 01:01:21.560]  Из определения следует. Вот. Что здесь это можно оценить как mu x k-x со звездой. У кого там открыты
[01:01:21.560 --> 01:01:25.200]  слайды на компьютере, можете как раз отмотать на те свойства, где как раз была положительная
[01:01:25.200 --> 01:01:36.360]  определенность десиана. Вот. И посмотреть, что в таком виде у вас там есть. Игры? Все норм. Шаг?
[01:01:36.360 --> 01:01:47.760]  Ой, да-да-да. Все. Шаг появился. Супер, да? Все вроде норм. Сейчас улавливаете, что происходит. Все. Так.
[01:01:47.760 --> 01:01:54.720]  Давайте это все сделаем красиво, на слайде соберем. Вот. Я здесь это все выписываю, конечно. Те
[01:01:54.720 --> 01:02:00.320]  свойства, которые вам нужны. А условия оптимальности, которые мы с вами добавили. И соответственно
[01:02:00.320 --> 01:02:06.840]  начинаю с этим всем играться. Вот они, выписанные вещи, которые мы только что с вами получили. И
[01:02:06.840 --> 01:02:12.880]  дальше их просто группирую. Понятно, что у меня везде теперь x-каты, x-каты, x-каты. Минус x со
[01:02:12.880 --> 01:02:23.320]  звездой. Все. В одну скобочку. Получили вот такую вот скобочку, как здесь. Что дальше? Что хочется?
[01:02:23.320 --> 01:02:34.600]  Предположение индукция. Ну, а чего вам вообще от этой скобочки хочется? Зачем? Супер. Видно,
[01:02:34.600 --> 01:02:41.520]  что мы по факту получили рекуррент в зависимости нового расстояния от предыдущего. Для любой
[01:02:41.520 --> 01:02:46.520]  или гладкой сильно выпуклых функций. То есть, какую бы мы функцию не взяли, вот это будет верно,
[01:02:46.520 --> 01:02:54.000]  что градиентный спуск вот такое гарантирует. Так? Верхняя оценка. Вот. Хочется приближаться к
[01:02:54.000 --> 01:02:59.440]  решению. Вы согласны, чтобы расстояние уменьшалось? Поэтому вот мы говорим, что вот эта скобочка должна
[01:02:59.440 --> 01:03:10.040]  быть строго меньше единицы. Согласны? Супер. Вот. Мы хотим соответственно меньше единицы. Подбираем.
[01:03:10.040 --> 01:03:18.360]  Что будем делать? Как подобрать, чтобы она была меньше единицы? Вот. Как подобрать? Парабола такая.
[01:03:18.360 --> 01:03:26.960]  Что это у нас вообще с точки зрения гамма? Квадратная неравенствия. Это парабола с ветвями.
[01:03:26.960 --> 01:03:35.280]  Куда? Вверх. Вот. У нее есть где-то минимум. У нее есть где-то минимум. Вот. Как минимум в нуле это равно
[01:03:35.280 --> 01:03:40.680]  единицы, и скорее всего есть минимум, который даст значение меньше единицы. Ну сколько у нее там
[01:03:40.680 --> 01:03:49.320]  минимума? Какой точкой у нее минимума будет? Сейчас. Берем производную, получаем тут двоечка еще
[01:03:49.320 --> 01:03:56.360]  вылезет, это уйдет, это уйдет. Получится mu делить на l в квадрат. Правильно посчитал производную? Я
[01:03:56.360 --> 01:04:04.560]  уже старый, вы сейчас мне подскажете. Правильно, да? Скорее всего, да. Вот. Ответик. Соответственно,
[01:04:04.560 --> 01:04:12.280]  да. mu делить на l в квадрате. Ну дальше я это все подставляю просто в шаг. И получаю, что вот
[01:04:12.280 --> 01:04:22.000]  эта скобочка у меня отображается как 1 минус mu в квадрате l в квадрате. Согласны? Супер. То есть
[01:04:22.000 --> 01:04:27.200]  получается все, мы уже начинаем выедать, мы начинаем приближаться, у нас расстояние до решения
[01:04:27.200 --> 01:04:34.040]  меняется, причем уменьшается, гарантированно уменьшается. Вот. Записано это все? Безобразие. Что дальше?
[01:04:34.040 --> 01:04:43.680]  Ну можно просто запустить рекурсию. Сказать, что я теперь знаю, что у меня x-каты там, ну зачем
[01:04:43.680 --> 01:04:47.960]  уже предполагать индукция, по факту она все доказана. Вот. Ну да, это можно называть индукцией,
[01:04:47.960 --> 01:04:53.640]  но я могу сейчас уже запускать рекурсию и вот здесь вот соответственно вытаскивать сначала,
[01:04:53.640 --> 01:05:01.040]  например, x-ката минус 1 x звездой. Тут у меня соответственно степень 2 возникнет. И так далее.
[01:05:01.040 --> 01:05:09.680]  Дальше-дальше-дальше-дальше запускаю. Вот. И дохожу до вот такого. Вот. За ка шагов я вот выеду
[01:05:09.680 --> 01:05:15.560]  столько. А давайте вы мне расскажете. Я вчера как раз посмотрел, как вы написали тесты с точки
[01:05:15.560 --> 01:05:21.040]  зрения про лекционные вопросы, какая это скорость сходимости? Линейная. То есть скорость геометрической
[01:05:21.040 --> 01:05:28.280]  прогрессии, когда вы приближаетесь к решению, выедая показатель геометрической прогрессии. Это
[01:05:28.280 --> 01:05:35.040]  линейная скорость сходимости. На графике я напоминаю, где у вас логарифмический масштаб по оси x,
[01:05:35.040 --> 01:05:41.280]  ой, по оси y, вы как раз имеете линию, поэтому она и называется линейной. Вот. Это линейная скорость
[01:05:41.280 --> 01:05:48.600]  сходимости. Супер. Вот. Дальше, соответственно, что? Дальше хочется получить из этого всего
[01:05:48.600 --> 01:05:54.840]  безобразия число итерации. Сколько мне нужно сделать итерации, чтобы гарантированно дойти до
[01:05:54.840 --> 01:06:01.120]  решения? Вот. Хочу я, например, получить какую-то точность epsilon. Ну вот. Хочу я гарантировать,
[01:06:01.120 --> 01:06:09.600]  чтобы у меня xкат-x звездой было меньше epsilon. Меньше либо равно от точности epsilon. Вот. Сколько
[01:06:09.600 --> 01:06:16.520]  мне нужно сделать итерации тогда? Да. То есть смотрите, в чем суть. Я говорю то, что... я хочу,
[01:06:16.520 --> 01:06:22.120]  чтобы вот это гарантированно. Значит, я хочу, чтобы вот моя оценка, которую я получил, я подбирал k
[01:06:22.120 --> 01:06:27.320]  исходя из того, что оценка будет обязательно меньше epsilon. Тогда k подберется всегда так,
[01:06:27.320 --> 01:06:36.240]  что epsilon будет достигаться до точности epsilon. Вот. Отлично. Тут единственная проблема в том,
[01:06:36.240 --> 01:06:45.240]  что... вот это логарифмировать сложно. Ну, какой-то показатель, какая-то степень непонятная. Я предлагаю
[01:06:45.240 --> 01:06:51.280]  тут перейти к экспоненте. Вот. Ну, таким образом, что у вас 1-x меньше либо равно, чем экспонента
[01:06:51.280 --> 01:06:58.600]  в степени минус x. Кто понимает, откуда это свойство берется? Для x, которыми там от нуля до единицы.
[01:06:58.600 --> 01:07:07.680]  Это же просто разложение экспонента. У вас как там 1, дальше как раз первая степень минус x,
[01:07:07.680 --> 01:07:15.760]  дальше уже будет квадрат. Вот здесь вот будет возникать 1-x. Дальше x квадрате пополам. Вот. А
[01:07:15.760 --> 01:07:22.040]  здесь x квадрата уже нету пополам. Поэтому правой части нам больше. Вот. Простое свойство из Тейлора.
[01:07:22.040 --> 01:07:31.080]  Вот. Такое. Техническое. Здесь все норм? Все. Тогда вот уже здесь прологарифмировать значительно
[01:07:31.080 --> 01:07:36.360]  проще. Логарифмировать значительно проще. Можно взять натуральный логарифм. Я как раз требую вот
[01:07:36.360 --> 01:07:41.480]  то, что мы с вами обговорили, степень меньше epsilon. Но я там, соответственно, писал без этих квадратов.
[01:07:41.480 --> 01:07:49.040]  Здесь квадраты появились, я их теперь добавил. Вот. Прологарифмировал и получил, что у меня k должно
[01:07:49.040 --> 01:07:54.440]  быть больше, чем l в квадрат, делить на mu в квадрат. Ну там можно этот логарифм. Вот. Такое число
[01:07:54.440 --> 01:08:00.080]  итераций, которое мне необходимо до достижения. Первая наша оценка на метод, который мы с вами
[01:08:00.080 --> 01:08:11.240]  получили. Окей? Супер. Вот. Но вообще проблема в том, что это не очень хорошая оценка. Вот. Я вас
[01:08:11.240 --> 01:08:18.320]  повел не туда изначально. Просто чтобы показать, что в принципе вот эти верхние оценки, как нижние,
[01:08:18.320 --> 01:08:24.320]  можно получать довольно грубо. Ну то есть сейчас я загрубил. Оценка нехорошая. Можно лучше,
[01:08:24.320 --> 01:08:32.960]  мы сейчас сделаем лучше. Вот. Вот. Ну и получается, что так, что можно иногда реально грубо оценить,
[01:08:32.960 --> 01:08:36.640]  получить какую-то скорость сходимости, расстроиться, что метод работает так на практике
[01:08:36.640 --> 01:08:42.520]  значительно лучше, чем здесь. Вот. Ну соответственно с этим надо как-то бороться и делать более тонкий
[01:08:42.520 --> 01:08:50.400]  анализ. Сейчас давайте попробуем этот тонкий анализ и сделать. Вот. Хорошо. Вернулись ровно туда,
[01:08:50.400 --> 01:09:00.040]  откуда начали. Все, что я расписывал, вот здесь я просто раскрыл квадраты, как я напоминаю. И здесь
[01:09:00.040 --> 01:09:08.440]  я в одном месте добавил f от x градиент в f от x звездой. Наш 0. Так. Супер. Давайте чуть-чуть
[01:09:08.440 --> 01:09:13.400]  вспоминать другие свойства, которые были. Я же не зря для гладкости тоже свойства доказывал.
[01:09:13.400 --> 01:09:18.960]  Вот. Специально потратил время на свойства, которые на нормы градиента. Отмотайте себе и
[01:09:18.960 --> 01:09:24.840]  расскажите, как мне оценить вот эту. Вот здесь вот. В конспекте найдите, как вот это оценить. И я
[01:09:24.840 --> 01:09:30.760]  даже его выписал. А, вот. Даже выписал. Ну ладно, давайте вы мне расскажете. Вот. Как вот его оценить.
[01:09:30.760 --> 01:09:42.920]  Давайте, давайте. Не смотать. Не, так не интересно. Давайте вы, наоборот, верхов натайте. Вот. Мы ж
[01:09:42.920 --> 01:09:47.320]  как раз договорились, что будете сейчас это не сами подсказывать, рассказывать, и мы будем в режиме
[01:09:47.320 --> 01:09:52.720]  онлайн все выводить. Что с подсказками-то делать? Как оценить вот эту норму разности? Вот. Было
[01:09:52.720 --> 01:09:58.000]  свойство, где у нас как раз была норма разности градиента для l-гладкой выпуклой функции.
[01:09:58.000 --> 01:10:11.840]  Нет, вот определение мы пользовались и получилось плохо. А там были свойства еще. Были свойства,
[01:10:11.840 --> 01:10:20.880]  как раз мы на них потратили время, где нормы градиентов есть. Что там? Рассказывайте. Много букв.
[01:10:20.880 --> 01:10:31.320]  Я запишу, давайте. Как оценить норму разности градиентов? Так, ну что, не нашли, что ли?
[01:10:31.320 --> 01:10:44.600]  Дальше нашли? Ну, продиктуйте уж дальше, как это будет. Ну, это скалярное произведение. Давайте
[01:10:44.600 --> 01:10:50.480]  норму разности градиента. Я уж. Давайте сразу. Что там? Ну, найдите это свойство. Мы же ее там
[01:10:50.480 --> 01:10:57.880]  доказывали. Где-то в районе как раз после l-гладкости, где я доказал сначала для произвольной
[01:10:57.880 --> 01:11:10.120]  гладкой, а потом для выпуклой. Там в теореме было две строчки. 2l, f от x, kt, минус f от x со звездой.
[01:11:10.120 --> 01:11:20.000]  Плюс что там еще? Плюс что-то есть? Все? Ну, давайте диктуйте плюс.
[01:11:20.000 --> 01:11:39.200]  x кт и минус x со звездой? Вот так? Вроде норм, да? Вот. А почему не продиктовал человек,
[01:11:39.200 --> 01:11:47.240]  который посмотрел по ответу? Вот за ответ написан. Ну, давайте кто-нибудь тогда без ответа знает,
[01:11:47.240 --> 01:11:55.800]  почему вот только это останется? Ну, вот градиент же равен нулю, да? Поэтому вот этого всего нет.
[01:11:55.800 --> 01:12:06.160]  Хорошо. Вот. Осталось еще поиграться вот с этим скалярным произведением. Ну, давайте,
[01:12:06.160 --> 01:12:15.200]  что там уж? По выпуклости, сильной выпуклости будем оценивать. Вот. Что там будет? Два гамма к,
[01:12:15.200 --> 01:12:23.400]  mu пополам, xk минус x со звездой. Я просто по определению пишу. Можете прямо проверять,
[01:12:23.400 --> 01:12:33.920]  что это ровно то, что написано в определении. Плюс fxk минус f от x со звездой. Верно все?
[01:12:33.920 --> 01:12:42.480]  Все. Первое. Для скалярного произведения расписали просто по определению, самому первому
[01:12:42.480 --> 01:12:49.000]  базовому определению сильной выпуклости, а вот для норм градиента чуть-чуть поигрались с свойством.
[01:12:49.000 --> 01:13:01.440]  Вот. Получили вот такое вот безобразие. Вот. Выписано оно у меня раз и выписано оно у меня два.
[01:13:01.440 --> 01:13:18.120]  Дальше я группирую. То есть видно, что у меня есть... Zoom закончился что ли? Так, беда какая. Почти
[01:13:18.120 --> 01:13:30.040]  дошли до конца. Интернет может закончился у меня, потому что надо было к Миктон Гэп подключиться,
[01:13:30.040 --> 01:13:44.440]  а я к мобильному своему подключился. Вот. Сейчас посмотрим. Это видимо Zoom. Вот. Так. Это вот мы
[01:13:44.440 --> 01:13:55.680]  с вами только что поняли, как получили. Теперь я просто сгруппировал xk минус x со звездой и f от x минус f от x со звездой.
[01:13:55.680 --> 01:14:05.680]  Ничего сложного, просто манипуляции с алгеброй. Вот. А теперь к вам вопросик. Что хотим-то? Вроде бы
[01:14:05.680 --> 01:14:10.880]  опять к чему-то хорошему идёт. Опять появилась вот этот хороший коэффициентик, который даёт вот
[01:14:10.880 --> 01:14:19.440]  эту сходимость геометрическую. Что-то проблема есть одна. Где? Второе слагаемое, оно может быть как
[01:14:19.440 --> 01:14:24.440]  положительным, так в принципе отрицательным. Вот. Хотелось бы, чтобы оно было отрицательным,
[01:14:24.440 --> 01:14:31.160]  чтобы мы его как раз сверху просто нулём оценили. Как его сделать отрицательным? Гамма-катой подобрать,
[01:14:31.160 --> 01:14:39.680]  причём здесь прямо очень видно, как его нужно подбирать. Как? Гамма-катой меньше, чем один
[01:14:39.680 --> 01:14:45.480]  делительный. И тогда второе слагаемое будет отрицательным, ну, неположительным. И тогда мы
[01:14:45.480 --> 01:14:58.080]  сверху его просто берём и убьём. Всё. Вот. Получили вот такую вот рекурренту. Опять очень похожа
[01:14:58.080 --> 01:15:05.760]  на ту, что у нас была в принципе. Вот. Запускаем. Запускаем. Тут я прям пишу в виде гамма-иты. Но в
[01:15:05.760 --> 01:15:10.200]  итоге подставляю гамма равна 1 делит на л, потому что это максимальный шаг, который мне разрешает
[01:15:10.200 --> 01:15:18.600]  взять теорема. Так. Вот. И получаю вот такую вот рекурренту. Лучше или хуже, чем была? Не рекурренту,
[01:15:18.600 --> 01:15:28.560]  уже результат. Я уже всё раскрыл. Вот. Лучше или хуже, чем была? Вот этот? Вот это, да? Смотрите,
[01:15:28.560 --> 01:15:35.600]  вот эта скобочка, вот это больше либо равно 0, согласно, потому что f от x каты, а это оптимум. Вот.
[01:15:35.600 --> 01:15:44.680]  Это больше либо равно 0. Скобку делаем меньше 0 и убиваем. Вот. Результатик. Лучше или хуже? Выглядит
[01:15:44.680 --> 01:15:49.560]  лучше, потому что там были μ в квадрате или в квадрате. Да? Когда, соответственно, мы сейчас всё
[01:15:49.560 --> 01:15:56.520]  пролагарифмируем. Вот. Здесь я уже всё пролагарифмировал. Ну, посмотрели уже, как это делается. Экспоненты
[01:15:56.520 --> 01:16:02.760]  нужно сделать. Потом у вас получается там этот лагарифм. Вот. Всё. Теперь уже теорема о сходимости,
[01:16:02.760 --> 01:16:08.640]  потому что мы её получили. Вот. Вот такая вот сходимость. И, соответственно, оценка на число
[01:16:08.640 --> 01:16:14.280]  итерации можно записать в виде вот в таком виде. L на μ, лагарифм, или вот там L делить просто на μ.
[01:16:14.280 --> 01:16:19.080]  Здесь я как раз сразу с вас знакомлю с той нотацией, которая принята в численных методах
[01:16:19.080 --> 01:16:26.240]  оптимизации. Это о-нотация, как раз когда мы хотим убивать константы, численные, не вот эти,
[01:16:26.240 --> 01:16:32.640]  параметрически, которые там от свойств функции зависит L, μ, точность решения епсилон. Вот.
[01:16:32.640 --> 01:16:36.640]  А константы нам двоечку, потому что вот здесь вот лагарифм, мне так-то выскакивало двойка,
[01:16:36.640 --> 01:16:41.040]  потому что был квадрат. Я здесь должен был вытащить двойку, она выскочила из лагарифма,
[01:16:41.040 --> 01:16:48.920]  но я её убил о-нотации. Вот. А можно вообще убивать и все лагарифмы. Тоже часто принятая вещь.
[01:16:48.920 --> 01:16:54.240]  Тут нужно добавить ещё тильдочку сверху на до. Если вы видите ось тильдой, то скорее всего убиваются
[01:16:54.240 --> 01:17:01.440]  ещё не только какие-то константы, численные, не параметры функции и точность решения, а убиваются
[01:17:02.000 --> 01:17:08.560]  ещё и лагарифм лишним. Вот такой вот результат L делить на μ. Итерацией нам нужно, чтобы достичь
[01:17:08.560 --> 01:17:19.000]  точности епсилон. Хороший результат. Где, где, где максимум? Ось тильдой, а какой максимум?
[01:17:19.000 --> 01:17:27.040]  Эпсилон. Смотрите, я ещё раз говорю, ось тильдой убирает лагарифмические факторы, в том числе те,
[01:17:27.040 --> 01:17:33.360]  которые зависят от параметров. Потому что лагарифм часто... Сейчас мы увидим по оценкам, что
[01:17:33.360 --> 01:17:44.920]  лагарифм это вещь хорошая. Вот. Часто ситуация хуже. Ось тильдой? Вот. То есть точное определение
[01:17:44.920 --> 01:17:51.720]  в данном случае это просто реальная оценка минус вот эти все константы. То есть это скорее
[01:17:51.720 --> 01:17:56.000]  не совсем так, как у вас в алгоритмах даже определяются. Ось тильдой у вас просто оценка сверху была.
[01:17:56.000 --> 01:18:14.760]  Константы числены? Числены. Ну, значит это даже так же. Потому что, видите, здесь мы чиселки убиваем.
[01:18:14.760 --> 01:18:22.920]  Ось тильдой, и вот вы убиваете. Там ещё получаются любые лагарифмические факторы, которые, соответственно,
[01:18:22.920 --> 01:18:29.160]  ваша реальная оценка. Вот эта оценка умножит на какие-то лагарифмические факторы, любые
[01:18:29.160 --> 01:18:35.320]  лагарифмические факторы. Вот. Как более формально это записать, надо подумать. Просто вот так принято,
[01:18:35.320 --> 01:18:45.360]  что мы убиваем вот стильдой нотации все лагарифмы. Нет, смотрите, просто лагарифм от экспонента,
[01:18:45.360 --> 01:18:51.480]  это вот он уже у вас как раскрывается, как что-то, что аргумент экспонента. Понятно, предполагается,
[01:18:51.480 --> 01:18:59.920]  о чем мы зашевелились, я пока никуда не разрешал, мы еще разговариваем с человеком. Так. Вот. Ну,
[01:18:59.920 --> 01:19:03.960]  понятно, предполагается, что мы убиваем так, что вот у нас уже там есть зависимость духи,
[01:19:03.960 --> 01:19:08.960]  как вот здесь, вот один делить на е или в таком. Вот. Формально я подумал, как это определить,
[01:19:08.960 --> 01:19:13.200]  потому что я справедливости ради в литературе по оптимизации тоже не видел, там всегда пишут,
[01:19:13.200 --> 01:19:18.280]  ну мы убиваем лагарифм. Вот. Окей, я подумаю, потому что самому стало даже интересно,
[01:19:18.280 --> 01:19:23.520]  как это формально определить. Но смысл понятен. Можно, в принципе, лагарифм и оставлять. Вот.
[01:19:23.520 --> 01:19:31.400]  Так. Немного интуиции, доказательства, быстренько. Вот. Смотрите, что на самом деле происходит
[01:19:31.400 --> 01:19:36.400]  в случае градиентного спуска. Когда мы, помните, подбираем шаг, мы подбираем шаг исходя из верхней
[01:19:36.400 --> 01:19:40.720]  аппроксимации, чтобы не сделать довольно большой шаг, потому что, видите, верхняя
[01:19:40.720 --> 01:19:45.400]  аппроксимация довольно резко растет. Вот. И как мы знаем, если функция резко растет,
[01:19:45.400 --> 01:19:51.560]  сделаем большой шаг и улетим. Вот. Поэтому шаг мы подбираем исходя из аппроксимации верхней,
[01:19:51.560 --> 01:20:00.240]  1 на L, чтобы не было вот этой резкости. Но в худшем случае она себя может вести как нижняя граница,
[01:20:00.240 --> 01:20:06.720]  поэтому подбирается шаг как верхняя. Но из-за того, что шаг такой большой, в итоге он может быть
[01:20:06.720 --> 01:20:11.120]  довольно маленький, потому что сам градиент может меняться довольно слабо, потому что как бы
[01:20:11.120 --> 01:20:16.280]  физика может быть и как ни в нижней границе. В итоге вот у нас и получается, кто играет 1 на L,
[01:20:16.280 --> 01:20:21.000]  что как бы шаг исходя из верхней границы, а сходимость в худшем случае исходя из нижней
[01:20:21.000 --> 01:20:26.760]  границы, то есть скорость того, насколько там большой градиент и так далее. Вот. Вот это физика. То
[01:20:26.760 --> 01:20:32.200]  есть вы выедаете как бы поверхней, но сходитесь довольно медленно, потому что mu может быть сильно
[01:20:32.200 --> 01:20:39.880]  меньше, чем L. Вот. Так-так-так-так-так. Здесь я вот как раз это пишу вам. И здесь быстренько оценки
[01:20:39.880 --> 01:20:45.720]  сходимости, чтобы просто понимать картину. Это все будут способии. Мы вывели в одном случае. Теперь
[01:20:45.720 --> 01:20:50.160]  нужно понимать, что происходит в других. Вот это мы с вами вывели. И это оценка, как ни странно,
[01:20:50.160 --> 01:20:55.280]  не улучшаемая. То есть градиент, испуск. Таки вот анализ у нас теперь оптимален. До этого мы получили L в
[01:20:55.280 --> 01:20:59.240]  квадрате делить на mu в квадрате. Это плохо. Вот это уже не улучшаемая для градиентного
[01:20:59.240 --> 01:21:06.880]  спуска. В выпуклом случае вот такая вот оценка. 1 делить на epsilon. Вот. Какая скорее всего эта
[01:21:06.880 --> 01:21:16.320]  сходимость? Хуже это и лучше или лучше, чем у нас было? В сильно выпуклом случае. Хуже. Хуже. Это
[01:21:16.320 --> 01:21:23.080]  сублинейная сходимость. Это сублинейная сходимость 1 делить на k. Вот. Она отсюда пришла. Как раз вы как раз 1
[01:21:23.080 --> 01:21:28.000]  делить на k сказали, что это порядка epsilon, и у вас появилась эта epsilon в знаменателе. Поменяли k и
[01:21:28.000 --> 01:21:34.840]  epsilon местами. Соответственно, вот если у вас так, то тогда у вас k порядка 1 делить на epsilon. Вот.
[01:21:34.840 --> 01:21:39.480]  Сублинейная сходимость в выпуклом случае все хуже. Причем еще и сходимость у вас будет по
[01:21:39.480 --> 01:21:45.280]  аргументу. По функции. То есть здесь вы по аргументу, потому что решение уникально. А там по функции,
[01:21:45.280 --> 01:21:49.720]  потому что решение не уникально. У вас может быть их много, и вы просто сошлись к какому-то из
[01:21:49.720 --> 01:21:55.920]  решений. Но по функции сошлись. По значению функции. Также есть оценки и в невыпуклом случае. Тут
[01:21:55.920 --> 01:22:01.560]  вообще 1 делить на epsilon в квадрате еще хуже. Вот. Но в невыпуклый случай у вас сходимость к
[01:22:01.560 --> 01:22:07.200]  стационарной точке не лучше. Что это за стационарная точка? Ну непонятно. Ближайшая ли она вообще была
[01:22:07.200 --> 01:22:12.920]  к решению? Ну не к решению, а к тому место, где вы стартовали. Непонятно. Хорошая ли она с точки
[01:22:12.920 --> 01:22:18.880]  зрения там глобального, локального минимума? Непонятно. Потому что в невыпуклом случае мы
[01:22:18.880 --> 01:22:25.360]  не можем ничего доказать. Вот. В случае липшицевой функции, когда у вас не липшицев градиент, а липшицевая
[01:22:25.360 --> 01:22:31.960]  функция, тоже вот есть оценки 1 и 2. Вот. Они здесь выписаны. Тоже сублинейной скорости сходимости.
[01:22:31.960 --> 01:22:37.400]  Когда у нас невыпуклая липшицевая функция, мы с вами разбирали все ужасно. Первая лекция, там вот эти
[01:22:37.400 --> 01:22:45.920]  примерчики оттуда. Вот. Смотрите. То есть вот такая ситуация. И что вообще, как дела еще у градиентного
[01:22:45.920 --> 01:22:52.480]  спуска среди методов первого порядка? На самом деле довольно хорошо. То есть градиентный спуск вот
[01:22:52.480 --> 01:22:58.800]  в этом случае. Ой, не вот в этом. Вот в этом случае, вот в этом и вот в этом. Здесь выписаны оценки
[01:22:58.800 --> 01:23:04.600]  только для градиентного спуска, кроме первой лекции. В трех случаях из пяти он оптимален. То есть
[01:23:04.600 --> 01:23:11.880]  среди методов, которые вызывают информацию, градиенты, градиенты. Вот. Лучше не придумать, чем
[01:23:11.880 --> 01:23:17.880]  градиентный спуск. Вот. Вот в случае, соответственно, невыпуклой оптимизации, в случае м липшицевой оптимизации,
[01:23:17.880 --> 01:23:23.120]  то есть для липшицевых функций. Но есть еще два случая, которые мы сегодня как раз разбирали.
[01:23:23.120 --> 01:23:31.320]  Гладкий сильно выпуклый и гладкий выпуклый. И вот там градиентный спуск не оптимален. Его результаты
[01:23:31.320 --> 01:23:35.520]  можно улучшать. Не анализ, но наш анализ оптимален. То есть для градиентного спуска результат уже
[01:23:35.520 --> 01:23:41.560]  не улучшаем. Но есть другие методы, которые работают быстрее. И мы вот уже покажем, что какие-то
[01:23:41.560 --> 01:23:50.240]  методы и насколько они работают быстрее, и почему они оптимальны на следующей лекции. Все. Заканчиваем.

[00:00.000 --> 00:15.000]  Ну что, сегодня состоится лекция, которой мне самому не хватало долгое время, и вот она появилась наконец. Я ее очень рад. Надеюсь, что я вас тоже порадую.
[00:15.000 --> 00:24.000]  Почему ее раньше не было? Потому что она сложная. Не то чтобы она сложная алгоритмически, а как-то очень сложная. Нет, ничего такого не будет.
[00:25.000 --> 00:31.000]  Но в ней много очень больших идей, которые скапливались, скапливались, наконец оформились. И мне кажется, очень хорошо.
[00:31.000 --> 00:39.000]  Помните, когда-то на самой первой лекции я вас спрашивал, зачем нам вообще нужны распределенные системы? Почему они распределенные?
[00:39.000 --> 00:46.000]  Зачем быть такими? И первая причина, наверное, самая очевидная, это отказа устойчивости.
[00:46.000 --> 00:57.000]  Ну, мы не можем доверять конкретной машине. Конечно, она всегда может пересгрузиться, поэтому все, что мы в ней храним, все, что мы делаем, мы должны хранить, видимо, персистентно на жестком диске.
[00:57.000 --> 01:08.000]  И мы такую задачу решали с вами, когда говорили про LSM, про Lockstruct 4.3. Как сделать упорядоченное key value хранилище поверх жесткого диска.
[01:08.000 --> 01:15.000]  Причем хранилище должно быть с произволенным доступом, а жесткий диск умеет только не спеша вращаться и умеет только последовательно доступ эффективно.
[01:15.000 --> 01:18.000]  Вот мы эту задачу решали с помощью LSM.
[01:22.000 --> 01:33.000]  Мы могли пережить рестарту узла. Кроме того, вместе с рестартом любая машина может не перезагнула, выключится, но не включится потом, она может отказать.
[01:33.000 --> 01:44.000]  Поэтому мы надеемся, что если она вдруг поднимется, то на жестком диске данные останутся, но если она не поднимется, то, видимо, нам нужно задублировать ее состояние на другой машине.
[01:44.000 --> 01:47.000]  И для этого мы решали задачу репликации.
[01:53.000 --> 01:55.000]  Давайте я чуть аккуратнее напишу.
[01:55.000 --> 02:03.000]  Задача хранения данных и задача репликации.
[02:03.000 --> 02:11.000]  Здесь у нас был LSM или B plus деревья, а на уровне репликации у нас multipax или RAF, другой ваш любимый алгоритм консенсуса.
[02:11.000 --> 02:15.000]  Но это же не единственная причина делать распределенные системы.
[02:15.000 --> 02:20.000]  Вторая причина, про которую мы сегодня будем говорить, это масштабируемость.
[02:20.000 --> 02:28.000]  Может быть, скорее всего, наши данные просто не помещаются в одну машину, даже если мы считаем, что она надежная, потому что мы ее зареплицировали.
[02:28.000 --> 02:35.000]  Может быть, у нас настолько много данных, что они не помещаются даже в 100 машинах, в 1000 и, может быть, в 10000 там помещаются.
[02:35.000 --> 02:46.000]  Сегодня мы хотим решать такие задачи. Мы хотим говорить про масштабируемость распределенных систем.
[02:46.000 --> 02:54.000]  И в качестве примера у нас будут два класса систем. Это key value хранилища и это будет распределенная файловая система.
[02:54.000 --> 03:02.000]  Казалось бы, мы уже распределенную файловую систему делали, key value хранилища мы, может быть, интуитивно представляем, как делать распределенным.
[03:02.000 --> 03:08.000]  Мы с этого начнем сегодня. Но я хочу вам сразу сказать, объяснить, в чем сложность здесь.
[03:08.000 --> 03:15.000]  Вот эти задачи в каком-то смысле вам знакомы. В смысле, сложность, которая там возникает, она должна быть хорошо знакома.
[03:15.000 --> 03:22.000]  Ну, скажем, задача реализации упорядоченного хранилища, упорядоченного контейнера с произвольным доступом поверх жесткого диска.
[03:22.000 --> 03:31.000]  Это вполне себе алгоритмическая задача. Мы зафиксировали модель стоимости и сказали, что у нас нет произвольного доступа к жесткому диску,
[03:31.000 --> 03:34.000]  но есть доступ к последовательно большими блоками, и он эффективен.
[03:34.000 --> 03:40.000]  И вот мы в этой модели стоимости, где мы хотим минимизировать количество секов, делаем упорядоченное хранилища,
[03:40.000 --> 03:44.000]  хранилища, который умеет путагетомо-произвольному ключу.
[03:44.000 --> 03:53.000]  Для этого мы что делали? Ну, мы там использовали SSTable, какие-то BloomFilter, в общем, какая-то алгоритмическая идея была за этим всем.
[03:53.000 --> 03:58.000]  Задача репликации это вообще чистая алгоритмическая задача.
[03:58.000 --> 04:06.000]  Она целиком про конкарнси. У нас есть узлы, какие-то распределенные акторы, они как-то неупорядочно действуют, как-то конкурируют друг с другом,
[04:06.000 --> 04:10.000]  в общем, сообщения летают по сети в произвольное время, ну и в итоге складываются какие-то конфигурации,
[04:10.000 --> 04:14.000]  какие-то исполнения, и в любом таком исполнении наш алгоритм должен вести себя корректно.
[04:14.000 --> 04:19.000]  Это вот более-менее прошлый семестр, когда мы занимались многопоточностью, те же самые проблемы.
[04:19.000 --> 04:25.000]  Ну, строго говоря, в storage же есть еще одна проблема инженерная, про которую мы еще не поговорили,
[04:25.000 --> 04:32.000]  но об этом будет отдельный семинар, про то, как жить поверх файловой системы жесткого диска, какие там есть проблемы.
[04:32.000 --> 04:38.000]  Ну, потому что задача-то не только в том, чтобы эффективно обращаться к данным по произвольному ключу,
[04:38.000 --> 04:42.000]  а еще и в том, чтобы пережить restart машины в произвольный момент времени.
[04:42.000 --> 04:48.000]  Вот вы пишете что-то в лог, добавляете там какую-то большую запись о том, что вы вставляете по ключу K значение V,
[04:48.000 --> 04:52.000]  а где-то в середине вас перезагружают. И вот какие-то ваши данные успели записаться в лог,
[04:52.000 --> 04:57.000]  какие-то не успели записаться в лог, система в каком-то неожиданном состоянии завершилась.
[04:57.000 --> 05:02.000]  Ну и после этого оно должно перезагрузиться и начать работать корректно.
[05:02.000 --> 05:10.000]  Так что нужно здесь учесть какие-то аспекты, какие-то нюансы работы файловой системы и жесткого диска.
[05:10.000 --> 05:15.000]  Про это мы поговорим, но все же такие локальные, понятные задачи.
[05:15.000 --> 05:20.000]  Вот задача масштабируемости, у нее сложность совсем другого рода.
[05:20.000 --> 05:26.000]  Тут не нужно, скажем, оптимизировать число фаса или число раунд трипов, чего-то такое делать.
[05:26.000 --> 05:31.000]  Задача масштабируемости, она про то, как взять какую-то систему, которая, не знаю,
[05:31.000 --> 05:36.000]  может быть, хорошо живет на одной машине или на 100 машинах и масштабировать ее в тысячу раз.
[05:36.000 --> 05:46.000]  И решение этой задачи, оно не про алгоритмы и не про инженерию, а про то, чтобы найти в системе узкие места
[05:46.000 --> 05:50.000]  и вот эти узкие места научить масштабировать.
[05:50.000 --> 05:55.000]  Вот спроектировать такой дизайн, выделить такие абстракции, такие слои, так их скомпоновать,
[05:55.000 --> 05:57.000]  чтобы даже не складеть что-то еще сложнее.
[05:57.000 --> 06:00.000]  И так скомпоновать все это, чтобы вот в системе не было узкого места,
[06:00.000 --> 06:02.000]  чтобы она могла расти горизонтально бесконечно.
[06:02.000 --> 06:06.000]  Мы могли бы добавлять в нее машины, и она бы эти машины как-то утиризировала,
[06:06.000 --> 06:10.000]  то есть хранила там больше данных и обслуживала больше запросов.
[06:10.000 --> 06:14.000]  Вот эта задача для нас новая, мы еще ее не решали толком.
[06:14.000 --> 06:17.000]  Ну, мы пробовали делать с файловыми системами и к какому-то решению там пришли,
[06:17.000 --> 06:21.000]  но вот сегодня мы хотим его еще большему масштабируем, вспомнить,
[06:21.000 --> 06:24.000]  на чем мы остановились на каком-то прошлом семинаре
[06:24.000 --> 06:28.000]  и довести задачу до логического конца.
[06:28.000 --> 06:31.000]  Почему мы сегодня говорим про масштабируемость,
[06:31.000 --> 06:36.000]  например, двух классов систем киевареохранилища и файловые системы?
[06:36.000 --> 06:40.000]  Ну, тут есть два объяснения, одно историческое, другое, более современное.
[06:40.000 --> 06:44.000]  Историческое такое, что в издревле, вот последние два десятилетия,
[06:44.000 --> 06:49.000]  люди строили в больших компаниях, как правило, было два параллельных пайплайна работы с данными.
[06:49.000 --> 06:52.000]  Была бач-обработка и была реалтайма-обработка.
[06:52.000 --> 06:56.000]  Ну, скажем, вы индексируете интернет, у вас есть какой-то гигантский граф,
[06:56.000 --> 07:01.000]  его нужно как-то обходить и что-то с ним делать, не знаю, поджарамку словно считать.
[07:01.000 --> 07:09.000]  И вы пишете системы, которые, скажем, умеют в фоне, не спеша, вот эти огромные массивы данных перемалывать.
[07:09.000 --> 07:12.000]  Ну, тут можно представить себе какой-нибудь MapReduce.
[07:12.000 --> 07:16.000]  Нам не важна здесь аутентная степерация, нам важно, чтобы мы отказоустойчиво
[07:16.000 --> 07:21.000]  хранили и обрабатывали огромные массивы данных, петабайты данных.
[07:21.000 --> 07:25.000]  Ну и где такие данные хранить? Конечно же, это распределенная файловая система,
[07:25.000 --> 07:29.000]  где у вас помещаются огромные бесконечного размера файлы.
[07:29.000 --> 07:34.000]  Киеварю хранили еще исторически, были про интерактивную обработку.
[07:34.000 --> 07:38.000]  Ну, не знаю, представьте себе Amazon, который продает вам какие-то книги и девайсы,
[07:38.000 --> 07:41.000]  вы приходите к нему, добавляете товары в корзину, нажимаете купить.
[07:41.000 --> 07:47.000]  Это интерактивная работа и, конечно же, Amazon под капотом у себя сохраняет все ваши покупки
[07:47.000 --> 07:50.000]  в какую-то базу, покупки вашей корзине в какую-то базу,
[07:50.000 --> 07:55.000]  потом, когда вы нажимаете купить, эта транзакция куда-то дальше улетает.
[07:55.000 --> 08:03.000]  То есть два таких кейса разных. Сейчас немного не так, но и киеварю хранилища,
[08:03.000 --> 08:07.000]  и распределенная файловая система все еще нужны, потому что, грубо говоря,
[08:07.000 --> 08:12.000]  если вы Google, то у вас есть распределенная файловая система, которая называется Colossus.
[08:12.000 --> 08:17.000]  Это следующая интерация, масштабируемая файловая система GFS,
[08:17.000 --> 08:22.000]  которую мы обсуждали как-то. И более-менее все, что вы Google храните у себя
[08:22.000 --> 08:27.000]  на ваших машинах, а сколько их у вас, кстати, у вас оказывается миллионы,
[08:27.000 --> 08:31.000]  ну, единицы миллионов. Вот все, что вы на этих единицах миллионов машин храните,
[08:31.000 --> 08:35.000]  хранится, в конце концов, не на локальных жестких дисках, в смысле, конечно же, там, где еще,
[08:35.000 --> 08:40.000]  но логически оно хранится не на конкретной машине, а в распределенной файловой системе.
[08:40.000 --> 08:44.000]  И вот все эти огромные массивы данных хранятся в итоге в DFS,
[08:44.000 --> 08:49.000]  и вот эта DFS должна масштабироваться до таких объемов.
[08:49.000 --> 08:55.000]  Что касается кейварю хранилищ, то они сами по себе, конечно, еще актуальны,
[08:55.000 --> 09:00.000]  но сейчас поверх них научились делать, поверх масштабируемого кейварю хранилища
[09:00.000 --> 09:07.000]  с транзакциями, научились делать таблицы и запросы, и в конце концов научились делать базы данных.
[09:07.000 --> 09:12.000]  Поэтому, да, вы, наверное, работаете с базами данных, но в конце концов под вами кейварю хранилищ
[09:12.000 --> 09:16.000]  и под вами распределенная файловая система, даже если вы напрямую с ними не работаете,
[09:16.000 --> 09:21.000]  и вот все ваши данные хранятся там. Поэтому задача масштабирования кейварю и DFS
[09:21.000 --> 09:29.000]  это наша главная задача. Ну и давайте мы с чего-нибудь начнем.
[09:29.000 --> 09:35.000]  Начнем мы сегодня с кейварю хранилищ.
[09:35.000 --> 09:40.000]  Что это такое, вы, наверное, хорошо уже помните. У нас есть операция put, у нас есть операция get, delete,
[09:40.000 --> 09:48.000]  у нас есть чтение диапазонов, потому что так можно делать snapshot и так можно делать транзакции.
[09:48.000 --> 10:00.000]  Можно было бы сказать, что у нас есть вот такая большая-большая таблица.
[10:00.000 --> 10:07.000]  Вот здесь написан ключ, здесь написано значение.
[10:07.000 --> 10:13.000]  Но мы такую задачу пока не умеем решать, но мы умеем решать задачу в пределах одной машины.
[10:13.000 --> 10:21.000]  Мы умеем строить lsm. Дальше мы умеем этот lsm реплицировать с помощью алгоритма multipaxos или raft.
[10:21.000 --> 10:26.000]  Так что мы задачу умеем решать, но вот до тех пор...
[10:26.000 --> 10:32.000]  Только если объем данных, которые мы храним в кейваре, умещается в одну машину, в один жесткий диск.
[10:32.000 --> 10:39.000]  Ну а теперь у нас таблица большая. Давайте назовем ее bigtable.
[10:39.000 --> 10:43.000]  В смысле система bigtable? Просто большая таблица.
[10:43.000 --> 10:47.000]  Она настолько большая, что она не помещается, конечно, в одну машину.
[10:47.000 --> 10:54.000]  Как мы поступим с ней?
[10:54.000 --> 11:07.000]  Мы ее протиционируем горизонтально. То есть мы возьмем и эту большую таблицу поделим на какие-то разумные размеры части.
[11:07.000 --> 11:15.000]  И каждую такую часть мы назовем как?
[11:15.000 --> 11:22.000]  Мы назовем каждую такую часть таблет.
[11:22.000 --> 11:28.000]  В самом деле ключи в этой таблице независимые. Пока мы не говорим про транзакции, они независимые.
[11:28.000 --> 11:36.000]  Поэтому мы можем разделить таблицу на части и обеспечить отказоустойчивость каждой части независимо от остальных.
[11:36.000 --> 11:40.000]  Каким образом? Ну, просто сделать RSM.
[11:40.000 --> 11:56.000]  Вот у нас есть эта огромная таблица, и у нас рядом с ней есть pull машин.
[11:56.000 --> 12:00.000]  Ну вот давайте за хранение каждого таблета будут отвечать какие-то машины.
[12:00.000 --> 12:11.000]  Вот у нас есть красный таблет, и у него будут три реплики здесь.
[12:11.000 --> 12:24.000]  Эти три реплики образуют, видимо, RSM. Они реализуют multipaxos или raft для того, чтобы каждый из этих реплик локально хранит данные таблета в локальном хранилище, в roxdb или leveldb,
[12:24.000 --> 12:36.000]  и упорядочивает апдейты в это локальное хранилище на этих репликах с помощью алгоритма multipaxos или raft.
[12:36.000 --> 12:41.000]  Понятная идея, да? Это мы с вами делать умеем.
[12:41.000 --> 12:45.000]  Ну, давайте подумаем про какие-то нюансы.
[12:45.000 --> 12:48.000]  Ну, например, какого размера эти таблеты?
[12:48.000 --> 12:56.000]  Она бесконечная, она может расти и расти, и вот неограничена. Мы, по крайней мере, к такому стремимся.
[12:56.000 --> 13:01.000]  А таблет какого размера?
[13:01.000 --> 13:08.000]  Фиксированного размера. Но очень трудно, когда у тебя ключи появляются, удаляются, что-то фиксированного размера иметь.
[13:08.000 --> 13:11.000]  Нам нужно какую-то верхнюю границу, видимо, выбрать.
[13:11.000 --> 13:14.000]  Ну и вроде бы, естественно, верхняя граница у нас есть.
[13:14.000 --> 13:22.000]  Мы не можем в таблете хранить больше, чем помещается в одну машину просто потому, что это такое ограничение RSM.
[13:22.000 --> 13:26.000]  Все, что реплицирует RSM, должно помещаться в одну машину в один диск.
[13:26.000 --> 13:30.000]  Ну, у машины, наверное, какие-то, не знаю, терабайтные диски.
[13:30.000 --> 13:39.000]  Вот верно ли, что нужно делить эту большую таблицу на таблеты на шарды размером терабайт?
[13:39.000 --> 13:49.000]  Ну, что значит слишком много? Их будет много.
[13:49.000 --> 13:54.000]  Подожди, очень странное замечание, потому что мы себе крупнее делать таблеты не можем просто.
[13:54.000 --> 13:58.000]  А ты говоришь, что их уже много получается. Нет, дело не в этом.
[13:58.000 --> 14:10.000]  Ну как бы, разом-то их зачем читать? У нас приходит пользователь, говорит PUT.
[14:10.000 --> 14:13.000]  По ключу значения. Мы находим каким-то образом таблет.
[14:13.000 --> 14:20.000]  Он приходит в какую-то строчку со своим PUT.
[14:20.000 --> 14:24.000]  Вот мы должны понять, на каком таблете эта строчка лежит.
[14:24.000 --> 14:35.000]  Ну, вот, допустим, на этом.
[14:35.000 --> 14:39.000]  Ну, это правильно до запроса. Там это обычный PUT, который попадает в RSM, там в multipax,
[14:39.000 --> 14:45.000]  среплицируется и в конце концов применяется к локальному состоянию, то есть помещается в levelDB.
[14:45.000 --> 14:50.000]  Ну, вот такой как-то ответственный получится, потому что если у нас один таблет имеет
[14:50.000 --> 14:56.000]  веточную границу по размеру диска машины, то нам нужно давать гарантию по диску этого таблета.
[14:56.000 --> 15:00.000]  И тогда мы не можем размещать по машине больше одного таблета.
[15:00.000 --> 15:05.000]  Так и зачем нам размещать по машине больше одного таблета?
[15:05.000 --> 15:12.000]  Нет, на ход мысли правильный. Есть таблица, она делится на таблеты как-то произвольно по ключу,
[15:12.000 --> 15:16.000]  но в конце концов пользователь же неравномерно к этой таблице обращается.
[15:16.000 --> 15:19.000]  Может быть, там есть какие-то горячие ключи, горячие таблеты, есть какие-то менее активные,
[15:19.000 --> 15:22.000]  где нагрузка меньше, данных меньше.
[15:22.000 --> 15:28.000]  Поэтому мы можем только запустить систему и смотреть, как пользователь с ней работает.
[15:28.000 --> 15:33.000]  Он может в какой-то диапазон писать больше, делать большую операцию над каким-то диапазоном,
[15:33.000 --> 15:39.000]  над каким-то меньше. И в итоге какой-то таблет, он же разделен по ключам, условно говоря.
[15:39.000 --> 15:44.000]  У него есть какой-то стартовый ключ.
[15:44.000 --> 15:49.000]  И может быть, между этими ключами будет очень много записей, и таблет начнет расти, расти, расти.
[15:49.000 --> 15:54.000]  И с одной стороны, он может переполнить машину, которая его хранит, но машины, реплики.
[15:54.000 --> 15:57.000]  А с другой стороны, просто эти машины могут не справляться.
[15:57.000 --> 16:01.000]  Но точнее, это же RSM, и там все записи обслуживают один лидер.
[16:01.000 --> 16:06.000]  И вот он может стать узким местом для этого таблета, потому что запросов слишком много.
[16:06.000 --> 16:11.000]  В этом случае что мы захотим сделать? Мы захотим этот таблет разделить.
[16:14.000 --> 16:19.000]  И тогда нам нужно как-то будет их перебалансировать в нашем кластере.
[16:19.000 --> 16:24.000]  Сказать, что вот часть данных уезжает на другие машины, например.
[16:24.000 --> 16:27.000]  Ну или так получится, да.
[16:27.000 --> 16:32.000]  Ну а если мы начнем эти таблеты двигать, то если у нас таблет размером терабайта,
[16:32.000 --> 16:35.000]  то их двигать будет очень тяжело, эта система будет очень не гибкой.
[16:35.000 --> 16:38.000]  Поэтому мы, видимо, хотим небольшие таблеты.
[16:38.000 --> 16:45.000]  И в промышленных системах их размер измеряется, не знаю, десятками, сотнями мегабайт.
[16:45.000 --> 16:49.000]  У нас будет еще в будущем система Google Spanner, из которой был TrueTime.
[16:49.000 --> 16:54.000]  У нее есть Open Source Clon, Open Source реализации по мотивам, как ROGDB.
[16:54.000 --> 16:58.000]  Это база данных с транзакциями и таблицами под капотом Key Value.
[16:58.000 --> 17:02.000]  И в этом Key Value таблеты размером 64 мегабайта.
[17:02.000 --> 17:05.000]  Ровно для того, чтобы их можно было легко двигать по кластеру.
[17:05.000 --> 17:10.000]  Ну и тогда, разумеется, на каждой машине у вас может быть не один таблет, а несколько.
[17:10.000 --> 17:16.000]  То есть у вас одна машина является репликой красного, является репликой зеленого.
[17:19.000 --> 17:23.000]  И если вдруг этой машине станет слишком тяжело, потому что очень много запросов,
[17:23.000 --> 17:26.000]  какой-то таблет или таблеты станут очень большими,
[17:26.000 --> 17:30.000]  то их можно разделить на части и размазать по кластеру.
[17:32.000 --> 17:37.000]  Хорошо, тогда следующий шаг.
[17:37.000 --> 17:41.000]  Видимо, эти таблеты нужно балансировать.
[17:41.000 --> 17:44.000]  Кто этим занимается?
[17:48.000 --> 17:52.000]  Ну какая-то нода. Если мы возьмем какой-то узел, который будет этим всем заниматься,
[17:52.000 --> 17:54.000]  и выделим его, то он может умереть.
[17:54.000 --> 17:58.000]  Тогда система, видимо, останавливается, потому что никто не сможет координировать действие.
[17:58.000 --> 18:03.000]  Ну вот мы выделим такую роль.
[18:03.000 --> 18:05.000]  Shard Manager.
[18:05.000 --> 18:08.000]  Название довольно условное.
[18:08.000 --> 18:12.000]  Это в данный момент какой-то конкретный узел из кластера, который понимает,
[18:12.000 --> 18:18.000]  где какие таблеты находятся, то есть какими машинами обслуживаются какие-то таблеты.
[18:18.000 --> 18:21.000]  Он следит за тем, чтобы все были живы, следит за нагрузкой
[18:21.000 --> 18:25.000]  и умеет таблеты двигать между этими узлами.
[18:25.000 --> 18:29.000]  Разумеется, будет плохо, если это будет конкретная машина, потому что она откажет.
[18:29.000 --> 18:33.000]  Ну что мы можем сделать?
[18:33.000 --> 18:39.000]  Сделать ее отказоустойчивой тоже.
[18:39.000 --> 18:43.000]  Давайте я вам покажу какие-то картинки.
[18:43.000 --> 18:55.000]  Shard Manager это, конечно, одна машина, просто реплицированная.
[18:55.000 --> 19:01.000]  То есть логически это один актор.
[19:01.000 --> 19:07.000]  Мы сегодня говорим про киевое хранилище, у нас будет несколько примеров.
[19:07.000 --> 19:12.000]  Bigtable, про которую вы могли читать статью, ZPDB это киевое хранилище в Facebook.
[19:12.000 --> 19:19.000]  Ну и слой Key Value, который реализован как RoachDB, это Open Source-система по мотивам Google Spanner.
[19:19.000 --> 19:22.000]  Ну вот, пожалуйста, как RoachDB, вот его дизайн.
[19:22.000 --> 19:27.000]  Вы строите SQL поверх какого-то большого монолитного Key Value хранилища.
[19:27.000 --> 19:30.000]  И в этом хранилище что у вас есть?
[19:30.000 --> 19:37.000]  Отдельные машины. На каждой машине есть SSD-диск с RoachDB.
[19:37.000 --> 19:41.000]  Ну и вот каждая машина является репликой для отдельных таблетов.
[19:41.000 --> 19:45.000]  Но тут они называются range-диапазоны.
[19:45.000 --> 19:48.000]  Пожалуйста, одна машина хранит сразу несколько.
[19:48.000 --> 19:57.000]  Ну и разумеется, каждый таблет принадлежит, каждый range реплицирован на трех нодах.
[19:57.000 --> 19:59.000]  По поводу Shard Manager я хотел показать.
[19:59.000 --> 20:03.000]  Вот ZPDB картинки, вот еще раз та же самая конструкция.
[20:03.000 --> 20:05.000]  Это, наверное, очень хорошо видно.
[20:05.000 --> 20:08.000]  У нас есть пять машин.
[20:08.000 --> 20:10.000]  Каждый shard в трех репликах.
[20:10.000 --> 20:13.000]  Ну и вот они как-то по этим машинам распределены.
[20:17.000 --> 20:23.000]  Вот в ZPDB, в Key Value хранилище Facebook, что еще делает Shard Manager?
[20:23.000 --> 20:28.000]  Вот помните, мы говорили про multiprocess и пришли к тому построить алгоритм,
[20:28.000 --> 20:30.000]  но в нем кое-чего не хватало.
[20:30.000 --> 20:34.000]  Мы не сказали, как именно в multiprocess выбирается лидер.
[20:34.000 --> 20:37.000]  Ну то есть понятно, что его можно выбрать со старшим ID,
[20:37.000 --> 20:40.000]  но это вроде бы не очень эффективно, потому что у меня может быть пустой лог.
[20:40.000 --> 20:47.000]  А еще мы не сказали, как в multiprocess новый лидер выбирает себе n, эпоху, в которой он будет жить.
[20:47.000 --> 20:49.000]  В принципе, RAF то обе эти проблемы решал.
[20:49.000 --> 20:53.000]  Там процедура выбора лидера была разумной, и этот лидер принадлежал некоторому терму,
[20:53.000 --> 20:55.000]  то есть у него это n свое было тоже.
[20:55.000 --> 20:59.000]  Так вот, ZPDB используется в multiprocess для репликации каждого таблета.
[20:59.000 --> 21:01.000]  И shard manager что делает?
[21:01.000 --> 21:07.000]  Он следит за лидерами каждого таблета, и если лидер умирает,
[21:07.000 --> 21:11.000]  то shard manager это понимает, выбирает нового лидера для данного таблета
[21:11.000 --> 21:13.000]  и назначает ему новую эпоху.
[21:13.000 --> 21:20.000]  Ну то есть вот эта задача, выбор нового лидера и назначение ему новой эпохи
[21:20.000 --> 21:23.000]  происходит централизовано shard manager.
[21:23.000 --> 21:28.000]  Эта задача решается разом для всех таблета, для всех RSM в одном месте.
[21:28.000 --> 21:30.000]  Понятная идея?
[21:32.000 --> 21:36.000]  Хорошо. А теперь представим, что вы клиент,
[21:39.000 --> 21:43.000]  и у вас есть какой-то пут по ключу значения.
[21:44.000 --> 21:49.000]  И вы должны прийти на этот кластер и попасть в какой-то нужный вам таблет к лидеру,
[21:49.000 --> 21:52.000]  который его обслуживает. Как вы это сделаете?
[21:52.000 --> 22:00.000]  Как вы в этом большом пуле машину, не знаю, тысячи или десятки тысяч машин,
[22:00.000 --> 22:04.000]  как вы найдете ваш таблет и лидера, который его обслуживает?
[22:07.000 --> 22:09.000]  Ну вот идем сюда.
[22:11.000 --> 22:14.000]  Что мы можем заметить? Что во-первых, shard manager страдает,
[22:14.000 --> 22:19.000]  потому что вы к нему ходите по такому пустящему поводу, как маленький пут.
[22:19.000 --> 22:23.000]  Это все-таки координатор, то есть он не хочет находиться на пути записей
[22:23.000 --> 22:25.000]  или чтений у клиентов.
[22:26.000 --> 22:28.000]  Он начинает стать туским местом.
[22:28.000 --> 22:30.000]  Кроме того, сколько у нас здесь таблетов?
[22:33.000 --> 22:35.000]  Порядочно.
[22:39.000 --> 22:41.000]  Справедливое замечание. Порядочно.
[22:41.000 --> 22:44.000]  До каких вообще объемов мы хотим штаблироваться?
[22:45.000 --> 22:47.000]  До каких величин? До какой емкости?
[22:49.000 --> 22:53.000]  Это правильно. До бесконечности хорошо масштабировать.
[22:53.000 --> 22:58.000]  Если мы масштабируемся до бесконечности, то нам на любую задачу хватает запас.
[22:58.000 --> 23:03.000]  Но вот бесконечность чему сейчас равна на данном этапе развития человечества?
[23:10.000 --> 23:14.000]  А сколько сейчас данных нужно хранить? Вот если у тебя миллион машин?
[23:20.000 --> 23:22.000]  Ты не хочешь умножать, я понял.
[23:23.000 --> 23:28.000]  Ну мы не знаем, у тебя миллионы машин, а не какие-то тарабайта дискового пространства.
[23:28.000 --> 23:31.000]  Но тут, конечно, нужно учесть, что это все реплицировано еще, то есть
[23:31.000 --> 23:38.000]  логический размер данных меньше, чем они занимают физически на дисках.
[23:39.000 --> 23:41.000]  Нет, петабайт это уже прошлый век.
[23:41.000 --> 23:51.000]  Вот мы хотим масштабироваться до экзобайтов. Больше человечество сейчас не умеет.
[23:51.000 --> 23:55.000]  Ну в смысле, не нужно ему. Ну вот до каких масштабов мы хотим?
[23:55.000 --> 24:01.000]  И тут возникают некоторые технические проблемы, а именно, что вот этих таблетов много,
[24:01.000 --> 24:08.000]  прям вот очень много. И, конечно же, вот этому менеджеру, который следит за кластером,
[24:08.000 --> 24:14.000]  ему нужно знать, за какие таблеты какие машины отвечают.
[24:14.000 --> 24:20.000]  Ему нужно просто хранить такое отображение. Вот у тебя есть вот эта большая таблица,
[24:20.000 --> 24:26.000]  и тебе нужно знать, ну она поделена, одна таблица, ну или много таблиц, скорее всего.
[24:26.000 --> 24:33.000]  Вот в системе Bigtable много больших таблиц. И тебе нужно для каждого таблета
[24:33.000 --> 24:44.000]  знать, кто сейчас его обслуживает. Ну давайте назовем его таблет-сервер.
[24:52.000 --> 24:56.000]  Тут есть проблема в том, что вот это отображение, оно большое.
[24:56.000 --> 25:02.000]  Ну просто много метаданных у этого хранилища, потому что таблеты маленькие, а таблицы большие.
[25:02.000 --> 25:08.000]  Вот, и ну просто вот этот шард-менеджер, он отказаустойчивый, потому что он реплицирован.
[25:08.000 --> 25:14.000]  Но с другой стороны, вот просто такой объем данных, у него с трудом помещается.
[25:14.000 --> 25:19.000]  Нет, мы, конечно, можем его шардировать, в смысле, в конце концов, разные таблицы,
[25:19.000 --> 25:23.000]  разные таблеты, они более-менее независимы, поэтому можно взять и как-то поделить,
[25:23.000 --> 25:26.000]  взять несколько шард-менеджеров, каждый отвечает за свой набор таблетов,
[25:26.000 --> 25:32.000]  статически поделить по какому-нибудь хэшу. Ну и вот сделать много шард-менеджеров.
[25:32.000 --> 25:36.000]  Вполне себе разумное решение. В смысле, оно будет работать, видимо.
[25:36.000 --> 25:44.000]  Вот, но можно подумать и сделать несколько элегантнее.
[25:44.000 --> 25:50.000]  Вот у нас bigtable, любая большая таблица, это отображение исключение значения.
[25:50.000 --> 25:55.000]  Вот, и чтобы поддерживать эти таблицы с отображением исключения значения,
[25:55.000 --> 26:05.000]  нужно хранить еще одно отображение исключения значения. Ну правда, служебное.
[26:05.000 --> 26:16.000]  Вот, ну вот давайте мы сделаем для вот этого отображения еще одну таблицу.
[26:16.000 --> 26:21.000]  Тут уже можно говорить про саму систему Google Bigtable. Вот Google Bigtable так и устроен.
[26:21.000 --> 26:26.000]  То есть там есть пользовательские таблицы, и для каждого таблета,
[26:26.000 --> 26:30.000]  мы должны для каждого таблета хранить точку обслуживания этого таблета.
[26:30.000 --> 26:36.000]  И это еще одна служебная большая таблица. То есть Bigtable хранит свои методанные в Bigtable.
[26:36.000 --> 26:39.000]  Это довольно удобно, потому что сам Bigtable масштабируется вроде как.
[26:39.000 --> 26:41.000]  Ну точнее, мы делаем его масштабированным. Если он будет масштабироваться,
[26:41.000 --> 26:48.000]  то и методанные будут масштабироваться. Поэтому не нужно каким-то специальным образом
[26:48.000 --> 26:55.000]  решать вот такую задачу, как шардировать менеджера всех таблетов.
[26:55.000 --> 27:00.000]  Мы можем методанные положить отдельно. Но смотрите, какая беда.
[27:00.000 --> 27:05.000]  В конце концов, шард менеджер должен быть. Он будет с этой таблицей работать.
[27:05.000 --> 27:11.000]  Но чтобы работать с этой таблицей, ему нужно знать, кто ее обслуживает.
[27:11.000 --> 27:15.000]  Понятно почему, да? То есть он хочет что-то записать,
[27:15.000 --> 27:19.000]  чтобы знать, что какой-то таблет обслуживается такой-то машиной.
[27:19.000 --> 27:24.000]  А для этого ему нужно пойти в какой-то таблет, который обслуживает таблеты этих методанных
[27:24.000 --> 27:29.000]  и к нему обратиться с этим запросом. А кто обслуживает эти таблеты? Как это узнать?
[27:34.000 --> 27:43.000]  Ну вот. Ну смотри, у тебя есть вот эти таблеты, а есть вот эти методанные таблеты.
[27:43.000 --> 27:48.000]  И для них нужно решить такую же задачу.
[27:48.000 --> 27:54.000]  Видимо, там нужна еще одна таблица методанных и методанных.
[27:54.000 --> 27:58.000]  Но чем она хороша? Тем, что она меньше становится.
[27:58.000 --> 28:02.000]  Потому что для методанных нужна была большая таблица.
[28:02.000 --> 28:08.000]  А для методанных и методанных мы делим еще на сколько-то, на 64 мегабайта.
[28:08.000 --> 28:12.000]  И эти донны становятся еще меньше.
[28:12.000 --> 28:17.000]  И вот может быть, здесь уже будет один таблет,
[28:17.000 --> 28:23.000]  который знает, кто обслуживает каждый методанный таблет здесь.
[28:23.000 --> 28:29.000]  Но для этого таблета же тоже нужно знать, кто его обслуживает, чтобы в него записать.
[28:29.000 --> 28:39.000]  Ну а этот таблет обслуживает, в конце концов, одна же машина.
[28:39.000 --> 28:45.000]  Вот куда мы эти данные положим? Ну, если мы в Bigtable, мы Google.
[28:45.000 --> 28:51.000]  У нас есть Bigtable, там есть таблицы пользователей, там есть Bigtable,
[28:51.000 --> 29:00.000]  там есть таблица уже совсем небольшая с методанными и методанными.
[29:00.000 --> 29:04.000]  И в конце концов все упирается в какую-то одну машину, которая обслуживает вот эту.
[29:04.000 --> 29:08.000]  И вот знание про эту машину нужно хранить отказоустойчиво.
[29:08.000 --> 29:13.000]  И вот знание про машину, которая обслуживает этот таблет, хранится в Google Chabi.
[29:13.000 --> 29:17.000]  Это сервис координации дерева, в котором мы можем брать локи
[29:17.000 --> 29:21.000]  и можем в его узлы писать небольшие данные размером килобайта.
[29:21.000 --> 29:25.000]  Вот мы можем туда записать одну эту машину.
[29:25.000 --> 29:31.000]  Так что когда мы клиент приходим с путом, мы идем не в Shard Manager, потому что мы не хотим выгрузить.
[29:31.000 --> 29:41.000]  Мы поначалу идем в Chabi первый раз, потом идем к этому таблету, узнаем из какого же таблета методанных,
[29:41.000 --> 29:47.000]  какая машина обслуживает таблет методанных для нашей таблицы и нашего ключа.
[29:47.000 --> 29:53.000]  И отсюда узнаем уже машину, которая нам нужна, которая хранит и обслуживает наш конкретный таблет,
[29:53.000 --> 29:56.000]  в котором мы хотим что-то записать.
[29:56.000 --> 29:59.000]  Понятная идея?
[29:59.000 --> 30:07.000]  То есть мы строим этот Bigtable и методанный Bigtable кладем в сам Bigtable, потому что он хорошо масштабируется.
[30:07.000 --> 30:18.000]  Эта идея, которая прижилась и ее использует как сама система Bigtable, так и более-менее все Open Source аналоги.
[30:18.000 --> 30:23.000]  Так делает и ROGDB, который я показывал вам, и так делает, скажем, HBase.
[30:23.000 --> 30:31.000]  Вот у вас на параллельном курсе должна быть рано или поздно лекция про HBase.
[30:31.000 --> 30:34.000]  Это аналог Bigtable Open Source, написанный на джаве.
[30:34.000 --> 30:46.000]  Ну и вот там есть такая же модель с таблицами, и есть специальная таблица с методанными.
[30:46.000 --> 30:55.000]  И вот в ней ключ выглядит так, имя вашей таблицы с вашими данными,
[30:55.000 --> 31:05.000]  запятая ключ, с которого начинается таблет этой таблицы, ну там что-нибудь еще,
[31:05.000 --> 31:10.000]  и вот по этому ключу вы находите точку обслуживания, машину, которая отвечает от этих данных на запрос.
[31:10.000 --> 31:16.000]  И вы приходите со своей, ну вы хотите что-то записать или что-то прочесть из вашей таблицы по вашему ключу,
[31:16.000 --> 31:25.000]  и вы вот в такой таблице ищете первый ключ, ну последний ключ не больше, чем ваш,
[31:25.000 --> 31:33.000]  и таким образом через служебную таблицу вы находите точку обслуживания и уже идете в ваш целевой таблет.
[31:33.000 --> 31:35.000]  Ну а все начнется с Google Chabi.
[31:35.000 --> 31:41.000]  Ну разумеется, вы пишете по своему ключу, видимо, много раз, поэтому вы можете закашировать.
[31:41.000 --> 31:46.000]  Конечно, вы закашируете, кто обслуживает корневой таблет, но чтобы его часто не грузить, вы, конечно,
[31:46.000 --> 31:51.000]  закашируете, кто обслуживает каждый таблет метод данных, ну или большинство таблетов метод данных,
[31:51.000 --> 31:57.000]  с которыми вы работаете. Поэтому вот на Google Chabi, конечно, не то чтобы все клиенты на каждую запись
[31:57.000 --> 32:06.000]  сходят в Google Chabi, это было безумием, она потом нарывалась.
[32:06.000 --> 32:14.000]  Ну вот такая идея, и она в этом состоит мастерство дизайна архитектуры распределенных систем.
[32:14.000 --> 32:18.000]  Как это придумать? А как это вообще придумать можно?
[32:18.000 --> 32:22.000]  Ну то есть это можно придумать просто так, как бы без всякого бэгграунда,
[32:22.000 --> 32:25.000]  а на самом деле эта же конструкция хорошо вам знакома.
[32:31.000 --> 32:34.000]  Не то чтобы люди выдумали что-то совершенно принципиально новое.
[32:34.000 --> 32:37.000]  Ну очень удаленно, но виртуально.
[32:37.000 --> 32:39.000]  Она не то что очень удаленная, это одно и то же.
[32:39.000 --> 32:43.000]  То есть у тебя есть в операционных системах механизмы в виртуальной памяти.
[32:43.000 --> 32:48.000]  Ты процессор должен на каждое обращение транслировать виртуальный адрес физический.
[32:48.000 --> 32:52.000]  У тебя есть ключ, ты должен транслировать его в точку обслуживания таблета.
[32:52.000 --> 32:56.000]  Что у тебя для этого есть? У тебя есть память, она поделена на страницы.
[32:56.000 --> 33:00.000]  И у нас здесь есть таблицы, они поделены на таблеты.
[33:00.000 --> 33:05.000]  В этих страницах есть твои данные, вот они.
[33:05.000 --> 33:08.000]  А еще есть специальные страницы, в которых лежат адреса других страниц,
[33:08.000 --> 33:12.000]  чтобы ты нашел по виртуальным адресу свою физическую.
[33:12.000 --> 33:17.000]  Это таблица PageTable, это страница PageTable.
[33:17.000 --> 33:21.000]  И вот страницы PageTable, они же образуют такой бор.
[33:21.000 --> 33:24.000]  Ну и здесь то же самое. У тебя есть корень, ты попадаешь сюда,
[33:24.000 --> 33:28.000]  выбираешь один из этих таблет, попадаешь дальше.
[33:28.000 --> 33:31.000]  Вот так же, как у тебя фиксировано количество уровней таблиц и страниц,
[33:31.000 --> 33:34.000]  так же у тебя зафиксировано количество уровней здесь.
[33:34.000 --> 33:40.000]  У тебя есть одна большая таблица с методанными, у тебя есть одна маленькая таблица из одного таблета,
[33:40.000 --> 33:46.000]  которая помогает найти тебе нужный таблет в методанных.
[33:46.000 --> 33:53.000]  Ну а вообще, как процессоры находят таблицу страниц для процесса текущего?
[33:53.000 --> 33:58.000]  А где он лежит?
[33:58.000 --> 34:01.000]  За хардкожем? Ну видишь, за хардкожем.
[34:01.000 --> 34:04.000]  Ну у тебя же процессы меняются, может быть, за хардкожем.
[34:04.000 --> 34:06.000]  Он лежит в процессоре, в регистре.
[34:06.000 --> 34:10.000]  Ну вот Chabi, это как раз очень маленькая память,
[34:10.000 --> 34:13.000]  и там есть специальный регистр, который хранит корень всего этого.
[34:13.000 --> 34:18.000]  А дальше ты от него начинаешь идти вверх и находишь свою физическую страницу.
[34:18.000 --> 34:21.000]  Вот та же самая идея.
[34:21.000 --> 34:34.000]  Здесь, кажется, границ никаких нет.
[34:34.000 --> 34:37.000]  Ну вот, мне кажется, что супер изящная идея.
[34:37.000 --> 34:42.000]  Ну а теперь можно, это еще не все, что нужно сказать про киеварю хранилища,
[34:42.000 --> 34:46.000]  но пока мы остановимся и поговорим про другой класс систем,
[34:46.000 --> 34:50.000]  про распределенные файловые системы.
[34:50.000 --> 34:53.000]  Их же тоже нужно масштабировать.
[34:53.000 --> 34:58.000]  Ну и давайте вспомним наш прогресс, что мы успели сделать на каком-то прошлом семинаре.
[34:58.000 --> 35:01.000]  Мы говорили, что файловая система, она из себя что представляет?
[35:01.000 --> 35:04.000]  Ну во-первых, она хранит файлы.
[35:04.000 --> 35:08.000]  И видимо, распределенная файловая система должна хранить сколь годно большие файлы.
[35:08.000 --> 35:11.000]  Поэтому мы делим их на блоки.
[35:11.000 --> 35:18.000]  Ну а помимо этого у файловой системы есть иерархия имен, некоторое дерево.
[35:18.000 --> 35:26.000]  И для каждого узла этого дерева, для каждого листа этого дерева нам нужно помнить информацию,
[35:26.000 --> 35:31.000]  из каких блоков этот файл состоит.
[35:31.000 --> 35:34.000]  Вот так что у нас есть это дерево.
[35:34.000 --> 35:42.000]  Для каждого файла у нас есть структура, которая называется inode.
[35:42.000 --> 35:45.000]  И там лежат какие-то атрибуты.
[35:45.000 --> 35:47.000]  Ну скажем, там размер.
[35:47.000 --> 35:55.000]  И список блоков.
[35:55.000 --> 35:59.000]  И вот по таким идентификаторам блоков и дальше где-то можно на диске найти,
[35:59.000 --> 36:03.000]  на блоке устройств.
[36:03.000 --> 36:07.000]  Значит, нам нужно поддерживать дерево.
[36:07.000 --> 36:10.000]  Нам нужно хранить inode, списки блоков.
[36:10.000 --> 36:16.000]  Ну и нам нужно хранить сами блоки где-то.
[36:16.000 --> 36:20.000]  Так вот, что мы сделали с вами тогда?
[36:20.000 --> 36:24.000]  Мы сказали, что есть в файловой системе логически два уровня.
[36:24.000 --> 36:27.000]  Данные и методанные.
[36:27.000 --> 36:30.000]  И когда мы делаем распределенную файловую систему,
[36:30.000 --> 36:34.000]  то мы хотим вот эти два уровня выделить в две подсистемы.
[36:34.000 --> 36:39.000]  Уровень... вот методанные будут находиться в подсистеме,
[36:39.000 --> 36:41.000]  в котором мы назвали методанную стор,
[36:41.000 --> 36:46.000]  а данные будут находиться в системе под названием chunkStore.
[36:46.000 --> 36:50.000]  Ну chunk потому что мы блоки переименовали в чанки.
[36:50.000 --> 36:56.000]  Ну и давайте вспомним, как мы chunkStore и metastore строили.
[36:56.000 --> 36:59.000]  Ну во-первых, мы сказали, что давайте не будем пытаться
[36:59.000 --> 37:03.000]  создать API настоящей локальной файловой системы.
[37:03.000 --> 37:06.000]  Мы вместо этого ограничимся какими-то более простыми операциями.
[37:06.000 --> 37:08.000]  Скажем, мы не будем делать перезаписи файла.
[37:08.000 --> 37:10.000]  Будем делать только append.
[37:10.000 --> 37:14.000]  И append сразу большими блоками, большими порциями.
[37:14.000 --> 37:18.000]  То есть на каждый append порождается новый chunk.
[37:18.000 --> 37:20.000]  И нужно его сначала положить в chunkStore,
[37:20.000 --> 37:24.000]  а потом прийти в metastore сказать, что мы создали новый chunk.
[37:24.000 --> 37:28.000]  Это как списку чанков в inode для данного файла.
[37:28.000 --> 37:31.000]  В чем было преимущество такого дизайна?
[37:31.000 --> 37:34.000]  Ну он был прост, потому что чанки становились имутабельными,
[37:34.000 --> 37:37.000]  они просто один раз добавлялись и все.
[37:37.000 --> 37:40.000]  И вот chunkStore у нас имел очень простой API.
[37:40.000 --> 37:44.000]  Мы могли положить в него какие-то данные.
[37:50.000 --> 37:53.000]  И нам от метаса chunkStore отдавал ID.
[37:53.000 --> 37:57.000]  А дальше по этому ID можно было этот chunk потом прочитать.
[38:06.000 --> 38:08.000]  Metastore хранил все остальное.
[38:08.000 --> 38:11.000]  То есть он хранил дерево, и он хранил inode файлов.
[38:11.000 --> 38:13.000]  Из каких чанков они состоят?
[38:13.000 --> 38:16.000]  Вот эти самые идентификаторы, которые генерировал chunkStore.
[38:16.000 --> 38:18.000]  Вспоминаем, да?
[38:18.000 --> 38:21.000]  Как мы делали chunkStore и как мы делали metastore?
[38:21.000 --> 38:23.000]  Помните?
[38:26.000 --> 38:29.000]  Вы должны что-то сказать мне.
[38:29.000 --> 38:31.000]  Как мы делали chunkStore?
[38:31.000 --> 38:34.000]  Ну, брали pool-машин для начала.
[38:41.000 --> 38:44.000]  Давайте я коротко напомню, что мы там достигли.
[38:44.000 --> 38:46.000]  Брали pool-машин.
[38:46.000 --> 38:49.000]  Брали какую-то выделенную машину, которая всем этим управляла.
[38:49.000 --> 38:51.000]  ChunkMaster.
[38:51.000 --> 38:56.000]  И когда мы делали pool, мы приходили к этой машине.
[38:56.000 --> 39:01.000]  А она выбирала из этого всего pool-а свободных машин для хранения данных.
[39:01.000 --> 39:05.000]  Выбирала какие-то, допустим, три и писала реплики на них.
[39:15.000 --> 39:18.000]  Генерировал идентификатор, конечно.
[39:20.000 --> 39:22.000]  Вот сюда.
[39:22.000 --> 39:24.000]  Сюда.
[39:24.000 --> 39:26.000]  И сюда.
[39:26.000 --> 39:28.000]  Вот здесь появлялся наш chunk теперь.
[39:28.000 --> 39:32.000]  Когда мы хотели его прочитать, то мы приходили к этому chunk-мастеру.
[39:32.000 --> 39:34.000]  Чанк-сервер.
[39:34.000 --> 39:36.000]  Чанк-мастеру давайте.
[39:36.000 --> 39:40.000]  И он, поскольку помнил, где что хранится,
[39:40.000 --> 39:43.000]  направлял нас в одну из этих машин и мы с нее читали данные.
[39:43.000 --> 39:45.000]  Это не работало. Почему?
[39:45.000 --> 39:50.000]  Потому что машина умирает, и мы всю информацию теряем.
[39:52.000 --> 39:55.000]  Прежде чем решать эту задачу, мы сказали следующее.
[39:55.000 --> 39:59.000]  Вообще говоря, вот эта машина, этот контроллер этого абстрактного диска
[39:59.000 --> 40:03.000]  не обязан помнить, где что лежит.
[40:03.000 --> 40:05.000]  Почему?
[40:05.000 --> 40:08.000]  Пусть он просто поднимается пустой, абсолютно ничего не знает.
[40:08.000 --> 40:12.000]  Но каждая машина из этого кластера будет ему периодически сообщать,
[40:12.000 --> 40:15.000]  какие чанки на ней лежат, с какими идентификаторами.
[40:15.000 --> 40:18.000]  Так что эта машина может подняться пустой и подождать некоторое время
[40:18.000 --> 40:21.000]  и постепенно в себе восстановить все состояние.
[40:26.000 --> 40:28.000]  Пока не понятно, чему это помогает.
[40:28.000 --> 40:34.000]  Это помогает, когда мы хотим направить реплики у этого chunk-мастера.
[40:34.000 --> 40:35.000]  Почему?
[40:35.000 --> 40:39.000]  Потому что репликации между этими chunk-мастерами никакой не нужно.
[40:39.000 --> 40:43.000]  Пусть теперь каждая машина сообщает о своих chunk-ах не только ему периодически,
[40:43.000 --> 40:46.000]  а вот всем chunk-мастерам.
[40:46.000 --> 40:50.000]  Так что мы можем прийти на какую-то конкретную из них,
[40:50.000 --> 40:53.000]  на какой-то конкретный мастер, сделать пут через него.
[40:53.000 --> 40:56.000]  Он положит этот chunk сюда, сюда и сюда.
[40:56.000 --> 41:02.000]  И чуть позже вот эти три машины расскажут о новом chunk-е всем остальным chunk-мастерам.
[41:02.000 --> 41:06.000]  Они, конечно, не синхронны, то есть тут eventual consistency,
[41:06.000 --> 41:08.000]  но в конце концов все не сойдутся.
[41:08.000 --> 41:13.000]  И вот такой дизайн тем хорош, что тут легко повышать козоустойчивость.
[41:13.000 --> 41:17.000]  И тут не нужна никакой сложной координации, не нужна задача консенсуса.
[41:17.000 --> 41:19.000]  Нам не нужно ничего упорядочивать.
[41:19.000 --> 41:22.000]  Мы реплицируем монотонно растущее множество.
[41:25.000 --> 41:32.000]  Вот так мы получали chunk-store, который вроде бы легко обеспечивал козоустойчивость.
[41:32.000 --> 41:37.000]  И он рос до тех пор, пока эти машины справлялись с нагрузкой.
[41:37.000 --> 41:39.000]  Они могут ее делить между собой.
[41:39.000 --> 41:42.000]  Но даже если они перестанут справляться,
[41:42.000 --> 41:46.000]  потому что просто очень-очень много chunk-ов, и они про все помнить не могут,
[41:46.000 --> 41:49.000]  то мы эту конструкцию шардировали.
[41:49.000 --> 41:52.000]  То есть мы говорили, будем делить теперь chunk-ы по хэшу.
[41:52.000 --> 41:56.000]  То есть возьмем эту картинку, удвоим ее здесь рядом,
[41:56.000 --> 42:00.000]  и когда нам приходит пользователь с путом, мы берем хэш от его chunk-а,
[42:00.000 --> 42:06.000]  от его данных, и идем статически либо в первый chunk-store, либо во второй chunk-store.
[42:06.000 --> 42:11.000]  И этот chunk-store нам генерирует такой идентификатор, в котором будет зашифровано,
[42:11.000 --> 42:15.000]  а где потом эту chunk искать в первом или втором chunk-store.
[42:15.000 --> 42:22.000]  Так что этот слой довольно тривиальным образом горизонтально масштабируется бесконечно,
[42:22.000 --> 42:24.000]  без задачи консенсуса.
[42:24.000 --> 42:27.000]  То есть вот здесь масштабируемость достигалась.
[42:27.000 --> 42:29.000]  А вот с metastore были проблемы.
[42:29.000 --> 42:32.000]  Здесь данные имутабельные, и реплицировать их одно удовольствие.
[42:32.000 --> 42:34.000]  В metastore данные имутабельные.
[42:34.000 --> 42:38.000]  Мы постоянно, не знаю, переименовываем, копируем, создаем, удаляем,
[42:38.000 --> 42:41.000]  дописываем новые chunk-ы в iNode.
[42:41.000 --> 42:48.000]  И, конечно же, в этой задаче порядок модификации важен.
[42:48.000 --> 42:54.000]  Нам важно, в каком порядке происходят create-файлы и delete-файлы.
[42:54.000 --> 42:59.000]  Поэтому chunk-у metastore мы делали RSL.
[43:02.000 --> 43:07.000]  То есть пусть у нас теперь три машины хранят дерево и iNode
[43:07.000 --> 43:10.000]  и реплицируют все операции добавления chunk-ов,
[43:10.000 --> 43:16.000]  или переименование файлов, создание файлов с помощью multipax или raft.
[43:16.000 --> 43:21.000]  И эта конструкция в целом работает.
[43:21.000 --> 43:24.000]  То есть в ней единой точки отказа нет, как было в GFS.
[43:24.000 --> 43:31.000]  В GFS iNode, дерево и вообще расположение chunk-ов на дисках
[43:31.000 --> 43:34.000]  хранила всего одна машина, один выделенный мастер,
[43:34.000 --> 43:36.000]  единая точка отказа.
[43:36.000 --> 43:40.000]  Но вот здесь мы расположение chunk-ов на дисках перенесли в chunk-store,
[43:40.000 --> 43:46.000]  а все оставшиеся вместо данные мы реплицируем с помощью multipax.
[43:46.000 --> 43:51.000]  Так что отказоустойчивость есть, и масштабируемость в определенных границах тоже есть.
[43:51.000 --> 43:53.000]  Это не бесконечный вопрос.
[43:53.000 --> 43:58.000]  У нас не умеет, что мы придем к chunk-мастеру с лутом,
[43:58.000 --> 44:02.000]  и потом почти сразу придем к соседнему.
[44:02.000 --> 44:06.000]  Но нас это не беспокоит, потому что если мы получили от chunk-store ID,
[44:06.000 --> 44:11.000]  а потом с ним приходим, то мы уверены, что в нем данные есть.
[44:11.000 --> 44:14.000]  Правда ведь? Но он же нам этот ID сгенерировал.
[44:14.000 --> 44:17.000]  Поэтому если мы приходим в другой chunk-сервер,
[44:17.000 --> 44:20.000]  и он нам говорит, что пока не может прочесть,
[44:20.000 --> 44:22.000]  то видимо это означает только одно.
[44:22.000 --> 44:25.000]  То, что он потерял данные, что совсем плохо,
[44:25.000 --> 44:29.000]  либо что он просто не успел про них узнать еще.
[44:29.000 --> 44:33.000]  Поэтому мы просто подождем некоторое время, притраемся, и рано или поздно он нам ответит.
[44:33.000 --> 44:37.000]  То есть это не проблема. Мы уверены, что в нем данные есть.
[44:37.000 --> 44:45.000]  Ну и если мы знаем, с какой периодичностью каждый chunk-сервер шлет свои chunk-мастерам,
[44:45.000 --> 44:50.000]  мы знаем, через какое время примерно сделать ретро и данные прочесть.
[44:50.000 --> 44:53.000]  Так что здесь, оказывается, устойчивость штаммировости есть бесконечная,
[44:53.000 --> 44:55.000]  а вот здесь мы упираемся в RSM.
[44:55.000 --> 45:00.000]  Упираемся в RSM почему? Потому что, во-первых, у нас может быть слишком много файлов в файловой системе,
[45:00.000 --> 45:04.000]  а во-вторых, потому что у нас может быть слишком много chunk-ов в файловой системе.
[45:04.000 --> 45:07.000]  Даже chunk-и большие.
[45:07.000 --> 45:11.000]  О чем пишет нам Facebook в своей свежей статье, кажется, 21 года,
[45:11.000 --> 45:16.000]  про свою файловую систему. Что если вы делаете такой дизайн,
[45:16.000 --> 45:19.000]  то не то чтобы система не будет работать,
[45:19.000 --> 45:23.000]  вы, конечно, поставите какие-то квоты, что пользователь не может заводить там
[45:23.000 --> 45:26.000]  больше какого-то количества файлов, больше какого-то количества chunk-ов,
[45:26.000 --> 45:28.000]  и система способна будет жить.
[45:28.000 --> 45:31.000]  Но она не способна будет бесконечно масштабироваться.
[45:31.000 --> 45:35.000]  И вот с таким дизайном Facebook пишет, что вы можете промасштабироваться
[45:35.000 --> 45:38.000]  до десятков петабайт данных.
[45:38.000 --> 45:42.000]  Потому что, когда вы достигнете такого объема данных в chunk-сторе,
[45:42.000 --> 45:47.000]  вы просто упретесь в количество файлов и количество chunk-ов в мета-сторе.
[45:47.000 --> 45:50.000]  То есть у вас узкое место, оно вот здесь.
[45:50.000 --> 45:52.000]  И вы упираетесь в RSM.
[45:52.000 --> 45:56.000]  Потому что все, что реплицируется на RSM, должно помещаться в одну машину.
[45:56.000 --> 45:59.000]  Ну, вот тут можно было сказать, шардируем.
[45:59.000 --> 46:02.000]  Потому что есть таблиты, они как-то независимые.
[46:02.000 --> 46:05.000]  А вот здесь дерево, и оно логически цельное.
[46:05.000 --> 46:08.000]  Его сложно шардировать.
[46:08.000 --> 46:12.000]  Ну, не то что невозможно, но сложно да возможно.
[46:12.000 --> 46:16.000]  То есть можно хранить iNode на разных машинах.
[46:16.000 --> 46:20.000]  Ну, то есть можно придумать какую-то схему, которая позволит мета-стор шардировать.
[46:20.000 --> 46:24.000]  Другое дело, что это сложно и неудобно.
[46:24.000 --> 46:30.000]  Так вот, как же в этом месте справиться с узким местом, с мета-стором,
[46:30.000 --> 46:35.000]  и масштабироваться за десятки петабайт?
[46:35.000 --> 46:38.000]  А мы хотим экзобайты.
[46:38.000 --> 46:41.000]  То есть это сотни петабайт, тысячи петабайт.
[46:41.000 --> 46:44.000]  Мы хотим на несколько порядков больше данных.
[46:44.000 --> 46:49.000]  В одну машину это не поместится уже никак.
[46:49.000 --> 46:52.000]  Понятна проблема?
[46:52.000 --> 46:56.000]  То есть у нас вот здесь узкое место.
[46:56.000 --> 46:59.000]  Но оно было здесь узким местом, здесь оно шардировалось проще.
[46:59.000 --> 47:03.000]  А тут теперь сложнее.
[47:15.000 --> 47:19.000]  У вас же начался HDFS?
[47:19.000 --> 47:22.000]  Наконец. Здорово.
[47:22.000 --> 47:26.000]  HDFS умеет так шардироваться, это называется, кажется, федерация.
[47:26.000 --> 47:30.000]  Но это костыль.
[47:30.000 --> 47:35.000]  То есть вы говорите, что вот у меня есть одно поддерево, есть другое поддерево, я с ним никогда не работаю.
[47:35.000 --> 47:41.000]  Но это не совсем правда.
[47:41.000 --> 47:43.000]  Мы хотим какого-то...
[47:43.000 --> 47:47.000]  Все, что происходит сегодня, оно должно быть в какой-то степени астроумно.
[47:47.000 --> 47:50.000]  Вот эта идея была астроумной, правда? Сложности поспорить.
[47:50.000 --> 47:52.000]  Но мне так кажется, что оно астроумное.
[47:52.000 --> 47:54.000]  Положить методанные Bigtable и Bigtable.
[47:54.000 --> 47:58.000]  Вот здесь тоже нужно что-то придумать похожее.
[47:58.000 --> 48:02.000]  Нет, не понимаете?
[48:02.000 --> 48:04.000]  У нас, смотрите, есть методанные.
[48:04.000 --> 48:06.000]  Чем они отличаются от чанков?
[48:06.000 --> 48:09.000]  Тем, что чанки мутабельны, а здесь все очень мутабельно, наоборот.
[48:09.000 --> 48:12.000]  И порядок апдейтов важен.
[48:12.000 --> 48:18.000]  Мы вот точечно меняем файлы, точечно меняем записи в айнодах.
[48:18.000 --> 48:20.000]  Отходим в произвольные места и что-то там меняем.
[48:20.000 --> 48:24.000]  Порядок важен.
[48:24.000 --> 48:25.000]  Нет?
[48:25.000 --> 48:32.000]  Не понимаем, да?
[48:32.000 --> 48:34.000]  У нас имутабельные чанки, извиняюсь.
[48:34.000 --> 48:37.000]  У нас единица имутабельности, единица хранения очень большая.
[48:37.000 --> 48:39.000]  Это там гигабайты, например, могут быть.
[48:39.000 --> 48:43.000]  Ты хочешь сделать Copy-on-Write на объектах, например, гигабайтами?
[48:43.000 --> 48:46.000]  Гигабайты это не сработает.
[48:46.000 --> 48:48.000]  А у нас же с лева были мутабельные данные.
[48:48.000 --> 48:50.000]  С лева? В смысле, с лева где?
[48:50.000 --> 48:51.000]  Вот на левой части.
[48:51.000 --> 48:53.000]  Да, вот здесь были мутабельные данные.
[48:53.000 --> 48:55.000]  Блестящие идеи.
[48:55.000 --> 48:57.000]  Давайте положим не только методанные Bigtable и Bigtable,
[48:57.000 --> 48:59.000]  давайте положим методанные файловые системы,
[48:59.000 --> 49:01.000]  которые в ее хранилище тоже.
[49:01.000 --> 49:03.000]  Вот ровно так Facebook и поступает.
[49:11.000 --> 49:16.000]  Вот, у них есть файловая система под названием Tectonic.
[49:16.000 --> 49:21.000]  И нам будет, конечно, сложно разглядеть.
[49:21.000 --> 49:25.000]  Но они хранятся, смотрите, сейчас структура их.
[49:25.000 --> 49:29.000]  Вот, у них есть ChunkStore, который бесконечно масштабируется,
[49:29.000 --> 49:31.000]  он хранит данные файлов.
[49:31.000 --> 49:33.000]  Блоки, чанки, как вы их назовете.
[49:33.000 --> 49:35.000]  Там есть некоторые нюансы.
[49:35.000 --> 49:37.000]  У вас есть методанные.
[49:37.000 --> 49:39.000]  Это дерево-файловые системы,
[49:39.000 --> 49:41.000]  это inode-файлов и это еще один вспомогательный уровень,
[49:41.000 --> 49:43.000]  сейчас в нем не хочу.
[49:43.000 --> 49:47.000]  Так вот, эти данные помещаются в KeyValue-хранилище.
[49:47.000 --> 49:51.000]  И переложить их в KeyValue-хранилище можно довольно понятным образом.
[49:51.000 --> 49:55.000]  Сложно, не знаю, что нужно сделать.
[49:55.000 --> 50:01.000]  Давайте, может быть, если сделать поярче, это не работает, да?
[50:01.000 --> 50:05.000]  Ну ладно, я не хочу так делать часто просто.
[50:05.000 --> 50:07.000]  Экран опускается.
[50:07.000 --> 50:11.000]  Давай по такому поводу подождем его.
[50:11.000 --> 50:15.000]  Вот, невероятно.
[50:15.000 --> 50:19.000]  Остановись.
[50:19.000 --> 50:21.000]  Сейчас, давайте еще немного.
[50:21.000 --> 50:27.000]  Вот, это схема укладки методанных в KeyValue.
[50:27.000 --> 50:31.000]  Как в KeyValue представлена директория?
[50:31.000 --> 50:35.000]  Директория — это набор ключей, составных,
[50:35.000 --> 50:39.000]  где составляют стандарты.
[50:39.000 --> 50:41.000]  В KeyValue представлена директория.
[50:41.000 --> 50:45.000]  Директория — это набор ключей, составных,
[50:45.000 --> 50:49.000]  где сначала идет ID-директория, а потом идет имя файла.
[50:49.000 --> 50:53.000]  Откуда появляется ID-директория, тут, видимо, такой же слой, неважно.
[50:53.000 --> 50:57.000]  Вы уже узнали ID-директории по другому запросу,
[50:57.000 --> 51:01.000]  по вспомогательному ключу, из директории в ID-директории.
[51:01.000 --> 51:05.000]  А теперь вы хотите, скажем, пролистать файлы директории.
[51:05.000 --> 51:09.000]  Для этого вы делаете итерацию по префиксу dir-id.
[51:09.000 --> 51:15.000]  И вот вы итерируетесь по ключам, где второй компонент ключа, служебный, — это имя файла.
[51:15.000 --> 51:21.000]  Если вы хотите узнать, из каких блоков состоит файл, чтобы его прочитать,
[51:21.000 --> 51:29.000]  вы сначала получаете директорию, потом вы обращаетесь по такому ключу и узнаете файл ID.
[51:29.000 --> 51:37.000]  А дальше вы листите хранилище по ключу с таким префиксом.
[51:37.000 --> 51:41.000]  И вот в тех ключах, которые вы перечисляете итератором,
[51:41.000 --> 51:45.000]  вы из второго компонента извлекаете идентификатор-блок.
[51:45.000 --> 51:54.000]  Точнее, вы знаете, что блоки вашего файла перечислены в отдельных ключах,
[51:54.000 --> 51:58.000]  и вот задав итерацию по такому префиксу, вы можете перечислить все его блоки
[51:58.000 --> 52:01.000]  и узнать в конце концов про их расположение.
[52:01.000 --> 52:04.000]  Ну, короче, понятно, что происходит.
[52:04.000 --> 52:10.000]  То есть у вас каждая директория выложена как набор ключей с одинаковым префиксом,
[52:10.000 --> 52:14.000]  и каждый файл, список чанков каждого файла, их же может быть очень много,
[52:14.000 --> 52:18.000]  тоже выложен как набор ключей.
[52:18.000 --> 52:22.000]  Мы не можем положить их в одну запись, потому что она может быть очень большой.
[52:22.000 --> 52:30.000]  Мы хотим ограничить размер записи, поэтому мы делаем для каждого файла много служебных ключей,
[52:30.000 --> 52:34.000]  где второй компонент ключа — это идентификатор-блока, который...
[52:34.000 --> 52:41.000]  блока, в который составляют этот файл.
[52:41.000 --> 52:47.000]  Понятно? Но есть проблемы.
[52:47.000 --> 52:50.000]  Ну вот, смотрите, k-value мы масштабировать умеем.
[52:50.000 --> 52:55.000]  Поэтому мы умеем масштабировать метод данной файловой системы более-менее бесконечно.
[52:55.000 --> 52:59.000]  Так что мы умеем бесконечно масштабировать файловую систему теперь.
[52:59.000 --> 53:05.000]  Но есть неудобства, которые возникают из-за такого дизайна, связанные именно с k-value.
[53:05.000 --> 53:12.000]  Вот мы кое-что потеряли относительно дизайна с R7, который у нас был раньше.
[53:12.000 --> 53:16.000]  Понятно или нет, что мы потеряли?
[53:16.000 --> 53:22.000]  Ну вот, раньше мы могли сделать атомарные операции, потому что RSM — это же...
[53:22.000 --> 53:26.000]  вам хочется, не знаю, взять и переименовать 100 файлов разом.
[53:26.000 --> 53:32.000]  Вы берете и в RSM отправляете служебную команду, которая говорит, вот атомарно переименуем 100 файлов.
[53:32.000 --> 53:40.000]  И эта вот служебная команда, такая транзакция, она через лог упрощается с другими командами, как единое целое.
[53:40.000 --> 53:45.000]  И RSM применяется все эти 100 переименований подряд, как будто бы атомарно.
[53:45.000 --> 53:50.000]  Потому что это одна машина, в конце концов.
[53:50.000 --> 53:59.000]  Вот здесь уже разные файлы и разные директории, они находятся потенциально в разных таблетах k-value хранилища.
[53:59.000 --> 54:03.000]  И атомарной работы с ними уже нет.
[54:03.000 --> 54:06.000]  Ну точнее нет, если у вас нет транзакций.
[54:06.000 --> 54:12.000]  В Bigtable транзакций нет, в k-value хранилища Facebook, вот этот самый ZPDB,
[54:12.000 --> 54:19.000]  а вот это k-value хранилища, это ZPDB, про который я говорил раньше, там тоже транзакций нет.
[54:19.000 --> 54:25.000]  Короче говоря, вы не можете сделать ренейм из произвольного места в произвольное место файловой системы.
[54:25.000 --> 54:27.000]  Ну там move сделать.
[54:27.000 --> 54:34.000]  Но вы все-таки хотите какую-то атомарность иметь.
[54:34.000 --> 54:41.000]  Тут все понятно, да? Я могу это убрать.
[54:41.000 --> 54:46.000]  Хотя нет, рано.
[54:46.000 --> 54:53.000]  Но все-таки какую-то атомарность вы хотите иметь, скажем, вы хотите, я не знаю, иметь атомарность в пределах одной директории.
[54:53.000 --> 54:55.000]  Как Facebook этого достигает?
[54:55.000 --> 55:00.000]  Ну у вас есть k-value хранилища, там есть таблицы, в ней есть таблеты.
[55:00.000 --> 55:02.000]  И вот каждый таблет это RSM.
[55:02.000 --> 55:06.000]  Поэтому внутри таблета вы можете работать с данными атомарно.
[55:06.000 --> 55:08.000]  Вы можете там транзакции делать.
[55:08.000 --> 55:14.000]  Вообще говоря, вот в Bigtable, если вы читали статью, там нет транзакций между разными строчками,
[55:14.000 --> 55:17.000]  потому что границы таблетов неизвестны.
[55:17.000 --> 55:19.000]  Но вот Facebook делает так.
[55:19.000 --> 55:29.000]  Он раскладывает данные так, чтобы ключи с одинаковым префиксом вот таким вот попадали строго в один шарт, в один таблет.
[55:29.000 --> 55:33.000]  Поэтому Facebook уверен, что вот так расположив данные в k-value хранилище,
[55:33.000 --> 55:37.000]  любая операция над одним файлом будет атомарной.
[55:37.000 --> 55:47.000]  Но у вас нет атомарности на совершенно произвольных местах в дереве файловой системы.
[55:47.000 --> 55:49.000]  Ну и еще один нюанс.
[55:49.000 --> 55:52.000]  Вы, скажем, не можете легко посчитать, сколько у вас занимает под дерево,
[55:52.000 --> 55:57.000]  потому что это много запросов, нужно вот так вот рекурсивно обходить.
[55:57.000 --> 56:02.000]  Что? Нет, в директорию можно положить хоть миллионы файлов.
[56:02.000 --> 56:16.000]  Ну как? Я думаю, что миллионы файлов в директории помещаются все-таки в таблетах размером 100 мегабайт.
[56:16.000 --> 56:19.000]  У тебя же записи очень маленькие, это важно здесь.
[56:19.000 --> 56:21.000]  Про метод данные говорим.
[56:21.000 --> 56:24.000]  Нет, а данные, они имутабельные из чанков.
[56:24.000 --> 56:26.000]  Нет, мы говорим про метод данные.
[56:26.000 --> 56:29.000]  Директория одна, но файлы, конечно, не могут поместиться в один таблет,
[56:29.000 --> 56:33.000]  но как бы данные-то и не в таблетах хранятся в конце концов, они хранятся в чанк-сторе.
[56:33.000 --> 56:38.000]  А метод данные помещаются в один таблет, поэтому можно с этим жить.
[56:38.000 --> 56:46.000]  Ну и плюс Фейсбук пишет, что файлы одной директории в конце концов размазываются по разным таблетам,
[56:46.000 --> 56:51.000]  потому что у них там разные файлы ID, поэтому когда они читают даже параллельно файлы из одной директории,
[56:51.000 --> 56:54.000]  они хорошо распределяют нагрузку по всему киеварю-хранилищу.
[56:54.000 --> 56:58.000]  Ну в общем, такая вот идея.
[56:58.000 --> 57:00.000]  Понятно?
[57:00.000 --> 57:04.000]  Что мы научились масштабировать киеварю-хранилищу,
[57:04.000 --> 57:07.000]  положив метод данные и киеварю-хранилищу в киеварю-хранилищу,
[57:07.000 --> 57:10.000]  и мы научились масштабировать файловую систему,
[57:10.000 --> 57:16.000]  положив и метод данные тоже в киеварю-хранилищу, но правда аккуратно положив.
[57:16.000 --> 57:20.000]  Следующий шаг.
[57:20.000 --> 57:25.000]  А вот теперь мы поговорим про бектейбл.
[57:25.000 --> 57:30.000]  Смотрите, мы сделали DFS через киеварю.
[57:30.000 --> 57:35.000]  Теперь забудем во всем этом, в смысле, что мы масштабировали DFS, вернемся киеварю-хранилищу.
[57:35.000 --> 57:39.000]  И подумаем, насколько разумен этот дизайн, который у нас был придуман.
[57:39.000 --> 57:44.000]  Вот что мы делали?
[57:44.000 --> 57:47.000]  Где мой карандаш?
[57:47.000 --> 57:52.000]  Как выглядела вся конструкция для киеварю-хранилища?
[57:55.000 --> 58:01.000]  Мы брали узлы с дисками и на каждый узел помещали локальное хранилище.
[58:01.000 --> 58:06.000]  LSM. LevelDB и ROXDB.
[58:06.000 --> 58:10.000]  Для того, чтобы если машина перезагрузится, она данные не потеряла.
[58:10.000 --> 58:21.000]  Но машина могла совсем отказать, поэтому мы эти LSM-ы реплицировали между машинами.
[58:21.000 --> 58:24.000]  Мы решали задачу репликации.
[58:24.000 --> 58:34.000]  Этот уровень, мультипаксис, он упорядочивал на разных репликах одного таблета в записи в копии LevelDB.
[58:34.000 --> 58:43.000]  А поверх этого мы сделали еще шардирование.
[58:43.000 --> 58:48.000]  Посмотрим сюда, насколько эта конструкция разумна.
[58:48.000 --> 58:50.000]  Она понятна, да?
[58:50.000 --> 58:54.000]  Но зачем нам здесь репликация?
[58:54.000 --> 58:58.000]  Потому что мы не доверяем конкретной машине с конкретным LevelDB.
[58:58.000 --> 59:06.000]  Она рестарта переживет, а вот если она совсем сломается, то мы потеряем ее диск целиком и LevelDB вместе с ним.
[59:06.000 --> 59:19.000]  Поэтому мы берем этот LevelDB и реплицируем, потому что он живет поверхненадежной файловой системы.
[59:19.000 --> 59:22.000]  Я бы сказал, что мы это уже сделали.
[59:22.000 --> 59:24.000]  У нас уже есть файловая система понадежнее.
[59:24.000 --> 59:26.000]  Она распределенная и отказаустойчивая.
[59:26.000 --> 59:30.000]  Она реплицирует методанные, она реплицирует данные.
[59:30.000 --> 59:32.000]  Почему бы нам...
[59:32.000 --> 59:36.000]  Мы здесь делаем LSM, потом его реплицируем.
[59:36.000 --> 59:39.000]  Почему бы нам не перевернуть эти два уровня?
[59:39.000 --> 59:45.000]  Почему бы нам не сделать LSM поверх файловой системы, которая не может потеряться?
[59:45.000 --> 59:50.000]  Почему бы нам не сделать LSM поверх распределенной файловой системы?
[59:50.000 --> 59:53.000]  Ну вот ровно так и делает Google.
[59:59.000 --> 01:00:04.000]  Ну а что поделать? Придется немного подождать.
[01:00:09.000 --> 01:00:11.000]  Вот устройство их таблета.
[01:00:15.000 --> 01:00:20.000]  Вот это буквально один LSM.
[01:00:22.000 --> 01:00:24.000]  Но вот этот LSM из чего состоит?
[01:00:24.000 --> 01:00:28.000]  Он состоит из лога, из s-tables и из m-tables.
[01:00:28.000 --> 01:00:32.000]  И мы реплицируем всю эту конструкцию вот здесь под проектором,
[01:00:32.000 --> 01:00:36.000]  потому что мы боимся, что мы потеряем диск, на котором лежат лог и s-tables.
[01:00:36.000 --> 01:00:38.000]  Мы берем три диска.
[01:00:38.000 --> 01:00:42.000]  Ну вот здесь мы берем просто файловую систему, которая под капотом все реплицирует.
[01:00:42.000 --> 01:00:45.000]  Поэтому мы кладем в нее один лог.
[01:00:45.000 --> 01:00:50.000]  Ну для каждого таблета один лог и один набор s-tables.
[01:00:53.000 --> 01:00:55.000]  То есть фактически мы переворачиваем два уровня.
[01:00:55.000 --> 01:00:59.000]  У нас теперь репликация не над LSM находится, а под LSM.
[01:01:02.000 --> 01:01:08.000]  Ну у тебя были машинки с levelDB, чтобы надежно хранить большие данные внутри себя.
[01:01:08.000 --> 01:01:11.000]  Но ты и боялся, что ты каждую машину можешь потерять.
[01:01:11.000 --> 01:01:15.000]  Поэтому ты реплицировал эти levelDB через multipax.
[01:01:15.000 --> 01:01:17.000]  Мы говорим, реплицировать автомат.
[01:01:17.000 --> 01:01:20.000]  Автомат в данном случае это levelDB, это локальное хранилище.
[01:01:23.000 --> 01:01:25.000]  Ну метод данных плюс данных.
[01:01:25.000 --> 01:01:28.000]  Метод данных реплицируется через QVAL, мы только что это сделали.
[01:01:28.000 --> 01:01:30.000]  Данные реплицируются через chunk store.
[01:01:30.000 --> 01:01:33.000]  Когда мы писали каждый chunk, мы писали его в три копии в chunk store.
[01:01:34.000 --> 01:01:37.000]  Вот, так что мы теперь не боимся это потерять.
[01:01:37.000 --> 01:01:40.000]  Файловая система отвечает за надежное хранение этих данных.
[01:01:40.000 --> 01:01:44.000]  Единственное, что нам остается сделать, выбрать точку обслуживания.
[01:01:44.000 --> 01:01:52.000]  Потому что непонятно, у кого в голове мем тейбл и кто в этот лог делает записи.
[01:01:55.000 --> 01:01:58.000]  Почему это разумно? Почему такой дизайн лучше?
[01:01:58.000 --> 01:02:00.000]  Понятно ли вам?
[01:02:01.000 --> 01:02:04.000]  Это же очень глубокая идея.
[01:02:04.000 --> 01:02:07.000]  Это важнее, чем алгоритмы, чем оптимизация мультипаксиса.
[01:02:07.000 --> 01:02:11.000]  То есть это тоже важно, но сложность она вот здесь примерно, в этой идее.
[01:02:11.000 --> 01:02:16.000]  Давайте перевернуть. Не делать репликацию поверх LSM, а LSM поверх репликации.
[01:02:16.000 --> 01:02:19.000]  Каковы преимущества такого дизайна?
[01:02:19.000 --> 01:02:36.000]  Блестящее замечание. У нас был слой репликации над слоем хранилища.
[01:02:36.000 --> 01:02:41.000]  И вот хранилище было очень мутабельное. Мы туда писали и читали постоянно.
[01:02:41.000 --> 01:02:44.000]  И репликация была сложная, это был консенсус.
[01:02:44.000 --> 01:02:47.000]  Потому что он упорядочивал конкурирующие апдейты.
[01:02:47.000 --> 01:02:52.000]  Если мы перевернем всю конструкцию, сделаем LSM поверх DFS,
[01:02:52.000 --> 01:02:57.000]  то в файловую систему будут спускаться уже имутабельные чанки.
[01:02:57.000 --> 01:03:01.000]  И репликация будет имутабельных данных.
[01:03:01.000 --> 01:03:05.000]  А реплицировать имутабельные данные намного проще, чем мутабельные.
[01:03:05.000 --> 01:03:08.000]  Ну и намного эффективнее.
[01:03:08.000 --> 01:03:12.000]  У нас была репликация вот здесь.
[01:03:12.000 --> 01:03:16.000]  Она была тройной. У нас были три LSM на трех машинах.
[01:03:16.000 --> 01:03:20.000]  И мы через Multipax их кормили одинаковыми последовательствами команд.
[01:03:20.000 --> 01:03:24.000]  То есть в три раза у нас есть гигабайт ваших данных.
[01:03:24.000 --> 01:03:27.000]  Они превращались в три гигабайта данных на трех машинах.
[01:03:27.000 --> 01:03:31.000]  Когда мы говорили про определенную файловую систему, я вам рассказывал,
[01:03:31.000 --> 01:03:35.000]  что имутабельные чанки не обязательно хранить в трех копиях.
[01:03:35.000 --> 01:03:38.000]  Для того, чтобы получить отказу устойчивости две машины, два диска.
[01:03:38.000 --> 01:03:41.000]  Вы можете использовать erasure coding.
[01:03:41.000 --> 01:03:45.000]  То есть избыточное кодирование, которое будет экономнее.
[01:03:45.000 --> 01:03:49.000]  Вы делите ваш чанк на шесть блоков.
[01:03:49.000 --> 01:03:54.000]  Добавляете к нему еще, к этим шести блокам, три чек суммы специальные
[01:03:54.000 --> 01:03:57.000]  с помощью кода Фридес Ламона.
[01:03:57.000 --> 01:04:01.000]  И вот вы переживаете два отказа, но при этом у вас overhead по диску в полтора,
[01:04:01.000 --> 01:04:03.000]  а не в три раза.
[01:04:03.000 --> 01:04:06.000]  Ровно потому, что у вас данные теперь имутабельные.
[01:04:06.000 --> 01:04:10.000]  Ну да, это не совсем бесплатно, потому что там нужно уметь их восстанавливать,
[01:04:10.000 --> 01:04:13.000]  еще эти данные при потере каких-то блоков чанка.
[01:04:13.000 --> 01:04:17.000]  Но все же вы можете сэкономить себе в два раза больше дисков.
[01:04:17.000 --> 01:04:20.000]  Ну представьте, у вас миллион дисков, а теперь у вас 500 тысяч дисков.
[01:04:20.000 --> 01:04:22.000]  Это очень много.
[01:04:22.000 --> 01:04:26.000]  В смысле денег очень много, дисков много и денег очень много.
[01:04:26.000 --> 01:04:31.000]  И вот просто переставивая два уровня, вы получаете, вы экономите 500 тысяч дисков.
[01:04:31.000 --> 01:04:33.000]  Неплохо, да?
[01:04:37.000 --> 01:04:39.000]  Ну можно еще меньше хранить.
[01:04:39.000 --> 01:04:43.000]  Там коэффициент избыточности не один, не полтора, а там 1.33.
[01:04:43.000 --> 01:04:49.000]  С репликацией через multipax ты такого не получишь, потому что там данные имутабельные.
[01:04:49.000 --> 01:04:55.000]  Второй бонус, что ты логически разделяешь хранение данных и обслуживание данных.
[01:04:55.000 --> 01:04:59.000]  Вот раньше тебе нужно было три реплики, которые там в памяти держат эти memtable в себе,
[01:04:59.000 --> 01:05:01.000]  ну, воспроизводят этот LSM.
[01:05:01.000 --> 01:05:05.000]  У тебя здесь для каждого таблета должна быть только одна машина,
[01:05:05.000 --> 01:05:11.000]  которая хранит у себя в памяти memtable и обслуживает путы и геты.
[01:05:11.000 --> 01:05:14.000]  И при этом эта машина может легко умереть.
[01:05:14.000 --> 01:05:20.000]  Она умрет, не страшно, мы выберем другую, потому что данные не потеряются, данные не нужно двигать.
[01:05:20.000 --> 01:05:24.000]  Вот гораздо легче балансировать таблеты.
[01:05:24.000 --> 01:05:29.000]  Раньше, чтобы передвинуть таблет на другую машину, нужно было данные двигать, как-то тяжело.
[01:05:29.000 --> 01:05:34.000]  Вот сейчас мы можем просто подвинуть, то есть сказать, что эта машина перестанет обслуживать запросы,
[01:05:34.000 --> 01:05:39.000]  живая и другая, а данные все равно в DFS лежат уже, их двигать не нужно.
[01:05:39.000 --> 01:05:43.000]  Ну, или их можно дури прецентрировать под капотом DFS, но это вот уже гораздо проще.
[01:05:43.000 --> 01:05:49.000]  Это вообще очень фундаментально важная идея про разделение точки обслуживания и отказа устойчивого хранения.
[01:05:49.000 --> 01:05:53.000]  Вот вы приходите в облако, заказываете себе виртуалку, я уже говорил вам об этом.
[01:05:53.000 --> 01:05:56.000]  Вы получаете себе конкретную машину с процессором, который исполняет ваш код.
[01:05:56.000 --> 01:05:58.000]  Вот эта машина может умереть.
[01:05:58.000 --> 01:06:03.000]  Но диск под ней не умрет, потому что диск это некоторая абстракция.
[01:06:03.000 --> 01:06:08.000]  Ваша файловая система, ваша виртуалка работает поверх бочного устройства сетевого,
[01:06:08.000 --> 01:06:15.000]  но и в случае Google ваш диск на самом деле хранится в системе Colossus DFS.
[01:06:15.000 --> 01:06:20.000]  Вот здесь та же самая идея, и это все приводит к более гибкому дизайну.
[01:06:20.000 --> 01:06:22.000]  Давайте я еще один пример вам покажу.
[01:06:22.000 --> 01:06:28.000]  У нас в будущем, не таком ударёком, кстати, уже, будет система Spanner.
[01:06:28.000 --> 01:06:32.000]  Это будет разговор про транзакции, про TrueTime, про транзакции, про все вместе.
[01:06:32.000 --> 01:06:39.000]  Но вот смотрите, как устроены шарды в Spanner.
[01:06:39.000 --> 01:06:45.000]  Там тоже есть таблеты, у таблета есть реплика в каждом DC,
[01:06:45.000 --> 01:06:51.000]  и в каждом DC этот таблет, это RSM, все, мультипакс здесь работает.
[01:06:51.000 --> 01:06:57.000]  Но при этом каждая машина, каждая реплика, эта RSM хранит данные не на своем диске,
[01:06:57.000 --> 01:07:01.000]  а хранит данные в Colossus, в файловой системе распределенной.
[01:07:01.000 --> 01:07:05.000]  То есть Google нигде про диски не думает, нигде с дисками конкретными не работает,
[01:07:05.000 --> 01:07:06.000]  потому что они ломаются.
[01:07:06.000 --> 01:07:10.000]  А вот файловая система не ломается, то есть она скрывает все эти сбои отдельных дисков.
[01:07:18.000 --> 01:07:22.000]  Ну что, осилили ли идею?
[01:07:22.000 --> 01:07:26.000]  Сделали K-Value поверх RSM.
[01:07:26.000 --> 01:07:31.000]  Сейчас сделали, соберусь, сделали RSM поверх файловой системы.
[01:07:31.000 --> 01:07:51.000]  Так нет, в этом же идее и есть.
[01:07:51.000 --> 01:07:55.000]  Ну то есть в Google никакая система, кажется, не должна использовать
[01:07:55.000 --> 01:07:58.000]  локальную файловую систему, локальный диск.
[01:07:58.000 --> 01:08:02.000]  Она должна прямо либо косвенно, через другие системы, хранить свои данные в Colossus.
[01:08:02.000 --> 01:08:06.000]  Вот ровно поэтому важно уметь масштабировать распределенную файловую систему.
[01:08:21.000 --> 01:08:23.000]  Блестящее замечание.
[01:08:23.000 --> 01:08:25.000]  Мы продили некоторый цикл.
[01:08:25.000 --> 01:08:28.000]  Это довольно неудобно, да?
[01:08:28.000 --> 01:08:30.000]  Ясно?
[01:08:30.000 --> 01:08:32.000]  Проблема понятна? Повторить?
[01:08:32.000 --> 01:08:34.000]  Мы, смотрите, научились масштабировать.
[01:08:34.000 --> 01:08:37.000]  Мы можем построить просто отдельные K-Value.
[01:08:37.000 --> 01:08:42.000]  Вот сделать, завести много машин, поставить на каждые там быстрые диски,
[01:08:42.000 --> 01:08:45.000]  на каждые машины поставить LSM в виде RocksDB,
[01:08:45.000 --> 01:08:49.000]  реплицировать где-то наши данные, протеционировать их там вот горизонтально,
[01:08:49.000 --> 01:08:52.000]  сделать каждый таблет отдельным RSM,
[01:08:52.000 --> 01:08:56.000]  реплицировать эти LLDB с помощью MultiPax из пределов таблета,
[01:08:56.000 --> 01:09:00.000]  ну вот построить такое автономное K-Value хранилище.
[01:09:00.000 --> 01:09:02.000]  Вот Facebook так и сделала.
[01:09:02.000 --> 01:09:05.000]  Они построили автономное K-Value хранилище без зависимости.
[01:09:05.000 --> 01:09:10.000]  И через него сделали масштабируемую файловую систему.
[01:09:10.000 --> 01:09:14.000]  Вот у них там миллиарды файлов, и там экзобайты данных в них хранятся.
[01:09:14.000 --> 01:09:19.000]  И DFS, Tectonic, файловая система Facebook, они используют ZPDB и K-Value хранилище
[01:09:19.000 --> 01:09:22.000]  для хранения метод данных.
[01:09:22.000 --> 01:09:25.000]  Тут проблемы никакой нет у них.
[01:09:25.000 --> 01:09:29.000]  Но если мы гугл, то мы говорим, что вот K-Value хранилище можно делать эффективнее,
[01:09:29.000 --> 01:09:33.000]  если делать наоборот, не DFS делать через K-Value хранилище,
[01:09:33.000 --> 01:09:37.000]  а K-Value хранилище через DFS.
[01:09:37.000 --> 01:09:42.000]  Потому что мы хотим хранить LSM каждого таблета прямо в DFS сразу.
[01:09:44.000 --> 01:09:49.000]  Тогда получается, что мы не умеем масштабировать DFS.
[01:09:49.000 --> 01:09:58.000]  Потому что мы не можем использовать в Bigtable DFS, а в DFS Bigtable.
[01:09:58.000 --> 01:10:01.000]  Нам нужно что-то выбрать.
[01:10:01.000 --> 01:10:05.000]  На самом деле как? На самом деле можем.
[01:10:05.000 --> 01:10:13.000]  Смотрите, вот у вас есть уже DFS, вы его написали.
[01:10:13.000 --> 01:10:18.000]  Он правда плохой, отказаустойчивый, отказа неустойчивый.
[01:10:18.000 --> 01:10:21.000]  То есть у него есть одна точка отказа мастер.
[01:10:21.000 --> 01:10:26.000]  Но при этом у вас он уже есть.
[01:10:26.000 --> 01:10:29.000]  Почему бы им не воспользоваться?
[01:10:29.000 --> 01:10:40.000]  Сейчас пишите GFS версии 2, который хочет масштабироваться.
[01:10:40.000 --> 01:10:43.000]  В нем может быть очень много данных.
[01:10:43.000 --> 01:10:45.000]  Но как это сделать?
[01:10:45.000 --> 01:10:57.000]  Нужно поделить систему на мета-store и chunk-store.
[01:10:57.000 --> 01:11:10.000]  И зря стерозите эту картинку, chunk-store у нас уже какой-то есть.
[01:11:10.000 --> 01:11:14.000]  И он там горизонтально масштабируется.
[01:11:14.000 --> 01:11:17.000]  Мета-store у нас нет.
[01:11:17.000 --> 01:11:23.000]  В смысле, у нас есть пока отдельная, в GFS версии 1 это отдельная машина.
[01:11:23.000 --> 01:11:26.000]  Ну неприятно, мы бы хотели заменить эту отдельную машину на key-value.
[01:11:26.000 --> 01:11:32.000]  И в это key-value уложить каким-то образом дерево нашей файловой системы.
[01:11:32.000 --> 01:11:35.000]  Ну у нас же есть Bigtable в Google.
[01:11:35.000 --> 01:11:41.000]  Ну вот давайте положим данные мета-store в Bigtable.
[01:11:41.000 --> 01:11:47.000]  Правда Bigtable нужна файловая система ведь все равно.
[01:11:47.000 --> 01:11:52.000]  Но что приятно, что она нужная, а она правда поменьше файловой системы.
[01:11:52.000 --> 01:11:58.000]  Ну то есть вот здесь мы хотим построить систему, которая масштабируется очень широко.
[01:11:58.000 --> 01:12:01.000]  Но у нас есть GFS, которая не может масштабироваться широко,
[01:12:01.000 --> 01:12:05.000]  потому что он опирается в объеме данных в мастере.
[01:12:05.000 --> 01:12:11.000]  Но с другой стороны, мы же что в этом Bigtable собираемся хранить?
[01:12:11.000 --> 01:12:18.000]  Мета-данные большой файловой системы.
[01:12:18.000 --> 01:12:22.000]  А сколько их будет по сравнению с объемом данных?
[01:12:22.000 --> 01:12:26.000]  Ну вот Google говорит, что когда вы строите систему для хранения данных,
[01:12:26.000 --> 01:12:34.000]  то мета-данные у нее это примерно одна десятитысячная от объема данных.
[01:12:34.000 --> 01:12:41.000]  Так вот, вот нам в этом Bigtable для хранения мета-данных GFS версии 2
[01:12:41.000 --> 01:12:51.000]  нужно в десять тысяч раз меньше данных хранить, чем хранится мета-данных, чем данных.
[01:12:51.000 --> 01:12:53.000]  Так что что мы можем сделать здесь?
[01:12:53.000 --> 01:12:55.000]  Как мы можем сделать этот Bigtable?
[01:12:55.000 --> 01:13:02.000]  Сделать его поверх GFS 1.
[01:13:03.000 --> 01:13:07.000]  То есть он не масштабируется, в нем есть точка отказа,
[01:13:07.000 --> 01:13:13.000]  но по крайней мере с масштабируемостью проблемы нет.
[01:13:13.000 --> 01:13:17.000]  Потому что масштабируемости GFS версии 1 достаточно для того,
[01:13:17.000 --> 01:13:23.000]  чтобы вместить все мета-данные для GFS версии 2.
[01:13:23.000 --> 01:13:30.000]  Вот этот GFS хранит мета-данные, этот GFS через Bigtable косвенно.
[01:13:30.000 --> 01:13:32.000]  Но правда есть точка отказа.
[01:13:34.000 --> 01:13:36.000]  Что с ней делать?
[01:13:45.000 --> 01:13:48.000]  То есть вся эта конструкция держится где-то на одной машине.
[01:13:48.000 --> 01:13:50.000]  Вот если она умрет, то...
[01:13:50.000 --> 01:13:53.000]  Не то чтобы система остановится, это неправда, но мы проще поговорим еще.
[01:13:53.000 --> 01:13:55.000]  Но как-то некомфортно.
[01:13:56.000 --> 01:13:59.000]  Либо придумать что-то, либо укреплять.
[01:13:59.000 --> 01:14:04.000]  Давайте переименуем GFS 2 в Colossus, чтобы было масштабнее.
[01:14:07.000 --> 01:14:09.000]  Мы делаем Colossus.
[01:14:09.000 --> 01:14:10.000]  Такой же идеей.
[01:14:10.000 --> 01:14:11.000]  Что?
[01:14:11.000 --> 01:14:12.000]  Можем это?
[01:14:12.000 --> 01:14:13.000]  Ну сейчас.
[01:14:13.000 --> 01:14:15.000]  Ну я про это расскажу.
[01:14:17.000 --> 01:14:22.000]  У них нет статьи про Colossus, но у них есть маленькая презентация,
[01:14:22.000 --> 01:14:25.000]  слайды без записи даже.
[01:14:25.000 --> 01:14:28.000]  Но по ним, мне кажется, я понимаю, что происходит сейчас.
[01:14:34.000 --> 01:14:36.000]  Так вот, что делать-то?
[01:14:40.000 --> 01:14:43.000]  Мы переименовали, пока лучше не стало.
[01:14:43.000 --> 01:14:45.000]  Не всегда помогает.
[01:14:45.000 --> 01:14:50.000]  Вот мы сейчас решаем главную задачу нашей лекции.
[01:14:50.000 --> 01:14:52.000]  Нам нужно промасштабировать файловую систему
[01:14:52.000 --> 01:14:56.000]  через кейвельную хранилищику, которая требует зависимости файловую систему.
[01:14:58.000 --> 01:15:00.000]  Не понимаете, что нужно сделать?
[01:15:03.000 --> 01:15:06.000]  Нужно сделать взаимную рекурсию.
[01:15:06.000 --> 01:15:08.000]  Нужно здесь написать Colossus.
[01:15:11.000 --> 01:15:12.000]  Вот.
[01:15:13.000 --> 01:15:14.000]  Вот.
[01:15:14.000 --> 01:15:17.000]  И это более-менее, ну почти решение задачи.
[01:15:17.000 --> 01:15:18.000]  Что?
[01:15:18.000 --> 01:15:21.000]  Ну ты писал к ним взаимную рекурсию.
[01:15:21.000 --> 01:15:24.000]  Вот функция A вызывает функцию B, функция B вызывает функцию A.
[01:15:24.000 --> 01:15:26.000]  У нас система Colossus зависит от Bigtable,
[01:15:26.000 --> 01:15:28.000]  Bigtable зависит от Colossus.
[01:15:30.000 --> 01:15:33.000]  Ну понимаешь, не всякая рекурсия плохая.
[01:15:33.000 --> 01:15:36.000]  Рекурсия плохая, когда она не сходится никуда.
[01:15:36.000 --> 01:15:38.000]  Если сходится, то нормальная.
[01:15:38.000 --> 01:15:39.000]  Жить можно.
[01:15:39.000 --> 01:15:41.000]  Так вот, смотри.
[01:15:41.000 --> 01:15:42.000]  Что мы сделали?
[01:15:42.000 --> 01:15:45.000]  Мы вот перейдя от этого Colossus к этому Colossus,
[01:15:45.000 --> 01:15:46.000]  что сделали, по сути?
[01:15:46.000 --> 01:15:49.000]  Мы уменьшили объем данных в 10 тысяч раз.
[01:15:49.000 --> 01:15:50.000]  Вот.
[01:15:50.000 --> 01:15:52.000]  И если у нас было 150 байт данных,
[01:15:54.000 --> 01:15:57.000]  ну которые мы хотели хранить вот здесь, вот в этом Colossus,
[01:15:58.000 --> 01:16:02.000]  то в этом Colossus, который хранит метаданные этого Colossus через Bigtable,
[01:16:02.000 --> 01:16:04.000]  сколько данных уже будет?
[01:16:07.000 --> 01:16:08.000]  10 терабайт.
[01:16:09.000 --> 01:16:11.000]  Ну а что мы сделаем дальше?
[01:16:12.000 --> 01:16:16.000]  Ну как бы дальше у нас снова есть Metastore здесь
[01:16:17.000 --> 01:16:19.000]  и Chunkstore.
[01:16:21.000 --> 01:16:24.000]  Ну Colossus он как бы и тут, и тут,
[01:16:24.000 --> 01:16:26.000]  поэтому почему бы Chunkstore не сделать у них общий?
[01:16:30.000 --> 01:16:32.000]  Вот, а Metastore, ну у него уже свой.
[01:16:33.000 --> 01:16:35.000]  Это снова Bigtable.
[01:16:36.000 --> 01:16:39.000]  А Bigtable снова нужен Colossus для того, чтобы работать,
[01:16:39.000 --> 01:16:41.000]  чтобы данные хранить.
[01:16:43.000 --> 01:16:45.000]  А сколько в этом Colossus будет данных?
[01:16:50.000 --> 01:16:51.000]  Вот один гигабайт.
[01:16:52.000 --> 01:16:53.000]  Вот.
[01:16:53.000 --> 01:16:55.000]  Ну давайте еще один раз, еще один хоп сделаем.
[01:17:00.000 --> 01:17:03.000]  Сейчас, ну как бы дело тут не в том, что в оперативке отказу истойчиво.
[01:17:03.000 --> 01:17:05.000]  Мы получим 100, сколько?
[01:17:05.000 --> 01:17:08.000]  1 гигабайт в 10 тысяч.
[01:17:08.000 --> 01:17:10.000]  100 килобайт, да?
[01:17:11.000 --> 01:17:12.000]  Вот.
[01:17:12.000 --> 01:17:14.000]  Ну то есть пам-пам-пам.
[01:17:14.000 --> 01:17:17.000]  И когда у нас останется 100 килобайт данных, мы положим в чабе.
[01:17:24.000 --> 01:17:26.000]  Ну как-то так, да.
[01:17:26.000 --> 01:17:30.000]  Ну то есть в конце концов мы храним вот 100 килобайт этих метаданных,
[01:17:31.000 --> 01:17:33.000]  метаданных, метаданных, метаданных.
[01:17:33.000 --> 01:17:35.000]  Ну вот вы понимаете, что получается.
[01:17:38.000 --> 01:17:40.000]  Ну тут как бы матрешка из Colossus.
[01:17:42.000 --> 01:17:43.000]  Такой каскад.
[01:17:48.000 --> 01:17:50.000]  Ну так нет, базовый Colossus, ну то есть базовый рекурс – это чабе.
[01:17:50.000 --> 01:17:52.000]  Ну то есть нам нужно же от Colossus Frenary просто отказаться,
[01:17:52.000 --> 01:17:55.000]  потому что мы не можем вот таким вот уже кое-что делать.
[01:17:55.000 --> 01:17:56.000]  Ну не нужно просто.
[01:17:56.000 --> 01:17:57.000]  Вот.
[01:17:57.000 --> 01:17:59.000]  Ну а дальше мы вот как бы вытягиваем себя за волосы.
[01:17:59.000 --> 01:18:01.000]  Мы берем Colossus с помощью него, строим Colossus побольше,
[01:18:01.000 --> 01:18:03.000]  с помощью него строим Colossus еще побольше.
[01:18:03.000 --> 01:18:04.000]  Вот.
[01:18:04.000 --> 01:18:06.000]  Это же потрясающе красивые идеи,
[01:18:06.000 --> 01:18:09.000]  и она вот не сводится к тому, чтобы просто добавить много машин
[01:18:09.000 --> 01:18:11.000]  и поделиться все на равной части.
[01:18:11.000 --> 01:18:13.000]  Это вот такой странный каскад, такая рекурсия.
[01:18:13.000 --> 01:18:15.000]  Ну вот bootstrapping.
[01:18:15.000 --> 01:18:17.000]  И вот откуда придумать эту идею?
[01:18:17.000 --> 01:18:19.000]  Понимаете ли вы?
[01:18:21.000 --> 01:18:23.000]  Вот, да, отличное замечание.
[01:18:23.000 --> 01:18:26.000]  Вот представьте, что вы пишете новый язык программирования.
[01:18:26.000 --> 01:18:28.000]  Вам нужно писать комператор.
[01:18:28.000 --> 01:18:29.000]  В смысле не так.
[01:18:29.000 --> 01:18:31.000]  Вы пишете новый язык программирования,
[01:18:31.000 --> 01:18:33.000]  и что вам хочется сделать в первую очередь на этом языке?
[01:18:33.000 --> 01:18:35.000]  Ну, проверить, что он вообще разумен в этом языке,
[01:18:35.000 --> 01:18:37.000]  что на нем можно программу писать.
[01:18:37.000 --> 01:18:39.000]  Вы выбираете программу Compereator.
[01:18:39.000 --> 01:18:41.000]  Пишете на вашем любимом новом языке программирования
[01:18:41.000 --> 01:18:43.000]  комператор для этого языка.
[01:18:43.000 --> 01:18:46.000]  Это довольно сложно, потому что у вас еще нет комператора для этого языка.
[01:18:46.000 --> 01:18:49.000]  Так что вам нечем скомпедиировать ваш комператор.
[01:18:49.000 --> 01:18:51.000]  Поэтому вы что делаете?
[01:18:51.000 --> 01:18:53.000]  Берете C++ и пишете комператор на нем.
[01:18:55.000 --> 01:18:57.000]  Ну, как бы не для вашего языка,
[01:18:57.000 --> 01:18:59.000]  потому что он сложный какой-то, вы его даже не придумали.
[01:18:59.000 --> 01:19:01.000]  Вы пишете комператор на C++
[01:19:01.000 --> 01:19:03.000]  для некоторого маленького,
[01:19:03.000 --> 01:19:05.000]  полного подможества вашего нового языка.
[01:19:07.000 --> 01:19:09.000]  То есть теперь вы научились комперировать
[01:19:09.000 --> 01:19:13.000]  очень простые программы вашего нового языка.
[01:19:14.000 --> 01:19:16.000]  И вот с помощью этого комператора
[01:19:16.000 --> 01:19:18.000]  вы пишете на вашем уже новом языке
[01:19:18.000 --> 01:19:20.000]  с простыми конструкциями комператор
[01:19:20.000 --> 01:19:22.000]  для более сложной версии вашего языка.
[01:19:24.000 --> 01:19:26.000]  Ну и так можно продолжать дальше.
[01:19:26.000 --> 01:19:28.000]  То есть вы каждый следующий комператор пишете
[01:19:28.000 --> 01:19:30.000]  с помощью комператора предыдущей версии.
[01:19:31.000 --> 01:19:33.000]  Обычное дело, так вы постоянно делаете,
[01:19:33.000 --> 01:19:35.000]  если вы пишете комператор.
[01:19:35.000 --> 01:19:37.000]  Только так все и поступают.
[01:19:42.000 --> 01:19:43.000]  Нет, еще раз.
[01:19:43.000 --> 01:19:45.000]  Ты хочешь написать комператор языка Go
[01:19:45.000 --> 01:19:47.000]  на языке Go, а у тебя нет как бы...
[01:19:53.000 --> 01:19:55.000]  Ну, потому что это же все равно что признать поражение.
[01:19:55.000 --> 01:19:57.000]  Ты написал язык, на котором даже комператор
[01:19:57.000 --> 01:19:59.000]  для себя невозможно написать.
[01:19:59.000 --> 01:20:01.000]  Это же провал, это позор.
[01:20:03.000 --> 01:20:05.000]  Нет, это такая как бы проверка
[01:20:05.000 --> 01:20:07.000]  на вообще разумность твоего языка.
[01:20:07.000 --> 01:20:09.000]  Если на нем комператор пишется,
[01:20:09.000 --> 01:20:11.000]  то, наверное, все хорошо с ним.
[01:20:12.000 --> 01:20:14.000]  Ты хочешь сразу Bigtable писать на новом языке, да?
[01:20:15.000 --> 01:20:17.000]  Пишет комператор, и вот так же
[01:20:17.000 --> 01:20:19.000]  ты себя за волосы вытягиваешь.
[01:20:19.000 --> 01:20:21.000]  Вот здесь та же самая конструкция, такой же каскад.
[01:20:21.000 --> 01:20:23.000]  Правда, смотрите, задача еще не решена на самом деле.
[01:20:23.000 --> 01:20:25.000]  Вот представьте, что я
[01:20:25.000 --> 01:20:27.000]  пользователь, прихожу
[01:20:27.000 --> 01:20:29.000]  и говорю, хочу
[01:20:29.000 --> 01:20:31.000]  добавить chunk в колоссус.
[01:20:33.000 --> 01:20:35.000]  Что происходит?
[01:20:35.000 --> 01:20:37.000]  Что происходит,
[01:20:37.000 --> 01:20:39.000]  если мы делаем все неаккуратно?
[01:20:39.000 --> 01:20:41.000]  Помните, как мы делали chunkStore и metastore?
[01:20:41.000 --> 01:20:43.000]  Мы говорили, что там все данные
[01:20:43.000 --> 01:20:45.000]  мутабельные, поэтому нужно добавить новый chunk,
[01:20:45.000 --> 01:20:47.000]  а потом записать его в metastore.
[01:20:47.000 --> 01:20:49.000]  Вот мы создаем здесь новый chunk в chunkStore,
[01:20:49.000 --> 01:20:51.000]  и должны добавить
[01:20:51.000 --> 01:20:53.000]  этот chunk в metastore,
[01:20:53.000 --> 01:20:55.000]  в inode файла.
[01:20:55.000 --> 01:20:57.000]  Запись в metastore — это запись в Bigtable.
[01:20:57.000 --> 01:20:59.000]  Запись в Bigtable —
[01:20:59.000 --> 01:21:01.000]  это append в log
[01:21:01.000 --> 01:21:03.000]  lsm, который хранится
[01:21:03.000 --> 01:21:05.000]  в GIF в новом колоссусе.
[01:21:05.000 --> 01:21:07.000]  Append в конец
[01:21:07.000 --> 01:21:09.000]  этого лога — это запись
[01:21:09.000 --> 01:21:11.000]  еще одного chunk в этот колоссус.
[01:21:11.000 --> 01:21:13.000]  Ну и в итоге мы так начнем
[01:21:13.000 --> 01:21:15.000]  спускаться по этой
[01:21:15.000 --> 01:21:17.000]  рекурсии взаимной вниз,
[01:21:17.000 --> 01:21:19.000]  пока не запишем что-то в чабе.
[01:21:19.000 --> 01:21:21.000]  Кажется, что это не будет работать.
[01:21:25.000 --> 01:21:27.000]  Мы хотим, чтобы записи,
[01:21:27.000 --> 01:21:29.000]  аппенды, которые случаются много,
[01:21:29.000 --> 01:21:31.000]  не трогали вот эти колоссусы.
[01:21:31.000 --> 01:21:33.000]  Поэтому
[01:21:33.000 --> 01:21:35.000]  на самом деле chunkStore должен быть более
[01:21:35.000 --> 01:21:37.000]  сложным, и chunkStore должен
[01:21:37.000 --> 01:21:39.000]  уметь мутабельные чанки.
[01:21:39.000 --> 01:21:41.000]  То есть у нас есть chunk,
[01:21:41.000 --> 01:21:43.000]  и мы должны уметь
[01:21:43.000 --> 01:21:45.000]  в него добавлять
[01:21:45.000 --> 01:21:47.000]  новую порцию
[01:21:49.000 --> 01:21:51.000]  и не добавляем
[01:21:51.000 --> 01:21:53.000]  это данных,
[01:21:53.000 --> 01:21:55.000]  потому что иначе мы провалимся вниз
[01:21:55.000 --> 01:21:57.000]  до самого чаба.
[01:21:57.000 --> 01:21:59.000]  Вот GFS пробовал так делать.
[01:21:59.000 --> 01:22:01.000]  Там были перезаписи chunk, там были аппенды chunk
[01:22:01.000 --> 01:22:03.000]  и никаких хороших гарантий у них не было.
[01:22:03.000 --> 01:22:05.000]  Но с перезаписями chunk там была проблема,
[01:22:05.000 --> 01:22:07.000]  потому что ваша перезапись могла
[01:22:07.000 --> 01:22:09.000]  попасть на границу chunk, и в итоге
[01:22:09.000 --> 01:22:11.000]  два
[01:22:11.000 --> 01:22:13.000]  primary или два таблиты здесь
[01:22:13.000 --> 01:22:15.000]  будут упридачивать их произвольным образом.
[01:22:15.000 --> 01:22:17.000]  Вот эти две записи на границе.
[01:22:17.000 --> 01:22:19.000]  Colossus
[01:22:19.000 --> 01:22:21.000]  запрещает вам перезаписи.
[01:22:21.000 --> 01:22:23.000]  В нем нет опеды для перезаписей.
[01:22:23.000 --> 01:22:25.000]  В нем есть только аппенды.
[01:22:25.000 --> 01:22:27.000]  У GFS
[01:22:27.000 --> 01:22:29.000]  и с аппендами были проблемы.
[01:22:29.000 --> 01:22:31.000]  Потому что вы
[01:22:31.000 --> 01:22:33.000]  опендили, опендили, потом primary
[01:22:33.000 --> 01:22:35.000]  перевыбирался, который отвечал
[01:22:35.000 --> 01:22:37.000]  за упорядочивание аппендов,
[01:22:37.000 --> 01:22:39.000]  и в итоге новый primary не знал про старый, а какие-то
[01:22:39.000 --> 01:22:41.000]  другие клиенты приходили и делали повторно свои аппенды.
[01:22:41.000 --> 01:22:43.000]  Короче, ничего не понятно.
[01:22:43.000 --> 01:22:45.000]  Как делает GFS
[01:22:45.000 --> 01:22:47.000]  это делает любая разумная файловая система,
[01:22:47.000 --> 01:22:49.000]  но подозреваю, что и Colossus.
[01:22:49.000 --> 01:22:51.000]  Так делает Facebook уж точно.
[01:22:51.000 --> 01:22:53.000]  Они говорят, что у каждого chunk
[01:22:53.000 --> 01:22:55.000]  должен быть только один писатель.
[01:22:55.000 --> 01:22:57.000]  Но это разумно.
[01:22:57.000 --> 01:22:59.000]  То есть, если вы
[01:22:59.000 --> 01:23:01.000]  киевольное хранилище с LSM
[01:23:01.000 --> 01:23:03.000]  и вы пишете в этот LSM
[01:23:03.000 --> 01:23:05.000]  влог новой записи,
[01:23:05.000 --> 01:23:07.000]  то только вы
[01:23:07.000 --> 01:23:09.000]  один в этот лог и пишете, больше никто.
[01:23:09.000 --> 01:23:11.000]  А упорядочивать
[01:23:11.000 --> 01:23:13.000]  записи одного
[01:23:13.000 --> 01:23:15.000]  клиента очень легко.
[01:23:15.000 --> 01:23:17.000]  То есть, не нужно думать, что
[01:23:17.000 --> 01:23:19.000]  кто-то сломается. Просто chunk закрывается.
[01:23:19.000 --> 01:23:21.000]  То есть, если вдруг машина,
[01:23:21.000 --> 01:23:23.000]  которая обслуживала аппендов chunk,
[01:23:23.000 --> 01:23:25.000]  поломается, то chunk просто выявляется законченным.
[01:23:25.000 --> 01:23:27.000]  Если
[01:23:27.000 --> 01:23:29.000]  изменится писатель, то тоже chunk считается законченным.
[01:23:29.000 --> 01:23:31.000]  Поэтому очень легко
[01:23:31.000 --> 01:23:33.000]  делать аппендов chunk
[01:23:33.000 --> 01:23:35.000]  можно делать аппендов chunk
[01:23:35.000 --> 01:23:37.000]  без
[01:23:37.000 --> 01:23:39.000]  взаимодействия с метастором.
[01:23:39.000 --> 01:23:41.000]  И при этом
[01:23:41.000 --> 01:23:43.000]  сохранится голосованность.
[01:23:43.000 --> 01:23:45.000]  Потому что мы ограничили single writer.
[01:23:47.000 --> 01:23:49.000]  Черт возьми, я забыл кусок
[01:23:49.000 --> 01:23:51.000]  лекции рассказать.
[01:23:51.000 --> 01:23:53.000]  Почти все готово, думал. На самом деле,
[01:23:53.000 --> 01:23:55.000]  уже нет.
[01:23:55.000 --> 01:23:57.000]  Давайте я с этим закончу.
[01:23:57.000 --> 01:23:59.000]  Мы пропустили огромную у нас дыра
[01:23:59.000 --> 01:24:01.000]  в понимании.
[01:24:01.000 --> 01:24:03.000]  Но, во-первых, колоссус,
[01:24:03.000 --> 01:24:05.000]  это понятно, как работает.
[01:24:07.000 --> 01:24:09.000]  Про него статьи на самом деле нет.
[01:24:09.000 --> 01:24:11.000]  Но,
[01:24:11.000 --> 01:24:13.000]  смотрите, что есть.
[01:24:13.000 --> 01:24:15.000]  То есть,
[01:24:15.000 --> 01:24:17.000]  можно...
[01:24:27.000 --> 01:24:29.000]  Один раз Google
[01:24:29.000 --> 01:24:31.000]  в этой системе что-то рассказал.
[01:24:31.000 --> 01:24:33.000]  В виде таких скромных
[01:24:33.000 --> 01:24:35.000]  слайдов, и там
[01:24:35.000 --> 01:24:37.000]  информация очень компактная.
[01:24:37.000 --> 01:24:39.000]  Вот мы хотим
[01:24:39.000 --> 01:24:41.000]  масштабировать это данные.
[01:24:43.000 --> 01:24:45.000]  Но при этом для Bigtable нужна своя файловая система.
[01:24:45.000 --> 01:24:47.000]  Поэтому они говорят, ну, где же
[01:24:47.000 --> 01:24:49.000]  разместить эти данные?
[01:24:49.000 --> 01:24:51.000]  В GFS нельзя, потому что
[01:24:51.000 --> 01:24:53.000]  не подходят в модели данных.
[01:24:53.000 --> 01:24:55.000]  MySQL
[01:24:55.000 --> 01:24:57.000]  не вариант.
[01:24:57.000 --> 01:24:59.000]  Keywall storage,
[01:24:59.000 --> 01:25:01.000]  локальное хранилище не масштабируется.
[01:25:01.000 --> 01:25:03.000]  Вот есть Bigtable, давайте класть Bigtable.
[01:25:03.000 --> 01:25:05.000]  Ну и рисуем вот такую картинку.
[01:25:07.000 --> 01:25:09.000]  У них есть общий
[01:25:09.000 --> 01:25:11.000]  чанг-стор.
[01:25:11.000 --> 01:25:13.000]  На каждой машине кластера у них
[01:25:13.000 --> 01:25:15.000]  работает сервис, который называется D.
[01:25:15.000 --> 01:25:17.000]  Такой диск, потому что
[01:25:17.000 --> 01:25:19.000]  отвечает за хранение данных локально.
[01:25:19.000 --> 01:25:21.000]  И поверх этих дисков вы можете собирать
[01:25:21.000 --> 01:25:23.000]  вот этот чанг-стор.
[01:25:23.000 --> 01:25:25.000]  И вот вы можете взять этот...
[01:25:27.000 --> 01:25:29.000]  Немного странная
[01:25:29.000 --> 01:25:31.000]  картинка, но в итоге мы сводим все
[01:25:31.000 --> 01:25:33.000]  к GFS, а дальше GFS можно
[01:25:33.000 --> 01:25:35.000]  заменить на...
[01:25:35.000 --> 01:25:37.000]  Ну как бы сделать рекурсию теперь.
[01:25:37.000 --> 01:25:39.000]  Ну и вот
[01:25:39.000 --> 01:25:41.000]  промасштабировав
[01:25:41.000 --> 01:25:43.000]  несколько раз, мы
[01:25:43.000 --> 01:25:45.000]  уместим данные
[01:25:45.000 --> 01:25:47.000]  в чане.
[01:25:47.000 --> 01:25:49.000]  Ну то есть
[01:25:49.000 --> 01:25:51.000]  не то чтобы
[01:25:51.000 --> 01:25:53.000]  по этим слайдам дизайн легко читается,
[01:25:53.000 --> 01:25:55.000]  но если некоторое время подумать и
[01:25:55.000 --> 01:25:57.000]  почитать какие-то другие статьи, это становится понятно,
[01:25:57.000 --> 01:25:59.000]  что они имели в виду.
[01:25:59.000 --> 01:26:01.000]  Но, к сожалению, нигде они об этом хорошо
[01:26:01.000 --> 01:26:03.000]  не написали.
[01:26:05.000 --> 01:26:07.000]  Ну а теперь дыра в понимании.
[01:26:07.000 --> 01:26:09.000]  Смотрите, у нас есть Bigtable.
[01:26:09.000 --> 01:26:11.000]  И
[01:26:11.000 --> 01:26:13.000]  мы отделили хранение
[01:26:13.000 --> 01:26:15.000]  данных LSM.
[01:26:15.000 --> 01:26:17.000]  Данные LSM-а от точки
[01:26:17.000 --> 01:26:19.000]  обслуживания. Вот машина, которая получает
[01:26:19.000 --> 01:26:21.000]  команды и пишет в лог, хранит
[01:26:21.000 --> 01:26:23.000]  меня на table у себя в память.
[01:26:23.000 --> 01:26:25.000]  Ну это вот одна машина.
[01:26:25.000 --> 01:26:27.000]  Что если она умрет?
[01:26:29.000 --> 01:26:31.000]  Что?
[01:26:31.000 --> 01:26:33.000]  Нет, ну в смысле
[01:26:33.000 --> 01:26:35.000]  не то чтобы новая машина не нашлась,
[01:26:35.000 --> 01:26:37.000]  она найдется.
[01:26:37.000 --> 01:26:39.000]  Но у себя теперь, ну смотри,
[01:26:39.000 --> 01:26:41.000]  если эта машина умерла,
[01:26:41.000 --> 01:26:43.000]  то на смену ей пришла другая.
[01:26:43.000 --> 01:26:45.000]  Каким образом?
[01:26:49.000 --> 01:26:51.000]  Распределенные блокировки.
[01:26:51.000 --> 01:26:53.000]  Ну вот,
[01:26:53.000 --> 01:26:55.000]  для этого используется Chabi.
[01:26:55.000 --> 01:26:57.000]  В Chabi таблет
[01:26:57.000 --> 01:26:59.000]  Server берет распределенную блокировку.
[01:27:01.000 --> 01:27:03.000]  Но мы знаем, что распределенные блокировки
[01:27:03.000 --> 01:27:05.000]  это фундаментально сломанный механизм координации.
[01:27:05.000 --> 01:27:07.000]  И распределенные блокировки
[01:27:07.000 --> 01:27:09.000]  могут владеть 2 узла.
[01:27:09.000 --> 01:27:11.000]  Ну, в смысле, по мнению Chabi
[01:27:11.000 --> 01:27:13.000]  только один узел, но сами узлы могут думать иначе.
[01:27:13.000 --> 01:27:15.000]  То есть у вас уже отобрали
[01:27:15.000 --> 01:27:17.000]  блокировку, но вы все считаете, что она у вас есть.
[01:27:17.000 --> 01:27:19.000]  Вот.
[01:27:19.000 --> 01:27:21.000]  И это вот под задачу,
[01:27:21.000 --> 01:27:23.000]  которую нужно решать. В принципе,
[01:27:23.000 --> 01:27:25.000]  ее решал консенсус. Как избавиться от конкуренции
[01:27:25.000 --> 01:27:27.000]  лидеров.
[01:27:27.000 --> 01:27:29.000]  Ну вот, в таком
[01:27:29.000 --> 01:27:31.000]  бай-дизайне вам нужно решать ее на уровне выше.
[01:27:31.000 --> 01:27:33.000]  То есть у вас была машина,
[01:27:33.000 --> 01:27:35.000]  которая обслуживала данный таблет
[01:27:35.000 --> 01:27:37.000]  с данными в GFS.
[01:27:37.000 --> 01:27:39.000]  Она почему-то залипла.
[01:27:39.000 --> 01:27:41.000]  Ну, кстати, почему она залипла, легко объяснить.
[01:27:41.000 --> 01:27:43.000]  Потому что у вас еще не было
[01:27:43.000 --> 01:27:45.000]  HBase. HBase
[01:27:45.000 --> 01:27:47.000]  это open-source реализация Bigtable.
[01:27:47.000 --> 01:27:49.000]  Она написана на Chabi. В Chabi
[01:27:49.000 --> 01:27:51.000]  сборка мусора. Вот.
[01:27:51.000 --> 01:27:53.000]  Ну и сценарий залипания
[01:27:53.000 --> 01:27:55.000]  мастеров, таблет серверов,
[01:27:55.000 --> 01:27:57.000]  это просто бай-дизайн, то, что ожидается.
[01:27:57.000 --> 01:27:59.000]  Вот какой-то тикет
[01:27:59.000 --> 01:28:01.000]  из HBase.
[01:28:01.000 --> 01:28:03.000]  Ну вот они пишут, что
[01:28:03.000 --> 01:28:05.000]  нормально, тут у них
[01:28:05.000 --> 01:28:07.000]  RS это Region Server, то есть это машина,
[01:28:07.000 --> 01:28:09.000]  которая обслуживает регион таблицы,
[01:28:09.000 --> 01:28:11.000]  регион этот таблет. Вот. Ну и
[01:28:11.000 --> 01:28:13.000]  разработчики ожидают, что машина
[01:28:13.000 --> 01:28:15.000]  может уснуть на ГЦ, потом проснуться и
[01:28:15.000 --> 01:28:17.000]  все еще думать, что она владеет блокировкой.
[01:28:17.000 --> 01:28:19.000]  Ну вот,
[01:28:19.000 --> 01:28:21.000]  для этого
[01:28:21.000 --> 01:28:23.000]  для этого
[01:28:27.000 --> 01:28:29.000]  ну,
[01:28:35.000 --> 01:28:37.000]  это проект Apache, HBase.
[01:28:41.000 --> 01:28:43.000]  Так,
[01:28:43.000 --> 01:28:45.000]  большая картинка не стала, да?
[01:28:45.000 --> 01:28:47.000]  Ну вот, нужно делать
[01:28:47.000 --> 01:28:49.000]  fencing, то есть нужно,
[01:28:49.000 --> 01:28:51.000]  чтобы каждый новый
[01:28:51.000 --> 01:28:53.000]  таблет сервера, который обслуживает
[01:28:53.000 --> 01:28:55.000]  таблет, понимал, в какой он и пофиг
[01:28:55.000 --> 01:28:57.000]  находится относительно других.
[01:28:59.000 --> 01:29:01.000]  Вот. Ну и...
[01:29:01.000 --> 01:29:03.000]  Супер мелко, конечно, не видно ни черта.
[01:29:03.000 --> 01:29:05.000]  Но проблема решается
[01:29:05.000 --> 01:29:07.000]  на уровне файловой системы.
[01:29:07.000 --> 01:29:09.000]  То есть, смотрите,
[01:29:09.000 --> 01:29:11.000]  это важная деталь, мы ее
[01:29:11.000 --> 01:29:13.000]  упустили, к сожалению, в середине реакции, очень жаль.
[01:29:13.000 --> 01:29:15.000]  Вы берете блокировку
[01:29:15.000 --> 01:29:17.000]  в чабе.
[01:29:17.000 --> 01:29:19.000]  Вы таблет-сервер, который
[01:29:19.000 --> 01:29:21.000]  обслуживает таблет большой таблицы.
[01:29:21.000 --> 01:29:23.000]  Вот.
[01:29:23.000 --> 01:29:25.000]  А пишете данные файловую систему
[01:29:25.000 --> 01:29:27.000]  во внешнюю систему.
[01:29:27.000 --> 01:29:29.000]  Так что чабе
[01:29:29.000 --> 01:29:31.000]  может знать уже, что блокировку у вас забрал.
[01:29:31.000 --> 01:29:33.000]  Но
[01:29:33.000 --> 01:29:35.000]  пишете вы не в чабе после этого файловую систему.
[01:29:37.000 --> 01:29:39.000]  И вот уже на уровне файловой системы вам
[01:29:39.000 --> 01:29:41.000]  нужен какой-то механизм, который позволит
[01:29:41.000 --> 01:29:43.000]  файловой системе защититься от старого лидера.
[01:29:43.000 --> 01:29:45.000]  Ну вот, это
[01:29:45.000 --> 01:29:47.000]  механизм lease. То есть,
[01:29:47.000 --> 01:29:49.000]  если у вас был старый лидер, он открыл файл
[01:29:49.000 --> 01:29:51.000]  в HDFS.
[01:29:51.000 --> 01:29:53.000]  В HDFS есть только один писатель.
[01:29:53.000 --> 01:29:55.000]  Ну и сейчас я такую виртуальную схему
[01:29:55.000 --> 01:29:57.000]  рассказываю. Необязательно так сделано в HBase.
[01:29:57.000 --> 01:29:59.000]  В HBase сделано немного не так.
[01:29:59.000 --> 01:30:01.000]  Но можно себе представить такой механизм,
[01:30:01.000 --> 01:30:03.000]  что тот, кто владеет логом,
[01:30:03.000 --> 01:30:05.000]  владеет LSL.
[01:30:05.000 --> 01:30:07.000]  Вот вы открыли, вы получили,
[01:30:07.000 --> 01:30:09.000]  вы взяли блокировку в чабе,
[01:30:09.000 --> 01:30:11.000]  вы стали лидером таблета,
[01:30:11.000 --> 01:30:13.000]  открыли лог на запись, пишете в него.
[01:30:13.000 --> 01:30:15.000]  Если вдруг
[01:30:15.000 --> 01:30:17.000]  вы залипли
[01:30:17.000 --> 01:30:19.000]  и в системе выбился другой лидер,
[01:30:19.000 --> 01:30:21.000]  то он переоткрывает
[01:30:21.000 --> 01:30:23.000]  лог в DFS.
[01:30:23.000 --> 01:30:25.000]  И тем самым он
[01:30:25.000 --> 01:30:27.000]  инвалидирует дескриптер у
[01:30:27.000 --> 01:30:29.000]  первого лидера.
[01:30:29.000 --> 01:30:31.000]  И первый лидер все еще думает, что он
[01:30:31.000 --> 01:30:33.000]  лидер, пытается в лог
[01:30:33.000 --> 01:30:35.000]  что-то записать, какую-то новую мутацию.
[01:30:35.000 --> 01:30:37.000]  Но система HDFS
[01:30:37.000 --> 01:30:39.000]  говорит ему, что все, ты опоздал,
[01:30:39.000 --> 01:30:41.000]  у тебя уже отобрана риза.
[01:30:41.000 --> 01:30:43.000]  То есть, кто-то другой уже открыл файл
[01:30:43.000 --> 01:30:45.000]  после тебя.
[01:30:45.000 --> 01:30:47.000]  И у тебя право на запись отобрали.
[01:30:47.000 --> 01:30:49.000]  Вот такую задачу нужно решать, если вы
[01:30:49.000 --> 01:30:51.000]  используете такой дизайн в целом с PowerDFS.
[01:30:51.000 --> 01:30:53.000]  То есть, вам нужно на уровне чуть выше
[01:30:53.000 --> 01:30:55.000]  решать задачу
[01:30:55.000 --> 01:30:57.000]  защищаться от старого лидера,
[01:30:57.000 --> 01:30:59.000]  от их конкуренции, как-то его
[01:30:59.000 --> 01:31:01.000]  нейтрализовать.
[01:31:01.000 --> 01:31:03.000]  В HBase, кажется, это
[01:31:03.000 --> 01:31:05.000]  решается переименованием директории.
[01:31:07.000 --> 01:31:09.000]  Это важная деталь, потому что
[01:31:09.000 --> 01:31:11.000]  Facebook файловая система умеет
[01:31:11.000 --> 01:31:13.000]  отомарность на одной директории.
[01:31:13.000 --> 01:31:15.000]  Это может быть важно, если вы строите такие протоколы.
[01:31:17.000 --> 01:31:19.000]  Ну что ж,
[01:31:19.000 --> 01:31:21.000]  какие уроки мы извлекли
[01:31:21.000 --> 01:31:23.000]  из этой лекции?
[01:31:23.000 --> 01:31:25.000]  Уроки не
[01:31:25.000 --> 01:31:27.000]  тривиальные.
[01:31:27.000 --> 01:31:29.000]  Первый урок простой,
[01:31:29.000 --> 01:31:31.000]  что нужно всегда искать
[01:31:31.000 --> 01:31:33.000]  узкое место в системе.
[01:31:33.000 --> 01:31:35.000]  И узкое место в наших системах
[01:31:35.000 --> 01:31:37.000]  это всегда методанные.
[01:31:37.000 --> 01:31:39.000]  А как вы собираетесь
[01:31:39.000 --> 01:31:41.000]  сделать это?
[01:31:41.000 --> 01:31:43.000]  Ну что ж,
[01:31:43.000 --> 01:31:45.000]  уроки не тривиальные.
[01:31:45.000 --> 01:31:47.000]  А как вы собираетесь масштабировать
[01:31:47.000 --> 01:31:49.000]  это узкое место?
[01:31:49.000 --> 01:31:51.000]  Тут уже есть варианты.
[01:31:51.000 --> 01:31:53.000]  Вы можете в обоих случаях
[01:31:53.000 --> 01:31:55.000]  как-то наивно делить,
[01:31:55.000 --> 01:31:57.000]  шардировать это методосостояние, придумывать какие-то
[01:31:57.000 --> 01:31:59.000]  не то чтобы костыли, но что-то неудобное.
[01:31:59.000 --> 01:32:01.000]  А вы можете проявить некоторую
[01:32:01.000 --> 01:32:03.000]  изобретательность вместо этого.
[01:32:03.000 --> 01:32:05.000]  И положить данные Bigtable и Bigtable, и придумать идею,
[01:32:05.000 --> 01:32:07.000]  которая похожа на
[01:32:07.000 --> 01:32:09.000]  виртуальную память в операционных системах.
[01:32:09.000 --> 01:32:11.000]  Или придумать вот такой вот bootstrapping,
[01:32:11.000 --> 01:32:13.000]  как в компеляторах.
[01:32:13.000 --> 01:32:15.000]  Не первый и не второй, кажется, тривиальный не является.
[01:32:15.000 --> 01:32:17.000]  И не сводится к тому,
[01:32:17.000 --> 01:32:19.000]  что мы просто добавляем машину для масштабируемости.
[01:32:19.000 --> 01:32:21.000]  Нет, мы делаем что-то иногда очень хитрое.
[01:32:21.000 --> 01:32:23.000]  Ну и
[01:32:23.000 --> 01:32:25.000]  такая сложная мораль,
[01:32:25.000 --> 01:32:27.000]  что
[01:32:27.000 --> 01:32:29.000]  трудно
[01:32:31.000 --> 01:32:33.000]  смотреть
[01:32:33.000 --> 01:32:35.000]  на историю последних 20 лет
[01:32:35.000 --> 01:32:37.000]  и на Google,
[01:32:37.000 --> 01:32:39.000]  очень сложно представить, что вы в конце концов придумаете такое.
[01:32:39.000 --> 01:32:41.000]  То есть как бы тут одна ошибка,
[01:32:41.000 --> 01:32:43.000]  одно неправильное движение,
[01:32:43.000 --> 01:32:45.000]  и вы построите совсем другие системы,
[01:32:45.000 --> 01:32:47.000]  и вот такая конструкция уже не сработает.
[01:32:47.000 --> 01:32:49.000]  Как это сделать, не совсем понятно.
[01:32:49.000 --> 01:32:51.000]  Это сложно.
[01:32:51.000 --> 01:32:53.000]  То есть как выстроить
[01:32:53.000 --> 01:32:55.000]  такую глобальную декомпозицию
[01:32:55.000 --> 01:32:57.000]  на сервис-координации,
[01:32:57.000 --> 01:32:59.000]  на кивалию хранилища поверх файловой системы,
[01:32:59.000 --> 01:33:01.000]  на файловую систему через кивалию
[01:33:01.000 --> 01:33:03.000]  хранилища, через файловую систему.
[01:33:03.000 --> 01:33:05.000]  Вот как бы вся эта конструкция,
[01:33:05.000 --> 01:33:07.000]  одна большая целая, она сама по себе
[01:33:07.000 --> 01:33:09.000]  очень нетривиальна.
[01:33:09.000 --> 01:33:11.000]  А люди придумали ее впервые,
[01:33:11.000 --> 01:33:13.000]  только они ее сделали, по сути.
[01:33:13.000 --> 01:33:15.000]  Повторить ее сложно, но и кажется,
[01:33:15.000 --> 01:33:17.000]  что на самом деле неразумно, потому что даже
[01:33:17.000 --> 01:33:19.000]  вот Facebook не пытается.
[01:33:19.000 --> 01:33:21.000]  Проще делать систему, которая не имеет внешних зависимости.
[01:33:21.000 --> 01:33:23.000]  А тут мы взяли
[01:33:23.000 --> 01:33:25.000]  две очень большие сложные системы и научились
[01:33:25.000 --> 01:33:27.000]  делать их друг через друга.
[01:33:27.000 --> 01:33:29.000]  Наверное, потому что они уже были, но все же,
[01:33:29.000 --> 01:33:31.000]  то, что это получилось так сделать,
[01:33:31.000 --> 01:33:33.000]  это, по-моему, поразительно.
[01:33:33.000 --> 01:33:35.000]  Ну и это такой совершенно
[01:33:35.000 --> 01:33:37.000]  отдельный навык, в смысле
[01:33:37.000 --> 01:33:39.000]  масштабирование, проектирование.
[01:33:39.000 --> 01:33:41.000]  То есть он не сводится к тому, что мы там
[01:33:41.000 --> 01:33:43.000]  оптимизируем какие-то сложности, какие-то
[01:33:43.000 --> 01:33:45.000]  маленькие задачки решаем.
[01:33:45.000 --> 01:33:47.000]  Вот тут какой-то совершенно
[01:33:47.000 --> 01:33:49.000]  другой способ мышления
[01:33:49.000 --> 01:33:51.000]  подходит к задаче.
[01:33:53.000 --> 01:33:55.000]  Ну что, мы решили
[01:33:55.000 --> 01:33:57.000]  теперь, мы умеем делать
[01:33:57.000 --> 01:33:59.000]  локальные хранилища с вами.
[01:33:59.000 --> 01:34:01.000]  Мы умеем реплицировать, ну почти умеем
[01:34:01.000 --> 01:34:03.000]  остаться написать.
[01:34:03.000 --> 01:34:05.000]  И мы умеем
[01:34:05.000 --> 01:34:07.000]  строить распределенные системы
[01:34:07.000 --> 01:34:09.000]  из этих вот отдельных
[01:34:09.000 --> 01:34:11.000]  отказоустойчивых единиц
[01:34:11.000 --> 01:34:13.000]  RSM-ов.
[01:34:13.000 --> 01:34:15.000]  Вот таким способом, вот таким способом.
[01:34:15.000 --> 01:34:17.000]  Что нам осталось? Нам осталось
[01:34:17.000 --> 01:34:19.000]  делать транзакции и получить Google Spanner.
[01:34:19.000 --> 01:34:21.000]  Вот этим мы займемся
[01:34:21.000 --> 01:34:23.000]  в ближайшее.
[01:34:23.000 --> 01:34:25.000]  Ну, через неделю
[01:34:25.000 --> 01:34:27.000]  мы начнем этим заниматься, через две недели, вернее.
[01:34:31.000 --> 01:34:33.000]  Что?
[01:34:37.000 --> 01:34:39.000]  Я думаю, что...
[01:34:39.000 --> 01:34:41.000]  Ну,
[01:34:41.000 --> 01:34:43.000]  не то чтобы каждый байт,
[01:34:43.000 --> 01:34:45.000]  который хранится, хранится в колоссусе.
[01:34:45.000 --> 01:34:47.000]  Я думаю, что это, ну может быть и не так, конечно,
[01:34:47.000 --> 01:34:49.000]  но это почти так.
[01:34:51.000 --> 01:34:53.000]  Ну, это разумно. Ты как бы избавился
[01:34:53.000 --> 01:34:55.000]  от... Ты не думаешь нигде
[01:34:55.000 --> 01:34:57.000]  про сбойные диски.
[01:34:57.000 --> 01:34:59.000]  Ты везде работаешь с отказоустойчивой файловой системой.
[01:34:59.000 --> 01:35:01.000]  В чем мотивация
[01:35:01.000 --> 01:35:03.000]  не использовать ее?
[01:35:05.000 --> 01:35:07.000]  Она позволяет себе отказоустойчивой
[01:35:07.000 --> 01:35:09.000]  и очень экономно хранить данные.
[01:35:09.000 --> 01:35:11.000]  Но она, конечно, оптимизирована
[01:35:11.000 --> 01:35:13.000]  для того, чтобы все быстро работало.
[01:35:13.000 --> 01:35:15.000]  Она может работать даже
[01:35:15.000 --> 01:35:17.000]  быстрее, чем твой жесткий диск, который может затупить.
[01:35:17.000 --> 01:35:19.000]  Ну, потому что там
[01:35:19.000 --> 01:35:21.000]  один жесткий диск может затупить, а вот там,
[01:35:21.000 --> 01:35:23.000]  не знаю, много жестких дисков разом,
[01:35:23.000 --> 01:35:25.000]  им затупить сложнее.
[01:35:25.000 --> 01:35:27.000]  Ну да, тут еще нужно учесть
[01:35:27.000 --> 01:35:29.000]  такой нюанс инженерный, что
[01:35:29.000 --> 01:35:31.000]  с одной стороны,
[01:35:31.000 --> 01:35:33.000]  мы писали локально раньше в свой диск,
[01:35:33.000 --> 01:35:35.000]  а теперь мы ходим по сети, пишем в какой-то другой диск,
[01:35:35.000 --> 01:35:37.000]  в какой-то другой системе.
[01:35:37.000 --> 01:35:39.000]  Но на самом деле у тебя система
[01:35:39.000 --> 01:35:41.000]  Bigtable и система Colossus,
[01:35:41.000 --> 01:35:43.000]  как бы их узлы могут располагаться
[01:35:43.000 --> 01:35:45.000]  на одной и той же машине физически.
[01:35:45.000 --> 01:35:47.000]  То есть ты можешь буквально писать
[01:35:47.000 --> 01:35:49.000]  на свой диск просто через такую цепочку абстракций.
[01:35:49.000 --> 01:35:51.000]  Если система это учитывает,
[01:35:51.000 --> 01:35:53.000]  если там менеджер, который
[01:35:53.000 --> 01:35:55.000]  оставляет эти сервисы по машинам,
[01:35:55.000 --> 01:35:57.000]  это учитывает, а он это учитывает,
[01:35:57.000 --> 01:35:59.000]  то ты можешь оверхедить большого и не
[01:35:59.000 --> 01:36:01.000]  получить.
[01:36:01.000 --> 01:36:03.000]  Вот так что разумно все, что
[01:36:03.000 --> 01:36:05.000]  ты хранишь, хранить в такой вот файловой системе.
[01:36:07.000 --> 01:36:09.000]  Ну и кажется, что вот любая,
[01:36:09.000 --> 01:36:11.000]  если ты не используешь Colossus прямо, то ты используешь
[01:36:11.000 --> 01:36:13.000]  Colossus косвенно, потому что вот Spanner
[01:36:13.000 --> 01:36:15.000]  база данных,
[01:36:15.000 --> 01:36:17.000]  ты хочешь использовать ее, потому что тебе
[01:36:17.000 --> 01:36:19.000]  удобно с таблицами работать,
[01:36:19.000 --> 01:36:21.000]  но ты кладешь данные в Spanner, Spanner
[01:36:21.000 --> 01:36:23.000]  ты кладешь данные в Colossus все равно,
[01:36:23.000 --> 01:36:25.000]  ты не можешь его избежать.
[01:36:27.000 --> 01:36:29.000]  У Black Colossus он
[01:36:29.000 --> 01:36:31.000]  еще не один, да?
[01:36:31.000 --> 01:36:33.000]  Он свой на каждый датацентр.
[01:36:33.000 --> 01:36:35.000]  То есть Spanner это обычный RSM,
[01:36:35.000 --> 01:36:37.000]  то есть каждый кусочек
[01:36:37.000 --> 01:36:39.000]  этого Spanner, каждый кусочек твоей таблицы
[01:36:39.000 --> 01:36:41.000]  это три реплики, они находятся в разных дец,
[01:36:41.000 --> 01:36:43.000]  но при этом каждая реплика работает не со своим диском,
[01:36:43.000 --> 01:36:45.000]  а хранить данные в своем Colossus.
[01:36:47.000 --> 01:36:49.000]  То есть у тебя коэффициент репликации не три, а три умноженные
[01:36:49.000 --> 01:36:51.000]  на коэффициент репликации внутри Colossus,
[01:36:51.000 --> 01:36:53.000]  а он может быть там не знаю, полтора.
[01:37:07.000 --> 01:37:09.000]  Когда мы читаем откуда?
[01:37:15.000 --> 01:37:17.000]  Ну нет, конечно.
[01:37:17.000 --> 01:37:19.000]  Вот смотри, мы спускаемся в Colossus ниже,
[01:37:19.000 --> 01:37:21.000]  раз там, грубо говоря, в 10 тысяч операций.
[01:37:27.000 --> 01:37:29.000]  Сейчас, ну подожди, если у тебя данные имутабельные,
[01:37:33.000 --> 01:37:35.000]  подожди,
[01:37:37.000 --> 01:37:39.000]  давай в порядку, если мы говорим про key value,
[01:37:39.000 --> 01:37:41.000]  то в кэше хранились
[01:37:41.000 --> 01:37:43.000]  адреса
[01:37:43.000 --> 01:37:45.000]  узлов, которые обслуживают
[01:37:45.000 --> 01:37:47.000]  таблет служебные,
[01:37:47.000 --> 01:37:49.000]  но и таблеты твои.
[01:37:49.000 --> 01:37:51.000]  Это просто адреса узлов, которые обслуживают твои запросы.
[01:37:51.000 --> 01:37:53.000]  Если они вдруг перестали
[01:37:53.000 --> 01:37:55.000]  обслуживать твои запросы, они об этом скажут тебе,
[01:37:55.000 --> 01:37:57.000]  и ты попробуешь заново, пройдешь через чай,
[01:37:57.000 --> 01:37:59.000]  через всю таблицу страниц заново.
[01:37:59.000 --> 01:38:01.000]  То есть тут ничего, никакой проблемы нет.
[01:38:01.000 --> 01:38:03.000]  Если у тебя данные вообще имутабельные,
[01:38:03.000 --> 01:38:05.000]  то тоже проблем нет, если у тебя данные там немного
[01:38:05.000 --> 01:38:07.000]  имутабельные, то
[01:38:07.000 --> 01:38:09.000]  проблема тоже решается.
[01:38:09.000 --> 01:38:11.000]  Ну кэше используется, конечно,
[01:38:11.000 --> 01:38:13.000]  без кэше это все
[01:38:13.000 --> 01:38:25.000]  не работает. Ты к тому, что кашировать нужно аккуратно? Да, нужно аккуратно кашировать.
[01:38:25.000 --> 01:38:29.000]  Ну, смотри, вот от того, что у тебя здесь получился такой странный каскад сложный,
[01:38:29.000 --> 01:38:33.000]  это же не означает, что ты вглубь его ходишь каждый раз.
[01:38:33.000 --> 01:38:38.000]  Ты его чаще всего не покидаешь в первый уровень, то есть ты работаешь как с обычной файловой системой.
[01:38:38.000 --> 01:38:42.000]  Просто иногда ты спускаешься на уровень Рижа. В Чабе ты спускаешься супер редко.
[01:38:42.000 --> 01:38:47.000]  Да, забыл сказать, что в принципе система может лишиться Google-Чабе,
[01:38:47.000 --> 01:38:51.000]  он отказаустойчивый, но можно потом, не знаю, 5 репель взорвутся.
[01:38:51.000 --> 01:38:54.000]  Так вот, это не остановит конструкцию все.
[01:38:54.000 --> 01:38:58.000]  Потому что, в принципе, пока ты пишешь в Чанке и не создаешь новые,
[01:38:58.000 --> 01:39:01.000]  тебе даже не обязательно с Меда-Стором разговаривать.
[01:39:01.000 --> 01:39:04.000]  Так что система может жить без Чаби вот на этом уровне очень долго,
[01:39:04.000 --> 01:39:07.000]  пока кто-нибудь заметит и починит.
[01:39:07.000 --> 01:39:10.000]  То есть спускаться в нее часто не нужно,
[01:39:10.000 --> 01:39:13.000]  потому что она не сможет выдержать такую нагрузку,
[01:39:13.000 --> 01:39:16.000]  как выдержит колоссус все равно.
[01:39:20.000 --> 01:39:22.000]  Чего?
[01:39:22.000 --> 01:39:25.000]  Нет, подожди, Чаби он в основании лежит, он про все, что выше, ничего не знает.
[01:39:25.000 --> 01:39:27.000]  Чаби это RSM.
[01:39:32.000 --> 01:39:35.000]  Данные, которые лежат в Чабе, они, слава богу, хранятся на его дисках.
[01:39:35.000 --> 01:39:38.000]  Нужно остановиться когда-то.
[01:39:38.000 --> 01:39:41.000]  Это просто обычный RSM, ты берешь дерево с локами,
[01:39:41.000 --> 01:39:45.000]  с какими-то маленькими записями размером 100 килобайт-мегабайт.
[01:39:45.000 --> 01:39:51.000]  И ты реприцируешь его на семи машинах, в семи регионах.
[01:39:51.000 --> 01:39:54.000]  Ты надеешься, что они разом не откажут.
[01:39:54.000 --> 01:39:57.000]  Точнее, большинство будет доступно.
[01:39:57.000 --> 01:40:07.000]  В чабе есть кэши, я предлагал тебе статью почитать для этого.
[01:40:07.000 --> 01:40:11.000]  Там кэши есть, чтобы к чабе часто не ходить.
[01:40:11.000 --> 01:40:14.000]  Они там сделаны определенным образом, довольно странно.
[01:40:14.000 --> 01:40:19.000]  Там некий протокол когеренности, если ты помнишь, что это.
[01:40:19.000 --> 01:40:23.000]  Если ты идешь в чабе, что ты пишешь,
[01:40:23.000 --> 01:40:30.000]  то чабе берет и инвалидирует кэши у всех клиентов.
[01:40:30.000 --> 01:40:38.000]  Довольно странный дизайн.
[01:40:38.000 --> 01:40:40.000]  Ну что, тогда перерыв.
[01:40:40.000 --> 01:40:42.000]  Кажется, вопросы кончились.

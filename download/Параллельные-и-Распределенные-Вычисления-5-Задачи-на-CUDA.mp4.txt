[00:00.000 --> 00:11.640]  Во-первых, мы должны рассмотреть блок, связанный с особенностями синхронизации, и понять вообще,
[00:11.640 --> 00:17.720]  какие примитивы синхронизации существуют в куде, а после этого разобрать задачу подсчета
[00:17.720 --> 00:24.720]  суммы чисел массива и задачу нахождения суммы на префиксе. Это очень интересная задача,
[00:24.720 --> 00:33.000]  поэтому, я бы так сказал, эта история даже пережила некоторую эпопею с тем,
[00:33.000 --> 00:38.080]  как ее решали еще, не знаю, 10 лет назад, и по сравнению с тем, как решают ее сейчас.
[00:38.080 --> 00:43.320]  Вот, и это мы с вами все действительно рассмотрим. Значит, по идее, у вас был
[00:43.320 --> 00:50.320]  некоторый семинар, не знаю, был у вас он или нет, у моей группы, по крайней мере, был,
[00:50.320 --> 00:54.280]  в котором мы говорили следующее, что у нас существует разделяемая память,
[00:54.280 --> 01:00.360]  и память, которая выделяется в разделяемой памяти, у нас выделяется именно на один блок,
[01:00.360 --> 01:07.640]  и она располагается у нас на чипе, и имеет скорость доступа, сравнимую со скоростью доступа в L1 cache.
[01:07.640 --> 01:16.040]  Я не знаю, говорили вам это или нет, но я о своей группе говорил. Правда, на прошлый семинаре тоже
[01:16.240 --> 01:24.960]  людей было. Видимо, все сдавали или отходили от задания по MPI, либо наступил еще один какой-то дедлайн.
[01:24.960 --> 01:33.160]  Вот. Либо уже март наступил. Ну, кто его знает. В общем, в чем состоят? Сейв в том, что у нас
[01:33.160 --> 01:42.440]  некоторые потоки, которые у нас могут работать внутри одного блока, они у нас синхронно работают
[01:42.440 --> 01:49.600]  только в пределах одного варпа. То есть мы в пределах одного варпа действительно гарантируем то,
[01:49.600 --> 01:58.640]  что у нас все инструкции выполняются атомарно. Но важен один момент. Это работало до архитектуры
[01:58.640 --> 02:06.680]  вольта. То есть если у вас видеокарта V100, то в некоторый момент времени это может быть нарушено.
[02:07.400 --> 02:12.680]  При этом те потоки, которые работают не в пределах одного варпа, а в разных варпах, но при этом они
[02:12.680 --> 02:28.360]  находятся внутри одного блока, выполняют свои операции неодновременно. То есть у нас один ворп
[02:28.360 --> 02:36.360]  может начать работу раньше, чем другой ворп. А если у нас возникает какая-то операция синхронизации,
[02:36.840 --> 02:45.480]  вот здесь нам нужно взять значение этого элемента и сложить с значением этого элемента, то вернут
[02:45.480 --> 02:52.480]  результат в этот поток. Тогда что у нас может быть? У нас может быть следующая ситуация. Представь
[02:52.480 --> 02:59.280]  себе, что в момент времени ноль мы записали значение. Тогда у нас что может произойти? У
[02:59.280 --> 03:03.320]  нас может произойти следующее, что в первый момент времени у нас отработал этот ворп,
[03:03.320 --> 03:20.320]  потом в следующий такт времени отработал этот ворп. А дальше происходит следующее. Поскольку этот
[03:20.320 --> 03:26.640]  ворп уже дошел до этой операции, то он может во второй такт времени начать записывать значение,
[03:26.640 --> 03:33.000]  свою ячейку. А вот эта сумма, которая здесь у нас находится, она приходит в третий момент времени.
[03:33.000 --> 03:42.120]  И если у нас было значение здесь х, здесь у нас было значение у, здесь было значение у',
[03:42.120 --> 03:49.400]  то во второй момент времени какой результат у нас будет? Если мы складываем элементы во
[03:49.400 --> 04:06.000]  второй момент времени, вот здесь. Ну тут вариантов немного. Либо х плюс у, либо х плюс у'. Кто
[04:06.000 --> 04:18.120]  голосует за х плюс у'? Можно вопрос мне задать. Смотрите, еще раз. Значит, у нас есть временная
[04:18.120 --> 04:24.880]  шкала. Вот у нас есть в момент времени ноль. У нас в ячейках записаны какие-то наборы значений.
[04:24.880 --> 04:34.680]  Значит, у нас здесь изначально было записано значение х. Давайте здесь х0. А здесь у нас было
[04:34.680 --> 04:45.840]  записано значение пусть у. Что дальше происходит? Пусть будет у0. Что происходит в дальнейшем? После
[04:45.880 --> 04:54.320]  первой операции здесь у нас появляется значение х1. После записи значения в третьем момент времени
[04:54.320 --> 05:06.140]  здесь появляется значение у3. А дальше мы в момент времени складываем значения, которые были в этой
[05:06.140 --> 05:12.880]  ячейке и в этой ячейке. И мы получаем с вами некоторое значение х2. Так вот, вопрос. Чему равняется
[05:12.880 --> 05:28.040]  х2? Есть здесь два варианта в ответа. Первый это х1 плюс у0 или х1 плюс у3? Кто за какой голосует?
[05:28.040 --> 05:47.800]  К регистрам они атомарно обращаются? Нет, несколько варпов.
[05:47.800 --> 06:05.880]  Она в принципе параллельно может работать с несколькими варпами. Просто мы не знаем,
[06:05.880 --> 06:17.680]  с какими варпами она работает. Ну может. Так вот, вопрос. Какое значение у нас может получиться?
[06:17.680 --> 06:26.600]  Ну мусорно вряд ли. Все-таки мы читаем значение вот здесь, из регистра, которое находится в этой
[06:26.600 --> 06:36.120]  ячейке. Смотрите, если у нас порядок тот, который я задал временной, то есть сначала у нас идет
[06:36.120 --> 06:43.560]  запись в эту ячейку, потом сумму сюда, а после этого у нас приходит значение сюда, то у нас
[06:43.560 --> 06:51.400]  получается значение х1 плюс у0. А если у нас с вами порядок идет такой, что сначала мы делаем
[06:51.400 --> 06:59.120]  запись в варпе, после этого мы делаем сумму, то тогда у нас значение будет х1 плюс у3. Какой
[06:59.120 --> 07:09.280]  результат мы ожидаем с вами увидеть логически в порядке исполнения? Мы хотим допустим здесь
[07:09.280 --> 07:18.160]  выполнить операцию, здесь выполнить операцию, после этого сложить элементы. Ожидаемый для нас х1 плюс у3.
[07:18.160 --> 07:28.600]  Но такое быть не всегда возможно. Поэтому, чтобы выполнить вот эту вот операцию, нам нужна операция
[07:28.600 --> 07:45.040]  sync threads. Что делает операция sync threads? Она берет и делает глобальную блокировку на блок. Мы рассматриваем все
[07:45.040 --> 07:50.320]  внутри одного блока и гарантируем, что все операции записи внутри этого блока у нас будут отработаны.
[07:50.320 --> 08:05.360]  Так, это понятно? То есть всегда, как только вы хотите прочитать петличку, вот так ее сделаем,
[08:05.360 --> 08:13.600]  вот теперь у него катится. Теперь смотрите, что получается. Прежде чем мы хотим прочитать
[08:13.600 --> 08:21.160]  значение из соседнего элемента разделяемой памяти, лучше делать sync threads. Причем он
[08:21.160 --> 08:27.040]  должен вызываться всеми блоками, и всеми потоками в одном блоке, потому что это барьер. Если какой-то
[08:27.040 --> 08:33.520]  из потоков у нас с вами не дойдет до sync threads, мы получим с вами deadlock. То есть аккуратнее с ним
[08:33.520 --> 08:45.040]  внутри EFA. Но не поверите, это еще не все. Давайте мы с вами предъявим некоторые пример кода
[08:45.040 --> 08:57.560]  на видеокарте. Я специально его подготовил в примерах. Вот такой код у нас. Представим себе следующую
[08:57.560 --> 09:04.800]  ситуацию. Что у нас поток номер один выполняет функцию writexy, а второй поток выполняет функцию
[09:04.800 --> 09:17.320]  readxy. Смотрите внимательно на код. Здесь указано следующее, что первый поток выполняет функцию
[09:17.320 --> 09:23.240]  writexy, то есть у нас первый поток восстановляет значение х равно 10, у равно 20. А второй поток
[09:23.760 --> 09:32.920]  указывает значение b равно y и a равно x. Вопрос, какие варианты результатов a и b у нас могут быть?
[09:32.920 --> 09:50.800]  Все. Вы не поверите, варианты могут быть все. У нас x это 10, x это у нас один или сколько?
[09:50.800 --> 10:13.480]  10, у это 2, 20. Получается, а это у нас x. 1, 10, 2, 2, 20, 20. Хорошо. Давайте подумаем в
[10:13.480 --> 10:17.120]  концепциях классического параллельного программирования, какой вариант был бы невозможен.
[10:17.120 --> 10:27.440]  Утверждается, что в концепции классического параллельного программирования какой-то из
[10:27.440 --> 10:48.920]  вариантов был бы невозможен. Да, 21. Не был бы возможен вариант. Вот этот вариант невозможен на
[10:48.920 --> 11:04.280]  цепу. Почему? Потому что когда мы ставим переменную a равно x, то у нас получается следующее, что перед этим
[11:04.280 --> 11:12.320]  выполняется уже код b равно y. Значит, если у нас выполняется код b равно y, это означает, что в данном
[11:12.320 --> 11:24.920]  случае b равняется 20, а a равно x, получается у нас в этот момент времени x уже равен 10. То есть,
[11:24.920 --> 11:32.480]  по идее у нас если y равно 20, то a равно 10. На видеокарте это неправда. Точнее, далеко не всегда
[11:32.480 --> 11:40.840]  гарантируется. Почему? Потому что здесь указано следующее. Здесь способ вычисления, который
[11:40.840 --> 11:52.160]  используется в модель памяти. Это weekly ordered memory model. То есть, пока вы явно не запрашиваете
[11:52.160 --> 11:57.160]  результат операции из какого-то потока, вы явно не получите результат. То есть, в принципе,
[11:57.160 --> 12:03.920]  видеокарта сама вольна себе переставлять значение элементов и порядок операции,
[12:03.920 --> 12:12.400]  который она исполняет. То есть, в принципе, она может переставить опции b равно y и a равно x.
[12:12.400 --> 12:19.920]  И каким образом упорядочить как раз эту модель? Для этого как раз возникает такое понятие как
[12:19.920 --> 12:29.240]  threadfans. Собственно, они здесь описаны в документации достаточно хорошо. И здесь есть следующие
[12:29.240 --> 12:35.840]  гарантии. То есть, у нас есть threadfans-блок, у нас есть просто threadfans и есть threadfans-систем. То есть,
[12:35.840 --> 12:40.880]  что гарантирует вызов функции threadfans-блок перед следующей операцией? Она гарантирует,
[12:40.880 --> 12:49.400]  что все записи в память, которые были сделаны в thread, перед вызовом threadfans-блок, они будут
[12:49.400 --> 12:56.680]  обнаружены всеми потоками в блоке. То есть, это означает, что если мы с вами вот в том коде,
[12:56.840 --> 13:02.840]  который у нас был, поставим threadfans-блок, мы по факту зададим порядок, в котором эта операция
[13:02.840 --> 13:14.360]  происходит. То есть, по факту, вот если мы сюда вот после этой штуки поставим threadfans-блок,
[13:14.360 --> 13:25.080]  то у нас становится невозможным вариант. Какой? Вот этот вариант невозможен. Потому что мы
[13:25.080 --> 13:37.560]  явно зададим порядок. Это кажется логичным. Вот просто нужно уточнять, что видеокарта,
[13:37.560 --> 13:45.600]  она вот такая хитрая вещь. И есть вот такая глобальная блокировка. При желании ее можно
[13:45.600 --> 13:50.480]  использовать threadfans-систем. То есть, вы по факту все операции записи сихронизируете,
[13:50.480 --> 13:54.120]  ставите в правильном порядке. Но при этом, если вы будете использовать threadfans-систем,
[13:54.120 --> 13:58.120]  это приблизительно похоже на глобальную блокировку. То есть, вы всем потоком,
[13:58.120 --> 14:04.520]  который у вас есть в гряде, устанавливаете запись определенного значения. Поэтому лучше
[14:04.520 --> 14:10.000]  его не использовать, но в целом вы можете прямо явно прописывать порядок операций. Вот. И
[14:10.000 --> 14:16.040]  благодаря этому можно как раз реализовать глобальную блокировку. Значит, для этого нам
[14:16.040 --> 14:21.520]  нужны атомарные операции. То есть, вообще, если вам нужно посчитать количество элементов,
[14:21.520 --> 14:31.360]  то просто взять счетчик внутри блока нельзя. Вот. Потому что мы будем прислать неатомарную
[14:31.360 --> 14:36.240]  операцию. Поэтому нам нужна атомарная операция. Значит, главная особенность еще атомарных
[14:36.240 --> 14:41.240]  операций на видеокарте заключается в том, что они работают на уровне ледвакоша. То есть,
[14:41.240 --> 14:46.840]  они работают достаточно быстро. И это даже можно будет увидеть, если вы попытаетесь решить задачу
[14:46.840 --> 14:52.600]  подсчета суммы чисел в массиве и пытаетесь сравнить его с классическим решением,
[14:52.600 --> 14:59.120]  которое я предоставлю. Вот. И оказывается, что операция подсчета суммы при помощи атомарных
[14:59.120 --> 15:03.480]  операций работает быстрее, чем половина представленных реализаций. Просто потому,
[15:03.480 --> 15:07.840]  что это происходит на уровне ледвакоша. Значит, смотрите, операции на видеокарте,
[15:07.840 --> 15:14.000]  они пишутся на устройстве. Это функция atomic add, которая будет гарантировать, что после того,
[15:14.040 --> 15:19.880]  как вы ее выполните, у вас значение счетчика account увеличится на единичку. То есть, что у вас не
[15:19.880 --> 15:25.080]  будет датарейса в этот момент времени. То есть, у вас по факту будет внутренняя блокировка на
[15:25.080 --> 15:34.040]  уровне ледвакоша. Есть вот такая вещь. Есть операция atomic exchange, которая позволяет вам получить
[15:34.040 --> 15:43.800]  значение измененное по адресу. И есть еще одна операция atomic cas. Наверное, вы догадываетесь,
[15:43.800 --> 15:55.560]  что она делает. Кто знает, что такое cas? Compare and set. То есть, вы сравните значение,
[15:55.560 --> 15:59.640]  которое у вас есть. Если оно совпадает с тем, которое у вас есть, вы ставите определенное
[15:59.640 --> 16:11.400]  значение. Для чего нужен cas? Для синхронизации, для блокировок. Именно так. Ну и, собственно,
[16:12.320 --> 16:16.760]  вы можете сделать синхронизацию между блоками. Значит, вы берете специальную переменную типа
[16:16.760 --> 16:25.800]  device. Device int lock равно нулю. А дальше делаете следующее. Вы пишете вот такой вот код. Do while atomic
[16:25.800 --> 16:32.000]  cas lock 01. То есть, что он означает? Он означает следующее, что если у вас значение переменной lock
[16:32.000 --> 16:39.600]  равно нулю, то вы ставите единичку и выходите из этого цикла. Иначе двигаетесь дальше. После этого
[16:39.640 --> 16:48.680]  вы ставите операцию threadfence, чтобы все записи, которые были до текущего момента времени были
[16:48.680 --> 16:55.000]  синхронизированы. То есть, устанавливаете порядок. Дальше вы уже можете выполнять определенные
[16:55.000 --> 17:03.640]  операции. То есть, вот таким вот образом берется блокировка внутри GPU. Опять же,
[17:03.640 --> 17:09.000]  лучше сводить количество блокировок минимуму, потому что это приводит потом к печальным
[17:09.000 --> 17:18.560]  последствиям. Значит, смотрите. Тут я уже сказал, что sync threads используют внутри блок, как работает
[17:18.560 --> 17:23.640]  threadfence. Значит, она для shared памяти будет делать все операции внутри блок, синхронизировать.
[17:23.640 --> 17:32.200]  Плюс она указывает порядок для device элементов. То есть, мы ставим с вами sequential order для
[17:32.200 --> 17:38.680]  массивов. В конце, когда мы выполняем блокировку, нам нужно будет ее снять.
[17:38.680 --> 17:47.160]  Вот так вот работает глобальная блокировка. Так, идеологически понятно, как она работает?
[17:47.160 --> 17:59.120]  Хорошо. И последний момент, который очень важно делать, если вы хотите эффективно писать на
[17:59.120 --> 18:05.280]  видеокарте, это вам нужно понимать, какие есть ограничения на видеокарте. Для этого нужно
[18:05.280 --> 18:11.120]  понимать, сколько у нас в распоряжении имеется регистр, потому что операция регистр спилинг
[18:11.120 --> 18:16.160]  это очень неприятная операция, когда у вас значение, которое могло бы оказаться в регистре,
[18:16.160 --> 18:23.800]  падает в оперативную память видеокарту. Значит, смотрите, ограничение по регистрам. Всего можно
[18:23.800 --> 18:31.560]  получить 2 в 16 регистров на один блок. То есть, это 65 тысяч элементов. Ну, кажется, много. Но при
[18:31.560 --> 18:37.240]  этом количество регистров на поток, который выделяется один, их всего до 255. То есть по факту
[18:37.240 --> 18:45.160]  у вас количество регистров равняется одному байту. Одному байту бита. Значит, дальше, максимальное
[18:45.160 --> 18:52.800]  количество потоков на один стриминг-мультипроцессор. Точнее, я бы так сказал, что на один блок это 1024.
[18:52.800 --> 18:59.200]  Значит, как это проверить? Вы берете, меняете параметры ядра и увеличиваете размер блока в два раза.
[18:59.960 --> 19:06.480]  В какой-то момент времени у вас программа не запустится, и все. У вас по факту будет ошибка
[19:06.480 --> 19:12.760]  запуска ядра. Поэтому это нужно четко отслеживать. Значит, максимальное число потоков на стриминг-мультипроцессор
[19:12.760 --> 19:20.720]  на самом деле не 1024, в разных архитектурах разные. Где-то оно 1536, где-то 2048. То есть сколько у
[19:20.720 --> 19:26.240]  нас один стриминг-мультипроцессор может одновременно обрабатывать потоков. Значит, смотрите,
[19:26.240 --> 19:32.680]  если у нас размер блока 1024, то регистров на поток у нас будет 64. То есть мы спокойно
[19:32.680 --> 19:38.720]  можем с вами использовать 64 регистр на один поток без memory-спелинга. Значит, если размер блока 32,
[19:38.720 --> 19:44.200]  то число регистров, которое у нас получается, уже будет равняться 2048, которое трансформируется
[19:44.200 --> 19:56.120]  в 255. Да, потому что максимальный число регистров на поток у нас 255. 2048 больше, чем 255. Хорошо,
[19:56.360 --> 20:02.720]  давайте вопрос контрольный. Какой должен быть размер блока для того, чтобы мы сделали максимально
[20:02.720 --> 20:16.040]  возможное количество регистров, при этом не сильно теряли в производительности, исходя из этого. То есть
[20:16.040 --> 20:21.400]  смотрите, что у нас получается. У нас получается, когда размер блока 1024, у нас регистров на поток
[20:21.400 --> 20:28.600]  не хватает. Когда мы делаем размер блока достаточно маленький, то количество регистров на поток обрезается
[20:28.600 --> 20:36.160]  сверху. Да, смотрите, когда мы ставим размер блока 256, то мы используем максимальное количество
[20:36.160 --> 20:45.240]  регистров на поток. Это значение будет 255. Это крайне полезно для того, чтобы некоторые вещи
[20:45.240 --> 20:55.600]  оптимизировать. Хорошо, мы с вами добили тему, связанную с синхронизацией. Давайте вопрос.
[20:55.600 --> 21:18.160]  Где? Пока? Мы пока не дошли до подсчета суммы чисел массива. Пока что у нас были лог-при везде
[21:18.160 --> 21:24.960]  вычисления во всех тех примерах, которые мы рассматривали. Теперь пора брать блокировки.
[21:24.960 --> 21:35.120]  Едем к следующей презентации. Хорошо, давайте поедем к следующей презентации. Мы с вами сегодня
[21:35.120 --> 21:40.520]  просмотрим две задачи, по крайней мере начнем рассматривать. Первая задача это как посчитать
[21:40.520 --> 21:48.760]  сумму чисел массива, и второе это вычисление суммы чисел на префикс. Сразу скажу, что эти задачи
[21:48.760 --> 21:55.720]  можно распараллелить. Единственный момент, который я должен сказать, а симптотика алгоритма может
[21:55.720 --> 22:08.600]  быть большой. А симптотика алгоритма может быть нелинейной. Количество операций, которое мы делаем,
[22:08.600 --> 22:24.720]  нелинейное. Можно посчитать сумму чисел массива параллельно за n лог n. А симптотика будет больше,
[22:24.720 --> 22:36.240]  чем линейная. Наша цель сейчас будет рассмотреть базовый алгоритм, который работает за n лог n,
[22:36.240 --> 22:43.320]  но свести его к тому, чтобы он работал за линию. Давайте сформулируем задачу. Мы поняли с вами,
[22:43.320 --> 22:49.380]  что мы уже можем делать достаточно сложные операции. Мы можем складывать массивы между собой. Мы
[22:49.380 --> 22:55.060]  там даже можем попробовать матрицу перемножить на вектор, и это тоже будет параллельно. Но вот
[22:55.060 --> 23:03.060]  что будет, если мы попробуем с вами посчитать сумму чисел массива. Что такое сумма чисел массива?
[23:03.060 --> 23:09.460]  Мы с вами понимаем, что нам нужно взять все элементы в массиве, которые у нас есть,
[23:09.460 --> 23:16.740]  и каким-то образом сагрегировать в одну общую кучу. И сделать это еще каким-то образом параллельно.
[23:16.740 --> 23:28.620]  Пока что кажется, что это неприятно, потому что нам нужно большое количество блокировок. И здесь
[23:28.620 --> 23:34.180]  возникают два способа, которым мы это можем сделать. Но прежде чем мы это сделаем, давайте
[23:34.180 --> 23:42.140]  введем общее понятие задач Reduce или Reduction. У нас с вами есть некоторый массив А, и нам нужно
[23:42.140 --> 23:48.020]  выполнить некоторую операцию над этими элементами, которая будет являться, внимание,
[23:48.020 --> 23:55.420]  коммутативной, ассоциативной и с наличием нейтрального элемента. Какие операции подходят
[23:55.420 --> 24:08.460]  под это свойство? С нейтральным элементом. Да, конечно же, сложение. Увножение по модулю, да.
[24:08.460 --> 24:18.260]  То есть первое, что мы должны потребовать от операции звездочка, первое это звездочка B на C,
[24:18.260 --> 24:28.420]  второе, мы должны потребовать коммутативность, потому что если у нас не будет коммутативности,
[24:28.420 --> 24:34.620]  то один из алгоритмов, который я сегодня предоставлю, он сломается. Я даже специально
[24:34.620 --> 24:41.900]  спрошу, где ломается коммутативность. И третье, это наличие нейтрального элемента.
[24:48.260 --> 25:07.340]  Да, по умолчанию некоторые алгоритмы будут работать, но не все. Так, какие операции
[25:07.340 --> 25:18.020]  ультворяют этому свойству? Первое, это операция плюс. Какие еще операции? Умножить. Еще.
[25:18.020 --> 25:43.380]  Не, канкотинация не коммутативная. Миниум максимум. Сор, да. Нет, обратимость не нужна.
[25:43.380 --> 25:55.820]  Еще одна операция есть. Нот. Нот согласен. Вы не поверите, есть еще одна операция, но на самом
[25:55.820 --> 26:05.540]  деле она хитрая. И есть еще одна операция, это операция вычисления среднего, но важно здесь
[26:05.540 --> 26:16.380]  сказать, что она не напрямую является такой. Нужно сумму поделить на количество. То есть мы
[26:16.380 --> 26:24.380]  считаем сначала операцию плюс, потом делим на общее количество. Да, есть еще операция количество,
[26:24.380 --> 26:35.860]  в которой мы складываем единички, как ни странно. Потому что такое количество? Это сумма. Сумма
[26:35.860 --> 26:44.940]  единичка. Вообще каунт можно не считать. Хорошо. Наша цель будет посчитать сумму чисел в массиве.
[26:44.940 --> 26:52.300]  Классическое решение, у нас есть n операции сложения, мы делаем это все в одном потоке.
[26:52.300 --> 27:06.500]  Кажется несложно. Давайте вспомним решение, которое предлагалось на блоки, связанных с классическим
[27:06.500 --> 27:13.380]  параллельным вычислением, с mpi. Что мы можем сделать, когда у нас есть c потоков?
[27:13.380 --> 27:23.180]  Да. Именно так.
[27:23.180 --> 27:33.980]  Хорошо. Давайте как раз обсудим это решение. То есть у нас есть большой массив. Мы его делим
[27:33.980 --> 27:45.140]  на c больших блоков. Вот это блок 1, вот это блок 2, 3, блок c. Значит в каждом из них мы считаем сумму.
[27:45.140 --> 27:54.900]  Вот. А дальше складываем c элементов.
[27:54.900 --> 28:09.420]  Получаем суммарное количество операций какое? Какой у нас будет wall time? Симпатически.
[28:09.420 --> 28:16.700]  Ну это n, но если возьмем параметр c, равное количество потоков.
[28:16.700 --> 28:29.700]  Мы получим где-то n делить на c плюс c операций. Потому что для того, чтобы посчитать сумму в каждом
[28:29.700 --> 28:35.100]  блоке, в каждой ячейке, нам нужно n делить на c операции плюс сумме еще c операции.
[28:35.100 --> 28:51.380]  Можно залог c. Ну все равно, а симптойтка хорошая. Ну нет, она даже оптимально работает.
[28:51.380 --> 28:58.180]  Ну количество операций приблизительно. Хорошо, приблизительно количество операций.
[28:58.180 --> 29:08.060]  Да, суммарное количество операций, да, согласен. Суммарное количество операций,
[29:08.060 --> 29:13.940]  которое у нас есть, равняется n. Так, вопрос. В чем проблема этого алгоритма будет на GPU?
[29:13.940 --> 29:27.500]  Я как минимум две знаю. Ну первое. Первым проблемам последовательно складывать плохо,
[29:27.500 --> 29:43.420]  потому что у нас сломается кашление. Ну если варпам, то еще нормально, да, но все равно
[29:43.420 --> 29:53.260]  размер массива-то у нас большой. Хотелось бы, чтобы... Ну да, то есть по идее нам нужно разбить
[29:53.260 --> 30:01.540]  массив уже по-другому. То есть нам нужно взять наш массив. И, кстати, сразу подчеркну, что это
[30:01.540 --> 30:11.340]  требуется сделать в домашнем задании. Смотрите, как мы отдаем это все. Мы берем элемент нулевой,
[30:11.340 --> 30:26.820]  первый, c-1. Значит, дальше отдаем элемент цетой. Во-первых, чему может равняться c? Первый вариант
[30:26.820 --> 30:39.780]  c это размер блока. Это первый подход. Второй подход, который здесь есть c, это количество
[30:39.860 --> 30:53.540]  кудоядер. Да, в принципе, мы можем задать такие параметры, чтобы это у нас работало. Но третий
[30:53.540 --> 31:06.020]  вариант, правильный, это сделать следующее. Задать c. Нет, нет, смотрите, у нас есть три варианта,
[31:06.020 --> 31:18.660]  каким образом можем назначить c. Первое, это c, это размер блока. То есть это разбиение. То есть наша
[31:18.660 --> 31:27.020]  цель максимально распараллелить наш код. Параллельность идет как раз по вот этой ветке. То
[31:27.020 --> 31:32.260]  есть сверху вниз. И мы по факту должны сделать так, чтобы каждый поток максимально эффективно
[31:32.260 --> 31:37.820]  считал некоторую частичную сумму. То есть здесь мы считали ее горизонтально, а здесь мы будем ее
[31:37.820 --> 31:59.420]  считать вертикально. Вот. То есть здесь у нас будет c плюс 1, 2c минус 1. Да, да, да. Нет,
[31:59.420 --> 32:07.380]  здесь c это определенный параметр, пока мы его не специфицировали. То есть нам хотелось бы,
[32:07.420 --> 32:13.620]  чтобы вот тот алгоритм, где мы разбили наши все ядра для оптимального количества ядер c,
[32:13.620 --> 32:22.740]  наш код, наш массив на вот эти вот блоки. Блоки последних элементов. Каждый поток мы хотели бы,
[32:22.740 --> 32:30.820]  чтобы каждый поток вот эту сумму считал самостоятельно. То есть первый поток считает
[32:30.820 --> 32:35.220]  вот эту сумму, второй поток эту сумму, третий поток эту сумму и последний поток последнюю сумму.
[32:35.220 --> 32:42.340]  Да, но с видеокартой будет некоторая проблема. То есть мы хотели бы сказать,
[32:42.340 --> 32:53.820]  чтобы первый поток, первое кудоядро считало бы сумму 0.20. Второй кудоядро считало бы вот эту сумму,
[32:53.820 --> 33:00.220]  а последнее кудоядро считало бы вот эту сумму. Но нас здесь постигнет некоторая проблема,
[33:00.220 --> 33:09.380]  потому что количество кудоядер у нас не обязательно будет делиться на размер блока,
[33:09.380 --> 33:14.580]  чтобы у нас был оптимальный сдвиг. Поэтому здесь есть несколько подходов, как это решать.
[33:14.580 --> 33:23.220]  Значит первое это сделать следующее. Мы выделяем с вами один блок. Это подход номер один. И c это
[33:23.220 --> 33:36.020]  размер нашего блока. Мы тогда получаем ускорение, правда, получается не в с раз, не в оптимальное
[33:36.020 --> 33:42.500]  количество кудоядер раз, а всего лишь от силы в 128 раз, потому что у нас все обрабатывается одним
[33:42.500 --> 33:47.380]  стриминг мультипроцессором. В одном стриминг мультипроцессоре у нас 4 варпшедуллера,
[33:47.380 --> 33:53.340]  в каждом из которых по 32 потока. То есть мы вот при таком подходе, если мы назначим один блок и c
[33:53.340 --> 33:59.140]  равной размеру блока, мы получим ускорение где-то в зависимости от категории видеокарты в 64-128 раз.
[33:59.140 --> 34:08.260]  Но если у нас количество стриминг мультипроцессоров это там порядка двух с половиной тысяч или там 4532,
[34:08.260 --> 34:13.380]  если мы смотрим с вами видеокарту RTX 2080 Ti, то такой вариант не подходит.
[34:17.380 --> 34:22.580]  По 32.
[34:26.580 --> 34:31.780]  Нет, варп это физическая сущность.
[34:31.780 --> 34:35.260]  Сейчас.
[34:35.260 --> 34:42.380]  Нет, я говорю просто в одном стриминг мультипроцессоре у нас либо два варпшедуллера,
[34:42.380 --> 34:49.060]  и тогда количество CUDA-ядер в нём 64, либо четыре варпшедуллера, и тогда количество CUDA-ядер 128.
[34:49.060 --> 34:57.580]  Вот, второй вариант это просто назначить c равное количество CUDA-ядер, но тогда с этим будет работать
[34:57.580 --> 35:02.780]  сложно, потому что параметры ядра, которые нам нужно будет передать, это количество стриминг
[35:02.780 --> 35:08.940]  мультипроцессоров и 32. Но, как мы видели уже в предыдущей презентации, это не максимально,
[35:08.940 --> 35:14.540]  не оптимальный вариант, потому что количество регистров, которые будут выделяться, оно будет не максимально.
[35:14.540 --> 35:21.660]  И третий вариант, который у нас есть, он самый оптимальный. Собственно, мы назначаем параметры ядра
[35:21.660 --> 35:27.980]  равное количеству стриминг мультипроцессоров, и последнее мы назначаем размер ядра оптимальный,
[35:27.980 --> 35:35.340]  допустим 256. Да, конечно у нас агрегирующий массив в итоге с результатами получится в два раза больше,
[35:35.340 --> 35:50.980]  но асимптотика будет честная. Порядка n делить на c, получается n делить на c, плюс какое-то большое от c.
[35:50.980 --> 36:10.980]  Так, хорошо. Давайте я спрошу, понятно ли этот алгоритм? В чем его недостаток? У него есть один большой недостаток.
[36:10.980 --> 36:34.140]  Какой? Есть идея? Вы уверены, что видеокарта в этот момент времени параллельно не работает?
[36:34.140 --> 36:44.660]  И не загружена каким-то другим процессом? Вы точно удостоверились, что вы сначала запустили код на одной
[36:44.660 --> 36:50.820]  видеокарте, а потом кто-то ночью пришел, подменил вам видеокарту, и у вас вот эти параметры поехали?
[36:50.820 --> 36:56.620]  Внезапно ваш алгоритм начал работать в два раза медленнее.
[36:56.620 --> 37:07.900]  Я считаю, что посчитать сумму элементов в массиве – это самая важная задача на вашей видеокарте, а во-вторых, никто же не будет менять видеокарту.
[37:07.900 --> 37:20.380]  Ну а если она сгорела? Мне надоело просто массивы суммировать несколько лет подряд, а вот она и сгорела.
[37:20.380 --> 37:30.620]  Согласен, она может сгореть, но на самом деле главная проблема этого алгоритма заключается в том, что этот алгоритм не является стабильным.
[37:30.620 --> 37:41.980]  То есть как бы оптимальная производительность получить сложно. Поэтому есть другая идея, которую тоже придется реализовать.
[37:41.980 --> 37:50.060]  Давайте мы научимся с вами, попытаемся с вами эффективно вычислять сумму чисел внутри одного блока.
[37:50.540 --> 37:51.500]  Параллельно.
[37:53.980 --> 38:05.100]  Тогда смотрите, у нас будет такой пирамидальный алгоритм. Мы посчитаем с вами сумму чисел внутри блока, потом мы эти блоки агрегируем, посчитаем сумму чисел внутри-внутри блоков.
[38:06.540 --> 38:09.660]  И после этого мы получим одно итоговое значение.
[38:10.300 --> 38:12.860]  Давайте попробуем это на картинке нарисовать здесь.
[38:13.820 --> 38:15.340]  То есть у нас получается следующее.
[38:15.980 --> 38:35.180]  Раз, два, три, четыре блока. Мы считаем сумму S1, S2, S3, S4. А потом мы берем это все как один отдельный блок и еще делаем один разок сумму.
[38:35.420 --> 38:36.220]  Бинго.
[38:37.820 --> 38:39.820]  Вопрос как это реализовать эффективно.
[38:40.700 --> 38:46.380]  И как раз в следующую часть лекции мы посвятим тому, каким образом к этому всему можно подходить.
[38:48.300 --> 38:53.100]  А подходить к этому можно аж целых шестью или семью способами.
[38:54.380 --> 38:56.700]  Каждый способ будет улучшать предыдущий.
[38:57.660 --> 39:00.140]  Каждый способ будет улучшать предыдущий.
[39:01.420 --> 39:09.580]  Да, мы обсудили еще с вами очень тупой алгоритм, который не работает. Точнее он работает, работает достаточно быстро.
[39:10.460 --> 39:13.580]  Вы можете в принципе сделать следующее.
[39:14.540 --> 39:42.460]  Берете атомарную переменную sum и делаете for. Запускаете просто внутри каждого блока по счет суммы чисел и говорите, там у вас есть некоторые тит, thread.id.x, и вы пишете просто, ни на что не обращая внимания, вы пишете atomic at sum x от tit.
[39:42.700 --> 39:45.500]  То есть такое тоже никто не запрещает делать.
[39:47.260 --> 39:48.780]  Это даже работать будет быстро.
[39:50.300 --> 39:53.500]  Давайте мы перейдем к задаче подсчета суммы чисел внутри блока.
[39:55.020 --> 40:03.100]  И есть ли мысли у кого-нибудь из вас, как можно быстро попытаться посчитать за алгоритм сумму чисел внутри блока?
[40:07.020 --> 40:08.300]  За алгоритм тактов.
[40:12.460 --> 40:15.900]  Ну, вопрос. Представьте себе, у нас идет чемпионат мира.
[40:15.900 --> 40:19.260]  Как обычно, как мы за умереть могут это делать, вот так?
[40:20.140 --> 40:22.300]  Ну, турнирную сетку организовать.
[40:24.140 --> 40:32.540]  Ну, допустим по Фи-Фе, любой чемпионат мира берем по футболу, мы понимаем, что это огромное мероприятие, в котором нам нужно выяснить победителя.
[40:33.580 --> 40:37.420]  Количество победителей у нас один, да, нам нужно выяснить победителя.
[40:37.500 --> 40:41.740]  Количество партий, которые нам нужно провести для этого, это приблизительно порядка ОАТ.
[40:41.740 --> 40:46.700]  Но при этом мы разбиваем все на стадии, то есть у нас есть четверть финалы, полуфиналы, финалы.
[40:46.700 --> 40:51.740]  Причем четверть финала у нас могут проходить параллельно, полуфиналы тоже могут проходить параллельно.
[40:51.740 --> 40:53.740]  И мы доходим с вами до финала.
[40:54.380 --> 40:56.780]  Вот, поэтому мы будем делать следующую вещь.
[40:56.780 --> 41:02.620]  Значит, первая картинка заключается в том, что давайте мы как раз те самые партии, которые у нас есть,
[41:03.020 --> 41:09.980]  расположим в разделяемой памяти, а дальше каждый поток будет соответствующим заслужению своего элемента.
[41:09.980 --> 41:15.420]  То есть первый раунд у нас получается одна восьмая финала.
[41:15.420 --> 41:19.420]  Нам нужно определить победителя одной восьмой финала.
[41:19.420 --> 41:25.260]  Для этого у нас будет поток с номерами 0, 2, 4, 6, 8, 10, 12, 14,
[41:25.260 --> 41:27.260]  и мы будем делать первую картинку.
[41:27.340 --> 41:32.540]  Для этого у нас будет поток с номерами 0, 2, 4, 6, 8, 10, 12, 14,
[41:32.540 --> 41:36.540]  которые будут складывать значения соседних элементов.
[41:36.540 --> 41:41.340]  То есть в нулевую ячейку у нас будет записываться сумма нулевого и первого,
[41:41.340 --> 41:45.340]  во вторую ячейку сумма второго и третьего и так далее.
[41:47.340 --> 41:49.340]  Дальше что у нас происходит?
[41:49.340 --> 41:52.540]  Вторая ячейка у нас получается сумма нулевого и второго,
[41:52.540 --> 41:56.540]  это четверть финалы, потом четвертый и шестой,
[41:57.020 --> 42:01.020]  восьмой, десять, двенадцатый, четырнадцатый.
[42:01.020 --> 42:05.020]  После этого в третьем раунде у нас уже нулевой будет победитель,
[42:05.020 --> 42:07.020]  по факту агрегатор четверть финалов,
[42:07.020 --> 42:09.020]  встретятся между собой в полуфинале
[42:09.020 --> 42:13.020]  и посчитают сумму уже половины элементов массива.
[42:13.020 --> 42:17.020]  И в конце мы посчитаем общую сумму элементов массива.
[42:19.020 --> 42:21.020]  Алгоритм работает за алгорифм.
[42:23.020 --> 42:25.020]  Видно, за алгорифм тактов.
[42:25.500 --> 42:27.500]  При этом давайте посчитаем,
[42:27.500 --> 42:31.500]  какое количество операций мы по факту производим.
[42:35.500 --> 42:39.500]  Ну да, получаем 2n-1
[42:39.500 --> 42:41.500]  и в итоге осимпточка нашего алгоритма
[42:41.500 --> 42:43.500]  в итоге будет n на log n.
[42:47.500 --> 42:49.500]  В итоге, если у нас массив размером n,
[42:49.500 --> 42:53.500]  то у нас общее количество операций будет n на log n.
[42:55.500 --> 42:59.500]  Ой, тактов процессора.
[43:03.500 --> 43:05.500]  А, нет, стоп, не n на log n.
[43:05.500 --> 43:07.500]  Сейчас, секунду, о господи.
[43:11.500 --> 43:13.500]  Ну если они все в это время работают,
[43:13.500 --> 43:15.500]  на самом деле ничего не делают,
[43:15.500 --> 43:17.500]  то так.
[43:17.500 --> 43:19.500]  Да.
[43:19.500 --> 43:21.500]  Но если их как-то освобождать,
[43:21.500 --> 43:23.500]  то все не так?
[43:23.980 --> 43:25.980]  Да, на самом деле спасибо большое.
[43:25.980 --> 43:27.980]  Именно так нужно сказать.
[43:27.980 --> 43:29.980]  То есть наша цель как раз понять,
[43:29.980 --> 43:33.980]  действительно ли все операции у нас выполняются эффективно.
[43:33.980 --> 43:35.980]  И смотрите, вспоминаем
[43:35.980 --> 43:37.980]  омпцию дивергентности ворпа.
[43:37.980 --> 43:39.980]  Мы говорили следующее,
[43:39.980 --> 43:43.980]  что если у нас хотя бы один элемент в ворпе что-то делает,
[43:43.980 --> 43:45.980]  то весь ворп выполняет операцию.
[43:45.980 --> 43:47.980]  Возможно, просто элемент не записывается.
[43:49.980 --> 43:51.980]  Давайте поймем, на первой операции,
[43:52.460 --> 43:54.460]  сколько ворпов у нас задействовано.
[43:56.460 --> 43:58.460]  У нас идет сложение
[43:58.460 --> 44:00.460]  по четным элементам.
[44:02.460 --> 44:04.460]  Вопрос.
[44:04.460 --> 44:06.460]  Есть ли у нас ворпы, которые
[44:06.460 --> 44:08.460]  простаивают и ничего не делают?
[44:08.460 --> 44:10.460]  Или таких ворпов нету?
[44:14.460 --> 44:16.460]  Нет.
[44:16.460 --> 44:18.460]  Потому что у нас взаимодействует каждый
[44:18.460 --> 44:20.460]  четный элемент.
[44:20.460 --> 44:22.460]  И сейчас получается в каждом ворпе каждый четный элемент
[44:22.460 --> 44:24.460]  у нас в своем работе.
[44:24.460 --> 44:26.460]  То есть по факту мы, кажется, с вами
[44:26.460 --> 44:28.460]  производим 16 сложений,
[44:28.460 --> 44:30.460]  16 операций сложений
[44:30.460 --> 44:32.460]  внутри ворпа, если у нас размер ворпа 32.
[44:32.460 --> 44:34.460]  Но по факту мы производим с вами 32
[44:34.460 --> 44:36.460]  операций сложения.
[44:36.460 --> 44:38.460]  Потому что у нас каждый элемент ворпа
[44:38.460 --> 44:40.460]  берет и складывает элементы.
[44:42.460 --> 44:44.460]  То есть нечетный элемент ворпа тоже
[44:44.460 --> 44:46.460]  производит сложения.
[44:46.460 --> 44:48.460]  Просто они его опускают.
[44:48.460 --> 44:50.460]  Посмотрите.
[44:50.460 --> 44:52.460]  В итоге получается,
[44:52.460 --> 44:54.460]  если у нас размер блока 256,
[44:54.460 --> 44:56.460]  то на первом шаге мы вычисляем элементы 0, 2, 4
[44:56.460 --> 44:58.460]  сложений.
[44:58.460 --> 45:00.460]  У нас получается 8 ворпов.
[45:00.460 --> 45:02.460]  Второй шаг. 0, 4, 8.
[45:02.460 --> 45:04.460]  Тоже 8 ворпов. Все ворпы участвуют.
[45:04.460 --> 45:06.460]  На каком момента мы это будем повторять?
[45:12.460 --> 45:14.460]  Даже 64 пока он не станет.
[45:14.460 --> 45:16.460]  Третий шаг. 0, 8, 16.
[45:16.460 --> 45:18.460]  Четвертый шаг. 0, 16, 32.
[45:18.460 --> 45:20.460]  Пятый шаг. 0, 32, 64.
[45:22.460 --> 45:24.460]  Казалось бы, здесь количество ворпов
[45:24.460 --> 45:26.460]  у нас задействованных должно быть
[45:26.460 --> 45:28.460]  сильно уже меньше.
[45:28.460 --> 45:30.460]  То есть мы дошли с вами, когда у нас
[45:30.460 --> 45:32.460]  должно складываться 8 элементов,
[45:32.460 --> 45:34.460]  мы складываем с вами 8 элементов
[45:34.460 --> 45:36.460]  и на это тратим 8 ворпов.
[45:38.460 --> 45:40.460]  Хотелось бы использовать один ворп для этого.
[45:40.460 --> 45:42.460]  Да?
[45:42.460 --> 45:44.460]  Но у нас же есть там
[45:44.460 --> 45:46.460]  какой-то эльдорф кэш,
[45:46.460 --> 45:48.460]  а мы не могли вот просто
[45:48.460 --> 45:50.460]  для него вручить
[45:50.460 --> 45:52.460]  то, что мы не читали,
[45:52.460 --> 45:54.460]  чтобы сразу освобождать?
[45:56.460 --> 45:58.460]  Вот есть,
[45:58.460 --> 46:00.460]  это нужно смотреть инструкции,
[46:00.460 --> 46:02.460]  но здесь суть как раз будет состоять в том,
[46:02.460 --> 46:04.460]  чтобы правильно переномеровать потоки
[46:04.460 --> 46:06.460]  для того, чтобы этим не заниматься.
[46:06.460 --> 46:08.460]  Да.
[46:08.460 --> 46:10.460]  Да.
[46:10.460 --> 46:12.460]  Шестой шаг.
[46:12.460 --> 46:14.460]  Вот здесь у нас 4 получается ворп-операции,
[46:14.460 --> 46:16.460]  седьмой шаг, две ворп-операции,
[46:16.460 --> 46:18.460]  один шаг, последний шаг,
[46:18.460 --> 46:20.460]  это ноль операции.
[46:20.460 --> 46:22.460]  То есть смотреть в итоге,
[46:22.460 --> 46:24.460]  несмотря на то, что кажется,
[46:24.460 --> 46:26.460]  что у нас все выполняется
[46:26.460 --> 46:28.460]  за 8 тактов,
[46:28.460 --> 46:30.460]  за 8 операций,
[46:30.460 --> 46:32.460]  хотелось бы сказать, что у нас размер
[46:32.460 --> 46:34.460]  ворпа 32 элемента,
[46:34.460 --> 46:36.460]  поэтому по факту нам хотелось бы использовать
[46:36.460 --> 46:38.460]  порядка от 8 до 16 ворп-операций,
[46:40.460 --> 46:42.460]  то есть делать на 1-2 такты
[46:42.460 --> 46:44.460]  на каждый ворп,
[46:44.460 --> 46:46.460]  мы с вами получаем 47 ворпов
[46:46.460 --> 46:48.460]  на сложение 256 элементов.
[46:48.460 --> 46:50.460]  То есть мы делаем 47 тактов
[46:52.460 --> 46:54.460]  в 4 раза больше,
[46:54.460 --> 46:56.460]  чем мы хотим.
[46:56.460 --> 46:58.460]  Беда.
[47:00.460 --> 47:02.460]  Хотя здесь все честно.
[47:04.460 --> 47:06.460]  Понятен ли этот алгоритм?
[47:08.460 --> 47:10.460]  Хорошо. А теперь следующий,
[47:10.460 --> 47:12.460]  второй подход к решению задачи,
[47:12.460 --> 47:14.460]  давайте перенумеруем ворпы.
[47:14.460 --> 47:16.460]  Перенумеруем номера потоков.
[47:20.460 --> 47:22.460]  Ну это да.
[47:22.460 --> 47:24.460]  То есть смотрите, у нас все получается,
[47:24.460 --> 47:26.460]  нулевой поток складывает нулевой и первый элемент,
[47:26.460 --> 47:28.460]  первый поток будет складывать второй и третий элемент,
[47:28.460 --> 47:30.460]  второй поток будет складывать четвертый и пятый,
[47:30.460 --> 47:32.460]  и так далее.
[47:34.460 --> 47:36.460]  То есть как бы мы берем компексификацию
[47:36.460 --> 47:38.460]  наших потоков.
[47:42.460 --> 47:44.460]  Да.
[47:46.460 --> 47:48.460]  Ну, кажется алгоритм простой.
[47:54.460 --> 47:56.460]  Вот, это отдельный вопрос, который мы сейчас рассмотрим.
[47:56.460 --> 47:58.460]  Ну да, да, да.
[47:58.460 --> 48:00.460]  Да, скорее всего нужно каким-то образом
[48:00.460 --> 48:02.460]  убрать эту операцию.
[48:02.460 --> 48:04.460]  Так, давайте посчитаем, что с ворпами.
[48:04.460 --> 48:06.460]  Вопрос.
[48:06.460 --> 48:08.460]  Если мы сделаем так, то у нас получается,
[48:08.460 --> 48:10.460]  что первые 128 элементов
[48:10.460 --> 48:12.460]  внутри потока,
[48:12.460 --> 48:14.460]  внутри блока будут заниматься сложением.
[48:14.460 --> 48:16.460]  Сколько это ворпов будет?
[48:16.460 --> 48:18.460]  Было 8 ворпов.
[48:18.460 --> 48:20.460]  Теперь у нас первая половина
[48:20.460 --> 48:22.460]  потоков отвечает за статус
[48:22.460 --> 48:24.460]  ворпов.
[48:24.460 --> 48:26.460]  И у нас первая половина потоков отвечает
[48:26.460 --> 48:28.460]  за сложение. Вторая половина стоит.
[48:30.460 --> 48:32.460]  Сколько у нас ворпов будет
[48:32.460 --> 48:34.460]  задействовано?
[48:38.460 --> 48:40.460]  4, конечно же. У нас будет задействовано только первая половина.
[48:40.460 --> 48:42.460]  На втором шаге будет задействовано
[48:42.460 --> 48:44.460]  два ворпа,
[48:44.460 --> 48:46.460]  на третьем шаге будет задействован
[48:46.460 --> 48:48.460]  один ворп.
[48:48.460 --> 48:50.460]  Один ворп, один ворп,
[48:50.460 --> 48:52.460]  один ворп, один ворп, один ворп.
[48:52.460 --> 48:59.780]  то есть мы как бы берем и делаем следующее, что у нас первая половина будет отвечать за сложение
[48:59.780 --> 49:18.140]  элементов. Вот, поэтому у нас будет здесь ворп 1, ворп 2, ворп 4. Вот, давайте еще раз на картинку
[49:18.140 --> 49:23.900]  перемотаюсь. Тут важно смотреть именно на индекс. Видите, в кругляшках обозначены индексы.
[49:23.900 --> 49:38.820]  Ну что, берем алгоритм рассмотрения. Вот это уже сказали про подводный камень. А что будет в памяти
[49:38.820 --> 49:45.500]  в этот момент времени? Давайте обратим внимание, что мы с вами работаем с разделяемой памятью. Это
[49:45.500 --> 49:53.500]  очень важно. И у нас ломается с вами кагеретность киша. Но более того, она ломается здесь еще более
[49:53.500 --> 50:01.100]  критичным способом. Представим себе, что у нас с вами происходит сложение чисел массиве. И
[50:01.100 --> 50:29.860]  посмотрим внимательно на алгоритм. Сейчас я сотру с доски. Вот у нас массив. Сейчас подсохнет.
[50:29.860 --> 50:46.060]  Вот у нас поток номер 0 занимается сложением элемента 0 и 1. Поток номер 16 занимается сложением
[50:46.060 --> 50:59.220]  номера элементов 32 и 33. Но более важно то, куда они пишут свои значения. То есть этот пишет в поток
[50:59.220 --> 51:11.820]  с номером 0, а этот пишет с потоком номером 32. А теперь вопрос, как ни странный. Ходили ли вы
[51:11.820 --> 51:26.020]  когда-нибудь в МФЦ? В 32 элемент он пишет массива. Не, ну вот такой алгоритм просто, чтобы у нас
[51:26.620 --> 51:40.620]  данных не портился. Да, ходили в МФЦ. Вот вам говорят следующее. У вас в МФЦ 32 окна. В данном случае
[51:40.620 --> 51:49.540]  32 окна. У нас хиленькие МФЦ, так сказать. У нас с вами 32 окна. И говорят следующее. Значит,
[51:50.540 --> 51:59.740]  вот у вас обращение такое. Вы идете в окно, ваш номер по модулю 32. Ну, грубо говоря, будем считать,
[51:59.740 --> 52:03.500]  что у нас операции типа повторяются случайно. То есть у нас первый человек идет в первую,
[52:03.500 --> 52:11.020]  кабинку второй во вторую. 31 идет 31 кабинку. 31 и 32 человек идет в нулевое окно.
[52:11.020 --> 52:20.060]  Нет, сотрудников там 32. Ну вот теперь смотрите, у вас получается следующее, что у вас вот этот
[52:20.060 --> 52:31.260]  нулевой элемент, он должен идти в нулевое окно. Он должен идти в нулевое окно. А 32 элемент,
[52:31.260 --> 52:44.820]  он в какое окно должен идти? Тоже в нулевое. Ну а дальше что происходит? Они работают внутри одного
[52:44.820 --> 52:54.900]  варпа. То есть они делают это все за один такт времени. Смотрите, у нас возникает проблема,
[52:55.020 --> 53:04.140]  что у нас в один такт времени два элемента внутри одного варпа хотят записать значение в одно и то же окно.
[53:04.140 --> 53:18.300]  Да, вспоминаем то, как работает ворп. И вот эти вот 32 окна, которые я здесь перечислил,
[53:18.300 --> 53:26.140]  в концепции разделяемой памяти называется банк. Размер банка равняется размер варпа.
[53:26.140 --> 53:35.020]  И получается следующее, что если у нас два одинаковых потока, два потока внутри одного варпа,
[53:35.020 --> 53:41.140]  захотят записать ячейку в один и тот же банк, то у нас возникнет конфликт. И в итоге видеокарта,
[53:41.140 --> 53:48.980]  она должна быть user-friendly, она не должна кидать тик-фолд. Она сделает это за два такта,
[53:48.980 --> 54:01.780]  то есть она сделает запись в два такта, а не в один. Давайте подумаем, как это можно решить.
[54:01.780 --> 54:13.460]  Писать элементы в соответствующий номер потока. Хороший вариант. Единственное, что если мы здесь
[54:13.460 --> 54:19.460]  начнем писать не в 32-й элемент потока, а в 16-й, то то значение, которое у нас было в 16-й элемент
[54:19.460 --> 54:28.500]  потока, может перезатереться. Ну так, к слову. У нас еще появляется дополнительная правильная
[54:28.500 --> 54:33.460]  синхронизация. И вот здесь, смотрите, нам понадобится коммутативность, наша операция. То есть пока мы с
[54:33.460 --> 54:40.100]  вами нашим алгоритмом коммутативностью нигде не пользовались. То есть мы складываем элементы, как они
[54:40.100 --> 54:59.380]  складываются. Что мы могли бы? Давайте придумаем порядок, который нам позволит как раз записывать
[54:59.380 --> 55:20.820]  результат в свою собственную ячейку. Надо складывать с кем-то далеким. Бинго. Мы берем, бьем наш массив пополам
[55:21.420 --> 55:29.380]  и делаем следующее. Этот элемент будет складываться с тем же самым элементом из второй половинки.
[55:29.380 --> 55:38.420]  Этот элемент будет складываться с тем же самым элементом из второй половинки. В итоге у нас
[55:38.420 --> 55:49.420]  каждая ячейка будет записывать результат свой собственный элемент массива. Ура-ура. У нас теперь
[55:49.420 --> 55:54.600]  не будет конфликтов, потому что каждая ячейка, и мы записываем свой элемент массива, значит записывают
[55:54.600 --> 56:00.680]  результат в свою собственную банку. Поток, который будет записывать результаты в ту же самую банку,
[56:00.680 --> 56:08.520]  будет находиться уже в другом ворпе. Вот, поэтому вот такая вот вещь. Я тут промотаю, пример еще. Тут
[56:08.520 --> 56:13.840]  большое количество фору конфликтов, банк конфликтов возникает. И главная особенность программирования
[56:13.840 --> 56:19.300]  на видеокарте заключается в том, что когда вы работаете с разделяемой памятью, то нужно
[56:19.300 --> 56:29.140]  максимально сильно избегать банк конфликтов. Это особенность архитектурная видеокарта. Давайте
[56:29.140 --> 56:33.600]  еще раз. Что такое банк конфликт? Это поведение в shared memory, когда два потока внутри одного ворпа
[56:33.600 --> 56:41.120]  пытаются записать данные внутри разных кашлений по одному индексу. Вот, собственно, здесь есть как
[56:41.120 --> 56:47.480]  раз пример, что если у нас на каких-то дальнейших операциях будет запись вот такая, то мы получим
[56:47.480 --> 56:52.880]  с вами, что нулевой и первый поток будут конфликтовать по нулевому банку. То есть особенность
[56:52.880 --> 56:57.840]  здесь в том, что это мы на первом уровне обнаружили банк конфликта. То есть у нас получается где-то
[56:57.840 --> 57:04.200]  половина конфликта именно по парам конфликтует. Если мы дальше пойдем, то они будут уже по четверками
[57:04.200 --> 57:10.320]  конфликтовать, восьмерками конфликтовать и так далее. Но в этом расположении данных у нас
[57:10.320 --> 57:20.520]  такой проблемы не возникнет. Так, хорошо. Вот как это решается? Это решается вот таким образом. То есть
[57:20.520 --> 57:27.840]  еще раз. Мы берем значение в первые ячейки, складываем со второй половиной. И этот алгоритм уже
[57:27.840 --> 57:37.680]  будет работать эффективно. Значит так, хорошо. Давайте сделаем затравку на семинары, потому что
[57:37.680 --> 57:45.680]  здесь рассмотрелись только основные решения этой задачи. Значит решение следующее. Будет один хак,
[57:45.680 --> 57:53.280]  связанный с тем, что первую операцию можно делать до копирования в shared memory. То есть взять
[57:53.280 --> 57:59.400]  значение элементов, сложить, посчитать их сумму, отправить сумму в shared memory. Это ускорит где-то в
[57:59.400 --> 58:08.040]  два раза. Алгоритм в том, что количество операций в shared memory уменьшится в два раза. Вторая вещь. Вот видите,
[58:08.040 --> 58:19.160]  у нас здесь код. У нас после каждой операции, вторая оптимизация, у нас будет sync threads. То есть нам
[58:19.160 --> 58:33.000]  необходимо будет синхронизировать потоки. То есть у нас сначала 256, потом 128, потом 64, потом 32. И
[58:33.000 --> 58:44.920]  смотрите, когда мы уже будем получать элементы внутри одного варпа, нам окажется так, что операции
[58:45.400 --> 58:55.720]  уже не надо делать, потому что мы все операции будем производить внутри одного варпа. То есть мы
[58:55.720 --> 59:04.720]  можем снять пять блокировок, пять барьеров. Более того, есть некоторые прямо ассемблерные
[59:04.720 --> 59:09.600]  инструкции, которые работают по производительности так же, как размер варпа, которые позволят нам
[59:09.600 --> 59:18.520]  работать с этим всем эффективно. Задание со звездочкой. Знаете какое? Задание со звездочкой.
[59:18.520 --> 59:32.560]  Вспоминаем процесс математики. 1024. Что вы знаете про это число? Да, хорошо еще. Нет,
[59:32.600 --> 59:51.720]  тысячи нам не надо играть. Что еще мы знаем? Да, это 32 варпа. А это размер варпа. А теперь фишка в том,
[59:51.720 --> 59:57.880]  что если вы умеете складывать сумму чисел внутри варпа эффективно, то вы можете провернуть ту же
[59:57.880 --> 01:00:03.640]  самую операцию, но не на уровне блоков, а на уровне варпов. И в итоге у вас будет только две блокировки,
[01:00:03.640 --> 01:00:12.560]  а не блокировки на каждой из операций. То есть здесь у нас получается раз сингтрец, два сингтрец,
[01:00:12.560 --> 01:00:17.000]  три сингтрец, четыре сингтрец. И дальше снимаем блокировки. Здесь будет только две глобальные
[01:00:17.000 --> 01:00:26.600]  блокировки, если мы будем считать именно это все на уровне варпов. Вот так вот.
[01:00:26.600 --> 01:00:37.840]  Хитрая математика. Нет, я не знаю на семинарах. Если я успею к семинару этого дописать, то это будет
[01:00:37.840 --> 01:00:47.680]  на семинаре. Можем дать до балла за это, если я не успею к семинарам это дописать. Да, лучше сейчас
[01:00:47.680 --> 01:00:57.000]  спросить. То есть идея такая, значит еще раз. Мы по факту говорим следующее, что если нам
[01:00:57.000 --> 01:01:11.280]  нужно сложить элемент, то мы говорим, что S от TIT, S от TIT плюс 32. Код будет вот такой вот здесь.
[01:01:11.280 --> 01:01:20.480]  Ну когда у нас 64 элемента всего, мы пишем, что если номер нашего потока меньше чем 32,
[01:01:20.480 --> 01:01:31.200]  то мы делаем S от TIT плюс равно S от TIT плюс 32. И не выполняем операцию сингтрец. Почему не выполняем
[01:01:31.200 --> 01:01:36.520]  операцию сингтрец? Потому что у нас и так уже номер потока меньше чем 32. То есть у нас все
[01:01:36.520 --> 01:01:54.080]  потоки находятся внутри одного ворпа. Ну то есть нам не нужен вот тут вот. Да, да, да. Да, да. А просто
[01:01:54.080 --> 01:02:01.240]  их реализация занимает время. Звучит опасно, но ладно. Вот, поэтому что мы делаем? Мы берем,
[01:02:01.240 --> 01:02:10.800]  говорим, что это у нас ворп номер 0, это ворп номер 1, ворп номер 32, 31. Значит в каждом из них
[01:02:10.800 --> 01:02:25.720]  проворачиваем эту операцию. То есть мы пишем, грубо говоря. Что? Да, да, без синхронизации
[01:02:25.720 --> 01:02:35.080]  складывать можем. Да. А здесь мы говорим следующее, что S от 0 плюс равно S от 16. Значит S от 1.
[01:02:35.080 --> 01:02:49.560]  Во втором мы делаем S от 32 плюс равно S от 48. Складываем. А дальше у нас получается следующее,
[01:02:49.560 --> 01:03:03.440]  что у нас в ячейках S0 получается S32. Сколько-то там находится сумма чисел внутри своих собственных
[01:03:03.440 --> 01:03:18.000]  ворпов. Да, пройдет пять тактов. Без синхронизации, да. Пройдет пять тактов. Дальше мы вызываем общий
[01:03:18.000 --> 01:03:27.240]  sync threads и перекладывание элементов. Да, и кладем это все в один ворп. И опять это все повторяем.
[01:03:27.240 --> 01:03:39.440]  Одна блокировка. Да. Причем это дает ускорение где-то на 15-20 процентов по сравнению с самым
[01:03:39.440 --> 01:03:50.160]  оптимальным вариантом, который был рассказан здесь. Так. Аккуратнее с камерой. Так, понятно ли
[01:03:50.160 --> 01:03:59.200]  эта задача? Вообще на семинарах будут разобраны некоторые примеры, связанные с тем, что будет,
[01:03:59.200 --> 01:04:04.080]  если в каких-то моментах времени что-то не подключается, то есть где-то sync thread забывается,
[01:04:04.080 --> 01:04:12.280]  либо еще что-то. Ну вот. Но задача крайне полезная. И, собственно, дома, в домашнем задании вам ее
[01:04:12.280 --> 01:04:15.640]  нужно будет решить одним из двух способов. Первый способ, это который на правой доске,
[01:04:15.640 --> 01:04:21.040]  второй способ, это тот, который мы разобрали. Один из тех, который мы разобрали внутри блоков.
[01:04:21.040 --> 01:04:35.600]  Так, значит, да. Слайд И. Видимо, что-то с ним пошло не так. Не был выполнен sync thread.
[01:04:35.600 --> 01:04:42.720]  Давайте следующую задачу начнем, по крайней мере, рассматривать. Задача более интересная. Она
[01:04:42.720 --> 01:04:48.800]  заключается в том, что мы хотим с вами посчитать не только сумму чисел массива, но и посчитать
[01:04:48.800 --> 01:04:57.680]  сумму чисел на префексе. В каких случаях это может быть полезно? Знаете ли вы задачу под названием
[01:04:57.680 --> 01:05:05.600]  RSC? Да, то есть где нам нужно посчитать сумму на определенном отрезке. Тогда мы можем статически
[01:05:05.600 --> 01:05:12.920]  решить, первый раз посчитав сумму чисел на префексе, а во втором дальше вычислять раз
[01:05:12.920 --> 01:05:20.280]  на значении двух элементов. Вторая вещь, которая здесь достаточно полезна, которую можно решать,
[01:05:20.280 --> 01:05:27.400]  это задача фильтрации данных. Вы можете сказать, что я какие-то значения элементов массива оставить
[01:05:27.400 --> 01:05:31.680]  только те значения элементов, которые меньше определенного. Это можно эффективно делать на
[01:05:31.680 --> 01:05:39.720]  видеокарте. Давайте подумаем, вообще задача вычисления суммы чисел в массиве, суммы на префексе,
[01:05:39.720 --> 01:05:51.320]  она параллелизуется вот в лоб или нет? В лоб нет. То есть в отличие от вычисления суммы чисел в
[01:05:51.320 --> 01:06:00.080]  массиве, которые можно достаточно просто распараллелить, здесь парализация не происходит. Немножко подумать
[01:06:00.080 --> 01:06:07.920]  и за n лог n мы с вами сможем распараллелить эту задачу, не за o от n. Хотя можно и за o от n,
[01:06:07.920 --> 01:06:15.920]  но правда за o от n будет работать медленнее, чем за o от n лог n. Вот такие пироги, потому что асимптотика
[01:06:15.920 --> 01:06:21.800]  алгоритма будет сложная. Поэтому задача скан будет заключаться в следующем. У нас с вами есть массив
[01:06:21.800 --> 01:06:30.480]  a0, a1 и так далее, и наша цель посчитать сначала a0, потом a0 плюс a1, потом a0 плюс a1 плюс a2 и так далее.
[01:06:30.480 --> 01:06:37.960]  Первый способ это параллельный способ, то есть вы берете последний способ, берете последний
[01:06:37.960 --> 01:06:45.600]  и складываете элементы. Значит второй способ это сделать вот такую картинку. То есть смотрите,
[01:06:45.600 --> 01:06:57.760]  давайте я поясню здесь детально на доске. Вот у вас есть элементы, у вас есть 7 элементов,
[01:06:57.760 --> 01:07:19.240]  здесь индоксация будет 0, 0,4, 7, 6,
[01:07:28.240 --> 01:07:40.560]  что мы делаем? Давайте посмотрим, что такое сумма элементов с 0 по 3, с 0 по 2. Как ни странно,
[01:07:40.560 --> 01:07:46.560]  оказывается, что сумма элементов с 0 по 2 может быть подсчитана следующим образом. Мы берем,
[01:07:46.560 --> 01:07:57.640]  считаем сумму элементов с 1 по 2 плюс 0 элемент. Что такое сумма чисел, допустим, с 0 по 7?
[01:07:57.640 --> 01:08:08.640]  Это мы берем сумму чисел с 4 по 7 и складываем его с суммой чисел 0 по 3. Давайте я сотру,
[01:08:08.640 --> 01:08:29.840]  убрали. А здесь мы как раз считаем вот эту сумму. Сумма чисел с 0 до 6 может быть подсчитана
[01:08:29.840 --> 01:08:38.440]  следующим образом. Вы берете сумму чисел с 3 по 6 и складываете ее сумму чисел с 0 по 2.
[01:08:38.440 --> 01:08:45.680]  То есть каждый элемент массива на каждом уровне, на уровне D,
[01:08:45.680 --> 01:08:53.880]  будет хранить сумму 2 в D этой чисел нашего массива.
[01:08:53.880 --> 01:09:01.160]  Вот, то есть сначала мы будем складывать элементы последовательно.
[01:09:08.440 --> 01:09:18.280]  Потом у нас будет получаться, допустим, здесь у нас 0,1, здесь у нас будет посчитаться 2,3.
[01:09:18.280 --> 01:09:24.440]  А дальше мы будем считать сумму чисел через 2. То есть у нас 0,1 и 2,3 складываются между собой,
[01:09:24.440 --> 01:09:38.600]  получаем 0,3. И повторяем это до победного. Что мы с вами видим здесь? Мы с вами видим такую
[01:09:38.600 --> 01:09:44.840]  достаточно тяжелую пирамидку из значений. И сколько здесь операций у нас будет?
[01:09:44.840 --> 01:09:59.840]  Каждый раз примерно половина операций. Ну да. И слоев блоков. Да. То есть у нас получается асимптотика N лог N.
[01:09:59.840 --> 01:10:11.480]  На самом деле, если честно так подумать, то асимптотика для одного блока будет в данном случае
[01:10:11.480 --> 01:10:24.000]  блок size на логарифм блок size. То есть всего у нас асимптотика будет следующей. Если мы говорим в рамках одного массива,
[01:10:24.000 --> 01:10:36.400]  за один блок мы из N, N делить на блок size, будем получать за N на логарифм блок size элементов.
[01:10:36.400 --> 01:10:49.920]  Вот. Дальше потом у нас мы будем уменьшать размеры блоков. Мы будем получать с вами N делить на BS.
[01:10:49.920 --> 01:11:09.920]  Да. Да-да-да. Если так, то это N лог N, да. Да. Смотрите, давайте тогда я тут перемотаю как раз.
[01:11:09.920 --> 01:11:18.560]  Это мы тогда в следующий раз рассмотрим. Ну это на самом деле так. Во. Что мы делаем. То есть по факту,
[01:11:18.560 --> 01:11:26.280]  мы считаем блок scan. Дальше мы храним суммы блоков в определенном массиве. А дальше у нас получается,
[01:11:26.280 --> 01:11:32.960]  что для того, чтобы получить итоговый результат, вот у нас допустим есть вот здесь вот смотрите.
[01:11:32.960 --> 01:11:47.560]  Элементы 4, 5, 6, 7. У нас будет сумма значит 4, 4, 5, 4, 6, 4, 7. Как ее добить до общей суммы?
[01:11:47.560 --> 01:12:00.560]  Да. Прибавить то, что слева. То есть ставить сюда 0,3. А если у нас здесь элементы 8, 8, 9, 8, 10, 8, 11, то что нам нужно вставить?
[01:12:00.560 --> 01:12:09.520]  Префикс суммы по блокам. То есть получается следующее, чтобы посчитать это все, нам нужно посчитать значит суммы по блокам,
[01:12:09.520 --> 01:12:20.640]  посчитать суммы по префиксам, по суммам блоков и дальше вставить их в итоговый результат. Да, то есть у нас,
[01:12:20.640 --> 01:12:29.320]  если мы посчитаем, у нас выходят элементы здесь 0. Начало пустое множество. Вот у нас сумма по блокам,
[01:12:29.320 --> 01:12:39.040]  у нас есть элементы 0,3. Отсюда выходят у нас 4, 7. Отсюда у нас выходит 8, 11. А отсюда у нас выходит 12, 15.
[01:12:39.040 --> 01:12:44.600]  Если мы посчитаем сумму по префиксам по блокам, то у нас получается следующая сумма.
[01:12:44.600 --> 01:13:04.560]  Получается 0,3, 0,7, 0,11 и 0,15. Тогда мы можем еще сюда вставить пустое множество слева от этого и сделать следующее.
[01:13:04.560 --> 01:13:14.280]  Пустое сложить с этим, 0,3 сложить с этим. Что получается? 0,7 сложить с этим и 0,11 сложить в последнюю.
[01:13:14.280 --> 01:13:27.360]  То есть сделать общий инжект в конце. А поэтому здесь как раз за счет того, что у нас операция будет
[01:13:27.840 --> 01:13:34.960]  инжект, то асимптотика как раз этого алгоритма будет немножко меньше, чем n лог n. Из-за того, что здесь
[01:13:34.960 --> 01:13:43.720]  константа будет меньше. Поэтому те вот исхищения, которые будут здесь по поводу того, что мы хотим
[01:13:43.720 --> 01:13:48.520]  избавиться от этого алгоритма за счет увеличения константа, они будут на самом деле нерентабельными.
[01:13:48.520 --> 01:14:02.200]  Смотрите, сейчас объясню. Что тут будет? У нас будет асимптотика. Смотрите,
[01:14:02.200 --> 01:14:15.280]  n на алгоритм блок-сайза плюс n на блок-сайз на алгоритм блок-сайз плюс и так далее. То есть это
[01:14:15.280 --> 01:14:21.760]  вычисление как раз суммы на префиксах. Вот, сначала общий уровень, потом уровень меньше,
[01:14:21.760 --> 01:14:37.040]  уровень меньше, уровень меньше. На самом деле здесь еще будет некоторый момент,
[01:14:37.040 --> 01:14:44.640]  что нам нужно будет вставлять эти значения дальше вот в этот алгоритм. То есть мы должны
[01:14:44.640 --> 01:14:49.760]  будем вставить эти частичные суммы после того, как посчитаем. Поэтому здесь еще дополнительно где-то
[01:14:49.760 --> 01:14:59.920]  плюс n появится. Вот, ну и в итоге мы получаем с вами n на алгоритм блок-сайз
[01:14:59.920 --> 01:15:19.040]  плюс логарифом n по основанию блок-сайз. Вот такая формула, вот такая асимптотика у нас получится.
[01:15:19.040 --> 01:15:30.720]  Не, минимум мы искать не будем, потому что анафига нам нужно, если у нас это в итоге все делится на
[01:15:30.720 --> 01:15:44.920]  количество кудоядер. Ну то есть получается в числителе у нас будет порядка 10-15,
[01:15:44.920 --> 01:15:53.400]  образно говоря, а в знаменателе будет порядка четырех тысяч. Вот мы получаем,
[01:15:53.400 --> 01:16:02.160]  что ускорение будет приблизительно в 40 раз, даже на таком тупом алгоритме. Вот, а если мы как раз
[01:16:02.160 --> 01:16:08.720]  попытаемся избавиться от константа n, логарифм блок-сайз заменить здесь на единичку, у нас
[01:16:08.720 --> 01:16:18.160]  будет здесь константа не единичка, а намного больше, порядка 5-7. Поэтому вот выигрыш, он не будет
[01:16:18.160 --> 01:16:28.280]  работать. Вот давайте как раз с этим, чтобы понять, почему здесь константа будет достаточно
[01:16:28.280 --> 01:16:35.280]  большая, мы это как раз на следующей лекции разберем. Но идея там достаточно красивая,
[01:16:35.280 --> 01:16:40.480]  она может использоваться не только для вычисления суммы на префиксе, но и для
[01:16:40.480 --> 01:17:01.320]  вычтения корреляции Кенделла. Так, давайте вопросы. В этом алгоритме? Да-да, но если правильно
[01:17:01.320 --> 01:17:05.480]  организовать математику, то там нужно сделать IF на номер потока, не на четность-нечетность,
[01:17:05.480 --> 01:17:09.440]  а типа в первой половине, либо во второй мы в половине находимся. То есть здесь нам нужно как
[01:17:09.440 --> 01:17:20.920]  раз будет сделать IF номер нашего потока больше или равен чем offset? На самом деле вот в первом
[01:17:20.920 --> 01:17:25.720]  подходе, который был с проблемной математикой, там он повлияет, а остальных он не влияет.
[01:17:25.720 --> 01:17:39.040]  Все, наверное. Спасибо, что пришли. И тогда в следующий раз будем добивать эту тему.
[01:17:39.040 --> 01:17:42.040]  Немного поговорим про concurrency в видеокартах.

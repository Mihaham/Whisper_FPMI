[00:00.000 --> 00:05.440]  В общем, да, можно начать.
[00:05.440 --> 00:11.720]  Вначале я всё равно напомню, что было прямо от ожидания
[00:11.720 --> 00:14.240]  уже в прошлый раз.
[00:14.240 --> 00:17.560]  Мы предполагаем, что вероятность на пространстве или конечная,
[00:17.560 --> 00:19.840]  или в крайнем случае не более чем счётная, и тогда
[00:19.840 --> 00:23.200]  математические ожидания случайно увеличены.
[00:23.200 --> 00:40.680]  Букавка E обозначается, называется такая сумма
[00:40.680 --> 00:41.680]  по всем исходам.
[00:41.680 --> 00:50.000]  Я, напомню, случайно её лечена, это функция из вероятностного
[00:50.000 --> 00:56.640]  пространства в длительные числа.
[00:56.640 --> 01:02.320]  Я в прошлый раз не сказал, но вот если элементарных
[01:02.320 --> 01:04.600]  исходов счётное число, и тут написан ряд, надо
[01:04.600 --> 01:09.280]  ещё требовать, чтобы он абсолютно сходился, потому
[01:09.280 --> 01:11.520]  что мы, наверное, хотим, чтобы математическое
[01:11.520 --> 01:14.360]  ожидание не зависело от порядка нумерации элементарных
[01:14.360 --> 01:17.000]  исходов, поэтому, если он только условно сходится,
[01:17.000 --> 01:23.000]  то мы говорим, что математическое ожидание не существует.
[01:23.000 --> 01:47.880]  Всё, в прошлый раз я показал формулу
[01:47.880 --> 01:52.040]  о том, что математическое ожидание можно посчитать
[01:52.080 --> 01:56.520]  как сумму значений случайно увеличены на вероятность
[01:56.520 --> 01:58.320]  этого значения по всем возможным значениям.
[01:58.320 --> 02:08.400]  Я через кси от омега обозначаю множество значений случайно
[02:08.400 --> 02:09.400]  увеличены.
[02:09.400 --> 02:14.200]  Когда вы у меня спросили, почему обреление именно
[02:14.200 --> 02:17.240]  такое, почему среднее арифметическое, а не геометрическое или
[02:17.240 --> 02:20.400]  там среднее квадратическое или не медиано, я сказал,
[02:20.760 --> 02:23.560]  что оно такое, потому что у математического ожидания
[02:23.560 --> 02:25.840]  есть много всяких классных свойств.
[02:25.840 --> 02:30.880]  Вот, хорошо бы сейчас их сформулировать и доказать.
[02:30.880 --> 02:34.400]  Этим я и собираюсь заняться.
[02:34.400 --> 02:35.400]  Нумеруем.
[02:35.400 --> 02:49.400]  Если случайно увеличина не отрицательная, то и математическое
[02:49.400 --> 02:50.400]  ожидание не отрицательное.
[02:50.400 --> 02:52.760]  Я иногда буду доказывать, иногда буду просто как-то
[02:52.760 --> 02:53.760]  пояснять.
[02:53.760 --> 02:56.600]  Тут, я думаю, можно просто посмотреть сюда и сказать,
[02:56.600 --> 02:59.160]  что если слагаемые положительные, то, наверное, и сумма положительная.
[02:59.160 --> 03:15.560]  Второе свойство, самое главное свойство – линейность.
[03:15.560 --> 03:19.000]  Если мы умножаем на константу случайно увеличенную, то
[03:19.000 --> 03:23.120]  и математическое ожидание умножается на константу.
[03:23.120 --> 03:30.120]  И математическое ожидание суммы – это сумма математических
[03:30.120 --> 03:31.120]  ожиданий.
[03:31.120 --> 03:48.680]  Так, на всякий случай, вот эта буквка называется
[03:48.680 --> 03:52.520]  КСИ, вот эта буквка называется ЭТО, и это буквы греческого
[03:52.520 --> 03:54.520]  алфавита.
[03:54.520 --> 03:57.680]  Почему-то на экзамене все вот эту букву называют
[03:57.680 --> 04:01.600]  буквой ню, я не знаю, почему.
[04:01.600 --> 04:17.920]  Это свойство можно, наверное, даже доказать.
[04:17.920 --> 04:24.960]  Ну, с константой совсем все просто, да, можно просто
[04:24.960 --> 04:26.960]  из суммы вынести константу наружу.
[04:26.960 --> 04:29.280]  А вот линейность я распишу.
[04:29.280 --> 04:52.760]  Это по определению левая часть.
[04:52.760 --> 05:01.720]  Вот, ну тут нам надо, наверное, еще прокомментировать,
[05:01.720 --> 05:03.520]  что такое сумма случайных величин.
[05:03.520 --> 05:10.720]  Во-первых, если случайные величины действуют из
[05:10.720 --> 05:17.480]  разных вероятностных пространств, первая из Омега-1, а вторая
[05:18.480 --> 05:25.600]  из Омега-2 в Р, то их просто складывать нельзя, да?
[05:25.600 --> 05:29.160]  Когда я пишу сумму, я подразумеваю то, что случайные величины
[05:29.160 --> 05:32.040]  действуют из одного и того же вероятностного пространства,
[05:32.040 --> 05:36.160]  и тогда я их могу складывать просто поточечной, как
[05:36.160 --> 05:37.160]  функции.
[05:37.160 --> 05:38.160]  Вот.
[05:38.160 --> 05:45.920]  Тогда по определению суммы случайных величин тут
[05:46.000 --> 05:47.000]  написано что-то такое.
[05:47.000 --> 06:04.960]  Ну а теперь я могу перегруппировать солгаемое и разбить в две
[06:04.960 --> 06:05.960]  суммы.
[06:05.960 --> 06:31.480]  Первая сумма от ожидания КСИ, вторая сумма является
[06:31.480 --> 06:35.480]  от ожидания ЭТО.
[06:36.000 --> 06:59.000]  И вторая сумма от ожидания КСИ, вторая сумма от ожидания
[07:00.000 --> 07:03.000]  что если вот это существует и это существует, то тогда
[07:03.000 --> 07:04.000]  вот это существует.
[07:04.000 --> 07:17.880]  Ну, еще раз, вы можете считать, что сейчас вероятностное
[07:17.880 --> 07:20.560]  пространство конечное, и тогда никаких вопросов
[07:20.560 --> 07:26.760]  о сходимости вообще не возникает.
[07:26.760 --> 07:32.360]  Все эти свойства, монотонность, если одна случайная величина
[07:32.360 --> 07:44.360]  больше другой, то и от ожидания тоже больше.
[07:44.360 --> 07:46.840]  Это комбинация первого и второго свойств, потому
[07:46.840 --> 07:53.920]  что из того, что КСИ больше, чем это, следует, что разность
[07:53.920 --> 07:54.920]  больше нуля.
[07:54.920 --> 07:59.520]  Из того, что разность больше нуля, следует, что мотождание
[07:59.520 --> 08:00.520]  разности больше нуля.
[08:00.520 --> 08:10.320]  Мотождание разности, это разность мотожданий как
[08:10.320 --> 08:11.440]  следствие линейности.
[08:11.440 --> 08:33.360]  Больше нуля, это мотождание разности.
[08:33.360 --> 08:36.760]  Если разность мотожданий больше нуля, то тогда первое
[08:36.760 --> 08:40.080]  больше второго.
[08:40.080 --> 08:43.000]  Тут ничего интересного.
[08:43.000 --> 08:50.520]  Четвертое свойство будет про связь модуля мотожданий
[08:50.520 --> 08:55.120]  и мотождания модуля, или по-другому это еще называется
[08:55.120 --> 09:06.880]  неразница треугольника.
[09:06.880 --> 09:11.200]  Доказательство довольно простое, модуль суммы не
[09:11.200 --> 09:32.080]  больше, чем сумма модулей.
[09:32.080 --> 09:36.400]  Мы существенно используем, что математическое ожидание
[09:36.400 --> 09:39.760]  элементарного исхода положительно, значит его
[09:39.760 --> 09:43.320]  модуль, это просто есть вот эта вероятность, то
[09:43.320 --> 09:44.880]  есть модуль можно вот так перекинуть.
[09:44.880 --> 09:59.280]  И получается мотождание модуля кси.
[09:59.280 --> 10:04.320]  Пятое свойство обычно называют нераненством к ошибке Буниковского.
[10:04.320 --> 10:33.240]  Квадрат мотождания произведения не больше, чем произведение
[10:33.240 --> 10:36.760]  мотождания квадратов.
[10:36.760 --> 10:50.480]  Причем можно даже сказать, когда выполняется равенство.
[10:50.480 --> 10:52.280]  Равенство выполняется, когда случайные причины
[10:52.280 --> 10:53.280]  линейно-зависимы.
[10:53.280 --> 11:22.640]  Причем равенство, если, а тогда и только тогда, когда
[11:22.640 --> 11:23.640]  есть линейно-зависимость.
[11:23.640 --> 11:52.600]  Ну что то же самое, что или одна из них прожестная
[11:53.600 --> 11:58.840]  или кси можно выразить через это умножением на
[11:58.840 --> 11:59.840]  константу.
[11:59.840 --> 12:14.040]  Доказательства.
[12:14.040 --> 12:24.040]  Посмотрим произвольное t действительное.
[12:24.040 --> 12:30.280]  Не сложно понять, что квадрат случайной причины t кси
[12:30.280 --> 12:38.720]  минус это не отрицательный для любого t.
[12:38.720 --> 12:43.840]  Теперь я использую свойства математического ожидания,
[12:43.840 --> 12:46.320]  что такое мотождание вот этого квадрата.
[12:46.320 --> 13:09.040]  t у меня константа, t квадрат константа, тут сумма случайных
[13:09.040 --> 13:10.040]  причин.
[13:10.040 --> 13:12.440]  Могу написать, что это равно.
[13:40.040 --> 13:51.960]  Я еще тут немножко странно написал, что вот например
[13:51.960 --> 13:55.800]  t квадрат вынес константу не сюда, а вот сюда.
[13:55.800 --> 14:02.560]  Я это сделал для того, чтобы было очевидно, что тут написан
[14:02.560 --> 14:06.840]  квадратный трехчлен от переменной t, ну или возможно
[14:06.840 --> 14:08.160]  выраженный квадратный трехчлен.
[14:08.160 --> 14:14.120]  Я про него знаю, что он всегда больше лидерально
[14:14.120 --> 14:15.120]  нуля.
[14:15.120 --> 14:19.160]  Это как учат детей в восьмом классе означает, что дискриминат
[14:19.160 --> 14:20.160]  неположительный.
[14:20.160 --> 14:23.160]  Давайте я это напишу.
[14:38.160 --> 14:48.720]  Вот это дискриминант, и он меньше и равен нуля.
[14:48.720 --> 14:51.240]  Если перенести вот эту штуку в правую часть, получится
[14:51.240 --> 14:52.240]  неравенство к ошибнику.
[14:52.240 --> 14:55.720]  Теперь нужно разобраться с тем, когда достигается
[14:55.720 --> 14:56.720]  равенства.
[14:56.720 --> 14:59.240]  Для этого надо вспомнить, когда бывает так, что дискриминант
[14:59.240 --> 15:00.240]  равен нулю.
[15:00.240 --> 15:05.360]  Ну во-первых, такое может быть, если у нас квадратный
[15:05.360 --> 15:08.040]  трехчлен просто вырожденный.
[15:08.040 --> 15:10.720]  То есть вот эта штука равна нулю, вот эта равна нулю.
[15:10.720 --> 15:13.040]  У нас просто тут написано константы больше нуля.
[15:13.040 --> 15:20.360]  В этом случае, первый случай, вырожденный.
[15:20.360 --> 15:32.560]  Ну от ожидания x квадрат тогда тоже равно нулю.
[15:33.280 --> 15:39.240]  Осталось понять, что такое, когда такое вообще бывает,
[15:39.240 --> 15:40.240]  что от ожидания равно нулю.
[15:40.240 --> 15:47.160]  Ну от ожидания это сумма x омега квадрат вероятность
[15:47.160 --> 15:52.440]  омега, и это почему-то равно нулю.
[15:52.440 --> 15:57.040]  Тут все слагаемые не отрицательны, значит все они должны быть
[15:57.040 --> 16:00.040]  просто равны нулю.
[16:00.520 --> 16:03.920]  Каждым слагаемым или вот этот множество равен нулю,
[16:03.920 --> 16:08.920]  или вот этот множество равен нулю.
[16:08.920 --> 16:11.400]  Это значит то, что везде, где случайная величина
[16:11.400 --> 16:16.520]  не равна нулю, вероятность таких исходов равна нулю.
[16:16.520 --> 16:22.040]  По-другому, это можно переписать как вероятность, что кси
[16:22.040 --> 16:25.280]  равна нулю, равна единице.
[16:25.520 --> 16:28.520]  Кси равна нулю с вероятностью 1.
[16:28.520 --> 16:35.160]  Или еще в теории вероятности говорят, что тогда кси равна
[16:35.160 --> 16:36.160]  нулю почти наверно.
[16:36.160 --> 16:48.080]  В теории вероятности почти наверно это термин, и он
[16:48.080 --> 16:50.480]  означает то, что написано на доске.
[16:50.680 --> 16:57.720]  И тут мы возвращаемся, что я не совсем точно сформулировал
[16:57.720 --> 17:00.600]  пятые свойства, потому что вот тут тоже надо написать
[17:00.600 --> 17:01.600]  почти наверно.
[17:01.600 --> 17:08.600]  То есть случайные величины, они почти наверно линиенозависимы.
[17:14.600 --> 17:17.600]  Ну это альфа-бета, почему?
[17:17.720 --> 17:28.720]  Надо еще написать, что альфа-бета не равно нулю.
[17:35.720 --> 17:40.400]  В первом случае мы выяснили, что случайная величина
[17:40.400 --> 17:43.280]  кси почти наверно равна нулю, и тогда она понятна
[17:43.280 --> 17:44.600]  линиенозависима с это.
[17:48.600 --> 17:56.600]  Кси, например, кси плюс нулю умножить на это равно нулю
[17:56.600 --> 17:57.600]  почти наверно.
[17:58.600 --> 18:01.600]  Бета равно нулю, альфа равно единиц.
[18:01.600 --> 18:04.600]  Итак, выраженные случаи разобрали, осталось разобраться
[18:04.600 --> 18:07.600]  с невыраженным случаем, то есть когда там действительно
[18:07.600 --> 18:08.600]  квадратный трехчелем.
[18:10.600 --> 18:13.600]  Тогда тот факт, что дискриминант равен нулю, означает, что
[18:13.600 --> 18:14.600]  у него есть корень.
[18:14.600 --> 18:21.600]  То есть существует такое t, что математическое ожидание
[18:21.600 --> 18:25.600]  t кси минус это в квадрате равно нулю.
[18:25.600 --> 18:33.600]  Так же, как и в предыдущем случае, это означает, что
[18:33.600 --> 18:36.600]  вот эта случайная величина, она равна нулю почти наверно.
[18:36.600 --> 18:40.600]  Ну вот мы видим, что ты кси, и это линиенозависима.
[18:40.600 --> 18:41.600]  Пятое свойство доказали.
[18:44.600 --> 18:45.600]  Шестое свойство.
[18:57.600 --> 19:04.600]  О том, чему равна, равно математическое ожидание
[19:04.600 --> 19:05.600]  индикатора.
[19:05.600 --> 19:08.600]  Я сейчас напишу, а потом объясню, что здесь написано.
[19:18.600 --> 19:27.600]  Если для любого события из f, для любого события можно
[19:27.600 --> 19:29.600]  определить понятие индикатора.
[19:29.600 --> 19:33.600]  Этого события, индикатор, это будет случайно величина
[19:36.600 --> 19:37.600]  определенной следующим образом.
[19:43.600 --> 19:47.600]  Она равна единице, если линитарный исход попадает
[19:47.600 --> 19:50.600]  в событие, и нулю в противном случае.
[19:50.600 --> 19:54.600]  Кроме того, если случайная величина принимает значение
[19:54.600 --> 19:57.600]  только 0 или 1, то можно то множество значений, где
[20:00.600 --> 20:03.600]  то множество исходов, на которых случайная величина
[20:03.600 --> 20:06.600]  принимает значение 1, обозвать множеством a и получить,
[20:06.600 --> 20:08.600]  что случайная величина являетсяaron.
[20:08.600 --> 20:13.440]  то множество значений, где
[20:13.440 --> 20:16.720]  то множество из кодов, на которых случайная величина принимает значение 1
[20:16.720 --> 20:19.400]  обозвать множеством a и получи, что
[20:19.400 --> 20:22.500]  случайная величина является индикатором какого-то события
[20:22.500 --> 20:24.940]  То есть индикаторы в Iowa oriented, это просто
[20:24.940 --> 20:27.960]  случайная величина, которая принимает два значение 0 и 1
[20:27.960 --> 20:31.560]  свойство 6 이� chaque даёт то, attempt
[20:31.560 --> 20:34.320]  математически ожидания индикатора
[20:34.320 --> 20:38.280]  равно его вероятности. Вероятности события a
[20:38.280 --> 20:43.880]  Доказательства. Тут я воспользуюсь уже не определением, а вот этой формулой.
[20:43.880 --> 20:51.600]  Индикатор принимает два значения, 0 и 1, поэтому надо написать, что индикатор
[20:51.600 --> 21:00.680]  математически ожидания индикатора это 1 на вероятность, что индикатор
[21:00.680 --> 21:12.280]  равен 1 плюс 0 на вероятность, что индикатор равен 0. Второе слагаемое
[21:12.280 --> 21:17.680]  очевидно равно 0. Первое слагаемое равно 1 на вероятность, что индикатор
[21:17.680 --> 21:28.800]  равен 1. Но вот тут сюда подходят ровно те исходы, которые лежат в событии А.
[21:28.800 --> 21:39.600]  Здесь и написано просто вероятность события А. Шестое свойство доказано. Я
[21:39.600 --> 21:46.000]  предлагаю не делать перерыв, потому что у нас недавно лекция началась, кажется.
[21:46.000 --> 21:56.640]  Я думаю, кто-нибудь против? Никто не против. Отлично. Следующее свойство касается
[21:56.640 --> 22:10.080]  независимости. Пока у нас была только независимость событий. Вот. Независимость
[22:10.080 --> 22:14.800]  событий можно это понятие перенести на независимость случайных величин.
[22:14.800 --> 22:35.680]  Случайные величины называются независимыми, если для любых действительных x и y события вот
[22:35.680 --> 22:50.080]  такие. x равно x, это равно y независимо. Можно на самом деле брать только x и y из
[22:50.080 --> 23:02.760]  множества значений случайных величин. То есть x из x от омега и y из это от омега.
[23:02.760 --> 23:19.360]  Поскольку, если событие имеет вероятность 0 или 1, то оно независимо с чем угодно. Поэтому
[23:19.360 --> 23:26.040]  можно проверять только вот такие x и y, которые в принципе принимаются. Также надо сказать,
[23:26.040 --> 23:33.680]  что это опять же в общем случае будет неправильное определение, как это обычно бывает. Оно работает
[23:33.680 --> 23:44.200]  только для дискретного случая. Вот. И еще когда мы говорили про независимость событий, когда
[23:44.200 --> 23:49.560]  событий несколько, мы говорили про попарную и в совокупности. Тут тоже можно говорить про
[23:49.560 --> 23:53.840]  попарную независимость случайных величин и про независимость совокупности случайных величин.
[23:53.840 --> 24:07.320]  Ну, попарная независимость случайных величин, я просто скажу, что случайные
[24:07.320 --> 24:16.880]  величины попарно независимы, если любая пара независима. Я не буду этого писать, это и так
[24:16.880 --> 24:25.200]  понятно. Я напишу, что такое независимость совокупности. x1 и так далее, xn независима
[24:25.200 --> 24:35.600]  совокупности. Даже не обязательно. Пусть будет конечное число. Нет, я все же напишу, что их
[24:35.600 --> 24:56.640]  может быть бесконечно много, на всякий случай. Если любого x1, для любого x2, для любого x3 и так
[24:56.640 --> 25:12.600]  далее. Событие x1 равно x1, x2 равно x2, x3 равно x3, и так далее. Независимость совокупности.
[25:12.600 --> 25:31.320]  Мне кажется, очень логичное определение. Опять же, можно брать только те x,
[25:31.320 --> 25:41.160]  которые принадлежат множеству значений ксиитах. Я не помню, давал ли я определение независимости
[25:41.160 --> 25:54.440]  совокупности бесконечного числа событий? Если событий бесконечно много, то они называются
[25:54.440 --> 26:01.800]  независимыми совокупности, если любой конечный поднабор независима совокупности. Седьмое
[26:01.800 --> 26:12.360]  свойство. Если случайные величины независимы, то математическое ожидание произведения равно
[26:12.360 --> 26:21.480]  произведению математических ожиданий. Догадательства. Для догадательств я буду пользоваться не
[26:21.480 --> 26:31.400]  определением математического ожидания, а формулой. Обратное неверно. Да, я приведу к вам
[26:31.400 --> 26:46.840]  пример. Но сначала докажу. По определению, нет, не по определению, а по формуле математического
[26:46.840 --> 26:53.960]  ожидания произведения это такая сумма. Я качестве переменной выберу не x, а z для удобства.
[26:54.920 --> 26:56.600]  z из множества значений.
[26:56.600 --> 27:25.640]  А теперь надо подумать, что значит, что
[27:25.640 --> 27:33.080]  произведение случайных величин равно коммунточислу z. Это означает, что x равно коммунточислу x,
[27:33.080 --> 27:45.560]  это равно коммунточислу y, и произведение x, y равно z. Причем, понятно дело, эти события не
[27:45.560 --> 27:55.000]  пересекаются, поэтому я могу расписать, что вот эта вероятность равна сумме следующих
[27:55.000 --> 28:15.040]  вероятностей. Вероятность, что xi равно x, это равно y по всем x, y таким, что x, y равно z. Что я
[28:15.040 --> 28:20.800]  делаю дальше? Во-первых, если я здесь суммирую только по таким x, y, то я вот этот z могу заменить
[28:20.800 --> 28:28.720]  на x, y, а во-вторых, вот эту вероятность я могу заменить на произведение вероятностей,
[28:28.720 --> 28:35.120]  используя определение независимости. Нехорошо, да-да, то есть надо сначала z засунуть вот сюда,
[28:35.120 --> 29:04.240]  а потом заменить на x, y. Правильное замечание. Дальше я замечу, что если я перебираю все возможные
[29:04.240 --> 29:12.600]  значения x, y, то я получаю все возможные значения z. То есть, если мне не важно порядок в программах,
[29:12.600 --> 29:31.960]  я могу написать, что это просто сумма по всем x, сумма по всем y. Вот такой штуки. И осталось
[29:31.960 --> 29:37.960]  просто разложить на множители. У меня теперь переменные x и y независимые,
[29:37.960 --> 29:46.280]  значит это просто произведение вот такой суммы на вот такую сумму.
[30:02.840 --> 30:08.680]  Первая сумма равна математическому ожиданию x, вторая математическому ожиданию это. Теперь
[30:08.680 --> 30:30.440]  я приведу пример, когда в обратную сторону не работает. В общем, случайно я начну x,
[30:30.440 --> 30:35.320]  я возьму так то, что с вероятностью 1,4 она будет принимать значение плюс-минус 1,
[30:35.320 --> 30:53.560]  а с вероятностью 1,2 она будет принимать значение 0. А случайную личину это я определю как квадрат
[30:53.560 --> 31:08.560]  случайной личины x. Кто понимает, чему равно мотождание x? Нулё. Ну это очевидно,
[31:08.560 --> 31:16.120]  потому что просто распление симметричное. Так, а что нам надо проверить-то? Нам надо проверить,
[31:16.120 --> 31:26.680]  что мотождание x это равно произведению мотожданий. Правая часть уже равна нулю,
[31:26.680 --> 31:35.400]  потому что здесь один из множеств равен нулю, а левая часть это мотождание x в кубе. Ну,
[31:35.400 --> 31:44.560]  оно тоже равно нулю. Например, можно сказать, что просто x в кубе и x это то же самое.
[31:44.560 --> 31:52.520]  Одна и та же случайная величина. Ну или можно опять же сказать, что x в кубе имеет симметричное
[31:52.520 --> 32:02.160]  распление. В общем, вот это условие выполняется. Теперь надо догадать, что они зависимы. В принципе,
[32:02.160 --> 32:13.760]  интуитивно это вроде как очевидно, да, то есть это зависит от x вот так. Но если формально,
[32:13.760 --> 32:20.960]  то это надо доказывать. Чтобы это доказать, надо привести такие значения x и y, потому что вот
[32:20.960 --> 32:34.200]  эта вероятность, я сразу буду писать кс квадрат, не равнялась вероятности
[32:34.200 --> 32:57.440]  произведению вероятностей. Ну, например, можно взять x равно нулю, y равно нулю. Тогда слева написано
[32:57.440 --> 33:01.880]  одна вторая, потому что это происходит только когда x принимает значение ноль, это происходит
[33:01.880 --> 33:14.640]  вероятностей одна вторая. А справа тоже одна вторая умножена на вторая. Неравно. И последнее
[33:14.640 --> 33:27.680]  свойство, которое я сформирую, еще одна формула, которая обобщает вот эту формулу.
[33:27.680 --> 33:50.800]  Здесь phi это произвольная функция из r в r. Догадательство. Догадательство будет
[33:50.800 --> 34:20.560]  почти такое же, как догадательство вот этой формулы. Это я написал определение. А теперь
[34:20.560 --> 34:27.560]  я просто правильным образом группирую слагаемые. Для каждого значения x, величины,
[34:27.560 --> 34:37.440]  величины кс, я сгруппирую слагаемые так, чтобы сгруппировать все слагаемые, где сиатомика равна x.
[34:50.560 --> 35:13.440]  Опять нехорошо получилось. У нас омега здесь получается. Я прошу прощения. Надо, конечно же,
[35:13.440 --> 35:20.200]  сначала писать вот так. Сумму по всем.
[35:43.440 --> 35:58.400]  Теперь мы говорим, что раз сиатомика равно x, я могу вот здесь вот сиатомика заменить на x. И теперь я
[35:58.400 --> 36:17.640]  могу phi от x вынести за скобку. И получается сумма phi от x на сумма phi от омега тут по всем
[36:17.640 --> 36:29.000]  возможным x, а тут по тем исходам, где x от омега равна x. Но если мы суммируем вероятность всех
[36:29.000 --> 36:44.360]  исходов, где x от омега равна x, то мы получаем вероятность того, что x равна x. Ура, формула
[36:44.360 --> 36:51.320]  доказана. Теперь хотелось бы привести какой-то пример, как считать математическое ожидание.
[36:51.320 --> 37:14.200]  Я для примера возьму биномиальную случайную величину. Для подсчета использую вот ту формулу с первой
[37:14.200 --> 37:25.680]  доски. Я должен просуммировать по всем значениям. Биномиальная случайная величина принимает
[37:25.680 --> 37:34.200]  значение от 0 до n, поэтому я ставлю k от 0 до n. Беру вот это значение и умножаю на вероятность
[37:34.200 --> 37:59.360]  этого значения. Теперь нужно подставить. Вероятность значения вот такая. Теперь
[37:59.360 --> 38:06.400]  надо как-то такую сумму посчитать. Что я сделал? Я во-первых скажу, что пусть сумма будет от единицы,
[38:06.400 --> 38:15.960]  потому что если k на нулю, то тут с нулью слагаемая. А во-вторых, использую хитрую формулу k на cзн по k,
[38:15.960 --> 38:25.360]  это n на cзн минус 1 по k минус 1. Если кто-то не знает, я думаю, можно просто расписать через
[38:25.360 --> 38:50.840]  факториалы и сойдется. И по этой формуле получается n на вот эту c и на p вкатый 1
[38:51.840 --> 39:09.160]  n благополучно выносится за скобки, остается сумма. Давайте я еще за скобки вынесу p,
[39:09.160 --> 39:25.000]  чтобы здесь стояло k минус 1. c из n минус 1 по k минус 1 пока от единицы до n. Тут будет p в k минус 1,
[39:25.000 --> 39:46.320]  а тут будет 1 минус p. Я 1 минус p напишу как n минус 1 минус k минус 1. И видно,
[39:46.320 --> 39:56.000]  что вот здесь образовался бином ньютона. Если k минус 1 обозвать теперь за новую переменную,
[39:56.000 --> 40:01.440]  то эта перемена будет как раз от нуля до n минус 1, и по биному ньютону здесь все
[40:01.440 --> 40:13.280]  соберется в p плюс 1 минус p в n, то есть в единицу. Итого ответ получился np. В каком-то смысле
[40:13.280 --> 40:17.480]  можно было и проще это сделать, потому что ответ-то простой. Наверное есть какое-то
[40:17.480 --> 40:23.960]  простое объяснение, почему просто n умножить на p. Другой взгляд на вещи. Мы понимаем, что биномнальное
[40:23.960 --> 40:31.640]  распление берется из схемы с витанием b0. То есть у нас есть какое-то вероятностное пространство,
[40:31.640 --> 40:48.600]  где 2 вены исходов. Исходы это последовательность из нулей единиц. Вероятность одного исхода это
[40:48.600 --> 41:15.000]  p в степени количества единиц на 1 минус p в степени количества нулей. И кси это количество единиц.
[41:15.000 --> 41:25.800]  Давайте рассмотрим, для любого k, рассмотрим событие окаты, которое заключается в том, что
[41:25.800 --> 41:43.000]  на окатом месте стоит единица. Тогда я утверждаю, что кси будет равна сумме индикаторов событий окаты.
[41:43.000 --> 42:02.000]  Ну действительно, количество единиц, это как бы надо сложить несколько единиц, где каждый единица
[42:02.000 --> 42:11.200]  будет соответствовать единице вот здесь. Я надеюсь, что я сказал что-то понятное сейчас. В общем,
[42:11.200 --> 42:21.200]  это надо просто осознать. И тогда мы можем почитать математическое ожидание кси используя свойства
[42:21.200 --> 42:36.160]  линейности. Можно сказать, что мотождание кси это мотождание суммы индикаторов. А мотождание суммы
[42:36.160 --> 42:42.160]  это сумма мотожданий. А мотождание индикаторов по шестому свойству это вероятность события. То есть тут написано сумма
[42:42.160 --> 42:59.960]  вероятности событий окаты. И осталось просто сказать, что вероятность окаты равна p. Ну на интуитивном
[42:59.960 --> 43:13.760]  уровне это очевидно, потому что события окаты это события, что на катом броске случился успех. У нас
[43:13.760 --> 43:22.200]  вероятность успеха равна p, поэтому должно быть, наверное, вот так. Но если совсем строго, то мы же
[43:22.200 --> 43:31.400]  вероятностное пространство определили просто как множество исходов и набор их вероятностей. Поэтому
[43:31.400 --> 43:42.920]  формально вот это равенство надо проверить. То есть надо написать то, что это сумма вероятностей всех
[43:42.920 --> 43:51.040]  элементарных исходов, для которых xkt равно 1, а дальше вот эту сумму посчитать по binom-utom и получится p.
[43:51.040 --> 44:01.560]  Я думаю, стоит подробнее написать то, что эта сумма xkt равно 1 можно суммировать по всем остальным x.
[44:01.560 --> 44:20.400]  Суммирование по всем x кроме xkt и тут будет p в степени их количества на 1 минус p в степени их
[44:20.400 --> 44:29.640]  количества n минус их количества. Вот, только вот здесь надо еще прибавить единичку, потому что мы не
[44:29.640 --> 44:40.600]  учитываем теперь xkt. p выносится за знак суммы и тут по binom-utom получается единица.
[44:40.600 --> 44:50.000]  Окей, то есть вот здесь каждым слагаемым написано p, n раз складываем p и получаем np.
[44:50.000 --> 45:07.320]  Дисперсия, можно начать про дисперсию. Математическое ожидание это конечно очень хорошо, но вот бывает так,
[45:07.320 --> 45:22.560]  что случайная личина константная, просто всегда рано нулю, а бывает так, что она с вероятностью 1 вторая
[45:22.560 --> 45:26.600]  принимает миллион, а с вероятностью 1 вторая принимает значение минус миллион.
[45:40.600 --> 45:51.000]  Математическое ожидание здесь и здесь будет равно нулю, но вообще тут как бы большая разница, то есть если у нас вот
[45:51.000 --> 46:00.280]  такой вот доход и если у нас вот такой доход, это наверно совершенно разные вещи. Поэтому хочется
[46:00.280 --> 46:07.200]  иметь какой-нибудь еще параметр, чтобы измерять отклонение случайной личины от среднего значения.
[46:07.200 --> 46:24.560]  В качестве такого параметра можно рассматривать дисперсию, которая определяется как математическое ожидание
[46:24.560 --> 46:35.720]  квадрата от отклонения. Вот тут сразу можно вывести формулу,
[46:35.720 --> 46:52.400]  раскрываем квадрат.
[47:05.720 --> 47:15.760]  Раскрываем пленейности.
[47:15.760 --> 47:32.480]  Двойку забыл, точно. Спасибо.
[47:35.720 --> 47:51.960]  Еще есть ошибки? Это был тест на внимательность.
[47:51.960 --> 48:02.720]  Вот, осталось понять, что с этим делать. Ну, вот ожидание квадрата кси, так оно и останется.
[48:02.720 --> 48:12.320]  Дальше вот здесь двойка и мотождание. И то и то это просто константы. Константы можно выносить
[48:12.320 --> 48:19.600]  по линейности. То есть получается минус два мотождания кси умножить на мотождание кси.
[48:19.600 --> 48:26.600]  Или что то же самое, что то же самое удвоенный квадрат мотождания.
[48:26.600 --> 48:36.000]  Еще вот здесь вот мотождание, квадрат и мотождание. Но мотождание это константа,
[48:36.000 --> 48:40.800]  квадрат тоже константа, значит тут просто мотождание константы. А мотождание константы
[48:40.800 --> 48:53.440]  это просто сама эта константа. Получилось вот так. Можно что-то сократить, наверное.
[48:53.440 --> 49:15.800]  И получилось какая-то формула. Обычно при подсчете удобнее ее использовать, чем вот это.
[49:16.160 --> 49:23.800]  Какие у нас есть свойства? Первое свойство это не отрицательность, что очевидно, потому что это
[49:23.800 --> 49:44.840]  мотождание чего-то положительного по определению. Второе свойство такое. Во-первых, дисперсия не
[49:44.840 --> 49:58.760]  меняется, если сдвинуть на константу. Но это связано с тем, что если прибавить к
[49:58.760 --> 50:04.160]  случайной веществе константу, то как бы тут она прибавится, а тут она вычтется из мотождания.
[50:04.160 --> 50:11.760]  Точнее к мотожданию тоже прибавится. Короче, вот эта разность не изменится. Значит действительно
[50:11.760 --> 50:24.920]  дисперсия не поменяется. И еще можно выносить константу, только константа будет выноситься со
[50:24.920 --> 50:40.840]  знаком квадрата. Ну нужно вот здесь квадрат, значит там тоже должен стоять квадрат. Для того чтобы
[50:40.840 --> 50:56.920]  сформулировать третье свойство, мне понадобится объяснить, почему дисперсия определяется именно так.
[50:56.920 --> 51:04.280]  То есть если мы говорим про отклонение среднего, то мы могли бы, например, сказать, что дисперсия
[51:04.280 --> 51:16.600]  это мотождание модуля разности. Почему это? Почему не так? Это же логично было бы. Вот. Правильный ответ
[51:16.600 --> 51:22.600]  такой. Дисперсия определяется так, потому что так она обладает хорошими свойствами и с ней удобно работать.
[51:22.600 --> 51:42.960]  Определение к вариации случайно личной кси и это называется
[51:42.960 --> 51:50.840]  вот такое мотождание.
[51:50.840 --> 52:20.440]  Вот. Какие свойства есть у к вариации? Во-первых, если мы поставим кси равно
[52:20.440 --> 52:39.520]  это мы получим дисперсию. Ковариация кси-кси это дисперсия. Во-вторых, ковариация будет
[52:39.520 --> 52:50.240]  являться симметричной линейной формой, чтобы это не значило. Я думаю, это сочетание уже на втором курсе можно
[52:50.240 --> 53:01.640]  понимать после курса линейной алгебры. Может и не. Я напишу, что это значит. Значит, что, во-первых,
[53:01.640 --> 53:12.360]  ковариация кси-это это то же самое, что ковариация это-кси симметричность. А во-вторых, она билинейна,
[53:12.360 --> 53:24.440]  то есть линейна по каждой координате. То есть ковариация акси-это равна а на ковариацию кси-это,
[53:24.440 --> 53:33.880]  то есть можно константу выносить. А во-вторых, линейность.
[53:33.880 --> 53:58.600]  То есть свойства очень похожи на свойства скалярного произведения.
[53:58.600 --> 54:10.120]  То есть можно воспринимать ковариацию как скалярное произведение случайных величин. Особенно если посмотреть еще на первое свойство и
[54:10.120 --> 54:17.840]  соединить с первым свойством дисперсии, получится, что ковариация кси-кси не отрицательная, симметричная и билинейная.
[54:17.840 --> 54:36.800]  Вот. А дисперсии это ковариация кси-кси. В линейной алгебре это называется квадратичная форма со всеми вытекающими последствиями.
[54:36.800 --> 54:46.360]  А какие эти вытекающие последствия мы узнаем на следующей лекции. А сейчас закончилось.

[00:00.000 --> 00:27.040]  Невязки вот в такой норме бывают, вот, из корень
[00:27.040 --> 00:55.040]  из эм большой, да, минус корень из эм маленькая, в степени к, норма начальника невязкая, вот, и что такое маленькое, это границы спектра
[00:57.040 --> 01:03.040]  все собственные значения матрицы
[01:03.040 --> 01:06.040]  эм большой
[01:06.040 --> 01:13.040]  прекрасный результат, видно как, что управляет сходимость
[01:13.040 --> 01:24.040]  распределение собственных значений, эту оценку можно уточнять если ее сформулировать в терминах индивидуальных собственных значений
[01:24.040 --> 01:33.040]  вот, но тут, оценка сделана более грубой, но зато для целого класса матрицы
[01:33.040 --> 01:46.040]  тут не важно какая эрбитовая матрица, лишь бы вот эти маленькое и эм большое были фиксированы, и тогда вот эта оценка имеет место на любой такой эрбитовой матрице, для любой начальной точки
[01:46.040 --> 01:58.040]  ну в конце лекции просто я еще и задачку вам предложил, на самом деле число итераций зависит от начальной невязки
[01:58.040 --> 02:10.040]  вот, если начальная невязка раскладывается лишь по, ну скажем, с собственным векторам, то за эсатерацией точно и решение будет
[02:10.040 --> 02:16.040]  просил я вас это доказать, ну надеюсь что вы сделаете, вот вам задачка
[02:16.040 --> 02:33.040]  ну можно и дальше еще сказать, а если начальная невязка раскладывается по таким собственным векторам, что собственные значения лежат в интервале от эм маленького до эм большого
[02:33.040 --> 02:48.040]  ну где эм маленький может быть больше, а эм большое меньше, но тогда эта оценка тоже будет верна, то есть на самом деле, значит все здесь, извините, смотрю
[02:48.040 --> 03:08.040]  да, Наташа, сейчас буду занят, добрый день, нет, сегодня ни в коем случае не приеду, нет, я по четверкам не бываю
[03:08.040 --> 03:21.040]  ну можно там нарисовать, ладно, хорошо, спасибо, я прошу прощения
[03:21.040 --> 03:38.040]  вот, для начальной невязки, начальная невязка просто, вот начальная невязка вы разложили ее по собственным векторам и смотрите какие там собственные значения
[03:38.040 --> 03:51.040]  и там взять эм маленькое, большое, и эта оценка будет справедлива, да или нет, ну просто понятность с самого хода доказательства, будет именно такая оценка получаться
[03:51.040 --> 04:01.040]  то есть есть еще такая зависимость принципиальная, конечно, от выбора начального вектора, как фишка легла, что он знает, как он будет выбран
[04:01.040 --> 04:13.040]  ну здесь возникает вот какой вопрос важный, вообще-таки он для вычислительной линейной алгебы и для теории итерационных методов может быть один из главных вопросов
[04:13.040 --> 04:26.040]  значит, ну наше общее желание такое, чтобы число итерации было поменьше, число итерации, нужной для достижения нужной точности, было поменьше, понятно?
[04:26.040 --> 04:48.040]  и этого добиваться можно в принципе двумя способами, можно придумывать новые итерационные методы, а можно неким образом модифицировать уже известные итерационные методы
[04:48.040 --> 04:56.040]  а можно преобразовать матрицу и применять уже известные методы, вот два пути
[04:56.040 --> 05:08.040]  ну вы давайте, честно говоря, мне больше нравится второй путь, не плодить новых итерационных методов, хотя эти тоже можно менять
[05:08.040 --> 05:26.040]  а вот имея некий универсальный, хороший итерационный метод, например, для ермитовых матриц, метод с упражненным ингредиентом, пытаться преобразовать матрицу, ну то есть преобразовать систему, перейти к эквивалентной системе фактически
[05:26.040 --> 05:36.040]  но так, чтобы для этой новой системы метод сходился быстрее, о чем речь?
[05:36.040 --> 05:42.040]  ну что такое эквивалентная система?
[05:42.040 --> 05:52.040]  вот, понятно, ну тут, опять же, кому больше нравится, есть две возможности
[05:52.040 --> 05:56.040]  я могу вот так вот написать
[05:56.040 --> 06:04.040]  ну, Б, слишком много букв Б, давайте
[06:04.040 --> 06:08.040]  напишу букву П
[06:08.040 --> 06:12.040]  ну П должна быть невыраженым
[06:12.040 --> 06:17.040]  любую невыраженную взяли, ну получились они
[06:17.040 --> 06:23.040]  значит это одна возможность, но есть и вторая возможность
[06:23.040 --> 06:28.040]  написали вот так вот
[06:28.040 --> 06:34.040]  ну тут понятно, если вы эту систему решили, то чтоб найти Х, надо
[06:34.040 --> 06:39.040]  и найдено Y может быть
[06:39.040 --> 06:51.040]  вот две матрицы, вместо матрица возникает матрица либо П умножить на А, либо А умножить на С
[06:51.040 --> 06:57.040]  значит вот этот переход, переход от системы с матрицей А
[06:57.040 --> 07:03.040]  системы с матрицей П умножить на А, или А умножить на П, принято называть предвуславливаемым
[07:03.040 --> 07:07.040]  предвуславливаемым
[07:07.040 --> 07:14.040]  значит почему, происхождение этого термина
[07:14.040 --> 07:18.040]  ну вот если вы на эту оценку посмотрите, то ее можно переписать
[07:18.040 --> 07:22.040]  вот этот умножить можно переписать вот в таком виде
[07:31.040 --> 07:38.040]  вот то, что я обвел в кружочке, это есть число обусловленности нашей элементной матрицы
[07:38.040 --> 07:41.040]  правильное число обусловленности нашей элементной матрицы
[07:41.040 --> 07:46.040]  ну если считать, что М маленькое, это в точности самое большое, самое маленькое
[07:46.040 --> 07:54.040]  то есть в оценке, эта оценка зависит на самом деле от отношения большого и маленького
[07:54.040 --> 07:58.040]  то есть от оценки числа обусловленности
[07:58.040 --> 08:06.040]  ну как шлом обусловленности новому матрицу, новой формы ромобра
[08:06.040 --> 08:14.040]  оно у нас возникло, когда мы обсуждали влияние малых возмущений на решение элементной системы
[08:14.040 --> 08:21.040]  то есть предусловить, как бы изменить число обусловленности
[08:21.040 --> 08:25.040]  за счет умножения на матрицу П с левой или с правой
[08:29.040 --> 08:33.040]  ну матрицу П надо выбрать получше
[08:33.040 --> 08:37.040]  самый идеальный выбор, какой?
[08:38.040 --> 08:40.040]  аминус 1
[08:40.040 --> 08:44.040]  конечно, и личной матрице число обусловленности равно единицы меньше не бывает
[08:44.040 --> 08:47.040]  одна это рад
[08:51.040 --> 08:55.040]  но этот выбор практически невозможен
[08:55.040 --> 09:04.040]  потому что если мы знаем аминус 1, то мы вообще-то и решение системы легко находим
[09:04.040 --> 09:06.040]  если мы умеем его назвать на аминус 1
[09:07.040 --> 09:11.040]  то нужноять какой-то компромисс
[09:11.040 --> 09:19.040]  в каком-то смысле правильно думать, что предусловиватель есть некое приближение к обратной матрице
[09:19.040 --> 09:21.040]  но в каком-то смысле
[09:21.040 --> 09:24.040]  смысл добросет последний punto
[09:26.040 --> 09:31.040]  в некотором смысле предусловиватель есть приближение к обратной матрице
[09:31.040 --> 09:40.860]  еще раз подчеркиваю, этот смысл может быть очень широким, но нам бы хотелось, если исходить из вот
[09:40.860 --> 09:52.360]  этой оценки, нам бы хотелось, чтобы поменьше новые числа выслушать. Но этого мало. Что еще нужно? Нужно,
[09:52.360 --> 10:03.120]  если мы по первому пути идем и решаем такую систему, нужно, чтобы на матрицу П была
[10:03.120 --> 10:09.400]  процедура умножения. Значит, А по определению мы имеем эффективную процедуру умножения.
[10:09.400 --> 10:16.040]  Вот, чтобы применять метод сопряженных градиентов, нужна эффективная процедура умножения на П.
[10:16.040 --> 10:38.000]  Ну и здесь тоже. Ну тут надо опять же, ну да. Ну, понятно, что я не вкладываю
[10:38.160 --> 10:44.760]  математический смысл. Ну, приемлемо-быстрее. Матрица может быть остановически большой,
[10:44.760 --> 10:54.080]  но есть какая-то задана программа, которая умножает матрицу на вектор. А сама матрица,
[10:54.080 --> 11:02.240]  в виде массива своих элементов, отсутствует. Вся информация о матрице зашита в этой процедуре
[11:02.240 --> 11:08.600]  умножения. Вот, для А задана некая процедура. Ну и желательно, чтобы аналогичная процедура
[11:08.600 --> 11:15.440]  для П была, была бы еще более быстрая, чем процедура умножения на А желательно. То есть,
[11:15.440 --> 11:21.440]  желательно, чтобы была вообще еще быстрее. Ну а с тем сравнивать. Ну, цену одну операцию вы знаете,
[11:21.440 --> 11:26.760]  это у Атемовича, вместе с Громеджемином. Вот, конечно, идеальнее, если будет порядка,
[11:26.760 --> 11:37.000]  порядка. Эна Пираса это идеальней, конечно. Хотя, довольно редко сама процедура умножения на матрицу
[11:37.000 --> 11:49.160]  имеет такую сложность. Обычно она сложнее. Но есть еще один подводный камень, который, возможно,
[11:49.160 --> 11:56.480]  что-то из вас уже заметил. А для каких матриц применяется метод попрыженного объема?
[11:56.480 --> 12:10.880]  Для ермитовых положительных определенных. Вот смотрите, вот мы имели матрицу А. Ермитову
[12:10.880 --> 12:25.480]  положительную определенную. И вот умножили, ну скажем, я не знаю. Вот перешли к матрице вот такой.
[12:25.480 --> 12:40.520]  А что стало с ермитовскими? Говорю уж о положительных. Ну вот, допустим, мы П тоже,
[12:40.520 --> 12:49.320]  будем тоже выбирать П. Ну, логично, потому что если это приближение к обратной матрице в каком-то
[12:49.320 --> 13:02.760]  смысле, то обратная матрица тоже ермитовая положительная. Вот когда мы П умножим на А, ермитовость обычно теряет.
[13:02.760 --> 13:18.840]  Если вот весь метричный матрик спрямнулся, результат будет вообще не симметричный. А вот здесь, давайте,
[13:18.840 --> 13:27.880]  вот здесь полезен некоторый уровень абстракции, который, мне кажется, в наших построениях тоже был.
[13:27.880 --> 13:35.240]  Да мы-то как говорили, вот матрица и есть естественные скалярные произведения. Ну, может быть,
[13:35.240 --> 13:45.120]  вас учили на первом курсе или не знаю, что вот можно ввести совершенно произвольные скалярные
[13:45.120 --> 13:52.840]  произведения. Можно определять самые разные скалярные произведения. Ну вот, и определить
[13:52.840 --> 14:02.800]  для линейного оператора А понятие сопряженного оператора, он вот так вот обозначается, вот таким
[14:02.800 --> 14:14.400]  равенством. Точно на пространстве ввели скалярные произведения, и вот линейный оператор со звездой,
[14:14.400 --> 14:20.600]  который обеспечивает такую равенство для любых электрофон, называется сопряженным. Ну,
[14:20.600 --> 14:27.880]  теперь сопряженный оператор будет зависеть от выбора скалярного произведения. Понимаете?
[14:27.880 --> 14:39.000]  Изменим скалярные произведения. Сопряженный оператор изменится. Ну и можно говорить, значит,
[14:39.000 --> 14:44.000]  если, как будет здесь пониматься, эрмитовость относительно скалярного произведения теперь
[14:44.000 --> 14:52.040]  рвутова. То есть если сопряженный оператор совпадает с исходным оператором, в данном скалярном
[14:52.040 --> 14:58.840]  произведении, то он называется эрмитовым. В данном скалярном произведении появляется степень свободы,
[14:58.840 --> 15:05.920]  выбор скалярного произведения. А теперь смотрите, какое бы мы скалярное произведение не выбрали,
[15:05.920 --> 15:10.440]  если в этом скалярном произведении у нас эрмитовый оператор, то мы можем запускать метод
[15:10.440 --> 15:17.480]  сопряженного граммета. Ну при условии, что он еще положится определённо. Всё. Значит, теперь задача
[15:17.480 --> 15:29.160]  предобуславливания. Вот я утверждаю, что оператор вида П умножить на А, он будет эрмитовым и положить
[15:29.160 --> 15:36.360]  на определённом в некотором скалярном произведении. Его можно выбрать. А задача для вас, придумайте
[15:36.360 --> 15:43.720]  такое скалярное произведение. Ну для его придумывания надо знать матрицы П и матрицы А. То есть вот
[15:43.720 --> 15:48.440]  матрица П умножить на А является эрмитовой положительно определённой в некотором скалярном
[15:48.440 --> 15:56.720]  произведении. А мой вопрос к вам в каком? Не, не, не. В каком скалярном произведении вот этот
[15:56.720 --> 16:09.640]  линейный оператор умножение на матрицу П будет эрмитовым. Ну если это выполняется,
[16:09.640 --> 16:17.280]  если линейный оператор, значит вот это и есть некий линейный оператор. Значит это и сопряжённый
[16:17.280 --> 16:28.080]  линейный оператор. Относительно вот этого скалярного произведения. Вот, если звездой совпадается,
[16:28.080 --> 16:34.560]  то есть на самом деле тот же самый оператор, то тогда такой оператор называется сам сопряжённый.
[16:34.560 --> 16:48.120]  То есть теперь А это линейный оператор. Ну можно думать, что это оператор умножения на
[16:48.120 --> 16:56.240]  матрицу А. Ну а со звездой здесь не будет сопряжённой матрицы. Это некоторый линейный оператор.
[16:56.240 --> 17:07.400]  Вот. Распланировали и брали комплексное сопряжение. Но скалярное произведение было,
[17:07.400 --> 17:13.840]  оно было естественное скалярное произведение. Оно было. Ну вот здесь вот можно на более
[17:13.840 --> 17:20.400]  общем уровне эти понятия ввести. И это помогает. Потому что вот задачку вы всё-таки решите. Вот вы
[17:20.400 --> 17:26.680]  можете в качестве предуславления брать любую эрмитовую положительную определенную матрицу. И при
[17:26.680 --> 17:32.880]  этом имеется скалярное произведение, в котором умножение на матрицу П будет самосопряжённым
[17:32.880 --> 17:37.720]  положительно определенным оператором. В некотором скалярном произведении. А вот в каком? Вот это
[17:37.720 --> 17:44.400]  мой вопрос к вам. Ну а мы давайте продолжим дальше. Продолжим дальше. Значит, мое намерение сегодня
[17:44.400 --> 17:53.960]  рассмотреть методы для другой задачи. Тоже весьма важной. Почисление собственных значений,
[17:53.960 --> 18:03.560]  соответственно, второго. Есть такая задачка? Нет. Значит, попрошу с доски встретить.
[18:03.560 --> 18:17.680]  Некоторые классические методы для вот этой задачи. Спектральные. Такие задачи нужны спектральными.
[18:17.680 --> 18:30.960]  Здесь много постановок. Понимаете, если матрица астрономических размеров, то, конечно, а зачем
[18:30.960 --> 18:41.160]  все собственные значения? То их много. Обычно нужна какая-то информация собственных значений. Ну,
[18:41.160 --> 18:48.920]  например, какая-то локализация. Либо самое большое по модулю, или просто самое большое
[18:48.920 --> 18:57.240]  собственное значение. Либо самое маленькое, либо ближайшее к заданному числу. Много постановок.
[18:57.240 --> 19:04.160]  Ну, конечно, для матриц умеренных размеров и постановка такая обычная. Найти все собственные
[19:04.160 --> 19:12.560]  значения тоже имеют быть право рассмотрено. Ну, и собственные вектораны можно интересоваться.
[19:12.560 --> 19:25.480]  Значит, с чего начать? Вот есть одна идея. На самом деле, чрезвычайно такая распространенная.
[19:25.480 --> 19:32.360]  Ее следы можно, если присмотреться внимательно, можно разглядеть в самых изощренных алгоритмах.
[19:32.360 --> 19:42.520]  Вот что-то удивительно сложное. Смотрите на это слово и видите. А там, оказывается, вот эта простенькая идея обеспечивает.
[19:42.520 --> 19:54.960]  Значит, давайте рассмотрим матрицу А. Пусть даже произвольную, но предположим, что ее собственные
[19:54.960 --> 20:04.840]  значения такие, что лямбда 1 самая большая по модулю и оно отделено от остальных.
[20:08.840 --> 20:15.960]  Ну, бывает так. И нам бы хотелось найти лямбда 1.
[20:15.960 --> 20:32.040]  Ну, вот предложение. Давайте еще одно. Предположим, что все собственные значения просты.
[20:32.040 --> 20:38.520]  Ну, предположим, что есть базис собственных значений, извините, собственных векторов.
[20:39.080 --> 20:47.600]  Ну, это, конечно, ограничение, но с вероятностью единица комплексная матрица обладает.
[20:47.600 --> 20:58.200]  Это правда не помогает в жизни. Это замечательно.
[20:58.880 --> 21:15.880]  Ну, теоретически, значит, А запитая, лямбда запитая, она запитая.
[21:16.560 --> 21:24.440]  Возьмем некий вектор Х, произвольный, и разложим его по базису.
[21:24.440 --> 21:43.080]  Ну, плюс, я специально выделю вот этот самый первый вектор и его координаты.
[21:43.280 --> 21:54.280]  Значит, Х можно взять любой, но, тем не менее, пусть координаты по первому собственного вектора не нулевая.
[21:54.280 --> 21:57.280]  С вероятностью единицы тоже нет.
[22:00.280 --> 22:08.280]  Давайте поддействуем на вектор Х к этой степени матрицы А.
[22:08.480 --> 22:28.480]  Значит, что будет? Будет лямбда 1, степень К, а вот то, что здесь я напишу, я напишу так, это у маленькая лямбда 1, степень К.
[22:28.680 --> 22:39.680]  Стоит некий вектор, но его элементы это у малая от лямбда 1, степень К.
[22:39.680 --> 22:47.680]  Ну, ровно по вот этой причине всего отделенности, остальные что-то значение от самого большого.
[22:47.880 --> 22:58.880]  Ну, а что такое? Вот такое шкалярное произведение.
[22:58.880 --> 23:13.880]  Это лямбда 1, степень К, на К1, на Z1, на Л, плюс у малой от лямбда 1, степень К.
[23:13.880 --> 23:39.080]  Ну, а теперь что? А теперь давайте вот такое отношение рассмотрим.
[23:44.080 --> 23:58.080]  Ну, понятно, что... Ну, я приглашаю вас замечать, что это с лямбда 1, не с у малой от лямбда 1.
[24:01.080 --> 24:04.080]  И сразу возникает идея метода, да?
[24:04.080 --> 24:12.080]  Ну, умножайте, выберите вектор Х, умножайте его до матрицы А в степень, и вот последните на новый маттер.
[24:12.280 --> 24:16.280]  И вот частные такие вычисляющиеся в пределе получатся с лямбда 1.
[24:18.280 --> 24:24.280]  Даже скорость сходимости понятна какая, она поделится отношением 2 к лямбда 1.
[24:24.280 --> 24:28.280]  Маленькая, вот быстро будет сходиться.
[24:28.280 --> 24:35.280]  Вот такой метод простой, если угодно, ну, степенной метод, да, этот степенной метод.
[24:35.280 --> 24:41.280]  Но, кстати, при его реализации, вот если вы прям так будете реализовывать, ну, так не получится,
[24:41.480 --> 24:47.480]  вы не можете умножать на степенька, тут у вас может быть рост, быть цисел, да?
[24:47.480 --> 24:49.480]  То есть надо как-то нормировать.
[24:49.480 --> 24:58.480]  Ну, ясно, что можно... Нужно нормировать на каждой, на каждой итерации, вот, результат.
[24:58.480 --> 25:06.480]  Но, тем не менее, вот довольно естественный простой метод, который можно применять,
[25:06.680 --> 25:12.680]  можно придумывать обобщение, когда группа собственных значений возникает,
[25:12.680 --> 25:17.680]  можно превратить вот эту идею в метод итерации подпроскратства,
[25:17.680 --> 25:21.680]  тогда вы выбираете некую подпроскратство, нажать на матрицу А, подпросованную А,
[25:21.680 --> 25:26.680]  и вот на этом подпроскратстве вы строите маленькую матрицу,
[25:26.680 --> 25:30.680]  выходит ее собственные значения, вы берете их как приближение к собственному значению,
[25:30.680 --> 25:32.680]  исходной большой матрице.
[25:36.680 --> 25:40.680]  Важно, чтобы к нулю уходило.
[25:43.680 --> 25:45.680]  Где?
[25:49.680 --> 25:55.680]  К минус один. К минус один. Важно, чтобы прямо до один осталось, и остальное уходило.
[25:55.680 --> 25:58.680]  Так и будет. Идея эта простая.
[25:58.680 --> 26:04.680]  Вот эта идея возведения в степень так, чтобы на фоне чего-то все остальное уходило к нулю.
[26:06.680 --> 26:08.680]  Да.
[26:12.680 --> 26:14.680]  Нулямда один констант.
[26:20.680 --> 26:26.680]  Ну вот теперь такое нетривиальное развитие этой идеи.
[26:26.680 --> 26:30.680]  Вот авторы вот этого нетривиального развития даже категорически не согласны
[26:30.680 --> 26:33.680]  с тем, что это есть нетривиальное развитие простой идеи.
[26:33.880 --> 26:38.880]  Они склонны были утверждать, что это у них принципиально другой метод.
[26:38.880 --> 26:40.880]  Это, конечно, правда.
[26:40.880 --> 26:44.880]  Но как за сложными конструкциями иногда можно разглядеть следы
[26:44.880 --> 26:47.880]  уже известной простой идеи.
[26:47.880 --> 26:51.880]  Но это никак не умоляет значение более сложной конструкции.
[26:51.880 --> 26:55.880]  И она действительно необходима более сложной конструкции,
[26:55.880 --> 27:00.880]  потому что эта простая идея в таком виде, именно это ограничено.
[27:01.080 --> 27:06.080]  Я хочу познакомить вас с одним из совершенно знаменитых
[27:06.080 --> 27:11.080]  знаменитых классических методов вычислительного изменения.
[27:14.080 --> 27:19.080]  В речь пойдет о QR-алгоритме.
[27:24.080 --> 27:28.080]  Значит, этот алгоритм предназначен для вычисления
[27:28.280 --> 27:32.280]  собственных значений и собственных векторов произвольной комплекции
[27:32.280 --> 27:35.280]  матрицы умеренного размера.
[27:35.280 --> 27:38.280]  Не слишком, но не астрономически.
[27:38.280 --> 27:41.280]  Так, чтобы в память она помещалась.
[27:41.280 --> 27:44.280]  Потому что она будет преобразовываться.
[27:51.280 --> 27:54.280]  Сегодня ни одной девочки нет.
[27:54.480 --> 27:57.480]  Почему я об этом подумал?
[27:57.480 --> 28:00.480]  Можно было бы не спрашивать, почему я об этом подумал.
[28:00.480 --> 28:03.480]  Но в связи с QR-алгоритмом есть причина.
[28:03.480 --> 28:07.480]  Значит, один из авторов вот этого знаменитого метода
[28:07.480 --> 28:10.480]  это Вера Николаевна Кувлановская.
[28:10.480 --> 28:13.480]  К сожалению, ушедший уже.
[28:15.480 --> 28:18.480]  Ленинградский математик, вычислитель.
[28:18.480 --> 28:21.480]  Вот она.
[28:21.680 --> 28:24.680]  Во всем мире известна была.
[28:24.680 --> 28:27.680]  Во всем мире.
[28:27.680 --> 28:30.680]  Значит, как один из авторов.
[28:30.680 --> 28:33.680]  Ну, она и другими вещами занималась.
[28:33.680 --> 28:36.680]  Как один из авторов QR-алгоритма,
[28:36.680 --> 28:39.680]  о котором я сейчас вам хочу рассказать.
[28:39.680 --> 28:44.680]  Ну, были и западные коллеги независимо действующие.
[28:44.680 --> 28:49.680]  Но, тем не менее, ее вклад здесь признан.
[28:49.880 --> 28:52.880]  А что представляет QR-алгоритм?
[28:52.880 --> 28:55.880]  Ну, цель уже обозначена.
[28:55.880 --> 28:58.880]  Что он должен делать?
[28:58.880 --> 29:01.880]  Находить собственные значения и собственные векторы.
[29:08.880 --> 29:11.880]  Вот берется исходная матрица.
[29:11.880 --> 29:14.880]  Позже будет она.
[29:15.080 --> 29:18.080]  Ну, то, что я сделаю на первом шаге,
[29:18.080 --> 29:21.080]  будет выполняться потом и на втором, и на третьем.
[29:21.080 --> 29:24.080]  Совершенно аналогичным образом.
[29:24.080 --> 29:27.080]  И до матрицы А0 находится QR-разложение.
[29:31.080 --> 29:34.080]  То есть, матрица Q1 унитарная,
[29:34.080 --> 29:37.080]  а матрица R1 верхняя треугольная.
[29:37.080 --> 29:40.080]  То есть, до любой комплексной матрицы
[29:40.280 --> 29:43.280]  можно найти QR-разложение.
[29:43.280 --> 29:46.280]  То есть, представьте, в виде произведения
[29:46.280 --> 29:49.280]  витарной матрицы и верхней треугольной.
[29:49.280 --> 29:52.280]  Каким образом это сделать?
[29:52.280 --> 29:55.280]  Если хотите, можно провести организацию станций
[29:55.280 --> 29:58.280]  методом грамошмита.
[29:58.280 --> 30:01.280]  Но можно использовать вращение или отражение
[30:01.280 --> 30:04.280]  на полуципе прообразований
[30:04.280 --> 30:07.280]  и с их помощью преобразовать матрицу
[30:07.280 --> 30:10.280]  к верхней треугольной.
[30:10.280 --> 30:13.280]  Ну, так или иначе.
[30:13.280 --> 30:16.280]  После того, как найдено QR-разложение,
[30:16.280 --> 30:19.280]  вот эти множители ставятся в обратном порядке.
[30:21.280 --> 30:24.280]  И это будет новая матрица A1.
[30:24.280 --> 30:27.280]  Вот основной шаг QR-разложения.
[30:29.280 --> 30:32.280]  Ну, давайте напишу еще второй шаг.
[30:38.280 --> 30:41.280]  Какие-то чудеса, да?
[30:41.280 --> 30:44.280]  И так далее.
[30:44.280 --> 30:47.280]  Зачем делать такие преобразования?
[30:47.280 --> 30:50.280]  И какое отношение эти преобразования
[30:50.280 --> 30:53.280]  имеют к поиску собственных значений матрицы A?
[30:54.280 --> 30:57.280]  Имеют, на самом деле.
[30:57.280 --> 31:00.280]  Давайте на первый шаг посмотрим.
[31:02.280 --> 31:05.280]  Значит, A1
[31:07.280 --> 31:10.280]  это R1
[31:10.280 --> 31:13.280]  на Q1.
[31:13.280 --> 31:16.280]  Но я же могу вот так написать Q1,
[31:16.280 --> 31:19.280]  а здесь поставить Q1.
[31:19.280 --> 31:22.280]  Вот, скобочки здесь поставлю.
[31:22.280 --> 31:25.280]  И в скобочках появилась матрица 0.
[31:28.280 --> 31:31.280]  Правильно?
[31:31.280 --> 31:34.280]  Ну, дальше мы продолжим аналогично.
[31:34.280 --> 31:37.280]  Другая интересная формула.
[31:37.280 --> 31:40.280]  Катой матрица
[31:40.280 --> 31:43.280]  имеет вид такой.
[31:53.280 --> 31:56.280]  Ну, этот матрицы обозначен через Z-кат.
[31:59.280 --> 32:02.280]  З-кат и сопряженная,
[32:02.280 --> 32:05.280]  Z-ката.
[32:08.280 --> 32:11.280]  Ну, подождите.
[32:11.280 --> 32:14.280]  Значит, что понятно, что матрица Z-ката
[32:14.280 --> 32:17.280]  есть произведение унитарных матриц,
[32:17.280 --> 32:20.280]  поэтому сама будет унитарная.
[32:20.280 --> 32:23.280]  Значит, звездочка то же самое,
[32:23.280 --> 32:26.280]  что для унитарной матрицы сопряжение
[32:26.280 --> 32:29.280]  равносильно обращение.
[32:29.280 --> 32:32.280]  Вот эти матрицы Акката,
[32:32.280 --> 32:35.280]  которые мы будем производить в ходе коэр-алгоритма,
[32:35.280 --> 32:38.280]  они все подобны исходной матрице.
[32:38.280 --> 32:41.280]  Более того, они унитарно подобны.
[32:41.280 --> 32:44.280]  Это значит, никакой неприятности связанной
[32:44.280 --> 32:47.280]  с возможным ростом элементов нет.
[32:47.280 --> 32:50.280]  Это будет неплохо вычислять.
[32:50.280 --> 32:53.280]  Хороший вопрос, зачем мы это будем вычислять?
[32:53.280 --> 32:56.280]  Вычислять-то можно.
[32:56.280 --> 32:59.280]  Ну, наблюдение такое. Мы интересуемся собственными значениями.
[32:59.280 --> 33:02.280]  Значит, накатом шаги мы получаем матрицу Акката,
[33:02.280 --> 33:05.280]  а все остальное можем забыть,
[33:05.280 --> 33:08.280]  потому что матрица Акката имеет те же собственные значения,
[33:08.280 --> 33:11.280]  что и исходная матрица.
[33:11.280 --> 33:14.280]  Ну, если мы для Акката и найдем собственные векторы,
[33:14.280 --> 33:17.280]  то можно с помощью умножения на матрицу З-ката пересчитать
[33:17.280 --> 33:20.280]  и получить собственные векторы исходной.
[33:20.280 --> 33:23.280]  То есть мы на каждом шаге в коэр-алгоритм
[33:23.280 --> 33:26.280]  делаем аналогичные задачи с матрицей Акката.
[33:26.280 --> 33:29.280]  Но редукция задачи имеет смысл
[33:29.280 --> 33:32.280]  только в тех случаях, когда мы думаем,
[33:32.280 --> 33:35.280]  что новые задачи будут проще решаться.
[33:35.280 --> 33:38.280]  Ну а для какой матрицы
[33:38.280 --> 33:41.280]  собственные значения очень легко найти?
[33:41.280 --> 33:44.280]  Ну, это уж мечта, конечно.
[33:44.280 --> 33:47.280]  Если еще пошире классы.
[33:47.280 --> 33:50.280]  Для треугольной.
[33:51.280 --> 33:54.280]  Что там?
[33:54.280 --> 33:57.280]  Вне главной диагонали все равно
[33:57.280 --> 34:00.280]  на главной диагонали в треугольной матрице будет собственное значение.
[34:00.280 --> 34:03.280]  Ну, у нас есть теория возмущений.
[34:03.280 --> 34:06.280]  То есть можно так сказать, если матрица
[34:06.280 --> 34:09.280]  слабо отличается от верхней треугольной,
[34:09.280 --> 34:12.280]  то на главной диагонали стоят приближения их собственных значений.
[34:12.280 --> 34:15.280]  Таким образом,
[34:15.280 --> 34:18.280]  чтобы объяснить полезность
[34:18.280 --> 34:21.280]  вот этих итераций,
[34:21.280 --> 34:24.280]  нам надо понять, почему
[34:24.280 --> 34:27.280]  вот эти матрицы Аккаты
[34:27.280 --> 34:30.280]  по форме стремятся к верхней треугольной.
[34:30.280 --> 34:33.280]  Ну, в данном случае к верхней треугольной.
[34:33.280 --> 34:36.280]  Вот эти матрицы Аккаты
[34:36.280 --> 34:39.280]  по форме стремятся к верхней треугольной.
[34:40.280 --> 34:43.280]  Вот теория будет ровно такой.
[34:43.280 --> 34:46.280]  Матрица Аккаты по форме стремятся к верхней треугольной.
[34:46.280 --> 34:49.280]  Ну, что значит по форме стремятся?
[34:49.280 --> 34:52.280]  Значит, все поддиагональные элементы стремятся к нулю.
[34:52.280 --> 34:55.280]  Значит, вот
[34:55.280 --> 34:58.280]  еще раз. Матрица Аккаты
[34:58.280 --> 35:01.280]  все поддиагональные элементы стремятся к нулю.
[35:01.280 --> 35:04.280]  А значит, на главной диагонали мы
[35:04.280 --> 35:07.280]  с вами будем получать приближение их собственных значений.
[35:07.280 --> 35:10.280]  Остановиться надо тогда,
[35:10.280 --> 35:13.280]  когда вот эти поддиагональные элементы стремятся к нулю.
[35:15.280 --> 35:18.280]  Ну, об основании тем не менее
[35:18.280 --> 35:21.280]  сказано, но надо и сделать это.
[35:21.280 --> 35:24.280]  Как-то, так сказать, увидеть, почему то, что я сказал верно.
[35:24.280 --> 35:27.280]  Давайте попробуем.
[35:27.280 --> 35:30.280]  Ну,
[35:30.280 --> 35:33.280]  но
[35:33.280 --> 35:36.280]  некий есть простой.
[35:36.280 --> 35:39.280]  Ну, вообще-то говоря, когда это все создавалось
[35:39.280 --> 35:42.280]  многостраничной даже работы писали, чтобы
[35:42.280 --> 35:45.280]  объяснить то, о чем я сейчас говорю,
[35:45.280 --> 35:48.280]  не сразу же приходит простой способ
[35:48.280 --> 35:51.280]  обсуждения.
[35:51.280 --> 35:54.280]  Но вот достаточно простой способ обсуждения придумал
[35:54.280 --> 35:57.280]  классик в числительной линейной алгебе.
[35:57.280 --> 36:00.280]  Это Уилкинсон.
[36:00.280 --> 36:03.280]  Значит, у него есть замечательный труд
[36:03.280 --> 36:06.280]  под названием «Алгебраическая проблема сотни значений».
[36:06.280 --> 36:09.280]  Вот как, может быть, Гантмахер и так.
[36:09.280 --> 36:12.280]  Вот такого же формата.
[36:12.280 --> 36:15.280]  Это вот была долгое время.
[36:15.280 --> 36:18.280]  Да, может быть, остается Библии классической
[36:18.280 --> 36:21.280]  в числительной линейной алгебе.
[36:21.280 --> 36:24.280]  Посвящено, кстати, анализу численно-устойчивости
[36:24.280 --> 36:27.280]  алгоритма того,
[36:27.280 --> 36:30.280]  как влияют ошибки округления.
[36:30.280 --> 36:33.280]  Ну, у нас совершенно фундаментальная книга.
[36:33.280 --> 36:36.280]  Но вот он
[36:36.280 --> 36:39.280]  предложил
[36:39.280 --> 36:42.280]  некий простой набор условий,
[36:42.280 --> 36:45.280]  ограничений, при которых то, что я сказал
[36:45.280 --> 36:48.280]  верно. То есть никто не говорит, что это в общем
[36:48.280 --> 36:51.280]  случае верно.
[36:51.280 --> 36:54.280]  Значит, теорема будет вот такая.
[37:05.280 --> 37:08.280]  Значит, та
[37:08.280 --> 37:11.280]  диагонализована.
[37:11.280 --> 37:14.280]  Это уже ограничение.
[37:14.280 --> 37:17.280]  Уже ограничение.
[37:17.280 --> 37:20.280]  Значит, лямбда это матрица
[37:20.280 --> 37:23.280]  сотни значений.
[37:23.280 --> 37:26.280]  И они считаются
[37:26.280 --> 37:29.280]  попарно разными.
[37:29.280 --> 37:32.280]  Ну, давайте я это допишу
[37:32.280 --> 37:35.280]  на отдельной строчке.
[37:42.280 --> 37:45.280]  Ну, с лямбда
[37:45.280 --> 37:48.280]  один самый большой.
[37:57.280 --> 38:00.280]  Значит, это уже целых два предположения.
[38:00.280 --> 38:03.280]  Целых два предположения.
[38:03.280 --> 38:06.280]  Да я еще давайте предположу, что нулевого нет.
[38:15.280 --> 38:18.280]  То есть матрица комплексная,
[38:18.280 --> 38:21.280]  диагонализуемая. Все собственные значения
[38:21.280 --> 38:24.280]  попарно различны.
[38:24.280 --> 38:27.280]  Значит, можно их вот так и понять.
[38:27.280 --> 38:30.280]  Вот третье предположение
[38:30.280 --> 38:33.280]  кажется фантастическим.
[38:33.280 --> 38:36.280]  Кстати, х
[38:36.280 --> 38:39.280]  это матрица из собственных векторов.
[38:39.280 --> 38:42.280]  Так вот обратная
[38:42.280 --> 38:45.280]  к матрице из собственных векторов
[38:45.280 --> 38:48.280]  имеет Элю разложение.
[38:48.280 --> 38:51.280]  Это является строго регулярной.
[38:51.280 --> 38:54.280]  Эль здесь унитриугольная матрица, а уни выражена
[38:54.280 --> 38:57.280]  вверх.
[38:57.280 --> 39:00.280]  Ну, унитриугольная
[39:00.280 --> 39:03.280]  треугольная с единичками
[39:03.280 --> 39:06.280]  на главной диагональ.
[39:06.280 --> 39:09.280]  Значит, Эль нижняя унитриугольная
[39:09.280 --> 39:12.280]  с единичками.
[39:12.280 --> 39:15.280]  Ну, мы обсуждали с вами строго регулярно
[39:15.280 --> 39:18.280]  необходимые достаточные, что Элю разложена.
[39:18.280 --> 39:21.280]  Вот все три условия.
[39:21.280 --> 39:24.280]  И вот если эти три условия выполнены,
[39:24.280 --> 39:27.280]  то
[39:27.280 --> 39:30.280]  в матрице Аккаде
[39:30.280 --> 39:33.280]  любой элемент, для которого И
[39:33.280 --> 39:36.280]  больше, чем Ж, то есть это подглавный
[39:37.280 --> 39:40.280]  вот он
[39:40.280 --> 39:43.280]  соединится к нулю, при карте
[39:43.280 --> 39:46.280]  и начнется вот так.
[39:46.280 --> 39:49.280]  И это мы попробуем доказать.
[39:49.280 --> 39:52.280]  Это будет обоснование.
[39:52.280 --> 39:55.280]  Такой раз горит, правда же?
[39:55.280 --> 39:58.280]  Да?
[40:01.280 --> 40:04.280]  Так.
[40:04.280 --> 40:07.280]  Ну давайте, попробуем.
[40:14.280 --> 40:17.280]  Ну давайте, у меня тут есть тоже вклад
[40:17.280 --> 40:20.280]  в теорию куэрелгоритма.
[40:20.280 --> 40:23.280]  К этому статью написал,
[40:23.280 --> 40:26.280]  дал ей такой заголовок, который потом
[40:26.280 --> 40:29.280]  по совету рецензента
[40:29.280 --> 40:32.280]  изменил. Значит, статью я так
[40:32.280 --> 40:35.280]  дал. Одно бесполезное замечание куэрелгоритма.
[40:41.280 --> 40:44.280]  Бесполезное, я имел в виду, что оно практически
[40:44.280 --> 40:47.280]  не имеет никакого значения.
[40:47.280 --> 40:50.280]  А пафос того, о чем я тогда
[40:50.280 --> 40:53.280]  подумал, заключается в этом третьем
[40:53.280 --> 40:56.280]  условии.
[40:57.280 --> 41:00.280]  А что если обратная к матрице
[41:00.280 --> 41:03.280]  собственных вторых не является
[41:03.280 --> 41:06.280]  строго регулярной? Тогда иллюразложений нет.
[41:08.280 --> 41:11.280]  Ну я, значит, что сделал?
[41:11.280 --> 41:14.280]  Вот была одна из задачек, которые я вам
[41:14.280 --> 41:17.280]  предлагал. Ну какая бы не была
[41:17.280 --> 41:20.280]  невыраженная матрица, мы же можем
[41:20.280 --> 41:23.280]  лпо-разложение сделать, где p будет
[41:23.280 --> 41:26.280]  определенно однозначно.
[41:26.280 --> 41:29.280]  Кто-то из вас решал эту задачу.
[41:29.280 --> 41:32.280]  Я в этой заметке в статье
[41:32.280 --> 41:35.280]  доказал утверждение о том, что
[41:38.280 --> 41:41.280]  вот в предельной матрице
[41:41.280 --> 41:44.280]  собственные значения на главной
[41:44.280 --> 41:47.280]  диагонали будут расположены,
[41:47.280 --> 41:50.280]  их порядок будет определяться
[41:50.280 --> 41:53.280]  при установке p.
[41:59.280 --> 42:02.280]  То есть все то же самое, но будет
[42:02.280 --> 42:05.280]  в нем порядок.
[42:05.280 --> 42:08.280]  Ну ладно, эта лирика совершенно
[42:08.280 --> 42:11.280]  бесполезна, потому что с вероятностью
[42:11.280 --> 42:14.280]  единица условия строго регулярности
[42:14.280 --> 42:17.280]  выполнена. То есть при этом вы даже
[42:17.280 --> 42:20.280]  численно не проверите вот это мое
[42:20.280 --> 42:23.280]  утверждение, потому что будут
[42:23.280 --> 42:26.280]  возникать ошибки округления и все.
[42:26.280 --> 42:29.280]  Ну давайте попробуем
[42:29.280 --> 42:32.280]  посмотреть.
[42:32.280 --> 42:35.280]  Ну и может быть разглядеть
[42:35.280 --> 42:38.280]  предыдущие много методов.
[42:45.280 --> 42:48.280]  Так.
[42:48.280 --> 42:51.280]  Ну у нас есть
[42:51.280 --> 42:54.280]  замечательная форма для матрицы
[42:54.280 --> 42:57.280]  окатая.
[42:57.280 --> 43:00.280]  Вот если мы сразу вот о следах
[43:00.280 --> 43:03.280]  степенного метода.
[43:03.280 --> 43:06.280]  Возведем нашу исходную матрицу
[43:06.280 --> 43:09.280]  в степень k.
[43:13.280 --> 43:16.280]  Ну это есть
[43:16.280 --> 43:19.280]  q1 на r1.
[43:24.280 --> 43:27.280]  q1 на r1 кора.
[43:30.280 --> 43:33.280]  Или
[43:33.280 --> 43:36.280]  это есть q1
[43:36.280 --> 43:39.280]  на а1 в степени k-1
[43:39.280 --> 43:42.280]  на r1.
[43:54.280 --> 43:57.280]  Ну отсюда
[43:57.280 --> 44:00.280]  потом мы для а1 в степень k
[44:00.280 --> 44:03.280]  методам то самое повторим.
[44:03.280 --> 44:06.280]  Значит возникает формула такая для матрицы
[44:06.280 --> 44:09.280]  от степени k.
[44:13.280 --> 44:16.280]  Вот такая q1
[44:16.280 --> 44:19.280]  у катая, а здесь r1
[44:19.280 --> 44:22.280]  и rk.
[44:22.280 --> 44:25.280]  Ну вот эта матрица уже имела обозначение
[44:25.280 --> 44:28.280]  z-катая, правильно?
[44:28.280 --> 44:31.280]  А эту матрицу я обозначу через u-катая.
[44:33.280 --> 44:36.280]  z-катая, как мы уже говорили,
[44:36.280 --> 44:39.280]  это матрица унитарная,
[44:39.280 --> 44:42.280]  поскольку является произведением унитарных матриц.
[44:42.280 --> 44:45.280]  А вот эта матрица u-катая
[44:45.280 --> 44:48.280]  наверхне треугольная. Почему?
[44:48.280 --> 44:51.280]  Потому что она есть произведение
[44:51.280 --> 44:54.280]  в треугольных матрицах. Что?
[44:54.280 --> 44:57.280]  А в степени k.
[44:57.280 --> 45:00.280]  То есть это есть
[45:00.280 --> 45:03.280]  унитарная матрица z-катая
[45:03.280 --> 45:06.280]  наверхне треугольная матрица u-катая.
[45:06.280 --> 45:09.280]  Ну вот они следы степенного метода
[45:09.280 --> 45:12.280]  в этой формуле.
[45:12.280 --> 45:15.280]  То есть матрица в степени k
[45:15.280 --> 45:18.280]  возникает
[45:18.280 --> 45:21.280]  в другом порядке.
[45:32.280 --> 45:35.280]  Ну как, взяли вот эту форму.
[45:35.280 --> 45:38.280]  Дальше, а1 в степени k-1
[45:38.280 --> 45:41.280]  написали аналогичную.
[45:41.280 --> 45:44.280]  Возникло здесь q2 и r2.
[45:44.280 --> 45:47.280]  В другом порядке, правильно?
[45:47.280 --> 45:50.280]  Ну в другом порядке.
[45:50.280 --> 45:53.280]  Ошибка все-таки была.
[45:53.280 --> 45:56.280]  Ошибка была.
[46:10.280 --> 46:13.280]  Для последней будет q2.
[46:17.280 --> 46:20.280]  Ну, значит
[46:20.280 --> 46:23.280]  Ну, заниматься надо матрице-катой.
[46:23.280 --> 46:26.280]  Она нас волнует.
[46:26.280 --> 46:29.280]  Вот матрица-катая.
[46:29.280 --> 46:32.280]  Это есть
[46:32.280 --> 46:35.280]  z-катая с звездой.
[46:35.280 --> 46:38.280]  Заткатая с звездой.
[46:38.280 --> 46:41.280]  Заткатая с звездой.
[46:41.280 --> 46:44.280]  Заткатая с звездой.
[46:44.280 --> 46:47.280]  Заткатая с звездой.
[46:47.280 --> 46:50.280]  На а.
[46:50.280 --> 46:53.280]  На заткатая.
[46:53.280 --> 46:56.280]  Так, а для матрицы А есть у нас
[46:56.280 --> 46:59.280]  вот такая вот форма.
[47:03.280 --> 47:06.280]  Значит, это есть
[47:06.280 --> 47:09.280]  z-катая с звездой.
[47:09.280 --> 47:12.280]  x
[47:12.280 --> 47:15.280]  х
[47:15.280 --> 47:18.280]  х
[47:18.280 --> 47:21.280]  х
[47:21.280 --> 47:24.280]  х
[47:24.280 --> 47:27.280]  х
[47:27.280 --> 47:30.280]  х
[47:30.280 --> 47:33.280]  х
[47:33.280 --> 47:36.280]  х
[47:36.280 --> 47:39.280]  Лямба это матрица диагональная.
[47:39.280 --> 47:42.280]  Но, значит,
[47:42.280 --> 47:45.280]  здесь x-1 на заткатая,
[47:45.280 --> 47:48.280]  здесь обратная к ней.
[47:48.280 --> 47:51.280]  Здесь обратная к ней.
[47:51.280 --> 47:54.280]  Значит, вот эта матрица нас волнует.
[47:54.280 --> 47:57.280]  Если верно то, что нам надо доказать,
[47:57.280 --> 48:00.280]  то вот эта матрица,
[48:00.280 --> 48:03.280]  x-1 на заткатая,
[48:03.280 --> 48:06.280]  она стремится по форме
[48:06.280 --> 48:09.280]  при сделанных нам предположениях.
[48:09.280 --> 48:12.280]  При сделанных нам предположениях.
[48:12.280 --> 48:15.280]  Вот надо только
[48:15.280 --> 48:18.280]  сообразить, почему.
[48:18.280 --> 48:21.280]  Ну, давайте вот этой матрице будем заниматься.
[48:21.280 --> 48:24.280]  x-1
[48:24.280 --> 48:27.280]  на заткатая.
[48:27.280 --> 48:30.280]  Давайте заметим, что
[48:30.280 --> 48:33.280]  когда заткаты унитарные матрицы,
[48:33.280 --> 48:36.280]  поэтому вот вся эта пассета
[48:36.280 --> 48:39.280]  равномерно ограничена.
[48:39.280 --> 48:42.280]  Поскольку заткаты унитарные.
[48:42.280 --> 48:45.280]  Согласны, да?
[48:45.280 --> 48:48.280]  То есть все элементы этих матриц
[48:48.280 --> 48:51.280]  ограничены по модуле с таким числём.
[48:51.280 --> 48:54.280]  Большой, но ограничен.
[48:54.280 --> 48:57.280]  Так, теперь
[48:57.280 --> 49:00.280]  теперь
[49:00.280 --> 49:03.280]  давайте
[49:03.280 --> 49:06.280]  ...
[49:06.280 --> 49:09.280]  ...
[49:09.280 --> 49:12.280]  ...
[49:12.280 --> 49:15.280]  ...
[49:15.280 --> 49:18.280]  ...
[49:18.280 --> 49:21.280]  ...
[49:21.280 --> 49:24.280]  ...
[49:24.280 --> 49:27.280]  ...
[49:28.280 --> 49:31.280]  ...
[49:31.280 --> 49:34.280]  ...
[49:34.280 --> 49:37.280]  ...
[49:37.280 --> 49:40.280]  ...
[49:40.280 --> 49:43.280]  ...
[49:43.280 --> 49:46.280]  ...
[49:46.280 --> 49:49.280]  ...
[49:49.280 --> 49:52.280]  ...
[49:52.280 --> 49:55.280]  ...
[49:55.280 --> 50:10.280]  Это х-1 азоткатая. Куда смотреть то надо. А вот сюда. Азоткатая это есть афстепелика на укатой министерии.
[50:10.280 --> 50:34.280]  Нигде не обманулся. Смотрите внимательно. Иногда не хочешь обманешь.
[50:34.280 --> 50:46.280]  А вот теперь для катая воспользуемся вот матрица. Какой вид имеет афстепеликатая? Это х на лямбда степелика.
[50:46.280 --> 51:07.280]  Вот так и напишем. Х на лямбда степелика на х минус первой укатая.
[51:07.280 --> 51:26.280]  А вот здесь х-1 на х тоже сократится.
[51:26.280 --> 51:50.280]  А вот теперь смело подставим. Вот теперь на иллю мы сюда поставим.
[51:50.280 --> 51:59.280]  Ну неплохо. Тут уже следы верхних треугольных матриц имеются.
[51:59.280 --> 52:17.280]  Вот здесь л нижняя треугольная. Но она умножается на верхнюю треугольную. Правда, если мы нижнюю треугольную умножим на верхнюю треугольную, результат будет...
[52:17.280 --> 52:40.280]  Ну и теперь небольшой трюк. Совсем маленький.
[52:40.280 --> 52:54.280]  Вот здесь мне понадобилось, чтобы нулевого значения не было.
[52:54.280 --> 53:09.280]  Ну раз я здесь лямбда степелика написал, я здесь должен написать лямбда степелика.
[53:09.280 --> 53:20.280]  Я обманул?
[53:20.280 --> 53:26.280]  Да вон она есть, стоит между лямбда степеляка и лямбда степеля минуска.
[53:26.280 --> 53:30.280]  Здесь ничего не исчезло, кое-что появилось дополнительно.
[53:30.280 --> 53:40.280]  Появилось, что вот это лямбда степеля минуска и лямбда степеляка, чтобы они сократились.
[53:40.280 --> 53:48.280]  Вот эта матрица вторая, она при каждом к верхняя треугольная.
[53:48.280 --> 53:54.280]  Ну первая матрица нижняя треугольная, по-прежнему все печально.
[53:54.280 --> 54:00.280]  Но печаль все уменьшается и уменьшается.
[54:00.280 --> 54:09.280]  Значит я утверждаю следующее.
[54:09.280 --> 54:19.280]  Вот эта вот матрица стремится к единичному.
[54:19.280 --> 54:22.280]  Просто стремится к единичному.
[54:22.280 --> 54:28.280]  Печаль исчезает.
[54:28.280 --> 54:31.280]  Почему да?
[54:31.280 --> 54:34.280]  Давайте посмотрим.
[54:34.280 --> 54:36.280]  Давайте я нарисую.
[54:36.280 --> 54:59.280]  Допустим третьего порядка.
[54:59.280 --> 55:08.280]  Ну только здесь.
[55:08.280 --> 55:12.280]  Давайте посмотрим, что получится в результате.
[55:12.280 --> 55:15.280]  Здесь единичка.
[55:15.280 --> 55:21.280]  Здесь лямбда два.
[55:21.280 --> 55:24.280]  Удалить на лямбда один.
[55:24.280 --> 55:28.280]  Это не к, а л два один.
[55:28.280 --> 55:30.280]  Здесь единичка.
[55:30.280 --> 55:32.280]  Верно?
[55:32.280 --> 55:39.280]  Здесь лямбда три.
[55:39.280 --> 55:40.280]  Нет.
[55:40.280 --> 55:44.280]  Лямбда.
[55:44.280 --> 55:47.280]  Лямбда три поделить на лямбда один.
[55:47.280 --> 55:50.280]  Это не к, а л три один.
[55:50.280 --> 55:53.280]  Здесь лямбда два.
[55:53.280 --> 55:55.280]  На лямбда один.
[55:55.280 --> 55:58.280]  Это не к, а л два один.
[55:58.280 --> 56:00.280]  Здесь единичка.
[56:00.280 --> 56:05.280]  Ну видно, что под дегональными элементами числа меньше единиц поможет.
[56:05.280 --> 56:07.280]  Это реальность.
[56:07.280 --> 56:08.280]  12 нулевых.
[56:08.280 --> 56:10.280]  Ну я и утверждаю.
[56:10.280 --> 56:13.280]  Ну а матрица не стремится к единице.
[56:13.280 --> 56:14.280]  Нет.
[56:14.280 --> 56:15.280]  Не верхняя треугольная.
[56:15.280 --> 56:17.280]  Матрица нижняя треугольная.
[56:17.280 --> 56:19.280]  На главный диагональ всегда единицы.
[56:19.280 --> 56:23.280]  А все поддегональные элементы не стремятся к ней.
[56:23.280 --> 56:29.280]  То есть верно, что эти матрицы сходятся к единичной матрице.
[56:29.280 --> 56:35.280]  Таким образом, вот эта матрица,
[56:35.280 --> 56:37.280]  х минус первый.
[56:38.280 --> 56:42.280]  Она имеет вот такой единичка.
[56:42.280 --> 56:46.280]  Плюс умалые от единицы.
[56:46.280 --> 56:49.280]  Да вот такую вот матрицу.
[56:56.280 --> 56:59.280]  Ну естественно можно переписать.
[56:59.280 --> 57:02.280]  Вот такая вот матрица.
[57:08.280 --> 57:11.280]  Ну и хотелось бы написать вот так.
[57:11.280 --> 57:13.280]  Да?
[57:13.280 --> 57:17.280]  Ну нужно понять, что нам дает право это сделать.
[57:17.280 --> 57:19.280]  А?
[57:19.280 --> 57:22.280]  Нет, пока не может.
[57:22.280 --> 57:25.280]  Значит вот вопрос к вам.
[57:25.280 --> 57:27.280]  Почему нет?
[57:27.280 --> 57:31.280]  Ну вопрос о том, вот здесь матрица,
[57:31.280 --> 57:34.280]  элементы которой сходятся к нулю,
[57:34.280 --> 57:37.280]  умножается на вот такую вот матрицу.
[57:37.280 --> 57:39.280]  Почему у нас так получится матрица,
[57:39.280 --> 57:42.280]  элементы которой сходятся к нулю?
[57:45.280 --> 57:47.280]  Правильно.
[57:47.280 --> 57:52.280]  Потому что вот эта матрица равномерно ограничена.
[57:52.280 --> 57:57.280]  Потому что вот эта матрица равномерно ограничена.
[57:57.280 --> 57:59.280]  А почему она равномерно ограничена?
[57:59.280 --> 58:01.280]  Давайте запишем.
[58:01.280 --> 58:03.280]  Сейчас я напишу.
[58:03.280 --> 58:07.280]  Нет, не потому что она имеет вид какой.
[58:07.280 --> 58:11.280]  Вот эта матрица, которая равномерно ограничена,
[58:11.280 --> 58:16.280]  умножить на обратную вот такой вот матрицу,
[58:16.280 --> 58:19.280]  для которой можно ряд неймана написать.
[58:19.280 --> 58:23.280]  Видно, что это будет равномерно ограничена.
[58:26.280 --> 58:29.280]  Потому что ЗК унитарная.
[58:29.280 --> 58:31.280]  ЗК унитарная.
[58:31.280 --> 58:34.280]  Ее элементы по модуле на происходе видны.
[58:36.280 --> 58:39.280]  А Х-минус фиксированная.
[58:41.280 --> 58:44.280]  Так мы же все доказали.
[58:44.280 --> 58:46.280]  Мы все доказали.
[58:46.280 --> 58:50.280]  Потому что, раз вот эта матрица стремится к верхней треугольне,
[58:50.280 --> 58:53.280]  обратная к ней тоже стремится к верхней треугольне.
[58:54.280 --> 58:58.280]  Здесь, вот смотрите, вот эта к верхней треугольне стремится,
[58:58.280 --> 59:03.280]  а эта обратная к этой матрице тоже стремится к верхней треугольной.
[59:03.280 --> 59:06.280]  «Теорема о исходимости QR-агритма»
[59:06.280 --> 59:12.280]  в предположениях у Ексензона эта goblet-коп door написана.
[59:12.280 --> 59:15.280]  Доказано.
[59:16.280 --> 59:23.180]  Ну и, конечно, с вероятностью единицы все эти предположения
[59:23.180 --> 59:24.180]  выполнены.
[59:24.180 --> 59:29.300]  На практике, если вы наугад берете матрицу, у меня все
[59:29.300 --> 59:30.300]  собственные значения разные.
[59:30.300 --> 59:38.300]  Ну потому что вы x можете выбрать всегда так, чтобы
[59:38.300 --> 59:43.300]  обеспечить нужный порядок.
[59:43.300 --> 59:56.300]  Несмотря на то, что теоретически это не ясное ограбление,
[59:56.300 --> 01:00:02.300]  но практически, конечно же, если нарушены эти условия,
[01:00:02.300 --> 01:00:03.300]  мы имеем неприятность.
[01:00:03.300 --> 01:00:07.300]  Ну если лямбда 2 и лямбда 1 близки, там скорость сходимости
[01:00:07.300 --> 01:00:08.300]  никакая.
[01:00:08.300 --> 01:00:15.300]  На практике мы можем наблюдать медленную сходимость, и
[01:00:15.300 --> 01:00:22.300]  это может быть неприятно, и реально это может быть.
[01:00:22.300 --> 01:00:31.300]  Это правильно.
[01:00:31.300 --> 01:00:49.300]  Так вот, придумано было, как коэр-алгоритм можно
[01:00:49.300 --> 01:00:50.300]  ускорять.
[01:00:50.300 --> 01:00:55.300]  Это существенный часть коэр-алгоритма.
[01:00:56.300 --> 01:00:59.300]  Придуман способ его ускорять.
[01:00:59.300 --> 01:01:20.300]  Сейчас я вам расскажу, это такая классика нашей науки.
[01:01:20.300 --> 01:01:27.300]  Прежде всего, тут есть две вещи, конечно, есть скорость
[01:01:27.300 --> 01:01:32.300]  сходимости, а есть еще просто цена одной итерации.
[01:01:32.300 --> 01:01:38.300]  И то, и то важно.
[01:01:38.300 --> 01:01:43.300]  Давайте вначале обсудим цену одной итерации.
[01:01:43.300 --> 01:01:48.300]  Ну какая она, если порядок маты ЦН, то какова цена
[01:01:48.300 --> 01:01:49.300]  одной итерации?
[01:01:49.300 --> 01:01:50.300]  А?
[01:01:50.300 --> 01:01:51.300]  Н куб, правильно.
[01:01:51.300 --> 01:01:54.300]  Коэр-разложение будет стоить н кубе действия.
[01:01:54.300 --> 01:01:57.300]  Потом надо коэр в обратном политике переносить.
[01:01:57.300 --> 01:02:00.300]  Еще н кубе действия.
[01:02:00.300 --> 01:02:04.300]  Значит, вот так как написано, цена одной коэр-этерации
[01:02:04.300 --> 01:02:07.300]  есть н куб.
[01:02:07.300 --> 01:02:11.300]  Но я вот люблю слово ортодоксально, то есть то, что я вам рассказал,
[01:02:11.300 --> 01:02:15.300]  это есть ортодоксальный коэр-алгоритм.
[01:02:15.300 --> 01:02:22.300]  И в таком виде он никогда не применяет.
[01:02:22.300 --> 01:02:27.300]  Значит, можно снизить цену одной итерации.
[01:02:27.300 --> 01:02:32.300]  Вот мне кажется, что это англичанин по фамилии Фрэнсис заметил.
[01:02:32.300 --> 01:02:34.300]  Тоже интересные судьбы бывают.
[01:02:34.300 --> 01:02:40.300]  Вот несколько лет тому назад, те классики, которые еще были живы,
[01:02:40.300 --> 01:02:46.300]  они пытались этого Фрэнсиса найти,
[01:02:46.300 --> 01:02:51.300]  потому что он отметился статьей, которую все на проповую цитировали
[01:02:51.300 --> 01:02:54.300]  о коэр-алгоритме.
[01:02:54.300 --> 01:03:01.300]  Значит, он там предложил, как снизить цену одной итерации.
[01:03:01.300 --> 01:03:04.300]  Сейчас я вам расскажу, это вначале просто оказывается.
[01:03:04.300 --> 01:03:09.300]  Ну а кроме того, еще одна идея.
[01:03:09.300 --> 01:03:20.300]  Я обсуждал, как можно уменьшить само число итерации за счет таких манипуляций.
[01:03:20.300 --> 01:03:22.300]  Вот эти две вещи.
[01:03:22.300 --> 01:03:26.300]  Ну вначале, как можно снизить цену одной итерации.
[01:03:26.300 --> 01:03:29.300]  Наблюдение очень простое.
[01:03:29.300 --> 01:03:40.300]  Любую матрицу комплекции с помощью умножений слева и справа
[01:03:40.300 --> 01:03:49.300]  на некоторые просто вычисляемые унитарные матрицы специального вида.
[01:03:49.300 --> 01:03:53.300]  Ну, например, мог быть просто матрица отражения или перестановки.
[01:03:53.300 --> 01:03:56.300]  Больше ничего не надо.
[01:03:56.300 --> 01:04:05.300]  Можно привести любую матрицу, любая матрица унитарно подобна верхней треугольной матрице.
[01:04:05.300 --> 01:04:07.300]  Знаете, какая она такая?
[01:04:07.300 --> 01:04:12.300]  Любая матрица комплекции унитарно подобна верхней треугольной матрице.
[01:04:12.300 --> 01:04:17.300]  Это знаменитая теорема Шура.
[01:04:17.300 --> 01:04:20.300]  Вы не знаете?
[01:04:20.300 --> 01:04:22.300]  Ну это плохо.
[01:04:23.300 --> 01:04:31.300]  Любую матрицу можно найти в унитарной матрице КОТАК, чтобы получить верхнюю треугольную матрицу.
[01:04:31.300 --> 01:04:37.300]  Она, кстати, довольно легко по индукции доказывается.
[01:04:37.300 --> 01:04:40.300]  Можете испытать свои темы.
[01:04:40.300 --> 01:04:46.300]  Ну начать с собственного вектора к.
[01:04:46.300 --> 01:04:47.300]  нормированного.
[01:04:47.300 --> 01:04:52.300]  Вы потом его достроите, свести задачу к аналогичности для матрицы.
[01:04:52.300 --> 01:04:59.300]  И вот так вот собрать унитарную матрицу, ту, которую нужно получить верхнюю треугольную матрицу.
[01:04:59.300 --> 01:05:02.300]  Вот здесь, конечно, собственное значение.
[01:05:02.300 --> 01:05:04.300]  На счет теоремы Шура.
[01:05:10.300 --> 01:05:12.300]  Который вы должны уметь доказывать.
[01:05:12.300 --> 01:05:13.300]  Уверен.
[01:05:18.300 --> 01:05:22.300]  Ну в Жардановом базе оно-то имеет некоторые отношения.
[01:05:22.300 --> 01:05:29.300]  Но матрица кой-то унитарная матрица, а здесь всего лишь верхнюю треугольную.
[01:05:29.300 --> 01:05:37.300]  Если вы напишете Жарданову матрицу, там вот эта матрица, там примерно неподобная.
[01:05:37.300 --> 01:05:42.300]  Ну и матрица из собственных треугольных матриц, а тем не обязательно унитарная.
[01:05:42.300 --> 01:05:45.300]  Может она такой в принципе не будет.
[01:05:45.300 --> 01:05:47.300]  Это простая идея, это намного более простой результат.
[01:05:47.300 --> 01:05:49.300]  Элементарно доказывают.
[01:05:50.300 --> 01:05:52.300]  Элементарно доказывают.
[01:05:54.300 --> 01:06:02.300]  Но с вычислительной точки зрения это все-таки как результат существования.
[01:06:03.300 --> 01:06:06.300]  Но не способ вычисления.
[01:06:07.300 --> 01:06:15.300]  Потому что если мы найдем вот эту верхнюю треугольную матрицу, то мы уже собственное значение получим.
[01:06:19.300 --> 01:06:21.300]  Ну верно, но так не делают.
[01:06:21.300 --> 01:06:23.300]  Ну в принципе вы правы.
[01:06:23.300 --> 01:06:31.300]  Найдете в начале одно собственное значение, потом называется это словечком дефляция.
[01:06:32.300 --> 01:06:39.300]  Вот сведение к задаче меньшего размера можно.
[01:06:39.300 --> 01:06:41.300]  То есть это действительно метод некоторого права.
[01:06:41.300 --> 01:06:49.300]  Но тем не менее, да, индуктивные доказательства Теоремы Шуров в принципе конечно дает метод некоторого.
[01:06:49.300 --> 01:06:56.300]  Но я хочу вот какое есть наблюдение другое.
[01:06:57.300 --> 01:07:09.300]  Значит можно, давайте вот кротчурку забудем, а вот здесь добавим еще под догонат.
[01:07:13.300 --> 01:07:23.300]  Вот можно совершенно элементарно без всякой индукции доказать, что любую комплексную матрицу можно привести к почти треугольной матрице.
[01:07:24.300 --> 01:07:29.300]  С помощью последовательности умножения на унитарную матрицу.
[01:07:29.300 --> 01:07:31.300]  Сейчас я вам покажу как.
[01:07:33.300 --> 01:07:35.300]  Ну пять хорошего числа.
[01:07:37.300 --> 01:07:39.300]  Значит вот пять на пять.
[01:07:41.300 --> 01:07:46.300]  И здесь будут элементарные преобразования.
[01:07:46.300 --> 01:07:51.300]  Ну если там отражение, может быть корень надо будет извлекать.
[01:07:52.300 --> 01:07:55.300]  В длину выходить вектора.
[01:07:55.300 --> 01:07:57.300]  Больше ничего не надо.
[01:07:57.300 --> 01:07:59.300]  Значит что делаем?
[01:08:01.300 --> 01:08:03.300]  С помощью вот этого элемента.
[01:08:05.300 --> 01:08:07.300]  Ну зря его в кружочках было.
[01:08:11.300 --> 01:08:17.300]  Ну скажем, может быть вам проще, проще объяснить когда все вещественное.
[01:08:17.300 --> 01:08:20.300]  Давайте объясню когда все вещественное.
[01:08:20.300 --> 01:08:25.300]  В комплексном случае лучше преобразование отражения использовать.
[01:08:25.300 --> 01:08:28.300]  А вот в вещественном случае можно и вращение.
[01:08:28.300 --> 01:08:34.300]  Значит вот можно с помощью этого элемента получить здесь ноль.
[01:08:38.300 --> 01:08:44.300]  Повращав, умножив на матрицу вращение.
[01:08:45.300 --> 01:08:49.300]  Ну и аналогично и здесь можно получить и здесь.
[01:08:52.300 --> 01:08:57.300]  Но мы должны выполнять преобразование конкурентно.
[01:08:57.300 --> 01:09:02.300]  То есть мы должны справа выполнить те же преобразования со столбцами.
[01:09:02.300 --> 01:09:04.300]  Ну давайте заметим.
[01:09:04.300 --> 01:09:09.300]  Вот здесь меняются вторая и третья строчки.
[01:09:09.300 --> 01:09:11.300]  Будут меняться вторая и третья столбцы.
[01:09:11.300 --> 01:09:13.300]  Второй четвертый, второй пятый.
[01:09:13.300 --> 01:09:15.300]  Вот эти нольные нули сохранятся.
[01:09:17.300 --> 01:09:23.300]  Вот это преобразование подобия или унитарного подобия на самом деле.
[01:09:27.300 --> 01:09:31.300]  Ну взамен можно скажем было бы использовать матрицу отражения.
[01:09:31.300 --> 01:09:33.300]  Чтобы сразу здесь эти ноли получить.
[01:09:33.300 --> 01:09:35.300]  Здесь в комплексном случае прекрасно.
[01:09:39.300 --> 01:09:41.300]  И это элементарная операция.
[01:09:41.300 --> 01:09:43.300]  Практически арифметическая операция.
[01:09:43.300 --> 01:09:45.300]  Но можно надо его часто длиной вверх.
[01:09:48.300 --> 01:09:52.300]  Дальше закрепляем успех.
[01:09:52.300 --> 01:09:56.300]  Теперь с помощью этого элемента здесь вращение получаем.
[01:09:56.300 --> 01:09:58.300]  А вот эти нули сохранятся.
[01:09:58.300 --> 01:10:00.300]  Ну почти, но не совсем.
[01:10:10.300 --> 01:10:12.300]  Сейчас.
[01:10:12.300 --> 01:10:14.300]  Что-то нужно сделать.
[01:10:14.300 --> 01:10:16.300]  Ну вот.
[01:10:16.300 --> 01:10:18.300]  Ну вот.
[01:10:18.300 --> 01:10:20.300]  Ну вот.
[01:10:20.300 --> 01:10:22.300]  Ну вот.
[01:10:22.300 --> 01:10:24.300]  Ну вот.
[01:10:24.300 --> 01:10:26.300]  Ну вот.
[01:10:26.300 --> 01:10:27.300]  Ну вот.
[01:10:27.300 --> 01:10:29.300]  Что-то мне не нравится.
[01:10:29.300 --> 01:10:31.300]  Не в гауссе.
[01:10:31.300 --> 01:10:33.300]  В гауссе мы с этого элемента начинаем.
[01:10:35.300 --> 01:10:37.300]  А здесь мы не трогаем первую стручку.
[01:10:37.300 --> 01:10:39.300]  Так.
[01:10:57.300 --> 01:10:59.300]  Вот у нас.
[01:11:03.300 --> 01:11:05.300]  Вот такая матрица.
[01:11:05.300 --> 01:11:09.300]  И теперь мы с помощью этого элемента здесь ноль получаем.
[01:11:09.300 --> 01:11:11.300]  Ну а здесь два нуля комбинируется.
[01:11:11.300 --> 01:11:13.300]  С ним ничего не произойдет.
[01:11:15.300 --> 01:11:17.300]  Получаем здесь два нуля.
[01:11:17.300 --> 01:11:19.300]  Справа аналогичное преобразование.
[01:11:19.300 --> 01:11:21.300]  И один из полученных нулей не испортит.
[01:11:23.300 --> 01:11:25.300]  Ну наконец.
[01:11:25.300 --> 01:11:27.300]  Вот.
[01:11:27.300 --> 01:11:29.300]  Аналогично здесь получаем ноль.
[01:11:29.300 --> 01:11:31.300]  Справа преобразуем.
[01:11:31.300 --> 01:11:33.300]  Эти нули в соответствии.
[01:11:33.300 --> 01:11:35.300]  Вот и все.
[01:11:41.300 --> 01:11:43.300]  На диагональ никаких нулей ничего не будет.
[01:11:43.300 --> 01:11:45.300]  На диагональ никаких нулей.
[01:11:45.300 --> 01:11:47.300]  Ну а сейчас как у нас?
[01:11:47.300 --> 01:11:49.300]  Там элементы другие.
[01:11:51.300 --> 01:11:53.300]  Здесь семетра и не предполагается.
[01:11:53.300 --> 01:11:55.300]  Здесь семетра и не предполагается.
[01:11:55.300 --> 01:11:57.300]  Но вы нащупали одну интересную вещь.
[01:11:59.300 --> 01:12:01.300]  Значит смотрите.
[01:12:01.300 --> 01:12:03.300]  Значит что мы сейчас с вами увидели.
[01:12:03.300 --> 01:12:05.300]  Вот можно такую
[01:12:05.300 --> 01:12:07.300]  сэкономентальную фразу.
[01:12:07.300 --> 01:12:09.300]  После того, что мы увидели.
[01:12:09.300 --> 01:12:11.300]  При применении коэр-алгоритма
[01:12:11.300 --> 01:12:13.300]  не ограничивая вводчивость.
[01:12:13.300 --> 01:12:15.300]  Можно считать, что исходная матрица
[01:12:15.300 --> 01:12:17.300]  и ясная и точная диагональ.
[01:12:17.300 --> 01:12:19.300]  Мы можем вот эти преобразования выполнить.
[01:12:19.300 --> 01:12:21.300]  Нужно всего им в кубе действовать.
[01:12:21.300 --> 01:12:23.300]  И вот мы их выполним.
[01:12:23.300 --> 01:12:25.300]  Получим матрицу, которая имеет
[01:12:25.300 --> 01:12:27.300]  те же собственные значения.
[01:12:27.300 --> 01:12:29.300]  Но она будет верхней почти треугольной.
[01:12:29.300 --> 01:12:31.300]  И мы коэр-алгоритм
[01:12:31.300 --> 01:12:33.300]  начнем применять к верхней треугольной матрице.
[01:12:33.300 --> 01:12:35.300]  Вот так.
[01:12:35.300 --> 01:12:37.300]  Вот так.
[01:12:37.300 --> 01:12:39.300]  Вот так.
[01:12:39.300 --> 01:12:41.300]  Вот так.
[01:12:41.300 --> 01:12:43.300]  Вот так.
[01:12:43.300 --> 01:12:45.300]  Вот так.
[01:12:45.300 --> 01:12:47.300]  Вот теперь
[01:12:47.300 --> 01:12:49.300]  как применять
[01:12:49.300 --> 01:12:51.300]  коэр-алгоритм к верхней треугольной матрице.
[01:12:51.300 --> 01:12:53.300]  Сейчас покажу.
[01:13:05.300 --> 01:13:07.300]  К верхней почти треугольной.
[01:13:07.300 --> 01:13:09.300]  К верхней почти треугольной.
[01:13:15.300 --> 01:13:17.300]  Итак, вот эта матрица
[01:13:17.300 --> 01:13:19.300]  А или А0
[01:13:19.300 --> 01:13:21.300]  вот теперь она в верхней треугольной.
[01:13:21.300 --> 01:13:23.300]  У меня какое-то такое предварительное преобразование.
[01:13:23.300 --> 01:13:25.300]  Что б на это Фрэнсисе предложил
[01:13:25.300 --> 01:13:27.300]  делать предварительное преобразование.
[01:13:27.300 --> 01:13:29.300]  Но дальше еще кое-что надо заметить.
[01:13:29.300 --> 01:13:31.300]  Значит, вот надо коэр-разложение
[01:13:31.300 --> 01:13:33.300]  для этой матрицы выполнить.
[01:13:33.300 --> 01:13:35.300]  Сколько надо действовать на это?
[01:13:37.300 --> 01:13:39.300]  Чтобы вот этот ноль получить
[01:13:39.300 --> 01:13:41.300]  в порядке nb.
[01:13:47.300 --> 01:13:49.300]  Не n куба, а n квадрат.
[01:13:49.300 --> 01:13:51.300]  Правильно.
[01:13:51.300 --> 01:13:53.300]  n квадрат.
[01:13:53.300 --> 01:13:55.300]  А потом в обратном порядке перемножить.
[01:13:57.300 --> 01:13:59.300]  Но это будет равносильно тому, что вот эти
[01:13:59.300 --> 01:14:01.300]  нули опять станут нигулями.
[01:14:03.300 --> 01:14:05.300]  Значит, наблюдение
[01:14:05.300 --> 01:14:07.300]  на одной итерации коэр-разгоритма
[01:14:07.300 --> 01:14:09.300]  верхний почти треугольный вид
[01:14:09.300 --> 01:14:11.300]  сохраняется.
[01:14:11.300 --> 01:14:13.300]  То есть, если матрица А была,
[01:14:13.300 --> 01:14:15.300]  а ноль была в верхней почти треугольной,
[01:14:15.300 --> 01:14:17.300]  то А1 тоже останется в верхней почти треугольной.
[01:14:21.300 --> 01:14:23.300]  Есть реализация коэр-этерации.
[01:14:23.300 --> 01:14:25.300]  И такая, что
[01:14:25.300 --> 01:14:27.300]  на итерациях
[01:14:27.300 --> 01:14:29.300]  будет поддерживаться
[01:14:29.300 --> 01:14:31.300]  верхний почти треугольный вид.
[01:14:31.300 --> 01:14:33.300]  То есть, этот вид
[01:14:33.300 --> 01:14:35.300]  инвариантен относительно коэр-этерации.
[01:14:37.300 --> 01:14:39.300]  И как вы справедливо заметили,
[01:14:39.300 --> 01:14:41.300]  цена n квадрата.
[01:14:41.300 --> 01:14:43.300]  Значит, было n куб.
[01:14:43.300 --> 01:14:45.300]  Н куб, да, стало n квадрат.
[01:14:45.300 --> 01:14:47.300]  Чудесно.
[01:14:47.300 --> 01:14:49.300]  Чудесно.
[01:14:49.300 --> 01:14:51.300]  А теперь вот я
[01:14:51.300 --> 01:14:53.300]  хочу переосмыслить
[01:14:53.300 --> 01:14:55.300]  замечание, которое вы сделали.
[01:14:55.300 --> 01:14:57.300]  Вы это неправильно заметили, конечно.
[01:14:57.300 --> 01:14:59.300]  А если матрица Ермитова,
[01:14:59.300 --> 01:15:01.300]  то вот тогда
[01:15:01.300 --> 01:15:03.300]  то, что вы сказали, будет правдой,
[01:15:03.300 --> 01:15:05.300]  тогда и сверху будут нули получаться.
[01:15:05.300 --> 01:15:07.300]  То есть, если вот
[01:15:07.300 --> 01:15:09.300]  то же самое преобразование
[01:15:09.300 --> 01:15:11.300]  мы будем применять к Ермитовой матрице,
[01:15:11.300 --> 01:15:13.300]  то в результате получится матрица
[01:15:13.300 --> 01:15:15.300]  не только верхней почти треугольной,
[01:15:17.300 --> 01:15:19.300]  но еще и нижней одновременно
[01:15:19.300 --> 01:15:21.300]  почти треугольной.
[01:15:21.300 --> 01:15:23.300]  То есть, она будет
[01:15:23.300 --> 01:15:25.300]  трех диагонами.
[01:15:27.300 --> 01:15:29.300]  То есть, Ермитова почти
[01:15:29.300 --> 01:15:31.300]  треугольная матрица
[01:15:31.300 --> 01:15:33.300]  она вот приводится
[01:15:33.300 --> 01:15:35.300]  к Ермитовой треугольной матрице.
[01:15:37.300 --> 01:15:39.300]  к Ермитовой треугольной матрице.
[01:15:41.300 --> 01:15:43.300]  Что?
[01:15:53.300 --> 01:15:55.300]  Ну, ну лиспу.
[01:15:55.300 --> 01:15:57.300]  Ну, лиспу.
[01:15:57.300 --> 01:15:59.300]  То есть, в Ермитовом случае нет.
[01:16:03.300 --> 01:16:05.300]  Ну, там нет подобия,
[01:16:05.300 --> 01:16:07.300]  нет цыгана.
[01:16:07.300 --> 01:16:09.300]  Здесь преобразование должно быть
[01:16:09.300 --> 01:16:11.300]  преобразованием подобия.
[01:16:11.300 --> 01:16:13.300]  Ну, даже унитарного подобия.
[01:16:21.300 --> 01:16:23.300]  Ну, что вы хотите сказать?
[01:16:23.300 --> 01:16:25.300]  Что вы хотите сказать?
[01:16:25.300 --> 01:16:27.300]  Нет, что вы хотите сказать?
[01:16:31.300 --> 01:16:33.300]  Не сможете.
[01:16:33.300 --> 01:16:35.300]  Ну, посидите,
[01:16:35.300 --> 01:16:37.300]  посмотрите.
[01:16:37.300 --> 01:16:39.300]  То есть, если у вас матрица не Ермитова,
[01:16:39.300 --> 01:16:41.300]  не Ермитова,
[01:16:41.300 --> 01:16:43.300]  и вы хотите делать
[01:16:43.300 --> 01:16:45.300]  преобразование
[01:16:45.300 --> 01:16:47.300]  унитарное,
[01:16:47.300 --> 01:16:49.300]  унитарного подобия,
[01:16:49.300 --> 01:16:51.300]  то вы
[01:16:51.300 --> 01:16:53.300]  ну, к верхнему треугольному
[01:16:53.300 --> 01:16:55.300]  виду можете привести,
[01:16:55.300 --> 01:16:57.300]  как я рассказал.
[01:16:57.300 --> 01:16:59.300]  Ну, кто-то большую сделать.
[01:16:59.300 --> 01:17:01.300]  Вот, в случае Ермитовой,
[01:17:01.300 --> 01:17:03.300]  ну, просто понятно, что
[01:17:03.300 --> 01:17:05.300]  вот эта верхняя почти треугольная матрица, она же должна
[01:17:05.300 --> 01:17:07.300]  Ермитовой.
[01:17:07.300 --> 01:17:09.300]  А что такое Ермитова
[01:17:09.300 --> 01:17:11.300]  верхняя почти треугольная матрица?
[01:17:11.300 --> 01:17:13.300]  Она трагедионная.
[01:17:13.300 --> 01:17:15.300]  Правда?
[01:17:15.300 --> 01:17:17.300]  Она трагедионная.
[01:17:17.300 --> 01:17:19.300]  Вот. А теперь
[01:17:19.300 --> 01:17:21.300]  представьте себе, что вы все то же самое
[01:17:21.300 --> 01:17:23.300]  применяете QR-авгоритм
[01:17:23.300 --> 01:17:25.300]  Ермитовой.
[01:17:37.300 --> 01:17:39.300]  Да, спасибо.
[01:17:39.300 --> 01:17:41.300]  Что в диагональной матрице?
[01:17:41.300 --> 01:17:43.300]  Значит, вот эти нули получаете,
[01:17:43.300 --> 01:17:45.300]  сколько действий надо?
[01:17:45.300 --> 01:17:47.300]  И кто-то из вас замечательный матч,
[01:17:47.300 --> 01:17:49.300]  ну, сколько будет?
[01:17:49.300 --> 01:17:51.300]  Сколько действий будет?
[01:17:51.300 --> 01:17:53.300]  N, уже не N в квадрате,
[01:17:53.300 --> 01:17:55.300]  а N.
[01:17:55.300 --> 01:17:57.300]  И когда вы в обратном порядке
[01:17:57.300 --> 01:17:59.300]  перемножите,
[01:17:59.300 --> 01:18:01.300]  уже R и Q, да,
[01:18:01.300 --> 01:18:03.300]  у вас
[01:18:03.300 --> 01:18:05.300]  снова появится уже трех
[01:18:05.300 --> 01:18:07.300]  диагональной матрицы Ермитова.
[01:18:07.300 --> 01:18:09.300]  Правда?
[01:18:09.300 --> 01:18:11.300]  Ну, и это будет стоить порядка N.
[01:18:11.300 --> 01:18:13.300]  То есть, в
[01:18:13.300 --> 01:18:15.300]  Ермитовом случае
[01:18:15.300 --> 01:18:17.300]  никакая ретерация будет стоить
[01:18:17.300 --> 01:18:19.300]  по большой от N.
[01:18:19.300 --> 01:18:21.300]  Что совсем прекрасно.
[01:18:21.300 --> 01:18:23.300]  То есть, QR-авгоритм
[01:18:23.300 --> 01:18:25.300]  применяемый к Ермитовой
[01:18:25.300 --> 01:18:27.300]  матрице вообще быстрее.
[01:18:27.300 --> 01:18:29.300]  Но
[01:18:29.300 --> 01:18:31.300]  после предварительного преобразования.
[01:18:31.300 --> 01:18:33.300]  Значит, в общем случае
[01:18:33.300 --> 01:18:35.300]  предварительное преобразование
[01:18:35.300 --> 01:18:37.300]  дает верхнюю
[01:18:37.300 --> 01:18:39.300]  почти треугольную матрицу.
[01:18:39.300 --> 01:18:41.300]  А в Ермитовом случае
[01:18:41.300 --> 01:18:43.300]  трехдиагональная матрица.
[01:18:43.300 --> 01:18:45.300]  Никакой ретерации
[01:18:45.300 --> 01:18:47.300]  становится, либо
[01:18:47.300 --> 01:18:49.300]  сложности N в квадрат,
[01:18:49.300 --> 01:18:51.300]  либо сложности N.
[01:18:51.300 --> 01:18:53.300]  Значит, вот первая цена
[01:18:53.300 --> 01:18:55.300]  одной итерации может быть уменьшена
[01:18:55.300 --> 01:18:57.300]  за счет предварительного
[01:18:57.300 --> 01:18:59.300]  преобразования, как в один раз таком.
[01:18:59.300 --> 01:19:01.300]  N в кубе
[01:19:01.300 --> 01:19:03.300]  потратили,
[01:19:03.300 --> 01:19:05.300]  а после этого у вас, в общем случае,
[01:19:05.300 --> 01:19:07.300]  N в квадрате умножит
[01:19:07.300 --> 01:19:09.300]  на число итерации.
[01:19:09.300 --> 01:19:11.300]  Ну, а теперь займемся вопросом
[01:19:11.300 --> 01:19:13.300]  о числе итераций, их бы тоже
[01:19:13.300 --> 01:19:15.300]  хотелось бы как-то уметь уменьшать.
[01:19:15.300 --> 01:19:17.300]  Но здесь идея такая,
[01:19:17.300 --> 01:19:19.300]  это идея сдвигов.
[01:19:19.300 --> 01:19:21.300]  Значит, вот вместо
[01:19:21.300 --> 01:19:23.300]  исходной матрицы
[01:19:23.300 --> 01:19:25.300]  рассмотрим сдвинутую матрицу.
[01:19:27.300 --> 01:19:29.300]  Выберем сдвиг альфа.
[01:19:29.300 --> 01:19:31.300]  И для вот этой сдвинутой
[01:19:31.300 --> 01:19:33.300]  матрицы найдем QR-разложение.
[01:19:37.300 --> 01:19:39.300]  Вот, а новую матрицу
[01:19:39.300 --> 01:19:41.300]  построим.
[01:19:41.300 --> 01:19:43.300]  Перемножим эти множители
[01:19:43.300 --> 01:19:45.300]  в обратном порядке и добавим
[01:19:45.300 --> 01:19:47.300]  то, что вы читали.
[01:19:49.300 --> 01:19:51.300]  Вот QR
[01:19:51.300 --> 01:19:53.300]  итерации со
[01:19:53.300 --> 01:19:55.300]  сдвигом на альфа.
[01:19:55.300 --> 01:19:57.300]  Ну, опять таки,
[01:19:57.300 --> 01:19:59.300]  давайте заметим, что такое
[01:19:59.300 --> 01:20:01.300]  есть А1.
[01:20:01.300 --> 01:20:03.300]  Здесь я могу написать Q1 со
[01:20:03.300 --> 01:20:05.300]  звездой Q1
[01:20:05.300 --> 01:20:07.300]  R1 и Q1
[01:20:07.300 --> 01:20:09.300]  минус альфа и, правильно?
[01:20:11.300 --> 01:20:13.300]  Ну, здесь вот, в единичной матрице,
[01:20:13.300 --> 01:20:15.300]  где в скобочках
[01:20:15.300 --> 01:20:17.300]  увидели,
[01:20:17.300 --> 01:20:19.300]  что увидели в скобочках?
[01:20:21.300 --> 01:20:23.300]  Это есть
[01:20:23.300 --> 01:20:25.300]  А
[01:20:27.300 --> 01:20:29.300]  минус
[01:20:29.300 --> 01:20:31.300]  альфа
[01:20:33.300 --> 01:20:35.300]  плюс альфа и.
[01:20:37.300 --> 01:20:39.300]  То есть, как и раньше
[01:20:39.300 --> 01:20:41.300]  было,
[01:20:41.300 --> 01:20:43.300]  матрица Q1
[01:20:43.300 --> 01:20:45.300]  имитарно подобна исходной матрице.
[01:20:47.300 --> 01:20:49.300]  Бесмотря на то, что мы какое разложение
[01:20:49.300 --> 01:20:51.300]  делали не для матрица, а для
[01:20:51.300 --> 01:20:53.300]  сдвинутой матрицы.
[01:20:53.300 --> 01:20:55.300]  Ну, вот и в результате
[01:20:55.300 --> 01:20:57.300]  вместо вот степеней
[01:20:57.300 --> 01:20:59.300]  возникает
[01:20:59.300 --> 01:21:01.300]  вот, можно даже дальше
[01:21:01.300 --> 01:21:03.300]  посмотреть.
[01:21:03.300 --> 01:21:05.300]  Значит, это будет иметь
[01:21:05.300 --> 01:21:07.300]  разложение,
[01:21:07.300 --> 01:21:09.300]  как мы раньше
[01:21:09.300 --> 01:21:11.300]  писали
[01:21:11.300 --> 01:21:13.300]  и так далее.
[01:21:13.300 --> 01:21:15.300]  То есть, в результате
[01:21:17.300 --> 01:21:19.300]  выбор сдвигов
[01:21:19.300 --> 01:21:21.300]  значит,
[01:21:21.300 --> 01:21:23.300]  сходимостью
[01:21:23.300 --> 01:21:25.300]  будут управлять
[01:21:25.300 --> 01:21:27.300]  не собственные значения матрицы,
[01:21:27.300 --> 01:21:29.300]  а сдвинутые собственные значения.
[01:21:29.300 --> 01:21:31.300]  То есть, собственные значения вот этой
[01:21:31.300 --> 01:21:33.300]  матрицы.
[01:21:35.300 --> 01:21:37.300]  И это есть, да, да, да, да, да, да, да.
[01:21:39.300 --> 01:21:41.300]  Вот такие сдвинутые.
[01:21:41.300 --> 01:21:43.300]  То есть, это есть некий полином определенный
[01:21:43.300 --> 01:21:45.300]  сдвигами от матрицы А.
[01:21:45.300 --> 01:21:47.300]  И вот отношения вот этих
[01:21:47.300 --> 01:21:49.300]  новых собственных значений
[01:21:49.300 --> 01:21:51.300]  могут быть поменьше.
[01:21:51.300 --> 01:21:53.300]  Сдвиги выбираются именно с такой целью,
[01:21:53.300 --> 01:21:55.300]  чтобы сделать это отношение поменьше.
[01:21:55.300 --> 01:21:57.300]  Ну, вот общая идея сдвигов такая.
[01:21:57.300 --> 01:21:59.300]  Значит, вот QR-алгоритм
[01:21:59.300 --> 01:22:01.300]  обычно применяется
[01:22:01.300 --> 01:22:03.300]  с предварительным преобразованием
[01:22:03.300 --> 01:22:05.300]  к почти треугольной матрице
[01:22:05.300 --> 01:22:07.300]  либо к треугольгагональной в этом случае.
[01:22:07.300 --> 01:22:09.300]  И обычно со сдвигами.
[01:22:09.300 --> 01:22:11.300]  Ну, вот в случае сдвигов
[01:22:11.300 --> 01:22:13.300]  такой вот теория
[01:22:13.300 --> 01:22:15.300]  глобальной исходимости вроде бы нет.
[01:22:15.300 --> 01:22:17.300]  Но при разных стратегиях
[01:22:17.300 --> 01:22:19.300]  сдвига доказывается
[01:22:19.300 --> 01:22:21.300]  очень хорошей локальной исходимостью
[01:22:21.300 --> 01:22:23.300]  квадратичной
[01:22:23.300 --> 01:22:25.300]  или даже выше в ровном случае.
[01:22:25.300 --> 01:22:27.300]  Ну,
[01:22:27.300 --> 01:22:29.300]  это как раз и вышло.
[01:22:29.300 --> 01:22:31.300]  Вот, я должен сделать объявление,
[01:22:31.300 --> 01:22:33.300]  но кто-то может быть
[01:22:33.300 --> 01:22:35.300]  обрадуется, а кто-то огорчится.
[01:22:35.300 --> 01:22:37.300]  Следующий четверг, лет впереди.
[01:22:41.300 --> 01:22:43.300]  Вот подгадал как-то.
[01:22:45.300 --> 01:22:47.300]  А вот через две недели будет.
[01:22:49.300 --> 01:22:51.300]  Смотрите.
[01:22:51.300 --> 01:22:53.300]  Как-то мы один день пережили без вас.
[01:22:57.300 --> 01:22:59.300]  Очень красиво.
[01:22:59.300 --> 01:23:01.300]  Ну ладно,
[01:23:01.300 --> 01:23:03.300]  на сегодня все.
[01:23:07.300 --> 01:23:09.300]  Спасибо.

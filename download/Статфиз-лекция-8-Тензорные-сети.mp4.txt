[00:00.000 --> 00:13.120]  так давайте вернемся к той задачке которая была вот эта вот задачка передать
[00:13.120 --> 00:19.560]  сообщение квантовая в котором есть суперпозиции нуля единиц зачем это нужно был вопрос прошлого
[00:19.560 --> 00:25.440]  раз чтобы например связать два квантовых компьютера в сеть значит как общая процедура
[00:25.440 --> 00:31.200]  строилась у нас у нас состояние с помощью кодирования переводилось в частичное состояние
[00:31.200 --> 00:38.120]  кубитом каждый из этих кубит пересылался через канал на выходе получалось вот такое вот за
[00:38.120 --> 00:45.440]  шумленное состояние видите n это число раз использование канал вот с этим агрегатом который
[00:45.440 --> 00:50.880]  на выходе здесь есть нужно было провести операцию декодирование после декодирования вы получите
[00:50.880 --> 00:58.040]  какой то бек который мало должен отличаться от си степени отличия можно например вот такой
[00:58.040 --> 01:04.120]  вот нормы смотреть один норму а мы в прошлый раз с вами водили фиделить для этого различия
[01:04.120 --> 01:19.440]  так сейчас я найду примеры тоже рассматривали вот если общая схема то пси пси матрица плотности
[01:19.440 --> 01:25.160]  на входе после кодирования переводится в некоторое другое состояние пси большое здесь
[01:25.160 --> 01:31.160]  n подчеркнуто это число кубитов который используется дальше каждый из этих кубитов пересылается
[01:31.160 --> 01:38.280]  через канал потом после процедуры декодирования получаете ром вот заметьте что размерности всех
[01:38.280 --> 01:46.200]  этих пространства они как бы отличаются то есть вот пси это из пространства размера стека вот
[01:46.200 --> 01:51.360]  на выходе тоже вы получаете ро из пространства размерности к а вот промежуточная часть вот в
[01:51.360 --> 01:56.440]  этом кодировании она другой размером все обладает это аж в тензорной степени н где
[01:56.440 --> 02:01.480]  на число раз используем канал вот эта вот схема как раз это иллюстрирует то есть у вас здесь
[02:01.480 --> 02:08.720]  пси это кубитное состояние размерности в 2 она выходе вот после кодирования у вас состояние
[02:08.720 --> 02:18.120]  размерности 2 в степени 9 так ну вот степень совпадения здесь внизу написано это и есть как
[02:18.120 --> 02:26.280]  раз фиделити упоминаем вот минимальная фиделити это будет там по всем си взять меня я посмотрю есть
[02:26.280 --> 02:32.640]  на следующем слайде есть вот минимальная степень совпадения или как здесь написано минимальная
[02:32.640 --> 02:40.080]  точность воспроизведения это вот такая вот величина берем си на входе кодируем пропускаем
[02:40.080 --> 02:46.720]  через канал декодируем и результат должен мало отличаться от пси вот минимум по всем си возьмем
[02:46.720 --> 02:53.480]  получим некоторые фиделити если фиделити близки единицы значит мы свою задачу выполнена вы
[02:53.480 --> 02:58.760]  симптотики должны получить плянцами о чем секунду с конечностью должны получить и день ну вот
[02:59.280 --> 03:05.840]  скорость передачи данных квантовых это такая величина что существует последовательность
[03:05.840 --> 03:22.520]  пространств я вот это не говорил прошел раз говорил просто что отношения логарифма как n
[03:22.520 --> 03:29.120]  таких, что выполняется. Эти последовательности нужны просто, чтобы был предел определенный.
[03:29.120 --> 03:38.000]  Так, по этой части, это воспоминания предыдущей лекции. Есть вопросы или нет?
[03:38.000 --> 03:47.000]  А кодирование – это у нас вот эта вот схема из конца прошлого семестра, да?
[03:47.000 --> 03:53.200]  Например, такая схема. Для меня главное, чтобы вы поняли вот в этой схеме,
[03:53.200 --> 03:59.660]  как она осуществляется. То есть вы хотите на самом деле передать вот это состояние
[03:59.660 --> 04:06.200]  псималинка, но канал-то у вас шумит, вернее, в канале есть шум, поэтому вам нужно эту информацию
[04:06.200 --> 04:14.920]  исправлять ошибки в результате действия этих шумов. Коды исправляющей ошибки мы с вами в конце
[04:14.920 --> 04:21.400]  прошлого семестра смотрели. Они что позволяют эти коды исправления ошибки сделать? Вероятность
[04:21.400 --> 04:27.640]  ошибки уменьшить, правильно? То есть вот если, например, без исправления ошибка была там П,
[04:27.640 --> 04:35.180]  то после исправления она будет П квадрат. Вероятность ошибки падает. А тут смотрится
[04:35.180 --> 04:40.800]  симпатический предел, когда n стынет в бесконечности уже. Но и вот эта ψ при этом,
[04:40.800 --> 04:49.600]  его разменность тоже увеличивается. Вот давайте я сейчас напишу. Вот эта
[04:49.600 --> 04:59.000]  ψ это элемент какого пространства? С, К. Вот эта ψn это элемент какого пространства? Это С,
[04:59.000 --> 05:05.360]  например, 2 в степени n, если вы в кубитном состоянии кодируете. Видно, что я пишу?
[05:05.360 --> 05:13.840]  Видно. Вот вы n устремляете к бесконечности. Это значит, что у вас число кубитов вот здесь
[05:13.840 --> 05:19.720]  увеличивается. 2 в степени n, понятно, тоже увеличивается. К при этом тоже увеличивается.
[05:21.720 --> 05:31.600]  Как посчитать сколько здесь кубитов? Это примерно 2 в степени логарифм К. И вот этот логарифм К
[05:31.640 --> 05:38.960]  это число кубитов. Вот этот логарифм К разделим на n, и это будет отношение числа кубитов,
[05:38.960 --> 05:44.160]  которые можно надежно передавать к числу кубитов, используемых при пересылке через канал.
[05:44.160 --> 05:50.160]  Вот это отношение в пределе, когда n стынет к бесконечности. Видите, тут и верхняя часть
[05:50.160 --> 05:54.160]  стремится к бесконечности, и нижняя часть стремится к бесконечности. Но их отношение
[05:54.160 --> 06:04.240]  какой-то конечной. И вот это и будет ваша скорость передачи данных. При этом
[06:04.240 --> 06:09.400]  конкретную схему вы позволяете некоторую свободу.
[06:09.400 --> 06:17.040]  Вот это кодирование EN вы можете в принципе сами выбирать, какое хотите.
[06:17.040 --> 06:26.480]  Например, вот эта схема. Какой изометрия? Это будет ваше кодирование EN.
[06:26.480 --> 06:34.560]  Но понятно, что здесь EN зафиксировано. У вас EN равно, например, девятке, и если вы
[06:34.560 --> 06:40.880]  посчитаете фиделити, вот эту минимальную степень совпадения, она будет меньше единиц.
[06:40.880 --> 06:46.080]  Ну, например, там 0,9. Я точное значение не знаю, но будет какое-то.
[06:46.080 --> 06:53.960]  И понятно, почему так. Потому что EN у вас конечная. Если бы вы EN стремили к бесконечности,
[06:53.960 --> 07:01.760]  то есть эту схему как бы увеличивали, увеличивали еще, то вы бы эту степень совпадения сделали бы лучше.
[07:01.760 --> 07:09.520]  Так, Эдуард ответил на вопрос или наоборот запутал? Ответили абсолютно полностью.
[07:09.520 --> 07:16.680]  Хорошо. Вот последнее еще определение, вот здесь квантовая пропускная способность.
[07:16.680 --> 07:22.200]  Это супремум вот этих достижимых скоростей, когда вы можете менять вот эти кодирования,
[07:22.200 --> 07:28.520]  можете менять вот эти последовательства кодирования и так далее. Вот этого можете менять,
[07:28.520 --> 07:35.200]  вот эта величина есть квантовая пропускная способность. В прошлый раз я под занавес уже
[07:35.200 --> 07:42.760]  анонсировал верхнюю границу. Тут есть некоторые слайды с обоснованием этой верхней границы.
[07:42.760 --> 07:49.160]  Давайте быстренько пробежимся по ним. Вот представьте, что у вас есть перепутанное
[07:49.160 --> 07:57.800]  состояние. hk, hk это два подпространства. Ну, например, одно, второе. И у вас есть перепутанное состояние,
[07:57.800 --> 08:02.800]  которое является на самом деле максимально перепутанным. Теперь, если вы одну из этих
[08:02.800 --> 08:10.320]  частей закодируете, отправите через канал, декодируете, она вот здесь. И эта часть должна
[08:10.320 --> 08:17.080]  быть очень похожа, ну вы симкотики воспроизводите, вот эту часть перепутанного состояния. То есть,
[08:17.080 --> 08:25.960]  что вы на самом деле сделаете? Вы протягиваете эту перепутанность между частицей слева и вот
[08:25.960 --> 08:31.920]  этой частицей справа. У вас получается перепутанность на расстоянии, потому что здесь слева это
[08:31.920 --> 08:41.840]  подсистема справа от канала. Вы эту перепутанность в пространстве между лабораториями А и Б
[08:41.840 --> 08:49.560]  распространили. И дальше вы можете, например, использовать протокол квантовой телепортации с
[08:49.560 --> 08:56.680]  использованием классического канала. Таким образом, вы сможете передавать квантовые сообщения все
[08:56.680 --> 09:07.040]  отсюда сюда, с помощью протокола квантовой телепортации. Так, вот этот слайд это воспроизводит. Так, дальше,
[09:07.040 --> 09:14.080]  то есть, вы сможете в такой схеме передавать квантовые сообщения. То есть, это другая формулировка
[09:14.080 --> 09:20.920]  с перепутанностью, которую в книжках используют, например, в книжке Вильде, Ватроуз и других,
[09:20.920 --> 09:31.120]  для доказательства верхней грани. Поскольку у вас все-таки есть вот это вот асимпатическое
[09:31.120 --> 09:38.560]  поведение, у вас какая-то ошибочка epsilon всегда есть. Ну и в стандартной нашей формулировке,
[09:38.560 --> 09:45.360]  что для любого epsilon существует 0, такое, что для любых n больше, чем 0. Это мы кратко называем для
[09:45.360 --> 09:54.560]  достаточно больших. Для достаточно больших n не превосходит эта ошибка epsilon. Эта epsilon
[09:54.560 --> 10:00.640]  пропадет дальше в рассуждении, потому что мы n устремим в бесконечности, но для строгости давайте
[10:00.640 --> 10:09.120]  так напишем. И вот теперь, как выразить этот логариф МК? Логариф МК – это есть энтеропия максимально
[10:09.120 --> 10:14.400]  смешанного состояния, а максимально смешанное состояние – это есть подсистема максимально
[10:14.400 --> 10:24.480]  перепутанного. Вот здесь они написаны, максимально смешанное состояние. И вы видите, что можно этот
[10:24.480 --> 10:34.040]  логариф МК написать как энтропию просто подсистемы A или энтропию подсистемы R. А поскольку A и R в этой
[10:34.040 --> 10:44.320]  схеме – это и есть ваше состояние C+, то тогда энтропия есть. Теперь в это выражение
[10:44.320 --> 11:01.600]  можно переписать в другом виде. Вот картинку внизу надо посмотреть. Вот у вас S, A, R. После
[11:01.600 --> 11:16.000]  прохождения через канал у вас будет... А, все. Немножко запутался. Вот смотрите. У вас
[11:16.000 --> 11:23.400]  состояние ωRb мало отличается от максимально перепутанного. Вот если бы ε было равно 0, мы бы
[11:23.400 --> 11:30.600]  просто здесь написали Sb. Если ε не нулевое, то тогда вот такая есть граница, которая называется
[11:30.600 --> 11:37.880]  Алийский-Фанес-Винкер. Но вот эти вот приращения, которые здесь написаны, они при ε равно 0
[11:37.880 --> 11:50.960]  зануляются. Мы потом перейдем к пределу, и поэтому они пропадут. Это понятно. А можно еще раз,
[11:50.960 --> 11:59.440]  откуда у нас условная энтропия появилась? Вот, левее. Сейчас возьму другой цвет, чтобы
[11:59.440 --> 12:09.080]  различался от того, что уже есть. Так, вот это вот. Да. Вот смотрите, тут некоторый трюк просто. S, A, R, A – это
[12:09.080 --> 12:19.960]  вообще 0. Поэтому вы можете вычесть из любого величины 0, получите то же самое. Так, ну тут,
[12:19.960 --> 12:28.600]  наверное, логично было бы написать S, A, но не суть важная. S, A или S, R – одна и та же. Тут логика
[12:28.600 --> 12:36.120]  вот в чем. Вот смотрите на вот это вот неравенство. Это неравенство говорит вам, что Rb под система
[12:36.120 --> 12:45.120]  почти такая же, как в максимально перепутанном состоянии. Поэтому, если вы замените вот здесь
[12:45.120 --> 12:59.200]  вот букву A на букву B, то вы получите небольшую ошибку. Замена A на B не приведет к большой ошибке.
[12:59.200 --> 13:12.040]  Эта ошибка как раз таки записана в виде вот красного квадратика справа. А когда вы устремите
[13:12.040 --> 13:23.120]  n к бесконечности, в этом частном будет стоять n, и epsilon в пределении на n даст вам 0, потому что
[13:23.120 --> 13:34.480]  для достаточно больших это означает вот именно вот эту фразу. Поэтому там у вас будет 0. А мы условия
[13:34.480 --> 13:45.840]  такое уложили, что ω Rb недалеко от C+, лет откуда-то следует? Это мы хотим сделать, да. Вот представьте,
[13:45.840 --> 13:55.760]  что, да, это мы условенно уложили. Вот кодирование, канал и декодирование – это почти тождественное
[13:55.760 --> 14:09.400]  преобразование. В реанствамящемся к бесконечности должно быть тождественное. Это мы хотим. Вот смотрите,
[14:09.400 --> 14:16.800]  да, еще раз, что хотим сделать. Предположим, что у вас есть вот эти кодирования и декодирования,
[14:16.800 --> 14:21.760]  которые удовлетворяют всем вот этим вот свойствам, которые здесь перечислены. Мы теперь хотим верхнюю
[14:21.760 --> 14:28.760]  границу на Q найти. Вот что мы хотим. Мы хотим ограничить ее сверху, потому что мы ищем
[14:28.760 --> 14:36.360]  фундаментальные ограничения. Не превосходит она какой величины. Поэтому давайте предположим,
[14:36.360 --> 14:40.840]  что существуют вот эти кодирования и декодирования, которые всем свойствам удовлетворяют. Значит,
[14:40.840 --> 14:47.760]  это epsilon стремится к нулю, преанствуемящемся бесконечно. Вот. Когда у нас получится вот такое
[14:47.760 --> 15:10.480]  выражение, которое внизу написано, видите, тут sb-sωrb делить на n. Сейчас, можно еще вопрос? А что такое h2 в красной применении? А, это бидарная тропия. Помните, что такое или напомнить?
[15:10.480 --> 15:26.920]  Это, в смысле, для двух исходов или как? Да, да, да. Вот, смотрите, h2 от x, это и есть минус x логаритм x, минус 1 минус x логаритм 1 минус x.
[15:26.920 --> 15:39.440]  Ну, вот еще просто напомню, что h2 от нуля есть 0, поэтому, когда вот это epsilon мало,
[15:39.440 --> 15:48.360]  то у вас h2 тоже занулиться. То есть, вот эта вот величина в красной прямоугольнике, она при epsilon равна нулю точно равна 0.
[15:48.360 --> 16:03.680]  А потом разделите на n и получите величину, которая в пределе даст тоже. Вот так вот.
[16:03.680 --> 16:14.640]  Значит, остается вот эта вот величина, и ее надо ограничить. Для того, чтобы ее ограничить сверху,
[16:14.640 --> 16:25.360]  ну, тут можно, давайте я кратко расскажу, как я понимаю. Что такое sb? sb, вот если бы кодирования
[16:25.360 --> 16:34.960]  и декодирования не было, это было бы состояние на выходе. А что такое амега rb? Это было бы вот это
[16:34.960 --> 16:45.840]  вот состояние на выходе вспомогательной системы r и b. А если у вас исходное состояние чистое,
[16:45.840 --> 16:54.000]  оно чистое, то на выходе rb и окружение в представлении Stein-Spring тоже будет чисто.
[16:54.000 --> 17:13.040]  Поэтому эта энтропия rb будет равняться энтропии e top. Поэтому энтропия окружения, это будет
[17:13.040 --> 17:21.520]  энтропия выхода комплементарного канала. Когда у вас получится первая часть энтропии выхода
[17:21.520 --> 17:26.700]  прямого канала, минус энтропии выхода комплементарного канала. Ну, та самая формула для
[17:26.700 --> 17:31.800]  когерентной информации, которую в прошлый раз писали. Здесь немножко ситуация сложнее, потому что
[17:31.800 --> 17:37.960]  у вас здесь, видите, есть e кодирование и d декодирование, и они немножко портят ситуацию, но не сильно.
[17:37.960 --> 17:43.520]  То есть дальнейшие рассуждения направлены на то, чтобы избавиться от этого e, избавиться от этого d,
[17:43.520 --> 17:50.640]  и тогда как раз будет та интерпретация, которую я только что сказал. А чтобы избавиться,
[17:50.640 --> 17:59.880]  тут и свойства когерентной информации возникают. Видите, тут вот d и e пока присутствуют,
[17:59.880 --> 18:07.400]  потом мы от них захотим избавиться. Избавляемся, поскольку когерентная информация...
[18:07.400 --> 18:13.840]  Когерентная информация, то же самое, что квантовая взаимная информация.
[18:13.840 --> 18:29.280]  Нет, смотрите, просто взаимная информация и какой-то канал. Это есть энтропия выхода этого канала,
[18:29.280 --> 18:36.720]  плюс энтропия входа, минус энтропия выхода комплементарного канала. Это просто взаимная
[18:36.720 --> 18:43.120]  информация. А когерентная информация, у нее буква c здесь будет. Наука молодая,
[18:43.120 --> 18:49.600]  обозначения не устоялись. Используйте те, которые в книжке Холлива, чтобы вам было прочее тоже.
[18:49.600 --> 18:57.360]  Поэтому вам, может, не нравится то, что там она называется когерентная? Ну, в общем,
[18:57.360 --> 19:04.760]  то, что люди придумали и использовали, так же ему должно быть. Как я в прошлом семестре вам
[19:04.760 --> 19:09.680]  рассказывал про Фейман, что он там свои обозначения придумал, его никто не понимал.
[19:09.680 --> 19:15.240]  Точно так же и здесь. Надо использовать то, что там у них делают. Тогда у вас все будут...
[19:15.240 --> 19:23.480]  Так, видите, разницу в чем? Разница заключается в том, что из взаимной информации мы вычитаем
[19:23.480 --> 19:39.800]  ms.rho. Это немного другая величина. Хорошо. Вот что мы хотим с вами сделать. Избавиться от
[19:39.800 --> 19:46.440]  вот этих кодирований и декодирований и в итоге получить величину, которая ниже представлен.
[19:46.440 --> 19:54.200]  Ну, делается это с помощью там некоторых свойств. Когерентная информация удовлетворяет
[19:54.200 --> 20:00.840]  такому свойству. То есть можно выкинуть второй канал ФИ2. Тогда у вас получается,
[20:00.840 --> 20:07.160]  что можно выкинуть вот эти декодирования. Останется просто ФИ и кодирование на входе.
[20:07.160 --> 20:18.920]  А кодирование на входе, это была аизометрия. Она энтропию не меняет. Поэтому у вас тоже будет
[20:18.920 --> 20:26.480]  свойство то, которое нужно. Вот здесь написано, что EN это аизометрия. Энтропия не меняется под
[20:26.480 --> 20:32.240]  действием аизометрии. Это аналог унитарного преобразования только с разными размерностями входа.
[20:32.240 --> 20:37.040]  А изометрию мы не с двух сторон разве должны были бы умножать?
[20:37.040 --> 20:43.120]  Да, это вот вопрос вот как раз задавался Тослов в прошлый раз, по-моему. Почему энтропия,
[20:43.120 --> 20:52.120]  например, некоторого состояния РО равняется энтропии состояния W РО W КВЕСТИ, где W аизометрии.
[20:52.120 --> 21:04.840]  Но аизометрическое отображение. Это понятно. В смысле, EN это и есть действие на РО,
[21:04.840 --> 21:17.160]  это и есть по определению W РО W КВЕСТИ. А, хорошо. E это отображение. Это конконтинация
[21:17.160 --> 21:22.600]  отображения. E отображение, F отображение, D отображение. Все вместе тоже какое-то отображение.
[21:22.600 --> 21:41.000]  На экзамене этой части не будет, поэтому можете особо не вникать, если не хотите прям заниматься
[21:41.000 --> 21:47.720]  этими вещами. Но для понимания, вот что получается, что из когерентной информации вы можете выкинуть
[21:47.720 --> 21:55.640]  вот эти декодирования, декодирования. И вот тут у меня написано в правой части максимум еще поро,
[21:55.640 --> 22:06.000]  но это и будет тогда верхняя граница для той величины, которую мы с вами смотрели. Вот здесь
[22:06.000 --> 22:18.560]  СБ минус С омегарОБ РБ. Вот верхняя граница для нее, она на вот этом слайде и пристав. А как мы
[22:18.560 --> 22:26.960]  еще раз правую часть убрали? То есть декодирование мы убрали по свойству, которое вверху. А кодирование
[22:26.960 --> 22:54.920]  как убрали? Так, как мы убрали? Тут немножко хитрее у меня написано. Давайте, раз такой вопрос есть,
[22:54.920 --> 23:05.000]  доведем до конца. Значит, смотрите, вот здесь вот РОА, это была матрица плотности какого размера? К на К.
[23:05.000 --> 23:15.480]  Смотрите, это как раз таки в этой схеме, в исходной. А, ну, можно на эту схему смотреть, можно на
[23:15.480 --> 23:27.160]  вот эту схему смотреть. Видите, тут была размерность К. Вот эта РОА, которая у меня там написана, это
[23:27.160 --> 23:35.760]  есть матрица размером К на К. Дальше, что мы с вами сделаем? С того как мы обложим ее слева-справа
[23:35.960 --> 23:51.520]  вот этим W, то есть применим кодирование, мы получим уже матрицу плотности размера 2 в степени N на 2 в степени N.
[23:51.520 --> 24:02.240]  Эту матрицу плотности давайте назовем буквой РОА. И тогда мы можем избавиться от этого Е и написать
[24:02.240 --> 24:16.320]  здесь просто РОА. Вы можете меня спросить, а почему так? Ведь в когерентной информации есть еще
[24:16.320 --> 24:32.720]  комплементарный канал. Вот здесь пустое место есть. Смотрите, это энтропия выхода прямого канала,
[24:32.720 --> 24:47.600]  РОА. Это значит, я вот это заменяю просто на РОА. Отнять энтропию выхода комплементарного канала. То есть
[24:47.600 --> 25:01.160]  я должен написать ФИ тензор на НЕН, комплементарный действует на РОА. Вы можете спросить, а почему? Здесь-то
[25:01.160 --> 25:14.520]  понятно, мы заменили на РОА, а тут как так можно сделать? Но вы видите, что есть такое свойство. Вы
[25:14.520 --> 25:23.600]  оператор Крауса для ЕН. ЕН содержит только один оператор Краус. Если вы помните, как определяется
[25:23.600 --> 25:31.600]  комплементарный канал, там с оператором Крауса он записывается. Вы увидите, что поскольку ЕН изометрия,
[25:31.600 --> 25:40.360]  то вот эта величина, которая здесь написана, она в точность совпадает с ФИ тензор на Н комплементарное.
[25:40.360 --> 25:54.920]  Так, а тут что мне надо написать? Тут мне нужно написать вот так вот ЕН РОА. Почему? Поскольку ЕН
[25:54.920 --> 26:13.120]  содержит один оператор Краус. И снова получается РОА, ой, просто РОА. Снова получается РОА. А если кратко
[26:13.120 --> 26:21.400]  тут так, и вот РОА у вас остается, и вы можете взять максимум по этому РОА, и это будет верхняя
[26:21.400 --> 26:33.560]  граница для вашей когеретной информации. Вот так вот избавляемся от кодирования. Вот верхнюю
[26:33.560 --> 26:42.520]  границу мы теперь получили. Дальше что мы можем сделать? Воспользоваться вот этим вот пределом,
[26:42.520 --> 26:48.240]  ладарифм К делить на Н не превосходит, предел один делить на Н. Вот верхняя граница для этого
[26:48.240 --> 26:57.280]  выражения. Это есть теперь вот этот вот голубенький рамочек. Все, вот эта вот верхняя граница,
[26:57.280 --> 27:07.080]  мы ее с вами получили. А дальше то, что эта граница достигается при энтисеме и бесконечности,
[27:07.080 --> 27:17.320]  это намного более сложное доказательство, которое было закончено девятаком только в 2005 году.
[27:17.320 --> 27:26.960]  Окончательное доказательство было представлено, что эта граница достигается. Вот получается,
[27:26.960 --> 27:33.360]  что с 2005 года мы знаем вот такое выражение для квантовой пропускной способности канала,
[27:33.360 --> 27:39.200]  то есть Q-фи задается вот такой вот формулой. Еще говорят, что это формула регулиризованная,
[27:39.200 --> 27:46.240]  поскольку есть один делить на Н и вот этот предел. Теперь здесь те же самые рассуждения про
[27:46.240 --> 27:54.160]  аддитивность и так далее, которые стандартно возникают. В общем случае, аддитивности нет.
[27:54.160 --> 28:01.400]  Примеры отсутствия аддитивности к эгерентной информации это последовательное применение
[28:01.400 --> 28:08.400]  дефазирующего и стирающего каналов. Это вот недавняя работа Ледицкого в 2018 году. Есть вот
[28:08.400 --> 28:13.320]  еще такой класс обобщенных стирающих каналов, это я вот сделал в прошлом году,
[28:13.320 --> 28:23.160]  journal physics 8, у меня статья есть. То есть аддитивности нет для широкого класса каналов. В общем случае
[28:23.160 --> 28:31.160]  получается, что, например, если вы возьмете здесь двойку в этом выражении, вы получите величину
[28:31.160 --> 28:41.200]  больше, чем когда возьмете единичку. Ну это вот в этой рамке написано. Таких примеров много,
[28:41.200 --> 28:49.600]  но есть случаи, когда все-таки аддитивность к эгерентной информации имеет место. Она имеет
[28:49.600 --> 28:56.680]  место для деградируемых каналов. Эгерентная информация аддитивна. Доказательство есть в книжке Холева,
[28:56.680 --> 29:09.900]  оно же есть и в статье Шора, по-моему, из девятак, как раз таки, 2008 года. Аддитивность для деградируемых
[29:09.900 --> 29:18.620]  каналов-то оно несложное, просто я не хочу тратить время на это. А для антидеградируемых каналов у вас
[29:18.620 --> 29:24.820]  получается нулевая пропускная способность. Так, это было у вас на семинарах. Я знаю, что я рассказывал
[29:24.820 --> 29:34.180]  для 813 группы, а для других, для 11-12 были у вас деградируемые антидеградируемые каналы. Просто
[29:34.180 --> 29:45.500]  название их и определение были точно. А вы хотите так чуть-чуть побольше обсудить их, да, немножко?
[29:45.500 --> 29:55.180]  А, ну да, можно. Было бы неплохо. Ну, в общем, давайте на примере расскажу. Другой цвет ещё возьму.
[29:55.180 --> 30:04.900]  Вот представьте, что у вас, ну давайте начнём с того, что такое прямой канал. Прямой канал
[30:04.900 --> 30:13.380]  показывает, что будет на выходе, если вы какой-то вход пошлют. А комплементарный канал показывает,
[30:13.380 --> 30:19.540]  что при этом растворится в окружении, то есть какая информация об исходном состоянии РО останется
[30:19.540 --> 30:31.420]  в окружении. Вот теперь вы можете сказать, например, канал будет сильно шумящим, если в окружении
[30:31.420 --> 30:37.300]  растворяется больше информации, чем проходит. Вот так вы скажете. Тогда канал сильно шумит.
[30:37.300 --> 30:43.140]  Но что это означает с точки зрения математики? Это означает, что вы, например, можете из вот
[30:43.140 --> 30:50.340]  этого состояния получить состояние на выходе прямого канала. То есть если вот эта вот стрелочка,
[30:50.340 --> 30:58.460]  я сейчас её жирной сделаю, если вот эта стрелочка работает, то есть если вы из состояния окружения
[30:58.460 --> 31:05.540]  можете получить выход канала своего прямого, то это означает, что в окружении больше информации
[31:05.540 --> 31:13.540]  содержится. Это означает, что это вот будет как раз-таки антидеградированный случай, то есть что
[31:13.540 --> 31:19.540]  выход прямого канала можно представить как конкатинацию выхода комплементарного и какого-то
[31:19.540 --> 31:25.140]  вспомогательного канала тета. То есть вот эта стрелочка — это тет. То есть это сильно шумящий канал.
[31:25.140 --> 31:33.900]  А как так получается, что мы из информации... Сейчас, вот у нас была информация какая-то, которую мы
[31:33.900 --> 31:41.380]  послали, а потом мы говорим, что из информации, которая растворилась в environment, мы можем
[31:41.380 --> 31:48.340]  восстановить информацию, которую получили на выходе, да? Это значит, что у нас информация дублируется
[31:48.340 --> 31:55.300]  как-то. Вот-вот-вот-вот. Вы и пришли к тому, что я хочу сказать. Значит, получается, смотрите как.
[31:55.300 --> 32:04.300]  Если, да, вы в этом случае можете эту информацию дублировать, то есть клонировать, правильно? Это что
[32:04.300 --> 32:11.220]  означает? Вот смотрите внизу объяснение красным шрифтом. Это вот как раз тот сценарий, который вы
[32:11.220 --> 32:19.500]  запрашиваете. Допустим, пропускная способность канала phi больше нуля. Это означает, что если я использую
[32:19.500 --> 32:25.980]  его много раз в пределе энстеначности к бесконечности, я могу чистое состояние psi воспроизвести с
[32:25.980 --> 32:32.380]  напереданной точностью. Вот здесь вот, правильно? Но поскольку в окружении растворяется больше
[32:32.380 --> 32:38.820]  информации, чем проходит вперед, то я его могу и вот здесь вот восстановить. Понимаете?
[32:41.220 --> 32:51.300]  И тогда у нас возникает противоречие с чем? С теоремой запрете клонирования. Нельзя произвольное
[32:51.300 --> 32:57.700]  состояние psi воспроизвести в двух местах для любого psi. Поскольку клонирования у нас нет,
[32:57.700 --> 33:07.620]  то значит пропускная способность равняется нулю квантовая. Видите, что получается? То есть не существуют
[33:07.620 --> 33:15.500]  таких кодирований и декодирований, сколько бы раз вы не применяли канал, чтобы воспроизвести вот
[33:15.500 --> 33:26.500]  эти вот состояния psi, как на выходе канала, так и в… Ну да, на выходе канала. Пример здесь такой,
[33:26.500 --> 33:35.900]  например, стирающий канал. Если вы разбирали на… Там есть параметр p, вероятность того, что состояние
[33:35.900 --> 33:41.500]  будет задетектировано. Так вот, график для q выглядит таким образом. Сначала это просто ноль,
[33:41.500 --> 33:45.540]  при малых значениях p, вероятность успешного детектирования, а потом начинает расти.
[33:45.540 --> 33:54.220]  Любое значение для пропускной способности q как раз-таки означает, что шум настолько сильный,
[33:54.220 --> 34:02.580]  что вы не можете воспроизвести состояние… Ну, короче, не можете исправлять ошибки.
[34:02.580 --> 34:13.100]  Шум очень сильный. Так, про антидеградируемые более-менее понятно? Это сильно шумящие каналы.
[34:13.100 --> 34:18.660]  Антидеградируемые, сильно шумящие, квантовая пропускная способность для них равна нулю.
[34:18.660 --> 34:27.420]  А правда, что мы выход комплемендарного канала никогда не видим и не фиксируем никак? Мы просто
[34:27.500 --> 34:40.620]  используем его как модель. Да, тоже хорошо. Спасибо за вопрос. Сейчас скажу, почему это. Радуюсь.
[34:40.620 --> 34:47.780]  Радуюсь потому, что в конце прошлого года я делал там доклад в Мяне, и как раз-таки Холливый говорит,
[34:47.780 --> 34:54.860]  что окружение можно считать вообще по-разному, рассматривать окружение. Окружение – это не
[34:54.860 --> 35:02.860]  обязательно вот та физическая среда, которая окружает оптоволокно, когда вы по нему пересылаете
[35:02.860 --> 35:08.260]  сигнал. А в качестве окружения можно рассматривать еще и действие перехватчика, например. То есть
[35:08.260 --> 35:14.780]  окружение – это такое растяжимое понятие. То есть вот эта вот фии с волной, комплементарный канал,
[35:14.780 --> 35:22.620]  он показывает еще что? Возможные вмешательства в ваш канал связи и так далее. То есть вы же не
[35:22.620 --> 35:32.580]  знаете, за счет чего у вас возникает шум в линии. Вот шум может возникать из-за того, что кто-то
[35:32.580 --> 35:41.020]  вот пытается в ваш канал включиться, подсоединиться. Буква И – она не только environment может
[35:41.020 --> 35:51.420]  обозначать, но и eavesdropper – тот, кто вас подслушит. То есть в принципе вы правы, если у вас окружение
[35:52.060 --> 35:59.580]  представляет собой реальный физический объект, например, какие-то моды электромагнитного излучения,
[35:59.580 --> 36:05.540]  которые связывают с оптоволокном и так далее, то вы к ним доступа не имеете, но и никто другой не
[36:05.540 --> 36:12.740]  имеет доступа. Но в принципе шумы могут возникать из-за того, что кто-то в вашу линию связи внедряется.
[36:12.740 --> 36:21.100]  Если у вас есть только вход и выход, вы не знаете, как устроена промежуточная часть, то вы в худшем
[36:21.100 --> 36:26.500]  сценарии должны всегда предполагать, что это кто-то внедрился в ваш канал и пытается его как-то там
[36:26.500 --> 36:34.980]  подслушать, еще что-то. Значит у него будет у этого человека вот такая вот информация фикомплементарная.
[36:34.980 --> 36:46.580]  Так, про это был вопрос или не про это? Про это. Тут логика такая, для нас канал – это некоторый
[36:46.580 --> 36:53.500]  blackbox. Мы знаем, что происходит на выходе из него, но как его внутренняя структура устроена, мы не знаем.
[36:53.500 --> 37:03.700]  В худшем случае это кто-то нас там пытается подслушать. Так, а вот деградируемый канал – это мало
[37:03.700 --> 37:14.980]  шумящее. В каком смысле? В том смысле, что вы из выхода прямого канала можете получить выход
[37:14.980 --> 37:22.100]  комплементарно. Вот тут мне тоже написано, шумы не сильные, и в этом случае есть аддитивность
[37:22.100 --> 37:34.220]  когерентной информации. И формула тогда упрощается. Формула тогда, вот этот предел, который
[37:34.220 --> 37:43.420]  здесь написан, равняется просто максимуму по ρ и c, ρ, φ, пропадает тензорная степень, пропадает
[37:43.420 --> 37:58.300]  вот это деление на n, если φ является деградируемым каналом. Так, ну в принципе, вот в этой первой
[37:58.300 --> 38:06.460]  части я рассказал то, что хотел рассказать про квантовую пропускную способность. Так, дальше у
[38:06.460 --> 38:15.700]  меня пойдут тензорные сети. Давайте вопросики еще. Да, можно еще вопрос тогда. Так, можно еще
[38:15.700 --> 38:22.180]  раз про то, что вот мы сказали, если у нас канал деградируемый или антидеградируемый, то мы,
[38:22.180 --> 38:35.540]  получается, можем клонировать состояние. Давайте вот разделим. Если, допустим, вот так, сейчас возьму
[38:35.540 --> 38:46.940]  указку. Вот тут многоходовка. Я плохо рассказываю о многоходовке, сейчас постараюсь еще раз. Вот
[38:46.940 --> 38:56.820]  представим, что выполнены два условия. Канал антидеградируемый, то есть сильно шумит, вот выполнено
[38:56.820 --> 39:04.460]  вот такое свойство. И его пропускная способность больше нуля. То есть два условия выполнены. И одно и
[39:04.460 --> 39:13.580]  другое. Он и антидеградируемый, и q больше нуля. Когда мы получим противоречие? В чем будет
[39:13.580 --> 39:22.580]  заключаться это противоречие? Если канал сильно шумит, значит, у вас информации в окружении больше,
[39:22.580 --> 39:28.940]  чем на выходе из канала. А если q больше нуля, то вот здесь, на выходе из канала, вы уже можете
[39:28.940 --> 39:35.900]  восстанавливать psi с помощью некоторых операций кодирования деградитации. Но раз здесь phi rho содержит
[39:35.900 --> 39:39.660]  меньше информации, чем phi-комплементарный ров, то здесь тоже можно восстановить psi.
[39:39.660 --> 39:49.580]  Эти внизу тоже можно восстановить. Получается, что psi можно восстановить и там, и там. Значит,
[39:49.580 --> 39:54.540]  мы можем клонировать. А клонировать нельзя противоречие. Какое из этих двух утверждений
[39:54.540 --> 40:01.380]  неверно? Один или два? Или оба? Первое, это просто фиксировать свойство канала. Неправильно было
[40:01.380 --> 40:07.980]  наше предположение о том, что q больше нуля. Значит q равно нулю. Поэтому квантовая пропускная
[40:07.980 --> 40:13.740]  способность для антидеградированного канала равна нулю. Все эти рассуждения только для
[40:13.740 --> 40:24.220]  антидеградированного. Все, хорошо, я понял. А если у вас он просто деградируемый, то никакого
[40:24.220 --> 40:33.300]  противоречия нет. Если вы допустим, что q больше нуля, ну тогда что означает? Что вы берете psi,
[40:33.300 --> 40:39.620]  здесь воспроизводите psi, а здесь меньше информации. Укружение растворяется. То есть,
[40:39.620 --> 40:47.420]  может быть, вы psi не можете здесь получить. Ну как бы никакого противоречия нет. Но что хорошо
[40:47.420 --> 40:54.380]  для деградируемых каналов? Для деградируемых каналов хорошо то, что аддитивность есть для
[40:54.380 --> 41:00.240]  кигаретной информации. И тогда формула, вот эта вот сложная формула под середине слайда,
[41:00.240 --> 41:08.240]  преобразуется в простую формулу, которая справа написана. И вот это вы используете при решении
[41:08.240 --> 41:14.320]  домашней задачи для дефазирующего канала. Дефазирующий канал удовлетворяет такому свойству.
[41:14.320 --> 41:26.240]  Он является деградируемым. Ответил или не очень? Да, ответили. Так, еще вопросики есть про квантовую
[41:26.240 --> 41:37.160]  пропускную способность? Или все в предвкушении уже тензорных диаграмм и так далее. Видимо в
[41:37.160 --> 41:46.880]  предвкушении. Поехали тогда. Сейчас будет много картинок, поэтому и оправдана наша лекция онлайн,
[41:46.880 --> 41:55.520]  потому что рисовать эти картинки долго, проще тут уже показать на экране. Так, квантовые тензорные
[41:55.520 --> 42:02.320]  сети. Значит, тензорные диаграммы просто осложны. То, что раньше вы изучали, можно представить в виде
[42:02.320 --> 42:09.080]  простых картинок. Вот, например, тензор первого ранга или первого порядка, как говорят, это будем
[42:09.080 --> 42:22.000]  изображать в виде объекта с одной палочкой. Индекс, который пробегает, это палочка. Вот здесь
[42:22.000 --> 42:30.120]  нарисована у меня ситуация, когда вы различаете Contra и Co вариантные тензоры. Например, Contra это
[42:30.120 --> 42:37.800]  будут ножки вверх, а Co это ножки вниз. Все зависит от метрики. У вас метрический тензор это единичная
[42:37.800 --> 42:43.960]  матрица или не единичная. У нас потом, в нашем курсе, будет единичная метрическая, метрический
[42:43.960 --> 42:49.000]  тензор и единичная матрица издаваться, поэтому нам будет без разницы, куда ножки будут. Я просто
[42:49.000 --> 42:56.720]  хочу показать вам, как формулы из вашей теории поля, которую вы изучали год назад, можно записать
[42:56.720 --> 43:03.080]  в виде тензорных диаграмм в простом виде. Так, вот этот слайд понятен. Графическое представление вот
[43:03.080 --> 43:12.240]  такого тензора с компонентом. Пока просто картинка. Теперь вы можете Co вариантный тензор написать
[43:12.240 --> 43:20.040]  вот так. Ножку вниз. Дальше что вы можете сделать? Вы можете их объединить. Если вы их объединяете в один
[43:20.040 --> 43:26.840]  тензор C, то это будет тензор второго ранга. Ранг определяется количеством ножек, которые ни на
[43:26.840 --> 43:33.160]  что не замкнуты. Свободные ножки это и есть. Количество свободных ножек это и есть ранг тензора.
[43:33.160 --> 43:42.480]  Видите, раньше были у вас в курсе теории поля вот эти вот индексы И, Ж, вверху, внизу, путаница была,
[43:42.480 --> 43:51.440]  как там поднимать индекс, как там опускать. Здесь пока все понятно. Вы те ножки, которые смотрят
[43:51.440 --> 43:57.480]  вверх, это верхний индекс, те ножки, которые смотрят вниз, это вот тензор второго ранга. Если вы,
[43:57.480 --> 44:04.240]  например, их соедините в другом порядке, то есть соедините ноги, то тогда у вас что получится? У
[44:04.240 --> 44:12.480]  вас получится одна нога, но она замкнута на вот эти картинки А и В квадратики. Поэтому индекс
[44:12.480 --> 44:22.600]  уведется с суммированием. То есть как только у вас возникает АИТ, БИТ, индекс вот этот И совпадающий,
[44:22.600 --> 44:28.520]  по нему уведется суммирование правила Эйнштейна. Помните, что мы в результате получаем? Мы получаем
[44:28.520 --> 44:33.080]  вот этот вот тензор D, у которого нет ножек, смотрящих наружу. А если нет ножек, смотрящих на
[44:33.080 --> 44:43.120] ружу, значит, тензор нулевого ранга. Пока понятно? Да. Да, только я забыл, что такое коврянтный, чем
[44:43.120 --> 44:52.240]  означается коврянтный. У них правила преобразования разные, но сейчас не будем про это. Присмотрите. Если вы возьмете,
[44:52.240 --> 44:57.980]  например, и замкнете вот эти ножки А и Б, то вы получите тот же самый объект, что вот справа на
[44:57.980 --> 45:08.820]  нему нарисован. Но это вы просто взяли след у матрицы, которая тут была. Ну ладно. Вот тензор третьего ранга,
[45:08.820 --> 45:17.900]  например, нарисован. Т, И, Ж, К. Если вы, например, замкнете ножки вот так вот, то это называется еще
[45:17.900 --> 45:27.100]  свертка тензора по индексам ЖК. Значит, пишем Ж, Ж, суммирование по Ж, получаем тензор ранга 1. Понятно?
[45:27.100 --> 45:38.580]  По-моему, все очень понятно. Так, дальше. Ну вот, например, символ левичьего вито E, I, G, K, L для
[45:38.580 --> 45:48.100]  размеров с 4. Ну, это тензор четвертого ранга. Вы, когда его, например, соединяете с тензором F и G,
[45:48.100 --> 45:54.740]  тензором электромагнитного воля, вы получаете с коэффициентом 1,2. Вы получаете вот такой вот объект F,
[45:54.740 --> 46:05.060]  F, K, L. Видите? K, L, ножки смотрящие вверх. Все вместе, сейчас выделю. Все вместе. Это есть тензор второго ранга.
[46:05.060 --> 46:14.020]  Видите, потому что ноги K, L наружу, а все остальные суммируются. Это будет у вас дуальный тензор, тот самый,
[46:14.020 --> 46:20.900]  который в теории поля вы изучали год назад. То есть все те формулы, которые были год назад,
[46:20.900 --> 46:26.540]  можно записать в виде таких простых диагнозов. Теперь вот метрический тензор. Что значит, например,
[46:26.540 --> 46:34.580]  опустить индекс? Вот у вас был AIT, контравариантный тензор. Вы хотите сделать так, чтобы ножка
[46:34.580 --> 46:44.660]  смотрела вниз. Тогда вы применяете вот этот G и Gt. Это будет у вас метрический тензор.
[46:44.660 --> 46:56.700]  Если так напишете, то понятно. Свертка вот такая. У вас как раз таки переставляет верхний индекс
[46:56.700 --> 47:03.380]  сделать его нижним. Если сделаете G и Gt, где и наверху, и внизу, то это будет символ экрана.
[47:03.380 --> 47:17.020]  Все очень просто. В нашем случае нет. Вот это G будет задаваться единичной матрице. Поэтому
[47:17.020 --> 47:25.140]  нам неважно будет, куда смотрят ножки. Вверх-вниз, это все без разницы. Поэтому мы выявили две метрики,
[47:25.140 --> 47:32.140]  куда ножки смотрят нам без разницы. Правого вниз, влево. Теперь давайте поговорим,
[47:32.900 --> 47:40.100]  какие свойства здесь есть. Вот как записать матрицу. Матрицу можно записать в виде тензора второго
[47:40.100 --> 47:47.540]  ранга. M и Gt, это есть матричный элемент. Теперь, если у вас есть произведение матриц,
[47:47.540 --> 47:55.580]  то тогда вы можете взять M на N произведение. И, тк, и, элемент. Это есть сумма по G. M и Gt,
[47:55.580 --> 48:03.100]  N и Gt. Это первый курс формулы. Произведение матриц это будет просто соединение вот таких
[48:03.100 --> 48:16.020]  вот диаграммах для M и для N. Справа, которая показывает. Это понятно? Да. С недавних пор я
[48:16.020 --> 48:23.980]  начал немножко по-другому еще делать. Когда у вас есть вот такая вот картинка M с ножком,
[48:23.980 --> 48:29.380]  то не очень понятно, какая нога отвечает за нумерацию строк, а какая нога отвечает за
[48:29.380 --> 48:37.740]  нумерацию столбцов. Чтобы избежать этого, можно вот так вот написать, что это есть сумма по
[48:37.740 --> 48:47.860]  G, M и Gt. Тут, например, у вас будет ket и braji. Вот эта вот запись пока понятна? Есть M, как оператор.
[48:47.860 --> 48:58.660]  Понятно. Вот та ket ножка, которая выходит, я обычно теперь рисую стрелочкой исходящей. А та нога,
[48:58.660 --> 49:06.340]  которая соответствует bra компонентам, стрелочкой входящей. Это вот bra, а вот это вот ket. Значит,
[49:06.340 --> 49:11.460]  здесь будет i, а здесь будет j. Чем удобны эти стрелочки? Эти стрелочки удобны тем,
[49:11.460 --> 49:16.020]  что они показывают еще порядок произведения матрицы. То есть, если мы вот здесь нарисуем
[49:16.020 --> 49:24.900]  эти стрелочки, то они будут показывать сначала N, потом левее M. Вот так вот. Все очень просто.
[49:24.900 --> 49:30.620]  Значит, здесь слева еще показано, что... Взятия следа. То есть, если вы вот так вот
[49:30.620 --> 49:39.940]  соедините эти ноги, то вы получите след M, сумма по E, M и Gt. Очень хорошо. Это все произведение.
[49:39.940 --> 49:48.300]  След тут я тоже же сказал. Что значит транспонировать матрицу? В левом нижнем углу написано. Это значит
[49:48.300 --> 49:57.980]  ноги ее закрутить в другие стороны. Либо вот так вот сделать. Если у вас есть картинка M и вот эти
[49:57.980 --> 50:06.980]  стрелочки, то вы хотите поменять направление стрелочек. Но если вы меняете направление стрелочек,
[50:06.980 --> 50:17.860]  то внутри вы должны уже поставить M-транспонирование. След от произведения матриц. Здесь вот картинка.
[50:17.860 --> 50:27.220]  Все очень просто. Вы можете перетащить эту матрицу M, например, по красной стрелке. Она станет справа
[50:27.220 --> 50:34.940]  от матрицы N. И вы в диаграммном языке очень просто доказали вот это вот свойство. Что след
[50:34.940 --> 50:42.260]  произведения матриц? Равняется след произведения матриц в обратном порядке. Теперь тензорное
[50:42.260 --> 50:50.740]  произведение. Справа внизу картинка. Есть матрица M с элементами I, G. Есть матрица N с элементами KL.
[50:50.740 --> 50:58.340]  Вот когда мы просто их эти элементы умножаем, это все равно, что мы на тензорном языке записываем
[50:58.340 --> 51:11.140]  вот эти M, N рядом с другом. Получаем некоторую матрицу T, I, G, KL, где теперь I, K это один мультииндекс,
[51:11.140 --> 51:19.380]  а G, L это другой мультииндекс. Что мы получаем? Так, про тензорное произведение, наверное,
[51:19.380 --> 51:27.140]  надо вам немножко еще осмыслить. Давайте вопрос какой-нибудь. А мы обязательно рисуем прямоугольник
[51:27.140 --> 51:34.900]  вокруг M, N? Нет, любую картинку, может треугольничек. Нет, в смысле, что-то рисуем обязательно вокруг.
[51:34.900 --> 51:47.740]  Ну обычно да, очень да. Сейчас-то почему еще раз мы можем записать все внизу? За это все,
[51:47.740 --> 51:54.940]  из-за единичной метрики. Да-да-да, теперь неважно какие индексы вверху, какие внизу. А почему
[51:54.940 --> 52:05.740]  транспонирование вот так выглядит? Вот эти ноги менять местами или как? Или наверху? Нет,
[52:05.740 --> 52:14.540]  вот эти ноги транспонирование матрицы. А, только вам нужно поменять направление с строчек и столбцов.
[52:14.540 --> 52:24.180]  Давайте я стрелочками покажу. Вот раньше у вас было вот так, это была исходная матрица. Теперь
[52:24.180 --> 52:32.740]  у вас ноги будут выходить в другую сторону. Я думал, это мы ее зациклили, взяли слет и еще полку сверху положили.
[52:32.740 --> 52:38.700]  Нет-нет-нет, тут пересечение. Вот тут вот нет-нет-нет, это у меня ноги так пересекаются, когда рисовал.
[52:38.700 --> 52:50.820]  Хорошо. Ну, поняли, да? Тут нет. Так, мне бы пустой слайд еще сделать. Давайте знаете,
[52:50.820 --> 52:57.820]  как я сделал. Я сейчас ставлю пустой слайд, потому что хотел вам кое-что рассказать про
[52:57.820 --> 53:20.700]  тензорное произведение еще. Сделал стекущего слайда. Вот, про тензорное произведение еще
[53:20.700 --> 53:28.380]  чуть-чуть расскажу. Вот у вас, давайте в другую сторону нарисую, чтобы было более понятно. Вот,
[53:28.380 --> 53:35.860]  например, матрица M, матрица N. Вот тот объект, который сейчас есть, это объект четвертого ранга.
[53:35.860 --> 53:46.420]  Мы его с вами трактуем как тензорное произведение M на N. Видите, получается, что M это, грубо говоря,
[53:46.420 --> 53:51.540]  пояс, который по рельсам двигается верхним, N это пояс, который по рельсам движется нижним.
[53:51.540 --> 53:57.700]  Если вы их сдвинете, то ничего в принципе в плане картинки диаграммной не поменяется. Вот,
[53:57.700 --> 54:07.180]  например, вот так вот я сделал. Это та же самая картинка. Мы же ничего не сделали, просто линии
[54:07.180 --> 54:12.060]  продлили и сдвинули немножко тензор. Но при этом мы с вами получили некоторые свойства. Если я
[54:12.060 --> 54:18.380]  посмотрю на вот этот объект, что я здесь вижу? Я вижу здесь единичную матрицу и вот здесь тоже
[54:18.380 --> 54:28.500]  вижу внизу единичную. Получается, что M тензор на N равняется, ну, например, и тензор на N,
[54:28.500 --> 54:36.060]  а вот M тензор на N. Видите, простые выражения можно доказать с помощью этих тензорных диаграмм
[54:36.060 --> 54:47.020]  очень легко. Шок. Что еще? В такой момент у нас появилось произведение матриц. Вот на картинке
[54:47.020 --> 54:54.660]  произведение, что соответствует. Ага, да, сейчас сделаем. Вот M, это у вас была матрица, у нее были
[54:54.660 --> 55:04.620]  индексы IJ, да? Вот давайте я их пропишу. У N, например, KL. Вот когда мы два объекта вот так написали,
[55:05.020 --> 55:12.900]  мы и записали вот этот тензор четвертого ранга. Там, по-моему, на предыдущем слайде это T было. T и JKL.
[55:12.900 --> 55:18.740]  Так, где возникло произведение матрицы?
[55:18.740 --> 55:34.900]  Не знаю, где оно возникло. Ну вот, вы сказали, что это единичный на N произведение матриц.
[55:34.900 --> 55:44.860]  Так вот же оно, вот оно. Вот, смотрите, вот это уже было понятно. Вот когда мы соединяем M и N,
[55:44.860 --> 55:52.420]  вот это отдельно две матрицы. Вот матрица M, матрица N. Когда я соединяю линии, то я получаю
[55:52.420 --> 56:01.420]  произведение. Я соединяю их линиями, а линия тут и так есть. Вот я и получил произведение.
[56:01.420 --> 56:13.140]  Так, и почему сейчас там одна линия была, и это было суммирование по индексу. А, здесь
[56:13.140 --> 56:21.140]  суммирование по 2 индексам получается. Да, да, вот про это я и хотел сказать. Смотрите, у тензоров
[56:21.140 --> 56:32.500]  есть, бывает, форма. Форма тензора. Это, грубо говоря, сколько ножек торчит и какие измерения для
[56:32.500 --> 56:38.780]  каждой ножки возможны. То есть вот сколько значений индекса каждый пробегает. J, например, 1, 2, 3, 4, 5.
[56:38.780 --> 56:45.100]  Это форма тензора. Что такое форма тензора? Более-менее понятно, сколько ножек и сколько
[56:45.100 --> 56:52.220]  значений пробегает каждый ножок, индекс каждой ножки. Вот, теперь смотрите, вы эту форму тензора
[56:52.220 --> 57:03.180]  можете менять. Каким образом? Например, вы можете объединить индексы в одну ногу, понимаете, и это
[57:03.180 --> 57:14.980]  тогда даст вам мультииндекс. Мультииндекс. Вы изменили форму тензора. У вас теперь вот это все как одна
[57:14.980 --> 57:23.860]  нога, но то значение индексов, которое пробегает эта нога, есть произведение измерений для каждой
[57:23.860 --> 57:37.380]  ноги. Когда мы записали вот в таком виде, мы и подразумеваем, что мы объединили индексы ИК в один
[57:37.380 --> 57:47.860]  мультииндекс, а JL в другой мультииндекс. Так же, как в прошлом семестре. Да, хорошо.
[57:47.860 --> 57:58.580]  Смотрите, еще у нас появилось такое понятие, как форма тензора, и ее можно менять. Сейчас я
[57:58.580 --> 58:10.660]  промотаю слайды. Вот объединение ЖК в один мультииндекс дает вам такую жирную ногу. Видите,
[58:10.660 --> 58:17.940]  здесь Ж штрих. Это вы поменяли форму. Вы можете, например, местами поменять тут индексы. Тоже
[58:17.940 --> 58:24.980]  форма поменяется у тензора. Про форму тензора можно еще так рассуждать. Вот пусть у вас есть
[58:24.980 --> 58:32.820]  тензор третьего ранда, то есть три ноги и ЖК. Это получается, вы же информатики, математики. Это
[58:32.820 --> 58:39.020]  массив многомерный, правильно? Вы можете элементы этого массива, тут они цветами обозначены черным,
[58:39.020 --> 58:47.140]  один индекс синий, другой красный. По-другому перенумеровать. То есть вместо вот такой вот трехмерной
[58:47.140 --> 58:54.260]  таблицы вы можете все эти данные записать в виде, например, двумерной таблицы такого типа. Но вся
[58:54.260 --> 59:03.700]  информация та же самая, просто вы форму поменяли. Тут форма одна у тензора, а здесь другая. Понятно?
[59:03.700 --> 59:14.460]  А вот то, что я хотел вам еще рассказать, это вот такое простое свойство, которое внизу написано,
[59:14.460 --> 59:22.260]  вот это равенство. Помните, мы его доказывали с вами в прошлом семестре у доски. Там смотрели
[59:22.260 --> 59:29.420]  мультииндекс, там что-то такое выводили. Давайте теперь докажем на языке тензорный диагноз. Значит,
[59:29.420 --> 59:35.020]  давайте с правой части здесь начнем, на этом слайде. Что такое AC и BD? Вот зеленым выделено
[59:35.020 --> 59:43.660]  произведение матрицы AC и другой зеленой рамкой выделено произведение матриц BD. Вот рельсы не
[59:43.660 --> 59:50.860]  пересекаются для этих двух столбиков, поэтому здесь значок тензорного произведения между ними. Вот правая
[59:50.860 --> 59:58.980]  часть, понятно? Да, понятно. Теперь вот сейчас переключу на другой слайд, там будет по-другому
[59:58.980 --> 01:00:12.700]  разбито все это. AB будет вот такой вот формой, ACD вот этой вот нижней формы. Как только вы на желтые
[01:00:12.700 --> 01:00:20.700]  блоки смотрите, желтый блок это AB, вот он верхний, а нижний желтый блок это C тензорным умножить на D,
[01:00:20.700 --> 01:00:32.860]  а красные линии вертикальные в картинке это есть произведение. Оп, доказали с вами формулу.
[01:00:32.860 --> 01:00:40.140]  Видите, как все просто. А наверху тут доказать того, что след от тензорного произведения есть
[01:00:40.140 --> 01:00:46.460]  произведение следов. Короче, тензорный диаграмм может просто всякие матричные соотношения
[01:00:46.460 --> 01:00:54.420]  доказывать. Первый месседж, форму тензора можно менять, второй месседж. Теперь что такое тензорная
[01:00:54.420 --> 01:01:02.500]  сеть? Тензорная сеть это объединение тензоров некоторое, в котором у вас есть свертки. Видите,
[01:01:02.500 --> 01:01:11.340]  вот здесь вот сворачивается, вот здесь вот сворачивается. Потому что эти ножки соединены,
[01:01:11.340 --> 01:01:23.660]  соединяют разные объекты. Какие индексы свободными остаются? Вот одна свободная нога, другая, третья,
[01:01:23.660 --> 01:01:31.540]  четвертая. Еще есть, вроде не вижу. Вот это значит все вместе. Это есть тензор четвертого раунда,
[01:01:31.540 --> 01:01:53.580]  составленный из свертки других тензоров. Как выглядит вот это вот, как мы знаем,
[01:01:53.580 --> 01:02:00.700]  какие свертки делать. Есть граф. Граф показывает, с какими тензоры нужно соединить. Этот граф это
[01:02:00.700 --> 01:02:15.420]  есть структура тензорной сети. Граф сверток, плюс свободные ноги, это и есть архитектура
[01:02:15.420 --> 01:02:30.620]  тензорной сети. Так более-менее понятно? Теперь ваш ключевой вопрос. Зачем это надо? Поясняю на
[01:02:30.620 --> 01:02:39.420]  примере. Допустим, у вас есть многочастичное состояние. Допустим, из N убитых. Значит,
[01:02:39.420 --> 01:02:46.220]  C, которая здесь записана, это есть, видите, и 1 принимает два значения, 0,1, и 2, 0,1, и так далее,
[01:02:46.220 --> 01:02:59.180]  и N, 0,1, и C, и 1, и 2, и N. Это есть как раз таки тензор. Какого раунда? Раунда N. У него N ножик.
[01:02:59.180 --> 01:03:07.020]  При этом сам C, этот объект, как ket-вектор, это есть элемент скалькимерного пространства 2 в
[01:03:07.020 --> 01:03:13.820]  степени N. Размерность гигантская для больших N. Экспоненциально большая размерность. Экспоненциально
[01:03:13.820 --> 01:03:24.940]  большая размерность. Это означает, что чтобы задать общее состояние N убитых, вам нужно задать вот
[01:03:24.940 --> 01:03:33.020]  эти вот комплексные числа, C, и 1, и 2, и так далее, и N. То есть сколько вам нужно действительных
[01:03:33.020 --> 01:03:38.580]  параметров? Это 2 умножить на 2 в степени N. Если комплексные параметры считаем, то это будет 2 в
[01:03:38.580 --> 01:03:45.420]  степени N комплексных параметров. Чтобы задать вот этот объект C, вам нужно экспоненциально много
[01:03:45.420 --> 01:03:55.620]  параметров. Если же вы его представляете в виде вот такой вот свертки, где каждый тензор обладает
[01:03:55.620 --> 01:04:04.860]  малым рангом, то тогда у вас количество параметров необходимых может стать меньше. В этом как раз
[01:04:04.860 --> 01:04:13.620]  таки выиграешь описание в виде тензорных сетей. Вы учитываете корреляции между частицами, например,
[01:04:13.620 --> 01:04:20.020]  корреляция между этой частицей и этой частицей, учитываются посредством вот этого графа. Вот видите,
[01:04:20.020 --> 01:04:28.620]  какие тут сложные корреляции. Однако при этом число параметров, которые используются, будет меньше.
[01:04:28.620 --> 01:04:37.300]  Вот следующая картинка покажет. Тут правда я N заменил на N маленькое, но я думаю вы не запутаетесь.
[01:04:37.300 --> 01:04:44.540]  Здесь вот 2 в степени N комплексных чисел задают общее состояние. Состояние общего
[01:04:44.780 --> 01:04:53.940]  Общее состояние. А вот справа показана свертка таких вот тензорочков. Давайте смотреть. Вот здесь вот этот
[01:04:53.940 --> 01:05:08.420]  тензор третьего ранга. Так, значит смотрите, в правой части нарисована свертка, где граф линейный
[01:05:08.420 --> 01:05:18.420]  соединяет тензорочки маленькие. Каждый маленький тензорочек посерединке задается 2 умножить на R квадрат параметров.
[01:05:18.420 --> 01:05:31.420]  Вот столько параметров для их задания. Это понятно? Нет. А что такое R? R это размерность связи, ну индекс, сколько
[01:05:31.420 --> 01:05:41.420]  индекс, какие значения, сколько значений может принимать индекс в горизонтальной связи. 1, 3 точки R. Видите, вот написано.
[01:05:41.420 --> 01:05:51.420]  1, 3 точки R это те значения, которые может принимать индекс в горизонтальной связи. Так, откуда тогда еще 2? Каблярка квадрата?
[01:05:51.420 --> 01:06:03.420]  А 2, 2 внизу, вот она. А почему мы захотели, чтобы у нас тензор вот так вот выглядел, чтобы он у него горизонтальный?
[01:06:03.420 --> 01:06:16.420]  Хотим, вот так вот. Самый простой граф, линейный граф. А почему на выходе, ну вот почему там, где 2 у нас не R значений?
[01:06:16.420 --> 01:06:24.420]  А потому что это кубиты. Вот смотрите, вот здесь вот, например, смотрите, слева пишу сейчас. И2 это ножка.
[01:06:24.420 --> 01:06:36.420]  Вторая. Она показывает вам возможные состояния второго кубита. Либо он в нулевом состоянии, либо он в состоянии 1.
[01:06:36.420 --> 01:06:42.420]  Я смотрю, например, на ножку ИН. Она тоже может быть либо 0, либо 1.
[01:06:46.420 --> 01:06:55.420]  Вот эти же ножки нарисованы здесь. И2, ИН.
[01:06:55.420 --> 01:07:00.420]  И они пробегают два значения индекса соответствующих ножек.
[01:07:00.420 --> 01:07:15.420]  Хорошо, да. То есть получается, смотрите, слева представлено огромный массив. Это будет N-мерная таблица слева.
[01:07:15.420 --> 01:07:23.420]  Справа представлено много маленьких табличек, по которым ведется свёртка.
[01:07:23.420 --> 01:07:32.420]  Теперь оказывается, оказывается, что представление вот это вот слева, где экспоненциально много параметров,
[01:07:32.420 --> 01:07:43.420]  может замениться вот этим представлением справа, где линейное с ростом N число параметров.
[01:07:43.420 --> 01:07:53.420]  Для некоторых ситуаций, например, для основных состояний локальных гамильтиньянов спина в цепочек или других отрывов.
[01:07:53.420 --> 01:08:02.420]  Сейчас покажу пример, где вот это вот представление имеет место.
[01:08:02.420 --> 01:08:12.420]  Вот тот линейный граф, который мы рассмотрели, он называется, он приводит к состоянию матричного произведения.
[01:08:12.420 --> 01:08:20.420]  По-английски это Matrix Product State, MPS. А это название придумали люди, которые физикой занимаются.
[01:08:20.420 --> 01:08:26.420]  А те люди, которые занимаются сжатием данных, а сжатие здесь видно, как это происходит.
[01:08:27.420 --> 01:08:40.420]  Те люди, которые занимаются сжатием данных, это вот, например, прикладными вещами занимается оселедец в Румскалтехе.
[01:08:40.420 --> 01:08:48.420]  Он ученик тортышников. Они называют это Tensor Train.
[01:08:48.420 --> 01:08:53.420]  То есть те люди, которые занимаются сжатием данных, они называют это Tensor Train.
[01:08:53.420 --> 01:08:59.420]  Из картинки более-менее понятно, почему они так назвали. Поезд из тензоров.
[01:08:59.420 --> 01:09:07.420]  Смотрим еще раз на описание. Вектор ψ задается вот такой вот формулой.
[01:09:07.420 --> 01:09:24.420]  Коэффициенты С здесь записаны. Эти коэффициенты С, как тензоры Н того ранга, могут быть представлены как произведение некоторых матриц.
[01:09:24.420 --> 01:09:43.420]  А откуда берутся эти матрицы? Возьмите желтую область вот здесь и зафиксируйте ножку и зафиксируем значение И.
[01:09:43.420 --> 01:09:52.420]  Если я значение Икатой зафиксировал, ну, например, ноль, то тогда я получу уже матрицу.
[01:09:52.420 --> 01:10:02.420]  Видите, тогда я получаю вот здесь матрицу. Как я назову эту матрицу? Она стоит на окатом месте.
[01:10:02.420 --> 01:10:09.420]  У меня число букв алфавита может быть меньше, чем число кубитов, например, число кубитов тысячи.
[01:10:09.420 --> 01:10:17.420]  Поэтому я буду использовать букву А и здесь ставить К. Это будет как бы мое обозначение для буквы.
[01:10:17.420 --> 01:10:25.420]  И что я еще сделал? Я зафиксировал ножку Икатой, поэтому я запишу вот здесь вот Икатой.
[01:10:25.420 --> 01:10:33.420]  И все вместе это получилась матрица, у которой есть ножка влево, ножка вправо.
[01:10:33.420 --> 01:10:39.420]  Ногу левую обозначим альфа. Ногу правую обозначим альфакат.
[01:10:39.420 --> 01:10:45.420]  Тогда вот этот вот объект, вот он и получится, который здесь внизу представлен.
[01:10:45.420 --> 01:10:53.420]  Это матричные элементы. Матричные элементы объекта желтого.
[01:10:56.420 --> 01:11:01.420]  Так, ну много обозначений, вы могли запутаться. Давайте вопрос.
[01:11:01.420 --> 01:11:09.420]  Короче, это матрица R на R. Да, матрица R на R.
[01:11:09.420 --> 01:11:21.420]  Если вы возьмете крайний элемент, вот здесь вот, и зафиксируете И1, то вы получите матрицу какого размера?
[01:11:21.420 --> 01:11:33.420]  1 на R. Это что такое будет? Это будет строчка. Это будет строка.
[01:11:33.420 --> 01:11:41.420]  Если возьмете правую, то здесь будет матрица размера R на 1. Это будет столбец.
[01:11:41.420 --> 01:11:50.420]  Значит, смотрите, вот эта строка, это есть вот эта матрица. Вот этот столбец, это есть последняя матрица.
[01:11:50.420 --> 01:12:01.420]  Строчка, матрица, матрица и так далее, пока не получим столбец. Это все даст вам просто число.
[01:12:01.420 --> 01:12:07.420]  А потом обучаем на эту цепочку, чтобы она выучила представление?
[01:12:07.420 --> 01:12:16.420]  Да-да-да, это вариационный метод называется. То есть вы можете варьировать параметры вот этих маленьких табличек,
[01:12:16.420 --> 01:12:22.420]  чтобы она представляла ваше состояние ПСИ, которое слева написано.
[01:12:22.420 --> 01:12:28.420]  Она будет точной? Точной мы можем добиться представления? Или все равно приближение?
[01:12:28.420 --> 01:12:34.420]  Нет-нет-нет. Сейчас скажу. Все зависит от степени перепутанности.
[01:12:34.420 --> 01:12:48.420]  Если у вас перепутанность левой части с правой частью мала, то тогда вы можете с наберет заданной точностью это все сделать.
[01:12:48.420 --> 01:12:59.420]  Если перепутанность большая, в смысле этой энтропии перепутанности, то тогда вам нужно R увеличивать для увеличения точности.
[01:12:59.420 --> 01:13:04.420]  То есть, короче, буквовка R подстраивается под состояние ПСИ.
[01:13:04.420 --> 01:13:12.420]  Для некоторых состояний ПСИ R мало, для некоторых состояний ПСИ R большое.
[01:13:12.420 --> 01:13:18.420]  Вот мы будем интересоваться теми состояниями R, для которых R мало.
[01:13:18.420 --> 01:13:22.420]  Что такое R? Давайте вот про это поговорим.
[01:13:22.420 --> 01:13:27.420]  Альфа-ката у вас – это те значения, которые индексы горизонтальных связей принимают.
[01:13:27.420 --> 01:13:31.420]  Эта размерность связей еще называется по-английски bond dimension.
[01:13:31.420 --> 01:13:35.420]  Это R-ката называется bond dimension.
[01:13:35.420 --> 01:13:39.420]  Максимум из этих R-катах – это есть rank MPS.
[01:13:39.420 --> 01:13:43.420]  Это не rank матрицы, а вот так вот называется rank MPS.
[01:13:43.420 --> 01:13:52.420]  Пример.
[01:13:52.420 --> 01:13:59.420]  Если rank MPS равен 1, это означает, что размеры всех этих горизонтальных связей есть единичка,
[01:13:59.420 --> 01:14:04.420]  то тогда это означает, что у вас этих горизонтальных связей вовсе нет.
[01:14:04.420 --> 01:14:06.420]  Потому что суммы нет.
[01:14:06.420 --> 01:14:12.420]  Сумма есть по индексам. Если все индексы зафиксированы,
[01:14:12.420 --> 01:14:18.420]  если все альфы принимают одно значение, то тогда суммы нет.
[01:14:18.420 --> 01:14:24.420]  У вас факторизованное состояние – нет перепутанности.
[01:14:24.420 --> 01:14:29.420]  То есть rank MPS равен единице – это все равно, что состояние не перепутано.
[01:14:29.420 --> 01:14:34.420]  Если rank MPS равен 2, то уже можно описывать перепутанное состояние.
[01:14:34.420 --> 01:14:38.420]  Вот у вас в задании есть задачка про GZ-состояние.
[01:14:38.420 --> 01:14:44.420]  GZ-состояние N-кубитов.
[01:14:44.420 --> 01:14:50.420]  Давайте ее решим.
[01:14:50.420 --> 01:14:54.420]  Значит, нужно… Что это за GZ-состояние N-кубитов?
[01:14:54.420 --> 01:14:56.420]  Оно записывается таким образом.
[01:14:56.420 --> 01:15:04.420]  Один делить на корень из двух. Все нули плюс все единицы.
[01:15:04.420 --> 01:15:09.420]  В прошлом семестре, помните, были в задании 3 нуля, 3 единицы.
[01:15:09.420 --> 01:15:12.420]  Это было GZ-состояние для 3-х кубитов.
[01:15:12.420 --> 01:15:15.420]  А здесь GZ-состояние для N-кубитов.
[01:15:15.420 --> 01:15:21.420]  Вот это состояние перепутанное состояние.
[01:15:21.420 --> 01:15:25.420]  Вот давайте его представим в виде тензорной сети MPS.
[01:15:25.420 --> 01:15:27.420]  Как это сделать?
[01:15:27.420 --> 01:15:29.420]  А вот так вот очень просто.
[01:15:29.420 --> 01:15:33.420]  Вот эту строчку первую задайте таким образом.
[01:15:33.420 --> 01:15:36.420]  Матрицы задайте вот таким образом.
[01:15:36.420 --> 01:15:40.420]  1 0 0 0 или 0 0 0 1.
[01:15:40.420 --> 01:15:43.420]  Ну и столцы окончательно задайте таким образом.
[01:15:43.420 --> 01:15:45.420]  Почему так нужно делать?
[01:15:45.420 --> 01:15:54.420]  Потому что если вы возьмете, например, матрицу какую-то карту с индексом 0,
[01:15:54.420 --> 01:15:57.420]  то она будет у вас 1 0 0.
[01:15:57.420 --> 01:16:03.420]  Если вы возьмете матрицу АК плюс 1 с индексом 1,
[01:16:03.420 --> 01:16:07.420]  то она будет выглядеть вот так вот. 0 0 0 1.
[01:16:07.420 --> 01:16:14.420]  Если вы их перемножите, эти две матрицы, то вы получите нулевую матрицу.
[01:16:14.420 --> 01:16:18.420]  Что означает нулевая матрица?
[01:16:18.420 --> 01:16:34.420]  Это означает, что в состоянии вот таком вот состоянии C и 1 и так далее,
[01:16:34.420 --> 01:16:37.420]  вот дошли до катового места.
[01:16:37.420 --> 01:16:42.420]  На катом месте стоит 0, катая позиция.
[01:16:42.420 --> 01:16:46.420]  На АК плюс 1 месте стоит единичка.
[01:16:46.420 --> 01:16:49.420]  И дальше снова какие-то там индексы.
[01:16:49.420 --> 01:16:57.420]  Но нам не важно. Если встречается 0 и единичка, то коэффициент зануляется.
[01:16:57.420 --> 01:17:01.420]  Теперь поскольку матрица коммутирует, а не диагонально,
[01:17:01.420 --> 01:17:05.420]  то не важно, встретятся ли эти 0 и единичка рядом друг с другом
[01:17:05.420 --> 01:17:08.420]  или вообще будут представлены цепочки.
[01:17:08.420 --> 01:17:14.420]  С какие-то элементы, потом 0, потом какие-то элементы,
[01:17:14.420 --> 01:17:18.420]  потом 1, какие-то элементы, будет равно 0.
[01:17:18.420 --> 01:17:20.420]  А это как раз то, что нам нужно.
[01:17:20.420 --> 01:17:26.420]  Нам нужно, чтобы или все были нулями, или все были единичками, индексы.
[01:17:26.420 --> 01:17:31.420]  Понимаете?
[01:17:31.420 --> 01:17:35.420]  Но это сложно с первого раза въехать.
[01:17:36.420 --> 01:17:40.420]  Тут нам потребуется два значения.
[01:17:40.420 --> 01:17:43.420]  Один на корень из 2 и один на корень из 2.
[01:17:43.420 --> 01:17:47.420]  Вот на самом деле-то нет. Смотрите.
[01:17:47.420 --> 01:17:50.420]  Сколько параметров вам здесь нужно?
[01:17:55.420 --> 01:17:58.420]  Я утверждаю, что 2 в степени n.
[01:17:58.420 --> 01:18:01.420]  Вы правильно говорите.
[01:18:01.420 --> 01:18:05.420]  С 0, 0 и так далее, 0 это 1 делить на корень из 2.
[01:18:05.420 --> 01:18:10.420]  С 1, 1 и так далее, 1 это 1 делить на корень из 2.
[01:18:10.420 --> 01:18:16.420]  А вот С 0, 0, 1 вам же тоже нужно задать?
[01:18:16.420 --> 01:18:19.420]  Должны вы выбрать его равным нулю.
[01:18:19.420 --> 01:18:23.420]  То есть вот в этой таблице n-мерной,
[01:18:23.420 --> 01:18:27.420]  в n-мерной таблице у вас много нулей
[01:18:27.420 --> 01:18:32.420]  и местами есть вот эти вот элементы 1 делить на корень из 2.
[01:18:32.420 --> 01:18:37.420]  Но вы же должны прописать нули, где они должны быть.
[01:18:37.420 --> 01:18:42.420]  Поэтому в этой n-мерной таблице она sparse.
[01:18:42.420 --> 01:18:44.420]  Разреженная.
[01:18:44.420 --> 01:18:47.420]  Да, разреженная получается таблица.
[01:18:47.420 --> 01:18:52.420]  Но, извините, нули там тоже надо прописать.
[01:18:52.420 --> 01:18:58.420]  Когда вы задаете gz-состояние в виде вот такого вот тензорного поезда,
[01:18:58.420 --> 01:19:03.420]  то тогда у вас r равно 2.
[01:19:03.420 --> 01:19:06.420]  И сколько вам нужно здесь задать параметров?
[01:19:06.420 --> 01:19:11.420]  Значит, 2 на r в квадрате и на n.
[01:19:11.420 --> 01:19:15.420]  То есть получается 8n.
[01:19:15.420 --> 01:19:17.420]  Сейчас я отвечу на r2.
[01:19:17.420 --> 01:19:21.420]  А потому что размер матрицы 2 на 2.
[01:19:21.420 --> 01:19:23.420]  Вот эта матрица, это есть r на n.
[01:19:23.420 --> 01:19:25.420]  Да, все понял.
[01:19:27.420 --> 01:19:30.420]  Ну, смотрите, на самом деле мы только что решили
[01:19:30.420 --> 01:19:34.420]  под задачку, домашней задачей.
[01:19:34.420 --> 01:19:36.420]  Одной из задачки задания.
[01:19:36.420 --> 01:19:38.420]  Ну, на семинарах подробнее расскажете.
[01:19:38.420 --> 01:19:42.420]  Для меня главное, чтобы вы понимали общую структуру.
[01:19:42.420 --> 01:19:47.420]  Так, теперь давайте я покажу вам другие тензорные сети
[01:19:47.420 --> 01:19:51.420]  и на этом закончу, потому что 2 минуты и все осталось.
[01:19:51.420 --> 01:19:54.420]  Вот вы могли, например, такую тензорную сеть посмотреть,
[01:19:54.420 --> 01:19:57.420]  где первая соединяется с последней.
[01:19:57.420 --> 01:20:01.420]  Это будут периодически граничные условия,
[01:20:01.420 --> 01:20:03.420]  когда есть такое вот замыкание.
[01:20:03.420 --> 01:20:05.420]  Ну, для физических задач обычно ставится,
[01:20:05.420 --> 01:20:08.420]  где есть периодически граничные условия.
[01:20:08.420 --> 01:20:12.420]  Ну, gz-состояние тоже можно записать в таком виде.
[01:20:12.420 --> 01:20:16.420]  Нормировка матрицы плотности, это все объекты,
[01:20:16.420 --> 01:20:20.420]  которые я рассказываю, например, в своем семестровом курсе
[01:20:20.420 --> 01:20:23.420]  квантовой тензорной сети в Мияне.
[01:20:23.420 --> 01:20:27.420]  Там тоже какое-то видео есть, поэтому...
[01:20:27.420 --> 01:20:31.420]  Так, вот здесь задачка из домашнего задания.
[01:20:31.420 --> 01:20:34.420]  Ну, понятно, что на семинарах уже будете разбирать,
[01:20:34.420 --> 01:20:38.420]  как записать операторы в виде тензоров.
[01:20:38.420 --> 01:20:41.420]  Вот там будут вот такие объекты встречаться.
[01:20:41.420 --> 01:20:43.420]  Видите, ножки вверх-вниз, это означает,
[01:20:43.420 --> 01:20:46.420]  что у вас теперь есть преобразование
[01:20:46.420 --> 01:20:50.420]  для физических ваших систем.
[01:20:50.420 --> 01:20:53.420]  То есть это уже операторы, не сами состояния,
[01:20:53.420 --> 01:20:58.420]  а можно еще операторы записывать в виде произведения матриц
[01:20:58.420 --> 01:21:00.420]  MetaX Product Operator.
[01:21:00.420 --> 01:21:02.420]  Такие есть тензорные сети.
[01:21:02.420 --> 01:21:06.420]  Я сейчас покажу вам еще другие тензорные сети.
[01:21:06.420 --> 01:21:08.420]  Италии, которые не нужны.
[01:21:08.420 --> 01:21:10.420]  Вот есть древесные тензорные сети.
[01:21:10.420 --> 01:21:14.420]  MPS является частным случаем этой древесной тензорной сети.
[01:21:14.420 --> 01:21:16.420]  То есть линейный граф.
[01:21:16.420 --> 01:21:18.420]  Вот видите, вот так вот, если пройдете,
[01:21:18.420 --> 01:21:22.420]  то это есть частный случай этой древесной тензорной сети.
[01:21:23.420 --> 01:21:27.420]  Значит, есть квантовая ограниченная машина Больсона,
[01:21:27.420 --> 01:21:29.420]  где есть два уровня.
[01:21:29.420 --> 01:21:32.420]  Вот V – это visible layer.
[01:21:32.420 --> 01:21:34.420]  Видите, тут есть физические индексы.
[01:21:34.420 --> 01:21:36.420]  Например, ваши кубиты, индексы кубит.
[01:21:36.420 --> 01:21:38.420]  А есть hidden layer.
[01:21:38.420 --> 01:21:40.420]  То, что вы не видите.
[01:21:40.420 --> 01:21:47.420]  Но каждый из кубитов осуществляет корреляцию с другим кубитом
[01:21:47.420 --> 01:21:49.420]  посредством этого hidden layer.
[01:21:49.420 --> 01:21:51.420]  Вот такие тензорные сети есть.
[01:21:51.420 --> 01:21:53.420]  Активно изучаются.
[01:21:53.420 --> 01:21:55.420]  Потому что они похожи на нейронные.
[01:21:56.420 --> 01:21:59.420]  Значит, есть состояние проецированных перепутных пар.
[01:21:59.420 --> 01:22:01.420]  По-английски projected entangled pass states.
[01:22:01.420 --> 01:22:03.420]  Это вот такая решетка,
[01:22:03.420 --> 01:22:06.420]  где физические индексы вот здесь находятся.
[01:22:06.420 --> 01:22:11.420]  Это для двумерных всяких физических систем с корреляцией.
[01:22:12.420 --> 01:22:14.420]  Есть еще другая тензорная сеть,
[01:22:14.420 --> 01:22:16.420]  которая называется Mera.
[01:22:16.420 --> 01:22:18.420]  Сейчас я вам ее расскажу.
[01:22:18.420 --> 01:22:20.420]  И это будет конец.
[01:22:21.420 --> 01:22:23.420]  Давайте с этого слайда начнем.
[01:22:23.420 --> 01:22:25.420]  Вот здесь W – это изометрия.
[01:22:29.420 --> 01:22:32.420]  Как сделать изометрическую матрицу?
[01:22:32.420 --> 01:22:34.420]  Вы можете взять унитарную матрицу.
[01:22:34.420 --> 01:22:36.420]  Вот эту W унитарную.
[01:22:36.420 --> 01:22:38.420]  Ой, просто W унитарную.
[01:22:40.420 --> 01:22:43.420]  И вырезать из нее столбцы или строчки.
[01:22:44.420 --> 01:22:47.420]  Соединяя с вот такими вот
[01:22:47.420 --> 01:22:49.420]  фиксированными значениями кубитов 0,
[01:22:49.420 --> 01:22:51.420]  вы получаете изометрическую.
[01:22:51.420 --> 01:22:53.420]  Вот это первое. Понятно, Ильич?
[01:22:54.420 --> 01:22:56.420]  Когда-то мы с вами это делаем,
[01:22:56.420 --> 01:22:58.420]  когда представление ставится?
[01:23:00.420 --> 01:23:03.420]  Смотрите, что вы можете на квантовом компьютере реализовать.
[01:23:03.420 --> 01:23:06.420]  Вы можете на квантовом компьютере реализовать такую схему.
[01:23:06.420 --> 01:23:09.420]  Вот у вас 0, 0, 0 – это вспомогательные кубиты.
[01:23:11.420 --> 01:23:13.420]  И вы дальше можете начать их перепутывать.
[01:23:14.420 --> 01:23:19.420]  То есть зеленые – это унитарные некоторые вентели W,
[01:23:20.420 --> 01:23:25.420]  а вот это синенькие – это какие-то унитарные вентели Q.
[01:23:25.420 --> 01:23:28.420]  То есть вот эту вот схему можно на квантовом компьютере реализовать.
[01:23:28.420 --> 01:23:30.420]  Квантовый компьютер.
[01:23:32.420 --> 01:23:35.420]  Какую вы в итоге получите тензорную диаграмму?
[01:23:35.420 --> 01:23:37.420]  Вы получите…
[01:23:37.420 --> 01:23:39.420]  А где же она? Я не вижу.
[01:23:39.420 --> 01:23:41.420]  Я не нарисовал.
[01:23:44.420 --> 01:23:46.420]  Ну ладно, тогда здесь я остановлюсь.
[01:23:46.420 --> 01:23:48.420]  Вот такую вот тензорную диаграмму получите.
[01:23:48.420 --> 01:23:50.420]  Чем она замечательна?
[01:23:50.420 --> 01:23:54.420]  Она замечательна тем, что, видите, внизу у вас много кубитов.
[01:23:55.420 --> 01:23:57.420]  Много кубитов.
[01:23:59.420 --> 01:24:05.420]  Вот тут, вот на этом срезе у вас число кубитов во много раз меньше.
[01:24:08.420 --> 01:24:12.420]  Потом на следующем срезе у вас число кубитов еще меньше.
[01:24:13.420 --> 01:24:16.420]  На самом деле их становится экспоненциально меньше.
[01:24:16.420 --> 01:24:18.420]  То есть надо делить, например.
[01:24:19.420 --> 01:24:21.420]  Разделите число на три – получите здесь.
[01:24:21.420 --> 01:24:24.420]  Разделите еще на три – получите число тут.
[01:24:24.420 --> 01:24:26.420]  И вот так вот.
[01:24:26.420 --> 01:24:28.420]  Некоторое грубленное описание.
[01:24:30.420 --> 01:24:35.420]  То есть получается, что с помощью вот такой схемы можно описывать много кубитов.
[01:24:38.420 --> 01:24:40.420]  Много кубитов описывать.
[01:24:41.420 --> 01:24:51.420]  Используя логарифм от этого числа кубитов вентиля.
[01:24:51.420 --> 01:24:56.420]  И такое представление называется multi-scale многомасштабное.
[01:24:56.420 --> 01:24:57.420]  Почему?
[01:24:57.420 --> 01:25:04.420]  Потому что вот эти стрелочки показывают вам переход к грубленному описанию на другом масштабе.
[01:25:15.420 --> 01:25:18.420]  Ну вот такая еще тензорная сеть есть, тоже сейчас популярная.
[01:25:19.420 --> 01:25:21.420]  Больше ничего не могу сказать.
[01:25:21.420 --> 01:25:22.420]  Время вышло.
[01:25:23.420 --> 01:25:26.420]  В общем, тензорная сеть – это интересная штука.

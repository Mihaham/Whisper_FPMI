[00:00.000 --> 00:09.520]  Сегодня у нас лекция номер 13. Будем говорить про то, что такое
[00:09.520 --> 00:12.240]  Прексимальные методы и метод Франко Вульфа. Здесь успеем.
[00:12.240 --> 00:17.720]  Поставьте плюс, если слышно, видно экран, все в порядке, можно начинать.
[00:17.720 --> 00:22.120]  Супер. Давайте начнем вот с чего. Напоминаю, что у нас
[00:22.120 --> 00:25.440]  какое-то время назад был метод градиентного спуска замечательный,
[00:25.440 --> 00:32.640]  для которого была вот такая вот схема обновления для дифференцируемой функции.
[00:32.640 --> 00:38.840]  Эту схему обновления можно проинтерпретировать как схему дискретизации для соответствующего
[00:38.840 --> 00:46.280]  дифференциального уравнения. То есть это как будто бы dx от t по dt равняется минус f' x от t,
[00:46.280 --> 00:53.600]  где x от нуля равно x0. Понятно ли, что написано в этих трех строчках? Поставьте плюс, если понятно.
[00:54.040 --> 00:59.360]  Первую производную приближаем конечными разностями классическая схема приближения
[00:59.360 --> 01:04.840]  и пересчитываем по сетке следующие значения. Такая схема называется прямой схемой Эйдера.
[01:04.840 --> 01:11.440]  Темнология немножко другая из вычислительной математики. И все свойства по поводу сходимости
[01:11.440 --> 01:18.440]  градиентного спуска, ограниченность, максимального шага для сходимости и
[01:18.440 --> 01:23.400]  все прочее это целиком полностью коррелирует с тем, что известно про зависимость сходимости
[01:23.400 --> 01:29.000]  схемы Эйдера от размера шага. Не абсолютно устойчиво, то есть это условно устойчиво для
[01:29.000 --> 01:35.320]  соответствующих альф. То есть это все один в один. Теперь чтобы отсюда следует гипотеза о том,
[01:35.320 --> 01:39.080]  чтобы получить какой-то другой метод, надо сделать следующее. Давайте как-нибудь по-другому
[01:39.080 --> 01:44.640]  тот же самый дифференциальный уравнение дискретизуем. Вот, например, вот так. Здесь будет все как обычно,
[01:44.640 --> 01:49.280]  а тут мы возьмем и скажем, что это на самом деле x ка плюс 1. Это называется неявная схема Эйдера
[01:49.280 --> 01:54.400]  по понятным причинам, я надеюсь. Она неявная, потому что есть вот эта вот штучка, которая
[01:54.400 --> 01:59.040]  в неизвестной точке участвует в вычислении градиента, который мы пока что не знаем. Так,
[01:59.040 --> 02:04.840]  неявная схема. И, собственно, она абсолютно устойчива. То есть какой бы вы размер шага сюда
[02:04.840 --> 02:09.200]  не подставляли, у вас все будет работать. Может он будет работать не очень быстро, может быть будет
[02:09.200 --> 02:13.680]  работать с некоторыми вычислительными трудностями в силу того, что отдельное уравнение будет решать,
[02:13.680 --> 02:21.360]  но, тем не менее, все будет не будет какой-либо серьезно расходить. Вот. Значит, пока мы написали
[02:21.360 --> 02:26.720]  всего лишь, так нажал, всего лишь дискретизацию, давайте дискретизацию перейдем к методу. Вот.
[02:26.720 --> 02:33.000]  Ну для этого достаточно заметить, что, ну давайте сделаем тривиальное преобразование для начала. Вот.
[02:33.000 --> 02:38.240]  Равно нулю. И достаточно сейчас внимательно посмотреть на эту штуку и понять, что на самом деле,
[02:38.240 --> 02:44.680]  когда у нас f выпукло, ну понятно, плюс дифференцируемость, куда же без этого, вот, то вот эта штука,
[02:44.680 --> 02:53.040]  вот эта звездочка обозначена. То же самое, что и у-хкт, квадрат второй нормы, 1 на 2 альфа кт,
[02:53.040 --> 03:01.840]  плюс f' от u, все это штрих при u равном xk плюс о первом, равно нулю. То есть то, что стоит по звездочке,
[03:01.840 --> 03:07.920]  является градиентом вот этой функции. Все ли это видят? 12 лекции. Ставьте плюс, если вы видите,
[03:07.920 --> 03:13.640]  что это градиент. Так, окей, вижу, есть прогресс. Очень хорошо. Градиент равен нулю, это значит,
[03:13.640 --> 03:18.680]  что у нас точка xk плюс 1, это не что иное, как аргумен от этой самой. Это называется просто
[03:18.680 --> 03:23.920]  «проксимальный метод». То есть он вместо того, чтобы, то есть каждый раз на каждой итерации решается
[03:23.920 --> 03:30.160]  некоторая задача. Понятно, что это имеет смысл использовать только, когда она решается аналитически. Вот.
[03:30.160 --> 03:35.280]  Почему это еще хорошо? Это еще хорошо, потому что если вдруг по каким-то причинам вам надо
[03:35.280 --> 03:40.560]  решать численно, то вот эта функция, чем вот f' от u, отличается по своей структуре от вот этой
[03:40.560 --> 03:46.800]  функции, как вы думаете? По тем критериям, которые мы ранее обозначили для описания характеристик
[03:46.800 --> 03:54.240]  функции. Если f выпукло, то f' от u конечно сильно выпукло. Ну да, по-другому надо сказать, что если f
[03:54.240 --> 03:58.840]  просто выпукло, то вся вот эта сумма, она уже будет сильно выпуклая, потому что мы бы, ну,
[03:58.840 --> 04:03.040]  гисян от этой штуки – это единичная матрица, шкалированная на вот этот вот
[04:03.040 --> 04:07.640]  преэффициент 1 на 2 αк. Вот отсюда, ну там, 1 на αк будет понятно. Вот отсюда следует просто
[04:07.640 --> 04:12.520]  факт, что мы как будто бы спектр вот гисян от этой матрицы, если он существует, сдвигаем на чуть-чуть
[04:12.520 --> 04:17.880]  от нуля. Поэтому характеристики этой функции существенно образом улучшаются. Вот. Значит,
[04:17.880 --> 04:23.440]  это еще один важный момент, почему вот эта вот эта задача лучше, чем минимизация самой
[04:23.440 --> 04:30.440]  исходной функции фату. Вот. Так. Значит, это обсудили, это обсудили, про то, что шаг любой тоже
[04:30.440 --> 04:37.480]  обсудили. Теперь, что из этого можно сделать? Вот. Тут пришло время привести несколько, да-да,
[04:37.480 --> 04:44.520]  я же не ввел самое главное обозначение. Как бы теперь его правильно ввести-то? Давайте вот так
[04:44.520 --> 04:50.960]  сделаем. Так. Стука равна вот таким вот обозначениям. Prox от αк f – это не произведение, а это значит,
[04:50.960 --> 04:57.200]  что αк уехало сюда, вот, а f уехало вот сюда. Это просто подряд доводящий символ, поэтому не думать,
[04:57.200 --> 05:01.720]  что я там какой-то произведение считаю. Вот точки xk. Такое обозначение. Вот. И теперь вот это все
[05:01.720 --> 05:06.000]  называется… так, ага, да, правильно, теперь надо вот так сделать. Тиш-тиш-тиш-тиш. Называется
[05:06.000 --> 05:11.320]  проксимальный метод, то есть просто xk плюс 1 – это проксимальный оператор, результат действия
[05:11.320 --> 05:16.400]  проксимального оператора на паук xk. Наверное, еще вот так можно сделать. Так. Это уже кревато, конечно,
[05:16.400 --> 05:21.720]  но здесь понятно. Понятно, что хотел сказать, а? Окей. Люди смотрят на экран, это приятно. Так,
[05:21.720 --> 05:25.920]  теперь давайте поймем, как, в общем, какой-то новый метод, что-то произошло, какой-то аргумент
[05:25.920 --> 05:30.960]  появился, пока непонятно, почему все это идет. Вот. Ну, на самом деле, давайте найдем простые
[05:30.960 --> 05:35.960]  интерпретации этого всего безобразия, чтобы понять, как потом с этим дальше жить. Здесь это пока
[05:35.960 --> 05:40.280]  некоторый элементарный кирпичик, из которого потом будет много еще следовать. Значит, если мы…
[05:40.280 --> 05:45.200]  то есть, пункт номер один. Если мы вернемся на уровень, когда мы записывали наше, как бы,
[05:45.200 --> 05:49.640]  условие оптимальности, то его же можно переписать вот эту звездочку. Следующим образом, что у нас
[05:49.640 --> 05:55.280]  какой-то единичный оператор плюс альфа-каты на f' действует на xk плюс первое, и это, оказывается,
[05:55.280 --> 06:01.320]  равно x-катам. На самом деле, у нас, если формально так списывать, абсолютно, что-то с валютаристки
[06:01.320 --> 06:07.240]  без каких-либо объяснований пока что, то это вот эта штука. То есть, у нас в каком-то смысле идет
[06:07.240 --> 06:13.120]  речь об обратном операторе к такому вот странному оператору единичный тождественный плюс альфа на
[06:13.120 --> 06:18.400]  градиент. Ну и если мы вспомним, грубо говоря, как это можно приблизить, наверное, более-менее
[06:18.400 --> 06:22.960]  справедливо, не только для конечномерного пространства, то это можно приблизить как единицы
[06:22.960 --> 06:30.040]  минус альфа-кат f' примененное к x-катам. Что в точность, оказывается, равно тут альфа-ката типа к
[06:30.040 --> 06:37.040]  нулю стремится. Я так напишу, что оно типа мало. Вот. Минус альфа-кат f' от x-кат. И мы гениально
[06:37.040 --> 06:42.320]  получаем градиентность. Для малого шага можно примерно сказать, что будет все одно и то же.
[06:42.320 --> 06:46.880]  Теперь, второй пример. Вторая интерпретация. Вспоминаем прошлую лекцию. Не зря же мы там
[06:46.880 --> 06:53.800]  так долго мучились с выяснением свойств Меда-Ньютона и его способов его получения. Поскольку вот эта
[06:53.800 --> 06:59.120]  штуковина, она непонятно какая, то давайте мы попробуем в проксимальный оператор засунуть не f,
[06:59.120 --> 07:07.960]  а f с крышкой, которая будет являться квадратичной оценкой в данной точке. Вот так. Плюс. Вот. И посчитаем
[07:07.960 --> 07:14.800]  x-к плюс первое как пример на проксимальный оператор от альфа-ф с крышкой от x-к. То это такое.
[07:14.800 --> 07:19.360]  Готовьте листочек и ручку, сейчас вам надо будет кое-что посчитать. А то предупреждаю. Так. Это
[07:19.360 --> 07:34.240]  будет f от x-к плюс f' от x-к x минус только не x, а u минус x-к плюс на вторая и так 1 на 2 до альфа-к
[07:34.240 --> 07:40.520]  норма у минус x-к квадратик. Все, отлично. Смотрите, что получилось. У нас квадратичная функция
[07:40.520 --> 07:45.840]  под аргмином стоит. То есть квадратичный множитель ввод и квадратичный множитель ввод. Ничего более
[07:45.840 --> 07:50.160]  высокого порядка или более нелинейного, чем квадратичное здесь нет. Это значит, что этот аргмин
[07:50.160 --> 07:54.520]  можно найти аналитически из условий первого порядка. Давайте найдем. Пожалуйста, получите градиент
[07:54.520 --> 07:59.640]  вот этой функции. Ну, от всей вот этой функции, которая стоит под аргмин. Градиент по u. И либо
[07:59.640 --> 08:04.880]  напишите в чат, либо продиктуете. Думаю, за пару минут усправитесь. Сначала надо написать
[08:04.880 --> 08:12.680]  first order optimality conditions. Да, давайте. Градиент функции, кажется, под аргмином будет f' от x-к
[08:12.680 --> 08:19.240]  плюс, у нас же в общем случае гессиан не обязательно симметричный, кажется. Гессиан обязательно симметричный.
[08:19.240 --> 08:28.040]  Обязательно? Хорошо, тогда плюс f' от x-к на u минус x-к. Дальше? Плюс норма u минус x-к
[08:28.040 --> 08:33.560]  потереть на альфа-к-т. Уже неправильно норма числа. Градиент должен быть вектором. А просто u минус x-к
[08:33.560 --> 08:42.560]  без нормы. Что там 1 на альфа-к на u минус x-к? Окей, это нулю должно быть равно. Давайте теперь
[08:42.560 --> 08:51.720]  свернем все вместе. Что получается? Получается f' от x-к-т плюс 1 на альфа-к-т. Это все умножается на u,
[08:51.720 --> 09:02.360]  которая нам неизвестна и равняется минус градиенту. Плюс f' на x-к и плюс 1 на альфа-к-к. Окей, правая
[09:02.360 --> 09:11.320]  часть переписывается более-менее аналогичным образом. Плюс f' x-к плюс 1 на альфа-к единичная
[09:11.320 --> 09:17.880]  матрица умножается благополучно на x-к. Отлично. Все до чему будет равно u? Равно. Умножаем на
[09:17.880 --> 09:23.880]  минус 1. Тут регуляризация специальным образом придумана, чтобы это можно было сделать. Вот это
[09:23.880 --> 09:28.280]  вот гарантирует то, что можно будет посчитать обратную матрицу. Проблем с выраженностью здесь
[09:28.280 --> 09:34.920]  никаких не ожидается. Что получается? Вот эта штуковина. Так, каким бы надо цветом таким более-менее
[09:34.920 --> 09:41.120]  светленьким выделить зеленым. Вот эта штуковина благополучно сократится с вот этой штуковиной.
[09:41.120 --> 09:50.160]  Не останется x-к-т. Дальше будет минус f' x-к-т плюс 1 на альфа-к единичная матрица в минус
[09:50.160 --> 09:55.760]  1 действует на градиент в точке x-к. Получилась вот такая вот опроксимация. Ну и это как бы x-к плюс
[09:55.760 --> 10:01.280]  первый наш напоминаю по аналогии тем, какой опроксимальный метод опроксимировать. А что это
[10:01.280 --> 10:09.080]  похоже? Во-первых, всем ли понятно, что происходило, кто это бодро все это прописал, успели его осознать
[10:09.080 --> 10:14.680]  произошедшее. Или нужно какой-то из переходов пояснить. Вижу два плюс. Это радует. Как дела
[10:14.680 --> 10:21.240]  у остальных? Гуд. Здорово. Что происходит дальше? Дальше мы можем посмотреть, что будет для разных
[10:21.240 --> 10:26.520]  при разных альфах. Первый вариант. Альфа-к-т стремится к плюсу бесконечности. Чему будет равняться x-к
[10:26.520 --> 10:30.840]  плюс первое? Ну приближенно. То есть если альфа-к-т большой, то это что значит? Кто доминирует?
[10:30.840 --> 10:37.400]  Какое слагаемое? Гисян. Гисян доминирует, конечно. Знаю, их будет просто x-к-т минус f' x-к-т минус
[10:37.400 --> 10:44.600]  первое на f' от x-к-тов. Что такое? Просто непонятно, что это значит. За метод. Да-да-да. Да, это метод
[10:44.600 --> 10:48.920]  Ньютон. Гениально. Ура. Тут что-то помню с прошлого раза. Получили метод Ньютона всего лишь как
[10:48.920 --> 10:53.800]  опроксимацию, опроксимального градиентного метода при некотором предельном переходе по шагу.
[10:53.800 --> 10:59.640]  Если альфа-к-т ноль, чему равен x-к плюс первый? Какое слагаемое доминирует? Ну просто какой-то бешеный
[10:59.640 --> 11:04.600]  градиентный спуск получается. Почему обычный градиентный спуск получается? Потому что получается x-к
[11:05.240 --> 11:14.200]  плюс один альфа-к-т единицы минус первой на f' от x-к. Получаем тут и x-к минус альфа-к-т на f' от x-к.
[11:14.200 --> 11:19.000]  Все, прекрасно. Получили градиентный спуск. То есть вот эта штуковина, которая в принципе в случае
[11:19.000 --> 11:25.720]  некоторые приятные структуры для Гисяна, она позволяет в каком-то смысле регулиризовать метод
[11:25.720 --> 11:31.320]  Ньютона чтобы, точнее, ну градиентный спуск так чтобы он, грубо говоря, стал методом Ньютона,
[11:31.320 --> 11:37.800]  при этом Гисян был строго положительно определен. И тем самым мы бы балансировали
[11:37.800 --> 11:43.160]  между этими методами еще одним способом, помимо того, который был обсужден в прошлый раз в контексте
[11:43.160 --> 11:48.680]  квазинтунских методов. Вот, то есть видите, можно делать вот так и это связано у этого подхода. Есть
[11:48.680 --> 11:55.800]  очень красивые интерпретации в контексте задачи, там пример. Если у нас хотим минимизировать одну-вторую,
[11:55.800 --> 12:05.960]  сумму f и t от x в квадрате. То есть задача нелинейных, наименьших квадратов. И вот такой вот метод,
[12:05.960 --> 12:12.640]  который вот я тут обозначу плюсиком. Для вот такой вот целевой функции f кодной, которая распадается на
[12:12.640 --> 12:20.320]  сумму квадратов некоторых функций, то плюсик сводится к методу Левенберга-Маркварта на самом деле,
[12:20.320 --> 12:32.560]  для этой задачи. Это супер классический метод, который везде можно найти. Давайте даже
[12:32.560 --> 12:40.440]  можно показать, его можно найти, если есть какое желание. Вот если грубо говоря с IP Optimize залезть. Вот,
[12:40.440 --> 12:47.600]  сейчас показать screen. Вот, то тут есть видно, да, экран? Алё, видно или экран? Да, видно. Так,
[12:47.600 --> 12:57.280]  отлично. Вот, тут вот общего вида методы. Вот, однако же, если посмотреть на просто Optimize без
[12:57.280 --> 13:06.080]  минимайза, то где эта штука? Линалка, бласс, лопатка. А, вот Optimize. Вот, то тут будет, так это понятно,
[13:06.080 --> 13:13.120]  отдельные методы, границы, BFGS пресловутые, которые в прошлый раз обсуждали. Что-то про методы
[13:13.120 --> 13:21.320]  безградиентные, которые пытаются найти глобальный минимум в неупаковых задачах. Вот, и значит есть вот
[13:21.320 --> 13:28.640]  linear least squares, которые вот такое решают. Вот, а есть non-linear least squares, которые решают просто
[13:28.640 --> 13:34.160]  произвольные наименьшие квадраты. Вот, ровно то, что я написал. То есть, на вторая сумма РО от, то есть,
[13:34.160 --> 13:40.000]  тут ещё есть параметры, есть функция РО, которая функцию, ну, которая типа ошибку измеряет. Но есть она
[13:40.000 --> 13:45.760]  типа единичная, которая тут, кажется, есть такой дефолтный случай. Сейчас где-то написано что-ли РО. Не написано, что-ли?
[13:46.360 --> 13:52.880]  А, вот это написано, наверное, loss. И вот loss, если он, да, короче, если он linear, то это просто
[13:52.880 --> 13:57.280]  ажестная функция. Это ровно то, что я написал. Standard least squares problem. Ну, короче, вот. Тут ещё разные
[13:57.280 --> 14:02.800]  другие, вот, про которые можно, если кому интересно, можно потом в чате будет обсудить, вот, что они
[14:02.800 --> 14:09.080]  делают. Ну, как бы идея не в этом. Идея в том, что есть параметр method, который определяет то, каким
[14:09.080 --> 14:14.520]  solver это всё решается. Вот, и вот тут есть LM, который, собственно, есть вот тот самый Levenberg-Marquardt,
[14:14.520 --> 14:19.280]  который я сейчас упомянул. И идея его ровно в том, чтобы регулировать метод Ньютона, вот, дополнительно
[14:19.280 --> 14:25.200]  сдвинув его на некоторую единичную матрицу, умноженную на соответственную константу. Вот,
[14:25.200 --> 14:33.800]  ну да, в общем, small constraint problems, это, наверное, самый такой основной метод. Вот, и если хочется
[14:33.800 --> 14:43.200]  ещё чего-то большего посмотреть про него, то можно посмотреть Levenberg-Marquardt. Да, в каком
[14:43.200 --> 14:47.160]  огромном количестве библиотек он реализован. Вот это вроде бы страничка раньше была в виде педии,
[14:47.160 --> 14:53.040]  возможно, её сейчас уже дропнули, да, чуть я уже не вижу. Ну, то есть, да, что-то, конечно, не
[14:53.040 --> 15:00.640]  получится показать. Да, в общем, в принципе, если вы берёте любой пакет, решение задачи
[15:00.640 --> 15:05.840]  оптимизации более-менее цельные, то там, скорее всего, эта штука будет реализована. Поэтому лестно
[15:05.840 --> 15:11.400]  понимать, что для наимнейших квадратов у максимального метода есть такая милая интерпретация. Это
[15:11.400 --> 15:17.360]  были две интерпретации, которые позволяют как-то склеить то, что мы изучали раньше с тем,
[15:17.360 --> 15:22.680]  что начали изучать сегодня. Теперь ещё одно важное свойство про максимального оператора заключается
[15:22.680 --> 15:29.120]  в том, что, внимание, что х со звёздочкой является решением нашей задачи, равносильно тому, что это
[15:29.120 --> 15:33.560]  неподвижная точка максимального оператора. Вот, то есть, получается, что если, то есть, давайте так,
[15:33.560 --> 15:38.840]  кто понимает, какая польза от этого результата? Ну, кто понимает, какая польза, напишите, какая
[15:38.840 --> 15:43.040]  польза в чате. То есть, вопрос очень простой. Зачем это нужно с точки зрения вычислений,
[15:43.040 --> 15:48.040]  что-то нет идей. А, это правда. Это значит, что можно использовать невязку х со звёздочкой минус,
[15:48.040 --> 15:54.680]  ой, хк-хк, плюс первое, меньше либо равно эпсилон, как корритерия сходимости. Да, именно так. Это
[15:54.680 --> 15:58.760]  называлось раньше у нас корритерия сходимости. Потому что, ну, они должны подать. Давайте докажем.
[15:58.760 --> 16:04.480]  Для простоты сделаем это для дифференцируемых функций. Увидите, в каком месте это будет важно. Так,
[16:04.480 --> 16:10.120]  ну, в одну сторону. То есть, пусть х решение. Это значит, что f от х больше либо равно f от
[16:10.120 --> 16:14.640]  х со звёздочкой. Ну, теперь, давайте прибавим к обеим частям неотрицательные функции. Ну,
[16:14.640 --> 16:24.200]  а это больше либо равно, понятное дело, чем вот х со звёздочкой плюс 1 на 2 альфа х со звёздочкой
[16:24.200 --> 16:33.040]  минус х со звёздочкой. Следовательно, х со звёздочкой это аргумент для нашей функции f от х плюс 1 на 2
[16:33.040 --> 16:38.280]  альфа х минус х со звёздочкой 2 на 2 в квадрате. А это есть определение максимального оператора.
[16:38.280 --> 16:42.400]  Успели? То есть, видите, три строчки в эту сторону доказываются. Довольно прямолинейно.
[16:42.400 --> 16:48.360]  Пришло определение. Докидываем до нужного вида функции. Оцениваем. Так вижу два плюса.
[16:48.360 --> 16:54.480]  Дело у Дмитрия видим. Да, спасибо. Теперь, давайте в обратную сторону. Тут похитрее немножко,
[16:54.480 --> 16:59.640]  и тут как раз-таки будет важно дифференцируем. То есть, если у нас х со звёздочкой это аргумент,
[16:59.640 --> 17:05.840]  аргумент, понятно отчего, то это значит, что в этой точке выполнено слово первого порядка,
[17:05.840 --> 17:11.920]  что грагент ноль. То есть, f' от х со звёздочкой на самом деле, то есть, ноль равен f' от х со звёздочкой
[17:11.920 --> 17:20.400]  плюс 1 на альфа ката х минус х со звёздочкой в точке х равняется х со звёздочкой. А это в точности
[17:20.400 --> 17:27.880]  f' вот так. И вспомним, что это равно нулю. И, следовательно, х со звёздочкой решение сходной
[17:27.880 --> 17:32.720]  задачи. Вот. Обобщается это доказательство и на случай, когда f' не дифференцируемо,
[17:32.720 --> 17:37.080]  вот, но более трудоемко становится. Поэтому сейчас я так. Ну, то есть, вообще не важно.
[17:37.080 --> 17:40.440]  Вопрос, к примеру, такой. Просто пишут условия оптимальности, и из этого условия
[17:40.440 --> 17:44.360]  оптимальности выводится условие оптимальности для исходной задачи благодаря тому, как выглядит
[17:44.360 --> 17:50.040]  градиент для второй нормы, и что он в любом случае будет зануляться в точке х со звёздочкой. Понятно,
[17:50.040 --> 17:56.200]  что мы доказали, как мы доказали, и что, если в этого следует. Так, вижу плюсы, окей. Спасибо.
[17:56.200 --> 18:01.280]  Так, теперь немного про то, как это дело вычислять. Вычислять проще, чем могло показаться на первый
[18:01.280 --> 18:06.880]  взгляд. Для этого, так, мы увеличим масштаб. Посмотрим функцию, у которой перемены разделяются.
[18:06.880 --> 18:10.960]  Называется сепарабельная функция. Ну, тут у нас сепарабельная функция. Называется такая вот
[18:10.960 --> 18:17.280]  функция. Сейчас я нарисую. Называется сепарабельной, потому что происходит разделение перемен, а именно f
[18:17.280 --> 18:23.080]  от x составляется в виде f и t от x и t. Приведите какой-нибудь простой пример такой функции.
[18:23.080 --> 18:28.320]  Скалярное произведение с константой. Скалярное произведение с константой? Это что значит?
[18:28.320 --> 18:36.240]  Линейное, что ли? Ну да. Так, хорошо, линейное. Очень поинтереснее. Типа сумма c и t, x и t, да? Вот.
[18:36.240 --> 18:44.280]  И тогда у нас каждая f и t от x и t это c и t, x и t. Ну да, в общем-то справедливо. Ещё, очень более
[18:44.280 --> 18:49.000]  реалистичная и насущная. Всё, на линейных функциях Мир и Мир закончился. Больше не существует.
[18:49.000 --> 18:53.760]  Ну непонятно, что можно просто вместо x и t взять x и t в квадрате или что-нибудь такое, но как раз
[18:53.760 --> 18:58.840]  реалистично непонятно, что из этого сделать. Да, понятно. Ну то есть стандартный пример – это всякие
[18:58.840 --> 19:04.760]  нормы. Типа первая норма – сумма модулей. Вот. Вторая норма – сумма квадратов. Тоже прекрасная функция.
[19:04.760 --> 19:11.200]  Ну только надо же тогда и корень извлечь из этого всего. Не, ничего не надо. Просто берём квадрат и не
[19:11.200 --> 19:17.320]  страдаем. Так, что ещё полезное бывает? А, ну, например, вот для матрицы, если функция от матрицы, то
[19:17.320 --> 19:26.320]  тоже типа сумма модулей по компонентных. Вот. Тут вот x из, ну, обычно из SN+. Ну, что-то не так важно. Вот. То есть
[19:26.320 --> 19:31.960]  очень много функций, которые отвечают за свойства решений дополнительные, типа разреженность,
[19:31.960 --> 19:39.480]  малоранговость или что-то такое, они как раз-таки зависит напрямую поэлементно от агумента, скажем
[19:39.480 --> 19:45.760]  так, да. Вот. Поэтому этот час встречается, и если мы поймём, как вычитается максимальный оператор
[19:45.760 --> 19:51.880]  для этой функции, то много-много задач, и мы сейчас наоборот себе упростим решение. Вот. Ну, в общем,
[19:51.880 --> 19:57.800]  результат такой, утверждение, наверное, это лучше так написать, что если всепарабельно, то
[19:57.800 --> 20:08.000]  максимальный оператор от f точки от u этой компоненты – это максимальный оператор от f итого в u итой
[20:08.000 --> 20:13.280]  точки. То есть, внимание, следите за руками, где находится буква i. Слева она тут, справа она…
[20:13.280 --> 20:18.960]  слишком толсто. Слева она вот тут, а справа она вот тут и вот тут. То есть, везде, обратите
[20:18.960 --> 20:23.720]  внимание, скаляры стоят. То есть, лево и справа. Ну, вот тут вопрос. Всем ли это очевидно? Поставьте
[20:23.720 --> 20:29.120]  плюс, если вам это очевидно, и минус, если это лучше показать более подробно. Очевидно кому-то – это
[20:29.120 --> 20:33.880]  хорошо. Кому не очевидно. Кому-то не очевидно. Окей. Давайте посмотрим детальнее. Значит,
[20:33.880 --> 20:43.640]  xk плюс 1. Это у нас был… Ой, ну да, давайте так делать. Аргмин. Тут у нас появляется сумма f ит от x от f у
[20:43.640 --> 20:53.320]  итого плюс, а дальше я сразу распишу, 2 альфы каты, сумма у иты минус x, ну, типа t в квадрате. Это
[20:53.320 --> 21:01.800]  было наше определение. Сумма аргмин по u. Теперь, аргмин по u расписывается как сумма, и все вносит
[21:01.800 --> 21:11.360]  под одну сумму. У иты плюс 1 на 2 альфы каты, у иты минус x каты иты в квадрате. Вот. И мы чудо замечаем,
[21:11.360 --> 21:18.000]  что на самом деле вот каждый из этих вот и… Каждый из вот этих вот слагаемых, оно зависит только от
[21:18.000 --> 21:24.400]  своего красного x иты каты. Поэтому для поиска соответствующих компонентов достаточно рассмотреть
[21:24.400 --> 21:34.240]  x кап плюс 1 иты как аргмин… Ну, снова по u, только теперь вы будете скаляром. f иты от u плюс 1 на 2 альфы каты,
[21:34.240 --> 21:45.360]  у иты минус x иты каты в квадрате. А это ровно и есть. Проксимальный оператор от альфы f иты в точке… Ну,
[21:45.360 --> 21:50.480]  тут в точке x иты каты понятно, но, я надеюсь, не менее очевидно, как это переносится на обозначение
[21:50.480 --> 21:56.600]  u иты, стало понятно. Прекрасно. Вот. То есть достаточно взять вот из этих вот ингредиентов какую-нибудь
[21:56.600 --> 22:01.920]  одну функцию, вот из этих вот, вот из этих вот… Для нее найти, грубо говоря, решить одномерную задачу
[22:01.920 --> 22:07.960]  аналитически, потом просто викторизовать результаты и понять, как пересчитывается вся эта история вместе.
[22:07.960 --> 22:13.480]  Вот. Это полезный хинт о том, как это посчитать эффективно. Так. Ну, теперь там это были все некоторые
[22:13.480 --> 22:18.920]  истории про то… Так, было занятие прошло. Прекрасно. Про то, что такое проксимальный оператор,
[22:18.920 --> 22:23.320]  проксимальный метод и какими свойствами он обладает, теперь перейдем непосредственно к
[22:23.320 --> 22:28.960]  максимальному градиентному методу. Очень классная штука. Если вы будете уметь видеть,
[22:28.960 --> 22:33.520]  когда им надо пользоваться, многие изучители будут решаться гораздо быстрее. Чем, если вы этого
[22:33.520 --> 22:39.200]  видеть не будете и будете запускать солверы общего вида для них. Максимальный градиентный метод.
[22:39.200 --> 22:44.200]  Кроме идеи. Идея немножко похожа на то, что мы видели в стокастике, когда мы в черный ящик заглянули и
[22:44.200 --> 22:50.280]  обнаружили там, что у нас наша функция представляется в виде суммы большого числа функций.
[22:50.280 --> 22:54.760]  Помните, что было такое? Девчонки, что помните? Реакцию, конечно, я пока не вижу. Надеюсь, что как
[22:54.760 --> 23:00.200]  работают стокастические градиентные методы, более-менее большинство разобрали. Вот. Здесь немножко
[23:00.200 --> 23:06.040]  похожая история, только теперь у нас будет не сумма бесконечного числа большого числа функций,
[23:06.040 --> 23:11.560]  всего две функции. f от x и g от x. При этом, что мы будем знать? Мы будем знать, что f от x у нас гладкая,
[23:11.720 --> 23:19.280]  там все выпукло, плюс выпукло. g от x просто выпукло, плюс может принимать бесконечные значения. Вот.
[23:19.280 --> 23:23.800]  То есть такая типа может быть негладкая и вообще непонятно какой. В общем, все плохо, может быть.
[23:23.800 --> 23:29.080]  В плане дифференцируемости. То есть суммарно, вот эта функция, давайте обозначим h от x, нам один
[23:29.080 --> 23:33.720]  раз пригодится. То есть h от x в общем случае не дифференцируемая функция. Градиентный спуск
[23:33.720 --> 23:39.360]  примять нельзя. Однако, мы знаем, что есть кусок, который дифференцируем. Давайте это использовать.
[23:39.360 --> 23:46.880]  То есть мы можем сказать, что у нас h от x оценивается сверху как f от x кt плюс f штрих от x кt x
[23:46.880 --> 23:54.600]  минус x кt плюс l пополам, ой, l пополам, норма, x минус x кt, плюс g от x кt. То есть вот это,
[23:54.600 --> 24:00.040]  это наша квадратичная оценка сверху, с которой мы относимся уже к эту лекцию. Да, наверное. Так,
[24:00.040 --> 24:05.880]  поставьте плюс, если помните. Так, помнит, хорошо. Вот. А g от x как был, так и осталось. Давайте искать x
[24:05.880 --> 24:12.080]  кt плюс 1 как аргумент от вот этой вот шикенции. Вот. Почему? Это хорошо. Это будет хорошо. То есть тут
[24:12.080 --> 24:20.680]  будет, ой, тут прошу прощения, я немножко напортачил. Тут не x кt, а просто x. Так. Что дальше? Дальше
[24:20.680 --> 24:26.280]  напрашивается классический подход. Давайте мы полный квадрат выделим. Вот. И полный квадрат
[24:26.280 --> 24:31.360]  напрашивается выделить ровно вот в этой части. Потому что у нас есть квадрат как будто бы один.
[24:31.360 --> 24:36.520]  У нас есть произведение, может быть, под квадратом и градиентом. Вот. Поэтому, наверное,
[24:36.520 --> 24:42.680]  если мы тут аккуратненько сейчас перетинем в правильные стороны наши слагаемые, там
[24:42.680 --> 24:48.840]  множим поделим, добавим умный ноль, который не зависит от x. Вот. То, в общем-то, все должно
[24:48.840 --> 24:53.480]  получиться. И посмотрим, к чему это приведет. Аргумент я, в общем, не переписываю. Напишу просто,
[24:53.480 --> 24:58.880]  что это то же самое, что аргумент от t, g от x остается. От этого никуда не дается. Плюс 2 делить на l.
[24:58.880 --> 25:05.360]  Вот здесь дальше будьки кропки. Сначала у нас x минус x ка т в квадрате плюс одна вторая. Извините.
[25:05.360 --> 25:10.600]  Отсюда l пополам выносится сначала. l пополам выносится, поэтому здесь получается 2 на l. 2. Здесь
[25:10.600 --> 25:17.120]  образуется единица на l f штрих от x ка т, а тут x минус x ка т. То есть в итоге у нас что? Вот квадрат
[25:17.120 --> 25:23.520]  первого, удвоенное произведение, удвоение отдельно стоит. Первого на второе, то есть получается,
[25:23.520 --> 25:29.960]  что второе у нас это 1 на l на норму градиента в точке x... 1 на l на градиент в точке x ка от
[25:29.960 --> 25:36.160]  функции f. Значит, нам надо добавить квадрат 1 на l f штрих от x ка квадрате. Ну и вычислить этот
[25:36.160 --> 25:43.560]  самый квадрат, честностью. Вот. И плюс f от x ка. Так, и я что-то забыл. Ну да, тут еще,
[25:43.560 --> 25:48.920]  типа, конечно же, 2 на l f от x ка. Типа так. Вроде бы все правильно. Пожалуйста,
[25:48.920 --> 25:52.880]  проверьте. Поставьте плюс, если проверили и убедились, что все хорошо. Так вижу,
[25:52.880 --> 25:57.280]  что вроде нормально. Двух человек подтверждение получилось. А, все, отлично. Спасибо. Вот. То
[25:57.280 --> 26:04.480]  есть теперь давайте добро свернем. Получается аргумент по x. Тут все еще по x. Ж от x плюс l пополам
[26:04.480 --> 26:15.040]  на что? На норму x минус x ка плюс единица на l f штрих от x ка. Все остальное мы буквально выкинули,
[26:15.040 --> 26:21.840]  потому что от x это все остальное не зависит. Теперь, если мы обратно все это внесем. Сейчас,
[26:21.840 --> 26:29.920]  просто надо ли это все дело вносить обратно. Сейчас, давайте я постепенно сделаю. Аргумент
[26:29.920 --> 26:39.400]  же от x плюс l пополам норма x минус x ка минус 1 на l f штрих от x ка. Вот, что хотел продемонстрировать.
[26:39.400 --> 26:45.520]  Меня смущает этот дурацкий l пополам, который сейчас что-то придумать. Я же правильно его вынесу?
[26:45.520 --> 26:54.320]  Так, ну l пополам вынеслось правильно. Так, тут есть квадраты. Очень хорошо. Так, окей. Теперь,
[26:54.320 --> 27:00.880]  если мы вспомним, что у нас стандартный альфа ка был равен единиц на l, то тогда вся эта штукенция
[27:00.880 --> 27:11.280]  станет принимать более привычный вид. Аргумент же от x плюс 1 на 2 альфа ка норма x минус x ка минус
[27:11.280 --> 27:19.160]  альфа ка f штрих от x ка. Получилось. Смотрите, что происходит. Эту штуку можно записать в
[27:19.160 --> 27:24.880]  привычном уже иной терминологии. Мы считаем прохимальный оператор от альфа g точки x ка минус альфа
[27:24.880 --> 27:30.280]  ка f штрих от x ка. Это называется прохимальный градиентный метод. Понятно ли, как он у нас
[27:30.280 --> 27:34.600]  образовался? Ставьте плюс, если понятно, минус, если нет. Пока лишь только вроде бы один плюс.
[27:34.600 --> 27:39.920]  Появился второй. Хорошо. Ага, спасибо. Окей. Значит, что про него важно понимать? Важно понимать,
[27:39.920 --> 27:45.240]  что мы берем как бы лучше всего того, что у нас нам доступно. Изначально у нас функция спалась в сумму
[27:45.240 --> 27:51.480]  той функции, которая можно, ну не распалась, мы как бы полагаем. Вот. Функции, у которой можно
[27:51.480 --> 27:56.520]  написать градиент и той функции, у которой гипотетически, мы надеемся, легко посчитать
[27:56.520 --> 28:01.760]  прохимальный оператор. Поэтому мы берем, делаем как бы все, что можно сделать. x обновляется сначала
[28:01.760 --> 28:06.680]  как градиентный шаг по f. Потом важно отметить, что с таким же шагом, то есть тут важно их не
[28:06.680 --> 28:12.680]  начать варьировать, таким же шагом пересчитывается, делается прохимальное отображение по относительной
[28:12.680 --> 28:19.400]  функции g, для которой градиентный шаг не применим. Теперь, ну что, пример что ли или не пример? Сейчас
[28:19.400 --> 28:25.800]  секунду, я подумаю, как правильнее это было бы все пояснить. А, да, сначала давайте вот что,
[28:25.800 --> 28:29.800]  сходимость. Конечно же сейчас я доказывать это все не буду, потому что времени. Вот. Сходимость.
[28:29.800 --> 28:38.040]  Вот чудо, вот единица на k для гладких выпуклых функций. То есть, смотрите, у нас изначально функция
[28:38.040 --> 28:43.760]  недефинцируема, градиентный спуск неприменим, но скорость сходимости такого метода совпадает
[28:43.760 --> 28:47.840]  со скоростью сходимости градиентного спуска. Немножко магии. То есть, негладкая поправочка
[28:47.840 --> 28:54.680]  аддиктивная нивелируется помощью максимального оператора относительной добавки. Это здорово,
[28:54.680 --> 29:01.480]  потому что изначально у нас функция всего лишь обычная была, ну, сумма всего лишь обычная,
[29:01.480 --> 29:06.800]  негладкая, выпуклая. А тут мы взяли и получили скорость сходимости как будто бы все вместе гладко,
[29:06.800 --> 29:14.120]  что немножко необычно. Ну, небольшой анонс на следующие 15 минут, через 5-10 минут, наверное,
[29:14.120 --> 29:19.320]  дойдем. Что раз у нас есть скорость 1 на k, то, наверное, мы можем это ускорить по аналогии с тем,
[29:19.320 --> 29:23.760]  как мы это ускоряли для обычного градиентного спуска и получить 1 на k квадрат. Ну, собственно,
[29:23.760 --> 29:29.040]  так и происходит. И чуть позже я приведу формулу, ну, там, формула абсолютно аналогичной с точностью
[29:29.040 --> 29:32.720]  до того, как вот эта штука пересчитывается, а максимальный оператор как был, так и остается.
[29:32.720 --> 29:38.200]  Теперь частный случай называется метод проекции градиента. В чем идея? Смотрим на вот такую вот задачу
[29:38.200 --> 29:43.960]  и говорим, что, окей, у нас есть такое ограничение не очень понятное, давайте мы его перепишем вот
[29:43.960 --> 29:51.400]  в таком видео, где функция и c от x, да, ну тут понятно, c выпукло множество, и выпуклая функция,
[29:51.400 --> 29:56.680]  дифференцируемая, с ней все в порядке. Вот, а это индикаторная функция множества, которая равна нулю,
[29:56.680 --> 30:02.480]  если x в множестве лежит, плюс бесконечности, если x в множестве не лежит. По построению решение вот
[30:02.480 --> 30:07.040]  этой задачи будет совпадать с решением исходной задачи. Это только функция f минимизировалась,
[30:07.040 --> 30:10.960]  но на множестве. Сейчас она минимизируется не на множестве, но с такой вот аддитивной поправкой.
[30:10.960 --> 30:16.600]  Понятно ли преобразование? Поставьте плюс, если понятно. Так, окей, еще один плюс нужен, или минус,
[30:16.600 --> 30:22.360]  не знаю, какая-то реакция точно нужна. Да, здорово. Смотрите, что происходит. Мы получили ровно ту самую
[30:22.360 --> 30:27.160]  структуру, для которой применялся наш проксимальный градиентный гет. У нас есть одно слагаемое,
[30:27.160 --> 30:32.360]  которое выпукло и дифференцируемо, и другое слагаемое, которое выпукло, но не дифференцируемо.
[30:32.360 --> 30:35.880]  Давайте применим наш метод. Получится, получится, что xk, так, сейчас, секундочку.
[30:35.880 --> 30:44.720]  Получится, что xk плюс 1 это проксимальный оператор от функции альфа индикатор множества c. Давайте,
[30:44.720 --> 30:51.160]  я не буду аргумент написать. От чего? От x катова минус альфа f штрих от xk. Что такое
[30:51.160 --> 30:55.640]  проксимальный оператор индикаторной функции? Давайте минуту на размышление. Можно написать в чат
[30:55.640 --> 31:00.680]  ответ или сказать в микрофон. Пока определение напишу, может быть, оно поможет. Это просто аргумент
[31:00.680 --> 31:07.640]  второго слагаемого помножить в c. Как это называется? Это правда. Что это такое? Ближайшее к x.
[31:07.640 --> 31:13.480]  Нет, к u.c. У этого длинного определения есть одно конкретное название, которое, я думаю, все знают.
[31:13.480 --> 31:19.320]  О, гениально. Это проекция, правильно? Проекция. Поэтому это называется, вот я тут специально причернул,
[31:19.320 --> 31:24.120]  что он говорит, проекция-градиент. На самом деле, вот эта штука, проекция, но еще иногда обозначается
[31:24.120 --> 31:32.720]  p, c от u. То есть, если мы теперь запишем наш метод после вот этих вот всех замечательных выкладок,
[31:32.720 --> 31:39.720]  всего лишь проекция намножится в c точки xk минус альфа f штрих от xk. Видите? То есть, мы делаем
[31:39.720 --> 31:45.120]  градиентный шаг по нашей функции, по антиградиенту, понятное дело. Потом, куда бы мы ни пришли,
[31:45.120 --> 31:50.800]  ну типа вот было наше множество, мы где-то живем здесь, xk. Потом прыгнули сюда, каким-то причинам,
[31:50.800 --> 31:57.800]  не знаю, и спроецировались. Потом мы из этой точки куда-то шагнули и спроецировались. Это xk плюс 1,
[31:57.800 --> 32:03.440]  это xk плюс 2. Понятен ли метод и понятно ли почему это всего лишь частный случай
[32:03.440 --> 32:07.640]  проексимального градиентного метода? Ставьте плюс, если понятно, и минус, если не очень. Так,
[32:07.640 --> 32:13.720]  вроде понятно. Все отлично, good. Значит, это всего лишь частный случай, как я уже сказал. И поэтому
[32:13.720 --> 32:18.280]  все те технологии, которые были разработаны выше, она, собственно, и ниже будет еще доделана,
[32:18.280 --> 32:23.760]  она напрямую переносится на метод проекса градиента вот в таком вот виде тоже. Поэтому не обязательно
[32:23.760 --> 32:27.920]  его рассматривать какой-то отдельно изолированный такой уникальный метод. Настоящим всего лишь
[32:27.920 --> 32:33.720]  заметить, что это частный случай и проексимального градиентного метода. Теперь ускорение. Ускоренный,
[32:33.720 --> 32:40.720]  так давайте я то фаст, проексимальный градиентный метод. Идея очень простая. Давайте считать нашу
[32:40.720 --> 32:48.200]  точку относительно другой какой-то точки, а не предыдущей. Вот. Ну и y ка плюс 1 потом пересчитывается,
[32:48.200 --> 32:53.840]  как x ка плюс 1 плюс, ну не знаю, какой-то там коэффициентик обычно берут. Простоты ка на
[32:53.840 --> 32:58.080]  ка плюс 3, другие варианты тоже возможны. В необходимости, я не сомневаюсь, в литературе вы найдете
[32:58.080 --> 33:04.360]  их достаточное количество. И то. Так, вот тут вот плюсик, а там минус x ка, вот так. То есть берем
[33:04.360 --> 33:11.960]  линейную комбинацию двух соседних х, в ней считаем y и потом относительно этого y пересчитываем
[33:11.960 --> 33:17.280]  следующий х. Вот. Абсолютно понятная стратегия, сходимость также 1 на ка квадрат превращается
[33:17.280 --> 33:24.160]  для гладких выпуклых. Вот. Действительно можно, ну там, пять строчек реализации, можно
[33:24.160 --> 33:30.500]  пронаблюдать, насколько эта штука быстрее исходит к тому же сационарному, к той же сационарной
[33:30.500 --> 33:36.820]  точки, да. Так, есть минус 2, прекрасно. А, да, теперь еще вот что надо сказать, что вот сейчас
[33:36.820 --> 33:41.860]  и случаи про проекцию градиента мы рассмотрели. Вот. Теперь же, если мы в целом посмотрим на вот нашу
[33:41.860 --> 33:47.420]  структуру, то какие варианты возможны? Возможно, если у нас f-тождественный 0,
[33:47.420 --> 33:53.980]  мы получаем метод, просто, проексимальный метод. Потому что градиент будет нулем, и наш x ка плюс
[33:53.980 --> 33:59.060]  1 будет пересчитываться просто как, максимально отображая относительно функции g текущей
[33:59.060 --> 34:03.540]  точки. В том, если у нас, наоборот, g равно 0, мы получим, какой метод? Чему равно
[34:03.540 --> 34:10.260]  проексимальное отображение для нуля? Тяжело как-то. X ка т? То есть начальная точка? Почему? Вот наш метод.
[34:10.260 --> 34:15.220]  Отдается только норма у минуса, так что у нас там в качестве у по этому? Нет, ну, наверное, все-таки вот это
[34:15.220 --> 34:20.100]  останется. Даже вот такой метод. А, ну да. Что это за метод? Градиентный спуск. Именно так. Получится
[34:20.100 --> 34:24.500]  градиентный спуск. Ну и третье то, что мы уже обсудили, это то, что если у нас g это индикатор,
[34:24.500 --> 34:30.900]  получаем проекцию градиента. Видите, как много методов скрыто за одним всего лишь таким, казалось
[34:30.900 --> 34:35.740]  бы, наивным и простым предположением, что сумма, что функция совпадается на сумму двух функций.
[34:35.740 --> 34:43.300]  Здорово. Теперь, частный случай, конкретный пример. L1 регулиризация для, работает, для
[34:43.300 --> 34:48.900]  линейных наименьших квадратов. То есть, если мы такую задачу решаем, нам надо найти
[34:48.900 --> 34:52.660]  проексимальный оператор от вот этой штуковины. Ну и градиент от вот этой, я верю, все справится.
[34:52.660 --> 34:56.580]  Вот. Давайте просто на конкретном примере посмотрим, как это дело вычисляется. То есть,
[34:56.580 --> 35:02.060]  наша функция g от x. Первая норма. X равна сумме модулей. Поэтому нам достаточно для модуля посчитать
[35:02.060 --> 35:07.260]  проексимальное отображение и потом просто векторизовать. Надо посчитать следующую штуку.
[35:07.260 --> 35:16.380]  Аргмин. Чего? От модуля x плюс 1 на 2 альфа. Ну типа y минус x в квадрате. x минус y давайте лучше
[35:16.380 --> 35:25.340]  сделаем. И аргмин по x. Функция негладкая. Давайте из нее сделаем гладкую функцию. Вот. И потом
[35:25.340 --> 35:31.100]  вернемся к исходной задаче. То есть, будет сейчас гладкая функция с, ну гладкая задача с
[35:31.100 --> 35:38.620]  ограничениями. Ну понятно какая. Аргмин t плюс 1 на 2 альфы x минус y в квадрате при условии
[35:38.620 --> 35:44.620]  o меньше либо равно t. То есть, минус t меньше либо равно x меньше уровня t. Вот так. Понятно,
[35:44.620 --> 35:49.460]  почему это справедливо? Да, нет. Вроде как-то мы это уже изучали, насколько я помню. И нет. Вроде
[35:49.460 --> 35:56.420]  понятно. То есть, канат графику переходим по статистично в целевой функции и нам это, в общем-то,
[35:56.420 --> 36:01.340]  хватает. Сейчас, я думаю, в процессе будет понятно, почему. Сейчас, короче, давайте я все-таки напишу,
[36:01.340 --> 36:06.780]  как а t. Как раз вроде 10 минут должно хватить. Лагранжан. И тут, соответственно, аргмин по x и по t уже
[36:06.780 --> 36:16.940]  появился. То есть, t плюс 1 на 2 альфы x минус y в квадрате плюс лямбда, что на x минус t и плюс мю
[36:16.940 --> 36:24.740]  минус t минус x. Это нашел гарнжан. Ну, соответственно, лямбда мю больше либо равно нуля. Лямбда x минус
[36:24.740 --> 36:33.660]  t равно нулю. Мю минус x минус t равно нулю. Дополняющая не жесткость. Вот. Пам-пам-пам. А, вроде все, да?
[36:33.660 --> 36:40.260]  А, ну, градианта гарнжана, да. Давайте по x. Это что будет такое? Это будет 1 на альфы x минус y плюс
[36:40.260 --> 36:49.500]  лямбда минус мю. И по t это будет единица минус. Лямбда минус мю тоже равно нулю. Отставьте плюс,
[36:49.500 --> 36:53.580]  если вы понимаете, что я написал только что условия какая-то. Кроме там допустимости прямой
[36:53.580 --> 36:58.820]  задачи, но это мы держим умее еще, пока нам это сейчас не нужно. А, вижу. Отлично. Спасибо. Что из этого
[36:58.820 --> 37:06.020]  следует? Значит, лямбда плюс мю равно единица. Следовательно, лямбда не равно нулю и мю не равно...
[37:06.020 --> 37:11.460]  Ну, то есть, короче говоря, они не равны нулю одновременно. Вот. Хорошо. Это уже один случай мы
[37:11.460 --> 37:17.780]  исключили. Значит, если... Могут ли они быть неравными нулю одновременно? То есть, если лямбда не равно
[37:17.780 --> 37:23.780]  нулю и мю не равно нулю, из этого следует в силу дополняющей не жесткости, как бы мне тут это обозначить,
[37:23.780 --> 37:31.340]  не будь... Я вот так же делаю. Так вот, то х равно... Получилось. То х равно t и х равно минус t
[37:31.340 --> 37:38.940]  одновременно. Что возможно только если х равен нулю и t равно нулю. Но тогда будет ли это
[37:38.940 --> 37:44.980]  чему-то противоречить? Давайте посмотрим. По секунду. Как будто бы... Ну, хорошо, давайте вот,
[37:44.980 --> 37:50.420]  типа, следовательно, х равно t равно нулю. Пока какая-то изолированная ситуация, с которой сейчас
[37:50.420 --> 37:58.100]  будем бороться. Вот. В таком случае, если лямбда не ноль, следовательно, х равно t. Вот. Раз х равно t,
[37:58.100 --> 38:03.300]  то в каком-то смысле мы раскрыли так модуль. Это один из способов... Ну, один из вариантов мы получили
[38:03.300 --> 38:09.540]  раскрытие модуля за счет того, что записали один из вариантов дополняющей не жесткости. Вот. Если
[38:09.540 --> 38:16.300]  лямбда не ноль, а мю равно нулю, следовательно, из градиента Лагрензжана получаем, что лямбда
[38:16.300 --> 38:25.820]  равно единице. Из-за такого крестика. Отсюда получаем напрямую, что 1 альфа х минус y плюс лямбда
[38:25.820 --> 38:33.300]  лямбда единица плюс 1 минус мю мю ноль. Вот. Отсюда х выписывается. Очень легко. Х равняется минус альфа
[38:33.300 --> 38:42.700]  плюс y. Вот. Это раз. И тут сразу же появляется ограничение дополнительное, конечно же. Раз х равно...
[38:42.700 --> 38:50.500]  У нас х равен t. Не хватает... Так, ну понятно. Не хватает неявного ограничения, что t у нас больше нуля. Вот.
[38:50.500 --> 38:58.180]  Х равно t, да. Будет больше либо равно нуля в этом случае. Вот. То есть, значит, что это значит? Это
[38:58.180 --> 39:05.900]  значит, что y у нас больше либо равно альфа. Теперь, наоборот, лямбда у нас равно нулю, мю не равно нулю.
[39:05.900 --> 39:14.140]  Оттого, что мю не равно нулю, следует, что х равен минус t. Вот. И более того, следует, что мю равно единица.
[39:14.140 --> 39:19.460]  Теперь мы напишем все то же самое. То есть, 1 альфа х минус y. Только выражение плюс лямбда минус мю
[39:19.460 --> 39:27.140]  станет плюс ноль минус 1. Равно нулю. Отсюда х будет равен альфа плюс... Альфа плюс y. И эта штука
[39:27.140 --> 39:33.940]  меньше либо равна нуля, потому что х равен минус t, t положить. То есть, y меньше... Брайан минус альфа.
[39:33.940 --> 39:39.700]  Вот. То есть, получили вот такие два условия. Теперь осталось понять, что происходит, когда у нас y от
[39:39.700 --> 39:48.740]  минус альфа до альфы. Вот. Здесь мы можем сделать... Сейчас, секунду. Я пойму одну простую вещь. В какой
[39:48.740 --> 39:54.180]  случае мы еще не рассмотрели или не до конца рассмотрели? А, наверное, когда y величины не ноль.
[39:54.180 --> 40:01.940]  Значит, х ноль. И что мы тогда можем сказать относительно того, как y расположен у нас?
[40:01.940 --> 40:11.580]  То есть... Сейчас. Ну что получается? Минус 1 на альфа y плюс лямбда минус мю равно нулю. Вот. И мы знаем,
[40:11.580 --> 40:20.140]  что сумма лямбда и мю равна у нас единице. А тут эта штука не сумма. Обидно. Точно все правильно.
[40:20.140 --> 40:25.220]  Думаю, время это проверить. Да, вроде правильно. Ну да, градиент по t тут исключительно такой,
[40:25.220 --> 40:29.620]  никуда ему не деться. То есть, смотрите, как что получилось. Получилось, что... Так, здесь 15.
[40:30.620 --> 40:37.140]  Что если строить зависимость у икса от у, мы только что получили, что у нас, допустим, вот здесь вот у
[40:37.140 --> 40:46.020]  нас минус альф, тут альфа. И при у меньше, чем минус альфа, то есть вот здесь вот, у нас х равен, а у плюс...
[40:46.020 --> 40:51.380]  То есть, типа вот так. Если же наоборот, то так. Сейчас вы понимаете, что происходит между ними. Но между
[40:51.380 --> 40:57.260]  ними хочется, конечно, получить ноль. Вот. И я сейчас пытаюсь сообразить оперативно, каким образом это
[40:57.260 --> 41:03.780]  можно сделать. Итак, как будто бы нужно, чтобы... Верно, сейчас это легко получится. Сейчас давайте-ка посмотрим.
[41:03.780 --> 41:16.020]  Так. Это что значит? Что y равен mu минус лямбда делить на альфа. При этом лямбда плюс mu равно
[41:16.020 --> 41:21.860]  единице. Так, нужно ли нам еще что-то? Так, а тут, кажется, не совсем правильно. Вот так. Так, лямбда,
[41:21.860 --> 41:28.660]  получается, равняется единице минус mu. И если сюда подставлять, получается, что y равен чему? 1
[41:28.660 --> 41:36.060]  минус 2 mu делить на альфа. Вот. Если теперь... Что мы можем увидеть? Мы можем сказать, что у нас mu
[41:36.060 --> 41:45.500]  большое. Каким-то образом хочется получить ограничение на модуль y. Вот. И не хватает. Кто-нибудь,
[41:45.500 --> 41:49.460]  может быть, видит, чего не хватает? Ой, что-то я что-то неправильно написал. Никто меня не
[41:49.460 --> 41:55.540]  управляет. Да, прям. Это же все не так. Наоборот. Типа, я переношу, у меня получается, я умножаю на
[41:55.540 --> 42:01.620]  минус альфа, умножается на mu минус лямбда. Вот так. Поэтому после подстановки... Значит, я уже
[42:01.620 --> 42:08.740]  перестал спрятать немножко. Так. Лямбда равняется этой штуке и получается это альфа на лямбда минус
[42:08.740 --> 42:17.700]  mu. То есть это альфа на 1 минус 2 mu. Да, при этом mu может быть не отрицательно. Вот. И, соответственно,
[42:17.700 --> 42:25.140]  если mu 0, то x равно альфе. А если mu... Сколько там? 1, да? Ну, правильно. Да, все отлично как раз
[42:25.140 --> 42:32.660]  получается. Тут же вот эта штука означает, что у нас лямбда и mu не ноль одновременно, а значит mu ну
[42:32.660 --> 42:38.340]  нуля до одного. Вот. Отсюда следует, что y либо принадлежит от минус альфы до альфы. Все,
[42:38.340 --> 42:44.860]  отлично получилось. То есть при нуле альфа при единиц минус альфы. И в этом случае у нас получается,
[42:44.860 --> 42:50.380]  что здесь ноль. Отсюда следует, что вот это условие у нас лежит где? Вот здесь. В том,
[42:50.380 --> 42:57.300]  вот это условие у нас вот и собственно вот. И последнее, какой это еще нужен? Цвет такой,
[42:57.300 --> 43:03.820]  отличающийся существенно. Красный, розовый. Так, конечно, как палитра так себе. Ну,
[43:03.820 --> 43:09.180]  давайте так. Синий уже есть. И зеленый. Вот. И зеленый это вот это выражение. Вот оно,
[43:09.180 --> 43:18.820]  ну вот здесь. Что это нам дает? Это нам дает свойство решения, что если у нас x кт минус альф кт f
[43:18.820 --> 43:25.180]  штрих от x кт достаточно мало, то есть по норме меньше альфа, то после соответственно x ка плюс
[43:25.180 --> 43:30.140]  первое с соответствующими компонентами там it-ами будет равно нулю. Потому что оно поет просто-напросто
[43:30.140 --> 43:34.740]  вот в эту коле. Таким образом, вот это вот оператор называется soft thresholding понятным причинам. То
[43:35.740 --> 43:41.540]  есть мягкий порог. Вот. И является стандартом для получения разреженного решения, если вы
[43:41.540 --> 43:48.820]  таковой хотите, вашей задачи. Вот. То есть наличие такой вот регуляризации, где она тут, вот такой вот
[43:48.820 --> 43:54.180]  регуляризации приводит к тому, что решение для минимизации нашей исходной функции, вот этой
[43:54.180 --> 43:59.380]  например, будет являться разреженным. Понятно ли это? Поставьте плюс, если понятно, и минус иначе.
[43:59.380 --> 44:04.500]  Понятно ли каким образом получилось вывести выражение для soft thresholding? Так, вижу плюс,
[44:04.500 --> 44:10.580]  один, второй. А, окей, вижу, здорово. Вот. Значит, это, да, важно, важно отметить, что метод,
[44:10.580 --> 44:14.780]  максимальный градиентный метод для такой задачи называется ISTA, может быть где-то встретите,
[44:14.780 --> 44:20.900]  быстро его ускоряется FISTA. Спольза прямо такими на собственные для примера использования,
[44:20.900 --> 44:24.700]  собственно, максимального градиентного метода для получения разреженного решения в линейной
[44:24.700 --> 44:30.260]  модели. Вот. Здесь 23. Значит, мы сегодня, то есть вот такая же абсолютная технология, она
[44:30.260 --> 44:35.180]  воспроизводится и для случая матриц, когда вам как бы хотите получить разреженные матрицы.
[44:35.180 --> 44:39.700]  Если вот здесь вот вам, например, в качестве решения подходит матрица, где мало нулей,
[44:39.700 --> 44:45.020]  мало не нулей, делайте ровно то же самое только с элементами, с элементами матрицы. Если же вам
[44:45.020 --> 44:48.940]  нужна малоранговость, то тут чуть похитрее, и, видимо, с этого надо будет начать следующее занятие.
[44:48.940 --> 44:54.540]  Нонс. Как обеспечить малоранговость решения? Учитывая, что функция ранга не выпукла, вот,
[44:54.540 --> 45:01.100]  там нужно построить некоторые малоранговость решения. Нужно строить какую-то выпуклую
[45:01.100 --> 45:06.340]  аппроксимацию функции ранга. Вот. Нам потребуется вспомнить, что такое сингулярное разложение,
[45:06.340 --> 45:13.420]  сингулярные числа, как они связаны между собой, с артегональностью некоторых матриц и всем таким.
[45:13.420 --> 45:18.220]  Вот. И окажется, что проксимальный оператор от некоторых матричных функций, которые не распадаются
[45:18.220 --> 45:22.980]  на функции от их элементной матрицы, тем не менее, можно достаточно эффективно вычислить.
[45:22.980 --> 45:26.980]  Опираюсь исключительно на проксимальное отображение от сингулярных. Такая вот немножко
[45:26.980 --> 45:30.980]  магия будет. Но, в общем, это будет в следующий раз. Вот. И в следующий раз мы обсудим,
[45:30.980 --> 45:35.060]  что такое метод из штрафов, что такое метод модифицированный функции лагранжа. Хотя,
[45:35.060 --> 45:39.140]  по-моему, в плане было немножко не это. Сейчас, может быть, и я вас немножко обманываю.
[45:39.140 --> 45:46.660]  Где план-то? План, отлично. Да, слушайте, будет не это. Будет про полупределенную оптимизацию,
[45:46.660 --> 45:51.780]  но, видимо, и про все эти вещи тоже будет немножко сказано. А чтобы уж совсем не ломать,
[45:51.780 --> 45:56.580]  не ломать линейность, есть ли какие-то вопросы по сегодняшнему материалу? Ну,
[45:56.580 --> 46:01.460]  поскольку я не вижу в чате никаких вопросов, вроде никто тоже ничего не спрашивает. Вот.
[46:01.460 --> 46:10.340]  Тогда всем спасибо за внимание, за реакцию и за какие-то уточнения. Вот. Тогда до следующей
[46:10.340 --> 46:14.820]  недели заметки я сейчас выложу и примерный план на ближайшие несколько недель тоже.
[46:14.820 --> 46:18.180]  Всем спасибо и до свидания. Спасибо, до свидания.

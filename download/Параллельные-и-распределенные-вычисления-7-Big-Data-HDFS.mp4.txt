[00:00.000 --> 00:08.400]  Итак, всем привет, меня зовут Ивченко Олег, и на этом
[00:08.400 --> 00:11.680]  курсе я у вас проведу 4 занятия, одно из которых
[00:11.680 --> 00:15.240]  будет в зуме, и мы с вами обсудим большие данные,
[00:15.240 --> 00:19.400]  то есть экосистему ходуб.
[00:19.400 --> 00:22.400]  По плану у нас будет первое занятие сегодняшнее про
[00:22.400 --> 00:27.160]  HDFS, про хранение больших данных, и уже на этой неделе,
[00:27.160 --> 00:29.120]  по крайней мере, у некоторых групп начнутся семинары
[00:29.120 --> 00:30.120]  по HDFS.
[00:30.120 --> 00:33.840]  Потом еще будет два занятия про MapReduce, где мы будем
[00:33.840 --> 00:38.000]  писать на ходуб стриминга на питоне, и будет еще последнее
[00:38.000 --> 00:42.600]  занятие про Apache Hive, это SQL-движок поверх ходуба.
[00:42.600 --> 00:49.240]  Так что, кто SQL забыл, повторяйте SQL, вам он на этом курсе пригодится.
[00:49.240 --> 00:54.280]  Ну и так, давайте поймем сначала, что такое большие
[00:54.280 --> 00:58.320]  данные, потому что четкого критерия, которое отделяет
[00:58.320 --> 01:02.000]  маленькие данные от больших, его нет, но есть некоторые
[01:02.000 --> 01:03.000]  свойства.
[01:03.000 --> 01:06.520]  Например, свойства Volume, если данных так много, что
[01:06.520 --> 01:10.320]  они не помещаются на один ваш компьютер, на ваш домашний
[01:10.320 --> 01:15.520]  комп или на один сервер, то значит нужно какую-то
[01:15.520 --> 01:18.840]  придумать систему, которая будет из нескольких компов
[01:18.840 --> 01:21.840]  собирать что-то единое, вы будете этим пользоваться
[01:21.840 --> 01:24.440]  и хранить там эти данные.
[01:24.440 --> 01:29.200]  Потом Verity, это то, что данные у нас бывают разные, и вот
[01:29.200 --> 01:35.160]  тут самый яркий пример, это банковские системы, как
[01:35.160 --> 01:37.880]  они принимают решение, выдавать клиенту кредит
[01:37.880 --> 01:38.880]  или нет.
[01:38.880 --> 01:42.280]  Вот, кто может подсказать, какие для этого данные используются,
[01:42.280 --> 01:45.960]  ну кроме стандартных банковских каких-то историй.
[01:45.960 --> 02:01.640]  Платёжспособность, ну то есть какой у него доход,
[02:01.640 --> 02:04.600]  то есть понятно, что мы смотрим историю внутри банка, как
[02:04.600 --> 02:07.720]  он брал кредиты, не брал, как он их отдавал, есть ли
[02:07.720 --> 02:12.080]  у него вклады, мы ещё смотрим его доход, иногда это сложно
[02:12.080 --> 02:15.280]  оценить, потому что он допустим получает зарплату одну в
[02:15.280 --> 02:18.720]  одном банке, другую в другом, вот, но как можно оценить
[02:18.720 --> 02:22.280]  ещё, это можно использовать всякие сторонние ресурсы,
[02:22.280 --> 02:24.960]  например, можно вытащить какие-нибудь данные социальных
[02:24.960 --> 02:29.440]  сетей, допустим, какие он покупки совершает, в какие
[02:29.440 --> 02:32.640]  путешествия ездит, какое у него семейное положение,
[02:32.640 --> 02:35.560]  то есть вот эти вот косвенные данные, они поставляются
[02:35.560 --> 02:39.360]  из разных мест, в разном формате, с разной периодичностью,
[02:39.360 --> 02:42.920]  и нужно всё это запихать в какую-то единую воронку,
[02:42.920 --> 02:47.000]  потом это всё обработается и будет решение true и false,
[02:47.000 --> 02:51.600]  вот это вот проверяете, ну и про velocity это то, как часто
[02:51.600 --> 02:56.080]  мы данные обрабатываем, есть несколько вариантов,
[02:56.080 --> 02:59.240]  ну вот рассмотрим самые два крайних случая, первый
[02:59.240 --> 03:01.800]  крайний случай это когда вам нужно сформировать
[03:01.800 --> 03:06.240]  какой-нибудь отчёт раз в месяц или раз в квартал,
[03:06.240 --> 03:08.920]  когда вы собираете данные в течение трёх месяцев,
[03:08.920 --> 03:12.880]  запускаете какую-то задачу и она вам этот отчёт делает,
[03:12.880 --> 03:16.920]  и вторая крайность это real-time обработка, например,
[03:16.920 --> 03:20.640]  рекламная система, вы, наверное, все сталкивались с тем,
[03:20.640 --> 03:25.000]  что вот вы ищете какой-то запрос в магазине, хотим
[03:25.000 --> 03:29.040]  купить там, не знаю, новый powerbank, вы его выбираете
[03:29.040 --> 03:33.600]  и через какое-то время на всех сайтах, где есть рекламные
[03:33.600 --> 03:37.480]  баннеры, вам начинают рекомендовать powerbank, это делается быстро,
[03:37.480 --> 03:41.800]  если мы будем ждать два дня, пока эта джеба отработает,
[03:41.800 --> 03:48.160]  то это будет никому не нужно, это вот real-time обработка,
[03:48.160 --> 03:50.680]  ну и для всего этого, для того, чтобы хранить большие
[03:50.680 --> 03:54.480]  данные, хотя бы просто хранить, нам нужна какая-то
[03:54.480 --> 03:58.320]  новая система и нам нужно несколько серверов, желательно,
[03:58.320 --> 04:01.240]  то есть есть два варианта, мы можем купить сервер
[04:01.240 --> 04:04.640]  побольше, если нам не хватает текущего, но мы рано или
[04:04.640 --> 04:07.440]  поздно упрёмся в то, что уже некуда будет вставлять
[04:07.440 --> 04:11.000]  дополнительные плашки, ну или сервер будет просто
[04:11.000 --> 04:15.320]  тупо дорого стоить, если кто-нибудь видел, как зависит
[04:15.320 --> 04:20.440]  цена от мощности сервера, то, скажем так, есть стандартные
[04:20.440 --> 04:24.280]  сервера, линейка, их цена растёт примерно линейно
[04:24.280 --> 04:27.200]  в зависимости от мощности, в какой-то момент мы переходим
[04:27.200 --> 04:32.400]  в разряд high-performance computing, HPC, и там цена сразу подскакивает
[04:32.400 --> 04:37.920]  экспоненциально, ну вот, например, одна из историй
[04:37.920 --> 04:41.840]  из моего опыта, в одной компании мы как-то искали,
[04:41.840 --> 04:45.480]  какие сервера можно арендовать для работы нейронок, для
[04:45.480 --> 04:49.600]  нейронок что нужно, видеокарты, обычно в стандартном серваке
[04:49.600 --> 04:57.560]  сколько видеокарт, кто знает, 8, обычно 8, бывает 6, но мы
[04:57.560 --> 05:03.920]  хотели побольше, и вот с трудом получилось найти 12, 16 получилось
[05:03.920 --> 05:07.960]  найти только в одной какой-то южнокорейской компании,
[05:07.960 --> 05:11.520]  которая эти сервера производила, и стоил такой сервер не
[05:11.520 --> 05:18.640]  как два по восемь, а как десять по восемь, потому что
[05:18.640 --> 05:24.680]  там нужно другое железо, то есть нужна как бы, вся
[05:24.680 --> 05:29.040]  вот эта обвязка железная должна быть сделана по-другому,
[05:29.040 --> 05:32.320]  и на это затрачиваются какие-то допресурсы.
[05:32.320 --> 05:44.280]  Вот, поэтому лучше все-таки перейти к горизонтальному
[05:44.280 --> 05:47.720]  масштабированию, когда мы возьмем несколько серверов,
[05:47.720 --> 05:50.320]  объединим их в единую систему, и сейчас мы будем с вами
[05:50.320 --> 05:56.720]  разбираться, как такие системы работают.
[05:56.720 --> 05:59.560]  Их систем несколько, я буду рассказывать в основном
[05:59.560 --> 06:03.720]  про Hadoop, но Hadoop сейчас не единственно далеко, то
[06:03.720 --> 06:11.360]  есть сейчас есть и всякие облачные хранилища, например,
[06:11.360 --> 06:16.640]  наш МФТИ-центр обработки данных, его хранилища живут
[06:16.640 --> 06:20.480]  на системе CF, это тоже такая распределенная файловая
[06:20.480 --> 06:23.960]  система, в чем-то она похожа на Hadoop, но в чем-то она другая.
[06:23.960 --> 06:30.800]  С чего началась вообще вот такая эра распределенных
[06:30.800 --> 06:36.440]  систем, она началась со статьи 2003 года про Google File System,
[06:36.440 --> 06:41.480]  вот скриншот с этой статьей, но система была разработана
[06:41.480 --> 06:45.560]  в Гугле, она была закрытая, ей можно было только пользоваться,
[06:45.560 --> 06:49.680]  и поэтому через какое-то время появилась уже открытая
[06:50.680 --> 06:56.560]  Система называется Hadoop, потому что это было первое
[06:56.560 --> 07:00.520]  слово, которое сказал маленький сынишка основного разработчика
[07:00.520 --> 07:05.120]  этой системы, и, собственно, один из подмодулей этой
[07:05.120 --> 07:10.240]  системы, это Hadoop Distributed File System, HDFS.
[07:10.240 --> 07:12.240]  Как он устроен?
[07:12.240 --> 07:15.320]  Пока такой краткий обзор, дальше мы погрузимся в большее,
[07:15.320 --> 07:20.360]  на семинарах вы погрузитесь еще побольше, у нас есть
[07:20.360 --> 07:25.000]  несколько серверов, у серверов есть разные роли, например,
[07:25.000 --> 07:30.760]  вы здесь видите три основные роли, первая роль, с которой
[07:30.760 --> 07:33.960]  вы будете больше всего работать, это вот этот маленький
[07:33.960 --> 07:37.880]  клиент, то, что написано HDFS-клиент, еще называется в литературе
[07:37.880 --> 07:43.680]  H-Node, вам, скорее всего, на почту еще в начале курса
[07:43.680 --> 07:47.760]  кому-то только сейчас пришли аккаунты на сервере, который
[07:47.760 --> 07:54.120]  называется MIP-клиент, было такое?
[07:54.120 --> 07:57.120]  Ну кому не приходили, значит, еще придут до семинаров,
[07:57.120 --> 08:01.000]  кому-то уже пришли, вот этот сервер, на который вы будете
[08:01.000 --> 08:09.520]  заходить, это как раз H-Node, все остальные роли, вы на
[08:09.520 --> 08:12.520]  них, скорее всего, зайдете на одном семинаре, один
[08:12.520 --> 08:15.040]  раз, посмотрите, как там все устроено, как все это
[08:15.040 --> 08:21.520]  хранится, и на этом все закончится, потому что обычно, вы как
[08:21.520 --> 08:23.800]  разработчики на ходлупе, вам не нужно ходить на вот
[08:23.800 --> 08:27.240]  эти ноды, вам достаточно клиента, что у нас на этих
[08:27.240 --> 08:28.240]  нодах есть?
[08:28.240 --> 08:32.400]  У нас есть так называемые узлы имен и узлы данных,
[08:32.400 --> 08:36.920]  нейм-нод датонода, узел имен хранит логично имена,
[08:36.920 --> 08:39.240]  то есть сами данные там не хранятся, там хранится
[08:39.320 --> 08:44.200]  вот такая деревоподобная структура, в которой хранятся
[08:44.200 --> 08:48.320]  ссылки на данные, где хранятся какие данные, а сами данные
[08:48.320 --> 08:49.640]  хранятся на датонодах.
[08:49.640 --> 08:56.160]  Данные хранятся в виде блоков, ну как в любой файловой
[08:56.160 --> 09:00.840]  системе есть блоки, кто, кстати, помнит размер блока
[09:00.840 --> 09:08.960]  в стандартной файловой системе, FAT32 NTFS, 4 килобайта,
[09:09.680 --> 09:14.440]  кто не помнит, легко проверить, берёте какой-нибудь пустой
[09:14.440 --> 09:17.680]  носитель под этой файловой системой, неважно там диск,
[09:17.680 --> 09:23.080]  флешку, записываете минимальный файлик и смотрите в свойствах,
[09:23.080 --> 09:24.640]  в свойствах будет занято 4 килобайта.
[09:24.640 --> 09:29.480]  Вот в ходлупе тоже есть блоки, правда они сильно побольше.
[09:29.480 --> 09:35.120]  Ну и также важно понимать, что информация про эти блоки
[09:35.120 --> 09:37.680]  хранится на нейм-ноде в оперативке, почему?
[09:37.680 --> 09:39.120]  Потому что оперативка просто быстрее.
[09:39.120 --> 09:48.080]  Вот, то есть вот в самом упрощённом виде у нас вот такой вид
[09:48.080 --> 09:51.160]  имеет наша система HDFS.
[09:51.160 --> 09:54.280]  Конечно в реальной жизни тут всё посложнее, мы обсудим
[09:54.280 --> 09:59.360]  некоторые моменты как бы усложнения этой системы,
[09:59.360 --> 10:04.960]  но вот в самом простом варианте оно так.
[10:05.040 --> 10:06.040]  Какие есть проблемы?
[10:06.040 --> 10:09.680]  Ну наверное все увидели, что вот эта красненькая
[10:09.680 --> 10:16.640]  нода, она одна и если её не будет, то всё, вот поэтому
[10:16.640 --> 10:20.440]  у нас есть single point of failure и давайте разбираться что
[10:20.440 --> 10:21.440]  можно с ней сделать.
[10:21.440 --> 10:26.000]  Как можно решить проблему?
[10:26.000 --> 10:29.040]  Можно решить проблему тупо, давайте рядом поставим
[10:29.040 --> 10:33.240]  ещё одну нейм-ноду, они будут между собой синхронизироваться,
[10:33.240 --> 10:36.800]  одна упадёт, вторая будет жива, всё логично.
[10:36.800 --> 10:37.800]  Какие проблемы?
[10:37.800 --> 10:44.200]  Проблема в том, что нейм-нода дорогая и нам во-первых
[10:44.200 --> 10:46.960]  нужно дорого платить за сеть, потому что нужно гонять
[10:46.960 --> 10:50.040]  постоянно вот это дерево, оно часто меняется, оно
[10:50.040 --> 10:54.320]  меняется всё время, потому что хоть какая-то запись,
[10:54.320 --> 10:56.760]  всё у вас дерево поменялось, надо опять синхронизировать.
[10:56.760 --> 11:01.080]  Ну и оперативка, оперативки надо много, оперативка
[11:01.080 --> 11:04.840]  стоит дорого, поэтому это такой самый тупой дорогой
[11:04.840 --> 11:07.840]  вариант.
[11:07.840 --> 11:11.020]  Есть вариант не делать бэкапа, но немного помочь
[11:11.020 --> 11:16.120]  нашей нейм-ноде, вот наверное самый неудачный термин в
[11:16.120 --> 11:20.520]  литературе про ходу secondary-нейм-нода, потому что она как бы не
[11:20.520 --> 11:23.760]  secondary, она не вторичная нейм-нода, она просто помощник.
[11:23.760 --> 11:26.200]  Что этот помощник делает?
[11:26.200 --> 11:31.040]  Ну вот собственно написано в презе, что он делает, давайте
[11:31.040 --> 11:34.440]  посмотрим на картинке, то есть ещё раз в нейм-ноде
[11:34.440 --> 11:37.840]  хранится слепок файловой системы, но для того чтобы
[11:37.840 --> 11:40.800]  его постоянно не обновлять, не мержить вот это большое
[11:40.800 --> 11:44.600]  дерево с какими-то изменениями, мы изменения складываем
[11:44.600 --> 11:49.880]  в какое-то временное хранилище и через какое-то время берём
[11:49.880 --> 11:52.000]  батч изменений и их отплаиваем сразу.
[11:52.000 --> 11:58.600]  И вот всё, что делает secondary-нейм-нода, это она делает операцию
[11:59.440 --> 12:04.720]  берёт слепок файловой системы, берёт эти изменения, мержит
[12:04.720 --> 12:06.560]  одно с другим и возвращает новый слепок.
[12:06.560 --> 12:17.920]  Ну мержить каждый раз это будет просто долго, потому
[12:17.920 --> 12:19.480]  что структура сложная.
[12:19.480 --> 12:31.240]  Да, вот этот лог хранится, то есть там мы указываем,
[12:31.240 --> 12:33.720]  к какому файлу эти изменения относятся, но бывает так,
[12:33.720 --> 12:37.960]  что мы например пишем в один и тот же файл несколько
[12:37.960 --> 12:42.720]  раз, тогда перед тем как мерзить, мы вот эти 10 записей
[12:42.720 --> 12:45.680]  в один и тот же файл схлопнем и будет одна запись, но больше.
[12:45.680 --> 12:54.360]  За это отвечает две ноды, вот name-нода и secondary-нейм-нода.
[12:54.360 --> 13:01.720]  То есть к этим логам, да.
[13:01.720 --> 13:06.440]  У name-ноды он есть этот доступ, потому что оно на ней собственно
[13:06.440 --> 13:10.000]  хранится, а secondary-нейм-нода, как вы видите на схеме, она
[13:10.000 --> 13:11.000]  делает что?
[13:11.000 --> 13:15.160]  Так, тут нет никакого указателя, но в общем вы можете прочитать
[13:15.160 --> 13:20.880]  query for edit logs, то есть с какой-то периодичностью secondary-нейм-нода
[13:20.880 --> 13:24.440]  запрашивает у name-ноды вот этот файлик с edit-логами,
[13:24.440 --> 13:25.880]  мержит и отдает назад.
[13:25.880 --> 13:31.560]  Мержит со слепком файловой системы.
[13:31.560 --> 13:34.560]  Да.
[13:34.560 --> 13:39.560]  В name-ноде.
[13:39.560 --> 13:47.760]  Нет, в name-ноде не про update, в name-ноде есть два файла,
[13:47.760 --> 13:50.800]  слепок, но старый, и вот эти вот логи, которые надо
[13:50.800 --> 13:51.800]  апдейтить.
[13:51.800 --> 13:55.360]  Secondary-нейм-нода собственно их апдейтит, получается
[13:55.360 --> 13:57.760]  новый слепок, она его возвращает в name-ноду.
[13:57.760 --> 14:03.960]  Да, и с name-нод сломается все, да, это вот частый вопрос
[14:03.960 --> 14:07.000]  на собеседованиях, немного вы забежали вперед, что
[14:07.000 --> 14:10.760]  будет если сломается secondary-нейм-нода и что будет если сломается
[14:10.760 --> 14:11.760]  name-нода.
[14:11.760 --> 14:14.680]  Если сломается secondary, то просто вот эту операцию
[14:14.680 --> 14:17.880]  на себя возьмет name-нода, ничего как бы смертельного
[14:17.880 --> 14:18.880]  не случится.
[14:18.880 --> 14:21.800]  Если сломается основная name-нода, то все.
[14:21.800 --> 14:28.600]  В случае с бэкапом, да, все хорошо, только дорого.
[14:28.600 --> 14:32.000]  Здесь хуже, но дешевле, потому что secondary-нейм-нода,
[14:32.000 --> 14:34.600]  она делает одну операцию, ей не надо столько ресурсов.
[14:34.600 --> 14:40.040]  Но есть еще два варианта, как можно избавиться, хотя
[14:40.040 --> 14:51.040]  бы помочь избавиться от вот этого SPOV.
[14:51.040 --> 14:54.840]  Мы хотим еще и разгрузить name-ноду, потому что name-нода,
[14:54.840 --> 15:00.160]  она у нас контролирует ресурсы HDFS, она хранит слепок файловой
[15:00.160 --> 15:02.440]  системы, и если она еще и сливает вот эти слепки
[15:02.440 --> 15:05.880]  с логами, просто получается на нее очень много нагрузки.
[15:05.880 --> 15:19.560]  Нет, почему, он остается у нее просто.
[15:19.560 --> 15:23.240]  Откатиться можно, но смотри, а ты имеешь в виду, не можем
[15:23.240 --> 15:26.120]  ли мы и secondary-нейм-ноды восстановить, если у нас
[15:26.120 --> 15:27.600]  упала name-нода?
[15:28.600 --> 15:30.240]  Можем, но не все, и вручную.
[15:30.240 --> 15:37.520]  Этот слепок устаревший, но что значит устаревший?
[15:37.520 --> 15:41.000]  Это значит, что часть указателей на блоке уже не работает,
[15:41.000 --> 15:44.320]  мы их переместили, что-то мы обновили, что-то удалили,
[15:44.320 --> 15:47.560]  и вот это все мы восстановить не сможем, мы сможем восстановить
[15:47.560 --> 15:48.560]  какую-то часть.
[15:48.560 --> 15:54.560]  Ну какую-то часть, да, сможем, вот я знаю, что когда-то
[15:54.560 --> 15:58.160]  давно, лет, наверное, 7 назад, один мой коллега, работавший
[15:58.160 --> 16:01.680]  в Яндексе, потратил на это несколько дней полного
[16:01.680 --> 16:03.880]  рабочего времени, чтобы вот это все восстановить
[16:03.880 --> 16:04.880]  из secondary-нейм-ноды.
[16:04.880 --> 16:14.880]  В общем, сделали мы secondary-нейм-ноду, но с single point of failure у нас
[16:14.880 --> 16:15.880]  никуда не делся.
[16:15.880 --> 16:18.800]  Какие еще могут быть варианты?
[16:18.800 --> 16:21.600]  Могут быть варианты HDFS Federation, вот эти, кстати,
[16:21.600 --> 16:24.600]  вот эти две штуки, они вот прямо в реальной жизни
[16:24.600 --> 16:28.960]  используются, ну как и secondary-нейм-нода, то есть такого, чтобы не было
[16:28.960 --> 16:33.000]  ни secondary, ни вот этих двух штук, а была просто одна
[16:33.000 --> 16:38.120]  нейм-нода, такого нигде, ну, наверное, только в каких-то
[16:38.120 --> 16:42.240]  очень старых legacy-кластерах можно встретить, сейчас
[16:42.240 --> 16:43.240]  я такого уже не видел.
[16:43.240 --> 16:48.800]  Вот, то есть есть HDFS Federation, это когда у нас несколько
[16:48.800 --> 16:51.320]  нейм-нод и каждый из них берет себе свою какую-то
[16:51.320 --> 16:53.760]  папку или свой раздел файловой системы.
[16:53.760 --> 16:59.560]  Есть high availability-нейм-нод, это значит, что у нас несколько
[16:59.560 --> 17:02.360]  нейм-нод, но в отличие от бэкапа они могут быть
[17:02.360 --> 17:08.280]  не такие мощные, то есть в основной нейм-ноде нужно
[17:08.280 --> 17:11.720]  много оперативки, вот этим standby-нейм-нодом им много
[17:11.720 --> 17:14.400]  оперативки не нужно, они могут все свои данные хранить
[17:14.400 --> 17:18.480]  на диске, просто если основная нейм-нода отвалилась, то
[17:18.480 --> 17:22.040]  standby подхватит на какое-то время, будет тормозить,
[17:22.040 --> 17:24.840]  будет обращаться все время к своему диску, но, по крайней
[17:24.840 --> 17:26.640]  мере, как-то кластер работать будет.
[17:26.640 --> 17:38.600]  Да, они периодически синхронизируются с основной нейм-нодой и
[17:38.600 --> 17:39.600]  забирают слепок.
[17:39.600 --> 17:40.600]  Чего?
[17:40.600 --> 17:58.640]  Не может, потому что у нее нет функциональности
[17:58.640 --> 18:02.440]  отвечать на запросы клиента, то есть, помимо того, чтобы
[18:02.440 --> 18:05.480]  что-то хранить, должно быть еще API, с помощью которого
[18:05.480 --> 18:07.640]  будет взаимодействие, там такого API нету.
[18:07.640 --> 18:17.160]  Потому что они могут отваливаться, то есть, ты можешь одну сделать,
[18:17.160 --> 18:21.400]  но система как бы реализации HDFS поддерживает несколько,
[18:21.400 --> 18:24.960]  правда, желательно нечетное число, чтобы был quorum, чтобы
[18:24.960 --> 18:28.840]  можно было выбрать лидера и одну из standby на время
[18:28.840 --> 18:30.960]  сделать активной, пока основная лежит.
[18:30.960 --> 18:45.720]  Тем, что нам здесь не нужна такая мощная оперативка,
[18:45.720 --> 18:48.880]  мы здесь не стремимся работать быстро, а мы стремимся хоть
[18:48.880 --> 18:53.320]  как-то работать, то есть, хоть как-то подстраховать
[18:53.320 --> 18:58.600]  наш кластер, пока нейм-нода лежит.
[18:58.600 --> 19:02.400]  На счет того, что вот этот слепок, он лежит на диске,
[19:02.400 --> 19:06.400]  то есть, на все запросы клиента, вот этот standby, он идет
[19:06.400 --> 19:10.400]  на диск, это сильно дольше, чем идти в оперативку, но
[19:10.400 --> 19:13.000]  хотя бы какой-то адекватный отклик будет.
[19:13.000 --> 19:26.480]  Ну да, это как бы медленная замена, медленная, временная
[19:26.480 --> 19:27.480]  замена.
[19:27.480 --> 19:33.120]  Вот, теперь поговорим про датаноды.
[19:33.120 --> 19:37.360]  На датанодах файлы хранятся в виде блоков фиксированного
[19:37.360 --> 19:40.000]  размера, на самом деле, не всегда он фиксированный,
[19:40.000 --> 19:41.000]  но об этом позже.
[19:41.000 --> 19:48.320]  Ну и получается, что так как нод много, машинки будут
[19:48.320 --> 19:52.640]  падать чаще, и чтобы мы не теряли блоки при каждом
[19:52.640 --> 19:55.480]  падении, блоки реплицированы, то есть, у каждого блока
[19:55.480 --> 19:56.480]  есть n-копии.
[19:56.480 --> 20:00.360]  И эти копии распределяются по машинкам как можно дальше
[20:00.360 --> 20:03.360]  друг от друга, кстати, почему так, как вы думаете.
[20:03.360 --> 20:11.920]  Ну есть внутри системы некие данные про топологию
[20:11.920 --> 20:15.440]  сети, например, что вот у нас такие сервера находятся
[20:15.440 --> 20:18.560]  в одной стойке, такие в одном датацентре, а такие
[20:18.560 --> 20:22.800]  где-то георасподелены в разных ЦОДах, и вот система
[20:22.800 --> 20:25.960]  старается разнести вот эти блоки как можно дальше
[20:25.960 --> 20:27.840]  друг от друга в разные датацентры, желательно.
[20:27.840 --> 20:35.680]  А почему он быстрее будет?
[20:35.680 --> 20:42.680]  Да, и вот так и есть, а насчёт быстрее, ну тут скорее
[20:42.680 --> 20:46.000]  медленнее, правда есть такой вариант, что, например,
[20:46.000 --> 20:50.360]  если несколько команд работают с одним и тем же кластером,
[20:50.360 --> 20:52.840]  то вы просто можете поднять несколько вот этих edge
[20:52.840 --> 20:55.840]  нод, несколько клиентов, один клиент допустим в
[20:55.840 --> 20:59.120]  России, другой клиент где-то в США, и вы будете каждый
[20:59.120 --> 21:02.120]  со своих клиентов ходить на нужные вам ноды, и для
[21:02.120 --> 21:03.120]  вас это будет быстрее.
[21:03.120 --> 21:13.240]  Ну да.
[21:13.240 --> 21:16.600]  На самом деле экстренные ситуации на ходу кластера,
[21:16.600 --> 21:20.040]  они бывают часто, и цель файловой системы сделать
[21:20.040 --> 21:24.440]  так, чтобы мы на них не обращали внимания по возможности,
[21:24.440 --> 21:28.200]  то есть вот если взять вас, у каждого из вас есть
[21:28.200 --> 21:30.640]  свой нод, свой компьютер, как часто он ломается?
[21:30.640 --> 21:34.600]  Наверное раз в несколько лет, так?
[21:34.600 --> 21:38.520]  Вот, чего?
[21:38.520 --> 21:44.520]  Ну да, сколько-то лет там, 4-5 лет, как вы думаете,
[21:44.520 --> 21:51.120]  как часто ломаются сервера в больших ЦОДах?
[21:51.120 --> 21:52.120]  Берите выше.
[21:52.120 --> 21:57.240]  Нет, пару минут.
[21:57.240 --> 22:01.480]  Если мы берем большие промышленные ЦОДы, в которых там сотни
[22:01.480 --> 22:05.200]  тысяч серверов, то вот реально каждые несколько минут приходят
[22:05.200 --> 22:09.040]  отбивки, там перегрелась карточка, там диск отлетел,
[22:09.040 --> 22:13.480]  там еще что-то, ну как бы, а понятно, что в таких
[22:13.480 --> 22:17.720]  больших ЦОДах, когда мы чиним сервер, это обычно
[22:17.720 --> 22:21.080]  выглядит так, что-то сломалось, мы его выкинули и заменили
[22:21.080 --> 22:25.160]  на новое, потому что просто нет возможности чинить
[22:25.160 --> 22:29.200]  все детально, там брать ремапы, диски по целым
[22:29.200 --> 22:31.120]  суткам, ну такой возможности нет.
[22:31.120 --> 22:42.920]  Чем это плохо, то, что если у вас один клиент, одна
[22:42.920 --> 22:47.320]  клиентская машина, а кластер распределен по всей планете,
[22:47.320 --> 22:49.440]  то доступ до некоторых файлов он будет дольше.
[22:49.440 --> 22:58.600]  Вот, ну и как раз вот мы на этом слайде видим, что
[22:58.600 --> 23:01.800]  например блок Ц0 мы разнесли как можно дальше друг от
[23:01.800 --> 23:02.800]  друга.
[23:02.800 --> 23:07.120]  Там блок Ц3, вот он в датаноде 2 и больше его нигде нет,
[23:07.120 --> 23:09.840]  ну где-то он там есть в какой-то датаноде, просто мы его
[23:09.840 --> 23:10.840]  не видим.
[23:10.840 --> 23:16.080]  А вот блоки Ц1, они рядышком, блоки Ц5, они тоже рядышком.
[23:16.080 --> 23:17.080]  Почему?
[23:17.080 --> 23:20.800]  Потому что в Hadoop работает еще такой процесс, который
[23:20.800 --> 23:21.800]  называется балансер.
[23:21.800 --> 23:25.600]  В зависимости от того, как мы его настроим, он периодически
[23:25.600 --> 23:30.680]  пробегает весь скоп нашей файловой системы и балансирует
[23:30.680 --> 23:33.800]  ноды, то есть когда он видит, что одна нода перегружена,
[23:33.800 --> 23:37.240]  на ней очень много блоков, очень много информации,
[23:37.240 --> 23:39.680]  он переносит эти блоки по другим нодам так, чтобы
[23:39.680 --> 23:42.200]  все было равномерно.
[23:42.200 --> 23:45.560]  Поэтому так бывает, что изначально вот этот план
[23:45.560 --> 23:48.520]  разнести как можно дальше, он немного нарушается.
[23:48.520 --> 24:02.680]  Да, конечно.
[24:02.680 --> 24:05.240]  Мы делаем точную копию, вопрос в том, как мы ее
[24:05.240 --> 24:07.040]  хотим расположить на кластере.
[24:07.040 --> 24:19.880]  Потому что, смотри, мы это еще обсудим, когда посмотрим
[24:19.880 --> 24:22.200]  на чтение с файловой системы.
[24:22.200 --> 24:27.600]  Ну вот, допустим, у тебя есть блок, у него три реплики,
[24:27.600 --> 24:31.080]  одна в Москве, другая где-нибудь в Европе, а третья где-нибудь
[24:31.080 --> 24:32.080]  в Бразилии.
[24:32.080 --> 24:34.640]  Ну вот у тебя та нода, на которой лежит блок в Москве,
[24:34.640 --> 24:37.440]  она у тебя легла, тебе приходится идти в Бразилию, это долго.
[24:37.440 --> 24:46.640]  Вот, я сказал, что блоки все одинакового размера,
[24:46.640 --> 24:52.600]  на самом деле, это не совсем так, и давайте посмотрим,
[24:52.600 --> 24:55.560]  как это, как бы, посмотрим на деле, как это не так.
[24:55.560 --> 25:04.080]  Вот, а сейчас мы с вами зайдем на интерфейс Hadoop'a
[25:04.080 --> 25:06.800]  и посмотрим, что у нас там есть, как все хранится.
[25:06.800 --> 25:29.840]  Вот, мы с вами зашли на NameNodeUI, и что мы тут видим?
[25:29.840 --> 25:34.800]  Мы тут видим вот Configured Capacity, то есть сколько вообще
[25:34.800 --> 25:38.040]  данных мы можем хранить на нашем кластере с учетом
[25:38.040 --> 25:40.960]  репликаций, бэкапов, всего остального.
[25:40.960 --> 25:43.760]  DFS used, сколько у нас сейчас хранится, вот половину
[25:43.760 --> 25:49.400]  кластера у нас занято, non-DFS used, то есть полтерабайта
[25:49.400 --> 25:53.040]  у нас занято чем-то несвязанным с Hadoop'a, как вы думаете,
[25:53.040 --> 25:56.040]  что это может быть?
[25:56.040 --> 26:03.760]  Ну, корзина, если мы имеем дело с файловой системой
[26:03.760 --> 26:07.240]  Hadoop'a, то корзина, она будет тоже в этой файловой системе,
[26:07.240 --> 26:08.240]  значит, в Hadoop'a.
[26:08.240 --> 26:12.200]  А тут получается мы полтерабайта куда-то просто вот, куда-то
[26:12.200 --> 26:13.200]  использовали.
[26:13.200 --> 26:14.200]  Куда?
[26:14.200 --> 26:21.840]  Да, Hadoop'a, он написан на джаве, поэтому тоже часто
[26:21.840 --> 26:25.520]  на собесах задают вопросы, может ли Hadoop'a жить отдельно
[26:25.520 --> 26:26.520]  и общат всего?
[26:26.520 --> 26:29.560]  Ну, конечно же, нет, потому что Hadoop'a на джаве значит
[26:29.560 --> 26:33.240]  должна стоять джава, значит должна стоять ось, там библиотеки
[26:33.240 --> 26:37.960]  всякие, логи, и вот на вот это всё суммарно в масштабах
[26:37.960 --> 26:40.720]  всего кластера тратится полтерабайта.
[26:40.720 --> 26:50.520]  DFS Remaining, вот мы тут видим тоже, сколько осталось места.
[26:51.280 --> 27:00.160]  Вот видим, как у нас работают все ноды, сколько места занято
[27:00.160 --> 27:01.160]  на каждый из них.
[27:01.160 --> 27:16.000]  Ну, в принципе, тут с точки зрения нод больше ничего
[27:16.000 --> 27:18.760]  интересного нет, единственное, в каких режимах эти ноды
[27:18.760 --> 27:22.160]  могут находиться, то есть сейчас они у нас все в режиме
[27:22.160 --> 27:28.120]  in-service, вот может быть режим такой, что нода упала, может
[27:28.120 --> 27:32.240]  быть режим in-maintenance, то есть когда мы сами отключаем
[27:32.240 --> 27:41.000]  ноду для техподдержки какой-нибудь, и статус decommissioned это что
[27:41.000 --> 27:42.000]  значит?
[27:42.000 --> 27:43.400]  Это значит, что мы вывели ноду из кластера.
[27:43.400 --> 27:50.320]  Ну, неважно, списали, не списали, просто она у нас
[27:50.320 --> 27:53.880]  не является частью кластера, не хранит никакие блоки,
[27:53.880 --> 27:58.080]  то есть когда мы допускаем процесс decommissioned, это значит,
[27:58.080 --> 28:02.800]  что те блоки, которые были переместятся на другие
[28:02.800 --> 28:06.160]  ноды, чтобы сохранился фактор репликации, а саму ноду
[28:06.160 --> 28:10.440]  мы спокойно выведем, то есть вот это вот разные вещи,
[28:10.440 --> 28:15.320]  in-maintenance это мы ее просто штатно выключили или штатно
[28:15.320 --> 28:18.160]  отключили от кластера, но там данные остались, а
[28:18.160 --> 28:20.240]  вот это полностью мы ее убрали.
[28:20.240 --> 28:29.640]  Теперь, что касается блока, вот у нас есть возможность
[28:29.640 --> 28:32.320]  прогуляться по файловой системе, давайте посмотрим,
[28:32.320 --> 28:39.200]  что у нас тут есть, вот есть различные пользователи,
[28:39.200 --> 28:41.600]  скоро тут появятся и ваши, они будут начинаться на
[28:41.600 --> 28:51.880]  PD 2023 и так далее.
[28:51.880 --> 29:00.480]  Давайте посмотрим на какой-нибудь файл с данными, вот, например,
[29:00.480 --> 29:07.160]  файл 11,5 Гигов, что мы про него можем увидеть вообще.
[29:07.160 --> 29:12.120]  Фактор репликации 3, то есть он разбит на блоки, каждый
[29:12.120 --> 29:15.880]  блок имеет еще две копии.
[29:15.880 --> 29:18.920]  Блок size 128 Мб, вот тоже можно увидеть.
[29:18.920 --> 29:26.640]  И дальше вот давайте смотреть, что мы знаем про блок, ну
[29:26.640 --> 29:31.600]  кроме того, что мы можем его скачать, мы видим ID-шку
[29:31.600 --> 29:37.080]  блока, видим блокpool, блокpool это некое такое, как бы,
[29:37.080 --> 29:43.320]  некая такая область, в которой хранятся блоки,
[29:43.320 --> 29:46.120]  если у нас разветвленная структура, вот действительно
[29:46.120 --> 29:49.440]  разные дата-центры, разные стойки, тогда эти блокпулы
[29:49.440 --> 29:50.440]  будут разные.
[29:50.440 --> 29:54.680]  Но у нас стойка одна, дата-центр один и вообще весь этот
[29:54.680 --> 29:58.680]  кластер находится у нас там в подвале в КПМ, поэтому
[29:58.680 --> 29:59.680]  блокпул здесь один.
[29:59.680 --> 30:07.240]  Это значит, что каждый блок имеет еще две копии
[30:07.240 --> 30:08.240]  на разных нодах.
[30:08.240 --> 30:21.800]  Ну, как бы, само понятие блока и копии, у них семантика
[30:21.800 --> 30:26.440]  одна и та же, то есть если одна из копий недоступна,
[30:26.440 --> 30:28.920]  ты будешь читать другую копию, они равноценны полностью.
[30:28.920 --> 30:45.120]  А, здесь написано 128 мегабайт, вообще, в среднем это десятки
[30:45.120 --> 30:47.360]  или первые сотни мегабайт, то есть примерно вот так
[30:47.360 --> 30:51.080]  оно и есть, размер блока можно настраивать, то есть
[30:51.080 --> 30:54.000]  когда вы поднимаете кластер Hadoop, у вас там стандартный
[30:54.000 --> 30:57.360]  размер блока какой-то будет, а дальше для каждого файла
[30:57.360 --> 30:58.360]  можно задавать свой.
[30:58.360 --> 31:07.400]  Дальше, что мы видим еще, Generation Stamp, это некий номер,
[31:07.400 --> 31:10.840]  по которому Hadoop понимает, какой блок актуальный, какой
[31:10.840 --> 31:11.840]  нет.
[31:11.840 --> 31:15.320]  То есть, допустим, у нас есть у блока три реплики,
[31:15.320 --> 31:18.960]  но в какой-то момент машинка с одной репликой легла,
[31:18.960 --> 31:22.080]  мы сделали какие-то изменения, а до этой машинки они не
[31:22.080 --> 31:23.080]  докатились.
[31:23.080 --> 31:26.920]  Машинка проснулась, говорит, что у меня тоже есть реплика,
[31:26.920 --> 31:30.560]  но у нее уже другой Generation Stamp, и Hadoop понимает, что
[31:30.560 --> 31:32.040]  реплика уже устарела, ее надо обновить.
[31:32.040 --> 31:38.920]  Есть размер блока и написаны ноды, где эти сами реплики
[31:38.920 --> 31:39.920]  хранятся.
[31:39.920 --> 31:42.880]  Вот как раз три ноды у нас есть.
[31:42.880 --> 31:45.160]  Если мы будем переключаться по блокам, то мы увидим,
[31:45.160 --> 31:46.160]  что у нас меняется.
[31:46.160 --> 31:53.400]  У нас меняется ID, у нас меняется Generation Stamp, у нас меняются
[31:53.400 --> 31:56.400]  ноды, но у нас не меняется размер, то есть кажется,
[31:56.400 --> 31:57.400]  что он постоянный.
[31:57.400 --> 32:02.280]  Но если мы пойдем в самый последний блок, то, видите,
[32:02.280 --> 32:03.280]  размер изменился.
[32:03.280 --> 32:04.280]  Почему?
[32:04.280 --> 32:07.440]  Потому что, ну, просто размер файла не делится нацело
[32:07.440 --> 32:09.800]  на размер блока, и последний будет меньше.
[32:09.800 --> 32:15.320]  Есть еще и случаи, когда последний блок бывает больше.
[32:15.320 --> 32:16.320]  Когда это бывает?
[32:16.320 --> 32:18.600]  Это бывает тогда, когда вот этот самый последний
[32:18.600 --> 32:23.880]  блок, он очень маленький, и Hadoopу нет смысла тратить
[32:23.880 --> 32:26.320]  ресурсы на его хранение, он просто прикрепляется
[32:26.320 --> 32:27.320]  к предпоследнему.
[32:27.320 --> 32:31.600]  То есть если у нас имеются какие-то там первые проценты
[32:31.600 --> 32:38.120]  от размера блока, если бы тут было не 114, например,
[32:38.120 --> 32:42.680]  а просто 4, вот такой, если бы был размер, то он бы
[32:42.680 --> 32:46.320]  скорее всего прилепился к предпоследнему блоку.
[32:46.320 --> 32:48.960]  Зачем так делать?
[32:48.960 --> 32:52.560]  Дело в том, что про каждый блок мы храним метаинформацию.
[32:52.680 --> 32:56.200]  Мы храним ее на одной ноде, на нейм-ноде, в оперативке.
[32:56.200 --> 33:01.240]  И чем больше у нас блоков, тем хуже нашей оперативки
[33:01.240 --> 33:05.240]  нейм-ноды, тем быстрее она расходуется, поэтому хранить
[33:05.240 --> 33:06.600]  маленькие блоки вообще плохо.
[33:06.600 --> 33:13.440]  И если мы просто положим в Hadoop большой файл, то Hadoop
[33:13.440 --> 33:17.040]  его сам порежет по блокам, разнесет по нодам, все будет
[33:17.040 --> 33:18.040]  хорошо.
[33:18.040 --> 33:20.240]  А вот проблемы начинаются тогда, когда у нас много
[33:20.240 --> 33:21.240]  маленьких файлов.
[33:22.160 --> 33:27.360]  Hadoop не умеет слеплевать файлы в один блок, и просто
[33:27.360 --> 33:30.120]  он делает для каждого файла свой блок.
[33:30.120 --> 33:33.840]  Вы, например, сделали 100 файликов по 1 мегабайту.
[33:33.840 --> 33:36.680]  Если бы вы их слепили, у вас бы получился один блок
[33:36.680 --> 33:38.080]  обычного размера.
[33:38.080 --> 33:42.000]  Но Hadoop делает вам 100 маленьких блоков по 1 мегабайту.
[33:42.000 --> 33:46.680]  А вот нейм-нода будет расходоваться так же, как и 100 на 100, как
[33:46.680 --> 33:48.440]  на 100 блоков по 100 мегабайтам.
[33:51.240 --> 34:01.720]  Окей, по этой части какие-нибудь вопросы есть?
[34:01.720 --> 34:13.400]  По блоксайзу, вот тут надо понимать, что такое блоксайз.
[34:13.400 --> 34:17.200]  То есть, когда вы работаете с обычной файловой системой,
[34:17.200 --> 34:20.360]  то у вас выделится место на диске определенное.
[34:21.320 --> 34:24.920]  А здесь у нас как такового диска нет, у нас есть дата-ноды,
[34:24.920 --> 34:25.920]  есть нейм-ноды.
[34:25.920 --> 34:29.720]  И вот на нейм-ноде мета-информация, она одинакова действительно
[34:29.720 --> 34:32.280]  для любого блока, для какого бы размера он не был.
[34:32.280 --> 34:36.560]  Туда пишутся константы, кто владелец, какая дата-изменение,
[34:36.560 --> 34:39.280]  какие права доступа, к какому файлу он относится.
[34:39.280 --> 34:42.320]  То есть, вот такой словарик пишется, и он одинаковый,
[34:42.320 --> 34:43.640]  там нельзя сделать меньше никак.
[34:43.640 --> 34:48.240]  А с точки зрения дата-ноды, а какой смысл с точки зрения
[34:48.240 --> 34:51.160]  дата-ноды резервировать на диске лишнюю память?
[34:51.160 --> 34:52.160]  Зачем?
[34:52.160 --> 34:53.160]  Такого не делается.
[34:53.160 --> 34:56.080]  То есть, физически на дата-ноде будет храниться столько,
[34:56.080 --> 35:00.720]  сколько ты положишь, а вот нейм-нода будет загружаться
[35:00.720 --> 35:02.400]  неоптимально.
[35:02.400 --> 35:04.920]  И на самом деле, за нейм-нодом мы переживаем больше, потому
[35:04.920 --> 35:06.720]  что это оперативка, и она одна.
[35:06.720 --> 35:13.400]  В общем, когда мы читаем, мы находимся на клиенте,
[35:13.400 --> 35:14.840]  и читаем мы в два этапа.
[35:14.840 --> 35:20.480]  Сначала мы идем в нейм-ноду, спрашиваем, нам нужно прочитать
[35:20.480 --> 35:22.280]  такой-то файл, где нам взять данные.
[35:22.280 --> 35:26.920]  Нейм-нода возвращает нам вот эту вот пару, блок ID, блок
[35:26.920 --> 35:30.760]  location, то есть, она возвращает, какие блоки мы должны прочитать
[35:30.760 --> 35:32.280]  и где они находятся.
[35:32.280 --> 35:36.680]  Мы на клиенте выбираем самый ближайший блок по топологии,
[35:36.680 --> 35:38.120]  идем к нему и читаем.
[35:38.120 --> 35:42.080]  То есть, вот такие вот два этапа.
[35:42.080 --> 35:46.920]  Что касается записи.
[35:46.920 --> 35:49.720]  Запись тоже идет в несколько этапов.
[35:49.720 --> 35:51.920]  Сначала мы запрашиваем у нейм-ноды, куда нам можно
[35:51.920 --> 35:52.920]  записать.
[35:52.920 --> 35:55.320]  Нейм-нода говорит, куда, и мы идем на датоноды.
[35:55.320 --> 36:01.760]  И датоноды реплицируют данные друг с друга, то есть, мы
[36:01.760 --> 36:04.520]  не с клиента ходим по каждой ноде, а вот такой каскад
[36:04.520 --> 36:08.040]  получается, как вы видите на прессе.
[36:08.040 --> 36:11.160]  Записали на первую датоноду.
[36:11.160 --> 36:14.120]  Нейм-нода отправила OK в нейм-ноду, что все хорошо,
[36:14.120 --> 36:17.000]  мы записали, и дальше она уже передает данные на
[36:17.000 --> 36:18.000]  вторую.
[36:18.000 --> 36:21.400]  Вторая тоже говорит OK, и она уже передает на третью.
[36:21.400 --> 36:24.600]  Вот такой вот каскад, но пока этот каскад весь до
[36:24.600 --> 36:28.400]  конца не пройдет, наша команда записи не заканчивается.
[36:28.400 --> 36:32.320]  Вот такая репликация называется синхронная.
[36:32.320 --> 36:35.120]  Есть еще асинхронная, вам про нее расскажет Роман
[36:35.120 --> 36:38.120]  Липовский, когда будет говорить про кавку.
[36:38.600 --> 36:42.280]  Там вся соль в том, что репликация по факту не закончилась,
[36:42.280 --> 36:44.440]  система говорит, что уже все, и мы можем работать дальше.
[36:54.440 --> 36:58.320]  У HDFS, помимо UI, на которой мы только что смотрели,
[36:58.320 --> 37:02.480]  есть еще REST API, и сейчас мы как раз через REST API посмотрим,
[37:02.480 --> 37:04.240]  как работает чтение данных.
[37:08.240 --> 37:10.240]  Вот такая вот команда.
[37:10.240 --> 37:14.240]  Что такое REST API, как оно примерно работает, надо пояснять?
[37:17.240 --> 37:22.520]  Если совсем кратко, то когда мы заходим на какую-то
[37:22.520 --> 37:25.840]  веб-страницу, мы там видим, что презу видим, какие-то
[37:25.840 --> 37:31.800]  данные, они как-то там красиво отображаются, но можно сделать
[37:31.800 --> 37:35.200]  и по-другому, можно эти данные передавать в виде
[37:35.200 --> 37:38.040]  какой-то служебной информации, в виде джейсонов.
[37:38.040 --> 37:42.800]  То есть, когда у нас у какого-то сервиса есть набор страничек,
[37:42.800 --> 37:47.000]  при запросе к которым мы получаем какие-то служебные
[37:47.000 --> 37:50.120]  данные, плюс еще хорошо, если там есть возможность
[37:50.120 --> 37:53.360]  передавать параметры, есть возможность авторизации,
[37:53.360 --> 37:55.360]  то мы говорим, что у системы есть REST API.
[38:00.360 --> 38:05.360]  Честно, не вспомню, боюсь наврать, но посмотрю.
[38:08.360 --> 38:13.560]  Вот, ну и здесь мы тоже, собственно, делаем запрос
[38:13.560 --> 38:19.240]  к вот такой вот веб-страничке, у нее адрес, дальше через
[38:19.240 --> 38:22.960]  вот эти вот знаки вопроса и структуры типа Keira No Value
[38:22.960 --> 38:23.960]  мы передаем параметры.
[38:23.960 --> 38:29.600]  Вообще, есть несколько протоколов, чтобы обращаться
[38:29.600 --> 38:35.880]  к страничкам по HTTP, наверное, вы знаете, что есть GET-протокол,
[38:36.720 --> 38:39.720]  вот в данном случае, какой протокол используется.
[38:41.720 --> 38:44.720]  Да, используется GET, потому что чем характерен GET тем,
[38:44.720 --> 38:49.720]  что мы передаем плейнтекстом маленькие какие-то объемы
[38:49.720 --> 38:52.720]  данных и прямо их передаем через адресную строку.
[38:55.720 --> 38:57.720]  Вот, и так, что мы хотим этой страничкой сделать?
[38:57.720 --> 39:01.720]  Мы хотим подключиться к NameNode, мы только что на нее
[39:01.720 --> 39:04.720]  заходили MiptMaster и вот этот порт.
[39:05.560 --> 39:10.640]  Дальше WebHTFS Version 1, это вот тоже есть такое правило
[39:10.640 --> 39:14.040]  хорошего тона, что когда мы в системе разрабатываем
[39:14.040 --> 39:18.600]  Web API в какой-нибудь, мы сразу резервируем версию, вдруг
[39:18.600 --> 39:22.480]  у нас потом появится версия 2, версия 3, поэтому странички
[39:22.480 --> 39:26.400]  они содержат еще и версию, но в случае с ходубом версии
[39:26.400 --> 39:31.520]  2, версии 3 не появилось, поэтому вот версия 1, дальше
[39:31.520 --> 39:34.320]  путь к данным и дальше параметры, какие.
[39:35.320 --> 39:39.960]  OP, Operation равно Open и Length равно 10, то есть мы хотим открыть
[39:39.960 --> 39:41.920]  файл и прочитать 10 символов.
[39:47.920 --> 39:49.920]  Давайте мы выполним эту команду.
[39:53.920 --> 39:57.920]  Вот, мы хотели прочитать 10 символов, получилось у
[39:57.920 --> 40:00.920]  нас что-то не то, все видят, что не то получилось.
[40:01.920 --> 40:03.920]  Увеличить, наверное, надо.
[40:09.920 --> 40:12.920]  Я думаю, вам это просто понадобится в будущем.
[40:21.920 --> 40:24.920]  Есть целая куча кодов ответа разных, у них у каждого
[40:24.920 --> 40:30.920]  своя семантика, но для обычной жизни, я думаю, достаточно
[40:31.320 --> 40:33.320]  знать 4 основных класса.
[40:45.320 --> 40:48.320]  Кто знает, что вот эти классы HTTP ответов говорят?
[40:51.320 --> 40:53.320]  400, опять 500.
[40:53.720 --> 41:03.720]  Да, можно такую семантику использовать, чтобы запомнить,
[41:03.720 --> 41:04.720]  что это значит.
[41:05.720 --> 41:13.720]  200 проходи, то есть все хорошо, 300 уходи, 400 ты облажался,
[41:13.720 --> 41:17.720]  500 я облажался, то есть 400 это значит, что проблема
[41:17.720 --> 41:20.720]  на стороне юзера, как вы сказали, а 500 это вот сервис
[41:21.120 --> 41:24.120]  500, то есть там что-нибудь на нем не так, обычно пишет
[41:24.120 --> 41:25.120]  там тайм-аут.
[41:31.120 --> 41:35.120]  Редирект, ну уходи куда-то, да, вас куда-то послали,
[41:35.120 --> 41:36.120]  как здесь.
[41:37.120 --> 41:41.120]  Куда нас здесь послали, нас послали вот сюда, вот
[41:41.120 --> 41:43.120]  этот адрес, что это за адрес?
[41:43.120 --> 41:47.120]  Это адрес датоноды, то есть как вот мы только что
[41:47.520 --> 41:51.520]  увидели на схемке, мы пришли к нейм-ноде, нейм-нода нас
[41:51.520 --> 42:02.520]  переслала на датоноду, если мы пойдем сюда, то есть
[42:02.520 --> 42:10.520]  снова qrl-i, теперь мы видим вот это вот слово, это у
[42:10.520 --> 42:12.520]  нас как раз наши 10 символов.
[42:17.520 --> 42:23.520]  Ну на самом деле все это можно сделать за одну команду,
[42:23.520 --> 42:28.520]  надо просто добавить сюда еще минус l большое, тогда
[42:28.520 --> 42:32.520]  у нас сразу появятся редиректы, мы сразу по нему пройдем.
[42:47.520 --> 42:51.520]  Вот, как мы вообще можем работать с HDFS?
[42:51.520 --> 42:54.520]  Давайте я откачусь на вот этот вот слайд, то есть
[42:54.520 --> 42:59.520]  у HDFS-а есть несколько API, сам HDFS как бы, как часть
[42:59.520 --> 43:02.520]  Hadoop'а написан на джаве, поэтому у него есть Java API,
[43:02.520 --> 43:09.520]  есть Web API, мы на него только что смотрели, есть HDFS shell,
[43:09.520 --> 43:11.520]  то есть всякие команды типа
[43:17.520 --> 43:24.520]  Вот, то есть HDFS, DFS, минус что-нибудь, вот по ссылке,
[43:24.520 --> 43:30.520]  если хотите, можете зайти на вики, презы там выложены,
[43:30.520 --> 43:36.520]  вот пройти по ссылке, можете увидеть описание различных
[43:36.520 --> 43:41.520]  команд, и вот сейчас увеличу, многие команды, я думаю,
[43:41.520 --> 43:48.520]  то есть вот cat, chmod, chown, chgrp, cp, du, def, все вот это
[43:48.520 --> 43:51.520]  вот вы наверное знаете, кто работал с линуксом.
[43:51.520 --> 43:54.520]  Единственное, что вам не знакомо, это команды типа
[43:54.520 --> 43:58.520]  copy to local, copy from local, и оно же где-то тут есть get,
[43:58.520 --> 44:02.520]  и еще где-то тут есть put, то есть это перенести с
[44:02.520 --> 44:05.520]  обычной файловой системой в Hadoop и обратно, потому
[44:05.520 --> 44:09.520]  что вот когда мы заходим на клиент, здесь на самом
[44:09.520 --> 44:13.520]  деле у нас две файловые системы, это легко проверить.
[44:15.520 --> 44:19.520]  lshom, мы тут видим вот таких вот пользователей,
[44:23.520 --> 44:30.520]  HDFS, DFS, минус lshom выдаст нам ошибку, то есть такой папки
[44:30.520 --> 44:33.520]  нет, потому что это совершенно разные файловые системы.
[44:33.520 --> 44:38.520]  Вы наверное уже заметили, что команды типа HDFS, DFS они
[44:38.520 --> 44:41.520]  работают дольше, чем команды типа ls, почему?
[44:41.520 --> 44:45.520]  Понятно, что нам нужно сначала запустить java-процесс,
[44:45.520 --> 44:48.520]  потом постучаться в нейм-ноду, потом нам вернется ответ,
[44:48.520 --> 44:50.520]  в общем, это все работает дольше.
[44:50.520 --> 44:56.520]  Вот, ну, как бы java не все знают, с bash, bash слишком
[44:56.520 --> 45:00.520]  низкоуровневый тоже работать неудобно, а с REST-опи тем более,
[45:00.520 --> 45:02.520]  поэтому есть много разных оберток.
[45:04.520 --> 45:07.520]  Вот, например, несколько библиотек здесь можно увидеть,
[45:07.520 --> 45:10.520]  вот, например, в этой библиотеке у нас есть
[45:10.520 --> 45:12.520]  один библиотек, который работает дольше.
[45:12.520 --> 45:15.520]  И есть еще несколько других библиотек, они посложнее,
[45:15.520 --> 45:18.520]  потому что они еще и умеют map-reduce считать,
[45:18.520 --> 45:20.520]  потому что они еще и умеют map-reduce считать,
[45:20.520 --> 45:22.520]  потому что они еще и умеют map-reduce считать,
[45:22.520 --> 45:24.520]  потому что они еще и умеют map-reduce считать,
[45:24.520 --> 45:26.520]  потому что они еще и умеют map-reduce считать,
[45:26.520 --> 45:28.520]  потому что они еще и умеют map-reduce считать,
[45:28.520 --> 45:30.520]  потому что они еще и умеют map-reduce считать,
[45:30.520 --> 45:32.520]  потому что они еще и умеют map-reduce считать,
[45:32.520 --> 45:34.520]  потому что они еще и умеют map-reduce считать,
[45:34.520 --> 45:36.520]  потому что они еще и умеют map-reduce считать,
[45:36.520 --> 45:38.520]  потому что они еще и умеют map-reduce считать,
[45:38.520 --> 45:40.520]  потому что они еще и умеют map-reduce считать,
[45:40.520 --> 45:42.520]  потому что они еще и умеют map-reduce считать,
[45:42.520 --> 45:44.520]  потому что они еще и умеют map-reduce считать,
[45:44.520 --> 45:46.520]  потому что они еще и умеют map-reduce считать,
[45:46.520 --> 45:48.520]  потому что они еще и умеют map-reduce считать,
[45:48.520 --> 45:50.520]  потому что они еще и умеют map-reduce считать,
[45:50.520 --> 45:52.520]  потому что они еще и умеют map-reduce считать,
[45:52.520 --> 45:54.520]  потому что они еще и умеют map-reduce считать,
[45:54.520 --> 45:56.520]  потому что они еще и умеют map-reduce считать,
[45:56.520 --> 45:58.520]  потому что они еще и умеют map-reduce считать,
[45:58.520 --> 46:00.520]  потому что они еще и умеют map-reduce считать,
[46:00.520 --> 46:02.520]  потому что они еще и умеют map-reduce считать,
[46:02.520 --> 46:04.520]  есть обертки на плюсах, есть обертки на других разных языках,
[46:04.520 --> 46:06.520]  есть обертки на плюсах, есть обертки на других разных языках,
[46:10.520 --> 46:12.520]  ну и вот, например, если вы такую команду выполните,
[46:12.520 --> 46:14.520]  ну и вот, например, если вы такую команду выполните,
[46:14.520 --> 46:16.520]  вы сможете увидеть, что у вас
[46:16.520 --> 46:18.520]  команда du, которая в норме выдает одно число,
[46:18.520 --> 46:20.520]  она выдаст два числа для каждого файла,
[46:20.520 --> 46:22.520]  она выдаст два числа для каждого файла,
[46:22.520 --> 46:24.520]  один с учетом реплик, другой без учета реплик.
[46:28.520 --> 46:30.520]  Вот, теперь уже более такие интересные штуки,
[46:30.520 --> 46:32.520]  Давайте, мы с вами посмотрим на FSCK.
[46:32.520 --> 46:34.520]  Давайте, мы с вами посмотрим на FSCK.
[46:34.520 --> 46:36.520]  Вообще, что делает команда FSCK?
[46:36.520 --> 46:38.520]  Если не говорить про ходупа,
[46:38.520 --> 46:40.520]  вообще, что это за команда?
[46:46.520 --> 46:48.520]  File System Check, да, оно и есть.
[46:48.520 --> 46:50.520]  Если вы так детально не работали с линуксом,
[46:50.520 --> 46:52.520]  Если вы так детально не работали с линуксом,
[46:52.520 --> 46:54.520]  то, возможно, у вас была винда,
[46:54.520 --> 46:56.520]  и вы на этой винде запускали проверку диска.
[46:56.520 --> 46:58.520]  Это вот такой синий экран,
[46:58.520 --> 47:00.520]  который у вас есть в трансмерте,
[47:00.520 --> 47:02.520]  а тот, который пять шагов с процентами.
[47:02.520 --> 47:04.520]  Есть такой у вас?
[47:06.520 --> 47:08.520]  Ну, у кого будет возможность,
[47:08.520 --> 47:10.520]  дома посмотрите,
[47:10.520 --> 47:12.520]  там можно запустить проверку диска
[47:12.520 --> 47:14.520]  любого, который у вас есть в винде,
[47:14.520 --> 47:16.520]  и по факту будет запускаться тот же File System Check.
[47:16.520 --> 47:18.520]  и по факту будет запускаться тот же File System Check.
[47:18.520 --> 47:20.520]  Но если на вашем компьютере
[47:20.520 --> 47:22.520]  Но если на вашем компьютере
[47:22.520 --> 47:24.520]  желательно, чтобы у вас был File System Check 100%,
[47:24.520 --> 47:26.520]  то в ходупе File System Check
[47:26.520 --> 47:28.520]  далеко не всегда бывает 100%,
[47:28.520 --> 47:30.520]  это не страшно,
[47:30.520 --> 47:32.520]  но это позволяет нам поисследовать
[47:32.520 --> 47:34.520]  File System и понять, что где хранится.
[47:36.520 --> 47:38.520]  Давайте мы возьмем какую-нибудь папку.
[47:44.520 --> 47:46.520]  Вот, файл, с которым мы
[47:46.520 --> 47:48.520]  работали сегодня в начале занятия,
[47:48.520 --> 47:50.520]  вот он.
[47:56.520 --> 47:58.520]  То есть, QSCK с параметрами
[47:58.520 --> 48:00.520]  Minus Files, Minus Blocks, Minus Locations.
[48:00.520 --> 48:02.520]  Мы узнаем информацию,
[48:02.520 --> 48:04.520]  где у нас что хранится,
[48:04.520 --> 48:06.520]  по каким локациям.
[48:06.520 --> 48:08.520]  Сейчас выдастся большая простыня,
[48:08.520 --> 48:10.520]  и мы будем в ней разбираться.
[48:10.520 --> 48:12.520]  Вот она.
[48:12.520 --> 48:14.520]  Давайте смотреть
[48:14.520 --> 48:16.520]  с самого начальника,
[48:16.520 --> 48:18.520]  с самого начальника,
[48:18.520 --> 48:20.520]  с самого начальника,
[48:20.520 --> 48:22.520]  с самого начальника,
[48:22.520 --> 48:24.520]  с самого начальника,
[48:24.520 --> 48:26.520]  давайте смотреть с самого начала.
[48:26.520 --> 48:28.520]  Начало, вообще,
[48:28.520 --> 48:30.520]  когда мы что-нибудь запускаем в ходупе,
[48:30.520 --> 48:32.520]  вы в этом еще убедитесь,
[48:32.520 --> 48:34.520]  когда будет MapReduce, Hive.
[48:34.520 --> 48:36.520]  Начало, оно начинается вот так.
[48:36.520 --> 48:38.520]  Мы идем в name-ноду.
[48:40.520 --> 48:42.520]  Дальше, что нам эта команда выдала?
[48:42.520 --> 48:44.520]  Она выдала, что мы имеем дело с директорией.
[48:44.520 --> 48:46.520]  В этой директории
[48:46.520 --> 48:48.520]  у нас есть вот такой файл,
[48:48.520 --> 48:50.520]  он один, у него такой
[48:50.520 --> 48:52.520]  размер, столько-то блоков и статус OK.
[48:52.520 --> 48:54.520]  Дальше.
[48:54.520 --> 48:56.520]  Вот это вот
[48:56.520 --> 48:58.520]  0.1.2.3, это информация
[48:58.520 --> 49:00.520]  про блоки.
[49:00.520 --> 49:02.520]  Что мы видим про каждый блок?
[49:02.520 --> 49:04.520]  BlockPool.
[49:04.520 --> 49:06.520]  BlockID.
[49:06.520 --> 49:08.520]  Вот это
[49:08.520 --> 49:10.520]  второе число, кто помнит, что это
[49:10.520 --> 49:12.520]  с начала занятия.
[49:14.520 --> 49:16.520]  Версия GenerationStamp,
[49:16.520 --> 49:18.520]  правильно.
[49:18.520 --> 49:20.520]  Вот, дальше.
[49:22.520 --> 49:24.520]  Размер блока.
[49:24.520 --> 49:26.520]  Количество живых реплик,
[49:26.520 --> 49:28.520]  три.
[49:28.520 --> 49:30.520]  И дальше уже идет информация про каждую реплику.
[49:30.520 --> 49:32.520]  Что за информация?
[49:32.520 --> 49:34.520]  DataNodeWithStorage.
[49:34.520 --> 49:36.520]  Мы видим IP-шник датаноды,
[49:36.520 --> 49:38.520]  на которой все хранится.
[49:38.520 --> 49:40.520]  ID-шник внутренний,
[49:40.520 --> 49:42.520]  потому что IP может поменяться.
[49:42.520 --> 49:44.520]  И нам надо, чтобы
[49:44.520 --> 49:46.520]  ходуп как-то умел общаться
[49:46.520 --> 49:48.520]  все равно с этими нодами,
[49:48.520 --> 49:50.520]  даже если у них поменяется адрес.
[49:50.520 --> 49:52.520]  Ну и дальше флажок диск,
[49:52.520 --> 49:54.520]  потому что
[49:54.520 --> 49:56.520]  на самом деле датаноды можно развернуть
[49:56.520 --> 49:58.520]  не только на диске,
[49:58.520 --> 50:00.520]  можно использовать оперативку,
[50:00.520 --> 50:02.520]  можно развернуть ее
[50:02.520 --> 50:04.520]  в каком-нибудь облачном хранилище.
[50:04.520 --> 50:06.520]  Ну вот у нас диск.
[50:10.520 --> 50:12.520]  Это файлик, который
[50:12.520 --> 50:14.520]  хранит выборку
[50:14.520 --> 50:16.520]  со статей Википедии.
[50:20.520 --> 50:22.520]  Да, он занимает
[50:22.520 --> 50:24.520]  один этот с половиной гигов
[50:24.520 --> 50:26.520]  и вот 92 блока.
[50:32.520 --> 50:34.520]  Но это он совершенно не везде будет
[50:34.520 --> 50:36.520]  занимать 92 блока.
[50:36.520 --> 50:38.520]  Вот у меня есть параллельный кластер,
[50:38.520 --> 50:40.520]  на котором я тестирую разные задачки
[50:40.520 --> 50:42.520]  для курса.
[50:42.520 --> 50:44.520]  Там такие же файлы, но вот этот файл
[50:44.520 --> 50:46.520]  например занимает 367 блока.
[50:46.520 --> 50:48.520]  Потому что там другой размер блока.
[50:50.520 --> 50:52.520]  Вот дальше мы тут видим
[50:52.520 --> 50:54.520]  вот эту всю информацию.
[50:54.520 --> 50:56.520]  В конце мы видим,
[50:56.520 --> 50:58.520]  что вот у нас поменялся
[50:58.520 --> 51:00.520]  размер блока.
[51:04.520 --> 51:06.520]  Так, дальше статистика.
[51:08.520 --> 51:10.520]  Статистика по блокам.
[51:10.520 --> 51:12.520]  Статистика по блокам в каком-нибудь
[51:12.520 --> 51:14.520]  аномальном статусе, например
[51:14.520 --> 51:16.520]  underreplicated,
[51:16.520 --> 51:18.520]  то есть упала нода, реплик меньше,
[51:18.520 --> 51:20.520]  чем мы хотели бы.
[51:20.520 --> 51:22.520]  Misreplicated.
[51:22.520 --> 51:24.520]  Там реплика недоступна
[51:24.520 --> 51:26.520]  или вообще у нас все реплики потеряны.
[51:26.520 --> 51:28.520]  А есть еще
[51:28.520 --> 51:30.520]  overreplicated. Как вы думаете,
[51:30.520 --> 51:32.520]  в каких случаях может быть overreplicated?
[51:42.520 --> 51:44.520]  Все так.
[51:48.520 --> 51:50.520]  Ну вот и в конце
[51:50.520 --> 51:52.520]  мы видим, что наша файловая система
[51:52.520 --> 51:54.520]  из healthy.
[51:58.520 --> 52:00.520]  Можно также
[52:00.520 --> 52:02.520]  посмотреть инфу для любого блока.
[52:02.520 --> 52:04.520]  Возьмем блок.
[52:04.520 --> 52:06.520]  Вот этот допустим.
[52:18.520 --> 52:20.520]  Ты имеешь в виду вот это то,
[52:20.520 --> 52:22.520]  что в квадратной скобке здесь указано?
[52:22.520 --> 52:24.520]  Да, это каждая копия.
[52:24.520 --> 52:26.520]  Вот у нас, видите,
[52:26.520 --> 52:28.520]  live-replex 3.
[52:28.520 --> 52:30.520]  И вот у нас 1, 2, 3.
[52:36.520 --> 52:38.520]  Ну и обратная ситуация,
[52:38.520 --> 52:40.520]  когда мы увидели блок,
[52:40.520 --> 52:42.520]  выполнили для него FSTK
[52:42.520 --> 52:44.520]  и видим, какому файлу он относится.
[52:48.520 --> 52:50.520]  Вот на каких нодах хранится
[52:50.520 --> 52:52.520]  его реплики.
[52:52.520 --> 52:54.520]  Все это мы тоже видим.
[52:56.520 --> 52:58.520]  Healthy это значит,
[52:58.520 --> 53:00.520]  что у нас все в порядке.
[53:00.520 --> 53:02.520]  То есть статус здоров, нет никаких ошибок.
[53:02.520 --> 53:04.520]  На самом деле в реальной жизни
[53:04.520 --> 53:06.520]  спокойно может быть unhealthy.
[53:06.520 --> 53:08.520]  И это тоже ничего страшного.
[53:10.520 --> 53:12.520]  Да, у блока да.
[53:14.520 --> 53:16.520]  То есть вот здесь, вот в этой команде
[53:16.520 --> 53:18.520]  статусов много. Вот даже мы тут видим
[53:18.520 --> 53:20.520]  over-replicated, thunder-replicated
[53:20.520 --> 53:22.520]  и так далее.
[53:24.520 --> 53:26.520]  9 рабочих нод,
[53:26.520 --> 53:28.520]  еще плюс мастер и клиент.
[53:30.520 --> 53:32.520]  Ну вот мы с вами будем такие команды
[53:32.520 --> 53:34.520]  еще изучать на семинарах.
[53:38.520 --> 53:40.520]  Вот мы видим,
[53:40.520 --> 53:42.520]  что у нас доступны вот эти ноды
[53:42.520 --> 53:44.520]  и кто-то что-то на них даже
[53:44.520 --> 53:46.520]  запускает.
[54:02.520 --> 54:04.520]  Пока мы не перешли дальше,
[54:04.520 --> 54:06.520]  у меня будет к вам небольшой вопрос.
[54:06.520 --> 54:08.520]  Вы наверное знаете, что есть команды head
[54:08.520 --> 54:10.520]  и есть команды tail в линуксе.
[54:10.520 --> 54:12.520]  Head выводит первый n-строг, tail
[54:12.520 --> 54:14.520]  и последний n-строг. В HDFS
[54:14.520 --> 54:16.520]  такие команды тоже есть. Давайте проверим.
[54:16.520 --> 54:18.520]  HDFS, DFS
[54:20.520 --> 54:22.520]  minus tail
[54:22.520 --> 54:24.520]  file
[54:26.520 --> 54:28.520]  У нас вывелась одна строка.
[54:28.520 --> 54:30.520]  Тут много написано, это просто строка такая
[54:30.520 --> 54:32.520]  длинная.
[54:32.520 --> 54:34.520]  HDFS, DFS minus head
[54:38.520 --> 54:40.520]  не работает.
[54:40.520 --> 54:42.520]  Вопрос в том,
[54:42.520 --> 54:44.520]  почему разработчики
[54:44.520 --> 54:46.520]  HDFS и Hadoop
[54:46.520 --> 54:48.520]  не сделали head, а сделали tail?
[54:48.520 --> 54:50.520]  Через cat?
[55:08.520 --> 55:10.520]  То есть ты мечтал вот так,
[55:10.520 --> 55:12.520]  что мы через pipe передали и
[55:12.520 --> 55:14.520]  а что нам мешает
[55:14.520 --> 55:16.520]  точно так же сделать cat, потом tail
[55:16.520 --> 55:18.520]  через pipe?
[55:18.520 --> 55:20.520]  Прочитать все.
[55:20.520 --> 55:22.520]  А у нас тут может быть 5 терабайт.
[55:22.520 --> 55:24.520]  И придется 5 терабайт лопатить,
[55:24.520 --> 55:26.520]  пока мы до конца не найдем.
[55:26.520 --> 55:28.520]  Да, все именно так.
[55:42.520 --> 55:44.520]  Давайте тогда
[55:44.520 --> 55:46.520]  мы с вами решим
[55:46.520 --> 55:48.520]  вот такую задачку.
[55:48.520 --> 55:50.520]  Ее часто решают
[55:50.520 --> 55:52.520]  сис админы, когда нужно
[55:52.520 --> 55:54.520]  спланировать кластер Hadoop.
[55:54.520 --> 55:56.520]  То есть к нам
[55:56.520 --> 55:58.520]  пришел какой-то заказчик
[55:58.520 --> 56:00.520]  и хочет на нашем кластере
[56:00.520 --> 56:02.520]  сохранить данные.
[56:02.520 --> 56:04.520]  Мы им должны сказать,
[56:04.520 --> 56:06.520]  в каком виде мы должны
[56:06.520 --> 56:08.520]  эти файлы хранить
[56:08.520 --> 56:10.520]  с одной стороны.
[56:10.520 --> 56:12.520]  Вот эти минимальные объемы оперативки,
[56:12.520 --> 56:14.520]  при каких условиях он будет минимальный?
[56:14.520 --> 56:16.520]  Как нужно упаковать вот эти вот
[56:16.520 --> 56:18.520]  данные 2 питабайта, чтобы он был минимальный?
[56:18.520 --> 56:20.520]  Ну и какой он будет?
[56:40.520 --> 56:42.520]  А можно еще раз
[56:42.520 --> 56:44.520]  что мы сначала делаем?
[56:46.520 --> 56:48.520]  Так.
[57:10.520 --> 57:26.520]  А где, по твоему, будет храниться
[57:26.520 --> 57:28.520]  метаинформация
[57:28.520 --> 57:30.520]  и сам блок?
[57:34.520 --> 57:36.520]  Так нам именно
[57:36.520 --> 57:38.520]  на неймноде и надо,
[57:38.520 --> 57:40.520]  потому что на датаноде
[57:40.520 --> 57:42.520]  у нас понятно, что объем всех дисков
[57:42.520 --> 57:44.520]  2 питабайта.
[57:50.520 --> 57:52.520]  Нет, 2 питабайта
[57:52.520 --> 57:54.520]  это диск.
[57:54.520 --> 57:56.520]  А нам нужно посчитать,
[57:56.520 --> 57:58.520]  сколько мы хотим купить оперативы.
[57:58.520 --> 58:00.520]  Так.
[58:00.520 --> 58:02.520]  Как это посчитать?
[58:02.520 --> 58:04.520]  Еще раз.
[58:06.520 --> 58:08.520]  А зачем мы прибавляем
[58:08.520 --> 58:10.520]  размеры метаинформации?
[58:10.520 --> 58:12.520]  Она где хранится?
[58:12.520 --> 58:14.520]  Ага.
[58:22.520 --> 58:24.520]  6,4 на 3 умножить
[58:24.520 --> 58:26.520]  или поделить?
[58:26.520 --> 58:28.520]  Так.
[58:28.520 --> 58:30.520]  Потому что на этих 2 питабайтах
[58:30.520 --> 58:32.520]  должен помещаться не только сам блок,
[58:32.520 --> 58:34.520]  а и все его реплики.
[58:34.520 --> 58:36.520]  Поэтому 6,4 на 3.
[58:36.520 --> 58:38.520]  И вот 2 питабайта делим на 6,4 на 3,
[58:38.520 --> 58:40.520]  получаем количество блоков.
[58:40.520 --> 58:42.520]  У каждого блока метаинформация 600 байт.
[58:42.520 --> 58:44.520]  Домножаем, вот получаем.
[58:48.520 --> 58:50.520]  Потому что у каждого блока
[58:50.520 --> 58:52.520]  метаинформация 600 байт.
[58:52.520 --> 58:54.520]  Это вот данные про блок.
[58:56.520 --> 58:58.520]  Вот.
[59:10.520 --> 59:12.520]  600 байт нам нужно, чтобы хранить
[59:12.520 --> 59:14.520]  не только все 3 реплики,
[59:14.520 --> 59:16.520]  чтобы хранить вообще какую-то информацию
[59:16.520 --> 59:18.520]  про этот блок, чтобы NaimNode понимало,
[59:18.520 --> 59:20.520]  к чему этот блок относится.
[59:20.520 --> 59:22.520]  Логически.
[59:24.520 --> 59:26.520]  Логически.
[59:26.520 --> 59:28.520]  То есть с учетом всех физических кодей.
[59:32.520 --> 59:34.520]  Да.
[59:34.520 --> 59:36.520]  Да, да, да, да. Все так.
[59:50.520 --> 59:52.520]  У нас осталось еще немного времени.
[59:52.520 --> 59:54.520]  Я вам пока что покажу,
[59:54.520 --> 59:56.520]  как работает с HDFS на питоне.
[59:58.520 --> 01:00:00.520]  Сейчас я проверю,
[01:00:00.520 --> 01:00:02.520]  установлен ли у нас,
[01:00:02.520 --> 01:00:04.520]  установлен это либо или нет.
[01:00:04.520 --> 01:00:06.520]  Потому что
[01:00:10.520 --> 01:00:12.520]  на дисках, которые 2 питабайта,
[01:00:12.520 --> 01:00:14.520]  должен храниться не только
[01:00:14.520 --> 01:00:16.520]  сам блок, а и его копий.
[01:00:16.520 --> 01:00:18.520]  У каждого блока
[01:00:18.520 --> 01:00:20.520]  есть три копии.
[01:00:20.520 --> 01:00:22.520]  Где их хранить?
[01:00:22.520 --> 01:00:24.520]  У нас кроме вот этого диска,
[01:00:24.520 --> 01:00:26.520]  на каждом блоке
[01:00:26.520 --> 01:00:28.520]  есть 3 копии.
[01:00:28.520 --> 01:00:30.520]  Не всё так же, вот.
[01:00:30.520 --> 01:00:32.520]  Потому что у всех vantageков,
[01:00:32.520 --> 01:00:36.520]  Где их хранить? У нас, кроме вот этого диска 2 питабайта, больше ничего нет.
[01:00:36.520 --> 01:00:38.520]  Значит, они должны там поместиться.
[01:00:46.520 --> 01:00:53.520]  Мы храним информацию о всех копиях, но просто она хранится в одном месте, и вот эти вот 600 байты-то она и есть.
[01:00:53.520 --> 01:01:01.520]  Так, секунду.
[01:01:23.520 --> 01:01:33.520]  Да, где лежит, кто владелец, какие права доступа, все вот это вот.
[01:01:39.520 --> 01:01:43.520]  Ну, скорее не совсем так, просто ссылка на файл.
[01:01:43.520 --> 01:01:50.520]  Давайте тоже посмотрим, пока у нас есть время, как оно вообще хранится, всем кратко.
[01:01:51.520 --> 01:01:55.520]  Для этого нам надо зайти на ноду.
[01:01:55.520 --> 01:01:58.520]  Вот, мы находимся на одной из нод.
[01:01:58.520 --> 01:02:04.520]  Я сразу возьму себе root-up, потому что там часть файлов будет недоступна под обычным пользователем.
[01:02:04.520 --> 01:02:07.520]  Так или иначе, нода это у нас просто сервак под линуксом.
[01:02:07.520 --> 01:02:10.520]  Но в этом серваке под линуксом есть папка dfs.
[01:02:10.520 --> 01:02:13.520]  В папке dfs есть dn, датонода.
[01:02:13.520 --> 01:02:17.520]  Дальше есть current, это вот папка с блоками.
[01:02:17.520 --> 01:02:20.520]  Блок pull, он у нас один.
[01:02:20.520 --> 01:02:23.520]  Снова current.
[01:02:23.520 --> 01:02:27.520]  И там еще должна быть папочка finalized.
[01:02:27.520 --> 01:02:31.520]  Вот, в finalized лежат блоки.
[01:02:31.520 --> 01:02:36.520]  Такой вот странной структуре из списка подпапок.
[01:02:36.520 --> 01:02:39.520]  Можно сделать вот так.
[01:02:39.520 --> 01:02:44.520]  И мы увидим, что у нас бывают подпапки вложенные.
[01:02:44.520 --> 01:02:47.520]  Вложенность в глубины бывает и 2, и 3, и 4.
[01:02:47.520 --> 01:02:50.520]  Бывают пустые.
[01:02:50.520 --> 01:02:55.520]  В общем, вот эти вот сабдиры, на самом деле, только нейм-нода может разобраться,
[01:02:55.520 --> 01:03:00.520]  что соответствует этим сабдирам.
[01:03:00.520 --> 01:03:04.520]  То есть сама датонода, она не знает, чему это соответствует.
[01:03:09.520 --> 01:03:11.520]  Да, в обычной файловой системе.
[01:03:11.520 --> 01:03:15.520]  Но есть вопрос, почему мы не резервируем место на диске, когда у нас блок маленький.
[01:03:15.520 --> 01:03:18.520]  А зачем? Мы просто положили файл на ноду.
[01:03:26.520 --> 01:03:29.520]  Также интересно посмотреть на мастер.
[01:03:29.520 --> 01:03:34.520]  На мастере у нас все хранится в оперативке, но система, по крайней мере,
[01:03:34.520 --> 01:03:39.520]  на этом кластере настроена так, что периодически бэкапится на диск.
[01:03:41.520 --> 01:03:44.520]  У вас будет домашка по HDFS первая.
[01:03:44.520 --> 01:03:47.520]  Там нужно будет немного походить по нодам.
[01:03:47.520 --> 01:03:52.520]  Но в остальном вам больше, кроме первой домашки, это делать не придется.
[01:03:56.520 --> 01:04:02.520]  Но если не будете администратором Big Data инфраструктуры, то, наверное, не придется.
[01:04:02.520 --> 01:04:04.520]  Вот, тоже current.
[01:04:06.520 --> 01:04:10.520]  И у нас тут есть два типа файлов.
[01:04:10.520 --> 01:04:12.520]  Вот мы их видим.
[01:04:12.520 --> 01:04:14.520]  fs-image и edits.
[01:04:14.520 --> 01:04:18.520]  Мы с вами сегодня начали занятия с того, что у нас
[01:04:18.520 --> 01:04:21.520]  name-нода хранит слепок файловых файловых.
[01:04:21.520 --> 01:04:27.520]  И у нас есть два типа файловых файловых файловых файловых файловых файловых файловых.
[01:04:27.520 --> 01:04:30.520]  Мы с вами сегодня начали занятия с того, что у нас
[01:04:30.520 --> 01:04:33.520]  name-нода хранит слепок файловой системы,
[01:04:33.520 --> 01:04:36.520]  edit-логи. Вот у нас edit-in-progress.
[01:04:36.520 --> 01:04:39.520]  Это значит, у нас еще файл с логами не закрыт, не сохранен.
[01:04:39.520 --> 01:04:42.520]  Просто в него кто-то...
[01:04:42.520 --> 01:04:45.520]  Пока мы еще в него пишем.
[01:04:46.520 --> 01:04:50.520]  Если мы сейчас будем как-нибудь изменять файловую систему,
[01:04:50.520 --> 01:04:53.520]  то на логи запишутся сюда.
[01:04:57.520 --> 01:05:01.520]  Что касается Python, для того, чтобы у нас работал HDFS CLI,
[01:05:01.520 --> 01:05:04.520]  нам надо, чтобы
[01:05:06.520 --> 01:05:09.520]  в нашей home-директории был вот такой файлик.
[01:05:09.520 --> 01:05:11.520]  Вам на семинарах тоже покажут.
[01:05:11.520 --> 01:05:14.520]  Вам нужно будет такой файлик себе сделать
[01:05:14.520 --> 01:05:17.520]  в вашей домашней директории.
[01:05:17.520 --> 01:05:21.520]  Он будет у всех одинаковый, будет отличаться только последняя строчка.
[01:05:22.520 --> 01:05:25.520]  Теперь запускаем Python.
[01:05:27.520 --> 01:05:30.520]  Вот.
[01:05:30.520 --> 01:05:33.520]  Что это такое? Мы импортировали config.
[01:05:33.520 --> 01:05:36.520]  Config — это специальный класс, который содержит данные
[01:05:36.520 --> 01:05:39.520]  про файловую систему.
[01:05:39.520 --> 01:05:42.520]  Что-то он берет из самой системы, что-то берет из этого файла.
[01:05:42.520 --> 01:05:46.520]  То есть просто наш Python-процесс понимает, куда ему идти.
[01:05:51.520 --> 01:05:54.520]  Дальше мы создаем директор.
[01:05:55.520 --> 01:05:58.520]  Дальше мы создаем клиент.
[01:05:58.520 --> 01:06:01.520]  Сейчас будет понятно, зачем он нам нужен.
[01:06:01.520 --> 01:06:04.520]  Config get client.
[01:06:06.520 --> 01:06:09.520]  Дальше client, например, list.
[01:06:15.520 --> 01:06:18.520]  Вот. Вот такую команду можно выполнить,
[01:06:18.520 --> 01:06:21.520]  и мы увидим список вложенных файловых папок.
[01:06:21.520 --> 01:06:24.520]  Или вот так, например.
[01:06:27.520 --> 01:06:30.520]  Это статус по папке.
[01:06:30.520 --> 01:06:33.520]  То есть такой fsck в урезанной версии.
[01:06:33.520 --> 01:06:37.520]  Вы могли заметить, что библиотека на Python
[01:06:37.520 --> 01:06:40.520]  почему-то работает сильно быстрее, чем команды
[01:06:40.520 --> 01:06:43.520]  типа HDFS, DFS- что-то.
[01:06:43.520 --> 01:06:46.520]  Как вы думаете, почему?
[01:06:46.520 --> 01:06:49.520]  Ну так это же библиотека более
[01:06:49.520 --> 01:06:52.520]  высокого уровня, по идее.
[01:07:04.520 --> 01:07:07.520]  Оптимизирует.
[01:07:07.520 --> 01:07:10.520]  Понятно, что как-то оптимизирует.
[01:07:10.520 --> 01:07:13.520]  Вопрос как?
[01:07:13.520 --> 01:07:16.520]  Если мы хотим открыть команды HDFS, DFS- что-нибудь,
[01:07:16.520 --> 01:07:19.520]  они запускают Java.
[01:07:19.520 --> 01:07:22.520]  На то, чтобы запустилось Java, надо время.
[01:07:22.520 --> 01:07:25.520]  Потом они эту Java убивают,
[01:07:25.520 --> 01:07:28.520]  потому что мы не храним state.
[01:07:28.520 --> 01:07:31.520]  Если бы мы хранили state между командами,
[01:07:31.520 --> 01:07:34.520]  пришлось бы эти Java процессы держать
[01:07:34.520 --> 01:07:37.520]  открытыми постоянно.
[01:07:37.520 --> 01:07:40.520]  Это накладно, поэтому нам просто приходится
[01:07:40.520 --> 01:07:43.520]  каждый раз писать полные пути.
[01:07:43.520 --> 01:07:46.520]  Мы не можем в HDFS сделать CD.
[01:07:46.520 --> 01:07:49.520]  Мы просто не можем перейти между папками,
[01:07:49.520 --> 01:07:52.520]  потому что вот это состояние, где мы находимся,
[01:07:52.520 --> 01:07:55.520]  надо где-то хранить.
[01:07:55.520 --> 01:07:58.520]  Для этого надо держать Java процесс открытый.
[01:07:58.520 --> 01:08:01.520]  Поэтому мы каждый раз стартуем Java процесс,
[01:08:01.520 --> 01:08:04.520]  каждый раз его стопаем, и на это тратится куча времени.
[01:08:04.520 --> 01:08:07.520]  Здесь мы вот, собственно, вот этот клиент
[01:08:07.520 --> 01:08:10.520]  и этот конфиг мы его создали.
[01:08:10.520 --> 01:08:13.520]  В этот момент у нас стартовал Java процесс.
[01:08:13.520 --> 01:08:16.520]  И дальше мы уже с готовым Java процессом
[01:08:16.520 --> 01:08:19.520]  внутри него играемся.
[01:08:19.520 --> 01:08:22.520]  Вот кто проходил курс по базам данных,
[01:08:22.520 --> 01:08:25.520]  вам, наверное, рассказывали, что есть коннекшены
[01:08:25.520 --> 01:08:28.520]  в базе данных, и можно, когда вы работаете с BDS,
[01:08:28.520 --> 01:08:31.520]  сделать более примитивно,
[01:08:31.520 --> 01:08:34.520]  когда вы на каждое подключение
[01:08:34.520 --> 01:08:37.520]  делаете коннекшен, потом его убиваете.
[01:08:37.520 --> 01:08:40.520]  А можно использовать коннекшен пулы.
[01:08:40.520 --> 01:08:43.520]  То есть набор открытых коннекшенов,
[01:08:43.520 --> 01:08:46.520]  которые сами поддерживаются, сами переоткрываются,
[01:08:46.520 --> 01:08:49.520]  если упали.
[01:08:49.520 --> 01:08:52.520]  Ну, в частности, вот кто ходит на курс по Java,
[01:08:52.520 --> 01:08:55.520]  вам там расскажут про спринг, он использует именно такую модель.
[01:08:55.520 --> 01:08:58.520]  Вот тоже мы ускоряемся,
[01:08:58.520 --> 01:09:01.520]  за счет того, что мы не тратим время на создание коннекшен.
[01:09:01.520 --> 01:09:04.520]  На самом деле,
[01:09:04.520 --> 01:09:07.520]  вот эту вот проблему с тем,
[01:09:07.520 --> 01:09:10.520]  что мы не можем перейти между файлами,
[01:09:10.520 --> 01:09:13.520]  между папками, эту проблему пытались решать,
[01:09:13.520 --> 01:09:16.520]  и ее решили.
[01:09:16.520 --> 01:09:19.520]  Вот у нас на кластере стоит
[01:09:19.520 --> 01:09:22.520]  одна интересная библиотечка,
[01:09:22.520 --> 01:09:25.520]  вам ее покажут на семинаре,
[01:09:25.520 --> 01:09:28.520]  но она очень дырявая.
[01:09:28.520 --> 01:09:31.520]  Тоже семинаристы вам расскажут,
[01:09:31.520 --> 01:09:34.520]  что, пользуясь этой библиотекой,
[01:09:34.520 --> 01:09:37.520]  можно вообще получить рута внутри ходупа.
[01:09:37.520 --> 01:09:40.520]  Не внутри сервера, а внутри файловой системы HDFS
[01:09:40.520 --> 01:09:43.520]  можно получить рута.
[01:09:43.520 --> 01:09:46.520]  Таких дырок в ходупе, к сожалению, много.
[01:09:46.520 --> 01:09:49.520]  Просто когда разрабатывался ходуп,
[01:09:49.520 --> 01:09:52.520]  цель была создать систему, которая будет отказоустойчивой,
[01:09:52.520 --> 01:09:55.520]  надежной в плане работы с данными,
[01:09:55.520 --> 01:09:58.520]  но мы не думали о безопасности,
[01:09:58.520 --> 01:10:01.520]  так детально не думали.
[01:10:01.520 --> 01:10:04.520]  Получилось, что довольно много дырок,
[01:10:04.520 --> 01:10:07.520]  и в реальных бизнес-системах
[01:10:07.520 --> 01:10:10.520]  как минимум кубернетис,
[01:10:10.520 --> 01:10:13.520]  а керберас, прошу прощения,
[01:10:13.520 --> 01:10:16.520]  используется керберас, чтобы сделать хотя бы авторизацию.
[01:10:20.520 --> 01:10:23.520]  В новой версии сделали то,
[01:10:23.520 --> 01:10:26.520]  что более удобная связка с керберасом.
[01:10:26.520 --> 01:10:29.520]  То есть керберас уже прижился
[01:10:29.520 --> 01:10:32.520]  и просто он используется в связке с ходупом.
[01:10:32.520 --> 01:10:35.520]  В принципе, этого достаточно,
[01:10:35.520 --> 01:10:38.520]  керберас для авторизации
[01:10:38.520 --> 01:10:41.520]  и закрыть все лишние порты,
[01:10:41.520 --> 01:10:44.520]  потому что этот порт, который я вам даже сегодня показывал,
[01:10:44.520 --> 01:10:47.520]  этот 50070,
[01:10:47.520 --> 01:10:50.520]  если к нему подключиться, если он будет открыт наружу
[01:10:50.520 --> 01:10:53.520]  то сможет без какой-либо авторизации
[01:10:53.520 --> 01:10:56.520]  выполнить запросы
[01:10:56.520 --> 01:10:59.520]  из разряда того, что мы сегодня делали.
[01:10:59.520 --> 01:11:02.520]  Только, например, не open, а delete.
[01:11:02.520 --> 01:11:05.520]  Можно просто дропнуть все файлы, которые есть в ходу.
[01:11:08.520 --> 01:11:11.520]  Поэтому порты надо закрыть,
[01:11:11.520 --> 01:11:14.520]  керберас в реальной жизни надо поставить.
[01:11:16.520 --> 01:11:19.520]  На этом мы с вами теоретическую часть
[01:11:19.520 --> 01:11:22.520]  про HDFS заканчиваем,
[01:11:22.520 --> 01:11:25.520]  и все, что мне осталось сказать,
[01:11:25.520 --> 01:11:28.520]  это сделать такой обзор литературы,
[01:11:28.520 --> 01:11:31.520]  что можно про HDFS почитать подробнее.
[01:11:31.520 --> 01:11:34.520]  Нет.
[01:11:34.520 --> 01:11:37.520]  На кластере закрыты максимально порты,
[01:11:37.520 --> 01:11:40.520]  поэтому если кто-то что-то сделать нехорошее,
[01:11:40.520 --> 01:11:43.520]  то мы будем знать, что это вы,
[01:11:43.520 --> 01:11:46.520]  и убедительно просим так не делать.
[01:11:47.520 --> 01:11:50.520]  Порт только SSH открыт,
[01:11:50.520 --> 01:11:53.520]  все, больше ничего не открыто.
[01:11:59.520 --> 01:12:02.520]  Это все-таки не очень удобно для бизнеса,
[01:12:02.520 --> 01:12:05.520]  потому что иногда нужно зайти на WebMod,
[01:12:05.520 --> 01:12:08.520]  иногда нужно подключиться к какому-то порту напрямую,
[01:12:08.520 --> 01:12:11.520]  не пробрасывая...
[01:12:11.520 --> 01:12:14.520]  Мы с вами можем сделать SSH туннели,
[01:12:14.520 --> 01:12:17.520]  а иногда SSH туннели делать сложно,
[01:12:17.520 --> 01:12:20.520]  если мы какую-то большую систему запускаем,
[01:12:20.520 --> 01:12:23.520]  которая связывается с Hadoop.
[01:12:25.520 --> 01:12:28.520]  Что касается обзора литературы,
[01:12:28.520 --> 01:12:31.520]  что в действительности делает secondary name-node?
[01:12:31.520 --> 01:12:34.520]  То, что я вам объяснял, только более подробно текстом.
[01:12:34.520 --> 01:12:37.520]  Дальше, статья про архитектуру HDFS.
[01:12:37.520 --> 01:12:40.520]  Когда-то, где-то года два-три назад,
[01:12:40.520 --> 01:12:43.520]  существовала такая компания, как Hortonworks,
[01:12:43.520 --> 01:12:46.520]  она производила систему
[01:12:46.520 --> 01:12:49.520]  для автоматизации работы с Hadoop.
[01:12:49.520 --> 01:12:52.520]  То есть, некий пакетный менеджер
[01:12:52.520 --> 01:12:55.520]  для Hadoop, Aspark, Hyvo
[01:12:55.520 --> 01:12:58.520]  и всего, что в экосистему Hadoop сейчас входит.
[01:12:58.520 --> 01:13:01.520]  Помимо этого, ребята ввели блок,
[01:13:01.520 --> 01:13:04.520]  но в какой-то момент, в 2020 году,
[01:13:04.520 --> 01:13:07.520]  компания Hortonworks была куплена
[01:13:07.520 --> 01:13:10.520]  более крупной компанией Cloudera
[01:13:10.520 --> 01:13:13.520]  и этот блок перестал существовать.
[01:13:13.520 --> 01:13:16.520]  Осталась китайская копия
[01:13:16.520 --> 01:13:19.520]  на этом сайте, поэтому, кому интересно,
[01:13:19.520 --> 01:13:22.520]  посмотрите. Здесь детально описано,
[01:13:22.520 --> 01:13:25.520]  что хранится в datanode, в каком виде,
[01:13:25.520 --> 01:13:28.520]  что хранится в name-node.
[01:13:28.520 --> 01:13:31.520]  Мы с вами видели все эти сабдиры, блоки.
[01:13:31.520 --> 01:13:34.520]  То есть, все детально описано,
[01:13:34.520 --> 01:13:37.520]  что эти папочки хранят.
[01:13:38.520 --> 01:13:41.520]  Дальше, статья Константина Швачко,
[01:13:41.520 --> 01:13:44.520]  она такая чисто научная,
[01:13:44.520 --> 01:13:47.520]  без каких-то деталей про файлы.
[01:13:47.520 --> 01:13:50.520]  Сейчас подъявка догрузится.
[01:13:50.520 --> 01:13:53.520]  Там именно с помощью текста показано,
[01:13:53.520 --> 01:13:56.520]  как работает HDFS, почему именно так.
[01:13:56.520 --> 01:13:59.520]  Статья от Константина Швачко
[01:13:59.520 --> 01:14:02.520]  это один из довольно частых
[01:14:02.520 --> 01:14:05.520]  коммитеров в репозитории Hadoop.
[01:14:07.520 --> 01:14:10.520]  Так, страничка не работает.
[01:14:10.520 --> 01:14:13.520]  Видимо, надо зайти под VPN.
[01:14:13.520 --> 01:14:16.520]  У кого есть VPN, попробуйте зайти.
[01:14:16.520 --> 01:14:19.520]  Тут просто страничка
[01:14:19.520 --> 01:14:22.520]  с очень хорошим конспектом
[01:14:22.520 --> 01:14:25.520]  курса про Hadoop на курсене.
[01:14:25.520 --> 01:14:28.520]  То есть, в 2016-18 годах
[01:14:28.520 --> 01:14:31.520]  команда МФИТА,
[01:14:31.520 --> 01:14:34.520]  которая работает,
[01:14:34.520 --> 01:14:37.520]  делала курс на курсере.
[01:14:37.520 --> 01:14:40.520]  И вот один из слушателей нашего курса
[01:14:40.520 --> 01:14:43.520]  сам по своей инициативе написал
[01:14:43.520 --> 01:14:46.520]  очень подробный конспект занятия про HDFS
[01:14:46.520 --> 01:14:49.520]  и выложил его в GitHub.
[01:14:49.520 --> 01:14:52.520]  Дальше книжка Hadoop
[01:14:52.520 --> 01:14:55.520]  за Definitive Guide.
[01:14:55.520 --> 01:14:58.520]  Глава третья там про HDFS.
[01:14:58.520 --> 01:15:01.520]  На этом на сегодня все.
[01:15:01.520 --> 01:15:04.520]  Есть ли какие-то вопросы?

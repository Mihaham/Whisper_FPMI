[00:00.000 --> 00:12.000]  Александр, я заведую кафедру мат. основ управления и работаю на ней с 2005 года. И вот уже где-то лет
[00:12.000 --> 00:18.080]  15, может быть больше, но с момента как вот мы стали делать от кафедры всякие факультативы типа
[00:18.080 --> 00:25.440]  100-х анализ задач, была какая-то такая идея, что было бы здорово сделать объединяющий курс,
[00:25.440 --> 00:33.600]  который бы рассказывал о основных принципах, которые используются в анализе данных и в целом
[00:33.600 --> 00:39.840]  в такой математике, изучение каких-то больших задач. Эти задачи большие, они не обязательно
[00:39.840 --> 00:45.440]  связаны с анализом данных, это в том числе мат. моделирование, это стат. физика, я не знаю,
[00:45.440 --> 00:51.600]  это теория чисел, это что угодно, это например моделирование ковида, распространение эпидемии,
[00:51.600 --> 00:59.360]  это изучение биологических моделей, хищник, жертва. То есть, как бы сказать, когда мы пытаемся
[00:59.360 --> 01:04.600]  описать какое-то явление природы, мы часто делаем какие-то допущения предельные переходы. Как
[01:04.600 --> 01:10.600]  правило, эти предельные переходы связаны с тем, что агентов много, время достаточно большое,
[01:10.600 --> 01:16.600]  система живет. И вот эти предельные переходы, они приводят к так называемым эргодическим
[01:16.600 --> 01:22.280]  теоремам, к законам больших чисел, к нелинейным законам больших чисел, к явлению концентрации
[01:22.280 --> 01:29.320]  меры. И как побочный продукт, естественно, вся эта математика используется замечательным образом и
[01:29.320 --> 01:34.780]  в анализе данных. Но надо сказать, что эта математика, она же используется, например, в каком-то
[01:34.780 --> 01:40.760]  виде в перечислительной комбинаторике, она используется в биологии, математической естественной биологии.
[01:40.760 --> 01:46.880]  И как бы обозреть это студенту практически нереально, потому что ни одна программа,
[01:46.880 --> 01:53.920]  которая предусмотрена то или иной магистратурой, не предполагает, что как бы в основу будет
[01:53.920 --> 01:59.160]  положено какое-то единство в математике. Потому что вот вы изучаете какой-то тот или иной курс,
[01:59.160 --> 02:04.720]  и вам рассказывают, например, какой-нибудь из TFKP метод перевала или метод лоплассы,
[02:04.720 --> 02:09.760]  стационарной фазы, что-то такое очень специфическое. А потом оказывается, что все задачи
[02:09.800 --> 02:14.320]  симпатической комбинаторики, аналитической комбинаторики, ну не все, но большая заметная
[02:14.320 --> 02:19.120]  часть решается с помощью TFKP. Казалось бы, вот как это, естественно, в курсе TFKP вам это не
[02:19.120 --> 02:23.960]  расскажут. Да я больше того скажу, что в курсе TFKP, скорее всего, вам и метод перевала не рассказывали,
[02:23.960 --> 02:28.200]  но тем не менее. Или вы изучаете теории вероятности, вам рассказывают центральную предельную теорему,
[02:28.200 --> 02:34.240]  а в курсе оптимизации вам рассказывают всякие результаты о сходимости констат хастического
[02:34.240 --> 02:39.560]  градиентного спуска. И у вас это не совсем вяжется, потому что, ну как бы, где центральная
[02:39.560 --> 02:44.600]  предельная теорема и где вот эти результаты о сходимости SGD, стокастического градиентного спуска,
[02:44.600 --> 02:50.320]  на самом деле, собственно, чтобы аккуратно получить оценки скорости сходимости стокастического
[02:50.320 --> 02:54.160]  градиентного спуска, нужен неосимпатический вариант центральной предельной теоремы,
[02:54.160 --> 03:00.000]  который называется неравенство азума хевдинга. Для мартингала разности. Вот какие-то такие умные
[03:00.000 --> 03:04.480]  термины, но это настолько просто и настолько органично, это проще, чем CPT, доказывается. И это же
[03:04.480 --> 03:09.600]  неравенство азума хевдинга используется много где, в том числе, например, в изучениях
[03:09.600 --> 03:15.360]  романтических чисел графа, допустим, или еще что-нибудь. И вот этих связей огромное количество,
[03:15.360 --> 03:22.280]  и я не знаю, насколько у меня получится реализовать ту программу, которую я задумал, а программа
[03:22.280 --> 03:27.800]  связана с некой, так сказать, демонстрацией единства в математике на примере разных задач,
[03:27.800 --> 03:33.200]  прежде всего анализа данных, но не только. Моделирование интернета, это не совсем анализ данных,
[03:33.200 --> 03:39.640]  решение каких-то задач компьютер-сайенс, где невозможно чисто просто взять матрицу и умножить на
[03:39.640 --> 03:44.440]  другую матрицу, там надо как-то рандомизировать. И вроде как это все в каких-то отдельных курсах
[03:44.440 --> 03:49.960]  вам встречаться будет, безусловно. Например, на шестом курсе для студентов ФУПМА, ну это школа
[03:49.960 --> 03:55.240]  ПМИ, направление ФУПМ, там есть курс Кузюрина Фамина, ну сейчас его Фомин читает этот курс,
[03:55.240 --> 04:00.400]  там очень много алгоритмических вещей, связанных так называемые рандомизированными алгоритмами,
[04:00.400 --> 04:03.880]  вероятностным анализом алгоритмов, то есть какое-то пересечение будет. Естественно,
[04:03.880 --> 04:08.400]  с курсом случайных процессов будет пересечение по части эргодических там всяких теорем.
[04:08.400 --> 04:13.800]  Вот сегодня мы будем говорить о методе Markov Chain Monte Carlo, то есть популярный метод решения
[04:13.800 --> 04:19.160]  задач больших размеров, но как бы еще раз хочу подчеркнуть, что смотреть на это надо не как на
[04:19.160 --> 04:25.000]  набор примеров, а как на попытку продемонстрировать, прежде всего расставить акценты важность,
[04:25.000 --> 04:32.200]  каких-то, а более-менее одних и тех же математических концепций. Вот что такое конструкция Markov Chain Monte Carlo?
[04:32.200 --> 04:37.360]  Вот мы сегодня узнаем, что это на самом деле просто эргодическая теорема для Markov-х цепей и явление
[04:37.360 --> 04:43.080]  концентрации меры. Вот как бы и это же самое явление концентрации меры используется много где,
[04:43.080 --> 04:49.200]  и в основу вот конкретно из Markov Chain Monte Carlo положен часто принцип максимум правдоподобия
[04:49.200 --> 04:53.920]  еще дополнительно. Я какие-то сейчас слова говорю, которые в общем может все не все из вас знают,
[04:53.920 --> 04:59.480]  но эти слова будут многократно повторяться по ходу курса, и когда три-четыре раза вам в разных
[04:59.480 --> 05:04.440]  контекстах встретится одна и та же математическая конструкция, вы совершенно по-другому на нее
[05:04.440 --> 05:10.400]  начнете смотреть. Она не просто вам запомнится, она станет для вас родной, и мне очень важно показать,
[05:10.400 --> 05:17.000]  что в математике, и вот вообще в том, что вы изучаете на физтехе, есть десяток таких результатов,
[05:17.000 --> 05:22.420]  которые разбросаны по разным курсам, и если вам и рассказывались, то скорее всего в каком-то таком
[05:22.420 --> 05:27.820]  обрезанном виде, не очень акцентируя на этом внимание, а на них сидит вся современная наука,
[05:27.820 --> 05:32.900]  то есть вот многократно вы это же самое будете использовать практически чем бы вы не занимались,
[05:32.900 --> 05:39.580]  вот если вы идете в науку. И вот начать бы мне хотелось сегодня, ну да сразу скажу, что курс
[05:39.580 --> 05:44.940]  будет состоять из в основном очных лекций, но в какой-то момент возможно мы перейдем на онлайн
[05:44.940 --> 05:51.820]  формат, если потребуется привлечь специалистов, которые не смогут приехать. Ну вот у меня есть
[05:51.820 --> 05:56.060]  желание привлечь очень известного специалиста по теории информации, Григория Антоновича
[05:56.060 --> 06:02.980]  Кабатянского, который расскажет, надеюсь компактно, как теория информации и big data связаны,
[06:02.980 --> 06:08.420]  вот все основные принципы, которые вот совершили революцию в теории информации, и там в основе
[06:08.420 --> 06:13.100]  лежит явление концентрации меры, и часто это связано с шарами, с такими простыми достаточно
[06:13.100 --> 06:19.780]  явлениями, и таким простым фактом, что если шары трехмерные пересекаются, взять два центра,
[06:19.780 --> 06:26.620]  вот так вот, два шара-шара, и они пересекаются, этот объем достаточно заметен, то когда эта
[06:26.620 --> 06:32.500]  картинка будет рисоваться в Rn, и Rn стремится к бесконечности, то объем пересечения будет,
[06:32.500 --> 06:38.820]  ну можно чуть-чуть еще отдалить их, он будет стремиться к нулю, причем экспоненциально быстро,
[06:38.820 --> 06:45.900]  и вот такого типа какие-то простые наблюдения, ну аля там, что весь объем шара сосредоточен
[06:45.900 --> 06:53.220]  у корки, или если мы проведем экватор, то вся площадь многомерного шара сосредоточена около
[06:53.220 --> 06:58.060]  этой полоски, вот такого рода эффекты, довольно простые геометрически, мы их естественно выведем,
[06:58.060 --> 07:03.500]  они вот по сути лежат в основе там теории информации, и все, вот по сути вот эти принципы,
[07:03.500 --> 07:08.700]  и вот как бы это компактно рассказать и закрепить не только в теории информации, а много где еще,
[07:08.700 --> 07:14.260]  чтобы вы посмотрели, как более-менее одна и та же математика решает кучу разных задач,
[07:14.260 --> 07:18.900]  и более того, эта математика синонимична, то есть я могу что-то доказывать используя TFKP,
[07:18.900 --> 07:23.780]  могу что-то доказывать используя ну какую-то другую технику, например,
[07:23.780 --> 07:29.380]  ну теория мукаша вычетых и метод перевала, а могу доказывать, например, используя метод
[07:29.380 --> 07:33.220]  большого канонического ансамбля, то есть как стат физики там исследуют, то есть аппарат
[07:33.220 --> 07:38.580]  производящих функции, и уже немножко куда-то уходя, хотя тоже там в конечном итоге TFKP возникает,
[07:38.580 --> 07:45.540]  вот мы попробуем специально продемонстрировать разные техники, разную математику, но многократно,
[07:45.540 --> 07:52.620]  то есть не один раз, и начнем сегодня мы с задачи оранжирования веб-страниц, и на этой
[07:52.620 --> 07:58.100]  задачи мы продемонстрируем три замечательных результаты, торгодическая теорема, это всякие
[07:58.100 --> 08:03.620]  законы больших чисел, концентрация меры, ну и в принципе максимум правдоподобия, если получится,
[08:04.500 --> 08:13.620]  хотел бы это рассказать, еще немножко окрестности демонстрируя. Замечу, что в курс активно был
[08:13.620 --> 08:23.140]  вовлечен Максим Рахуба, это очень такой хороший специалист по матричному анализу, он ученик Ивана
[08:23.140 --> 08:30.060]  Валерьевича Селеца, он защитился, вот получил, прошел пост-дог, по-моему даже защитился в Швейцарии,
[08:30.060 --> 08:34.340]  может быть он там пост-дог делал, но сейчас он вернулся в Россию, и есть замечательная
[08:34.340 --> 08:39.060]  возможность его вовлечь, он очень хороший специалист по всяким разложениям матричным,
[08:39.060 --> 08:47.300]  и матричные разложения это вторая такая большая линия, что многие современные задачи, они требуют
[08:47.300 --> 08:52.860]  хранения данных в виде матриц, и те матрицы, которые получаются оказываться каких-то колоссальных
[08:52.860 --> 08:59.340]  размеров, надо как-то это в общем их представлять малорангово, то есть а-ля СВД, вот вокруг
[08:59.340 --> 09:05.500]  singular value decomposition, это очень важный сюжет, он как-то скомканно рассказывается в целом у нас в
[09:05.500 --> 09:10.300]  курсах всяких вот вокруг вычислительной математики, а это важная тема, то есть она рассказывается,
[09:10.300 --> 09:15.620]  но вот она очень часто в жизни современно используется, и мы тоже решили про это отдельно
[09:15.620 --> 09:21.180]  рассказать, там будет меньше всяких концентрационных вещей, но тоже будут тоже всякие такие эффекты
[09:21.180 --> 09:30.300]  больших размеров и будут проявляться, но давайте начнем с задачи пейдж ранг, в чем идет речь,
[09:30.300 --> 09:41.500]  я прям так и начну, гугл проблем, гугл проблем, это никак не связано с русском надзором, гугл
[09:41.500 --> 09:55.380]  проблем, ну почему гугл проблем, потому что в общем в конце 90-х бриллиный пейдж как раз положили в
[09:55.380 --> 10:00.260]  основу то, что я сейчас буду рассказывать, ну каких-то первых, даже не в конце 90-х, а в середине,
[10:00.260 --> 10:07.220]  вот идеи ранжирования веб-страниц, значит давайте представим себе такую ситуацию, что у нас есть
[10:07.220 --> 10:15.140]  граф, какой-то граф, ну вообще говоря большой, очень большой, это граф интернета, и каждое,
[10:15.140 --> 10:21.460]  ну по-хорошему конечно здесь надо рисовать ориентированный граф, каждая вот эта вот стрелочка
[10:21.460 --> 10:29.660]  это гиперсылка, а каждая вот эта вот вершинка, это давайте считать веб-страница, ну можно еще
[10:29.660 --> 10:34.500]  больше упростить считать, что это сайт, но конечно лучше точнее считать, что это веб-страница,
[10:34.500 --> 10:39.980]  естественно могут быть такие стрелки, ну в общем можете себе представить какой угодно граф,
[10:39.980 --> 10:46.300]  главное свойство этого графа, чтобы, ну я так упрощаю жизнь, чтобы если это понимать как
[10:46.300 --> 10:52.700]  систему дорог, то по этой системе дорог вы могли бы легко, ну не легко, а хоть как-то в принципе
[10:52.700 --> 10:59.540]  добраться из одной вершины в другую, то есть чтобы была полная доступность старту из любой
[10:59.540 --> 11:05.460]  точки, вы могли оказаться в любой другой, на самом деле вся, все что я буду рассказывать верно
[11:05.460 --> 11:11.140]  и при более слабых предположениях, а именно когда есть красная площадь, то есть такая точка,
[11:11.140 --> 11:16.460]  которую из любой точки можно приехать, ну вот где бы вы ни жили, вы на красную площадь всегда
[11:16.460 --> 11:21.780]  можете попасть, а уже, так сказать, дальше, что с красной площади вы куда-то можете попасть,
[11:21.780 --> 11:26.780]  это никто не утверждает, просто что вы можете добраться до красной площади, то есть до
[11:26.780 --> 11:30.740]  достаточно существования вершины, в которую можно свалиться, но вот нам сейчас будет удобнее
[11:30.740 --> 11:37.900]  считать, что все связано, это называется неразложимый граф, дальше мы просто, значит,
[11:37.900 --> 11:43.340]  ведем человечка, вот то, что я сейчас рассказываю, это, ну в общем, базовые вещи в курсе случайных
[11:43.340 --> 11:48.060]  процессов, мы увидим, что отчасти это вот сегодняшняя лекция, это будет повторять, но я постараюсь
[11:48.060 --> 11:51.940]  сделать так, чтобы вы что-то новое узнали, даже если вы третий курс, хорошо отучились,
[11:51.940 --> 11:58.380]  и знаете, что такое там марковский процесс и все такое. Так, у нас есть человечек, который
[11:58.380 --> 12:03.900]  блуждает по этому графу, ну а на графе какие-то, естественно, видены вероятности, с которыми
[12:03.900 --> 12:09.860]  человечек переходит со страницы на страницу, пускай эта страница и эта страница, допустим,
[12:09.860 --> 12:17.100]  g, ну и вообще говоря, у него есть какие-то разные варианты, куда там идти, ну и вот этот граф задан,
[12:17.100 --> 12:22.060]  матрица переходных вероятностей задана, поезжитое, вот, стрелка рисуется в том случае,
[12:22.060 --> 12:27.740]  если поезжитое больше нуля, поезжитое больше нуля, вот, матрицу переходных вероятностей мы
[12:27.740 --> 12:36.380]  называем p, и это индекс строки, g это индекс столбца, поезжитое, вот, и мы считаем, что строк и столбцов
[12:36.380 --> 12:41.900]  очень много, например, миллиард, 10 миллиардов, 100 миллиардов, ну какое-то очень большое число,
[12:42.280 --> 12:50.240]  давайте считать, что в начальный момент времени человек находится в каких-то состояне, в каком-то
[12:50.240 --> 12:55.900]  можно просто считать состояния, и вот начальное распределение вероятностей p от 0, это, например,
[12:55.900 --> 13:02.900]  просто есть там 0, 0, 1, 0, ну все, остальные нули, вот это вот начальное распределение вероятностей,
[13:02.900 --> 13:07.380]  это может быть какое-то распределение вероятностей реально, то есть может быть с вероятностью
[13:07.380 --> 13:13.620]  вероятностью одна вторая он там вон там но это сейчас неважно возникает следующий вопрос хорошо
[13:13.620 --> 13:22.500]  значит вот мы взяли этого человечка и дальше что что как бы естественно было бы спросить ну что
[13:22.500 --> 13:26.180]  будет происходить со временем если он будет блуждать согласно тем вероятностям которые
[13:26.180 --> 13:31.540]  нарисованы это вот абсолютно классическая постановка задачи она в чистом виде соответствует
[13:31.540 --> 13:38.100]  самой простой марковской цепи однородной дискретно неразложимый то есть означает что в общем
[13:38.100 --> 13:44.180]  просто однотипная ситуация из итерации в итерации повторяется просто надо следить за эволюцией
[13:44.180 --> 13:49.860]  вероятностной меры то есть если я обозначаю закон распределения этого человечка по вершинкам
[13:49.860 --> 13:56.860]  пат то чтобы написать эволюцию этого закона распределения я должен написать такую рекурренту
[13:56.860 --> 14:04.140]  давайте поймем откуда она появилась на п наверное удобнее было бы воспринимать это как-то не векторно
[14:04.140 --> 14:13.100]  а скалярно допустим для состояния 1 вот у нас есть какое-то состояние давайте не один а же же и
[14:13.100 --> 14:22.100]  вот значит тут состояние какие-то из которых он может сюда прийти вот и и же и же вероятность того
[14:22.100 --> 14:29.500]  что человечек окажется в состоянии же и в момент времени t плюс 1 это что такое это по формуле
[14:29.500 --> 14:36.300]  полной вероятности это есть сумма вероятности того что в момент времени и момент времени t он
[14:36.300 --> 14:43.620]  находился в состоянии и вообще говоря и может совпадать же и скакнул в состояние же вот так
[14:43.620 --> 14:48.820]  и сумма естественно идет по всем и вы согласны или я что-то не так написал не надо это подробнее
[14:48.820 --> 14:54.740]  пояснять или это очевидно это хорошо это очевидно да все отлично это называется формула полной вероятности
[14:54.740 --> 15:01.580]  то есть это группа полная группа событий что все пространство вероятностное разбивается на
[15:01.580 --> 15:07.140]  состояние это это это то есть где-то он находился и это не пересекающие события вот это полная
[15:07.140 --> 15:12.940]  группа событий а это вероятность перехода то есть формула по-хорошему выглядит вот так п ну я не
[15:13.920 --> 15:24.300]  переход переход переход вред при условии что мы находились вHer и значит при условии что
[15:24.300 --> 15:29.300]  мы находились и умножить на вероятность нахождение и в момент времени то есть вот
[15:29.300 --> 15:34.580]  просто важно что вот это вот есть как раз эту условная вероятность а это есть вероятность
[15:34.580 --> 15:38.480]  перехода то есть по-хорошему эту формула надо было писать в обратном порядке тогда это
[15:38.480 --> 15:43.460]  была классическая форма записи хорошо если я вектор на это соберу то я получу
[15:43.460 --> 15:49.020]  ровно вот это соотношение это все ну в принципе да то есть на самом деле самый
[15:49.020 --> 15:53.420]  главный шаг сделан я написал динамику я написал динамику эволюции вероятностных
[15:53.420 --> 15:58.400]  мер следующий вопрос который можно задать замечательно ну и что дальше то
[15:58.400 --> 16:02.660]  будет вот этот вопрос мобитурентов спрашиваем ну спрашивали потом меня
[16:02.660 --> 16:08.500]  перестали приглашать вот на собеседование на самом деле просто как
[16:08.500 --> 16:11.980]  бы сказать уже перестал быть замдекан и поэтому естественно как бы не стал
[16:11.980 --> 16:15.700]  участвовать собеседник а так когда участвовал в собеседованиях для
[16:15.700 --> 16:19.420]  абитурентов я их вот это почти спрашивал жестко но но я по-другому
[16:19.420 --> 16:24.180]  спрашивал сейчас поймете что все было не так жестко смотрите абитурентам
[16:24.180 --> 16:27.900]  которых действительно судьба решалась вот ну спорный бал и надо было понять
[16:27.900 --> 16:31.460]  человек может как там сообразить что-то я им давал такую задачку
[16:31.460 --> 16:37.820]  последовательность xn плюс один равняется xn плюс два и я говорил что
[16:37.820 --> 16:42.140]  предел есть то есть предел есть не надо это доказывать лимит xn при
[16:42.140 --> 16:46.700]  стремляющимся к бесконечности равен а считайте что это вы уже как бы знаете
[16:46.700 --> 16:52.220]  более того x0 выбирается как-нибудь так что там предел вполне как бы понятно
[16:52.220 --> 16:56.540]  какой будет то есть например 100 единицы и предел есть вопрос как к чему равен
[16:56.540 --> 17:03.740]  предел идея понятна вот этот простой трюк который я сейчас скажу он лежит в
[17:03.740 --> 17:09.540]  основе вообще почти ну вот всех каких-то результатов связанных с большими данными
[17:09.540 --> 17:14.340]  этот результатов и в общем связан с тем что есть есть есть есть какая-то
[17:14.340 --> 17:18.860]  динамика которая потенциально это надо доказывать к чему-то сходится то совсем
[17:18.860 --> 17:22.540]  не сложно бывает определить то к чему она сходится для этого просто надо
[17:22.540 --> 17:26.220]  найти так называемую вариантную меру а в данном случае эта мера просто число
[17:26.220 --> 17:31.180]  то есть надо найти такое число которое при подстановке в эту динамику это
[17:31.180 --> 17:36.900]  неподвижная точка ну то есть в нашем случае это а равняется корень из а плюс
[17:36.900 --> 17:45.300]  два как я это уравнение получил что нет это я с потолка взял ответил на вопрос
[17:45.300 --> 17:50.220]  вот я так умею не так еще брать вот значит смотрите им просто какая я ну
[17:50.220 --> 17:54.660]  как бы я не знаю откуда я взял из фехтангольца есть вот такой пример что
[17:54.660 --> 17:58.980]  надо найти предел как его найти но вообще говоря изучать что он существует
[17:58.980 --> 18:04.300]  это отдельная такая кропотливая работа а вот если я верю что он уже есть то я
[18:04.300 --> 18:08.380]  могу его само число найти число 2 как найти что этот предел равен именно
[18:08.380 --> 18:12.500]  двум что это именно два получается подстановка сюда потому что корень из
[18:12.500 --> 18:16.420]  двух вот x плюс два если x больше чем 0 даже больше чем минус два то это
[18:16.420 --> 18:20.100]  непрерывная функция что могу менять местами пределы корень и получается а
[18:20.100 --> 18:23.500]  равняется корень из а плюс два переходя к пределу вот здесь это простое
[18:23.500 --> 18:26.780]  наблюдение нам нужно для того чтобы то же самое сделать вот здесь
[18:26.780 --> 18:31.700]  но если то же самое нужно сделать вот здесь давайте это сделаем и получим что
[18:31.700 --> 18:37.060]  значит вектор p со звездой которая называется стационарное распределение
[18:37.060 --> 18:41.620]  удовлетворяет вот этому уравнению и этот вектор называется пейдж ранг почему
[18:41.620 --> 18:46.540]  называется пейдж ранг почему он как-то вот связан с транжированием страниц
[18:46.540 --> 18:56.520]  очень просто потому что если значит посмотреть на как бы сказать ну так
[18:56.520 --> 19:02.820]  содержательно на всю эту ситуацию то страница которая посещается человеком
[19:02.820 --> 19:08.060]  где-то на а симпатически с большей вероятностью она более популярна пока
[19:08.060 --> 19:12.220]  это еще не совсем хорошие объяснения но так интуитивно на самом деле вот этот
[19:12.220 --> 19:18.100]  вектор p со звездой он одновременно отвечает на вопрос как часто человек
[19:18.100 --> 19:25.060]  был в той или иной вершинке то есть можно задать вопрос вопрос это как бы на
[19:25.060 --> 19:29.660]  самом деле простое упражнение как часто за бесконечный горизонт времени ну в
[19:29.660 --> 19:34.420]  пределе человек посещал ту или иную вершинку и вот это p со звездой тоже
[19:34.420 --> 19:38.420]  самый дает ответ на этот вопрос то есть вообще говоря это уже содержатни то
[19:38.420 --> 19:43.500]  есть один и тот же человек проводил время в этих вершинках согласно этому
[19:43.500 --> 19:47.900]  вектору а вот это уже объяснение почему это разумно называть пейдж ранг ну
[19:47.900 --> 19:53.660]  хорошо а теперь возник вопрос ну брина пейджа соответственно кстати сказать
[19:53.660 --> 20:00.260]  если я правильно понимаю сергей брин это сын значит брина который был
[20:00.260 --> 20:05.820]  учеником сергей петровича новикова то есть это в общем как бы его отец
[20:05.820 --> 20:11.500]  заканчивал вместе новикова сергей петровича там по аспирантуре учился
[20:11.500 --> 20:15.500]  довольно такое интересное наблюдение то есть сказать идея положена в основе
[20:15.500 --> 20:21.820]  гугла в общем это идея частично вот как-то с россии связано может показать
[20:21.820 --> 20:26.420]  неожиданно да ну значит неважно мы возвращаемся сюда
[20:26.420 --> 20:30.860]  хорошо а теперь такой вопрос как это реально считать когда матрица миллиард
[20:31.340 --> 20:37.260]  ну можно конечно запустить человека но вообще говоря какая будет скорость вот
[20:37.260 --> 20:46.060]  вот у этого процесса и в этом смысле конечно здесь помогает некое понимание
[20:46.060 --> 20:51.980]  что на самом деле если матрица более-менее неплохая то вся эта динамика вот эта
[20:51.980 --> 20:57.380]  динамика она довольно быстро сходится то есть количество итерации которая
[20:57.380 --> 21:05.980]  требуется чтобы например п а т стало близко к п со звездой ну это по векторно
[21:05.980 --> 21:10.900]  да там норма векторная естественная два норма стала меньше эпсилон то вот
[21:10.900 --> 21:17.860]  число этих итераций ты большое значит оно порядка там некоторая константа дайте
[21:18.060 --> 21:27.180]  о большое о большое я напишу что такое альфа ну там на логарифм значит ну сюда
[21:27.180 --> 21:32.020]  вообще говоря может входить раз число вершины это n на эпсилон вот что-то
[21:32.020 --> 21:37.580]  такое может быть вот то есть что такое альфа это хороший вопрос и чтобы на него
[21:37.580 --> 21:42.620]  ответить давайте подумаем вообще говоря в чем геометрия вот этого всего
[21:42.620 --> 21:47.420]  происходящего значит верно ли что если п это вектор из симплекс а это
[21:47.420 --> 21:50.820]  распределение вероятности то после преобразования мы получим вектор из
[21:50.820 --> 21:55.260]  simplex а я могу это доказать но мне кажется это интуитивно понятно так ведь
[21:55.260 --> 21:59.220]  ну было бы странно если бы имели распределение вероятности
[21:59.220 --> 22:03.380]  все это имеет физический смысл и мы как бы сделав шаг оказались не в
[22:03.380 --> 22:06.760]  распределение врачи то есть�� будут доказывать это ну интуитивно понятно это
[22:06.760 --> 22:11.020]  значит что динамика имеет инвариант на вектор этот вектор собственно понятно
[22:11.020 --> 22:15.620]  что раз у нас здесь неподвижная точка то есть это уравнение имеет решение
[22:15.620 --> 22:23.820]  значит есть собственное значение единичка но это значит что матрица уже как бы не очень хорошая потому что она не сжимает то есть если бы это был
[22:23.820 --> 22:31.740]  оператор у которого спектр лежит в единичном круге трога то это было бы сжимающее отображение мы могли бы альфа под альфа понимать
[22:31.740 --> 22:38.900]  максимальное собственное значение и это был бы спектральный радиус это называется ну и тогда бы это все в общем работала тут не так
[22:38.900 --> 22:43.140]  тут не так потому что уп максимальное собственное значение единицы я пытаюсь пояснить почему
[22:43.140 --> 22:49.480]  собственно значение единица потому что у матрицы левые собственные значении правый совпадают то есть
[22:49.480 --> 22:54.980]  просто можно говорить о значит спектре матрицы собственных значениях не привязываясь к левому
[22:54.980 --> 23:01.080]  собственному вектору и правому так вот очевидно что поскольку есть вариантные вот это вот как бы
[23:01.080 --> 23:07.480]  элемент симплекса переводится элемент симплекса значит можно быть уверенных что есть правый
[23:07.480 --> 23:13.440]  собственный вектор, состоящий из единичек, и он переходит в правый собственный вектор, то есть в тот же единичек, то есть мы просто
[23:13.440 --> 23:18.760]  явно можем предъявить. Поэтому не удивительно, что и левый собственный вектор такой есть, то есть вопрос
[23:18.760 --> 23:25.680]  существования решения не стоит, это очевидно, что оно есть. А собственно, неразложимость вот этой вот цепи,
[23:25.680 --> 23:31.760]  что из любого состояния можно прийти в любое другое, означает, что единственно, то есть состояние в классе
[23:31.760 --> 23:36.800]  распределения вероятности такой вектор единственный. Вопрос, почему мы к нему сходимся, и что, ну,
[23:36.800 --> 23:45.200]  линейный, и что такое альфа? Ответ, а просто потому, что если перейти в пространство, которое связано
[23:45.200 --> 23:55.520]  с, соответственно, с вот с лучами на нетрицательном артанте, то вот эта динамика, она будет сжимать по
[23:55.520 --> 24:01.800]  направлению вот как раз к этому собственному вектору специальной метрики, метрики Бергофа-Гильберта.
[24:01.800 --> 24:06.360]  Я не буду сейчас про это подробно говорить, но идея в том, что максимальное собственное значение
[24:06.360 --> 24:12.920]  матрицы и правда единица, а вот сжимаемость определяется не им, оно определяет инвариант,
[24:12.920 --> 24:18.600]  вот это оно определяет, а определяется скорой сходимости следующим по величине модуля собственным
[24:18.600 --> 24:25.200]  значением. Вот это альфа, это так называемый spectral gap. Spectral gap это что такое? Это расстояние
[24:25.200 --> 24:30.120]  между максимальным собственным значением матрицы P. Я не случайно сейчас все эти вещи говорю,
[24:30.120 --> 24:34.400]  потому что они многократно нам будут встречаться в разных контекстах, то есть всегда есть у матрицы
[24:34.400 --> 24:40.840]  единица, а вот из-за того, что она неразложима, все остальные собственные значения, потому что этот
[24:40.840 --> 24:45.960]  граф такой, что из любой вершины можно в любую другую прийти, все остальные собственные значения
[24:45.960 --> 24:52.360]  лежат вот как-то тут и соответственно максимальный из них определяет то самое альфа, вот это
[24:52.360 --> 24:57.280]  называется spectral gap и он входит в оценку скорости сходимости, да.
[25:03.280 --> 25:12.280]  Значит задача сводится к поиску левого собственного вектора матрицы P, ну или если угодно,
[25:12.280 --> 25:16.640]  правого матрицы P транспонируем, но не просто собственного вектора, говоря по научному,
[25:16.640 --> 25:21.680]  собственного вектора фрабениуса перона, то есть собственного вектора, отвечающего максимальному
[25:21.760 --> 25:25.760]  собственному значению, вот такой вот ответ.crew-едет. Все остальные собственные вектора нам не
[25:25.760 --> 25:30.120]  так интересны. Ну и, в частности, правый собственный вектор матрицы P, отвечающий
[25:30.120 --> 25:33.700]  собственному значению единицы, нам не интересен, он тривиальный, он из 1 состоит.
[25:33.700 --> 25:38.240]  У нас динамика через левые векторы записанные, поэтому нам нужно искать левый собственно вектор.
[25:38.240 --> 25:44.360]  Они отличаются, когда матрица несимметрична, то левая и правая могут отличаться. А вопрос как бы
[25:44.360 --> 25:49.300]  следующий. Ну замечательно я все чем-то рассказываю, но пока не очень понятно где вот это вот какая-то
[25:49.300 --> 25:53.220]  это изюминка, потому что пока была только вот одна идея, но этого мало,
[25:53.220 --> 25:57.220]  потому что хочется какой-то больше концентрации, и вроде как анонсировано было
[25:57.220 --> 26:06.180]  сильно больше. Но давайте для начала мы поймем, что уже даже вот как бы в
[26:06.180 --> 26:13.300]  таком варианте, который я написал, есть на самом деле некоторая такая интересная
[26:13.300 --> 26:21.580]  математика, связанная с, грубо говоря, с тем, как вообще отсюда что-то
[26:21.580 --> 26:27.820]  вытаскивать. То есть, ясно, что мы можем подождать вот такое время и делать
[26:27.820 --> 26:33.460]  эволюцию просто с этими матрицами, и получается найти решение, если подождем
[26:33.460 --> 26:37.940]  вот столько времени. Но в таком случае, если матрица имеет число ненулевых
[26:37.940 --> 26:44.420]  элементов n, n, z от p, нам придется потратить вот такое время. То есть, число
[26:44.420 --> 26:49.540]  ненулевых элементов матрицы, ну я логарифмы опускаю, давайте напишу все-таки там
[26:49.540 --> 26:55.420]  логарифм, логарифм n на эпсилон. Ну ясно, что от эпсилона зависимость не такая
[26:55.420 --> 27:00.820]  чувствительная, но в общем проблема вот в этом. Если наша матрица миллиард на
[27:00.820 --> 27:05.300]  миллиард, и где-то из каждой веб-страницы в среднем там сотенка
[27:05.300 --> 27:11.860]  выходит в ссылок, то это 10 в 12, там может быть. 10, не знаю, в 10, в 11.
[27:11.860 --> 27:16.020]  Но это так себе удовольствие, если учесть, что альфа типично может быть там еще
[27:16.020 --> 27:21.380]  там 0,1, да, то у вас уже как получается число арифметических операций, 10 там
[27:21.380 --> 27:27.940]  не знаю, в 12, в 13. Ну так себе удовольствие. Самое главное, что вообще говоря, вот
[27:27.940 --> 27:33.860]  это все действие, ну конечно, можно хранить матрицу в виде списка смежности, но все
[27:33.860 --> 27:38.420]  равно это так себе удовольствие. А где ее хранить? В оперативной памяти, ну
[27:38.420 --> 27:43.300]  оперативная память, ну не знаю, 8 гигабайт. 8 гигабайт она не поместится.
[27:43.300 --> 27:46.620]  Ничего делать.
[27:48.220 --> 27:53.980]  Альфа и спектрал ГЭП это расстояние между, давайте так напишу, это единица
[27:53.980 --> 27:59.860]  минус лямбда 2 от П. Лямбда 2, второе по величине значение, собственное значение
[28:00.220 --> 28:06.420]  матрицы, увеличение модуля, то есть вот так. Вот что такое спектрал ГЭП, то есть
[28:06.420 --> 28:11.140]  максимальное собственное значение это единица, а следующее лямбда 2 и вот альфа
[28:11.140 --> 28:20.620]  это единица минус лямбда 2 П. Это вопрос, я так понял был. Хорошо, значит в принципе вот
[28:20.620 --> 28:25.500]  сама жизнь подсказывает, что надо делать. А давайте пустим человечка реальный, будем
[28:25.500 --> 28:32.180]  за ним следить. Но это уже, извините, другой вопрос, то есть если мы пустим,
[28:32.180 --> 28:37.860]  это не страшно, если лямбда кратный, тут просто берется, ну там возникают некоторые
[28:37.860 --> 28:41.580]  эффекты следующего порядка, но как бы в первом приближении все так же остается,
[28:41.580 --> 28:48.140]  это неважно. Меняет как бы синтетическое разложение, но первый
[28:48.140 --> 28:52.300]  член синтетического разложения в тех категориях, в которых я пишу, это
[28:52.300 --> 29:00.780]  ничего не меняет, но при условии что альфа больше нуля. Есть на самом деле замечательные
[29:00.780 --> 29:07.100]  результаты о том, что как бы я действительно могу, ну вот в идеале, представьте себе,
[29:07.100 --> 29:13.500]  что у меня на каждой итерации, вот как бы совсем в идеале, вот это неправда, но допустим,
[29:13.500 --> 29:19.780]  допустим я на каждой итерации, вот реально человечка помещаю в одну из вершин, вот с
[29:19.780 --> 29:24.420]  этим вектором вероятности, то есть это не марковский процесс, вот сейчас я скажу очень
[29:24.420 --> 29:27.700]  важные вещи, постарайтесь ее понять, потому что она постоянно будет использоваться, то есть еще
[29:27.700 --> 29:33.820]  раз, нам надо найти p со звездой, а я не знаю, чему равняется p со звездой, но грубо говоря,
[29:33.820 --> 29:40.260]  я могу в каком-то смысле сэмплить, это вот то, что используется в анализе, то есть я могу
[29:40.260 --> 29:45.740]  разыграть, я как бы могу разыграть, где находится человек, проблема только в том, что я это делаю,
[29:45.740 --> 29:50.740]  как бы с некоторой предысторией, но давайте вот идеализируем картину, давайте будем считать,
[29:50.740 --> 29:57.220]  что мой человек магическим образом на новую итерацию просто скачет согласно этому вектору
[29:57.220 --> 30:03.180]  распределения вероятности, то есть у меня есть соответственно вектор ню случайный, который
[30:03.180 --> 30:11.380]  имеет мультинамиальное распределение, то есть на итерации t ню t равняется k, k ты вершиню и давайте,
[30:11.380 --> 30:16.380]  и это равняется просто p, че там у меня снизу, блин, плохо, ладно, p со звездой от i,
[30:16.380 --> 30:22.220]  и t компонента, то есть понятно, о чем я говорю, то есть я предполагаю, что мой человек какой-то
[30:22.220 --> 30:29.180]  монстр, который вообще ему эти марковские блуждания не нужны, он просто сразу берет и скачет в одну из
[30:29.180 --> 30:37.460]  вершин согласно вектору page rank, ясно, что настоящий человек, настоящий человек, он как бы вот этого
[30:37.460 --> 30:42.660]  может добиться только в осимпотике, то есть если он будет достаточно долго гулять, и мы зафиксируем
[30:42.660 --> 30:50.340]  момент времени, то это будет верно, то есть вот если я здесь напишу лимит по t, то это будет верно,
[30:50.340 --> 30:56.700]  но тогда будет другая проблема, если я зафиксирую достаточно большой t, а потом возьму t плюс 1,
[30:56.700 --> 31:02.100]  то уже это будет неверно, то есть одновременно у меня там ну или там я не знаю, не ажи, то есть я
[31:02.100 --> 31:08.340]  не смогу вот так написать, это будет заведомо неверно, если t большое, то мне, конечно, хотелось,
[31:08.340 --> 31:14.740]  чтобы каждый новый sample давал бы мне независимую реализацию вот этого вектора, а потом бы я взял
[31:14.740 --> 31:20.900]  и оценивал настоящий вектор частот просто как средне арифметическое по траектории вот этих
[31:20.900 --> 31:27.620]  вот new t, это было бы как бы идеально, да, вот это как бы то, что хотелось бы сделать, то есть хотелось
[31:27.620 --> 31:36.220]  бы, чтобы этот человек, который вот так вот живет, просто вот я бы суммировал векторы, которые
[31:36.220 --> 31:43.700]  состоят ну соответственно из нулей единиц, в основном в смысле из нулей и одной единицы, если
[31:43.700 --> 31:49.060]  точнее говорить, и каждый раз эта единица где-то находится, но это случайный вектор, вот такие векторы
[31:49.060 --> 31:55.180]  я суммирую, позиция единицы определяется тем, где конкретно человек находится, ну подождите,
[31:55.180 --> 32:03.660]  давайте я, так сказать, неправильно написал, new t равняется значит единице new t и вот так вот,
[32:03.660 --> 32:09.660]  сейчас я понял, вот так я перепишу, это будет корректнее, вот new t равняется единице с
[32:09.660 --> 32:17.260]  вероятностью p, вот все, а иначе ноль, вот, ну соответственно я напишу иначе ноль, если,
[32:17.260 --> 32:27.020]  ну с вероятностью давайте так, p значит new t и равняется ноль с вероятностью соответственно
[32:27.020 --> 32:33.660]  единицы минус p со звездой и вот так сейчас корректнее, вот, и вот и в идеале было бы,
[32:33.660 --> 32:43.500]  конечно, суммировать вот такую сумму и тогда бы я получил, что да, здесь тоже лучше переписать,
[32:43.500 --> 32:53.300]  new t плюс первое, вы правы, new t плюс первое и g равняется единице, вот, и вот я бы что тогда
[32:53.300 --> 33:00.060]  мог сказать, ну по всяким неравенствам концентрации я бы мог, сейчас я про это скажу подробнее,
[33:00.060 --> 33:09.140]  написать, что если бы взять число шагов, ну значит один на eq, то замечательным образом я бы
[33:09.140 --> 33:15.860]  получил, я объясню откуда eq, я бы получил то, что надо, ну то есть что надо, ну я получил
[33:15.860 --> 33:25.340]  такой результат, то вот этот вот вектор единица на t частоты, ну вот эти new t, значит t от единицы
[33:25.340 --> 33:35.060]  до t, минус соответственно p со звездой, 0.2 нормия, вероятность того, что это больше,
[33:35.060 --> 33:43.420]  чем значит, ну аккуратно это так выглядит, там c1, это числовая константа, по-моему,
[33:43.420 --> 33:54.220]  она там условно 4 плюс там 2 логарифма, sigma минус 1, на соответственно корень из t, значит вот
[33:54.220 --> 34:00.140]  эта вся штука меньше либо равняется, меньше либо равняется sigma, вот такого типа результат я
[34:00.140 --> 34:04.820]  мог бы получить, вот что это такое, как такие результаты получаются, мы с вами будем многократно
[34:04.820 --> 34:11.380]  говорить, я к этому сейчас скоро вернусь, но пока просто придется поверить на слово, что вот как-то
[34:11.380 --> 34:17.060]  так, ну для тех, кто знает, что такое центральная предельная теорема, я думаю, что вот этот
[34:17.060 --> 34:23.060]  результат не есть, а так сказать, какое-то открытие, почему, потому что если у меня две вершины,
[34:23.340 --> 34:29.460]  давайте для простоты возьмем две вершины, то есть всего две вершины, тогда мне достаточно написать это
[34:29.460 --> 34:36.620]  неравенство для первой компоненты, то есть у меня есть случайная величина nu, nu соответственно первая
[34:36.620 --> 34:42.900]  компонента, вот и вот эта величина, она равна, если повторю, независимо они разыгрываются,
[34:42.900 --> 34:55.180]  значит она равна с вероятностью, с вероятностью p со звездой единица, вот ну и получается, что я могу
[34:55.180 --> 35:02.580]  написать для вот этой вот по сути схемы испытаний Бернули, для первой компоненты, никаких норм мне
[35:02.580 --> 35:06.620]  здесь ставить не надо, центральная предельная теорема, она формулируется таким образом,
[35:06.620 --> 35:17.700]  nu, t первая компонента, значит сумма, значит поделить на корень с t, на корень с t дисперсия
[35:17.700 --> 35:22.980]  здесь должна стоять, сейчас не буду ее выписывать, это не очень интересно, ну потом это мы сделаем,
[35:22.980 --> 35:28.380]  t от единицы до t большого минус, мат ожидания вот этой штуки умножить на t, ну уж мат ожидания,
[35:28.380 --> 35:35.500]  думаю, можем в ходу оценить, это будет p со звездой первая компонента на число итерации, вот это
[35:35.500 --> 35:44.340]  мат ожидания всей этой суммы, не вот эта штука должна стремиться к чему соответственно,
[35:44.340 --> 35:48.220]  ну к нормальному стандартному нормальному распределению, то есть быть как конечной,
[35:48.220 --> 36:01.140]  то есть это означает, что масштаб поведения вот этой вот штуки как раз действительно более-менее
[36:01.140 --> 36:09.820]  один на корень с t, что собственно здесь и написано, то есть если посмотреть на то,
[36:09.820 --> 36:17.900]  что здесь написано, здесь в общем-то ровно то же самое и получается, вот то есть мы имеем как бы
[36:17.900 --> 36:23.380]  по сути центральную предельную теорему, но в чем принципиальное отличие вот этих вот марков,
[36:23.380 --> 36:30.460]  этих марковских цепей, я сейчас просто написал, что было бы если человечек каждый раз независимо
[36:30.460 --> 36:35.660]  выбирал бы свою вершинку согласно p со звездой, но это же не так в нашем случае, это же неправда и
[36:35.660 --> 36:43.300]  вот к сожалению, поскольку у нас есть марковская цепь, то ну грубо говоря, вот это время альфа,
[36:43.300 --> 36:52.260]  один альфа, это есть время, между которыми, которое требуется, чтобы сечения стали независимы,
[36:52.260 --> 36:58.700]  то есть чтобы то, откуда я стартовал, полностью вымылось, то есть чтобы уже через ток и итерации
[36:58.700 --> 37:04.340]  человек в каком-то смысле забыл, где он, с чего он стартовал и можно было говорить о
[37:04.340 --> 37:11.900]  практической независимости сечений и вот в этом смысле качество вот этого результата падает как
[37:11.900 --> 37:19.100]  раз в альфа раз, то есть вот этот масштабный коэффициент альфа-то маленькая и мы теперь не можем
[37:19.100 --> 37:24.980]  так точно говорить о том, что эта вот величина именно меньше t, ну давайте я наверное напишу
[37:24.980 --> 37:32.420]  более понятно, вот я так напишу, чтобы это было значит правильную сторону, то есть с большой
[37:32.420 --> 37:39.180]  вероятностью, с большой вероятностью мы можем локализовать, где находится вот это вот вектор,
[37:39.180 --> 37:45.860]  то есть вот как бы резюме, мы по сути, по сути вот эту всю марковскую такую вот специфику,
[37:45.860 --> 37:54.180]  связанную с тем, что сечения зависимы, человечек блуждает, но нас это не сильно останавливает,
[37:54.180 --> 37:58.700]  то есть мы в каком-то смысле считаем, что сечение независимо просто с фактором альфа,
[37:58.700 --> 38:06.420]  который отражает mixing time время смешивания, вот процесс требует большей итерации,
[38:06.420 --> 38:10.840]  он требует итерации как раз больше такое количество раз, то есть грубо говоря,
[38:10.840 --> 38:15.780]  мы пропускаем такое количество итераций, делаем первую итерацию, потом ждем такое
[38:15.780 --> 38:20.140]  количество итераций и снова берем реально вектор, который там распределение потом
[38:20.140 --> 38:25.600]  снова пропускаем вот столько итераций снова берем понятно что все это в каком
[38:25.600 --> 38:32.900]  смысле делает сходимость медленнее но но что важно в конечном итоге мы тоже
[38:32.900 --> 38:37.900]  можем получить что число блужданий человечка вот этого число итераций
[38:37.900 --> 38:42.460]  необходимых чтобы найти вектор pageRank с точностью epsilon вот в этом смысле
[38:42.460 --> 38:47.980]  то есть если это epsilon приравнять то мы получим что t есть o большое давайте
[38:47.980 --> 38:53.860]  с точностью до логарифов я логариф мы опущу 1 на альфа множить 1 на epsilon
[38:53.860 --> 39:00.980]  квадрать да логариф мы опускаем и получается что вообще говоря размерность
[39:00.980 --> 39:05.820]  пространства удивительным образом вообще не входит я напомню с чем мы
[39:05.820 --> 39:12.060]  сравниваемся ты станте значит метод простой итерации ну на английский это
[39:12.060 --> 39:19.100]  пауэр метод я поэтому пауэр напишу значит это есть вот так 1 на альфа ну те
[39:19.100 --> 39:27.660]  же логарифмы я опустил но но извините ннз да ннз ннз вот это ннз в этой
[39:27.660 --> 39:32.260]  матрице п то есть понятно вот как бы big data это вот о чем если вот это много
[39:32.260 --> 39:35.540]  если вот это много то давайте мы будем использовать какой-нибудь
[39:35.540 --> 39:38.940]  рандомизированный алгоритм что это будет с какой-то большой вероятностью я
[39:38.940 --> 39:41.620]  естественно вот эту вероятность сейчас не пишу она под логарифом и
[39:41.620 --> 39:45.900]  соответственно не то не точность хуже будет входить то есть мы не будем так
[39:45.900 --> 39:50.540]  точно решать задачу с логарифмической точностью мы будем не точно решать но
[39:50.540 --> 39:55.340]  за счет этого мы существенно выиграем вот здесь это такая идея вот и
[39:55.340 --> 40:01.140]  соответственно расплата будет да в том что придется решать не так точно но
[40:01.140 --> 40:05.940]  зато без каких-то сильных заморочек связанных с большой размерностью это
[40:05.940 --> 40:08.940]  вроде выход но
[40:09.180 --> 40:12.780]  ты павуру естественно зависит от эпсилон но это вся зависимость от эпсилон
[40:12.780 --> 40:17.860]  воз волной ну хорошо если хотите я тут могу такие писать и на тепсилон и на
[40:17.860 --> 40:22.800]  тепсилон вот здесь здесь это все не так сейчас
[40:22.800 --> 40:25.660]  существенно там на самом деле еще появляются логарифмы я сейчас их не
[40:25.660 --> 40:30.540]  буду то есть там больше логарифмов там из-за из-за того что здесь немножко как
[40:30.540 --> 40:34.740]  бы тоже надо поаккуратнее писать там с логарифмами я сейчас не буду про это
[40:34.740 --> 40:37.660]  говорит то есть вот этот результат когда я добавляю к сожалению это
[40:37.660 --> 40:41.300]  происходит не бесплатно и здесь тоже появляются логарифма тен но это все
[40:41.300 --> 40:46.140]  абсолютно такие мелочи которые сейчас не хочу вас грузить важно сама идея вот
[40:46.140 --> 40:51.260]  идея фундаментальная если вы работаете с марковской цепью то миксинг тайм род
[40:51.260 --> 40:56.180]  один альфа замедляет сходимость это вот запомните это реально всегда спасает
[40:56.180 --> 40:59.620]  но практически всегда это работает и многие из этих результатов еще даже не
[40:59.620 --> 41:03.700]  получены то есть это на самом деле сложно получать но это практически всегда так
[41:03.700 --> 41:08.500]  работает а теперь самое интересное то есть я вам рассказал вообще говоря некий
[41:08.500 --> 41:15.940]  способ ухода от того что называется ну как бы детерминированный алгоритм и
[41:15.940 --> 41:20.500]  переход к рандомизируем но вопрос это есть марков чейн монтекарло то есть чтобы
[41:20.500 --> 41:25.420]  найти стационарное распределение давайте просто запустим допустим
[41:25.420 --> 41:30.780]  случайный процесс и он сам нам выдаст вот это стационарное распределение ну
[41:30.780 --> 41:37.020]  как бы и да и нет потому что на самом деле на самом деле марков чейн монтекарло
[41:37.020 --> 41:43.020]  имеет и другую цель он имеет цель не просто выйти на стационарное распределение
[41:43.020 --> 41:48.620]  он имеет свои цели еще как бы дополнительно среди всех состояний
[41:48.620 --> 41:53.180]  найти наиболее вероятное потому что представьте себе что у аспеса звездой
[41:53.180 --> 41:58.820]  каким-то магическим образом вырождается саврино так вот что вырождается что почти
[41:58.820 --> 42:04.420]  все компоненты ноль одна единичка и где это ничка находится вы в принципе чтобы
[42:04.420 --> 42:08.820]  найти вам надо все пересмотреть все что есть пессо звездой а это экспоненциально
[42:08.820 --> 42:13.020]  много состояний и вот когда вы запускаете марков чейн монтекарло вы не
[42:13.020 --> 42:17.340]  думаете сколько состояний человечек блуждает ему все равно насколько большой
[42:17.340 --> 42:22.740]  граф ведь так вы можете реализовать его блуждание абсолютно не задумываясь как
[42:22.740 --> 42:27.060]  это вообще ну сколько там вершин более того если у вас есть симметричная
[42:27.060 --> 42:32.420]  монетка то чтобы сгенерировать движение вот равновероятно ну скажем по
[42:32.420 --> 42:40.540]  трем направлениям в общем-то не надо не надо сильно много затрачивать сил
[42:40.540 --> 42:48.460]  значит смотрите значит вот у вас есть симметричная монетка вы хотите
[42:48.460 --> 42:54.260]  сгенерировать случайную величину которая принимает равновероятно значение 1 2 и
[42:54.620 --> 43:00.860]  3 равновероятно как вы можете это сделать ответ возьмите значит начальное
[43:00.860 --> 43:08.140]  состояние кидайте значит сейчас неправильно нарисовал конечно вот сейчас
[43:08.140 --> 43:15.660]  и нарисую правильно поторопиться значит вот так вот так это сюда уйдет значит это
[43:15.660 --> 43:22.020]  вот сюда уйдет а это вот туда уйдет вот вот у меня есть три состояния и я их
[43:22.100 --> 43:29.580]  1 2 3 я бросаю симметричную монетку значит решка орел резко орел в зависимости у того
[43:29.580 --> 43:35.080]  что выпала я дальше и ду либо сюда решка орел sponge либо сюда и да LA tu stitches
[43:35.080 --> 43:41.020]  орел режка орел вот если два раза выпал орел я беру и возвращаюсь вот
[43:41.020 --> 43:44.020]  сюда это повторяю а если соответственно выпала
[43:44.020 --> 43:49.500]  кинорешка решка орел орел решка то я уже все окончательно выбрал состояние но
[43:49.500 --> 43:53.700]  но то если вы проларел орел то я возвращаюсь и повторяю можно показать что
[43:53.700 --> 43:58.380]  ну в общем-то вероятность того что я зациклюсь она в общем экспоненциально
[43:58.380 --> 44:04.820]  убывает и вы получаете алгоритм который за логарифмическое время от числа
[44:04.820 --> 44:09.420]  состоянии с помощью такого количества подкидывания симметричной монетки дает
[44:09.420 --> 44:14.300]  вам любое распределение вероятностей поэтому человечку не так сложно будет
[44:14.300 --> 44:18.940]  естественно логарифмическое число раз затрачивая даже если экспоненциально
[44:18.940 --> 44:24.180]  много соседей ему не так сложно будет с помощью алгоритма кнута яу который
[44:24.180 --> 44:29.180]  описал поймать понять какому соседу пойти это если вы используете симметричную
[44:29.180 --> 44:32.580]  монетку как источник первозданной случайности если вы используете
[44:32.580 --> 44:37.220]  генератор случайных чисел равномерно распределенных на отрезке 0 1 вы просто
[44:37.220 --> 44:41.020]  берете разбиваете вот у вас есть распределение вероятности вы пишете п
[44:41.020 --> 44:46.420]  1 п 1 плюс п 2 и так далее и вот вот эти отрезки они будут соответственно длиной
[44:46.420 --> 44:52.060]  п 2 п 1 и вы мы приготовите таким образом память готовить таким образом память и
[44:52.060 --> 44:56.460]  дальше кидая случайно вот величину один раз от н при процессинге сделайте
[44:56.460 --> 45:01.060]  дальше кидая случайную величину равномерно распределенную вы будете каждый
[45:01.060 --> 45:04.620]  раз ну в общем определять в какой отрезок она попала единственная тут
[45:04.620 --> 45:08.620]  сложность что если эти числа очень близки то есть очень много засечек на
[45:08.620 --> 45:13.600]  отрезке 0 1 то с точность вот этого генератора должна быть столько же битов
[45:13.600 --> 45:18.840]  сколько сколько битов отличаются вот эти это требует дополнительных мы тоже
[45:18.840 --> 45:22.720]  сегодня об этом поговорим требует дополнительных усилий чтобы генератор
[45:22.720 --> 45:27.000]  равномерных случайных чисел позволял вам отличать ну то есть достаточно точно
[45:27.000 --> 45:32.200]  определять какой интервал попало ну еще раз это я только о том что не надо
[45:32.200 --> 45:37.360]  сильно беспокоиться что сделать шаг алгоритма вот этого это что-то сложное
[45:37.360 --> 45:41.040]  это не сложно потому что сгенерировать какой-то конечное дискретное
[45:41.040 --> 45:44.440]  распределение это в общем-то логарифмические проблемы единственное что
[45:44.440 --> 45:50.760]  там может быть сложно что возможно потребуется при процессе вот это все это
[45:50.760 --> 45:56.600]  конец истории или что-то еще можно сказать а вот давайте теперь сделаем
[45:56.600 --> 46:01.000]  такое наблюдение оно наверное будет как бы еще более жизненно ведь в жизни то
[46:01.000 --> 46:04.760]  не так вот я вам что-то рассказываю но в жизни ты не один человек блуждает по
[46:04.760 --> 46:10.920]  графу это же очевидно и в жизни все по-другому по-другому конечно но все-таки
[46:10.920 --> 46:14.880]  что-то похоже есть давайте представим себе реальный интернет он же состоит не
[46:14.880 --> 46:19.760]  из одного человека а из огромного количества людей n большое которые все
[46:19.760 --> 46:25.880]  живут на этом графе абсолютно тот же самый граф но они живут и блуждают по
[46:25.880 --> 46:33.960]  нему все вместе одновременно так ведь теперь такой момент а давайте возьмем
[46:33.960 --> 46:38.920]  количество человечков вот такое это число людей то есть ну так по порядку n
[46:38.920 --> 46:44.720]  то есть число людей у меня будет равняться вот этому самому 1 на x квадрате и
[46:44.720 --> 46:50.600]  каждый человечек будет жить жизнь длиною 1 на альфа но повторю что здесь на
[46:50.600 --> 46:56.080]  самом деле два логарифма один логарифм связан с mixing time там второй еще вот ну
[46:56.080 --> 47:01.120]  неважно в общем это сейчас на логарифм мы не смотрим я просто с точностью до
[47:01.120 --> 47:05.720]  логарифма беру 1 на x квадрате человечков эти человечки как-то
[47:05.720 --> 47:09.200]  распределены на графе транспортной сеть во и на графе вот этом вот пейдж
[47:09.200 --> 47:14.880]  ранга и они начинают блуждать независимо друг от друга вы даже можете
[47:14.880 --> 47:20.400]  пофантазировать кто-то со временем может делиться от пачковаться кто-то от него
[47:20.400 --> 47:23.840]  и начинать продолжать путь то есть человек блуждает блуждает а в какой-то
[47:23.840 --> 47:28.520]  момент он порождает знаю кого-то и тот еще тоже начинает независимо будет то
[47:28.520 --> 47:32.080]  есть вам не обязательно начинать не знаю с большого количества людей вы можете
[47:32.080 --> 47:36.720]  начать с трех человек они ну с трех как-то не естественно с двух человек
[47:36.720 --> 47:41.520]  значит они вот скажет поживут какое-то время их станет больше и вот они так и
[47:41.520 --> 47:45.560]  будут плодиться но дело в том что они уже когда будут плодиться они как бы
[47:45.560 --> 47:49.360]  жизнь какую-то часть прожили и они уже более-менее равномерно распределены и
[47:49.360 --> 47:53.320]  точка старта случайно выбирается но это это на самом деле какие-то поправки я
[47:53.320 --> 47:58.920]  сейчас не хочу в это вдаваться в общем идея в том что если каждый человек вот
[47:58.920 --> 48:04.680]  что что мы имеем если каждый человек проживет жизнь вот такую а их столько то
[48:04.680 --> 48:08.520]  верно лишь что я тот же самый результат получу если просто соберу
[48:08.520 --> 48:13.000]  статистику где кто находится но смысл и сколько человечка в той вершине в
[48:13.000 --> 48:21.840]  другой вершине улавливаете идею да подождите кто кто сказал что они зависимым
[48:21.840 --> 48:28.760]  образом блуждают это правда если я от пачков они конечно как бы
[48:28.760 --> 48:32.920]  как конкретно тот кто отпочковаться зависимым образом но во-первых у нас
[48:32.920 --> 48:37.880]  увидите марковская цепь нас не сильно смущает зависимость это как бы ну как бы
[48:37.880 --> 48:43.120]  минус а плюс том что хорошая точка старта но по факту да если говорить
[48:43.120 --> 48:47.400]  аккуратно давайте не будем заниматься отпочкованием они все независимо блуждают
[48:47.400 --> 48:50.840]  вот в этом случае
[48:50.840 --> 49:01.960]  так так ну хорошо они блуждают 1 альфа это что означает что каждый из них вот
[49:01.960 --> 49:05.320]  давайте как просто подумаем каждый из них что в итоге будет он же
[49:05.320 --> 49:09.840]  независимо блуждают что каждый из них выйдет на некий вектор состояния ну
[49:09.840 --> 49:14.040]  грубо говоря п со звездой правильно
[49:15.840 --> 49:20.080]  ну давайте вот вообще максимально дальше я упрощу ситуацию давайте у нас две
[49:20.080 --> 49:24.240]  вершины то есть у нас куча людей две вершины чего нам как бы заниматься
[49:24.240 --> 49:28.520]  какими-то общими формулами если можно ситуацию выродить и почувствовать что
[49:28.520 --> 49:33.760]  происходит вот у нас граф ну вот самая общая ситуация но из двух вершин 1 2 и
[49:33.760 --> 49:39.480]  здесь огромное количество человечков находятся они как-то блуждают эти люди
[49:39.480 --> 49:44.200]  туда-сюда но согласно тем вероятностям которые здесь нарисованы вот они
[49:44.200 --> 49:49.120]  блуждают блуждают и собственно у каждого человечка не у как мы там
[49:49.120 --> 49:55.280]  обозначить будем обозначать давайте теперь индекс будем использовать как не
[49:55.280 --> 50:02.120]  ука значит не ука это случайный вектор который ну давайте значит пускай не ука
[50:02.120 --> 50:12.880]  это равняется единице если давайте один если если катый человек
[50:12.880 --> 50:21.480]  катый человек так ну давайте зафиксируем момент времени t который
[50:21.480 --> 50:27.560]  есть один альфа там по большое что там у нас ну давайте не будем писать
[50:27.560 --> 50:32.040]  логарифмы на на эти вот зафиксировали момент времени на самом деле здесь 7
[50:32.040 --> 50:36.480]  он конечно нужно брать поточнее чем то которое мы в итоге хотим но это сейчас
[50:36.480 --> 50:41.200]  неважно и так если чк ты человек давайте напишу чтобы уника человек а
[50:41.200 --> 50:51.120]  катый человек человек в момент времени ты в момент и большое находится
[50:51.120 --> 51:00.840]  находится состояние 1 состояние 1 ну в вершине 1 вот ну соответственно не у
[51:00.840 --> 51:07.840]  катая 1 равняется 0 иначе ну то есть он находится в состоянии 2 ну что тут
[51:07.840 --> 51:15.480]  кубочках состояние 2 состояние 2 так хорошо что теперь ну теперь я могу
[51:15.480 --> 51:20.560]  сказать то же самое про каждого человека ката произвольный и просто
[51:20.560 --> 51:26.520]  задать вопросом вот у меня есть сумма не укатая тут именно в этот момент
[51:26.520 --> 51:34.680]  времени сумма не укатая от 1 к от 1 до ну чего там n большого так ведь да то
[51:34.680 --> 51:39.400]  вы можете сказать про эту сумму вот уже теперь вопрос чисто к вам вот ну просто
[51:39.400 --> 51:43.560]  чтобы проверить чтобы это не было голословно давайте вот подумаем значит
[51:43.560 --> 51:47.920]  что мы можем сказать мы близки к цели но еще это не цель еще ничего не
[51:47.920 --> 51:53.240]  обосновал итак у нас есть сумма случайных величин не укатая 1 ну даже
[51:53.240 --> 51:58.080]  как бы это случайная величина мы можем сказать какой у него закон распределения
[51:58.080 --> 52:01.480]  какой у него закон распределения ну приблизительно то есть вероятность того
[52:01.480 --> 52:07.720]  что не укатая от 1 равняется 1 чему равно ну приближенно чему это равно
[52:07.720 --> 52:20.920]  ну наверно п со звездой единица так ведь да ну это не очевидно потому что
[52:25.400 --> 52:30.540]  ну это просто результат о сходимости вот той динамике который я говорил ну
[52:30.540 --> 52:34.980]  то есть с точностью epsilon штрих с точностью epsilon штрих это так но если
[52:34.980 --> 52:39.660]  говорить совсем строго хорошо вы можете написать что по модулю это значит
[52:39.660 --> 52:43.980]  меньше чем epsilon штрих я могу epsilon штрих взять настолько маленький насколько хочу
[52:43.980 --> 52:48.100]  потому что он стоит под логорифом и поэтому я не морочусь такими вещами ну вот
[52:48.100 --> 52:51.900]  так сейчас согласны
[52:55.540 --> 53:02.180]  хорошо на самом деле я понял что смущает сейчас я тогда поясню это таким
[53:02.180 --> 53:07.060]  образом значит вас смущает что марковская динамика которая вот ну
[53:07.060 --> 53:11.180]  такая органическая марковская динамика что ей для того чтобы сходится требуется
[53:11.180 --> 53:19.180]  время 1 альфа так ведь или нет в это смущает откуда это один альф правильно
[53:19.180 --> 53:43.820]  ну хорошо ладно ну да да сейчас смотрите это все с одной стороны правда с
[53:43.820 --> 53:47.580]  другой стороны делал в деталях и надо количественно все это говорить значит
[53:47.580 --> 53:54.700]  в каком-то смысле у вас сомнения что муфосалин мудрец который проживет тысячу
[53:54.700 --> 54:00.620]  лет он в конце своей жизни будет знать больше чем 10 обычных людей которые живут
[54:00.620 --> 54:05.500]  по 100 лет неплохие обычные люди но неважно 10 людей по 100 лет проживут и они
[54:05.500 --> 54:09.580]  возьмут каждый в конце своей жизни соберутся вместе на съезде и скажут как
[54:09.580 --> 54:15.860]  надо жить среднем арифметическим вот об этом идет речь что 10 людей проживших
[54:15.860 --> 54:20.180]  по 100 лет и качество их прогноза это мы поняли что параллельно делается и
[54:20.180 --> 54:23.660]  качество прогноза сделанного муфосалином который прожил тысячи лет
[54:23.660 --> 54:28.900]  приблизительно одинаково ответ да это так точностью до логарифм в которые я
[54:28.900 --> 54:33.020]  опускаю вот логарифмы в случае муфосалина будут лучше а в случае вот
[54:33.020 --> 54:38.060]  этих вот обычных людей они будут их больше степень больше но это вопрос
[54:38.060 --> 54:42.260]  логарифмический а вот как ни странно это так то есть реально то самое время
[54:42.260 --> 54:47.060]  которое вот один альфа его и достаток им не надо больше каждый проживет столько
[54:47.060 --> 54:50.900]  времени там единственная проблема что я писал штрих надо брать точнее я потому
[54:50.900 --> 54:57.980]  написал аксен штрих ну как сказать понимаете вот совсем недавно то же самое
[54:57.980 --> 55:03.100]  к этому приду на одной из лекций произошло всток оптимизации я застал
[55:03.100 --> 55:10.300]  время общаясь с нестеровым когда просто не знаю это вот некоторые революции сейчас
[55:10.300 --> 55:17.820]  такая когда ну вот естественно было сказать что мы берем значит траекторию
[55:17.820 --> 55:23.220]  одного процесса достаточно длинную и по качеству но решение задачи сток
[55:23.220 --> 55:26.740]  оптимизации и по качеству это естественно должно быть лучше чем
[55:26.740 --> 55:31.420]  запустить параллельно траектории решение той же задачи независимый потом
[55:31.420 --> 55:34.940]  остановиться и взять средне арифметическая и то же самое сделать то
[55:34.940 --> 55:40.260]  есть прожить условно 100 итераций но 10 процессорами или одну большую
[55:40.260 --> 55:45.260]  траекторию из тысячи итераций но одну одну одни процессор и вот оказывается
[55:45.260 --> 55:52.980]  что ну естественно какими-то оговорками что в общем-то на самом деле это так ну
[55:52.980 --> 55:57.660]  то есть первым приближение это так и там дальше уже вопрос возникает ну с
[55:57.660 --> 56:02.940]  какими оговорками и да там бывают сказать вещи такие тонкие ситуации
[56:02.940 --> 56:07.100]  когда это не так но в целом это в первом приближении так это немного странно но
[56:07.100 --> 56:12.940]  этим надо пользоваться раз это так ну но но повторю что делал в деталях вот
[56:12.940 --> 56:17.500]  здесь этих оговорок меньше просто но они не одинаковые то есть там логарифмы
[56:17.500 --> 56:21.620]  разные ну хорошо коллеги я хотел бы просто еще чего-то успеть рассказать
[56:21.620 --> 56:25.420]  поэтому давайте мы уже как бы перейдем к тому что вы мне сами подскажете как
[56:25.420 --> 56:29.340]  вот это все получить как как вообще получить аналогичный результат но уже
[56:29.340 --> 56:34.620]  вот из этого подхода вот откуда он вытаскивается но давайте допустим цпт
[56:34.620 --> 56:39.260]  вот что нам говорит цпт что я должен куда написать вот эта сумма а нам
[56:39.260 --> 56:42.740]  интересно не столько сумма сколько вот эта штука так 1 на n давайте сразу
[56:42.740 --> 56:49.220]  писать в варианте который нам интересен что из этого я должен вычислить мат
[56:49.220 --> 56:53.340]  ожидания так замечательно мат ожидания чему равняется
[56:53.340 --> 57:01.620]  с звездой 1 так дальше я должен на что-то поделить ну у дисперсионер нужна
[57:01.620 --> 57:07.140]  чему она равна ну давайте давайте это я могу писать константа вам надо
[57:07.140 --> 57:15.060]  отвечать прям точно ну ну чему ровня дисперсия ну хорошо она равняется ps
[57:15.060 --> 57:20.100]  и звездой единица единица минус пресса звездой единицы вообще говоря если
[57:20.100 --> 57:23.620]  пресса звезды маленькая этим нельзя пренебрегать только там надо не дисперсия
[57:23.620 --> 57:30.020]  средне квадратичное отклонение по моему так ведь да ну еще на коре низен ну
[57:30.020 --> 57:34.860]  давайте разделим на коре низен так и эта штука приближенно равняется 0 1 так
[57:34.860 --> 57:39.860]  ведь ну по распределению вот что-то такое естественный слен большое
[57:39.860 --> 57:46.660]  естественный слен большое замечательно теперь я говорю следующее что значит вот
[57:46.660 --> 57:52.220]  эта разница вот эта разница если просто совершу некую махинацию перенесу
[57:52.220 --> 57:57.820]  знаменатель вот сюда вот эта разница она должна равняться должна равняться ну
[57:57.820 --> 58:03.500]  приближенно соответственно вот величине такого масштабы ps звездой от
[58:03.500 --> 58:08.780]  единички единица минус пресса звездой от единички на коре низен на соответственно
[58:08.780 --> 58:12.340]  стандартная нормальная случайная величина но это более-менее то чем мы тут
[58:12.340 --> 58:18.340]  занимаемся только тут было т альфа t а здесь было соответственно n большое то
[58:18.340 --> 58:22.220]  есть мы как бы перенесли то есть это означает что если я хочу сделать вот
[58:22.220 --> 58:26.940]  эту штуку масштаба эпсилон масштабы эпсилон я хочу сделать я должен
[58:26.940 --> 58:31.220]  поскольку эта величина более-менее порядка единицы но это и стандартная
[58:31.220 --> 58:35.300]  нормальная случайная величина она с большой вероятностью лежит диапазоне
[58:35.300 --> 58:39.180]  минус 3 3 и мне там неважно что она там может быть 10 это вероятность
[58:39.180 --> 58:43.620]  практически ничтожно поэтому поэтому чё
[58:44.340 --> 58:50.300]  ну вот я получаю что это равняется ипсим отсюда я получаю что n пропорционально
[58:50.300 --> 58:55.060]  1 на ипсин квадрат я правильно выбрал один на ипсин квадрате то есть вот
[58:55.060 --> 59:00.220]  откуда это и я как бы сейчас понятия я могу говорить по умному говорить что
[59:00.220 --> 59:03.060]  это приближенно на самом деле неправильный надо писать неравенство
[59:03.060 --> 59:07.740]  концентрации и мы будем этим заниматься но пока этого достаточно пока интуиция
[59:07.740 --> 59:11.820]  нас на как бы она о том что почему этот результат верен и в параллельной
[59:11.820 --> 59:16.500]  архитектуре а это время выхода то есть я тот же самый результат получил уже в
[59:16.500 --> 59:20.060]  варианте совсем практичном то есть я как бы запускаю параллельно человечков
[59:20.060 --> 59:24.660]  каждый живет вот такое время потом я смотрю где они находятся беру
[59:24.660 --> 59:30.500]  частотно где кто находится и соответственно определяю пропорции уже
[59:30.500 --> 59:35.860]  прям чисто ну как как реально в жизни и это тоже пейдж ранг неплохо правда
[59:35.860 --> 59:40.900]  то есть вообще говоря мы этот вектор пейдж ранг получили вполне уже
[59:40.900 --> 59:45.420]  практической процедурой возникает здесь огромное количество на самом деле
[59:45.420 --> 59:52.000]  вопросов связанных с тем что блин это как-то все очень слишком просто ведь на
[59:52.000 --> 59:57.520]  самом деле число моделей и вообще как бы ну вот то что нас интересует их как
[59:57.520 --> 01:00:01.440]  бы многообразие намного больше вот как это например связано с сетями массового
[01:00:01.440 --> 01:00:06.340]  обслуживания как например это связано со всякими сток химики на этика и как
[01:00:06.340 --> 01:00:10.480]  это связано с какими-то моделями они знают социодинамики как это связано
[01:00:10.480 --> 01:00:14.040]  например со моделированием распространения коронавируса и вообще
[01:00:14.040 --> 01:00:19.240]  эпидемии это все есть какая-то общая на общее начало ответ да вот это важно
[01:00:19.240 --> 01:00:24.720]  то есть вот на самом деле вы смотрите на некую математику и как бы замечательно
[01:00:24.720 --> 01:00:29.000]  что она лежит в основе еще много чего и вот обратите внимание чем я здесь
[01:00:29.000 --> 01:00:33.480]  пользовался вот вы как бы уйдете с лекции будете там дома о чем-то думать но
[01:00:33.480 --> 01:00:38.160]  если вы запомните два принципа пока только два и вы вы вам точно это ну как
[01:00:38.160 --> 01:00:42.400]  бы сказать пригодится еще уверен многократно вы можете забыть какие
[01:00:42.400 --> 01:00:46.560]  детали но запомните что мы пользовались органической теоремой и по сути
[01:00:46.560 --> 01:00:50.880]  центральной предельной теоремой по факту мы пользовались двумя пределами с
[01:00:50.880 --> 01:00:55.360]  одной стороны мы дали системе пожить достаточно долго и поэтому появилось
[01:00:55.360 --> 01:01:01.680]  понятие мера появился понятие понятие стационарное распределение вариантная
[01:01:01.680 --> 01:01:08.160]  мера а потом мы взяли скейлинг или то сказать чесну некий предел по числу
[01:01:08.160 --> 01:01:17.240]  агентов и мы стали эту меру как бы выделять как то-то что характеризует
[01:01:17.240 --> 01:01:22.760]  концентрацию вот вот уже как бы на каком многоагентной системы то есть то что
[01:01:22.760 --> 01:01:29.400]  нам надо было получить мы получили как два предела как предел числа агентов и
[01:01:29.400 --> 01:01:34.840]  предел на саму предел времени я мог в другом порядке это сделать то есть я
[01:01:34.840 --> 01:01:39.360]  мог например сначала сделать предел по числу агентов и и в этом случае и в
[01:01:39.360 --> 01:01:43.600]  этом случае если бы я сделал предел по числу агентов я бы получил ту самую
[01:01:43.600 --> 01:01:50.640]  динамику которая была вот это вот полноценная динамика которая ну как
[01:01:50.640 --> 01:01:54.560]  называется пауэр метод вот это то есть если бы обратный порядок предела сделал
[01:01:54.560 --> 01:01:58.520]  я сначала бы сделать бесконечно много агентов то у меня бы вот это эволюция
[01:01:58.520 --> 01:02:02.120]  полу функции плотности она бы в точности превратилась эволюции этого
[01:02:02.120 --> 01:02:06.560]  вектора и было бы не интересно а вот за счет того что я в обратном порядке
[01:02:06.560 --> 01:02:10.800]  предельные переходы сделать то есть сначала я время устремил бесконечности а
[01:02:10.800 --> 01:02:16.120]  потом число людей это вот получилось достаточно интересно интерпретируемо
[01:02:16.120 --> 01:02:20.400]  так вот что я хочу сказать это же самая конструкция используется многократно
[01:02:20.400 --> 01:02:25.640]  много где и вот эти эффекты как эргодическая теорема по сути принцип
[01:02:25.640 --> 01:02:30.160]  неподвижной точки и концентрации меры види цпт чего-то еще это те два
[01:02:30.160 --> 01:02:33.880]  атрибута которые будут в течение курса встречаться в совершенно разном
[01:02:33.880 --> 01:02:40.760]  контексте но они будут более-менее вот объяснять как из явления которое
[01:02:40.760 --> 01:02:47.000]  сложное вытащить что-то вот такое вот конечное этим пользоваться есть ли
[01:02:47.000 --> 01:02:52.720]  вопросы потому что я сейчас рассказал мы сейчас будем закреплять эту тему вот
[01:03:05.080 --> 01:03:11.520]  нет сейчас вот как бы представьте себе что помните здесь рисовал монетку как
[01:03:11.520 --> 01:03:16.960]  ну как бы кнутая алгоритм ведь когда мы кидаем монетку связанную с одним
[01:03:16.960 --> 01:03:20.760]  человеком и другим человеком просто постулируется что эти монетки разные
[01:03:20.760 --> 01:03:26.360]  независимые и они блуждают независимо точки старта выбираются независимо и
[01:03:26.360 --> 01:03:33.640]  все что как бы мы чем пользуемся это фактом что по прошествии вот этого времени
[01:03:33.640 --> 01:03:39.240]  распределение вероятности которая связана вот с тем или иным человеком оно
[01:03:39.240 --> 01:03:46.920]  значит близко кого-то к этому вектору пессо звездой и все независимость тут
[01:03:47.120 --> 01:03:51.400]  появляется вот этих случайных величин просто по построению то есть это как раз
[01:03:51.400 --> 01:03:58.200]  не сложно здесь как бы есть зависимость точки старта и текущего состояния в
[01:03:58.200 --> 01:04:02.660]  этот момент времени но нет зависимости между ними а зависимость от точки старта
[01:04:02.660 --> 01:04:07.280]  не страшная потому что мы пользуемся здесь когда пользуемся центральной
[01:04:07.280 --> 01:04:13.660]  предельной теоремой независимостью вот этих вот случайных величин то что это
[01:04:13.660 --> 01:04:18.900]  случайная величина частично определяется точкой старта так это не страшно это просто вопрос
[01:04:18.900 --> 01:04:25.860]  качество опроксимации вот этой штуки то есть ну будет не совсем точно ну и тут байс какой-то
[01:04:25.860 --> 01:04:30.500]  надо будет учитывать но поскольку это все под логорифом в той оценке то я могу все вот эти
[01:04:30.500 --> 01:04:36.060]  вопросы загнать ну с запасиком то есть это надо аккуратно делать но когда что-то под логорифом
[01:04:36.060 --> 01:04:44.300]  можно об этом сильно не думать этот крас тот случай так теперь я расскажу довольно странную
[01:04:44.300 --> 01:04:51.220]  историю насколько я понимаю реальная история была был перехват письма из тюрьмы тысяча символов
[01:04:51.220 --> 01:05:02.060]  из американской тюрьмы насколько я понимаю давно значит в этом письме люди видимо увлекающиеся
[01:05:02.060 --> 01:05:09.500]  перлаком холмсом подобно тому как пляшущие человечки начата код был ставил соответствия
[01:05:09.500 --> 01:05:15.260]  буквам английского языка и символом разных человечков вот так в этом же письме тоже были
[01:05:15.260 --> 01:05:23.780]  нарисованы человечки их было 10 тысяч ну так я приблизительно говорю и возник вопрос как бы
[01:05:23.780 --> 01:05:31.100]  что написано в этом письме ну потому что вообще говоря да кстати нет вопросов чате отлично
[01:05:31.100 --> 01:05:44.940]  вообще не отлично ну значит у меня есть время и дальше еще раз ссылку начать начать в общую
[01:05:44.940 --> 01:05:51.060]  группу за ведь так это как бы как противоречиво получается то есть если там и кинут ссылку начать
[01:05:51.500 --> 01:06:08.140]  ну ладно это не ко мне да хорошо значит м-7-7 то есть тема которую мы сейчас развиваем это
[01:06:08.140 --> 01:06:14.260]  продолжение вот этого называется марков чейн монте карл так вот было письмо 10 тысяч символов 10
[01:06:15.260 --> 01:06:22.020]  символов и функция которая сопоставляет буквы там английского алфавита человечка в общем таких
[01:06:22.020 --> 01:06:29.020]  функций столько сколько перестановок то есть у нас есть массив из 40 символов ну буквы плюс всякие
[01:06:29.020 --> 01:06:36.940]  символы типа запятая и человечки ну сколько уникальных там человечков было вот ну давайте
[01:06:36.940 --> 01:06:50.500]  для простоты считать 30 это сейчас не так важно вот один в один да ну то есть да да да да да да да
[01:06:50.500 --> 01:06:56.500]  действительно если взять все все правильно вы говорите сейчас все будет вопрос не в этом то есть
[01:06:56.500 --> 01:07:01.740]  я говорю у нас цель не столько решить задачу сколько продемонстрирует некое единобразие
[01:07:01.740 --> 01:07:07.500]  математики это действительно важно потому что ну очень как бы так классно получается что одна
[01:07:07.500 --> 01:07:10.740]  и та же математика решает огромное количество задач и действительно есть какие-то свои особенности
[01:07:10.740 --> 01:07:16.300]  у каждой задачи они нам важны но это второстепенно потому что главная математика вот в нашем курсе
[01:07:16.300 --> 01:07:23.580]  действительно предварительно можно просто посчитать вот мы берем обучаем например какую-нибудь
[01:07:23.580 --> 01:07:29.820]  ну программу которая считает частотно текстов война и мир на английском сколько в войне и
[01:07:29.820 --> 01:07:37.140]  мир на я не знаю на весь объем на условно миллион символов букв сколько буквы а раз встречается и
[01:07:37.140 --> 01:07:43.140]  вот частотным образом мы обучаем такую программу и соответственно просто смотрим сопоставление
[01:07:43.140 --> 01:07:50.020]  частот сортируем массив частот сортируем массив частот встречаемости человечков и получаем
[01:07:50.020 --> 01:07:57.260]  предварительно некую функцию f которая собственно и есть этот самый изоморфизм изоморфизм который
[01:07:57.260 --> 01:08:03.620]  вот здесь строится то есть у нас есть предварительный кандидат предварительный кандидат есть но дальше
[01:08:03.620 --> 01:08:10.140]  возникает следующий как бы этап понимание что качество такого ну это просто реально есть статья
[01:08:10.140 --> 01:08:16.020]  персидиаконис в которой прямо написано что будет если если так сделать то есть вот если вы
[01:08:16.020 --> 01:08:22.900]  сделаете 2009 года если вы сделаете такую процедуру там была реально письма письма с человечками и
[01:08:22.900 --> 01:08:29.660]  то что получается получается честно сказать но не очень хорошо я такое не могу понять я просто по
[01:08:29.660 --> 01:08:34.740]  английски не то чтобы классно читаю но это прям вообще непонятно и поэтому было решено сделать
[01:08:34.740 --> 01:08:40.300]  следующее что вообще говоря есть не просто частотная закономерность букв а есть закономерность с
[01:08:40.300 --> 01:08:46.620]  какой вероятностью после одной буквы идет другая буква то есть надо проходить по войне и миру не
[01:08:46.620 --> 01:08:54.020]  просто считая частоту встречаемости а а как бы определяя вероятность того что после буквы
[01:08:54.020 --> 01:09:06.820]  давайте последовательно все это получается при постановке человечка в букву английского
[01:09:06.820 --> 01:09:14.100]  алфавита и вот собственно x к f от x к плюс первая вот эту вероятность можно оценить в смысле не
[01:09:14.100 --> 01:09:20.780]  можно оценить она есть то есть для любых двух букв я не знаю па б да мы можем оценить чему
[01:09:20.780 --> 01:09:27.740]  равняется вероятность того что за букву а пойдет буква б и таким образом мы можем написать
[01:09:27.740 --> 01:09:34.460]  вероятность любого текста которые выявляется то есть мы просто берем первую букву по частотному
[01:09:34.460 --> 01:09:42.060]  закону пишем то есть п от f от x согласно частотной вот этой таблице потом мы берем п но и ф от
[01:09:42.060 --> 01:09:49.740]  x 0 потом мы берем п от f от x 0 на f от x 1 потом мы берем вероятность того что f от x 1 на f от
[01:09:49.740 --> 01:09:56.300]  x 2 и так далее и вот вот так вот посчитанная вероятность это есть вероятность того что ну
[01:09:56.300 --> 01:10:06.100]  значит как бы вероятность функции f то есть мы можем приписать конкретному способу дешифрования
[01:10:06.100 --> 01:10:14.260]  вероятность правильно то есть я уже ничего такого хитрого сейчас не сказал а какая разница нам
[01:10:14.260 --> 01:10:19.620]  же эти не нам же важно ни к чему равняется это вероятность а как они между собой соотносятся
[01:10:19.620 --> 01:10:25.500]  понятно что она будет не не ну понятно что она будет вы можете логарифм считать этой вероятности
[01:10:25.500 --> 01:10:31.100]  какая разница то есть это вы можете умножать на 10 десятые это же не очень важно вот ну больше
[01:10:31.100 --> 01:10:35.620]  того она сильно того ноль не улетит все-таки число пар оно не то чтобы колоссально большое
[01:10:35.620 --> 01:10:43.140]  ну 40 на 40 да ну сколько это будет значит ну 10 в третье ну будет значит соответственно
[01:10:43.140 --> 01:10:49.620]  каждый такая штучка 1 на 10 третьей но значит вы просто можете ну текстом 10 тысяч вы просто
[01:10:49.620 --> 01:10:55.700]  можете смотреть на этот порядок который одинаковый его просто не учитывать это для нас неважно то есть
[01:10:55.700 --> 01:11:00.620]  для нас эта функция определена как бы с точностью до множителя нам этот множитель абсолютно не
[01:11:00.620 --> 01:11:08.540]  важен для нас важно что они друг от друга вот отличаться будут ну сильно но не так чтобы
[01:11:08.540 --> 01:11:14.020]  очень вот мы на этом сказать будем играть ну да они кстати между собой могут сильно отличаться
[01:11:14.020 --> 01:11:20.780]  хорошо какой какая стратегия действия какой бы f вы могли бы выбрать то есть как бы как что
[01:11:20.780 --> 01:11:29.460]  вам кажется правильно выбрать вот в этой задачи максимум по f взять правильно ну то есть вы решить
[01:11:29.460 --> 01:11:35.220]  задачу максимизации то есть перебирать каким-то образом но это же переборная задача правильно
[01:11:35.220 --> 01:11:44.540]  понимаю ну и кто-то знает какой не переборно решать это максимум правдоподобия но а как
[01:11:44.540 --> 01:11:51.580]  ее решать то есть градиент не посчитайте а теперь такой вопрос я это зафиксировал а теперь вы давайте
[01:11:51.580 --> 01:11:59.340]  сюда вернемся вот почему ни у кого не возник вопрос что я написал средне арифметическая почему
[01:11:59.340 --> 01:12:04.300]  все как-то приняли это как ну на веру и никто не спросил а почему средне арифметическое почему
[01:12:04.300 --> 01:12:09.540]  я не взял какую-то другую напрямую не смещенную оценку ну то есть это настолько естественно что
[01:12:09.540 --> 01:12:15.980]  мы не задумываемся на самом деле но это не то чтобы прям ну как сказать всегда очевидно но еще
[01:12:15.980 --> 01:12:22.380]  раз вот почему почему имея значит такую случайную величину вот не укатнил давайте
[01:12:22.380 --> 01:12:31.580]  катая до 1 и зная что ем от ожидания равняется неизвестному параметру ps 2 1 почему я использовал
[01:12:31.580 --> 01:12:40.140]  именно средне арифметическое но замечательно представьте себе что я сумасшедший лучник беру
[01:12:40.140 --> 01:12:45.860]  так стрелу так бац и стенку так вот ну вот туда вот так вот с равной вероятностью любой угол
[01:12:45.860 --> 01:12:52.060]  выбираю ну и потом я прошу вас определить расстояние на котором я нахожусь от стенки чтобы
[01:12:52.060 --> 01:12:58.740]  вы делали правильно там распределение каши это значит что средне арифметическое будет
[01:12:58.740 --> 01:13:03.140]  иметь такой же закон как отдельное слагаемое то есть о том что возьмете средне арифметическое
[01:13:03.140 --> 01:13:07.860]  у вас закон распыли не поменяется там надо брать тогда что-то типа медианы или не знаю как-то
[01:13:07.860 --> 01:13:12.820]  обрезать там ну что-то хитрее действовать то есть еще раз ничего и ничего особо умного есть
[01:13:12.820 --> 01:13:18.780]  так он распределение как я кидая стрелы накидываю но как бы этот закон зависит от расстояния если я
[01:13:18.780 --> 01:13:24.940]  вот так стою то стрелы будут значит одним образом ложиться если я стою вот далеко то стрелы будут
[01:13:24.940 --> 01:13:29.820]  другим образом ложиться получается что этот параметр неизвестный можно как раз попытаться
[01:13:29.820 --> 01:13:35.220]  восстановить из того что там ну статистика моих попаданий куда-то а я равновероятно кидаю так вот
[01:13:35.220 --> 01:13:41.500]  и не очевидно что из этого закона надо бы именно вот то есть это не может быть не лучший способ
[01:13:41.500 --> 01:13:48.700]  возникает вопрос а как при каких условиях почему именно такая оценка возникла вот давайте
[01:13:48.700 --> 01:13:52.980]  попробуем сопоставить вот с этим подходом чтобы почувствовали что на самом деле об одном и том же
[01:13:52.980 --> 01:14:00.100]  а на самом деле все очень просто если у вас есть вероятностная модель она у вас есть параметрическая
[01:14:00.100 --> 01:14:05.700]  модель вот она у вас собственно есть что вы имеете совокупность простую выборку независимых
[01:14:05.700 --> 01:14:13.540]  одинаково распределенных случайных величин вот она вот ну дальше и возникает вопрос какова
[01:14:13.540 --> 01:14:19.620]  вероятность того что выпадет то что выпадет ну если истинная вероятность п давайте так назовем
[01:14:19.620 --> 01:14:25.220]  выпадение орла или выпадение вот состояние 1 то я могу написать что вероятность того что
[01:14:25.220 --> 01:14:30.820]  выпадет конкретная последовательность это есть вот такая вот сумма нюкаты от единицы кат одного
[01:14:30.980 --> 01:14:37.460]  да сколько у нас там было n большое на единицы минус п но эти п с извездой 1 буду писать
[01:14:37.460 --> 01:14:45.460]  п с извездой 1 на значит соответственно n большое минус сумма нюката согласны что это так или есть
[01:14:45.460 --> 01:14:51.900]  какие-то другие гипотезы как это должно выглядеть может биноминальный коэффициент я опустил или не
[01:14:52.420 --> 01:15:01.780]  правильно почему я не опустил почему мне должно здесь быть потому что те действительно я
[01:15:01.780 --> 01:15:06.620]  сформулировал задачу так какова вероятность того что выпадет конкретная последовательность но
[01:15:06.620 --> 01:15:10.980]  если конкретное последовательства мне не надо учитывать сколько в ней было успехов если я
[01:15:10.980 --> 01:15:15.540]  учитывал чтоmania успехи могут быть в разных местах я бы тогда здесь поставить биноминальный
[01:15:15.540 --> 01:15:19.980]  коэффициент а мне не интересно где как бы разные учитывsc underwear? конкретной послед utilities я
[01:15:19.980 --> 01:15:26.140]  вот я считаю сколько раз было орел это вот число успехов и сколько раз решка число неудач
[01:15:26.140 --> 01:15:31.100]  но это конкретная последовательность поэтому мне не надо учитывать кратность враждения теперь такой
[01:15:31.100 --> 01:15:37.180]  вопрос вот это вот что такое это есть правдоподобие это есть аналог того что здесь написано только
[01:15:37.180 --> 01:15:42.340]  здесь я подбираю f я работаю в пространстве функции устанавливающих азаморфиз между буквами
[01:15:42.340 --> 01:15:48.540]  человечками а здесь я работаю в пространстве ну как бы однопараметрическом вот этот параметр
[01:15:48.540 --> 01:15:55.020]  на самом деле я упростил ситуацию до двух вершин если бы я не упрощал до двух вершин естественно
[01:15:55.020 --> 01:16:00.060]  мне бы надо было здесь писать многопараметрическое все это дело и тогда бы здесь было мультинамиальное
[01:16:00.060 --> 01:16:04.620]  распределение и было бы немножко посложнее но физика та же самая то есть получились бы те же
[01:16:04.620 --> 01:16:10.100]  самые оценки только по компонентно и дальше что я должен делать что о чем говорит принцип максимум
[01:16:10.100 --> 01:16:18.420]  правдоподобия правильно то есть п оценка вот это вот оптимальная оценка некотором смысле п с
[01:16:18.420 --> 01:16:25.300]  крышкой она должна равняться ну как бы арг максимум арг максимум вот этого выражения арг максимум по
[01:16:25.300 --> 01:16:32.500]  п по п со звездой единичка это вот п с звездой единичка но возникает вопрос хорошо я вот такую
[01:16:32.500 --> 01:16:38.300]  оценку найду собственно я ее нашел это вот и есть такая оценка доказывается очень просто арг
[01:16:38.720 --> 01:16:43.140]  максимум от этого выражения есть арг максимум от логарифма этого выражения когда я возьму логарифм
[01:16:43.140 --> 01:16:48.300]  меня эта штука перейдет в числитель логарифм п будут здесь будет логарифма 1 минус по я
[01:16:48.300 --> 01:16:53.100]  могу продиференцировать по п минимум достигает максимум достигается в этой точке то есть вот
[01:16:53.100 --> 01:16:58.620]  это и есть та самая оценка а дальше работает такой результат фундаментальный результат который
[01:16:58.620 --> 01:17:02.480]  называется теоремы фишера мне хотелось бы это рассказать потому что мы сейчас этим будем
[01:17:02.480 --> 01:17:09.180]  пользоваться результат заключается в том что вот то что я здесь говорил про
[01:17:09.180 --> 01:17:15.860]  центральную предельную теорему это собственно общий результат для таких
[01:17:15.860 --> 01:17:21.080]  оценок то есть вот такая такого типа штука то есть вот эта оценка оценка
[01:17:21.080 --> 01:17:28.400]  f с крышкой f с крышкой да значит f с крышкой со звездой 1 причем это
[01:17:28.400 --> 01:17:33.560]  может быть вектор на минус п с звездой 1 значит будет меньше либо равняться
[01:17:33.560 --> 01:17:39.280]  некая константа ц давайте я напишу ц и от соответственно ц от настоящего
[01:17:39.280 --> 01:17:45.180]  значения она зависит п с звездой 1 но его здесь будет стоять логарифм сигма
[01:17:45.180 --> 01:17:50.560]  минус первый на н вот такая вот штука и все это верно с вероятностью с
[01:17:50.560 --> 01:17:55.880]  вероятностью 1 минут сигма вот это верно возникает вопрос но и чего я
[01:17:55.880 --> 01:18:01.480]  такого написал особо неожиданного ну то есть во первых во первых вот чтобы
[01:18:01.480 --> 01:18:06.840]  понять за что идет борьба вот сразу скажу что вот это дело оно
[01:18:06.840 --> 01:18:11.000]  получается при любых разумных подходах и вы можете брать не обязательно подход
[01:18:11.000 --> 01:18:15.560]  максимум правдоподобия какой-то еще и вот вот вот это будет более-менее так
[01:18:15.560 --> 01:18:20.960]  но если это разумно борьба идет за вот эту штуку то есть оценка максимального
[01:18:20.960 --> 01:18:25.840]  правдоподобия давайте я здесь явно напишу вот она вообще говоря
[01:18:25.840 --> 01:18:30.080]  естественно зависит от объема выборки который у нас и поэтому это
[01:18:30.080 --> 01:18:36.560]  коэффициентик вообще говоря зависит от н но теорема фишера утверждает что вот
[01:18:36.560 --> 01:18:44.040]  это вот тн цен то есть предел приен стремящимся к бесконечности вот этого цена
[01:18:44.040 --> 01:18:48.000]  тета равняется некий но не тета п с звездой в нашем случае
[01:18:48.000 --> 01:18:54.440]  п значит со звездой один с крышкой он равняется просто ц от п со звездой один
[01:18:54.440 --> 01:18:59.840]  с крышкой и вот эта штука она минимально среди всех возможных оценок то есть вы
[01:18:59.840 --> 01:19:05.360]  можете как угодно оценивать но в пределе когда число объем выборки в
[01:19:05.360 --> 01:19:08.600]  нашем случае число человечков стремиться к бесконечности вот вы не
[01:19:08.600 --> 01:19:12.760]  сможете предложить такой способ оценивания который был имел константу
[01:19:12.760 --> 01:19:18.200]  лучше чем подход максимум правдоподобия понятно о чем речь или не совсем
[01:19:18.200 --> 01:19:24.000]  предел смысле числовой последовательности это это все числовые числа
[01:19:24.000 --> 01:19:28.640]  вероятность здесь как бы она сидит в том что вот только это случайно это
[01:19:28.640 --> 01:19:33.360]  случайно и поэтому я говорю с вероятностью все остальное правая
[01:19:33.360 --> 01:19:40.200]  часть естественно какие-то числа оценка это с крышкой то есть вот это и есть вот
[01:19:40.200 --> 01:19:44.480]  это средне арифметическое не у меня к сожалению с обозначениями какая-то
[01:19:44.480 --> 01:19:49.000]  проблема я чувствую что обозначение меня плохие я прям это чувствую ничего
[01:19:49.000 --> 01:19:55.960]  сделать с этим не могу я поясню почему потому что я как бы частично импровизирую
[01:19:55.960 --> 01:20:01.080]  и вот мне кажется что мне сейчас они удачно выбрал обозначение да
[01:20:02.480 --> 01:20:07.800]  константа зависит все-таки от настоящего по значения параметра и
[01:20:07.800 --> 01:20:12.880]  истинного вот вопрос правильный я сейчас мы до этого дойдем а как на
[01:20:12.880 --> 01:20:17.120]  практике вычислять мы же не знаем это значение вот тут будет ответ на ваш
[01:20:17.120 --> 01:20:21.560]  вопрос то есть проблема в том что ну я не знаю то есть на самом деле мы эту
[01:20:21.560 --> 01:20:26.040]  константу в данном случае можем явно написать ну смыться я ее сейчас напишу
[01:20:26.040 --> 01:20:33.440]  cn от п со звездой единица с крышкой ну без крышки было но это в нашем случае
[01:20:33.440 --> 01:20:39.480]  равняется корень из п со звездой 1 на 1 минус п со звездой 1 мы же знаем чему
[01:20:39.480 --> 01:20:43.720]  это константа в нашем случае равна ну и тут какое-то число я ну просто число
[01:20:43.720 --> 01:20:48.720]  просто число там не знаю церк ну двое двойка там я не помню чему она равна и
[01:20:48.720 --> 01:20:53.360]  мы знаем чему она равна в нашем случае вот это константа и возникает проблема
[01:20:53.360 --> 01:20:59.200]  что а как использовать эту оценку на практике и вот тут подсказка как раз так
[01:20:59.200 --> 01:21:02.260]  и звучит что если вы хотите пользоваться это оценкой возьмите это
[01:21:02.260 --> 01:21:06.040]  неравенство и подставьте разверните его правильным образом и подставьте
[01:21:06.040 --> 01:21:10.960]  оценку сверху п со звездой от 1 то есть вы получите вместо п со звездой а тут
[01:21:10.960 --> 01:21:14.480]  оценку снизу то есть подставьте сюда оценку сверху вот отсюда
[01:21:14.480 --> 01:21:18.800]  п со звездой от 1 и отсюда то есть это будет п со звездой от 1 плюс вот это
[01:21:18.800 --> 01:21:22.920]  поправка но возникает в каком-то смысле порочный круг потому что в этой
[01:21:22.920 --> 01:21:27.360]  поправке в свою очередь сидит п со звездой и ясно что когда вы будете
[01:21:27.360 --> 01:21:30.840]  заниматься вот этим вот я вообще не советую этим заниматься но если так
[01:21:30.840 --> 01:21:34.360]  будет желание то в какой-то момент вам надо просто остановиться и уже не
[01:21:34.360 --> 01:21:38.760]  заниматься поправкой то есть вам надо это называется философия построения
[01:21:38.760 --> 01:21:42.700]  симпатического ряда то есть вы столкнулись с проблемой вы не знаете как
[01:21:42.700 --> 01:21:48.480]  сюда подставить п со звездой но вы имеете формулу которая оценивает п со
[01:21:48.480 --> 01:21:52.600]  звездой и сверху и снизу проблема в том что в этой формуле в свою очередь
[01:21:52.600 --> 01:21:57.960]  сидит п со звездой но она уже сидит как бы на уровень ниже то есть влияние
[01:21:57.960 --> 01:22:02.320]  вот этого п со звездой меньше будет потому что там еще корень добавится и
[01:22:02.320 --> 01:22:08.160]  вот вы значит оцениваете здесь п со звездой от 1 вот этим вот и следующие
[01:22:08.160 --> 01:22:11.240]  слагами будут уточняющие уже с дополнительным фактором один на корень
[01:22:11.240 --> 01:22:15.280]  из н и вы можете разложить в ряд по один на корень из н а симпатический ряд
[01:22:15.280 --> 01:22:20.680]  красиво правда ну ладно не удержался это не было в планах но раз вопрос
[01:22:20.680 --> 01:22:25.840]  возник так вот еще раз зачем я сейчас вам это рассказываю затем что понимаете
[01:22:25.840 --> 01:22:31.600]  вот это все об одном и том же вот здесь максимум правдоподобия там максимум
[01:22:31.600 --> 01:22:35.680]  правдоподобия но если объем выборки большой то это не просто какой-то
[01:22:35.680 --> 01:22:40.640]  принцип оценивания а это принцип оценивания который приводит к наименьшим
[01:22:40.640 --> 01:22:44.440]  дисперсии наименьшим ну вот это все-таки не очень строго ну как бы на
[01:22:44.440 --> 01:22:48.640]  меньшему доверительному интервалу доверительному множеству то есть вы был
[01:22:48.640 --> 01:22:53.400]  получается из данных вычленяете ну самый точный как бы вы самым точным
[01:22:53.400 --> 01:22:57.800]  образом оцениваете неизвестный параметр то есть заданной вероятностью вы на
[01:22:57.800 --> 01:23:01.520]  и более компактно локализуйте неизвестные параметры не с точки
[01:23:01.520 --> 01:23:05.680]  зрения зависимости от н а с точки зрения вот того как будет входить этот
[01:23:05.680 --> 01:23:10.800]  нождель вот ну а симпатически проблема в том что это результат а симпатически и
[01:23:10.800 --> 01:23:15.600]  на самом деле к сожалению хоть не а симпатической теории есть но она
[01:23:15.600 --> 01:23:19.640]  принципиально не не исправляет оговорку что это только на бесконечных
[01:23:19.640 --> 01:23:24.440]  временах то есть при конечных временах при конечных n это может быть не самый
[01:23:24.440 --> 01:23:28.040]  лучший способ то есть метод максимального продоподобия он
[01:23:28.040 --> 01:23:31.720]  гарантированно и лучше только для экспоненциального семейства которому
[01:23:31.720 --> 01:23:35.440]  принадлежит схемы испытаний бернули то есть это это гарантированно для любого
[01:23:35.440 --> 01:23:40.440]  n будет наилучшая но это не всегда так ну и когда много параметров понятия что
[01:23:40.440 --> 01:23:46.240]  такое лучшее не совсем понятно потому что это один компонент а у нас их много
[01:23:46.240 --> 01:23:52.320]  лучше по одной компоненте но вот оказывается что равномерно на и лучше по
[01:23:52.320 --> 01:23:57.000]  попа то сказать всем направлением мы еще скажу что когда мы говорим о лучшее не
[01:23:57.000 --> 01:24:01.360]  лучше это тоже сложно говорить потому что здесь есть п со звездой и в пункции
[01:24:01.360 --> 01:24:06.680]  может быть самая маленькая при то есть при заднем п а при другом п другая
[01:24:06.680 --> 01:24:11.040]  другой способ оценивания лучше вот что примечательно метод максимального
[01:24:11.040 --> 01:24:15.240]  правдоподобия он именно говорит о том что вся функция для всех своих
[01:24:15.240 --> 01:24:21.400]  аргументов имеет значение соответственно лучше чем любой другой способ не для
[01:24:21.400 --> 01:24:25.520]  конкретного для всех вот это не тривиально это действительно такой очень
[01:24:25.520 --> 01:24:30.520]  мощный принцип как оценивать что-то что вы не знаете вот ну согласитесь это
[01:24:30.520 --> 01:24:35.360]  просто вы пишите вероятность того что запараметризуйте запараметризуйте то
[01:24:35.360 --> 01:24:39.040]  что вы не знаете по это параметр и найдите оценку максимального
[01:24:39.040 --> 01:24:43.800]  правдоподобия а симпатически это оценка на и лучше ну при некоторых
[01:24:43.800 --> 01:24:49.880]  условиях регулярности вот ну и вы пользуетесь этим ну к слову насчет
[01:24:49.880 --> 01:24:55.920]  условий регулярности это вот такое хорошее упражнение я раньше когда в
[01:24:55.920 --> 01:24:59.400]  москве надо было часто бывать ну просто мне стало интересно сколько
[01:24:59.400 --> 01:25:03.600]  ступенек на эскалаторе на водонах я ездил через водонах я каждый раз вставал
[01:25:03.600 --> 01:25:07.080]  на ступеньку они пронумерованы ну там они на пронумерон через каждый 10
[01:25:07.080 --> 01:25:10.320]  ступенек и ты видишь только более менее ступеньку которая в твоей
[01:25:10.320 --> 01:25:17.400]  окрестности то есть я встаю на ступеньку вижу ага там 3 150 160 в какой-то день я
[01:25:17.400 --> 01:25:22.760]  увидел по моему 180 ступеньку и мне было вопрос сколько вообще ступенек вот я
[01:25:22.760 --> 01:25:29.320]  месяц так ездил а ступенек понятно что больше чем 30 и значит я наблюдал давайте
[01:25:29.320 --> 01:25:32.480]  для простоты считать что ступеньки все пронумерованы и вот я каждый день
[01:25:32.480 --> 01:25:36.680]  наблюдал ну словно каждый день ступеньку с номером возникает вопрос как
[01:25:36.680 --> 01:25:41.440]  мне оценить ступеньку максимальное число ступенек то есть у меня есть
[01:25:41.440 --> 01:25:47.000]  выборка из равномерного распределения на отрезке 1 и неизвестное мне число n
[01:25:47.000 --> 01:25:51.040]  давайте я назову это параметром это не что я отвлекаюсь но просто это реально
[01:25:51.040 --> 01:25:55.400]  как бы случай жизни по моим интересам а значит это вы не знаете как бы вы
[01:25:55.400 --> 01:25:59.920]  оценивали если у вас есть выборка x и t каждые случайно величайки каждый
[01:26:00.000 --> 01:26:04.680]  имеет равномерное распределение на отрезке 1 это вот чтобы вы сделали
[01:26:04.680 --> 01:26:10.120]  мат ожидания к ситово чему равно вот это пополам ну значит если я возьму
[01:26:10.120 --> 01:26:16.080]  средне арифметическая 1 на n и сумма иксит их ну или кат их у нас кайт
[01:26:16.080 --> 01:26:22.480]  обозначается сумма икс кат их кат одного до n то мат ожидания чему будет равно
[01:26:22.480 --> 01:26:28.040]  ну мат ожидания будет равняться это пополам так ведь но если я соответственно
[01:26:28.040 --> 01:26:34.320]  поставлю здесь двойку у меня мат ожидания будет это правильно то есть я
[01:26:34.320 --> 01:26:38.800]  могу оценивать неизвестный параметр это тем что взять средне арифметическая
[01:26:38.800 --> 01:26:43.080]  чесну вот каждой ступени которые мне встречались умножить на два и получить
[01:26:43.080 --> 01:26:47.120]  какую-то теорию которая приведет более-менее такой оценки так как ни
[01:26:47.120 --> 01:26:53.240]  странно так но это неверно с точки зрения теорема фишера то есть теорема
[01:26:53.240 --> 01:26:57.520]  фишера говорит об какой-то оптимальности только в случае когда носитель
[01:26:57.520 --> 01:27:00.760]  распределение не зависит от параметра обратите внимание на то что здесь
[01:27:00.760 --> 01:27:06.340]  написано у меня распределение от единицы до тета то есть у меня неизвестный
[01:27:06.340 --> 01:27:11.820]  параметр тета как бы входит носитель я не могу дифференцировать например по
[01:27:11.820 --> 01:27:15.480]  знаком каком-нибудь интеграл а то есть это не есть регулярное семейство у меня
[01:27:15.480 --> 01:27:19.880]  то множество мера где значит она функция отличная от нуля функции
[01:27:19.880 --> 01:27:23.800]  распределения зависит в свою очередь от параметра вот в предыдущем модели
[01:27:23.800 --> 01:27:28.180]  такого не было и как ни странно вот здесь оказывается что есть оценка
[01:27:28.180 --> 01:27:32.920]  которая намного лучше а именно я беру x максимальный то есть я беру это
[01:27:32.920 --> 01:27:38.340]  называется ну n-то порядковая статистика то есть максимальный элемент выборки я
[01:27:38.340 --> 01:27:44.840]  беру максимальный элемент выборки x ката кат одного дн и умножаю его на n плюс
[01:27:44.840 --> 01:27:49.760]  один поделить на n вот такую оценку беру то есть чуть-чуть корректирую и
[01:27:49.760 --> 01:27:55.040]  оказывается что эта оценка имеет дисперсию которая пропорционально 1 на n
[01:27:55.040 --> 01:27:59.800]  они 1 на n в квадрате и эта оценка естественно сильно лучше она вообще из
[01:27:59.800 --> 01:28:02.480]  другой лиги она просто но это противоречия с
[01:28:02.480 --> 01:28:08.460]  теоремой фишер но противоречия нет потому что потому что это сюда входит не
[01:28:08.460 --> 01:28:13.040]  совсем так как надо но это тонкости это я вам сейчас просто привел как бы некий
[01:28:13.040 --> 01:28:17.840]  результат что вы понимали что ну вообще не все что сказать ну не как сейчас
[01:28:17.840 --> 01:28:21.640]  модно говорить это другое это как бы немножко другое это не то же самое что
[01:28:21.640 --> 01:28:26.760]  вот потому что носитель зависит от распределения и не надо это как бы вот
[01:28:26.760 --> 01:28:30.880]  делать тут аналогии то есть все-таки мы пытаемся как-то единобразно смотреть но
[01:28:30.880 --> 01:28:35.800]  где-то надо проводить черту и говорить что извините не так не работает хорошо
[01:28:35.800 --> 01:28:40.040]  возвращаемся сюда значит что мы сейчас поняли то мы вообще узнали уже как бы
[01:28:40.040 --> 01:28:44.760]  кульминация мы узнали что есть вот органическая теорема есть всякие
[01:28:44.760 --> 01:28:48.240]  неравенство концентрации но мы пока пользуясь простейшими вариантами
[01:28:48.240 --> 01:28:53.000]  предельных теорем но и есть принцип максимума правдоподобия понятно что все
[01:28:53.000 --> 01:28:57.000]  это как-то уживается давайте посмотрим как это уживается марков чейн монте
[01:28:57.000 --> 01:29:04.800]  карло и это будет кульминация вот на этой задачи как это все вместе поженить
[01:29:04.800 --> 01:29:11.400]  смотрите значит все на самом деле достаточно просто если мы придумаем
[01:29:11.400 --> 01:29:19.080]  марковскую цепь какую-то динамику которая бы имела равномерное распределение
[01:29:19.080 --> 01:29:26.800]  стационарная неравномерно стационарная инвариантная вот это вот как бы это
[01:29:26.800 --> 01:29:31.440]  задача то есть если нам удается придумать такую динамику то дальше нам
[01:29:31.440 --> 01:29:39.440]  не обязательно я не знаю заниматься тем что просто реально сравнивать какие-то
[01:29:39.800 --> 01:29:48.000]  функции мы можем сделать очень простой процесс мы можем запустить как бы этот
[01:29:48.000 --> 01:29:53.000]  марковский процесс запустить марковский процесс и просто посмотреть куда он
[01:29:53.000 --> 01:29:59.000]  сойдется вот я сейчас опишу следуя той самой статье диакониса алгоритм и мы
[01:29:59.000 --> 01:30:02.800]  за оставшееся время попробуем понять почему он работает у нас как раз
[01:30:02.800 --> 01:30:07.720]  остается час если хотите можно сделать перерыв немножко снизить и следующей
[01:30:07.720 --> 01:30:12.360]  лекции они будут более конкретные то есть мне важно было и на первой лекции
[01:30:12.360 --> 01:30:16.680]  телом как показать немного что что у нас дальше будет и много из того что мы
[01:30:16.680 --> 01:30:23.000]  сейчас с корреговоркой проговаривали оно будет действительно нормально
[01:30:23.000 --> 01:30:29.600]  рассказано частично то есть вот то что я сейчас говорит с цпт там все как-то
[01:30:29.600 --> 01:30:34.520]  не аккуратно махал руками много из этого в другом контексте нам еще раз
[01:30:34.520 --> 01:30:39.720]  встретиться и там я докажу уже не цпт а не равенство концентрации и но в
[01:30:39.720 --> 01:30:42.680]  другом контексте но вы можете это не равенство применить вот просто чтобы
[01:30:42.680 --> 01:30:47.840]  здесь более аккуратно получить и вот это вся движуха она просто позволит ну
[01:30:47.840 --> 01:30:51.080]  где-то вы с корреговоркой это все как бы мы проговорим а где-то прям будет
[01:30:51.080 --> 01:30:55.440]  честно если посмотреть но я так хочу сделать на курс в целом то в конечном
[01:30:55.440 --> 01:30:59.600]  итоге почти все основные такие математические трюки будут рассказаны в
[01:30:59.600 --> 01:31:03.920]  деталях вот и у вас не должно остаться так впечатление что а вот это мы
[01:31:03.920 --> 01:31:08.280]  проскочили ну будет почти все покрыто такая цель я не знаю в какой степени это
[01:31:08.280 --> 01:31:14.560]  удастся ну давайте действительно подумаем что у нас пока есть на данный
[01:31:14.560 --> 01:31:20.880]  момент у нас есть граф которым состояние которым есть состояние то есть давайте
[01:31:20.880 --> 01:31:29.880]  этот граф так и опишем то есть вершина графа это различные функции f1 f2 f3 и так
[01:31:29.880 --> 01:31:34.840]  далее ну какие-то функции это ничего хитрого я сейчас не говорю вообще все
[01:31:34.840 --> 01:31:40.560]  что сейчас буду говорить она настолько тривиально что как бы сказать это
[01:31:40.560 --> 01:31:44.760]  доступно школьникам даже не то что школьникам а школьникам не знаю 9 классы
[01:31:44.760 --> 01:31:51.960]  18 но если вот как бы потом это все посмотреть в совокупности то получится
[01:31:51.960 --> 01:31:55.480]  что блин а ведь это совершенно универсальная штука которая классно
[01:31:55.480 --> 01:31:59.400]  работает но математика элементарно ну во всяком случае чтобы это запустить
[01:31:59.400 --> 01:32:04.120]  чтобы это заработала понять почему это не элементарно а вот как бы чистое
[01:32:04.120 --> 01:32:07.880]  описание будет элементарно и вот не удивляйтесь что сейчас все очень просто
[01:32:07.880 --> 01:32:12.040]  в этом есть некий подвох вот это простота она она специально так сделано
[01:32:12.040 --> 01:32:16.600]  что вы сейчас почувствовали на самом деле все это конечно может работать куда
[01:32:16.600 --> 01:32:21.840]  как более сложных контекстах но вот я хочу вас предупредить что простота она
[01:32:21.840 --> 01:32:27.520]  как бы немножко здесь как сказать опасно опасно в том смысле что ну
[01:32:27.520 --> 01:32:31.560]  имейте в виду что на самом деле это очень крутые идеи и они очень классно
[01:32:31.560 --> 01:32:35.800]  работают и несмотря на простоту это реально очень мощная вещь и так что я
[01:32:35.800 --> 01:32:40.840]  сейчас делаю я сказал что есть всякие способы кодирования теперь мне ходит
[01:32:40.840 --> 01:32:46.040]  идея я хочу составить граф в котором каждой вершине отвечает свой способ
[01:32:46.040 --> 01:32:50.680]  кодирования дальше на этом графе я хочу ввести такое распределение
[01:32:50.680 --> 01:32:55.680]  вероятностей которая бы отвечала вот этой функции то есть стационарное
[01:32:55.680 --> 01:33:00.640]  распределение вероятностей пи от f было бы вот это ну с точностью до какого-нибудь
[01:33:00.640 --> 01:33:05.440]  номирующего множителя чтобы это было адекватное выражение то есть у меня
[01:33:05.440 --> 01:33:11.280]  экспоненциально много ну 40 факториал состояний эти состояния я как-то
[01:33:11.280 --> 01:33:15.480]  пронумеровал мне неважно что их я не могу описать нигде хранить мне это
[01:33:15.480 --> 01:33:19.320]  неважно мне неважно что я реально не могу задать эту функцию это все сейчас
[01:33:19.320 --> 01:33:24.600]  неважно важно то что я могу придумать какую-то реальную динамику блуждания
[01:33:24.600 --> 01:33:30.240]  человечка который блуждает по этому графу достаточно долго а потом он
[01:33:30.240 --> 01:33:32.960]  соответственно после какого-то количества итерации выходит на
[01:33:32.960 --> 01:33:38.680]  стационарное распределение которое дается вот этой формулой потому что я
[01:33:38.680 --> 01:33:45.520]  подбираю способ блуждания ровно так чтобы стационарное распределение было
[01:33:45.520 --> 01:33:52.200]  вот это вот и дальше я пользуюсь таким фактом то если это вот есть распределение
[01:33:52.200 --> 01:34:00.720]  вероятностей то проявляется эффект эффект большой выборки то есть если
[01:34:00.720 --> 01:34:07.640]  дополнительно вот здесь x который максимальный xn там я не знаю он большой
[01:34:07.640 --> 01:34:14.520]  там x4 и так далее xn большое вот если n большое большое то у меня приблизительно
[01:34:14.520 --> 01:34:19.520]  то же самое наблюдается что и здесь когда n большое то есть я выписываю
[01:34:19.520 --> 01:34:24.360]  некую вероятность видите здесь тоже вероятность я просто говорю что давайте
[01:34:24.360 --> 01:34:28.880]  найдем максимум этой вероятности я вообще ничего хитрого не сказал я сказал
[01:34:28.880 --> 01:34:34.120]  давайте найдем максимум этой вероятности и качество этого максимума у нас один на
[01:34:34.120 --> 01:34:39.640]  корень из n это уже неплохо то есть один на корень из n это n объем выборки здесь
[01:34:39.640 --> 01:34:46.280]  объем выборки число букв число букв это каждая буква дает новую какую то есть
[01:34:46.280 --> 01:34:53.320]  это как бы аналог аналог числа числа человечков число букв есть у меня текст
[01:34:53.320 --> 01:35:00.200]  достаточно большой но 10 4 это вполне себе текст это больше страницы то ну это там
[01:35:00.200 --> 01:35:05.520]  как раз где-то страница была по моему то это вполне есть вот аналог что я
[01:35:05.520 --> 01:35:10.960]  надеюсь к чему я отклоню что я надеюсь что просто максимум этого выражения это
[01:35:10.960 --> 01:35:15.520]  будет некоторая формула но если я идеально найду этот максимум которые
[01:35:15.520 --> 01:35:20.760]  а симпатически ну в нашем случае это симпатика выполняется как бы она будет
[01:35:20.760 --> 01:35:26.120]  близко к настоящему так сказать ну как сказать к настоящему значению к истинному
[01:35:26.120 --> 01:35:32.280]  f то есть если я верю в то что текст сгенерирован согласно вот такой модели
[01:35:32.280 --> 01:35:37.120]  то я реально получу с большой ну с близкую близкое значение вы конечно
[01:35:37.120 --> 01:35:42.040]  можете сказать что блин но это же не так никто из заключенных не знает
[01:35:42.040 --> 01:35:46.040]  марковские процессы во всяком случае скорее всего и не занимается тем что
[01:35:46.040 --> 01:35:49.600]  генерирует случайный текст согласно марковской модели генерации текста
[01:35:49.600 --> 01:35:54.160]  предварительно прочитав войну и мир и сделав частот на анализ но это же бред но
[01:35:54.160 --> 01:36:00.240]  это бред он во всем анализе данных то есть когда вы пытаетесь что-то значит
[01:36:00.240 --> 01:36:04.760]  определить вы занимаетесь тем что проектируете реальность на вашу картину
[01:36:04.760 --> 01:36:08.920]  этой реальности ваша картинная реальности обычно параметрическая или
[01:36:08.920 --> 01:36:13.320]  какая-то которая как-то ну вот как вы видите мир и вы в этой картине мира
[01:36:13.320 --> 01:36:18.560]  пытаетесь подобрать наилучшие ну как бы так такое такие параметры такое
[01:36:18.560 --> 01:36:23.040]  мировосприятие сформировать которая на вашей картине мира наилучшим образом
[01:36:23.040 --> 01:36:26.480]  этот мир описывает это очень естественно и собственно даже если
[01:36:26.480 --> 01:36:32.000]  эта модель так называемо misspecified то есть она не как бы не настоящая вы хотя
[01:36:32.000 --> 01:36:37.040]  бы найдете проекцию реальности на эту модель наилучшую проекцию реальности то
[01:36:37.040 --> 01:36:41.360]  есть я как бы сейчас говорю вещи которые может быть реальны но это важно
[01:36:41.360 --> 01:36:45.440]  понимать потому что это постоянно нам встречается в жизни в анализе данных то
[01:36:45.440 --> 01:36:49.800]  есть не надо буквально понимать что жизнь так устроена вы просто ищите
[01:36:49.800 --> 01:36:55.520]  проекцию жизни на эту картину мира и вот если объем выборки достаточно большой
[01:36:55.520 --> 01:36:59.880]  число слагаемых число сомножить или тут большое тот тот же самый эффект мы в
[01:36:59.880 --> 01:37:04.880]  праве ожидать по принципу максимум правдоподобия ну повторю что максимум
[01:37:04.880 --> 01:37:09.360]  правдоподобия выборка простая здесь выборка марковская то есть у вас текст
[01:37:09.360 --> 01:37:14.240]  генерируется марковским процессом здесь есть проблемы что как бы в плане
[01:37:14.240 --> 01:37:18.640]  независимости скорости сходимости может быть медленнее но есть книжка
[01:37:18.640 --> 01:37:22.920]  брагимова хасминского там все это как бы не так существенно то есть принцип
[01:37:22.920 --> 01:37:26.480]  максимум правдоподобия вы можете вообще брать стационарный процесс или там
[01:37:26.480 --> 01:37:30.560]  необязательно и то же самое то есть вот это вот результат о том что если
[01:37:30.560 --> 01:37:35.080]  траектория достаточно длинная не обязательно независимой реализации просто
[01:37:35.080 --> 01:37:40.280]  траектория наблюдения достаточно длинная то это дает возможность оценивать
[01:37:40.280 --> 01:37:43.960]  параметры мы об этом будем говорить когда будем на эргодичность смотреть с
[01:37:43.960 --> 01:37:48.200]  другой стороны это все будет то есть это я вам точно обещаю этого будет много
[01:37:48.200 --> 01:37:53.040]  многократно это будем проверять про то как датчик случайных чисел работает
[01:37:53.040 --> 01:37:56.680]  вообще про эргодичность теории динамических систем как это связано со
[01:37:56.680 --> 01:38:02.720]  стахасик это небольшой сюжет но очень яркие я думаю все это ну оценить и ну
[01:38:02.720 --> 01:38:07.240]  пока пока идем дальше и пока мы поясняем следующее что у нас есть
[01:38:07.240 --> 01:38:13.600]  значит состояние у их 40 факториал столько сколько символов и дальше нам
[01:38:13.600 --> 01:38:18.400]  надо ввести правила блуждания по этому графу давайте считать что самым
[01:38:18.400 --> 01:38:21.560]  естественным правилом является транспозиция потому что с помощью
[01:38:21.560 --> 01:38:25.800]  транспозиции можно по любую перестановку получить что такое
[01:38:26.800 --> 01:38:32.300]  транспозиция заключается в том что буква а кодируется значит соответственно
[01:38:32.300 --> 01:38:35.380]  этого человечка а букву z кодирует вот этого
[01:38:35.380 --> 01:38:40.220]  человечка да ну я условно так транспозиция заключается в том что вот
[01:38:40.220 --> 01:38:49.200]  так я делаю то есть преобразование заключается в том что ну давайте напишу
[01:38:49.200 --> 01:38:59.560]  значит здесь как бы не знаю как это красиво написать но давайте то есть то
[01:38:59.560 --> 01:39:06.440]  есть смотрите если у нас здесь допустим f2 оно значит xк переводит в
[01:39:06.440 --> 01:39:15.080]  значит вот такого человечка а xп переводит в вот такого человечка
[01:39:15.080 --> 01:39:20.840]  ну вот вот такого два разных человечка два разных человечка xк и x
[01:39:20.840 --> 01:39:28.640]  какой-то петой то я рисую здесь стрелку подразумеваю что вот здесь xк
[01:39:28.640 --> 01:39:38.320]  переходит вот в такого человечка а соответственно xп буквы xp ну согласно
[01:39:38.320 --> 01:39:43.080]  этой функции переходит вот в такого человечка все остальное одинаково ну и
[01:39:43.080 --> 01:39:47.640]  обратно то есть я считаю что есть и обратное преобразование то есть значит
[01:39:47.640 --> 01:39:54.720]  f2 от f3 отличается тем куда переходит буква xк xп я понятно сейчас говорю или
[01:39:54.720 --> 01:40:00.840]  не очень то то есть у меня правило такое что вершинка соединена с другой
[01:40:00.840 --> 01:40:05.920]  вершинкой если она отличается на одну транспозицию только сколько транспозиции
[01:40:05.920 --> 01:40:10.800]  столько соседей ну то есть не то чтобы их очень много то есть только сколько
[01:40:10.800 --> 01:40:21.560]  там пар букв час сейчас правильно но свое время следующий вопрос возникает
[01:40:21.560 --> 01:40:28.240]  такой но хорошо я вел граф по которому вполне реально блуждать так ведь ну
[01:40:28.240 --> 01:40:32.920]  давайте сначала это поймем реально блуждать по такому графу или нереально ну
[01:40:32.920 --> 01:40:37.640]  с точки зрения практики вы можете как бы бросить монетку если есть тут
[01:40:37.640 --> 01:40:43.680]  вероятности и соответственно сделать эту транспозицию ну в чем проблема давайте
[01:40:43.680 --> 01:40:47.720]  я сейчас сначала просто опишу процедуру а потом соответственно мы
[01:40:47.720 --> 01:40:51.680]  поймем почему она такая и вот не удивляетесь про статья процедура такая
[01:40:51.680 --> 01:40:57.480]  у меня я допустим нахожусь здесь у меня есть соседи я равновероятно выбираю
[01:40:57.480 --> 01:41:04.320]  любого соседа равновероятно соответственно выбираю соседа и дальше
[01:41:04.320 --> 01:41:11.560]  смотрю следующее вот значит он сюда вот значит тут какая-то перестановка там
[01:41:11.560 --> 01:41:18.360]  f не знаю 4 и не важно как я пронумеровал и дальше я смотрю следующее если пи от
[01:41:18.360 --> 01:41:25.720]  f4 больше чем пи ну даже да больше либо равно чем пи от f
[01:41:25.720 --> 01:41:31.480]  чего тут 2 тогда я что делаю так вам кажется что я должен делать ну из
[01:41:31.480 --> 01:41:36.360]  логики как бы сказать ну я надо должен там и остаться ну потому что это лучка
[01:41:36.360 --> 01:41:40.920]  я же максимум еще значит это с вероятностью единица то есть ну как бы
[01:41:40.920 --> 01:41:45.920]  получается не с вероятностью единицы а с вероятностью 1 на м то есть если пи от
[01:41:45.920 --> 01:41:51.800]  f4 больше чем пи от f2 то тут стоит 1 на столько сколько соседей м число
[01:41:51.800 --> 01:42:02.280]  соседей и сло соседей так ведь еще раз смотрите а что я в чем и в чем как бы
[01:42:02.280 --> 01:42:08.360]  вообще идея я хочу найти перестановку это вообще задача комбинаторной
[01:42:08.360 --> 01:42:12.800]  оптимизации то есть я хочу найти перестановку которая по принципу максимума
[01:42:12.800 --> 01:42:18.120]  правдоподобия дает максимум правдоподобию а то есть я могу выписать
[01:42:18.120 --> 01:42:24.560]  для любой для любого способа кодирования могу выписать вероятность один способ
[01:42:24.560 --> 01:42:29.360]  имеет одну вероятность другой способ имеет другую вот эта матрица мне дана то
[01:42:29.360 --> 01:42:33.840]  есть я сначала прочитал льва толстого и частотным образом определил как много
[01:42:33.840 --> 01:42:39.320]  пар а б как много в тексте толстого на английском языке раз встречается пара
[01:42:39.320 --> 01:42:45.240]  а б я взял число встречаемости а б поделил на общий число пар буб и нашел
[01:42:45.240 --> 01:42:50.720]  чистоту то же самое я сделал любой другой пары букв у меня появилась матрица
[01:42:50.720 --> 01:42:55.800]  теперь я эту матрицу подставляю в зависимости того как я выбираю функцию это
[01:42:55.800 --> 01:43:01.680]  как а ракуле и к скает человечки я значит беру ставлю вот для двух человечков
[01:43:01.680 --> 01:43:06.520]  ставлю в зависимости от f ну вот такую матрицу переходных вероятность если
[01:43:06.520 --> 01:43:12.000]  другая функция здесь другая вероятность будет но одна но по матрице п а б ну а
[01:43:12.000 --> 01:43:16.040]  б пробегают все буквы я могу восстановить матрицу отвечающую любой
[01:43:16.040 --> 01:43:21.800]  функции дальше по этой матрице отвечающий функции я могу просто взять
[01:43:21.800 --> 01:43:25.080]  произведение их и просчитать вероятность текста какова вероятность
[01:43:25.080 --> 01:43:30.920]  того что заключенный написал текст вот такой если функция кодирования f то есть
[01:43:30.920 --> 01:43:35.520]  если я понимаю его то сказать и человечка вот так то какая вероятность
[01:43:35.520 --> 01:43:41.760]  что это вот вот вот вот текстом такой написал а если функция другая то текст
[01:43:41.760 --> 01:43:45.480]  будет другой расшифровывается по другому вероятность будет другая я хочу
[01:43:45.480 --> 01:43:49.600]  найти такое и при котором это вероятность наибольшая по аналогии с
[01:43:49.600 --> 01:43:55.060]  тем как я хочу найти здесь такое п при котором вот эта вероятность наибольшая
[01:43:55.060 --> 01:44:01.200]  роль п роль п здесь играет функция f только здесь п единичка и мне вот это все
[01:44:01.200 --> 01:44:06.880]  как бы сказать тривиально тут все получается а вот здесь это огромное
[01:44:06.880 --> 01:44:13.120]  пространство состоянии 40 факториал перестановок и собственно чтобы найти
[01:44:13.120 --> 01:44:16.760]  максимальный f которая здесь доставит максимум что я хочу сделать я хочу
[01:44:16.760 --> 01:44:22.960]  придумать хочу придумать такое такие веса рёбер чтобы блуждая человечком по
[01:44:22.960 --> 01:44:30.760]  этому графу я через какое-то время равное mixing time вот это 1 альфа я был в
[01:44:30.760 --> 01:44:36.400]  той или иной вершине с вероятностью вот такой то есть еще раз f это и есть
[01:44:36.400 --> 01:44:41.740]  состояние то есть вероятность оказаться в состоянии f2 стационарном
[01:44:41.740 --> 01:44:47.520]  режиме на 8 тотики это есть вот то что я сюда ставлю f2 а вероятность оказаться
[01:44:47.520 --> 01:44:51.200]  в состоянии f4 но надо сделать транспозицию пересчитать эту
[01:44:51.200 --> 01:44:55.280]  вероятность но замещу что мне это реально не надо делать по всем вершинам
[01:44:55.280 --> 01:45:01.080]  это я как бы гипотетически говорю где я живу и блуждаю по факту я должен делать
[01:45:01.080 --> 01:45:06.040]  что я должен находясь в текущей точке старта которая получается частотным
[01:45:06.040 --> 01:45:10.760]  образом просто по частотам определяют стартово ф и дальше начинаю блуждаешь
[01:45:10.760 --> 01:45:15.160]  как я начинаю блуждать ищу равновероятно соседа что такое ищу равновероятно
[01:45:15.160 --> 01:45:20.760]  соседа я фиксирую случайно выбираю 1 букву например я хочу заменить ну что
[01:45:20.760 --> 01:45:25.080]  я фиксируя первую букву я например беру человечка этого ну и случайно
[01:45:25.080 --> 01:45:29.900]  выбираю второго человечка смотрю какая букву отвечает вот этому человечку и
[01:45:29.900 --> 01:45:33.860]  второму но их случайно выбрал и просто меняю местами перей moonsky и
[01:45:33.860 --> 01:45:37.860]  и, соответственно, перехожу туда, куда, что я выбрал.
[01:45:37.860 --> 01:45:40.860]  Столько, сколько соседей, столько, сколько транспозиций,
[01:45:40.860 --> 01:45:42.860]  столько, ну их будет сколько.
[01:45:42.860 --> 01:45:46.860]  Соответственно, первую букву я могу выбрать количество раз n-1,
[01:45:46.860 --> 01:45:50.860]  а вторую, ну сейчас, первую букву выбирается n раз,
[01:45:50.860 --> 01:45:53.860]  вторая n-1. Ну то есть, если у меня число букв n,
[01:45:53.860 --> 01:45:55.860]  то это приблизительно n в квадрате.
[01:45:55.860 --> 01:45:57.860]  То есть, это m, это приблизительно n в квадрате.
[01:45:57.860 --> 01:45:59.860]  Ну вот, я, это не очень много.
[01:46:00.860 --> 01:46:04.860]  Ну, это и важно, что все вероятности одинаковы.
[01:46:04.860 --> 01:46:06.860]  Да, то есть, эти вероятности одинаковы.
[01:46:06.860 --> 01:46:10.860]  Значит, я выбираю одну из соседей, дальше я проверяю.
[01:46:10.860 --> 01:46:13.860]  Если ты мой сосед, и ты лучше меня,
[01:46:13.860 --> 01:46:16.860]  то я принимаю твою, как бы сказать, сторону,
[01:46:16.860 --> 01:46:18.860]  и я такой же, как ты становлюсь.
[01:46:18.860 --> 01:46:21.860]  В смысле, я просто, я сюда перехожу точно.
[01:46:21.860 --> 01:46:25.860]  А вот если мой сосед, ну то есть, если это первый же режим,
[01:46:25.860 --> 01:46:28.860]  давайте это как-то четко напишем, чтобы это закрепилось.
[01:46:28.860 --> 01:46:31.860]  То есть, первый режим, первый режим, что
[01:46:31.860 --> 01:46:37.860]  p от f4, допустим, больше, чем p от f2.
[01:46:37.860 --> 01:46:42.860]  Тогда соответствующая вероятность перехода f2, f4,
[01:46:42.860 --> 01:46:45.860]  вероятность перехода, она что?
[01:46:45.860 --> 01:46:50.860]  Она равняется 1 на m, то есть, равняется 1 на n, на n-1.
[01:46:50.860 --> 01:46:57.860]  Вот. Ну, как бы понятно, что это можно сделать для любой вершины.
[01:46:57.860 --> 01:47:00.860]  И по факту нам не надо заранее эту матрицу хранить.
[01:47:00.860 --> 01:47:03.860]  Вы это можете по факту определить. То есть, вы сначала
[01:47:03.860 --> 01:47:06.860]  генерируете соседа, это делается, ну как бы, без всякой математики.
[01:47:06.860 --> 01:47:10.860]  Просто равновероятно выбираете две буквы, два человечка,
[01:47:10.860 --> 01:47:13.860]  меняете местами f, а потом уже проверяете это условие.
[01:47:13.860 --> 01:47:16.860]  Если это условие выполнено, то вы туда переходите.
[01:47:16.860 --> 01:47:19.860]  А вот если оно не выполнено, если оно не выполнено,
[01:47:19.860 --> 01:47:24.860]  то, то есть, как бы, переходим, переходим, переходим, переходим.
[01:47:24.860 --> 01:47:27.860]  Я так проще напишу. А вот если оно не выполнено, то есть,
[01:47:27.860 --> 01:47:32.860]  π от f4 будет, соответственно, меньше, чем, ну давайте,
[01:47:32.860 --> 01:47:38.860]  меньше либо равно, чем π от f2. Ну, тогда, тогда с вероятностью,
[01:47:38.860 --> 01:47:42.860]  соответственно, π от f4 на π от f2, я все равно перехожу.
[01:47:42.860 --> 01:47:46.860]  То есть, я разыгрываю, значит, монетку с вероятностью
[01:47:46.860 --> 01:47:51.860]  π от f4 на π от f2. Это число меньше либо равняется единице.
[01:47:51.860 --> 01:47:55.860]  И вот с такой, с такой вероятностью все равно перехожу,
[01:47:55.860 --> 01:47:59.860]  с такой вероятностью перехожу, перехожу.
[01:48:02.860 --> 01:48:05.860]  Еще раз, я гарантированно перехожу, если мне туда,
[01:48:05.860 --> 01:48:08.860]  как бы сказать, если там лучше, оно и нормально,
[01:48:08.860 --> 01:48:13.860]  там лучше, я туда сразу пошел. Но если там хуже, я как бы,
[01:48:13.860 --> 01:48:16.860]  так сказать, исследователь, мне интересно, а что будет дальше?
[01:48:16.860 --> 01:48:19.860]  Я все равно туда перейду, но с какой-то вероятностью.
[01:48:19.860 --> 01:48:24.860]  И эта вероятность тем меньше, чем там хуже. Логично, да?
[01:48:24.860 --> 01:48:27.860]  Но пока это количественно я не объяснил.
[01:48:27.860 --> 01:48:33.860]  Результат. Если я буду так блуждать, то по прошествии
[01:48:33.860 --> 01:48:37.860]  некоторого времени я действительно выйду вот на такое
[01:48:37.860 --> 01:48:39.860]  стационарное распределение. Это Марковская цепь.
[01:48:39.860 --> 01:48:42.860]  Это Марковская цепь для вот такого графа.
[01:48:42.860 --> 01:48:44.860]  Естественно, я блуждаю на таком графе, выйду на
[01:48:44.860 --> 01:48:47.860]  стационарное распределение, которое будет даваться,
[01:48:47.860 --> 01:48:51.860]  я сейчас это буду обосновывать, будет даваться таким распределением.
[01:48:51.860 --> 01:48:55.860]  Но это еще не все. Я не просто выйду на это распределение,
[01:48:55.860 --> 01:48:58.860]  а я как бы по факту, я же один человечек сейчас блуждаю,
[01:48:58.860 --> 01:49:01.860]  я в итоге окажусь в каком-то состоянии.
[01:49:01.860 --> 01:49:05.860]  Но дальше возникает вопрос, о каком состоянии я окажусь?
[01:49:05.860 --> 01:49:09.860]  Ведь я буду распределен согласно этой мере.
[01:49:09.860 --> 01:49:13.860]  В разных состояниях я окажусь разной вероятностью, но
[01:49:13.860 --> 01:49:17.860]  поскольку у меня есть наиболее вероятное состояние,
[01:49:17.860 --> 01:49:21.860]  и вокруг него концентрируется мера, это и есть предельная теорема.
[01:49:21.860 --> 01:49:24.860]  Только здесь это центральная предельная теорема для Марковских процессов.
[01:49:24.860 --> 01:49:27.860]  И там все будет замедлено в альфа раз, тот самый фактор.
[01:49:27.860 --> 01:49:32.860]  То есть придется ждать в альфа раз дольше, в один альфа раз дольше,
[01:49:32.860 --> 01:49:36.860]  в отличие от независимых испытаний. Но Марковская цепь тоже хороша тем,
[01:49:36.860 --> 01:49:40.860]  что она как бы в каком-то смысле независима, только на масштабе 1 альфа.
[01:49:40.860 --> 01:49:44.860]  И вот придется здесь подождать чуть подольше, но эффект тот же самый.
[01:49:44.860 --> 01:49:50.860]  В конечном итоге, что вот объем текста позволит сконцентрироваться
[01:49:50.860 --> 01:49:54.860]  вот так же, как здесь 1 на n, только здесь еще будет фактор стоять,
[01:49:54.860 --> 01:49:58.860]  который связан с марковостью процесса. Здесь-то все независимо.
[01:49:58.860 --> 01:50:03.860]  Ну и замечательно. То есть я не просто, еще раз, я не просто выйду на состояние,
[01:50:03.860 --> 01:50:09.860]  я еще окажусь в наиболее вероятном состоянии, но мне ровно это и надо.
[01:50:09.860 --> 01:50:13.860]  То есть, короче говоря, если я буду блуждать по этому графу,
[01:50:13.860 --> 01:50:18.860]  и в какой-то момент достаточно большой, я скажу, все, хватит.
[01:50:18.860 --> 01:50:21.860]  Я, естественно, могу находиться не в оптимальном состоянии.
[01:50:21.860 --> 01:50:24.860]  Звучит как-то немножко странно, но вы поняли.
[01:50:24.860 --> 01:50:28.860]  Так вот, я буду находиться не в оптимальном состоянии, но рядом с оптимальным.
[01:50:28.860 --> 01:50:32.860]  И это получается, процедура дает возможность найти максимум,
[01:50:32.860 --> 01:50:36.860]  вообще говоря, просто занимаясь тем, что я блуждаю по графу,
[01:50:36.860 --> 01:50:40.860]  только за счет того, что я подобрал динамику переходов так,
[01:50:40.860 --> 01:50:44.860]  что стационарная мера такая, какая я хотел.
[01:50:44.860 --> 01:50:47.860]  И возникает естественный вопрос. Это магия какая-то.
[01:50:47.860 --> 01:50:52.860]  Я угадал такую простую схему, которая дала мне возможность
[01:50:52.860 --> 01:50:58.860]  понаперед заданной стационарной мере найти матрицы переходных вероятностей,
[01:50:58.860 --> 01:51:01.860]  за счет которых реально можно организовать блуждание,
[01:51:01.860 --> 01:51:06.860]  не думая о том, сколько у нас там состояние, не думая о 30-40 факториал.
[01:51:06.860 --> 01:51:09.860]  Вот это меня не интересовало. Мне неважно, сколько всего вершин.
[01:51:09.860 --> 01:51:12.860]  Большинство из этих вершин абсолютно дурацкие.
[01:51:12.860 --> 01:51:17.860]  Я никогда там не окажусь. Я уже стартовал с нормальной позиции за счет частотного анализа,
[01:51:17.860 --> 01:51:21.860]  а потом еще и каждый раз в каком-то смысле улучшаю состояние,
[01:51:21.860 --> 01:51:25.860]  но если не улучшаю, то с небольшой вероятностью в целом стараюсь
[01:51:25.860 --> 01:51:27.860]  только на повышение ставок играть.
[01:51:27.860 --> 01:51:31.860]  И вот это блуждание так организовано, оно по прошествии какого-то количества шагов
[01:51:31.860 --> 01:51:34.860]  выйдет вот сюда и получится классно.
[01:51:34.860 --> 01:51:37.860]  Возникает вопрос, вообще как все это быстро происходит?
[01:51:37.860 --> 01:51:39.860]  Вот проверим интуицию.
[01:51:39.860 --> 01:51:43.860]  Первый вопрос такой, вот насчет карт.
[01:51:43.860 --> 01:51:46.860]  Вот у меня колоды из 52 карт.
[01:51:46.860 --> 01:51:50.860]  Какие есть способы тасования колодокарт, чтобы все карты были,
[01:51:50.860 --> 01:51:54.860]  чтобы все 52 факториал способов перестановки были равновероятны?
[01:51:54.860 --> 01:51:57.860]  52 это много? 52 факториал это много?
[01:51:57.860 --> 01:51:59.860]  Это очень много.
[01:51:59.860 --> 01:52:04.860]  Получается, что если у нас пространство состояния марковской цепи 52 факториал,
[01:52:04.860 --> 01:52:07.860]  то кажется, что ждать надо очень долго.
[01:52:07.860 --> 01:52:12.860]  Интуиция такая, чем больше граф, тем, соответственно,
[01:52:12.860 --> 01:52:15.860]  дольше надо ждать время выхода на вот...
[01:52:17.860 --> 01:52:19.860]  Да, пожалуйста.
[01:52:19.860 --> 01:52:24.860]  Ну вот конкретно...
[01:52:27.860 --> 01:52:29.860]  Подождите, подождите.
[01:52:29.860 --> 01:52:32.860]  Ну я же это использовал в том, что мы переходим именно из f2 в f4.
[01:52:32.860 --> 01:52:35.860]  Я обосную это, то есть не торопитесь, просто это все будет.
[01:52:35.860 --> 01:52:38.860]  Я это обосную, то есть мы поймем, что это общий рецепт.
[01:52:38.860 --> 01:52:40.860]  Что?
[01:52:40.860 --> 01:52:45.860]  Мне бы хотелось это рассказать не как частный случай, а как общая схема.
[01:52:45.860 --> 01:52:48.860]  Это будет следствием некой общей схемы,
[01:52:48.860 --> 01:52:51.860]  которая называется алгоритм метрополиса Хастингса.
[01:52:51.860 --> 01:52:54.860]  Или просто метрополиса, но это будет чуть позже.
[01:52:54.860 --> 01:52:56.860]  Да, пожалуйста.
[01:52:56.860 --> 01:53:00.860]  Смотрите, здесь как бы последовательность событий.
[01:53:00.860 --> 01:53:03.860]  Сначала я разыгрываю соседа.
[01:53:03.860 --> 01:53:07.860]  Это уже 1 на m, то есть вероятность того, что будет выбран конкретный сосед,
[01:53:07.860 --> 01:53:10.860]  это вероятность 1 на число соседей.
[01:53:10.860 --> 01:53:13.860]  Стало быть, я не соединиться и перехожу сюда,
[01:53:13.860 --> 01:53:18.860]  с вероятностью 1 на m при условии, что f4 больше, чем f2.
[01:53:18.860 --> 01:53:21.860]  p от f4 больше, чем p от f2.
[01:53:21.860 --> 01:53:25.860]  Мы можем для любых пар вершин сравнить и заранее все это прописать, если хочется.
[01:53:25.860 --> 01:53:28.860]  А с другой стороны, если p от f4 меньше, чем p от f2,
[01:53:28.860 --> 01:53:33.860]  то вот это 1 на m, оно в свою очередь шкалируется на фактор p от f4 на p от f2.
[01:53:33.860 --> 01:53:36.860]  Поскольку p от f заданная функция в каждой вершинке,
[01:53:36.860 --> 01:53:39.860]  то при желании мы можем весь этот граф записать явно.
[01:53:39.860 --> 01:53:42.860]  Но я имею в виду не граф, а переходные вероятности.
[01:53:42.860 --> 01:53:44.860]  В этом нет проблемы.
[01:53:44.860 --> 01:53:47.860]  Просто нам это не нужно, нам это нужно по факту здесь и сейчас.
[01:53:47.860 --> 01:53:50.860]  Поэтому нам разумно это каждый раз делать, как бы с нуля,
[01:53:50.860 --> 01:53:52.860]  а не хранить это где-то в памяти.
[01:53:52.860 --> 01:53:54.860]  Я не знаю, ответил на вопрос или нет.
[01:53:54.860 --> 01:53:56.860]  Да, пожалуйста.
[01:54:06.860 --> 01:54:08.860]  Нет, нет, нет.
[01:54:08.860 --> 01:54:14.860]  Я имел в виду именно, что f, отвечающая максимальному вот этому значению,
[01:54:14.860 --> 01:54:20.860]  оно будет близко к f, которое оптимально.
[01:54:20.860 --> 01:54:23.860]  Найденное f будет близко к оптимальному.
[01:54:23.860 --> 01:54:25.860]  То есть дело в том, что...
[01:54:25.860 --> 01:54:28.860]  Я понимаю, о чем вы говорите.
[01:54:28.860 --> 01:54:32.860]  Мы решаем задачу оптимизации.
[01:54:32.860 --> 01:54:35.860]  И как бы естественно, если задача вырожденная...
[01:54:35.860 --> 01:54:40.860]  Например, попробуйте на питоне градиентным методом отминемизировать х в тысячной степени.
[01:54:40.860 --> 01:54:42.860]  Вы не найдете точку ноль.
[01:54:42.860 --> 01:54:45.860]  Градиентный метод становится уже где-нибудь на 0,9.
[01:54:45.860 --> 01:54:50.860]  Почему? Потому что вырожденная функция очень сильно становится на отрезке минус 1,1.
[01:54:50.860 --> 01:54:53.860]  И вроде как понятно, что минимум в нуле.
[01:54:53.860 --> 01:54:58.860]  Но как бы заранее магическим образом не знать, что это за функция,
[01:54:58.860 --> 01:55:02.860]  а просто использовать градиентный оракул, то градиентный оракул видит ноль.
[01:55:02.860 --> 01:55:07.860]  И вот вы на огромном поле должны найти самую низкую, ну как бы сказать, ямку в поле.
[01:55:07.860 --> 01:55:09.860]  Но как вы эту ямку найдете в низинку?
[01:55:09.860 --> 01:55:14.860]  Если поле практически плоскость, а смотреть вы только под ноги, как я лекции читаю.
[01:55:14.860 --> 01:55:16.860]  То есть вы вот так вот все делаете.
[01:55:16.860 --> 01:55:19.860]  И вам надо найти точку нынешнюю в этой, так сказать...
[01:55:19.860 --> 01:55:20.860]  Как вы это сделаете?
[01:55:20.860 --> 01:55:21.860]  Надо все поле обойти.
[01:55:21.860 --> 01:55:24.860]  Ну хорошо, поскольку функция выпукла, вы будете идти по антиградиенту.
[01:55:24.860 --> 01:55:26.860]  Но это тоже очень долго.
[01:55:26.860 --> 01:55:29.860]  И не в одномерном случае там непонятно, насколько это может.
[01:55:29.860 --> 01:55:31.860]  Ну короче так, сколько угодно долго может быть.
[01:55:31.860 --> 01:55:34.860]  Поэтому о сходимости по аргументу речь идти не может.
[01:55:34.860 --> 01:55:39.860]  Но дело в том, что специфика задач, которые возникают при оценке максимума правдоподобия,
[01:55:39.860 --> 01:55:45.860]  она заключается в том, что так называемая матрица Фишера, то есть матрица вторых производных,
[01:55:45.860 --> 01:55:49.860]  которая характеризует вот этот функционал с точки зрения логарифма.
[01:55:49.860 --> 01:55:51.860]  То есть логарифма от этого функционала.
[01:55:51.860 --> 01:55:53.860]  То есть гисиан логарифма целевой функции.
[01:55:53.860 --> 01:55:56.860]  В минимуме, что естественно, или в максимуме.
[01:55:56.860 --> 01:56:00.860]  У нас как бы этот гисиан положить на определенный.
[01:56:00.860 --> 01:56:05.860]  То есть у нас как бы естественно должно быть, если нет выраженности в параметрах,
[01:56:05.860 --> 01:56:09.860]  что он отделим от нуля, это дает сильную выпуклость, это дает сходимость по аргументу.
[01:56:09.860 --> 01:56:13.860]  Но в каком-то смысле наименьшая константа сильной выпуклости,
[01:56:13.860 --> 01:56:17.860]  то есть там матрица обращается, и это будет входить вот в эти оценки.
[01:56:17.860 --> 01:56:20.860]  То есть та проблема, о которой вы говорите, она сидит вот здесь.
[01:56:20.860 --> 01:56:27.860]  То есть в принципе вот здесь могут быть какие-то тонкости,
[01:56:27.860 --> 01:56:30.860]  связанные с тем, что эта константа может быть достаточно большая.
[01:56:30.860 --> 01:56:32.860]  Но все равно это сходимость по аргументу.
[01:56:32.860 --> 01:56:36.860]  Это сходимость по аргументу, равно как и там сходимость по аргументу.
[01:56:36.860 --> 01:56:43.860]  Так, еще раз, расстояние перестановками, и что?
[01:56:43.860 --> 01:56:52.860]  Нет, ну понятно, но у метриках эминга используется стандартная метрика.
[01:56:52.860 --> 01:56:58.860]  То есть если я понимаю вопрос, то рядом это значит, что из там сколько позиций,
[01:56:58.860 --> 01:57:05.860]  из большого количества позиций, вот у вас будет отличие в одной-двух позициях.
[01:57:05.860 --> 01:57:07.860]  То есть это и наблюдалось на практике.
[01:57:07.860 --> 01:57:12.860]  То есть текст был правильно угадан за исключением двух-трех букв.
[01:57:12.860 --> 01:57:15.860]  То есть вы можете посмотреть статью Диакониса.
[01:57:15.860 --> 01:57:20.860]  То есть действительно там из сорока символов неугадно было там меньше пяти.
[01:57:20.860 --> 01:57:22.860]  Вот об этом идет речь, если речь о практике.
[01:57:22.860 --> 01:57:27.860]  Понятно, что на таком категорном языке говорить о epsilon, delta невозможно,
[01:57:27.860 --> 01:57:30.860]  потому что у нас и правда понятие окрестности.
[01:57:30.860 --> 01:57:37.860]  То есть надо тогда симпатически еще делать по числу символов, но это уже проблема.
[01:57:37.860 --> 01:57:40.860]  То есть да, я понимаю, о чем вы говорите, но...
[01:57:40.860 --> 01:57:44.860]  Еще раз, значит конкретно.
[01:57:45.860 --> 01:57:51.860]  Еще раз, давайте я все-таки закончу мысль с картами, и тогда это немножко прояснит ситуацию.
[01:57:51.860 --> 01:57:55.860]  Пример с картами, чем он примечательен? Я же так и не договорил мысли.
[01:57:55.860 --> 01:58:01.860]  Она простая, что для того, чтобы более-менее перемешать колоду с 52 карт тасованием,
[01:58:01.860 --> 01:58:05.860]  снять верхнюю карту и засунуть ее случайно равновероятно в колоду,
[01:58:05.860 --> 01:58:08.860]  то таких действий достаточно где-то 200-300.
[01:58:08.860 --> 01:58:13.860]  И с колодой 52 карты вы ее перемешаете с очень хорошей точностью.
[01:58:13.860 --> 01:58:16.860]  Будет равновероятно все варианты. 200-300 раз.
[01:58:16.860 --> 01:58:20.860]  А если вы делаете так, снимаете, привязите на половинку и вот так...
[01:58:20.860 --> 01:58:22.860]  Это 8 раз достаточно.
[01:58:22.860 --> 01:58:25.860]  Удивительно, да, то есть это очень быстро.
[01:58:25.860 --> 01:58:34.860]  И надо сказать, что вообще говоря, вот эти вот mixing-таймы, время выхода марковской цепи на час,
[01:58:34.860 --> 01:58:39.860]  это время, характерно, что-то похожее на диаметр графа.
[01:58:39.860 --> 01:58:47.860]  То есть сколько надо сделать итераций, чтобы из одной точки дойти до самой плохой другой точки.
[01:58:47.860 --> 01:58:53.860]  То есть вот у вас есть город, и вот это вот время перемешивания, оно, естественно, не может быть меньше,
[01:58:53.860 --> 01:58:56.860]  чем диаметр графа, это логично, правильно?
[01:58:56.860 --> 01:59:03.860]  Но с другой стороны, для многих раздумных сетей это просто приблизительно с точностью до логарифма равно диаметру.
[01:59:04.860 --> 01:59:11.860]  Ну, как бы в том-то и дело, что да, по диаметру мы, чтобы идти, это надо идти, прям узнать, куда идти.
[01:59:11.860 --> 01:59:15.860]  Но как часто бывает, что с точностью до логарифма это одно и то же.
[01:59:15.860 --> 01:59:19.860]  Ну да, оно и понятно.
[01:59:19.860 --> 01:59:21.860]  А что такое диаметр графа?
[01:59:21.860 --> 01:59:25.860]  Ну просто за счет транспозиции вы из одной перестановки можете получить другую перестановку.
[01:59:25.860 --> 01:59:29.860]  Значит вам надо сделать таких перестановок приблизительно столько, сколько символов.
[01:59:29.860 --> 01:59:37.860]  Ну и по факту чуть больше, конечно, было итерации, но это не очень аккуратно.
[01:59:37.860 --> 01:59:40.860]  Я могу привести пример графа, например, с пинсне.
[01:59:40.860 --> 01:59:44.860]  Вот возьмите здесь какая-то клика, мост и клика.
[01:59:44.860 --> 01:59:47.860]  Ну и такой пинсне очки получились.
[01:59:47.860 --> 01:59:53.860]  Вот если вот такое сделаете, ничего хорошего не будет, потому что пока вы, так сказать, здесь будете перемешиваться,
[01:59:53.860 --> 01:59:56.860]  здесь откажетесь, потом сюда скакнете, это может уйти.
[01:59:56.860 --> 02:00:03.860]  Можно так подобрать вероятности, что вероятность сюда попасть экспоненциально мала по времени, по числу вершин.
[02:00:03.860 --> 02:00:05.860]  То есть вы просто здесь будете гулять.
[02:00:05.860 --> 02:00:10.860]  Если вероятность здесь оказаться маленькая, вероятно, что вы здесь окажетесь, а потом еще сюда скакнете,
[02:00:10.860 --> 02:00:13.860]  ну вот вы сами себя загнали в такую ситуацию.
[02:00:13.860 --> 02:00:15.860]  Это специально я придумал.
[02:00:15.860 --> 02:00:22.860]  А на самом деле, если такая типичная ситуация, то диаметр и есть хорошая оценка того, почему тут все работает.
[02:00:22.860 --> 02:00:28.860]  Понимаете, это такая интуиция, которая реально на самом деле в большинстве жизненных ситуаций помогает.
[02:00:28.860 --> 02:00:34.860]  А вот вместо того, чтобы абстрактно себе мыслить, всякие айфы, еще вот, пожалуйста, диаметр, все сразу понятно становится.
[02:00:34.860 --> 02:00:36.860]  Ну мне, во всяком случае.
[02:00:36.860 --> 02:00:39.860]  Ну хорошо, давайте все-таки еще раз.
[02:00:39.860 --> 02:00:44.860]  Есть какие-то вопросы по тому, почему так все происходит, коллеги, не стесняйтесь.
[02:00:44.860 --> 02:00:47.860]  Мы никуда не торопимся, и я могу меньше просто...
[02:00:47.860 --> 02:00:54.860]  Все, переходим тогда к следующему вопросу.
[02:00:54.860 --> 02:01:06.860]  Как подобрать матрицу переходных вероятностей так, чтобы понаперед заданной мере p от f, давайте абстрактно.
[02:01:06.860 --> 02:01:15.860]  Я хочу понаперед заданной p от f подобрать переходной вероятности так в графе, который задан, чтобы у меня стационарное распределение было p от f.
[02:01:15.860 --> 02:01:21.860]  И тогда я пишу общую процедуру, которая совершенно аналогичным образом вы можете ее использовать.
[02:01:21.860 --> 02:01:26.860]  Если я не буду привязываться конкретно к этой специфике, я буду привязываться только к абстрактной формуле p от f.
[02:01:26.860 --> 02:01:29.860]  Давайте это сделаем.
[02:01:29.860 --> 02:01:34.860]  Это распределение на вершинах, совершенно верно.
[02:01:34.860 --> 02:01:41.860]  Еще раз, p от f это вероятность того, что марковский процесс, который вот связан с этим графом,
[02:01:41.860 --> 02:01:51.860]  и с какими-то переходными вероятностями будет в симптотике находиться ту-ту-ту-ту согласно вот этому распределению вероятости.
[02:01:51.860 --> 02:01:54.860]  Как подобрать эти переходные вероятности?
[02:01:54.860 --> 02:01:56.860]  То есть по сути вопрос формулируется так.
[02:01:56.860 --> 02:02:08.860]  Если задана p со звездой, если задана структура матрицы p, в которой не нулевые элементы стоят там, где стоят, то есть у нас вот такая структура в таком виде записана,
[02:02:08.860 --> 02:02:18.860]  то, соответственно, как заполнить пустоты?
[02:02:18.860 --> 02:02:23.860]  Желательно как-то конструктивно и просто, чтобы это было верно.
[02:02:23.860 --> 02:02:30.860]  То есть все, что мне дано, это p со звездой и, соответственно, есть места, куда надо что-то вставить.
[02:02:30.860 --> 02:02:36.860]  Ну, естественно, по правилам того, чтобы это была матрица переходных вероятностей.
[02:02:36.860 --> 02:02:38.860]  Ну, здесь работает...
[02:02:42.860 --> 02:02:44.860]  Нет, ну, точки это там, где больше нуля.
[02:02:44.860 --> 02:02:48.860]  Это любая вершина, которая здесь нарисована, потому что я везде перехожу, безусловно.
[02:02:48.860 --> 02:02:52.860]  Если я что-то рисую, это общий правило вообще в теории марксских цепей.
[02:02:52.860 --> 02:02:55.860]  Если нарисована стрелка, значит, перехода по ней больше нуля.
[02:02:55.860 --> 02:02:57.860]  Если она ноль, то она не рисуется.
[02:02:57.860 --> 02:03:06.860]  И от f4 вы не поверите, это вот что такое. f4, f4, f4, f4, f4.
[02:03:06.860 --> 02:03:09.860]  Я просто четверку дорисовал. Вот что такое p.
[02:03:09.860 --> 02:03:14.860]  Вот и все. f4 – это конкретная перестановка, и я ее конкретную здесь и пишу.
[02:03:14.860 --> 02:03:16.860]  Вообще ничего хитрого.
[02:03:16.860 --> 02:03:18.860]  В смысле, вот правда, то есть не ищите подвоха.
[02:03:18.860 --> 02:03:22.860]  То есть вы понимаете, что это какая-то интересная математика, но схема максимально простая.
[02:03:22.860 --> 02:03:24.860]  То есть подвоха здесь нет.
[02:03:24.860 --> 02:03:28.860]  Смотрите, я сделал эту затравочную матрицу просто для некой общности.
[02:03:28.860 --> 02:03:33.860]  Вообще я мог ее взять как-то, чтобы это была стахастическая матрица.
[02:03:33.860 --> 02:03:37.860]  Я еще могу вести p00i, это оставаться в этом состоянии.
[02:03:37.860 --> 02:03:40.860]  То есть –p0i житое сумма пожи.
[02:03:40.860 --> 02:03:42.860]  Это, повторюсь, затравочная матрица.
[02:03:42.860 --> 02:03:45.860]  Она мне, в общем, я могу ее по-разному выбирать.
[02:03:48.860 --> 02:03:50.860]  Нет, это какой-то конкретный способ.
[02:03:50.860 --> 02:03:53.860]  Можно другим способом просто затравочная матрица,
[02:03:53.860 --> 02:03:58.860]  которая, в моем случае она, на самом деле, поскольку у меня симметричная картинка,
[02:03:58.860 --> 02:04:02.860]  то в моем случае эта матрица можно ее брать один на число соседей.
[02:04:02.860 --> 02:04:05.860]  Она будет одинакова по и житое.
[02:04:05.860 --> 02:04:08.860]  Но во всех вершинках она будет одинакова.
[02:04:08.860 --> 02:04:11.860]  От и жи она зависеть не будет, но это в моем случае.
[02:04:11.860 --> 02:04:13.860]  Я повторю, что это в моем случае.
[02:04:13.860 --> 02:04:16.860]  А в общем случае она может как-то более хитро выбираться.
[02:04:16.860 --> 02:04:18.860]  Я сразу хотел бы в общем случае вам рассказывать.
[02:04:18.860 --> 02:04:19.860]  Идем дальше.
[02:04:19.860 --> 02:04:24.860]  А дальше я хочу подобрать такие b и житое.
[02:04:24.860 --> 02:04:27.860]  То b и житое на p.
[02:04:27.860 --> 02:04:30.860]  Если я возьму матрицу переходных вероятностей p и житое,
[02:04:30.860 --> 02:04:34.860]  вот с такой вот, так определенную,
[02:04:34.860 --> 02:04:39.860]  то хочу, чтобы у меня действительно была вот эта мера инварианта.
[02:04:39.860 --> 02:04:46.860]  Я сейчас, значит, немножко подсмотрю, значит,
[02:04:46.860 --> 02:04:51.860]  что для этого надо, значит, хорошо.
[02:04:51.860 --> 02:05:00.860]  Значит, я хочу подобрать b и житое так, чтобы вот эта мера p была стационарной.
[02:05:00.860 --> 02:05:07.860]  Какое достаточное условие я могу выписать, чтобы прямо сходу сказать,
[02:05:07.860 --> 02:05:14.860]  что решение вот этого уравнения p со звездой транспонированной равняется p со звездой транспонированной p,
[02:05:14.860 --> 02:05:18.860]  чтобы это действительно, вот p, я сюда пишу, чтобы это было верно.
[02:05:18.860 --> 02:05:20.860]  Какое достаточное условие?
[02:05:20.860 --> 02:05:25.860]  Оказывается, что по физике процессы очень простые условия.
[02:05:25.860 --> 02:05:29.860]  Если у меня p и житое на p, соответственно,
[02:05:29.860 --> 02:05:47.860]  если у меня п и житое равняется p и житое на p, то у меня как бы будет баланс.
[02:05:47.860 --> 02:05:52.860]  То есть я гарантированно имею так называемые условия детального баланса,
[02:05:52.860 --> 02:05:56.860]  и вот это будет выполняться автоматически.
[02:05:56.860 --> 02:05:58.860]  Как это я и не знаю.
[02:05:58.860 --> 02:06:01.860]  Как это я и не знаю, если это то, с чем мы пляшем.
[02:06:01.860 --> 02:06:06.860]  То есть мне не обязательно пытаться подобрать какое-то условие такое,
[02:06:06.860 --> 02:06:09.860]  что оно настолько общее, что прям это выполняется.
[02:06:09.860 --> 02:06:15.860]  Мне достаточно будет найти такое p и житое, чтобы выполнялось вот это условие, естественно, для всех и житое.
[02:06:15.860 --> 02:06:17.860]  Почему этого достаточно?
[02:06:17.860 --> 02:06:20.860]  Ну, по физике просто, если у меня...
[02:06:20.860 --> 02:06:24.860]  Вообще, что такое как бы стационарное распределение?
[02:06:24.860 --> 02:06:29.860]  Значит, что человечек, находясь в состоянии И, в стационаре,
[02:06:29.860 --> 02:06:34.860]  вероятность того, что он находясь в состоянии И, перейдет в состояние жи,
[02:06:34.860 --> 02:06:38.860]  на следующем шаге, это есть вот это вот.
[02:06:38.860 --> 02:06:40.860]  То есть как бы даже не так.
[02:06:40.860 --> 02:06:44.860]  Вероятность того, что он именно находится в состоянии Я, потом оказывается в жи, равняется вот этому.
[02:06:44.860 --> 02:06:46.860]  Но то же самое в обратную сторону.
[02:06:46.860 --> 02:06:51.860]  То есть если вы как бы предположите, что пиита это доля людей, которые находятся в состоянии И,
[02:06:51.860 --> 02:06:54.860]  а это можно так вот с человечками также интерпретировать,
[02:06:54.860 --> 02:06:56.860]  а p и жи в состоянии жи это как бы доля людей,
[02:06:56.860 --> 02:07:02.860]  то доля людей, которые перейдут из состояния И в состояние жи, будет вот такая.
[02:07:02.860 --> 02:07:06.860]  То есть сколько людей в единицу времени, поток людей из И в жи.
[02:07:06.860 --> 02:07:08.860]  Это вот такой поток.
[02:07:08.860 --> 02:07:11.860]  Но ему будет рай встречный поток из жи в И.
[02:07:11.860 --> 02:07:15.860]  То есть вы не просто как бы приравниваете поток, сколько уходит во все состояния,
[02:07:15.860 --> 02:07:17.860]  поток у сколько входит.
[02:07:17.860 --> 02:07:19.860]  Это как бы в общем случае.
[02:07:19.860 --> 02:07:23.860]  Я сейчас говорю, что у меня поток, уходящий в конкретную вершину, в любую другую,
[02:07:23.860 --> 02:07:27.860]  равен потоку, приходящему из нее в обратную сторону.
[02:07:27.860 --> 02:07:29.860]  Это частный случай равновесия.
[02:07:29.860 --> 02:07:33.860]  То есть это как бы не просто какая-то абстрактная такая вот...
[02:07:33.860 --> 02:07:41.860]  Я сузил класс, я сузил класс матриц на то, чтобы вот еще дополнительное условие детального баланса
[02:07:41.860 --> 02:07:43.860]  выполнялось, это вот Колмогоров.
[02:07:43.860 --> 02:07:45.860]  Условие детального баланса.
[02:07:45.860 --> 02:07:49.860]  В языке, конечно, оно хорошо известно, детального баланса.
[02:07:51.860 --> 02:07:55.860]  Нет, это не критерий, потому что существует много примеров, когда у вас вот это верно,
[02:07:55.860 --> 02:07:59.860]  а это неверно. Возьмите практически любой граф в системе PageRank, и это будет не так.
[02:07:59.860 --> 02:08:03.860]  Просто случайно нарисуйте граф, то есть это скорее исключение, чем правило.
[02:08:03.860 --> 02:08:05.860]  В типичной ситуации это не так.
[02:08:05.860 --> 02:08:11.860]  Но давайте теперь, как бы раз я сузил класс, раз я конструктивно описал под класс того, что мне надо,
[02:08:11.860 --> 02:08:15.860]  давайте попробуем подобрать B и JT так, чтобы это было верно.
[02:08:15.860 --> 02:08:17.860]  Что для этого надо?
[02:08:17.860 --> 02:08:23.860]  Для этого надо, чтобы P и JT на P и JT равнялось, ну, соответственно, чему?
[02:08:23.860 --> 02:08:29.860]  P значит A и JT на P и JT, правильно?
[02:08:29.860 --> 02:08:35.860]  Но у меня P и JT с этой затравочной матрицей, P и 0 и JT, поэтому я могу сразу говорить про B.
[02:08:35.860 --> 02:08:44.860]  B и JT на, соответственно, B и JT, ну, соответственно, равняется P и JT на P и JT.
[02:08:44.860 --> 02:08:56.860]  И мне надо подобрать набор вот этих вот функций B и JT, B и JT, так что это верно.
[02:08:56.860 --> 02:09:01.860]  Давайте в этой связи обозначим эту величину Z.
[02:09:01.860 --> 02:09:10.860]  И, соответственно, будем говорить, что B и JT, это есть некоторая функция F от Z.
[02:09:10.860 --> 02:09:16.860]  Ну, то есть это есть некая функция от P и JT на P и JT.
[02:09:16.860 --> 02:09:19.860]  Каким свойством должна эта функция удовлетворять?
[02:09:19.860 --> 02:09:21.860]  Я вообще ничего хитрого не делаю.
[02:09:21.860 --> 02:09:24.860]  То есть, на самом деле, это элементарная математика, я поставил себе цель.
[02:09:24.860 --> 02:09:30.860]  Давайте подберем такую, такие матрицы переходных вероятности, чтобы вот было вот это выполнялось.
[02:09:30.860 --> 02:09:34.860]  Как я это делаю? Дело десятое, но просто я максимально упрощаю себе жизнь.
[02:09:34.860 --> 02:09:41.860]  То есть стараюсь упростить сначала это условие, потом вот как-то, ну, сохраняя некую общность все-таки за счет выбора матрицы P.
[02:09:41.860 --> 02:09:44.860]  Но я сразу сказал, что она симметричная, она сокращается в этих выкладках.
[02:09:44.860 --> 02:09:47.860]  Поэтому вся как бы содержательная часть, она вот здесь.
[02:09:47.860 --> 02:09:50.860]  И теперь мне надо понять, какая функция F от Z, чтобы это было верно.
[02:09:50.860 --> 02:09:58.860]  Но здесь сразу получается без вариантов, что если у меня должно выполняться вот это соотношение, то у меня должно быть так.
[02:09:58.860 --> 02:10:04.860]  F от Z делить на F1 на Z, и это должно равняться Z.
[02:10:04.860 --> 02:10:05.860]  Вот так вот.
[02:10:05.860 --> 02:10:09.860]  Ну, потому что, вот, я, смотрите, определил B и JT как F от Z.
[02:10:09.860 --> 02:10:14.860]  Ну, значит, тогда B JT это P JT на P JT, а тогда будет P JT на P JT.
[02:10:14.860 --> 02:10:16.860]  Вот, 1 на Z.
[02:10:16.860 --> 02:10:20.860]  Вот, то есть, я надеюсь, это понятно, да, что я...
[02:10:20.860 --> 02:10:25.860]  Вот, а теперь давайте найдем какие-то конкретные примеры функций, которые это удовлетворяют.
[02:10:25.860 --> 02:10:28.860]  Жалко, я стер, это то, что не надо было стирать.
[02:10:28.860 --> 02:10:33.860]  Ну, например, F от Z равняется
[02:10:37.860 --> 02:10:41.860]  минимум из Z на 1.
[02:10:41.860 --> 02:10:45.860]  Сейчас я проверю, это будет выполняться.
[02:10:47.860 --> 02:10:51.860]  Да, это как раз Хастингс метрополис.
[02:10:51.860 --> 02:10:55.860]  Вот это есть функция Хастингс метрополис.
[02:10:55.860 --> 02:10:59.860]  Именно она использовалась метрополис нами в этом алгоритме.
[02:10:59.860 --> 02:11:05.860]  Ну, а если я возьму F от Z, значит, что там у нас еще какая-то функция?
[02:11:05.860 --> 02:11:07.860]  Значит, Z...
[02:11:08.860 --> 02:11:12.860]  Корень тоже, наверное, подходит.
[02:11:14.860 --> 02:11:17.860]  Корень, подождите, корень...
[02:11:25.860 --> 02:11:30.860]  С точки зрения практики, то есть, вот наиболее эффективна вот эта функция.
[02:11:30.860 --> 02:11:34.860]  Она потому что в каком-то смысле, в каком-то смысле...
[02:11:34.860 --> 02:11:39.860]  Она потому что в каком-то смысле мажорирует все остальные,
[02:11:39.860 --> 02:11:42.860]  и получается как бы наибольшая движуха.
[02:11:42.860 --> 02:11:45.860]  То есть, с этой функцией вы как бы наиболее активны,
[02:11:45.860 --> 02:11:48.860]  а значит, миксинг тайм наиболее, ну, наименьше.
[02:11:48.860 --> 02:11:53.860]  Поэтому, в общем, можно доказать, что все функции, которые вы будете подбирать, их много.
[02:11:53.860 --> 02:11:57.860]  Но они, во всяком случае, в теории уступают вот этой.
[02:11:57.860 --> 02:12:01.860]  Вот, и поэтому, как бы, в общем-то, выбирают в основном вот эту функцию.
[02:12:01.860 --> 02:12:05.860]  Ну, и теперь получается, что, что надо делать?
[02:12:05.860 --> 02:12:13.860]  Ну, надо сначала сделать пристрелочно, как бы, выбрать, ну, вообще, соседа в каком-то смысле.
[02:12:13.860 --> 02:12:18.860]  Когда вы выбрали соседа, ну, вот тут, в нашем случае, вы дальше уже принимаете решение.
[02:12:18.860 --> 02:12:24.860]  Если этот сосед, ну, лучше вас, по тому критерию, что вероятность у него лучше,
[02:12:24.860 --> 02:12:29.860]  вы прям туда и переходите, но переходите, как бы, не с вероятностью единица,
[02:12:29.860 --> 02:12:33.860]  не с вероятностью единицы, а вот с этой вероятностью, которая связана с тем,
[02:12:33.860 --> 02:12:36.860]  ну, сколько там в целом соседей.
[02:12:36.860 --> 02:12:41.860]  Ну, а если он хуже вас, то вы все равно берете отношение,
[02:12:41.860 --> 02:12:48.860]  ну, z это значит отношение вероятностей, сравниваете и тоже переходите с какой-то вероятностью.
[02:12:48.860 --> 02:12:53.860]  Обратите внимание, что мы, мы, как бы, сделали некий общий трюк,
[02:12:53.860 --> 02:12:58.860]  который, вообще говоря, работает всегда в разных контекстах,
[02:12:58.860 --> 02:13:03.860]  и вы можете так решать совершенно разные задачи, которые, ну, вот, на первый взгляд,
[02:13:03.860 --> 02:13:08.860]  вообще никак не, вот, еще раз, вообще, при чем здесь все, что я рассказывал?
[02:13:08.860 --> 02:13:12.860]  Ну, кроме того, что я и там рисовал человечков, и здесь я рисовал человечков,
[02:13:12.860 --> 02:13:15.860]  больше прямой аналогии нет, если так чисто визуально,
[02:13:15.860 --> 02:13:20.860]  потому что там были человечки, они блуждали по графу, а здесь человечки, это реально элементы шифра.
[02:13:21.860 --> 02:13:24.860]  Ну, на самом деле, больше и такой связи содержательной нет,
[02:13:24.860 --> 02:13:28.860]  а содержательная связь возникает на этапе принципа максимум правдоподобия,
[02:13:28.860 --> 02:13:31.860]  на этапе эргодической теоремы для марковских процессов
[02:13:31.860 --> 02:13:36.860]  и на этапе в каком-то смысле связи принципа максимум правдоподобия с центральной предельной теоремой,
[02:13:36.860 --> 02:13:41.860]  потому что оценка принципа максимум правдоподобия, это оценка, в которой как-то резюмируется,
[02:13:41.860 --> 02:13:46.860]  ну, то есть, это максимум, как бы, логарифма некой функции,
[02:13:46.860 --> 02:13:49.860]  логарифм этой функции, это же функция вероятность,
[02:13:49.860 --> 02:13:55.860]  эта вероятность составлена из произведений вероятности того, что каждый элемент выборки как-то выпадет.
[02:13:55.860 --> 02:13:58.860]  Когда возьмете логарифм за счет простоты выборки,
[02:13:58.860 --> 02:14:03.860]  у вас это будет логарифм произведения, потому что независимые исходы,
[02:14:03.860 --> 02:14:05.860]  ну, в случае марковского процесса посложнее,
[02:14:05.860 --> 02:14:08.860]  и у вас получится надо минимизировать сумму,
[02:14:08.860 --> 02:14:11.860]  а за счет того, что это сумма случайных независимых величин,
[02:14:11.860 --> 02:14:14.860]  у вас наблюдается явление концентрации меры,
[02:14:14.860 --> 02:14:17.860]  то есть, это не чудеса, чудес здесь нет,
[02:14:17.860 --> 02:14:21.860]  потому что, еще раз, когда вы максимизируете правдоподобие,
[02:14:21.860 --> 02:14:24.860]  а правдоподобие получается по принципу независимости,
[02:14:24.860 --> 02:14:28.860]  то есть, независимая разыгрывание,
[02:14:28.860 --> 02:14:31.860]  то максимум этой функции то же самое, что максимум логарифма,
[02:14:31.860 --> 02:14:36.860]  а логарифм от произведения равняется сумме логарифмов слагаемых,
[02:14:36.860 --> 02:14:39.860]  и уже эти случайные вот слагаемые независимые,
[02:14:39.860 --> 02:14:41.860]  вы изучаете поведение вот этой суммы,
[02:14:41.860 --> 02:14:44.860]  поэтому и есть концентрация, это же сумма независимых величин,
[02:14:44.860 --> 02:14:46.860]  с марковской цепью посложнее,
[02:14:46.860 --> 02:14:49.860]  там тоже сумма будет, естественно,
[02:14:49.860 --> 02:14:52.860]  а независимая, но дело в том, что,
[02:14:52.860 --> 02:14:55.860]  помните, я говорил, что можно взять частоту,
[02:14:55.860 --> 02:14:58.860]  то же самое там, частота пребывания человечка в той или иной вершинке,
[02:14:58.860 --> 02:15:01.860]  так вот, ровно потому, что частоты,
[02:15:01.860 --> 02:15:03.860]  если брать траекторию одного человека,
[02:15:03.860 --> 02:15:06.860]  как часто он бывал в той или иной вершинке,
[02:15:06.860 --> 02:15:10.860]  вот ровно потому, что это тоже сходится вот к этому распределению,
[02:15:10.860 --> 02:15:14.860]  сходится приблизительно так, как в законе больших чисел,
[02:15:14.860 --> 02:15:17.860]  просто медленнее, то и тут вот такой эффект проявляется,
[02:15:17.860 --> 02:15:22.860]  и вот мы поигрались на больших всяких вот таких эффектах,
[02:15:22.860 --> 02:15:24.860]  на эффектах больших чисел,
[02:15:24.860 --> 02:15:27.860]  энергодичность, то есть время к бесконечности,
[02:15:27.860 --> 02:15:30.860]  ну в нашем случае число итерации этого буждания,
[02:15:30.860 --> 02:15:34.860]  ну и вот то, что в нашем случае много букв,
[02:15:34.860 --> 02:15:37.860]  а там много агентов буждает,
[02:15:37.860 --> 02:15:39.860]  на этом мы дополнительно получили концентрацию,
[02:15:39.860 --> 02:15:42.860]  то есть не просто вышли на стационарную меру,
[02:15:42.860 --> 02:15:46.860]  но так еще и эта стационарная мера вокруг чего-то сконцентрировалась,
[02:15:46.860 --> 02:15:48.860]  теперь их, собственно, объявления,
[02:15:48.860 --> 02:15:50.860]  и они, мне кажется, достаточно важные,
[02:15:50.860 --> 02:15:53.860]  значит, наверняка у многих возникает вопрос,
[02:15:53.860 --> 02:15:54.860]  вообще, что будет происходить дальше,
[02:15:54.860 --> 02:15:55.860]  и вообще, что это такое,
[02:15:55.860 --> 02:15:58.860]  потому что это не формат в некотором смысле,
[02:15:58.860 --> 02:16:00.860]  значит, первая лекция была такая водная,
[02:16:00.860 --> 02:16:04.860]  я на ней постарался, ну грубо говоря, рассказать много чего,
[02:16:04.860 --> 02:16:07.860]  дальше я буду делать немножко по-другому,
[02:16:07.860 --> 02:16:10.860]  потому что мне важно было вас в каком смысле увлечь
[02:16:10.860 --> 02:16:13.860]  и показать спектр, некоторый спектр,
[02:16:13.860 --> 02:16:14.860]  ну вот теперь давайте вспомним,
[02:16:14.860 --> 02:16:16.860]  а что я не рассказал,
[02:16:16.860 --> 02:16:19.860]  ну я, например, вообще ничего толком не рассказывал
[02:16:19.860 --> 02:16:22.860]  про вот эти варианты CPT концентрации,
[02:16:22.860 --> 02:16:23.860]  ну то есть что они асимпатические,
[02:16:23.860 --> 02:16:25.860]  а нам надо не асимпатические,
[02:16:25.860 --> 02:16:27.860]  не рассказывал про, вообще говоря,
[02:16:27.860 --> 02:16:30.860]  что реально в PageRange там надо как бы смотреть,
[02:16:30.860 --> 02:16:31.860]  когда агентов много,
[02:16:31.860 --> 02:16:34.860]  как вообще эффект, что надо не просто
[02:16:35.860 --> 02:16:37.860]  а там мультинамиальную схему брать,
[02:16:37.860 --> 02:16:38.860]  может быть, это кажется мелочью,
[02:16:38.860 --> 02:16:39.860]  но ведь на самом деле,
[02:16:39.860 --> 02:16:41.860]  когда мы переходим к серьезным задачам,
[02:16:41.860 --> 02:16:43.860]  там же все это тоже всплывает,
[02:16:43.860 --> 02:16:45.860]  и хочется какую-то единую математику,
[02:16:45.860 --> 02:16:47.860]  которая полезна много где,
[02:16:47.860 --> 02:16:49.860]  в разных совершенно контекстах,
[02:16:49.860 --> 02:16:51.860]  ну и вот мы это будем, собственно, дальше закреплять,
[02:16:51.860 --> 02:16:53.860]  то есть какие-то не только это,
[02:16:53.860 --> 02:16:56.860]  как вот, значит, там какие-то не разница концентрации помогают жить,
[02:16:56.860 --> 02:16:59.860]  как они помогают жить там, например, в сток оптимизации,
[02:16:59.860 --> 02:17:03.860]  как они помогают там в каких-то матричных вопросах,
[02:17:03.860 --> 02:17:05.860]  как они помогают в моделировании,
[02:17:05.860 --> 02:17:07.860]  я не знаю, макросистем, например,
[02:17:07.860 --> 02:17:10.860]  того же самого, ну там не столько не разница концентрации,
[02:17:10.860 --> 02:17:12.860]  сколько эффекты больших чисел,
[02:17:12.860 --> 02:17:13.860]  но можно и концентрацию,
[02:17:13.860 --> 02:17:15.860]  как, например, неравенство Санова,
[02:17:15.860 --> 02:17:17.860]  такое классическое неравенство,
[02:17:17.860 --> 02:17:21.860]  вот оно помогает вообще и в теории информации,
[02:17:21.860 --> 02:17:24.860]  и одновременно в сток химкинетики,
[02:17:24.860 --> 02:17:28.860]  и вот, например, в изучении всяких там моделей,
[02:17:28.860 --> 02:17:30.860]  я не знаю, взаимодействия там,
[02:17:30.860 --> 02:17:32.860]  животных друг с другом, там людей,
[02:17:32.860 --> 02:17:35.860]  всякие обменные модели в статфизике,
[02:17:35.860 --> 02:17:38.860]  и очень много-много чего такого,
[02:17:38.860 --> 02:17:40.860]  будут разные техники рассказаны,
[02:17:40.860 --> 02:17:41.860]  геометрия будет рассказана,
[02:17:41.860 --> 02:17:44.860]  например, в следующий раз я постараюсь вас увлечь
[02:17:44.860 --> 02:17:46.860]  явлением концентрации меры на сфере
[02:17:46.860 --> 02:17:50.860]  применения к безградиентным методам оптимизации,
[02:17:50.860 --> 02:17:52.860]  то есть, вообще говоря, я вам расскажу то,
[02:17:52.860 --> 02:17:54.860]  чем Пуан Каре занимался в конце жизни,
[02:17:54.860 --> 02:17:58.860]  это результаты, связанные там из формулы Гауса,
[02:17:58.860 --> 02:18:02.860]  почему из Гауса формула нормальное распределение так часто возникает,
[02:18:02.860 --> 02:18:05.860]  не только из-за счет CPT, а за счет геометрии,
[02:18:05.860 --> 02:18:07.860]  там будет теория динамических систем,
[02:18:07.860 --> 02:18:09.860]  там будет очень много связей,
[02:18:09.860 --> 02:18:11.860]  которые, я надеюсь, покажут,
[02:18:11.860 --> 02:18:13.860]  что математика едина с одной стороны,
[02:18:13.860 --> 02:18:16.860]  а с другой стороны, вот эта математика,
[02:18:16.860 --> 02:18:18.860]  она сейчас и в Big Data,
[02:18:18.860 --> 02:18:20.860]  именно в машинном обучении еще популярна,
[02:18:20.860 --> 02:18:22.860]  это будет не сразу,
[02:18:22.860 --> 02:18:25.860]  и вот такими москами мы будем потихоньку, значит, идти,
[02:18:25.860 --> 02:18:28.860]  и в частности, то, что я сегодня не успел нормально рассказать,
[02:18:28.860 --> 02:18:30.860]  это про эргодические всякие теоремы,
[02:18:30.860 --> 02:18:32.860]  вот у нас будет приложение финансовой математики,
[02:18:32.860 --> 02:18:37.860]  мы будем, как вот реально оценивают всякие дрифты,
[02:18:37.860 --> 02:18:39.860]  ну не знаю, снос или как это,
[02:18:39.860 --> 02:18:43.860]  за счет эргодичности уже случайных процессов,
[02:18:43.860 --> 02:18:46.860]  как, почему возникает вот эта вот мартингальная вероятность,
[02:18:46.860 --> 02:18:50.860]  то есть мы будем пытаться вскрыть не просто Big Data,
[02:18:50.860 --> 02:18:54.860]  будем пытаться понять, почему в жизни возникают те или иные законы,
[02:18:54.860 --> 02:18:56.860]  какие-то степенные законы,
[02:18:56.860 --> 02:19:00.860]  и вообще, как бы сказать, что является первой причиной?
[02:19:00.860 --> 02:19:03.860]  Стиль, в котором это будет происходить, приблизительно такой,
[02:19:03.860 --> 02:19:05.860]  но я, наверное, буду меньше рассказывать,
[02:19:05.860 --> 02:19:08.860]  но буду больше сосредотачиваться на каких-то проработке деталей,
[02:19:08.860 --> 02:19:10.860]  приблизительно, как было в конце вот здесь.
[02:19:10.860 --> 02:19:13.860]  В частности, я очень надеюсь рассказать метод перевала,
[02:19:13.860 --> 02:19:16.860]  метод стационарной фазы, метод Ла Пласса,
[02:19:16.860 --> 02:19:18.860]  это асимпатические методы, анализы, например,
[02:19:18.860 --> 02:19:20.860]  вот как вывести формулу стирлинга,
[02:19:20.860 --> 02:19:22.860]  я почти уверен, что никто не знает,
[02:19:22.860 --> 02:19:23.860]  хотя это несложно.
[02:19:23.860 --> 02:19:28.860]  Я это сделаю в три строчки с помощью элементарной замены в интеграле.
[02:19:28.860 --> 02:19:30.860]  Эта замена многократно используется,
[02:19:30.860 --> 02:19:33.860]  это отчасти объясняет появление энтропии,
[02:19:33.860 --> 02:19:37.860]  потому что везде, где есть равноправие, там есть факториалы.
[02:19:37.860 --> 02:19:40.860]  Везде, где есть факториалы по формуле стирлинга,
[02:19:40.860 --> 02:19:43.860]  есть N делить на E в степени N.
[02:19:43.860 --> 02:19:45.860]  Логарифм от этого – это энтропия.
[02:19:45.860 --> 02:19:47.860]  Вот почему энтропия так привилегирована,
[02:19:47.860 --> 02:19:49.860]  почему она в жизни так часто возникает.
[02:19:49.860 --> 02:19:52.860]  Мы будем это изучать, она будет повсеместно встречаться,
[02:19:52.860 --> 02:19:54.860]  мы будем встречать разные законы расстреливания,
[02:19:54.860 --> 02:19:55.860]  не только нормальные,
[02:19:55.860 --> 02:20:00.860]  и будем пытаться понять, почему одно и то же многократно встречается.
[02:20:00.860 --> 02:20:02.860]  И вообще, насколько важна первозданная случайность,
[02:20:02.860 --> 02:20:07.860]  может быть, достаточно случайность получать как результат динамической системы.
[02:20:07.860 --> 02:20:11.860]  В общем, я очень надеюсь, что курс будет интересный.
[02:20:11.860 --> 02:20:14.860]  И, собственно, в какой-то момент, где-то через месяц,
[02:20:14.860 --> 02:20:17.860]  мы будем вам предлагать реально какие-то сюжеты,
[02:20:17.860 --> 02:20:20.860]  то вы можете взять, например, то, что я рассказываю,
[02:20:20.860 --> 02:20:24.860]  взять парочку статей и попробовать получить что-то новое.
[02:20:24.860 --> 02:20:26.860]  Это может быть связано с программированием,
[02:20:26.860 --> 02:20:28.860]  может быть, с математикой, что вам ближе.
[02:20:28.860 --> 02:20:30.860]  И приблизительно это можно, например, объяснить так.
[02:20:30.860 --> 02:20:33.860]  То есть вам реально дадим какой-нибудь такой жизненный пример.
[02:20:33.860 --> 02:20:35.860]  Например, тор, на нем животные.
[02:20:35.860 --> 02:20:37.860]  Ну да, не очень жизненный.
[02:20:37.860 --> 02:20:39.860]  Просто нам важно, чтобы лес был...
[02:20:39.860 --> 02:20:42.860]  Ну, нам важно, чтобы животные не уходили из леса.
[02:20:42.860 --> 02:20:44.860]  Его, значит, модель хищник-жертва.
[02:20:44.860 --> 02:20:46.860]  Представьте себе, что они реально блуждают по решетке,
[02:20:46.860 --> 02:20:49.860]  если хищник встречает жертву, он ее кушает.
[02:20:49.860 --> 02:20:52.860]  И вы то, что я вам буду доказывать, вы можете это проверить.
[02:20:52.860 --> 02:20:54.860]  Вы можете проверить, какие скелинги должны быть.
[02:20:54.860 --> 02:20:56.860]  Вы можете проверить mixing time.
[02:20:56.860 --> 02:20:58.860]  Это все довольно просто делается,
[02:20:58.860 --> 02:21:00.860]  но вы реально можете скрывать какие-то законы.
[02:21:00.860 --> 02:21:02.860]  То же самое можно сделать в перестановках.
[02:21:02.860 --> 02:21:05.860]  Ну, и это все как бы подкрепляется математикой.
[02:21:05.860 --> 02:21:08.860]  И мы будем... Вы можете сами вот эти макросистемы
[02:21:08.860 --> 02:21:11.860]  или какие-то большие статистики там,
[02:21:11.860 --> 02:21:12.860]  что вам будет ближе.
[02:21:12.860 --> 02:21:15.860]  И где-то к середине октября, я думаю, я уже окончательно
[02:21:15.860 --> 02:21:17.860]  сформирую набор задач.
[02:21:17.860 --> 02:21:19.860]  Более того, скорее всего, Максим Рахуба подготовит
[02:21:19.860 --> 02:21:21.860]  даже Юпитер ноутбук-презентации.
[02:21:21.860 --> 02:21:24.860]  Возможно, какие-то и вам уже по готовому коду,
[02:21:24.860 --> 02:21:27.860]  по там уже реализованному какому-то методу,
[02:21:27.860 --> 02:21:29.860]  связанному с кем-то может матричным,
[02:21:29.860 --> 02:21:32.860]  вы можете просто что-то доделать и получить новый результат.
[02:21:32.860 --> 02:21:34.860]  Я со своей стороны смогу такое представить
[02:21:34.860 --> 02:21:37.860]  по всяким задачам вокруг там многорувких бандитов.
[02:21:37.860 --> 02:21:40.860]  Это тоже будет всякие вот безградиентные методы.
[02:21:40.860 --> 02:21:44.860]  И сток... Ну, то есть такая современная сток-оптимизация.
[02:21:44.860 --> 02:21:47.860]  И там будет много сюжетов, где вы просто можете взять
[02:21:47.860 --> 02:21:49.860]  существующую статью, что-то доработать
[02:21:49.860 --> 02:21:50.860]  и получить новый результат.
[02:21:50.860 --> 02:21:51.860]  Все, вы курс дали.
[02:21:51.860 --> 02:21:53.860]  То есть если вы в состоянии разобраться
[02:21:53.860 --> 02:21:56.860]  с новым материалом там в виде двух статей,
[02:21:56.860 --> 02:21:58.860]  понять, как, например, поженить эти статьи,
[02:21:58.860 --> 02:22:00.860]  получить новый результат, доказать что-то,
[02:22:00.860 --> 02:22:02.860]  годится отлично все.
[02:22:02.860 --> 02:22:05.860]  Или вы в состоянии закодить то, что в статьях написано,
[02:22:05.860 --> 02:22:07.860]  проверить, открыть экспериментально
[02:22:07.860 --> 02:22:08.860]  какой-то новый закон.
[02:22:08.860 --> 02:22:10.860]  Я в конце приведу один пример,
[02:22:10.860 --> 02:22:12.860]  о котором мы рассказывали президенту.
[02:22:12.860 --> 02:22:13.860]  Совершенно случайно.
[02:22:13.860 --> 02:22:15.860]  И все, на этом закончу.
[02:22:15.860 --> 02:22:17.860]  Была программа школьников в Сириусе,
[02:22:17.860 --> 02:22:19.860]  там талантливый молодежь, привозят.
[02:22:19.860 --> 02:22:21.860]  И, значит, преподавателям что-то рассказывают.
[02:22:21.860 --> 02:22:23.860]  Это я первый раз поехал в 2016 году.
[02:22:23.860 --> 02:22:26.860]  Просто чтобы вы сейчас почувствовали,
[02:22:26.860 --> 02:22:29.860]  насколько все это рядом и какой проект можно сделать.
[02:22:29.860 --> 02:22:31.860]  Ну вот, значит, мы делали проект такой,
[02:22:31.860 --> 02:22:33.860]  что я рассказал школьникам вот этот пейджранк,
[02:22:33.860 --> 02:22:36.860]  а потом сказал, что, ребят, на самом деле,
[02:22:36.860 --> 02:22:38.860]  вот эта матрица переходных вероятностей,
[02:22:38.860 --> 02:22:40.860]  она хитро формируется,
[02:22:40.860 --> 02:22:42.860]  потому что интернет растет по принципу
[02:22:42.860 --> 02:22:43.860]  деньги к деньгам.
[02:22:43.860 --> 02:22:45.860]  Если у вас уже есть какой-то веб-граф
[02:22:45.860 --> 02:22:47.860]  и тут есть какое-то количество вершин,
[02:22:47.860 --> 02:22:49.860]  то, соответственно, новый появившийся сайт,
[02:22:49.860 --> 02:22:51.860]  он будет делать ссылки,
[02:22:51.860 --> 02:22:53.860]  ну, как бы сказать, наиболее активно
[02:22:53.860 --> 02:22:55.860]  на то, что и так цитируется.
[02:22:55.860 --> 02:22:57.860]  И вы видите модели роста.
[02:22:57.860 --> 02:22:59.860]  Тут возникают степенные законы вершин.
[02:22:59.860 --> 02:23:01.860]  Это можно, мы будем это изучать.
[02:23:01.860 --> 02:23:02.860]  Все это будет.
[02:23:02.860 --> 02:23:03.860]  Будут скелинги, теорема курса,
[02:23:03.860 --> 02:23:04.860]  будет очень красиво.
[02:23:04.860 --> 02:23:05.860]  Но это потом.
[02:23:05.860 --> 02:23:06.860]  Это все потом.
[02:23:06.860 --> 02:23:08.860]  Как бы школьникам я это рассказал,
[02:23:08.860 --> 02:23:10.860]  я сказал, что доказать степенной закон
[02:23:10.860 --> 02:23:12.860]  для вершин, как бы, ну, не изи,
[02:23:12.860 --> 02:23:14.860]  но я им буквально это доказал.
[02:23:14.860 --> 02:23:16.860]  И, соответственно, они как бы
[02:23:16.860 --> 02:23:17.860]  классно вообще.
[02:23:17.860 --> 02:23:18.860]  Вот тут макросистема,
[02:23:18.860 --> 02:23:20.860]  тут еще степенной закон.
[02:23:20.860 --> 02:23:21.860]  А дальше-то что?
[02:23:21.860 --> 02:23:23.860]  Степенной закон вершин есть,
[02:23:23.860 --> 02:23:25.860]  а можно ли то же самое сказать про PageRank?
[02:23:25.860 --> 02:23:27.860]  Потому что самое интересное,
[02:23:27.860 --> 02:23:29.860]  это степенной, ну, как бы, это хорошо,
[02:23:29.860 --> 02:23:30.860]  что у этого есть.
[02:23:30.860 --> 02:23:32.860]  Но верно ли, что если отранжировать
[02:23:32.860 --> 02:23:33.860]  компоненты вектора PageRank,
[02:23:33.860 --> 02:23:35.860]  то тоже будет степенной закон?
[02:23:35.860 --> 02:23:36.860]  Открытая задача.
[02:23:36.860 --> 02:23:38.860]  То есть для степеней вершин.
[02:23:38.860 --> 02:23:40.860]  Вот в такой модели Buckley-Откуса
[02:23:40.860 --> 02:23:42.860]  Preferential Attachment это можно доказать.
[02:23:42.860 --> 02:23:44.860]  Ну, это вот Андрей Михайлович любит Рогородский.
[02:23:44.860 --> 02:23:46.860]  А для PageRank нет.
[02:23:46.860 --> 02:23:48.860]  Я сказал, ребят, ну, чего, собственно,
[02:23:48.860 --> 02:23:50.860]  давайте, значит, проверим.
[02:23:50.860 --> 02:23:52.860]  То есть они реально брали какие-то фрагменты
[02:23:52.860 --> 02:23:54.860]  интернета, скачивали,
[02:23:54.860 --> 02:23:56.860]  они доказывали, они проверяли
[02:23:56.860 --> 02:23:58.860]  гипотезу, что если здесь есть
[02:23:58.860 --> 02:24:00.860]  степенной закон роста,
[02:24:00.860 --> 02:24:02.860]  а он есть, если модель роста графа
[02:24:02.860 --> 02:24:04.860]  Preferential Attachment, то она будет и здесь.
[02:24:04.860 --> 02:24:06.860]  То есть матрица P порождена,
[02:24:06.860 --> 02:24:08.860]  она не любая.
[02:24:08.860 --> 02:24:10.860]  Она порождена определенным законом роста.
[02:24:10.860 --> 02:24:12.860]  То есть у нас есть динамика, которая порождает
[02:24:12.860 --> 02:24:14.860]  матрицу P. Матрица P в итоге
[02:24:14.860 --> 02:24:16.860]  сформировалась. И она сформировалась
[02:24:16.860 --> 02:24:18.860]  тоже стахастическим образом, но
[02:24:18.860 --> 02:24:20.860]  согласно неким правилам. Деньги к деньгам.
[02:24:20.860 --> 02:24:22.860]  И, соответственно, они потом
[02:24:22.860 --> 02:24:24.860]  должны были исследовать численно,
[02:24:24.860 --> 02:24:26.860]  насколько вот вектор PageRank
[02:24:26.860 --> 02:24:28.860]  тоже имеет степенной закон, вообще говоря,
[02:24:28.860 --> 02:24:30.860]  с другим параметром. Что значит степенной закон?
[02:24:30.860 --> 02:24:32.860]  Отсортируйте компоненты вектора
[02:24:32.860 --> 02:24:34.860]  PageRank и нарисуйте их. То есть самая большая,
[02:24:34.860 --> 02:24:36.860]  следующая. И нарисуйте
[02:24:36.860 --> 02:24:38.860]  в масштабе lock-lock. В масштабе lock-lock
[02:24:38.860 --> 02:24:40.860]  если получается прямая, то наклон
[02:24:40.860 --> 02:24:42.860]  прямой отвечает... Вот они это проверяли
[02:24:42.860 --> 02:24:44.860]  на большом количестве
[02:24:44.860 --> 02:24:46.860]  для большого,
[02:24:46.860 --> 02:24:48.860]  для того, что известно,
[02:24:48.860 --> 02:24:50.860]  то, что доказано, это для индекса вершины.
[02:24:50.860 --> 02:24:52.860]  То есть вот для такого. А это не было
[02:24:52.860 --> 02:24:54.860]  известно. Вот они это проверили, установили
[02:24:54.860 --> 02:24:56.860]  закон и даже получили формулу аналитическую,
[02:24:56.860 --> 02:24:58.860]  потому что можно здесь
[02:24:58.860 --> 02:25:00.860]  менять параметры. И в зависимости
[02:25:00.860 --> 02:25:02.860]  от этих параметров меняются параметры
[02:25:02.860 --> 02:25:04.860]  вот здесь, и можно угадать какой-то закон.
[02:25:04.860 --> 02:25:06.860]  Вот есть что доказывать. Если бы они еще
[02:25:06.860 --> 02:25:08.860]  доказали, было бы совсем круто, но они
[02:25:08.860 --> 02:25:10.860]  тогда, 2016 год, чисто экспериментально
[02:25:10.860 --> 02:25:12.860]  это установили, и вот значит мы так
[02:25:12.860 --> 02:25:14.860]  уже заканчивается смена, сидим,
[02:25:14.860 --> 02:25:16.860]  все это обсуждаем, а я их там вот
[02:25:16.860 --> 02:25:18.860]  отучил деньги к деньгам. Ну значит приходит
[02:25:18.860 --> 02:25:20.860]  сначала, боюсь соврать вам песков,
[02:25:20.860 --> 02:25:22.860]  потом еще кто-то. Это потихоньку
[02:25:22.860 --> 02:25:24.860]  потом входит президент. Почему занимаетесь?
[02:25:24.860 --> 02:25:26.860]  А мы как бы вроде не планировали
[02:25:26.860 --> 02:25:28.860]  ничего такого, ну просто
[02:25:28.860 --> 02:25:30.860]  было обычное. Ну его школьники начали
[02:25:30.860 --> 02:25:32.860]  ему рассказывать деньги к деньгам,
[02:25:32.860 --> 02:25:34.860]  все такое.
[02:25:34.860 --> 02:25:36.860]  Да, да, да, да, деньги к деньгам, да.
[02:25:36.860 --> 02:25:38.860]  Вот так там,
[02:25:38.860 --> 02:25:40.860]  все такие кивают.
[02:25:40.860 --> 02:25:42.860]  Ну довольно
[02:25:42.860 --> 02:25:44.860]  интересно, ну и реально,
[02:25:44.860 --> 02:25:46.860]  насколько проект понравился, и
[02:25:46.860 --> 02:25:48.860]  потом вышла статья,
[02:25:48.860 --> 02:25:50.860]  в общем, вы можете найти ее
[02:25:50.860 --> 02:25:52.860]  в интернете, там, PageRank,
[02:25:52.860 --> 02:25:54.860]  и посмотрите, что там написано, как раз это
[02:25:54.860 --> 02:25:56.860]  вот такой типичный пример проекта.
[02:25:56.860 --> 02:25:58.860]  Можете по моей фамилии и, соответственно,
[02:25:58.860 --> 02:26:00.860]  кстати, в основном эти школьники пошли
[02:26:00.860 --> 02:26:02.860]  на ФКН. Моей руки тут
[02:26:02.860 --> 02:26:04.860]  нет. То есть, по-моему, половина из них
[02:26:04.860 --> 02:26:06.860]  сейчас на ФКН не учится. Я-то в основном
[02:26:06.860 --> 02:26:08.860]  на физ.тех агитирую, но вот так получились.
[02:26:08.860 --> 02:26:10.860]  Вот.
[02:26:10.860 --> 02:26:12.860]  Ну кого-то из них даже вот Варвара знает,
[02:26:12.860 --> 02:26:14.860]  это Лаунов,
[02:26:14.860 --> 02:26:16.860]  по-моему, и Сергей Ким.
[02:26:16.860 --> 02:26:18.860]  Может кого-то вы знаете. Ладно,
[02:26:18.860 --> 02:26:20.860]  сейчас не так важно.
[02:26:20.860 --> 02:26:22.860]  Значит,
[02:26:22.860 --> 02:26:24.860]  собственно, это все, вот это просто пример,
[02:26:24.860 --> 02:26:26.860]  что такого будет много,
[02:26:26.860 --> 02:26:28.860]  и отчасти вы можете управлять процессом,
[02:26:28.860 --> 02:26:30.860]  это удобно. То есть, в каком-то смысле
[02:26:30.860 --> 02:26:32.860]  это не просто фильм, а это как бы
[02:26:32.860 --> 02:26:34.860]  вы можете кликать пультом и говорить
[02:26:34.860 --> 02:26:36.860]  замедлитесь, ускорьтесь, а да расскажите
[02:26:36.860 --> 02:26:38.860]  вот это, и я могу это корректировать.
[02:26:38.860 --> 02:26:40.860]  Пока, во всяком случае, время есть,
[02:26:40.860 --> 02:26:42.860]  и, в общем, жду обратную связь.
[02:26:42.860 --> 02:26:44.860]  Постараюсь со временем
[02:26:44.860 --> 02:26:46.860]  рассказывать более строго,
[02:26:46.860 --> 02:26:48.860]  потому что сегодня была такая вводная
[02:26:48.860 --> 02:26:50.860]  лекция.
[02:26:50.860 --> 02:26:52.860]  Так, спасибо.

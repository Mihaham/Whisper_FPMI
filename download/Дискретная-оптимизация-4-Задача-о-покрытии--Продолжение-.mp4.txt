[00:00.000 --> 00:12.240]  Разбираемся с общей задачей покрытия. Давайте мы вспомним про вершинное покрытие быстро.
[00:12.240 --> 00:18.960]  Мы с вами там что делали? Мы говорили, что у графа должны быть покрыты все ребра за счет того,
[00:18.960 --> 00:24.240]  что мы берем только некоторые вершины. И мы говорили, что если вершина взята,
[00:24.240 --> 00:32.360]  то мы соответствующую переменную выставляем в единицу, иначе мы выставляем в ноль. Вот. И
[00:32.360 --> 00:42.160]  тогда мы хотим, чтобы для каждого ребра x у плюс x в было больше или равно единице. Ну вот у нас
[00:42.160 --> 00:49.600]  такая с вами была, значит, такая программа, линейная программа, да, потому что мы ставили
[00:49.600 --> 01:00.400]  это как задачу линейного программирования, и мы минимизировали сумму w витых на x витых. Вот.
[01:00.400 --> 01:05.800]  Это задача о вершинном покрытии. Вот. Сегодня мы с вами ведем задачу, такую общую задачу о
[01:05.800 --> 01:10.520]  покрытии, и увидим очень быстро, что задача о вершинном покрытии, это только такой очень
[01:10.520 --> 01:17.080]  специальный частный случай, и про общую задачу с вами тоже поговорим. Вот. Значит, общая задача
[01:17.080 --> 01:25.520]  о покрытии. Ну давайте я ее сюда, я думаю, вмещу. Она называется set cover или просто cover.
[01:25.520 --> 01:39.240]  В этой задачке у нас есть уже теперь не вершины ребра графа, а просто какие-то абстрактные
[01:39.240 --> 01:48.840]  покрываемые объекты, сущности, да, и покрывающие. Я, по-моему, на первой встрече наши с вами приводил
[01:48.840 --> 01:55.360]  пример, например, когда мы хотим объявлениями какими-то, да, рекламой покрыть целевую аудиторию,
[01:55.360 --> 02:01.800]  или покрыть, значит, какими-то сотрудниками множество тех скиллов, которые необходимы для
[02:01.800 --> 02:09.920]  реализации какого-то проекта. Вот. Или мы хотим покрыть, например, какими-то магазинами, или там
[02:09.920 --> 02:15.040]  школами, или детскими садами, мы хотим покрыть инфраструктурно какой-то микрорайон, то есть мы
[02:15.040 --> 02:20.440]  думаем там, где именно размещать там школу, детский сад, и так далее. В общем, задача о покрытии
[02:20.440 --> 02:26.720]  может быть очень много, они все очень полезны. И мы с вами все такие задачи будем абстрактным
[02:26.720 --> 02:34.400]  образом математически описывать так. Мы будем говорить, что у нас есть матрица, у которой строчки,
[02:34.400 --> 02:46.320]  ну давайте строчки, да, значит, у которой строчки это покрывающие объекты, а столбцы покрываемые,
[02:46.320 --> 02:56.320]  то есть вот я здесь напишу, что столбцы соответствуют покрываемым, покрываемые. А строчки
[02:56.320 --> 03:12.200]  соответствуют объектам покрывающим. Покрывающие. И на пересечении строки из столбца у нас будет
[03:12.200 --> 03:21.720]  ставится единица, если соответствующий объект, покрывающий, он действительно охватывает вот
[03:21.720 --> 03:30.240]  этот вот покрываемый объект. Я думаю, что вы, если графы, в принципе, проходили, то вы проходили
[03:30.240 --> 03:36.760]  разные способы задавать граф. И в числе таких способов вы упоминали обычно матрицу инциденций,
[03:36.760 --> 03:45.000]  или матрица инцидентности, или матрица инцидентностей. Матрица смежности, а если ты и та и та матрица,
[03:45.000 --> 03:50.120]  просто есть на графе два отношения, инцидентности и смежности. Помните, чем они отличаются?
[03:50.120 --> 04:00.080]  Ну, можно сказать так. Я бы сказал, смежность – это отношение между объектами одного типа,
[04:00.080 --> 04:05.160]  то есть, например, две вершины смежны, если между ними ребро есть. Два ребра тоже можно назвать
[04:05.160 --> 04:10.040]  смежными, если у них есть общий конец. А инцидентность – это отношение между объектами
[04:10.040 --> 04:16.440]  разных типов. Вершина и ребро являются инцидентными, если вершина является концом этого ребра. Вот у
[04:16.480 --> 04:21.480]  нас есть матрица смежности, которая кодирует граф, есть матрица инцидентности, которая тоже способна
[04:21.480 --> 04:29.160]  закодировать этот граф. И в матрице инцидентности, ну, ее можно по-разному вращать, но давайте посчитаем,
[04:29.160 --> 04:35.600]  что в матрице инцидентности строчки соответствуют вершинам, а столбцы соответствуют ребрам. И смотрите,
[04:35.600 --> 04:41.840]  как получается. Получается, что как раз покрывающие объекты – это у нас вершины, а столбцы – это
[04:41.840 --> 04:47.400]  покрываемые – это ребра. И вот вершинами покрывать ребра в графе – это то же самое,
[04:47.400 --> 04:53.840]  что в матрице инциденции этого графа строчками этой матрицы покрывать столбцы. То есть теперь,
[04:53.840 --> 04:59.680]  когда мы с вами переходим в эту общую постановку, что нам нужно сделать? Нам нужно выбрать такое
[04:59.680 --> 05:05.720]  наименьшее либо по мощности, либо по весу под множество строчек этой матрицы, чтобы покрыть все
[05:05.720 --> 05:10.920]  столбцы. А что значит покрыть все столбцы? Это значит, чтобы в каждом столбце была хоть одна
[05:10.920 --> 05:18.240]  единичка на пересечении с выбранным под множеством строк. Вот такая у нас с вами будет задача –
[05:18.240 --> 05:34.560]  выбрать множество строк минимального веса, ну или минимальной мощности, покрывающие все столбцы.
[05:34.560 --> 05:49.680]  Покрывающие все столбцы. Ну вот, ну давайте мы сформулируем эту задачу, как всегда,
[05:49.680 --> 05:55.960]  как задачу линейного программирования, посмотрим, какие у нее есть вариации и какие у нее отличия
[05:55.960 --> 06:03.000]  вот этой вот очень частной задачки. Понятно, что в каждой строке матрицы мы способны сопоставить
[06:03.480 --> 06:11.080]  переменную выбора decision variable. Берем мы эту строчку в покрытие или не берем? Ну давайте считать,
[06:11.080 --> 06:20.680]  что у нас m-строчек. Вот, и тогда линейно-программистская формулировка для задачи о покрытии целочисленная.
[06:20.680 --> 06:37.960]  Как она будет выглядеть? Значит, мы минимизируем, ну как всегда, в случае с вершинным покрытием,
[06:37.960 --> 06:52.720]  абсолютно то же самое функция, сумма w i t x i t по i от единицы до m. Ну да, вроде так. Значит,
[06:52.720 --> 06:59.160]  сумма весов всех выбранных строчек. Как раз получается такая. Переменные у нас целочисленные,
[06:59.160 --> 07:11.760]  и от нуля до единицы, конечно, тоже все. И как мы будем с вами условия покрываемости записывать?
[07:11.760 --> 07:18.760]  Ну, видимо, раньше у нас концы ребра, переменные соответствующие концам ребра в сумме должны
[07:18.760 --> 07:24.720]  были давать не меньше единицы что-то здесь. Мы смотрим, а где в столбце стоят единички,
[07:24.800 --> 07:31.880]  и соответствующие переменные суммируем и говорим, что сумма этих переменных должна быть как минимум
[07:31.880 --> 07:38.520]  единичка. Чтобы хотя бы одна из строк, которая была способна покрыть этот столбец, была взята. Но
[07:38.520 --> 07:45.520]  если мы будем считать, что в матрице у нас n столбцов, то тогда для каждого n, для каждого g виноват.
[07:45.520 --> 07:56.640]  Вот единица до n. Мы с вами потребуем, чтобы сумма иксов каких-то, сейчас с вами посмотрим каких,
[07:56.640 --> 08:05.360]  была больше или равна единице. Вообще, если эту матрицу мы с вами обозначим матрицей A, например.
[08:05.360 --> 08:13.760]  Да, то могу, конечно. Да то же самое, что сейчас я просто не подписал еще, что у нас снизу суммы,
[08:13.880 --> 08:19.960]  по какому множеству мы суммируем. Вот смотрите, в вершинном покрытии вот такое было неравенство,
[08:19.960 --> 08:27.880]  потому что мы хотели, чтобы для каждого ребра хотя бы один из его концов был взят. Вот хотя
[08:27.880 --> 08:36.160]  бы один из концов ребра был взят. Здесь мы то же самое, что практически хотим, только у нас не такая
[08:36.160 --> 08:41.840]  задачка ограниченная. Вот в матрице инциденции у нас в каждом столбце было бы ровно по две единички,
[08:41.840 --> 08:47.960]  и поэтому здесь возникает слева сумма двух иксов. А так вообще, если в столбце три единички,
[08:47.960 --> 08:53.000]  то здесь будет сумма трех иксов больше или равна единице. Такое неравенство, потому что мы
[08:53.000 --> 08:58.320]  кодируем как бы, что хотя бы одна из строчек, которая покрывает этот столбец, должна войти в
[08:58.320 --> 09:04.280]  покрытие. Вот это у нас было, хотя бы одна из вершин, которая способна покрыть это ребро,
[09:04.280 --> 09:10.600]  должна войти в покрытие. Здесь то же самое. Вот, только надо бы нам как-то компактно это записать,
[09:10.600 --> 09:16.120]  но если это матрица, вот эту матрицу, которая кодирует нам задачу вообще, обозначить через А,
[09:16.120 --> 09:26.640]  и ее элементы обозначить А и Т житые, то тогда мы с вами можем записать здесь вот так. Сумма тех
[09:26.640 --> 09:37.120]  иксов, для которых А и Т житые равняется единице. То есть как раз сумма по тем рочкам, которые
[09:37.120 --> 09:44.160]  способны покрыть нам житый столбец. Но можем мы вот это как-то записать по-другому? Видимо можем,
[09:44.160 --> 09:51.440]  поскольку ашки сами это нолики единицы, то вместо того, чтобы писать здесь какую-то сумму по
[09:51.440 --> 09:58.760]  непонятному множеству ашек, ну то есть оно понятное, но оно какое-то... Где тряпка? Это
[09:58.760 --> 10:07.240]  действительно непонятно, это гораздо менее тривиально. О, спасибо. Найти тряпку не менее
[10:07.240 --> 10:14.680]  важно, чем найти решение патрудной задачи иногда. Вот, значит мы можем ту же самую сумму
[10:14.680 --> 10:22.040]  выписать по-другому. Просто сумма х и на А и Т житые и просуммировать уже по всем и. То есть
[10:22.040 --> 10:31.120]  сумма А и Т житые помножить на х и. Сумма по и единичке до н. То есть там, где ашка равна нулю,
[10:31.120 --> 10:37.120]  там слагаемая в сумме автоматически пропадает. Там, где ашка равна единице, мы получаем просто
[10:37.120 --> 10:44.720]  х и ты участвуют в этой сумме. То есть в принципе то, что было написано вот здесь внизу, это
[10:44.720 --> 11:06.920]  эквивалентно вот такой вот сумме формально. М на н. Да, спасибо, да, здесь м. Это хорошее
[11:06.920 --> 11:12.480]  замечание. А то у меня бы сейчас доска вызвала недопустимую операцию и была бы закрыта за
[11:12.480 --> 11:21.720]  переполнение индексов в массиве. Да нет, теперь это вообще произвольная матрица из 0 единиц,
[11:21.720 --> 11:27.080]  которая нам кодирует задачу. Да, то есть я просто пытаюсь вам все время привести параллель как бы
[11:27.080 --> 11:35.320]  с уже известной нам задачей вершинного покрытия, но сейчас это уже произвольная матрица. Значит да,
[11:35.320 --> 11:44.600]  кстати, давайте заметим, что вот от такой задачки CLP не трудно перейти к задаче CLP для каких-то
[11:44.600 --> 11:51.880]  задач типа покрытия шаг влево, шаг вправо. Вот, например, не всегда нам нужно покрыть прям все-все-все
[11:51.880 --> 11:59.160]  столбцы. Давайте предположим, что нам нужно покрыть только 90 процентов, например, столбцов. Вот,
[11:59.160 --> 12:07.480]  давайте пример такой разберем. Пример, как моделировать, когда нужно покрыть только 90
[12:07.480 --> 12:25.760]  процентов столбцов. Значит требуется покрыть не менее 90 процентов столбцов, например. Вот,
[12:25.760 --> 12:35.000]  в CLP постановке это вполне можно обеспечить небольшой модификацией. Давайте мы введем еще
[12:35.000 --> 12:40.440]  одни переменные. Это уже не будут наши переменные выбора непосредственно, какие строчки мы выбираем,
[12:40.440 --> 12:47.240]  это будут такие вспомогательные переменные. Давайте для каждого столбца введем переменную
[12:47.240 --> 12:56.160]  YG, которая будет означать, что этот столбец покрыт. Вот YG по смыслу это единичка, если покрываем
[12:56.160 --> 13:08.360]  столбец, покрываем житый столбец и ноль иначе. Тогда как можно вот это неравенство преобразовать?
[13:08.360 --> 13:14.480]  Если нам столбец надо покрыть, то тогда вот такое неравенство должно быть выполнено. Если столбец
[13:14.480 --> 13:19.960]  не обязательно покрывать, то тогда это неравенство просто как бы нужно исключить эффективно. Как нам
[13:19.960 --> 13:29.960]  его видоизменить тогда, если есть вот такие переменные YG? А или вот нельзя формально добавлять
[13:29.960 --> 13:38.160]  задачу CLP? А минимум тоже так просто нельзя добавить. То есть у нас, смотрите, у нас должна быть
[13:38.160 --> 13:44.520]  задача, система такая большая, никаких совокупностей. Система с фиксированным
[13:44.520 --> 13:50.480]  числом неравенства с фиксированными коэффициентами, да, с константными. Что же делать?
[13:50.480 --> 14:01.440]  А если бы мы здесь нолик поставили вот в этом неравенстве, вот такое неравенство всегда
[14:01.440 --> 14:07.280]  выполняется? Нет, то есть оно вот такое неравенство по факту не является ограничением, да, это такое
[14:07.280 --> 14:13.280]  чисто эффективное ограничение. А вот это правда ограничение. Не для всех наборов X-ов такое
[14:13.280 --> 14:17.040]  неравенство выполнится, да, только для некоторых. Так что же нам делать? Как его преобразовать так,
[14:17.040 --> 14:24.800]  чтобы вот эту переменную сюда как бы вставить? Мы как раз можем поставить вместо единички в правой
[14:24.800 --> 14:33.720]  части ровно вот эту переменную. То есть написать сумма A и G X и по всем и от единицы до m больше или
[14:33.720 --> 14:43.920]  равна YG. И это мы сделаем для всех G, да, то есть мы теперь требуем, чтобы для всех G вот единицы до m было
[14:43.920 --> 14:49.440]  выполнено вот такое неравенство. А это все равно по-прежнему линейное неравенство, не смотрите на
[14:49.440 --> 14:56.200]  то, что здесь переменная есть в правой части. Вот это как раз никто никто не обещает, что переменные
[14:56.200 --> 15:00.680]  только в левой части неравенства нам удобно записывать. В конце концов мы его можем преобразовать
[15:00.680 --> 15:06.960]  как бы в неравенство, где все переменные сгружены влево, а справа константа ноль. Но по смыслу неравенство
[15:06.960 --> 15:12.080]  получается такое, да, что если эта переменная выставлена в единицу, то мы правда должны покрыть этот
[15:12.080 --> 15:16.480]  столбец, а если она выставлена в нолик, то это неравенство оно и так всегда выполнено при любых
[15:16.480 --> 15:23.400]  х. Ура! А как нам тогда вот это вот условие потребовать с помощью неравенства?
[15:23.400 --> 15:48.800]  Чего у нас? Точно, 0,9 на n. Сумма по всем живут единички. Видите, как просто. То есть на самом деле мы с
[15:48.800 --> 15:54.280]  помощью вот этого механизма CLP можем задачки превращать из абсолютно строгих, таких часто,
[15:54.280 --> 16:03.560]  в задачки, где нам нужно вот там на 90 процентов достичь результат или на любой фиксированный
[16:03.560 --> 16:12.080]  процент, да, то есть можно немножко смягчать вот жесткость задачи, и это очень такой гибкий
[16:12.080 --> 16:18.680]  механизм CLP. Мы потихоньку с вами вот будем рассматривать примеры, как такие или другие
[16:18.680 --> 16:24.600]  условия смоделировать на этом языке. Вот как язык программирования прям получается. Ну ладно,
[16:24.600 --> 16:30.160]  это на самом деле просто пример. Мне хотелось вам показать, а решать-то мы будем все равно вот такую
[16:30.160 --> 16:38.320]  вот задачку. Давайте представим, что мы ее решили таки, но не CLP, а просто LP, как мы это всегда
[16:38.320 --> 16:48.560]  делаем. Рассмотрели CLP, потом перешли просто к задаче линейного программирования, потом эту
[16:48.560 --> 17:00.840]  задачу решили и получили некоторое решение. x1 со звездочкой и так далее, xm со звездочкой,
[17:00.840 --> 17:11.960]  но решение это уже на отрезке 0,1. Вот, ваши предложения, что нам делать с округлением? Вот
[17:11.960 --> 17:17.640]  помните, как мы округляли в задаче о вершинном покрытии? Мы там как округляли? Помните, тоже
[17:17.640 --> 17:34.240]  получали набор. Что мы там делали? Так, точно. Ну, к ближайшему целому, у нас тут все было так
[17:34.240 --> 17:39.640]  здорово и замечательно, к ближайшему целому. Все округляли, и это интуитивно понятно как-то,
[17:39.640 --> 17:45.720]  это то, что мы привыкли понимать под округлением, да, но еще и работало у нас, действительно. Вот,
[17:45.720 --> 17:51.600]  а что у нас вот тут? Можем ли мы всегда гарантировать, что вот эти переменные,
[17:51.600 --> 17:59.120]  так можно на раз округлить к ближайшему целому, получится корректное решение? Вот,
[17:59.120 --> 18:09.600]  а в чем может быть проблема? Да, вот тут почему все работало, да, почему мы гарантировали,
[18:09.600 --> 18:14.240]  что у вас в таком неравенстве хотя бы одна переменная точно уже будет не меньше одной
[18:14.240 --> 18:20.120]  и второй, потому что их всего две было, да, и когда две переменные, да, наибольшие из них не меньше
[18:20.120 --> 18:25.560]  одной и второй, а когда у нас матрица произвольная, здесь может быть по три единички в столбце и больше,
[18:25.560 --> 18:31.080]  то и неравенства возникают с тремя и больше переменными, и тогда может быть что неравенство
[18:31.080 --> 18:36.960]  выполнено, но там все переменные по одной трети, например. Окей, а мы можем все равно вот какое-то
[18:36.960 --> 18:41.760]  правило сформулировать, давайте пофантазируем немножко, какое-то правило, чтобы округлять
[18:41.760 --> 18:50.480]  переменные вот чисто по порогу и говорить, что если х со звездочкой этой больше или равен
[18:50.480 --> 18:56.280]  какого-то порога, ну тогда переменную округляем к единице, если меньше порога того же самого,
[18:56.280 --> 19:02.360]  вот, то тогда смело можем в ноль округлить, и вот чтобы это работало, чтобы это приводило
[19:02.360 --> 19:15.800]  к корректному решению. 1 минус 1 на n, ну да, вот не может быть больше m переменных здесь,
[19:15.800 --> 19:22.560]  1 минус 1 на n, наверное. Ну и в целом можно сказать, что если у нас есть какое-то ограничение на
[19:22.560 --> 19:27.960]  максимальное количество единиц в столбце, то тогда можно единицы поделить на вот это количество,
[19:27.960 --> 19:37.120]  взять в качестве порога, да. А можем мы это записать как-то формально? Единицы поделить на что?
[19:37.120 --> 19:47.440]  Ничего, ну на m, да, ну что-то может быть более точное, что от матрицы зависит,
[19:47.440 --> 20:03.200]  давайте попробуем поставить. Но нет, боюсь, что с ранга мне не получится. Давайте, может быть,
[20:03.200 --> 20:08.120]  я неправильно просто вопрос оформлировал, что от вас хочу. Я хочу, чтобы мы в терминах матрицы
[20:08.120 --> 20:13.360]  как-то записали это максимальное количество единиц в столбце, давайте я так это и запишу,
[20:13.360 --> 20:21.440]  максимум по всем столбцам, по всем столбцам, а количество единиц в столбце как вообще?
[20:21.440 --> 20:33.480]  Да, сумма ежитых, по всем и от единицы до m. Ну, не очень компактная формула, но тем не менее, да.
[20:33.480 --> 20:48.000]  Конечно, одно число такое для всей матрицы. И тогда, в частном случае, вот, например,
[20:48.000 --> 20:54.320]  такое округление, в частном случае, когда эта матрица инциденции графа, это дает нам просто в
[20:54.320 --> 21:00.880]  точности алгоритм для вершинного покрытия, который мы с вами уже рассматривали. Окей, вот такой
[21:00.880 --> 21:11.600]  выбор иксов, он корректный. А что можно сказать про, поясните здесь еще раз, да, вот, почему мы
[21:11.600 --> 21:17.640]  гарантируем, что при таком пороге решение точно оказывается корректным? Потому что у нас было вот
[21:17.640 --> 21:24.920]  такое вот неравенство, так. А сколько переменных иксов входило по факту в левую часть такого
[21:24.920 --> 21:36.080]  неравенства? Ну, не больше, чем максимальное количество ашек, да, вот этих равных единиц. То есть
[21:36.080 --> 21:40.480]  икс, каждый раз, когда ашка равна единице, икс у нас появляется, действительно, в этой сумме. Когда
[21:40.480 --> 21:45.640]  ашка равна нуле, он по факту здесь не появляется, не участвует. Значит, максимальное количество
[21:45.640 --> 21:51.320]  ненулевых слагаемых вот в этом неравенстве у нас равняется максимальному количеству единиц
[21:51.320 --> 21:58.440]  среди ашек в житом столбце. Максимум берется по всем ж, чтобы можно было использовать один порог
[21:58.440 --> 22:04.320]  на все случаи жизни, да, при округлении всех переменных. Значит, максимум берется по всем
[22:04.320 --> 22:09.840]  столбцам. И вот это, это просто количество слагаемых максимально в этой сумме ненулевых, ну,
[22:09.840 --> 22:16.760]  зависящих от икса, да. И мы используем то же самое рассуждение, что здесь. Здесь у нас было два икса,
[22:17.200 --> 22:21.000]  мы говорили, что максимальный из них точно не меньше одной второй. Если у нас три икса, то
[22:21.000 --> 22:27.540]  максимальный из них точно не меньше одной третьей. Если у нас десять иксов, то максимальный из них
[22:27.540 --> 22:33.720]  не меньше одной десятой. Вот и все. И тогда мы фактически гарантируем, что после округления,
[22:33.720 --> 22:39.360]  вот этот самый икс, который точно не меньше одной десятой, он выставится в единичку. И вот здесь
[22:39.360 --> 22:44.160]  вот у нас, за счёт этой единички, будет по-прежнему что-то больше или равной единице, и неравенство
[22:44.160 --> 22:49.680]  даже после округления при смене звездочек вот здесь на крышечке, оно останется верным. Нам же
[22:49.680 --> 22:55.080]  это нужно, по сути, перейти от нецелочисленных к целочисленным переменным, так чтобы все
[22:55.080 --> 23:06.240]  неравенства у нас сохранились. А теперь давайте тоже вспомним, как мы оценивали значение
[23:06.240 --> 23:12.360]  целевой функции на округленном решении в задаче о вершинном покрытии, и посмотрим, как нам оценить
[23:12.360 --> 23:19.680]  значение целевой функции вот в этой задаче. Значит, наша целевая функция, давайте я ее назову,
[23:23.680 --> 23:32.840]  ладно, давайте я назову obj. Но я здесь неспроста это называю obj, дело в том, что, как правило,
[23:32.840 --> 23:37.120]  когда программируют, пишут программы в ограничениях, в частности там какие-нибудь
[23:37.120 --> 23:43.200]  линейные программы, у профессионалов, профессионалы часто называют целевую функцию как раз таким
[23:43.200 --> 23:48.160]  сокращением obj, точно так же, как мы привыкли называть сокращением opt, оптимальное значение
[23:48.160 --> 23:53.640]  целевой функции. И вот то значение, которое мы получаем, округлив эти переменные, как его
[23:53.640 --> 24:00.480]  можно сравнить с оптимальным значением? Ну, по аналогии, если действовать с задачкой о вершинном
[24:00.480 --> 24:08.720]  покрытии, там помните, какая константа была? 2, да. А здесь какая константа будет? Вот эта, да,
[24:08.720 --> 24:13.640]  которая здесь стоит в знаменателе. То есть и рассуждение абсолютно такое же, опять-таки,
[24:13.640 --> 24:19.960]  когда мы округляем от вот такого значения как минимум к единице, то наша переменная возрастает
[24:19.960 --> 24:26.920]  как максимум вот во столько раз. И раз каждая переменная возрастает как максимум во столько раз,
[24:26.920 --> 24:32.520]  то и вот такая сумма, которая является нашей целевой функцией, она тоже возрастает как
[24:32.520 --> 24:38.800]  максимум во столько же раз. Так что здесь мы способны поставить вот эту самую константу,
[24:38.800 --> 24:57.400]  да, значит, максимум сумма а ежи по и, а максимум по жи. Пока что не делаю так все конспективно,
[24:57.400 --> 25:02.400]  потому что это просто чистая аналогия с вершинным покрытием, просто один в один,
[25:02.400 --> 25:09.000]  поэтому не расписываю все. Вот. Давайте посмотрим какие еще, ну понятно, наверное,
[25:09.000 --> 25:16.400]  что эта штука, когда она большая, то алгоритм получается какой-то ну не очень, да. Мы вынуждены
[25:16.400 --> 25:22.440]  округлять большинство переменных к единичке, когда порог очень низкий, и тогда показатель
[25:22.440 --> 25:30.520]  апроксимации у нас тоже страдает. Чем он выше, тем хуже. Вот. Какие мы модификации можем проделать?
[25:30.520 --> 25:36.400]  Ну, во-первых, мы можем округлять каждую переменную со своим порогом, то есть мы можем смотреть,
[25:36.400 --> 25:42.720]  вот надо нам округлить переменную х1. Мы же можем вот здесь вот взять максимум только по тем
[25:42.720 --> 25:49.400]  столцам, которые этой переменной покрываются, да. То есть если переменная участвует не во всех
[25:49.400 --> 25:54.760]  неравенствах, а только в каких-то, то для нее не надо выставлять супер мега порог, да, какой-то
[25:54.760 --> 26:03.520]  низкий супер мега, да, очень низкий порог не надо выставлять. То есть здесь максимум можно
[26:03.520 --> 26:12.880]  брать только по тем столцам, которые покрываются вот конкретной этой строкой. Это раз. Но во-вторых,
[26:12.880 --> 26:20.240]  то, что может быть более интересно, это можно попытаться выбрать наименьший порог, который общий
[26:20.240 --> 26:31.440]  для всех переменных так, чтобы переменные образовали покрытие. Адаптивный порог.
[26:50.240 --> 27:01.680]  Значит, давайте я попытаюсь это как-то изобразить картинкой. Вот у нас x1, x2, x3 и так далее, xm со звездочками.
[27:08.480 --> 27:18.360]  Это у нас нолик, это единичка. Ну а так вообще x и где-то вот каждый x, он на какой-то отметке
[27:18.400 --> 27:27.960]  здесь стоит. Какой-то выше, какой-то ниже. Когда мы выбираем некий порог, мы здесь рисуем такую, да,
[27:27.960 --> 27:33.600]  линию фактически. И все x, которые попали выше этого порога, мы соответствующие строчки матрицы
[27:33.600 --> 27:40.240]  берем в покрытие. Ну или, что то же самое, округляем соответствующие x единицы. Проводим такую здесь
[27:40.240 --> 27:46.520]  черту между 0 и единицей. Смотрим, кто выше нее оказался. Воду наливаем в аквариум и смотрим,
[27:46.520 --> 27:51.320]  какие рыбки оказались в воде. Вот или не рыбки, хомячки, а какие не задохнулись. Вот какие не
[27:51.320 --> 28:00.240]  задохнулись, мы, значит, берем их в покрытие. Они, значит, сильны духом. Ну вот, и вот здесь вот мы
[28:00.240 --> 28:06.160]  выставляем порог очень низкий, да, какой-то стелищееся здесь, близко к нулю. Получается,
[28:06.160 --> 28:13.000]  что много хомячков оказываются в покрытии. Ну и ладно, я оставлю это негуманная аналогия,
[28:13.000 --> 28:19.880]  извините, с хомячками. Но дальше давайте попробуем посмотреть, а если мы будем двигать вот этот
[28:19.880 --> 28:24.960]  порог, будем плавненько поднимать, да, вот эту вот линию, или наоборот, скорее плавно опускать.
[28:24.960 --> 28:30.840]  Нам же нужно, чем выше порог сделать, тем лучше, потому что чем выше порог, тем у нас меньше показатель
[28:30.840 --> 28:37.080]  аппроксимации. Он с порогом, они связаны как инверстные друг другу числа, да, обратные друг
[28:37.080 --> 28:43.720]  другу числа. Вот поэтому мы можем начать, просто представьте себе, да, вот с такой прямой, равной единице,
[28:43.720 --> 28:49.320]  потом потихонечку ее опускать. А что будет с переменами происходить? Мы опускаем-упускаем, ага,
[28:49.320 --> 28:55.880]  сначала х3 у нас выставится единицей, да, потом снова опускаем-опускаем-опускаем, потом х1 выставится
[28:55.880 --> 29:02.400]  единицей, потом снова опускаем-опускаем-опускаем, потом хм выставится единицей, потом х2 выставится
[29:02.400 --> 29:08.960]  единицей, да, вот так потихоньку все больше-больше переменных выставляются единичками. Ну понятно,
[29:08.960 --> 29:15.160]  что в конце концов мы опустимся до нуля, точно получим покрытие, но где-то посередине существует
[29:15.160 --> 29:22.800]  какое-то значение порога, для которого мы уже имеем покрытие, правда? Вот, и вот его-то, наверное,
[29:22.800 --> 29:29.080]  и надо взять. А как вы можете выбирать такое значение порога? Вопрос. Как его быстро выбрать?
[29:29.080 --> 29:37.160]  Вы же не можете вот так вот плавно менять, ну не знаю, может и можете, можно написать вуличную
[29:37.160 --> 29:41.320]  программу, где есть такой ползунок, который двигает пользователь и смотрит, как вот эти
[29:41.320 --> 29:46.680]  вот штуки оказываются над водой, под водой. Вот, но это красиво, но не очень прагматично, да?
[29:46.680 --> 29:56.600]  Минимум из переменных, но тогда все они в единицу окажутся. Нам нужно выбрать как-то
[29:56.600 --> 30:02.480]  такой максимальный порог, что если взять только переменные выше этого порога, мы получаем покрытие.
[30:02.480 --> 30:07.280]  Вот такое максимальное число, при котором это корректное решение. Как вообще такое искать?
[30:07.280 --> 30:21.360]  Бин поиском можно, да? Вот у нас есть... Что такое? А чего такого в том, что ваше предположение в том,
[30:21.360 --> 30:25.240]  что оно актуально, почему нет? У нас есть универсальный алгоритм, действительно,
[30:25.240 --> 30:30.160]  потому что вот проверка того, при фиксированном пороге, проверка того, что он дает корректное
[30:30.160 --> 30:35.920]  решение, она не такая уж, не такая быстрая, то есть надо как бы не очень много попыток вообще
[30:35.920 --> 30:41.080]  сделать выбор этого порога, поэтому бин поиск это то, что отлично здесь подходит. За логарифмичное
[30:41.080 --> 30:48.960]  число действий мы можем подобрать такой порог, чтобы и проверить, что соответствующее покрытие
[30:48.960 --> 30:54.960]  корректно. А что мы еще можем сделать? Давайте посмотрим альтернативный взгляд на вещи. Значит,
[30:54.960 --> 31:02.120]  вот я здесь сотру этот пример и напишу как раз первая это ваша идея с бин поиском в выборе
[31:02.120 --> 31:21.840]  порога. Значит, подход один. Да, берем порог одна-вторая и смотрим. Да, вот образуется некое множество
[31:21.840 --> 31:27.440]  иксов. Образует оно покрытие или нет? Не образует. Окей, значит, порог нужно взять выше, ниже,
[31:27.440 --> 31:37.080]  ниже, да, значит, ниже. Берем порог одна четверть тогда. Ого, уже не покрытие. Нет,
[31:37.080 --> 31:44.480]  еще допустим, уже не покрытие, еще покрытие. Окей, значит, порог выше тогда берем, то есть
[31:44.480 --> 31:51.720]  одна-вторая, потом одна-четверть, потом что там будет? Да, что-то странно. Одна-вторая,
[31:51.720 --> 31:59.160]  одна-четверть, а посредине-то что? Три восьмых. Вот, и так дальше, да, но обычный бин поиск. Вот,
[31:59.160 --> 32:11.400]  подход один, бин поиск. А подход два мы узнаем после перерыва. Все, перерыв. Во-первых,
[32:11.400 --> 32:16.160]  я открыл форму, теперь если у кого-то она не работала, наверное, если она у всех не работала
[32:16.160 --> 32:24.560]  до селя, то теперь она работает тоже у всех. Можно, да, да, да, даже нужно. Вот, зачем нужно
[32:24.560 --> 32:29.000]  стэпик от всех, даже те, кто не собирается додавать задания по курсу, я вот стэпик традиционно
[32:29.000 --> 32:35.680]  использую стэпик ID как ваш просто ID-шник на курсе, чтобы не по фамилии вас там перечислять,
[32:35.680 --> 32:43.640]  а по номеру. Так просто веселее по номеру, вот, в табличке перечислять. Но дело в том,
[32:43.720 --> 32:49.800]  что это какой-то компромисс. На западе вообще в некоторых вузах запрещено публиковать общие
[32:49.800 --> 32:56.040]  списки оценок, например, студентов, вот, но чтобы как бы не как-то соответствовать,
[32:56.040 --> 33:02.040]  но не до конца, я решил в таблицах указывать стэпик ID. Вопрос. Да.
[33:02.040 --> 33:18.120]  Стэпик ID в любом случае потребуется, то есть я его все равно, он мне нужен. Вы финальный тест,
[33:18.120 --> 33:26.080]  ну, как бы этот тест на получение оценки, помните, про который мы говорили, чтобы ну просто такой тест
[33:26.080 --> 33:30.160]  по теории, чтобы просто проставить оценку, от него сама оценка не зависит, но его надо закрыть
[33:30.160 --> 33:35.640]  таки, чтобы эту оценку проставить, чтобы подытожить материал. Вот, он все равно в любом случае,
[33:35.640 --> 33:43.560]  он точно будет на стэпике, поэтому на стэпике все равно придется регистрироваться. И если вы на
[33:43.560 --> 33:50.520]  курсере, то тоже мне нужно, потому что я вас буду идентифицировать по стэпик ID, я вас по нему
[33:50.520 --> 33:59.080]  идентифицирую. Так, ну чего там у нас? Бинарный поиск, это мы с вами выяснили. А как еще можно
[33:59.080 --> 34:08.080]  посмотреть на вот эту ситуацию с иксами? Вот эти иксы мы сразу можем сказать при понижении порога
[34:08.080 --> 34:14.240]  сверху вниз, при плавном движении порога сверху вниз, в каком порядке эти иксы будут бамс-бамс-бамс
[34:14.240 --> 34:22.880]  оказываться над водой последовательно. Мы это понимаем или нет? В порядке убывания, конечно. И это
[34:22.880 --> 34:34.160]  как раз подход два. Подход два, да, просто отсортировать иксы в порядке убывания. Значит,
[34:34.160 --> 34:48.920]  пересортировать или просто вот сортировать по убыванию. Отсортировать по убыванию. Вам знакомы
[34:48.920 --> 34:58.480]  такое обозначение x с индексом p от единички, p от двойки, нет? Да, перестановка, просто берем
[34:58.480 --> 35:07.080]  такую перестановку, которая сортирует наши иксы. Так это отлично. Смотрите, когда нам что-то
[35:07.080 --> 35:15.520]  незнакомое, это вообще здорово, потому что мы что-то новое узнали. Икс со звездочкой с индексом p от m.
[35:15.520 --> 35:24.840]  Вот, то есть p это перестановка, но их часто, самые две буквы частые для обозначения перестановок,
[35:24.840 --> 35:33.960]  это p и сигма. Значит, а перестановка это взаимно однозначное отображение между множеством,
[35:33.960 --> 35:38.880]  в данном случае номеров от единички до n, и этим же множеством от единички до n. То есть,
[35:38.880 --> 35:43.720]  вот эта последовательность p от единицы, p от двойки и так далее, p от m, это просто какая-то
[35:43.720 --> 35:49.840]  перестановка этих же самых чисел от одного до n. В таком порядке, который является сортировкой
[35:49.840 --> 36:00.400]  вот этих вот переменных. Вот и все. Вот вся суть этого обозначения. Ну вот, и дальше понимаете,
[36:00.400 --> 36:05.000]  что мы делаем. Дальше мы вместо того, чтобы вообще рассматривать порог теперь, мы просто
[36:05.000 --> 36:11.880]  проходимся последовательно по иксам в этом списке и берем очередную строчку матрицы. И
[36:11.880 --> 36:17.200]  первый же момент, когда взятых строчек достаточно, чтобы у нас получилось покрытие,
[36:17.200 --> 36:23.040]  мы останавливаемся. Больше того, здесь можно тоже сэкономить. Ведь не всегда,
[36:23.040 --> 36:28.760]  беря очередную строчку матрицы, мы с вами вообще покрываем новые столбцы. Представьте,
[36:28.760 --> 36:33.080]  что вы уже достаточно много строк взяли, и у вас там осталось два столбца непокрытым,
[36:33.080 --> 36:38.240]  например, из всей матрицы. Но велика вероятность, что взяв очередную строчку,
[36:38.240 --> 36:42.880]  вы никаких новых столбцов не покроете. Тогда его можно просто пропустить. И получается,
[36:42.880 --> 36:48.760]  что вообще это уже даже будет что-то эффективнее, чем просто пороговое округление. Это какое-то
[36:48.760 --> 36:52.960]  округление такое с подбором. Мало того, что порога, так еще мы некоторые переменные,
[36:52.960 --> 36:57.520]  несмотря даже на этот порог, все равно округляем к нулю, если они нам ничего не дают на фоне
[36:57.520 --> 37:05.040]  остальных переменных. Вот такая забавная получается, такой взгляд на вещи. Это знаете,
[37:05.040 --> 37:13.440]  на что похоже? Вот такой взгляд со стороны порога и со стороны переменных. Мне очень нравится,
[37:13.440 --> 37:21.680]  как поступили создатели Пэкмен. Знаете такую игру Пэкмен? Ну, конечно, кто не знает. Такой
[37:21.680 --> 37:29.840]  колобок, который ходит и всех ест. Нет, он не всех ест, его едят. Привидения за ним гоняются,
[37:29.840 --> 37:36.080]  а он там съедает шарики. Так вот, как в этой игре в те времена, допотопные, когда не было
[37:36.080 --> 37:41.000]  никаких мощных компьютеров, игры вообще работали не в компьютерах, а в автоматах. Каким-то монетку
[37:41.000 --> 37:46.240]  надо опускать, и вот тогда игра запускается на несколько минут, и ты в нее играешь. Значит,
[37:46.240 --> 37:51.000]  все было ж примитивным, и игра должна была быть очень быстрой. А как в таких играх реализовать
[37:51.000 --> 37:56.080]  искусственный интеллект, чтобы привидения бегали, ну как-то перемещались более-менее туда,
[37:56.080 --> 38:01.520]  куда Пэкмен движется. А он же движется, добавок, по лабиринту. Там еще какой-то лабиринт из этих
[38:01.520 --> 38:09.920]  шариков и стен. И был подход первый. Взять и каждый раз, каждый момент времени решать задачу о
[38:09.920 --> 38:16.120]  кратчайшем пути. Для каждого привидения запускать, как оно может добраться к самым коротким маршрутам
[38:16.120 --> 38:21.560]  до Пэкмена. И мало того, что это тормозно, так еще Пэкмен в следующий момент времени уже уйдет
[38:21.560 --> 38:26.960]  оттуда, окажется где-то в другом месте абсолютно. И гениальный подход, который создатели этой игры
[38:26.960 --> 38:32.400]  реализовали, они даже статью опубликовали в свое время по этому поводу. Значит, он такой,
[38:32.400 --> 38:38.080]  просто Пэкмен в лабиринте он оставляет такой запахок. Вот представьте, что Пэкмен не пользовался
[38:38.080 --> 38:44.320]  дезодорантом, а он бегает шеей, он поэтому так пахнет. Вот, и он по лабиринту движется и оставляет
[38:44.320 --> 38:49.800]  за собой такой запаховый след. Ну или как муравьи, можно сказать. Вот муравьи, насекомые, они тоже
[38:49.800 --> 38:57.680]  запахами общаются вообще-то. Когда муравей ползет к чему-то вкусному, когда он ползет просто куда-то
[38:57.680 --> 39:02.320]  и видит там что-то вкусное, он потом ползет обратно к муравейнику, а по дороге он оставляет такой
[39:02.320 --> 39:07.680]  след запаха. Есть даже дискретная оптимизация алгоритм муравьиных колоний, который основан вот
[39:07.680 --> 39:13.720]  ровно на таком подходе. Так вот, в этой игре Пэкмен, значит, в каждой клетке лабиринта просто
[39:13.720 --> 39:19.720]  хранилась переменная, целочисленная или не целочисленная, уже не помню, в которой хранился запах.
[39:19.720 --> 39:24.520]  И этот запах, когда там есть Пэкмен, он максимальный, и как только Пэкмен выходит из клетки, запах со
[39:24.520 --> 39:31.680]  временем начинает притухать, вот так вот, до нуля опускаться, и приведения движутся просто по запаху,
[39:31.680 --> 39:38.320]  они из текущей клетки движутся туда, где запах выше всего. И вот так вот получается довольно
[39:38.320 --> 39:46.800]  эффективная модель, которая неплохо соответствует какому-то искусственному интеллекту, ну не очень,
[39:46.800 --> 39:53.800]  конечно, продвинутому, но забавно. Вот, то есть, это взяли и посмотрели вместо того, чтобы смотреть
[39:53.800 --> 39:59.520]  с точки зрения агентов, с точки зрения приведений, посмотрели на всю ситуацию с точки зрения игрового
[39:59.520 --> 40:04.720]  поля. Игровое поле теперь является хранилищем информации само по себе, вот этого запаха
[40:04.720 --> 40:12.160]  условного. И у нас здесь тоже, мы наоборот перешли от как бы игрового поля к какого-то общего порога
[40:12.160 --> 40:18.080]  там для переменных, мы взяли и посмотрели аккуратно, как эта вся ситуация вообще выглядит со стороны
[40:18.080 --> 40:23.120]  переменных, и тогда мы смогли понять в каком порядке эти переменные по сути округлять.
[40:23.120 --> 40:40.880]  Конечно, и приведения идут в сторону возрастания запаха. Они перестают, не ищут больше, каждый раз
[40:40.880 --> 40:48.800]  тоже кратчайший путь до точки, где пыкмен опять. Нет, они просто вот в соседние клетки рассматривают
[40:48.800 --> 40:59.280]  четыре штуки, да, и идут в ту клетку из соседних, где запах больше. Так что так, да. Так, ну вот, я уже
[40:59.280 --> 41:06.200]  забыл, нет, не забыл, почти забыл, что хотел рассказать дальше, но дальше нам нужно рассматривать
[41:06.200 --> 41:11.200]  еще какой-то продвинутый алгоритм для этой задачки. Вот давайте это делать. А у вас тервьер пока
[41:11.200 --> 41:19.000]  вообще еще не так совсем. Давайте тогда мы сегодня начнем жадный алгоритм для этой задачки. А если у
[41:19.000 --> 41:28.560]  вас все-таки тервьер придет к вам в дом, тервьер, то так он только в следующем не придет он в дом
[41:28.560 --> 41:33.200]  в этом семестре. Ну, в общем, может, тогда мы и пропустим вероятностное округление. Да, слушайте,
[41:33.200 --> 41:39.120]  давайте я просто скажу, расскажу вероятностное округление, но мы ничего про него не будем доказывать,
[41:39.120 --> 41:53.120]  ладно? Какие вы продвинутые. А вы конструировали неизмеримое полибегу множество? Да. О, какие
[41:53.120 --> 42:13.960]  молодцы. А я уже не знаю, как его конструировать, когда ты вот знал. Третий подход, который подход 3,
[42:13.960 --> 42:21.880]  который мы с вами без доказательства, я сформулирую просто результат, ну а сам подход мы с вами,
[42:21.880 --> 42:26.920]  конечно, целиком просто разберем. Он называется вероятностное округление или рандомизированное
[42:26.920 --> 42:33.720]  округление randomized rounding. И он основан на очень простом соображении. Если у нас переменные все
[42:33.720 --> 42:41.240]  равно от 0 до 1, то вообще любое число от 0 до 1 можно трактовать как вероятность чего-то. А вероятность
[42:41.240 --> 42:46.520]  чего? Ну, того, что мы соответствующую строку матрицы возьмем в покрытие. Нет у нас больше
[42:46.520 --> 42:54.440]  детерминированного какого-то порога. Мы монетку подбрасываем. А вот и вся идея. То есть теперь мы
[42:54.440 --> 43:07.720]  с вами можем так сформулировать алгоритм. Пробегаемся по И от единички до М. И если,
[43:07.720 --> 43:19.000]  ну да, значит, и берем эту строчку матрицы в покрытие с вероятностью х и т со звездой. Я здесь
[43:19.000 --> 43:30.960]  напишу такую штуку интересную. Если random меньше или равен х и т со звездой, то х и т с крышкой
[43:30.960 --> 43:42.480]  устанавливаем единицу. Знаете такой прием программирования? Если функция random возвращает
[43:42.480 --> 43:48.440]  случайно равномерно распределенное на отрезке 0 и 1 число, то вот такое вот неравенство как раз
[43:48.440 --> 43:55.360]  и выполняется с вероятностью в точности х и со звездочкой. То есть фактически здесь можете
[43:55.360 --> 44:02.400]  написать просто с вероятностью х и со звездочкой, подбросив монетку или там что-то еще, кубик,
[44:02.400 --> 44:12.520]  да, мы берем строчку в покрытие. То есть округляем х и т к единице. Будет у нас покрытие в результате
[44:12.520 --> 44:18.400]  вот по истечении этого цикла, когда мы пробежались по каждой строке едино и подбросили
[44:18.400 --> 44:27.800]  монетку и с вероятностью какой-то отключили строчку покрытия. Непонятно, непонятно,
[44:27.800 --> 44:32.240]  потому что все случайно, да, то есть может быть мы вообще все строчки возьмем, а может быть мы
[44:32.240 --> 44:40.720]  ни одной строчки не возьмем, как монетки выпадут, как карта ляжет. И значит это нужно включить в
[44:40.720 --> 44:45.200]  еще один какой-то цикл, который будет таким стражником все-таки, он будет следить за тем,
[44:45.200 --> 44:50.360]  чтобы решение у нас получалось корректно. Поэтому давайте мы этот цикл 4 поместим внутри еще одного
[44:50.360 --> 45:07.320]  цикла while. Но я так напишу, да, пока у нас не покрытие, мы будем делать вот эту. Причем давайте
[45:07.320 --> 45:12.880]  мы алгоритм оставим вот именно так, как он написан, без реанитализации. То есть можно
[45:12.880 --> 45:20.160]  считать вот что, что если мы какой-то х итой округлили к единице, то когда мы пойдем на
[45:20.160 --> 45:25.440]  следующую итерацию вот этого цикла while, мы этот х итой оставим равным единице. Мы не будем
[45:25.440 --> 45:30.200]  снова для него подбрасывать монетку. Видите, мы здесь нигде не полагаем х итой равными нулю,
[45:30.200 --> 45:37.800]  здесь только один assignment, одно присваивание. х итой равным нулю можно положить в самом начале,
[45:37.800 --> 45:48.200]  вот здесь до while. х итой полагается равным нулю для всех и. Это такая инициализация. А потом уже
[45:48.200 --> 45:53.640]  х итые только в единичку устанавливаются. Понятно, что если этот цикл while проработает
[45:53.640 --> 46:00.440]  достаточно долго, то у нас все х итые вообще установятся в единицу. Но если бы здесь не было
[46:00.440 --> 46:07.440]  вот этого условия, если просто цикл while бесконечно долго вращался, то тогда все х и у нас
[46:07.440 --> 46:12.640]  остановились бы в единицу и, конечно, у нас было бы покрытие. Вот, но естественно, что мы вот
[46:12.640 --> 46:17.640]  крутимся только до тех пор, пока у нас не покрытие и останавливаемся, когда сможем.
[46:17.640 --> 46:30.960]  С реинициализацией тоже, в принципе, да, безусловно, да. Вот, но теорема, я почему так написал
[46:30.960 --> 46:37.360]  все-таки без реинициализации внутри while, потому что теорему я только для такой версии алгоритма
[46:37.360 --> 46:58.920]  доказал бы вам, если бы у вас был тервер. Смотрите, теоремка такая, что с вероятностью,
[46:58.920 --> 47:10.960]  ну все числа условные тоже, надо понимать их, все можно немножко двигать. С вероятностью больше
[47:10.960 --> 47:28.280]  чем, ну скажем, 0,9. С вероятностью больше, чем 0,9, алгоритм остановится за время,
[47:28.280 --> 47:48.680]  о большое, вот алгоритма n раундов. Что такое раунд? Вот один раунд вероятностного округления – это
[47:48.680 --> 47:59.920]  один пробег цикла 4. Давайте я это здесь вот напишу. Вот этот цикл 4 – это один раунд. Раунд округления.
[48:10.200 --> 48:15.960]  Хорошо, что можно писать на двух языках, потому что округление – это и так раундинг, и так
[48:15.960 --> 48:21.680]  получилось по раундов раундинг. То есть очень получилось непонятно, наверное. А так понятно,
[48:21.680 --> 48:28.440]  но это тоже, наверное, правда понятно. Значит, алгоритм остановится за логарифмичное число раундов,
[48:28.440 --> 48:35.800]  ну то есть за полинамиальное и очень вполне какое-то разумное время, и вес покрытия. Так,
[48:35.800 --> 48:40.600]  вот теперь про вес покрытия, что можно сказать. Поскольку это алгоритм опять-таки с какой-то
[48:40.600 --> 48:47.800]  случайностью внутри, то для таких алгоритмов мы обычно говорим про ожидаемый вес покрытия,
[48:47.800 --> 48:54.200]  мат ожидания. Мат ожидания, вы точное определение, даже если кто не знает, можете считать, что это
[48:54.200 --> 49:00.920]  в среднем. Если много-много раз перезапускать этот алгоритм, то в среднем он будет выдавать
[49:00.920 --> 49:07.400]  вот ожидаемое какое-то, ожидаемое по весу покрытие. Ожидаемый вес
[49:07.400 --> 49:31.360]  покрытия будет O от OPT на лоноритм N. Давайте я не буду здесь O писать под O,
[49:31.360 --> 49:41.080]  просто O от лоноритм N умножить на O. На самом деле можно здесь поставить любую константу,
[49:41.080 --> 49:47.000]  близкую к единице, настолько, насколько хочется. Просто от того, насколько эта константа близко к
[49:47.000 --> 49:51.840]  единице, будет зависеть константа вот в этом большом, алгоритм будет чуть тормознее, и
[49:51.840 --> 49:59.040]  соответственно ожидаемый вес покрытия тоже получится чуть побольше. Смотрите-ка, в отличие
[49:59.040 --> 50:04.880]  от задачки о вершинном покрытии, мы здесь не имеем константного показателя апроксимации.
[50:04.880 --> 50:11.200]  Чем матрица больше, чем она шире, чем больше в ней столбцов, тем хуже у нас показатель
[50:11.200 --> 50:21.360]  апроксимации. Дальше мы с вами рассмотрим жадную иуристику, немножко отдохнем от линейного
[50:21.360 --> 50:28.080]  программирования, от всяких округлений, и так уже хватило нам и вероятностного округления
[50:28.080 --> 50:32.880]  появилось, и Бог знает, что еще. Вот мы сейчас рассмотрим вполне детерминированную жадную иуристику
[50:32.880 --> 50:39.800]  для задачи о покрытии, но увидим, что для нее оценка показателя апроксимации, к сожалению,
[50:39.800 --> 50:47.880]  такая же. Тоже логариф мотен, не лучше. Ну, давайте начнем это дело. Мы сегодня вряд ли успеем
[50:47.880 --> 50:55.680]  закончить с доказательством, но начать мы точно сможем. Сейчас я вот это вот постираю, ладно, могу.
[50:58.080 --> 51:19.520]  Какая схема доказательства? Мы рассматриваем сначала такое событие, вероятность того,
[51:19.520 --> 51:24.600]  что за один раунд округления какой-то столбец остался непокрытым. Оказывается, что эта вероятность
[51:24.600 --> 51:33.000]  можно оценить константой единицы на Е. Е — это основа натурального алгорифма. Дальше, исходя из того,
[51:33.000 --> 51:38.200]  что с хорошей вероятностью мы один столбец покрываем за один раунд, можно оценить,
[51:38.200 --> 51:45.040]  что за логарифмичное число раундов мы все столбцы покроем, скорее всего. А дальше, учитывая,
[51:45.040 --> 51:51.000]  что мы логарифмичное число раундов всего работаем с хорошей вероятностью, при этом условии там уже
[51:51.000 --> 52:00.560]  нетрудно оценить мат ожидания. Дело в том, что, смотрите дальше, как оценивается, вот каково
[52:00.560 --> 52:08.000]  мат ожидания веса строк, которые будут взяты в покрытие. Вот примерно можно сказать, что раз строка
[52:08.000 --> 52:17.280]  берется с вероятностью х и т со звездочкой, а вес у нее равен w и t, то мат ожидания как бы вклада
[52:17.280 --> 52:24.760]  этой строки за один раунд получается вес строки помножить на х и т со звездочкой. А поскольку сумма
[52:24.760 --> 52:32.280]  весов w и t, не переписывайте это только, я просто напомню, вот наша целевая функция, сумма весов
[52:32.280 --> 52:40.560]  на х и т со звездочкой — это нижняя оценка на оптимум, то у нас и получается, что мат ожидания
[52:40.560 --> 52:46.960]  равняется за один раунд веса вот такой штуки, за логарифмическое число раундов оно равняется
[52:46.960 --> 52:54.080]  вот такой штуке помножить на логарифм n, что не превосходит логарифм n помножить на оптимум. То есть
[52:54.080 --> 52:59.280]  мы опять там пользуемся, конечно, безусловно, тем, что х со звездочкой — это не а во что, а это
[52:59.280 --> 53:04.280]  оптимальное решение задачи не целочисленной, задачи лениного программирования, которая связана
[53:04.280 --> 53:15.240]  с значением оптимального веса покрытия. Ну вот, так что все то же самое, только с тервером. Вот,
[53:15.240 --> 53:19.440]  но мы опять используем основное свойство ленинной релаксации. А теперь мы не будем
[53:19.440 --> 53:24.160]  использовать никакие свойства ленинной релаксации, мы просто с вами жадную евристику обсудим для
[53:24.160 --> 53:44.880]  задачи о покрытии. Жадная евристика. Жадная евристика работает следующим образом.
[53:44.880 --> 53:59.760]  А вы, наверное, сами сейчас опишите, как она работает. Так, чтобы покрыть как можно больше
[53:59.760 --> 54:08.880]  столбцов за раз, да? Окей, эти столбцы надо покрыть. Да, то есть понятно, что раз эти столбцы уже
[54:08.880 --> 54:15.280]  покрыты, нам не надо выбирать строчку, покрывающую наибольшее число среди них. Мы берем наибольшее,
[54:15.280 --> 54:22.720]  еще не покрытое количество столбцов, строчку, покрывающую их и так дальше. А как сюда добавить
[54:22.720 --> 54:29.160]  веса? Теперь вот у нас же задача взвешенная, в общем случае. Значит, сначала давайте я сформулирую
[54:29.160 --> 54:37.400]  вот ваше предложение по жадному алгоритму. Значит, мы берем покрытие, сначала пустое множество,
[54:37.920 --> 55:04.280]  а потом, пока s это не покрытие, мы берем строчку, знаете, argmax обозначение. Это то,
[55:04.280 --> 55:14.080]  на чем достигается максимум. Максимум по i от единички до m. Число столбцов,
[55:14.080 --> 55:28.960]  еще не покрытых множеством s, но покрываемых этой строкой. Число еще не покрытых столбцов,
[55:28.960 --> 55:45.360]  покрываемых этой рокой. Ух, ну я тут и написал, конечно. Здесь такой номер строки,
[55:45.360 --> 55:52.120]  которая покрывает максимальное число еще не покрытых столбцов, еще не покрытых множеством
[55:52.120 --> 55:57.400]  столбцов. Ну, то есть, можете это вообще не писать формально, мне важно, чтобы вы
[55:57.400 --> 56:02.200]  исключительно понимали содержательно. Ну, жадная евристика, она и в Африке жадная евристика.
[56:02.200 --> 56:11.160]  Хотя интересно, в какой стране и на каком континенте живут самые жадные евристики,
[56:11.160 --> 56:18.040]  может не в Африке. Если у нас теперь веса появляются, как же сюда примешать веса? Вот,
[56:18.040 --> 56:26.080]  допустим, у нас есть строка покрывающая, ну скажем, четыре столбца. Вот, но она веса там
[56:26.080 --> 56:37.120]  семь, а есть строка покрывающая три столбца, и она веса, ну скажем, пять. Вот какую из этих строк
[56:37.120 --> 56:55.480]  вы бы взяли? Три пять, почему? Стоимость, да, можно рассмотреть стоимость покрытия одного столбца,
[56:55.480 --> 57:05.760]  да, у какой строки она выше? Пять третих versus семь четвертых. Семь четвертых выше, чем пять
[57:05.760 --> 57:12.240]  третих, да, то есть, вот эта строка в расчете на один покрываемый столбец, она чуть подешевле. За
[57:12.240 --> 57:19.080]  один столбец мы платим меньше. Вот, если все веса были бы одинаковые, то тогда понятно,
[57:19.080 --> 57:26.440]  что выбирать строку самую дешевую, в смысле, стоимость покрытия одного столбца, это то же
[57:26.440 --> 57:31.040]  самое, что выбирать строку просто покрывающую, максимальное количество столбцов. Ну, как только
[57:31.040 --> 57:36.680]  у нас появляется еще вот это вот число стоимости строки, здесь мы можем с вами теперь поставить
[57:36.680 --> 57:45.240]  другую немножко константу, да, что мы максимизируем. Мы максимизируем, да, можно считать, что мы
[57:45.240 --> 57:54.040]  максимизируем. Давайте я минимизацию теперь перепишу, минимальную стоимость покрытия одного
[57:54.040 --> 58:05.060]  столбца. Минимум вес строки поделить на число столбцов. Я уже не буду здесь писать покрываемых
[58:05.060 --> 58:12.580]  этой строку, еще не покрытых раньше. Вот, мы понимаем с вами, что здесь стоит. И еще раз, да,
[58:12.580 --> 58:19.380]  если бы все W были одинаковые, равные единичке, например, то тогда минимизировать вот такую
[58:19.380 --> 58:24.180]  величину, это было бы то же самое, что максимизировать просто тупо число столбцов новых покрываемых.
[58:24.180 --> 58:45.420]  Теорема такая, что вес жадного покрытия
[58:54.180 --> 59:07.100]  не превосходит веса оптимального покрытия, помножить на единица плюс лоноритм числа столбцов.
[59:07.100 --> 59:16.100]  Это получается даже как-то поточнее, чем использовать вероятность на округление. Здесь все-таки O
[59:16.100 --> 59:21.580]  большое, это еще и константа в O большом, какая будет, да, непонятно. А здесь у нас просто единичка.
[59:21.580 --> 59:26.780]  Множитель при логарифме, ну еще вот плюс вот эта единица, но это ерунда, конечно.
[59:26.780 --> 59:35.420]  Видите, как получается, совершенно две разные ивристики, одна на основе линейного
[59:35.420 --> 59:40.380]  программирования, другая жадная, и вот в обеих в итоге вылезает вот этот множитель,
[59:40.380 --> 59:46.700]  неконстантный, логариф Маттен. Ну, что же делать? Значит, видимо, это какая-то природозадача.
[59:46.820 --> 59:50.060]  Может быть, лучше и не получится приближать.
[01:00:02.060 --> 01:00:07.700]  Давайте доказывать начнем, что мы вряд ли успеем сегодня целиком доказать,
[01:00:07.700 --> 01:00:13.820]  но мы хотя бы начнем доказывать, что действительно у нас такая оценка имеется.
[01:00:13.820 --> 01:00:21.900]  И мы с вами введем такие веса на столбцах. Ну, не веса, давайте я их буду называть пометки
[01:00:21.900 --> 01:00:33.660]  на столбцах. Рассмотрим пометки на столбцах. Значит, пусть алгоритм работает на очередном шаге,
[01:00:33.660 --> 01:00:39.500]  у нас выбирается, значит, некая строка, да, и эта строка покрывает несколько новых столбцов,
[01:00:39.500 --> 01:00:46.460]  и вот у нас у этой строки вес Wt. Пусть эти столбцы еще не были покрыты раньше. Давайте
[01:00:46.460 --> 01:00:54.580]  возьмем и над каждым столбцом напишем такую пометку. Wt поделить на три, то есть вот ровно
[01:00:54.580 --> 01:01:04.660]  это число, которое минимизируется при выборе строки, мы напишем на столбце. Wt на три, Wt на три.
[01:01:09.500 --> 01:01:22.380]  Пометка на столбце, пометка на столбце. Давайте мы эти пометки Zg обозначим. Это как раз и есть
[01:01:22.380 --> 01:01:34.940]  Wt поделить на число покрытых столбцов, и эта пометка определяется в момент покрытия столбца,
[01:01:34.940 --> 01:01:51.180]  определяется в момент покрытия столбца жадным алгоритмом, жадным алгоритмом. Вопрос,
[01:01:51.180 --> 01:02:03.340]  к чему будет равна сумма всех этих Z житых? Ну да, понятно, что сумма Z житых,
[01:02:03.340 --> 01:02:11.740]  это сумма позже от 1 до n, Z житых. Это Wt, но сумма вот этих вот конкретно Z житых,
[01:02:11.740 --> 01:02:17.380]  это Wt, то есть вес той строки, которая была выбрана на текущем шаге алгоритма. А если
[01:02:17.380 --> 01:02:27.500]  я теперь просуммирую Zg вообще по всем столбцам матрицы? Да, это и есть вес жадного покрытия.
[01:02:34.140 --> 01:02:38.820]  То есть нам с вами нужно будет просто немножко по-другому поработать с вот этой суммой и
[01:02:38.820 --> 01:02:44.700]  по-другому ее оценить. И мы это сейчас с вами проделаем потихоньку.
[01:02:56.700 --> 01:02:59.900]  Давайте рассмотрим произвольную строчку матрицы.
[01:02:59.900 --> 01:03:07.980]  Осмотрим какую строчку матрицы.
[01:03:07.980 --> 01:03:29.260]  Сумма от 1 до n. Сумма по всем столбцам Z житых. Каждый столбец рано или поздно покроется жадным
[01:03:29.260 --> 01:03:42.100]  алгоритмом. Он же работает до победы. Рассмотрим строчку матрицы и рассмотрим столбцы,
[01:03:42.100 --> 01:03:55.740]  которые покрываются этой строчкой. Пусть s маленькая l и так далее, s маленькая 1,
[01:03:55.740 --> 01:04:09.620]  это столбцы, покрываемые этой строкой. Рассмотрим произвольную строчку матрицы.
[01:04:09.620 --> 01:04:18.780]  Пусть s l и так далее, s 1, это столбцы, которые ей покрываются. Я их довольно странно
[01:04:18.780 --> 01:04:26.660]  занумировал. Зачем-то от конца к началу. Пока так, потом поймем, почему так удобнее нумировать.
[01:04:26.660 --> 01:04:38.980]  Давайте считать, что эти столбцы занумированы в том порядке, в котором они покрываются жадным
[01:04:38.980 --> 01:04:52.700]  алгоритмом. В порядке покрываем жадным алгоритмом. Что это означает? Смотрите,
[01:04:52.700 --> 01:04:58.460]  что я хочу здесь сказать. Вот мы с вами смотрим на строчку, выделяем столбцы,
[01:04:58.460 --> 01:05:07.420]  которые ей покрываются, там где здесь единички. Вот у нас, например, четыре столбца. Жадный
[01:05:07.660 --> 01:05:11.460]  алгоритм, не факт, что он выбирает именно эту строчку. Я вообще сейчас не говорю про строчку
[01:05:11.460 --> 01:05:15.860]  выбираемую непременно жадным алгоритмом. Я просто говорю про произвольную строчку матрицы.
[01:05:15.860 --> 01:05:21.860]  И вот эти столбцы, это не те столбцы, которые там покрыты жадным алгоритмом, когда он выберет
[01:05:21.860 --> 01:05:25.960]  эту строчку. Да нет, это просто все столбцы, на пересечении которых с этой строкой,
[01:05:25.960 --> 01:05:31.960]  стоят единички в матрице. Но все эти столбцы, рано или поздно, покроются же жадным алгоритмом,
[01:05:31.960 --> 01:05:38.680]  правда, потому что он вообще все столбцы матрицы покрывает к окончанию работы. И вот какие-то из
[01:05:38.680 --> 01:05:44.120]  этих столбцов могут покрываться вместе. Например, если бы жадный алгоритм выбрал именно эту строку,
[01:05:44.120 --> 01:05:49.720]  то столбцы все вместе сразу были бы покрыты, все эти за один раз. Но может быть жадный алгоритм
[01:05:49.720 --> 01:05:55.160]  выбирает из своих соображений какие-то другие строки. И, например, сначала покрываются вот
[01:05:55.160 --> 01:06:02.520]  эти вот два столбца. Тогда они у меня окажутся как раз S4 и S3 в любом порядке. А потом, например,
[01:06:02.520 --> 01:06:06.800]  жадный алгоритм выбирает еще какую-то строчку, которая покрывает вот эти два столбца. Ну и вместе
[01:06:06.800 --> 01:06:14.600]  с ними там может еще какие-то. Но тогда у меня дальше будут стоять вот эти вот S1 и S2. То есть
[01:06:14.600 --> 01:06:23.200]  вот это S1 и S2, это S3 и S4. Вот в этой нумерации. Я хочу сказать, что эти столбцы могут покрываться
[01:06:23.200 --> 01:06:28.960]  группами. Не проблема. Жадным алгоритмом они могут покрываться группами. Главное вот в этой
[01:06:28.960 --> 01:06:36.280]  нумерации, что если жадно алгоритм покрыл какой-то столбец позже, строго позже, то он и в этом списке,
[01:06:36.280 --> 01:06:40.880]  я предполагаю, что он стоит строго позже. Ну просто я их так присвоил им номерами. Это же не важно,
[01:06:40.880 --> 01:06:47.920]  в каком порядке я эти числа записал. Вот запишу в таком порядке, который мне удобен. В порядке
[01:06:47.920 --> 01:06:58.080]  покрытие жадным алгоритмом. Давайте посмотрим пристально на шаг жадного алгоритма, на котором он
[01:06:58.080 --> 01:07:15.880]  покрывает ну какой-то кат из столбец. Шаг жадного алгоритма, на котором он покрывает S катом. Вот
[01:07:15.880 --> 01:07:31.120]  кат из столбец этого списка. К началу этого шага этот столбец не покрыт. На вот этом шаге
[01:07:31.120 --> 01:07:36.440]  столбец из ката покрыт. К началу этого шага этот столбец еще не покрыт. Какие столбцы еще
[01:07:36.440 --> 01:07:44.520]  гарантированно не покрыты. Ну с номерами, ну вот в этом списке, да, идущие правее. То есть
[01:07:44.520 --> 01:07:57.520]  S кат, S к-1, S к-2 и так далее, вплоть до S1. К началу этого шага, к началу шага не покрыты,
[01:07:57.520 --> 01:08:15.680]  не покрыты как минимум столбцы S к, S к-1 и так далее S1. О, смотрите, значит к началу этого шага у
[01:08:15.680 --> 01:08:23.560]  нас есть не покрытые как минимум к столбцов в матрице. Если алгоритм выберет вот эту строчку
[01:08:23.560 --> 01:08:36.160]  текущую, давайте мы ее как-нибудь обозначим. R, например, от слова row строка. Если бы алгоритм
[01:08:36.160 --> 01:08:45.520]  выбрал сейчас вот текущую строчку, то он какую стоимость заплатил бы за покрытие вот всех этих
[01:08:45.520 --> 01:08:59.040]  столбцов? Мы просто по определению смотрим сюда. Если бы алгоритм выбрал текущую строчку, если бы
[01:08:59.040 --> 01:09:04.680]  она вдруг минимизировала вот эту вот величину, то алгоритм заплатил бы какую стоимость. Можем
[01:09:04.680 --> 01:09:13.120]  мы как-то оценить? Стоимость строки WR. Количество новых столбцов, которые она бы покрыла, как
[01:09:13.120 --> 01:09:22.560]  минимум к. Значит, стоимость покрытия вот этих вот столбцов у нас была бы WR поделить на к как
[01:09:22.560 --> 01:09:41.200]  максимум. Значит, если бы жадный алгоритм выбрал строку R, да, просто какая строка матрицы,
[01:09:41.280 --> 01:09:45.280]  произвольную. Не обязательно та, которая действительно выбрана жадным алгоритмом,
[01:09:45.280 --> 01:09:53.680]  просто любая из строчек матрицы. Вот, но мы сейчас предполагаем, что давайте еще раз, да,
[01:09:53.680 --> 01:09:58.120]  проговорим этот момент. Рассматриваем произвольную строку матрицы без относительно всякого жадного
[01:09:58.120 --> 01:10:04.480]  алгоритма. А вот столбцы, которые, все столбцы, которые эта строка способна покрыть, мы упорядочим в том
[01:10:04.480 --> 01:10:08.880]  порядке, в котором они покрываются жадным алгоритмом. Поскольку жадный алгоритм все столбцы
[01:10:08.880 --> 01:10:14.960]  матрицы рано или поздно покрывает, то, значит, мы способны упорядочить как-то разумно вот этот
[01:10:14.960 --> 01:10:22.360]  список. Занумируем их просто от L к единице, а не от единицы к L, так нам удобно. Вот, значит,
[01:10:22.360 --> 01:10:30.880]  рассмотрим шаг, на котором покрывается столбец S катой. S катой правее него стоят к столбцов,
[01:10:30.880 --> 01:10:38.520]  включая него самого, да, и все эти столбцы по определению порядка в этом списке они не покрыты
[01:10:38.520 --> 01:10:43.360]  еще к текущему шагу. Но на текущем шаге у них есть шанс быть покрытыми, если жадный алгоритм
[01:10:43.360 --> 01:10:49.440]  выбрал бы вот конкретно эту строку, он тогда бы все оставшиеся столбцы покрыл вот в этом списке.
[01:10:49.440 --> 01:11:00.960]  И стоимость, которую жадный алгоритм бы заплатил, не превосходит WR поделить на K, количество новых
[01:11:00.960 --> 01:11:17.640]  столбцов, покрываемых алгоритмом в знаменателе, а в числителе W этой строки. Да просто так обозначили,
[01:11:17.640 --> 01:11:25.480]  просто какой-то номер K рассмотрели столбца, который покрывается вот на каком-то текущем шаге.
[01:11:25.480 --> 01:11:33.560]  Не факт, что этой строкой, вот в чем дело. Сейчас последний я здесь допишу, вот этот вот кусочек.
[01:11:33.560 --> 01:11:50.720]  Значит, если бы жадный алгоритм выбрал строку R, то заплатил бы не больше WR поделить на K за вот
[01:11:50.720 --> 01:12:01.240]  этот столбец. И за все вот эти столбцы. Но поскольку жадный алгоритм вообще выбирает
[01:12:01.240 --> 01:12:09.440]  минимальную строчку, то мы можем сделать вывод, что вес, который, ну не вес, лейбл,
[01:12:09.440 --> 01:12:16.840]  пометка, которая будет приписана вот этому столбцу SK, она не превосходит точно вот такой вот величины.
[01:12:16.840 --> 01:12:25.400]  Мы не знаем, как бы был ли покрыт столбец SK этой строкой или не этой строкой. Но мы точно знаем,
[01:12:25.400 --> 01:12:34.280]  что если бы он был покрыт этой строкой сейчас, то у него Z была бы вот такой, да, не больше. Но
[01:12:34.280 --> 01:12:40.320]  поскольку жадный алгоритм как можно меньше вообще старается сделать этот Z, то мы получаем такое
[01:12:40.320 --> 01:12:46.360]  неравенство, уж во всяком случае. Есть возможность добиться вот такого результата, но мы ищем еще
[01:12:46.360 --> 01:12:52.320]  более хорошего результата, может, если он есть. Поэтому в итоге Z с индексом SK получается не
[01:12:52.320 --> 01:12:59.300]  больше, чем WR поделить на K. Вот и все на сегодня. И мы с вами уходим на следующий раз в максимальной
[01:12:59.300 --> 01:13:04.240]  непонятности, что произойдет дальше, как мы это вообще используем и как мы сможем оценить все
[01:13:04.240 --> 01:13:09.960]  эти Z-ки и откуда возникает алгоритм. Непонятно. Но мы это узнаем в следующий раз.

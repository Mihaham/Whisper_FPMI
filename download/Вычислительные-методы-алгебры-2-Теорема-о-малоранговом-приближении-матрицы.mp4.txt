[00:00.000 --> 00:04.000]  Редактор субтитров Е.Воинова Корректор А.Кулакова
[00:30.000 --> 00:33.000]  Корректор А.Кулакова
[01:00.000 --> 01:03.000]  Корректор А.Кулакова
[01:30.000 --> 01:33.000]  Корректор А.Кулакова
[02:00.000 --> 02:03.000]  Корректор А.Кулакова
[02:30.000 --> 02:33.000]  Корректор А.Кулакова
[03:00.000 --> 03:03.000]  Корректор А.Кулакова
[03:30.000 --> 03:33.000]  Корректор А.Кулакова
[04:00.000 --> 04:03.000]  Корректор А.Кулакова
[04:30.000 --> 04:33.000]  Корректор А.Кулакова
[05:00.000 --> 05:03.000]  Корректор А.Кулакова
[05:30.000 --> 05:33.000]  Корректор А.Кулакова
[06:00.000 --> 06:03.000]  Корректор А.Кулакова
[06:30.000 --> 06:33.000]  Корректор А.Кулакова
[07:00.000 --> 07:03.000]  Корректор А.Кулакова
[07:30.000 --> 07:33.000]  Корректор А.Кулакова
[08:00.000 --> 08:03.000]  Корректор А.Кулакова
[08:30.000 --> 08:33.000]  Корректор А.Кулакова
[09:00.000 --> 09:03.000]  Корректор А.Кулакова
[09:30.000 --> 09:33.000]  Корректор А.Кулакова
[10:00.000 --> 10:03.000]  Корректор А.Кулакова
[10:30.000 --> 10:33.000]  Корректор А.Кулакова
[11:00.000 --> 11:03.000]  Корректор А.Кулакова
[11:30.000 --> 11:33.000]  Корректор А.Кулакова
[12:00.000 --> 12:03.000]  Корректор А.Кулакова
[12:30.000 --> 12:33.000]  Корректор А.Кулакова
[13:00.000 --> 13:03.000]  Корректор А.Кулакова
[13:30.000 --> 13:33.000]  Корректор А.Кулакова
[13:34.000 --> 13:37.000]  Корректор А.Кулакова
[13:38.000 --> 13:41.000]  Корректор А.Кулакова
[13:42.000 --> 13:45.000]  Корректор А.Кулакова
[13:46.000 --> 13:49.000]  Корректор А.Кулакова
[13:50.000 --> 13:53.000]  Корректор А.Кулакова
[13:54.000 --> 13:57.000]  Корректор А.Кулакова
[13:57.000 --> 14:00.000]  Корректор А.Кулакова
[14:01.000 --> 14:04.000]  Корректор А.Кулакова
[14:05.000 --> 14:08.000]  Корректор А.Кулакова
[14:09.000 --> 14:12.000]  Корректор А.Кулакова
[14:13.000 --> 14:16.000]  Корректор А.Кулакова
[14:17.000 --> 14:20.000]  Корректор А.Кулакова
[14:21.000 --> 14:24.000]  Корректор А.Кулакова
[14:24.000 --> 14:38.000]  единичной длины, а здесь а-b на х, длина лектора а-b умножить на х.
[14:38.000 --> 14:48.000]  То есть это спектральная норма, это операторная норма, но так уж оказывается, она равна старшему шигляновому числу.
[14:48.000 --> 14:58.000]  Теперь, вот мы максимизируем по некоторому множеству, если это единичная сфера.
[14:58.000 --> 15:06.000]  Если мы множество уменьшим, что произойдет с максимумом?
[15:06.000 --> 15:16.000]  А? Ну увеличится. Правильно.
[15:16.000 --> 15:26.000]  А уменьшить, я вот что сделаю, сейчас напишу, как я хочу уменьшить.
[15:27.000 --> 15:35.000]  Только по-такому, только такие х рассматриваю, такие, чтобы b на х ясно.
[15:35.000 --> 15:40.000]  Ну, значит, максимум уменьшится.
[15:40.000 --> 15:49.000]  Ну, зачем я это делаю? А чтобы b исчезло?
[15:49.000 --> 15:55.000]  Я могу произвольно уменьшить множество и максимум уменьшится, правильно?
[15:55.000 --> 16:02.000]  Я хочу, чтобы b исчезло. Я в точности сказал свою задумку.
[16:02.000 --> 16:12.000]  Я хочу, чтобы здесь не было... Нет, почему можно? Ещё раз. Можно взять любое меньшее множество.
[16:12.000 --> 16:21.000]  Да, любое можно взять. Вот я и возьму некоторые поменьше, но с идеей, чтобы b у меня исчезло.
[16:21.000 --> 16:31.000]  Вот здесь. Нет здесь b. Множество меньше.
[16:31.000 --> 16:41.000]  А что это за множество такое? Вот сейчас мы тоже немножко вспоминаем.
[16:41.000 --> 16:50.000]  Вот вы знаете, как это называется. Это называется ядро матрицы B.
[16:50.000 --> 17:01.000]  Ядро матрицы B. И размерность этого ядра, размерность, то есть ядро матрицы B от лейной подпространства.
[17:01.000 --> 17:09.000]  Ну, это уж наверняка каждый понимает почему. Подпространство, значит, там можно говорить о размерности.
[17:09.000 --> 17:16.000]  Размерность ядра, кстати, в теории матрицы называется дефектом матрицы.
[17:16.000 --> 17:25.000]  Ранг матрицы – это размерность образа этой матрицы. Дефект – это размерность ядра.
[17:25.000 --> 17:34.000]  Вот, если вы сложите дефекты ранг, ну, почти что один из основных результатов, связанных с системой нелинейной алгебритики.
[17:34.000 --> 17:44.000]  Что получится? Число столбцов матрицы.
[17:44.000 --> 17:57.000]  Значит, размерность ядра – это есть n минус ранг.
[17:57.000 --> 18:17.000]  А ранг у нас меньше или равен k, так? Значит, это больше или равно, чем n минус k. Согласно, да?
[18:17.000 --> 18:25.000]  N минус k. Снизу оценка на размерность.
[18:25.000 --> 18:34.000]  Вот, а теперь я хочу взять еще одно подпространство.
[18:34.000 --> 18:48.000]  Вот с бухты Барахты послепит еще одно подпространство.
[18:48.000 --> 18:57.000]  А это будет линейная оболочка. Тоже знакомая, да? Линейная оболочка, натянутая в систему векторов. На какую систему векторов?
[18:57.000 --> 19:09.000]  У нас сингулярные векторы есть. Вот я на них и натяну. Возьму k плюс один сингулярный вектор и натяну на них линейную оболочку.
[19:09.000 --> 19:19.000]  Это будет подпространство m. Ну, а ядро пусть это будет l. Два подпространства – l и m.
[19:19.000 --> 19:29.000]  Размерность l не меньше, чем n минус k. Размерность m вы мне сейчас посчитаете, да?
[19:29.000 --> 19:39.000]  Чему равна размерность m? Молодцы! Правильно?
[19:39.000 --> 19:51.000]  А теперь еще один факт линейной алгебры. Знаете, не знаете? Вот такая есть арема чудесная.
[19:51.000 --> 20:05.000]  Размерность суммы двух линейных подпространств. Ну, сумма как устроена? Это всевозможные суммы векторов из l и из m. Это будет тоже линейное подпространство.
[20:05.000 --> 20:17.000]  И размерность можно вычислять так. Надо вычислить размерность l, прибавить размерность m и вычесть размерность пересечения.
[20:17.000 --> 20:27.000]  Ну, пересечение, очевидно, будет тоже линейным подпространством. Вот такая формула. У нее даже есть имя. Это формула Грасмана.
[20:27.000 --> 20:45.000]  Не знаю, встречалось, не встречалось. Я вот когда объясняю эту формулу, я люблю говорить о том, как я бы объяснял эту формулу выпускникам детского сада.
[20:45.000 --> 20:55.000]  На пальцах совершенно. В том, что я сейчас скажу, фактически доказательство есть. А что понятно выпускникам детского сада?
[20:55.000 --> 21:01.000]  Вот у вас две кучки яблок. Одна кучка и еще одна. Но они пересекают.
[21:01.000 --> 21:07.000]  Знаешь, сколько всего яблок? Число яблок в одной кучке, во второй минус число яблок в пересечении.
[21:07.000 --> 21:20.000]  Но здесь, когда вы будете эту теорему Грасмана доказывать, роль яблока играют векторы базисов. Векторы базисов в l, в пересечение, в l и в m.
[21:20.000 --> 21:26.000]  То есть так можно собрать их. И вот то, что в пересечение, это будут вот эти общие яблоки.
[21:26.000 --> 21:33.000]  Ну, примерно так. Хорошо, что знакомы эти ариамы. Вот давайте мы ее применим.
[21:33.000 --> 21:42.000]  Давайте мы ее применим. Только применим вот как мы. l и m это те самые l и m, которые у нас возникли.
[21:42.000 --> 21:56.000]  Вот посчитаем размерность пересечения. Значит, это есть размерность l. Размерность l у нас снизу оценена как m-k.
[21:56.000 --> 22:06.000]  Плюс размерность m, а это есть k плюс 1. Вот минус размерность суммы.
[22:06.000 --> 22:12.000]  Ну, размерность суммы не больше, чем размерность всего пространства. А это n, да?
[22:12.000 --> 22:18.000]  Значит, минус. Смело можем вычесть n. И что здесь получилось в результате?
[22:18.000 --> 22:30.000]  1. А что это значит? А это значит, что в пересечении под пространство m имеется не нулевой вектор.
[22:31.000 --> 22:38.000]  Это не нулевой подпространство. Вообще есть нулевой вектор z.
[22:38.000 --> 22:46.000]  Ну, всегда можно считать, что z имеет единичную длину. Такой вектор z.
[22:46.000 --> 22:56.000]  Вот следствие теоремы Грасмана. И выбора подпространства l и m.
[22:56.000 --> 23:04.000]  Это линейная оболочка. Ну, можно с pen написать. Ну, по-русски.
[23:04.000 --> 23:10.000]  Мог бы писать рукописную l, но рукописную l у многих вызывает вопросы, что это такое.
[23:14.000 --> 23:20.000]  Значит, вот z появился. Есть такой вектор z. А здесь максимум.
[23:21.000 --> 23:30.000]  А z принадлежит к ядру матрицы b? Принадлежит. Он и l принадлежит, и m? Ну, и l принадлежит.
[23:30.000 --> 23:46.000]  Значит, я могу написать. Правильно? Максимум же он больше, чем значение на каком-то отдельном векторе.
[23:46.000 --> 23:56.000]  З имеет единичную длину, аж вот так. Ну, а теперь смотрите ход мыслей, как продолжается.
[24:03.000 --> 24:05.000]  Ну, не по z, конечно.
[24:16.000 --> 24:26.000]  Ну, написал очевидную вещь. Правильно? z принадлежит m.
[24:26.000 --> 24:35.000]  И значит, если мы по большему, по множеству ям будем минимизировать, то получим не больше, чем вот это значение.
[24:35.000 --> 24:45.000]  Правильно? А вот этот минимум легко вычисляется. Остается вот этот минимум вычислять. Сейчас мы это сделаем.
[24:45.000 --> 25:13.000]  Ну, что надо сделать? Надо... x принадлежит m. Ну, что это значит? Значит, x раскладывается по каким векторам?
[25:15.000 --> 25:39.000]  По первым каплю с единичкой векторам u. Ну, с какими-то коэффициентами. Вот, дальше подействовали батрицы a на этот вектор x умножили.
[25:39.000 --> 25:41.000]  А что будет?
[25:45.000 --> 25:53.000]  Значит, x а, а здесь а. А что такое а у а?
[25:59.000 --> 26:04.000]  А это, помните, это sigma alpha умножить на v alpha.
[26:09.000 --> 26:29.000]  Ну, вектор v alpha образует ортонормированную систему. Значит, длина этого вектора вычисляется как корень квадратный из суммы коэффициентов в квадрате.
[26:29.000 --> 26:31.000]  Что?
[26:33.000 --> 26:35.000]  Как фиксировано?
[26:35.000 --> 26:54.000]  Нет, ну, sigma, если sigma больше, чем r plus 1, то sigma равно 0. Если sigma больше, то r plus 1.
[26:54.000 --> 27:04.000]  Ну, мы заведомо k выбираем не больше, чем r, правда же? Иначе тривиальная будет задача.
[27:05.000 --> 27:21.000]  Значит, длину этого вектора легко посчитать. Это из корень квадратный из суммы вот этих вот координат в квадрате.
[27:24.000 --> 27:26.000]  Правильно?
[27:28.000 --> 27:30.000]  Ну, что же это за сумма такая?
[27:32.000 --> 27:44.000]  Здесь можно sigma alpha вытащить, а здесь sigma вообще положительные числа.
[27:45.000 --> 27:49.000]  А вот это координаты, вообще говоря, комплексные.
[27:50.000 --> 27:54.000]  Ну, вот так вот. И самая большая sigma здесь какая?
[27:55.000 --> 27:57.000]  Sigma 1, а самая маленькая?
[28:00.000 --> 28:06.000]  Значит, совершенно очевидно, что мы можем все эти sigma на sigma k plus 1 заменить и получим меньше.
[28:14.000 --> 28:16.000]  А вот это чему равно?
[28:19.000 --> 28:35.000]  Это длина вектора х, а длина вектора х равна 1. То есть это есть sigma k plus 1.
[28:37.000 --> 28:39.000]  Ну, что произошло?
[28:39.000 --> 28:41.000]  Ну, что произошло?
[28:43.000 --> 28:50.000]  Значит, мы доказали, что какую бы матрицу B, какую бы матрицу B ранка, которая не выше ка, мы взяли, мы не взяли,
[28:52.000 --> 28:58.000]  спектральная норма матрицы A-B больше или равна, чем sigma k plus 1?
[28:59.000 --> 29:01.000]  Теорема заказана.
[29:02.000 --> 29:06.000]  Ну, правда, часть теоремы та, которая связана со спектральной нормой.
[29:09.000 --> 29:11.000]  Ну, что, получили удовольствие?
[29:12.000 --> 29:14.000]  Мне, конечно, вполне такие элегантные рассуждения.
[29:15.000 --> 29:17.000]  Ну, есть еще вторая часть.
[29:18.000 --> 29:20.000]  С нормой фробениуса.
[29:21.000 --> 29:23.000]  Давай это ее попробуем тоже.
[29:27.000 --> 29:29.000]  Спасибо.
[29:30.000 --> 29:31.000]  Да.
[29:32.000 --> 29:34.000]  Ну, вот эти вот, вот эти можно.
[29:35.000 --> 29:37.000]  А действительно, жидкость.
[29:39.000 --> 29:42.000]  Нет, не все. Формула фотоставки, вот эта вот.
[29:48.000 --> 29:51.000]  Мы сейчас доказательство теоремы пока продолжаем.
[29:52.000 --> 29:58.000]  А вот это вот, в утверждении теоремы второе равно sigma k plus 1.
[29:59.000 --> 30:01.000]  Нет, это выше, выше.
[30:02.000 --> 30:04.000]  Это мы получили за прошлой лекции.
[30:05.000 --> 30:07.000]  Это вам как упражнение, однако, предлагалось.
[30:07.000 --> 30:09.000]  Ну, это факт.
[30:10.000 --> 30:12.000]  Значит, тут, как это получать?
[30:13.000 --> 30:17.000]  Ну, во-первых, спектральная норма равняется старшему сингулярному числу.
[30:18.000 --> 30:20.000]  Ну, вот, вам тоже надо было бы упражняться. Почему, да?
[30:21.000 --> 30:23.000]  Если вы поняли уже, что равно.
[30:24.000 --> 30:26.000]  Значит, вот есть а-к.
[30:27.000 --> 30:29.000]  А для а-к сингулярное это разложение легко написать.
[30:30.000 --> 30:32.000]  Там будут какие сингулярные числа?
[30:33.000 --> 30:36.000]  Sigma k plus 1 старше, sigma k plus 2, и sigma k plus 1 старше.
[30:37.000 --> 30:39.000]  Значит, ответ sigma k plus 1.
[30:40.000 --> 30:42.000]  Вот и все.
[30:46.000 --> 30:51.000]  Ну, фактически, здесь надо опираться на то, что при умножении на унитарную матрицу
[30:52.000 --> 30:55.000]  спектральная норма не меняется. Умножать ее слева или справа.
[30:56.000 --> 30:58.000]  Ну, кстати, норма Фробениуса тоже не меняется.
[30:59.000 --> 31:02.000]  И это дает возможность вычислить норму Фробениуса для матрица.
[31:03.000 --> 31:05.000]  Просто надо норму от sigma посчитать.
[31:05.000 --> 31:07.000]  Норму Фробениусу матрицу.
[31:08.000 --> 31:10.000]  Спасибо большое.
[31:11.000 --> 31:13.000]  Итак, вот вторая часть.
[31:14.000 --> 31:16.000]  Вторая часть.
[31:17.000 --> 31:23.000]  Ну, здесь мы берем по-прежнему производную матрицу B.
[31:24.000 --> 31:27.000]  Только же ранг меньше или равен k.
[31:35.000 --> 31:37.000]  Вот и пишем.
[31:46.000 --> 31:48.000]  А минус B нас интересует.
[31:55.000 --> 31:56.000]  Норма Фробениуса.
[31:57.000 --> 31:59.000]  Помните, что такое норма Фробениуса?
[32:00.000 --> 32:03.000]  Корень квадратной суммы модулей элементов в квадрате.
[32:05.000 --> 32:14.000]  И только что мы вспоминали, что если мы умножим матрицу на произвольную унитарную матрицу, то норма Фробениуса не изменится.
[32:15.000 --> 32:17.000]  Вспоминали?
[32:18.000 --> 32:22.000]  Давайте возьмем произвольную унитарную матрицу X.
[32:25.000 --> 32:28.000]  И тогда я пишу вот так вот.
[32:36.000 --> 32:39.000]  Ну, верно, да?
[32:40.000 --> 32:42.000]  А теперь выберем X с умом.
[32:43.000 --> 32:45.000]  С умом.
[32:46.000 --> 32:49.000]  Выберем унитарную матрицу X с некоторым специальным образом.
[32:52.000 --> 32:58.000]  Ну, у меня та же навязчивая идея здесь убрать B.
[33:01.000 --> 33:02.000]  Избавиться от B.
[33:02.000 --> 33:04.000]  Ну, раньше тоже избавлялись от B.
[33:05.000 --> 33:07.000]  И сейчас давайте.
[33:08.000 --> 33:10.000]  Так, ну давайте так вот.
[33:11.000 --> 33:13.000]  Я выберу.
[33:14.000 --> 33:17.000]  Вот есть такое ядро матрицы B.
[33:19.000 --> 33:21.000]  Размерность этого ядра.
[33:22.000 --> 33:26.000]  Ну, давайте я как-нибудь обозначу размерность этого ядра.
[33:27.000 --> 33:31.000]  Через буквку какую-нибудь.
[33:33.000 --> 33:35.000]  Нет, M занято.
[33:36.000 --> 33:38.000]  Буквок нет никаких.
[33:39.000 --> 33:41.000]  Ну, что? T.
[33:44.000 --> 33:47.000]  И T у нас снизу имеет оценку.
[33:48.000 --> 33:50.000]  Я их не понимаю.
[33:51.000 --> 33:53.000]  Вот.
[33:56.000 --> 33:58.000]  Ну, что мы сделаем?
[33:59.000 --> 34:03.000]  А мы выберем вот в этом пространстве.
[34:04.000 --> 34:06.000]  Артонормированный базис.
[34:08.000 --> 34:10.000]  Из векторов X1.
[34:11.000 --> 34:13.000]  И так далее, Xt.
[34:16.000 --> 34:18.000]  Значит, это базис.
[34:20.000 --> 34:23.000]  И достроим этот базис.
[34:24.000 --> 34:26.000]  До артонормированного базиса.
[34:28.000 --> 34:30.000]  Всего инмерного пространства.
[34:32.000 --> 34:34.000]  И получим матрицу X.
[34:35.000 --> 34:38.000]  Собрав в нее столбцы, которые мы построили.
[34:39.000 --> 34:41.000]  Вот эти векторы X1, X2 и так далее.
[34:42.000 --> 34:44.000]  Получится унитарная матрица X.
[34:49.000 --> 34:52.000]  Вот. Ну, это вот унитарная матрица X.
[34:53.000 --> 34:55.000]  Она устроена даже так вот.
[34:55.000 --> 34:57.000]  Здесь матрица, давайте Xt напишем.
[34:58.000 --> 35:00.000]  Xt содержит только вот t векторов.
[35:01.000 --> 35:03.000]  От первого до вектора t.
[35:04.000 --> 35:06.000]  Xt.
[35:07.000 --> 35:09.000]  Значит, вот это первые столбцы.
[35:10.000 --> 35:12.000]  И если там...
[35:13.000 --> 35:15.000]  Давайте здесь сим.
[35:16.000 --> 35:18.000]  X с домиком это оставшийся вектор.
[35:19.000 --> 35:21.000]  Понятно. То есть вот два блока.
[35:22.000 --> 35:26.000]  Здесь t векторов, здесь n минус t векторов остался.
[35:33.000 --> 35:35.000]  Вот. И, конечно же,
[35:36.000 --> 35:38.000]  конечно же,
[35:39.000 --> 35:41.000]  я теперь могу вот так написать.
[35:52.000 --> 35:54.000]  Ну, посмотрите внимательно.
[35:55.000 --> 35:57.000]  Если вы мне скажете, о, это очевидно, мы продолжим.
[36:00.000 --> 36:02.000]  Не очевидно, конечно.
[36:04.000 --> 36:06.000]  Ну, раз кто-то говорит не очевидно,
[36:07.000 --> 36:08.000]  придется доказать.
[36:09.000 --> 36:11.000]  Ну, что такое норма фробениуса?
[36:12.000 --> 36:14.000]  Взяли длину каждого столбца в квадрате и все сложили.
[36:15.000 --> 36:17.000]  Вот здесь мы...
[36:18.000 --> 36:20.000]  Вот t столбцов их сложили.
[36:21.000 --> 36:23.000]  Это плюс первый, это плюс первый.
[36:24.000 --> 36:26.000]  Ну, теперь стало очевидно.
[36:27.000 --> 36:29.000]  А?
[36:30.000 --> 36:32.000]  Вот.
[36:33.000 --> 36:35.000]  Ну, объясните.
[36:36.000 --> 36:38.000]  Он не понимает, что такое норма фробениуса.
[36:39.000 --> 36:41.000]  Вот у вас столбцы матрицы,
[36:42.000 --> 36:44.000]  норма фробениуса, это сумма,
[36:45.000 --> 36:47.000]  квадрат нормы фробениуса матрицы,
[36:48.000 --> 36:50.000]  это сумма квадратов длины ее столбцов.
[36:51.000 --> 36:53.000]  Ну, если вы возьмете часть столбцов,
[36:54.000 --> 36:56.000]  эта сумма уменьшится.
[36:57.000 --> 36:59.000]  Вот так вот.
[37:00.000 --> 37:02.000]  Очевидно, что вы там голову оборачиваете.
[37:07.000 --> 37:09.000]  А b-то теперь исчезло.
[37:10.000 --> 37:12.000]  Правда? Потому что b на xt это новость.
[37:14.000 --> 37:18.000]  Мы же взяли вектор x1, xt в ядре матрицы b.
[37:21.000 --> 37:23.000]  Вот так вот.
[37:33.000 --> 37:35.000]  Вот так вот.
[37:46.000 --> 37:48.000]  Столбцы матрицы xt, первые t столбцов,
[37:48.000 --> 37:50.000]  они в ядре матрицы b.
[37:51.000 --> 37:53.000]  Мы так выбирали.
[37:57.000 --> 37:59.000]  Вот. Ну, что это такое написано?
[38:01.000 --> 38:03.000]  Можно вот так написать.
[38:04.000 --> 38:06.000]  Это есть след?
[38:07.000 --> 38:09.000]  Какой матрице?
[38:10.000 --> 38:12.000]  a xt сопряженная,
[38:13.000 --> 38:15.000]  а xt,
[38:16.000 --> 38:18.000]  правильно?
[38:19.000 --> 38:21.000]  Ну, только корень еще.
[38:24.000 --> 38:26.000]  Ну, давайте, посмотрите.
[38:27.000 --> 38:29.000]  Опять кто-то скажет очевидно, кто-то скажет нет.
[38:32.000 --> 38:34.000]  Ну, да.
[38:37.000 --> 38:39.000]  Нет, ну, не очевидно.
[38:40.000 --> 38:42.000]  Ну, вообще, что такое норма Фрабениуса?
[38:42.000 --> 38:44.000]  То есть, общая формула такая,
[38:44.000 --> 38:46.000]  квадрат нормы Фрабениуса это след
[38:46.000 --> 38:48.000]  вот такой матрицы.
[38:52.000 --> 38:54.000]  Это сумма квадратов элементов матрицы a.
[38:55.000 --> 38:57.000]  Так это и есть норма Фрабениуса в квадрате.
[39:02.000 --> 39:04.000]  То есть, вот этот след,
[39:04.000 --> 39:06.000]  как вы правильно сказали,
[39:06.000 --> 39:08.000]  это сумма квадратов всех элементов.
[39:08.000 --> 39:10.000]  А, точно.
[39:10.000 --> 39:12.000]  Это сумма Фрабениуса по определению.
[39:13.000 --> 39:15.000]  Так, теперь вот смотрим на этот самый.
[39:17.000 --> 39:19.000]  Ну, понятное, преобразование очевидное.
[39:23.000 --> 39:25.000]  Вот такое.
[39:33.000 --> 39:35.000]  Это очевидно, да?
[39:35.000 --> 39:37.000]  Скопки просто поставил по-другому.
[39:41.000 --> 39:43.000]  Вот.
[39:47.000 --> 39:49.000]  Вот здесь
[39:49.000 --> 39:51.000]  замечательная матрица возникла.
[39:54.000 --> 39:56.000]  Замечательная матрица
[39:56.000 --> 39:58.000]  а со звездой на а.
[40:00.000 --> 40:02.000]  Что вам об этой матрице
[40:02.000 --> 40:04.000]  можете хорошего сказать?
[40:06.000 --> 40:08.000]  Само сопряженная
[40:08.000 --> 40:10.000]  или, как говорят Эрмитова,
[40:10.000 --> 40:12.000]  молодцы.
[40:12.000 --> 40:14.000]  А еще, знаете,
[40:14.000 --> 40:16.000]  еще не отрицательно определенная.
[40:16.000 --> 40:18.000]  Помните, что это такое?
[40:20.000 --> 40:22.000]  То есть, это само сопряженная
[40:22.000 --> 40:24.000]  или Эрмитовая, и к тому же
[40:24.000 --> 40:26.000]  еще не отрицательно определенная матрица.
[40:26.000 --> 40:28.000]  А что вы знаете о собственных значениях
[40:28.000 --> 40:30.000]  Эрмитовой матрицы?
[40:30.000 --> 40:32.000]  Нет.
[40:32.000 --> 40:34.000]  Они вещественные.
[40:34.000 --> 40:36.000]  А если
[40:36.000 --> 40:38.000]  Эрмитовая матрица не отрицательно
[40:38.000 --> 40:40.000]  определена,
[40:40.000 --> 40:42.000]  как в нашем случае, то они не отрицательны.
[40:44.000 --> 40:46.000]  Ладно.
[40:46.000 --> 40:48.000]  Значит, вещественные собственные значения.
[40:50.000 --> 40:52.000]  Вот.
[40:52.000 --> 40:54.000]  Ну а теперь
[40:54.000 --> 40:56.000]  давайте-ка я
[40:56.000 --> 40:58.000]  вместо вот этой матрицы
[40:58.000 --> 41:00.000]  рассмотрю вот такую.
[41:06.000 --> 41:08.000]  Рассмотрю вот такую.
[41:10.000 --> 41:12.000]  Это, знаете,
[41:12.000 --> 41:14.000]  буквкой H обозначу.
[41:14.000 --> 41:16.000]  Эта матрица по-прежнему будет
[41:16.000 --> 41:18.000]  Эрмитовой, не отрицательно определенной.
[41:18.000 --> 41:20.000]  Правильно?
[41:20.000 --> 41:22.000]  Х-то унитарные матрицы.
[41:22.000 --> 41:24.000]  Значит, вот эта матрица H,
[41:26.000 --> 41:28.000]  как собственные значения
[41:28.000 --> 41:30.000]  матрицы H
[41:30.000 --> 41:32.000]  связаны с собственными значениями
[41:32.000 --> 41:34.000]  матрицы Эрмитовой?
[41:34.000 --> 41:36.000]  Газаны с собственными значениями матрицы
[41:36.000 --> 41:38.000]  со звездой умножена.
[41:40.000 --> 41:42.000]  Вот он контрольный,
[41:42.000 --> 41:44.000]  это вопрос.
[41:44.000 --> 41:46.000]  Правильно. Совпадает. Почему?
[41:48.000 --> 41:50.000]  Потому что
[41:50.000 --> 41:52.000]  х со звездой это х-1.
[41:54.000 --> 41:56.000]  То есть это есть подобные матрицы,
[41:56.000 --> 41:58.000]  а подобные матрицы имеют одинаковые характеристические значения.
[42:00.000 --> 42:02.000]  Так?
[42:02.000 --> 42:04.000]  Совпадает.
[42:04.000 --> 42:06.000]  Значит, вот H,
[42:06.000 --> 42:08.000]  собственные значения матрицы H,
[42:08.000 --> 42:10.000]  это квадраты сингулярных чисел
[42:10.000 --> 42:12.000]  нашей матрицы А.
[42:12.000 --> 42:14.000]  Правильно?
[42:14.000 --> 42:16.000]  Хорошо.
[42:16.000 --> 42:18.000]  А теперь
[42:18.000 --> 42:20.000]  ну, ведь
[42:20.000 --> 42:22.000]  х
[42:22.000 --> 42:24.000]  теперь давайте вот так вот напишу H.
[42:28.000 --> 42:30.000]  У подобных матриц совпадает.
[42:32.000 --> 42:34.000]  То есть
[42:36.000 --> 42:38.000]  подобные матрицы
[42:38.000 --> 42:40.000]  имеют одинаковые собственности значения,
[42:40.000 --> 42:42.000]  а х со звездой это х-1,
[42:42.000 --> 42:44.000]  поскольку их сумметарная матрица.
[42:44.000 --> 42:46.000]  То есть это подобие.
[42:46.000 --> 42:48.000]  А теперь я вот умножаю
[42:48.000 --> 42:50.000]  вот это х,
[42:50.000 --> 42:52.000]  я могу х так записать.
[42:52.000 --> 42:54.000]  Ну а здесь вот так вот.
[42:56.000 --> 42:58.000]  Правильно?
[42:58.000 --> 43:00.000]  Значит, естественно
[43:00.000 --> 43:02.000]  результат тоже
[43:02.000 --> 43:04.000]  некоторую блочную форму приобретает.
[43:12.000 --> 43:14.000]  Правильно.
[43:14.000 --> 43:16.000]  Нет, нет.
[43:16.000 --> 43:18.000]  Это правильно.
[43:18.000 --> 43:20.000]  Это вы правильно сказали.
[43:20.000 --> 43:22.000]  Я не хочу, я хочу со звездой.
[43:24.000 --> 43:26.000]  Вот, и здесь вот,
[43:26.000 --> 43:28.000]  и вот матрица, которая здесь
[43:28.000 --> 43:30.000]  возникает, это в точности вот эта вот матрица.
[43:30.000 --> 43:32.000]  Правильно?
[43:34.000 --> 43:36.000]  Ну давайте я ее через Ht обозначу.
[43:40.000 --> 43:42.000]  Значит, вот эта матрица,
[43:42.000 --> 43:44.000]  вот эта матрица, это H есть.
[43:44.000 --> 43:46.000]  А вот этот блок,
[43:46.000 --> 43:48.000]  как он получается?
[43:48.000 --> 43:50.000]  хt со звездой
[43:50.000 --> 43:52.000]  на матрицу А со звездой А на хt.
[43:52.000 --> 43:54.000]  Ну а здесь какие-то будут блоки,
[43:54.000 --> 43:56.000]  я их вычислять не буду.
[43:56.000 --> 43:58.000]  Вот эта матрица,
[43:58.000 --> 44:00.000]  Эрмитова матрица,
[44:00.000 --> 44:02.000]  а Ht,
[44:02.000 --> 44:04.000]  это есть ее
[44:04.000 --> 44:06.000]  под матрица.
[44:06.000 --> 44:08.000]  Причем под матрица, расположенная
[44:08.000 --> 44:10.000]  в левом верхнем углу.
[44:12.000 --> 44:14.000]  Термин есть такой,
[44:14.000 --> 44:16.000]  если под матрица, расположенная в левом верхнем углу,
[44:16.000 --> 44:18.000]  то она называется ведущая.
[44:18.000 --> 44:20.000]  Давайте договоримся,
[44:20.000 --> 44:22.000]  мы еще может быть
[44:22.000 --> 44:24.000]  поиспользуем этот термин.
[44:24.000 --> 44:26.000]  Ведущая под матрицу.
[44:30.000 --> 44:32.000]  Значит,
[44:32.000 --> 44:34.000]  вот здесь, вот эта величина,
[44:34.000 --> 44:36.000]  что это такое?
[44:36.000 --> 44:38.000]  Это сумма квадратов
[44:38.000 --> 44:40.000]  собственных значений
[44:40.000 --> 44:42.000]  матрицы Ht.
[44:42.000 --> 44:44.000]  Просто сумма собственных значений
[44:44.000 --> 44:46.000]  матрицы Ht.
[44:46.000 --> 44:48.000]  Сумма квадратов сингулярных чисел,
[44:48.000 --> 44:50.000]  для матрицы А на хt.
[44:50.000 --> 44:52.000]  А для матрицы Ht
[44:52.000 --> 44:54.000]  это просто сумма ее собственных значений.
[44:58.000 --> 45:00.000]  А сумма собственных значений
[45:00.000 --> 45:02.000]  матрицы Ht, это есть в точности
[45:02.000 --> 45:04.000]  фробениусовая норма матрицы Ht.
[45:12.000 --> 45:14.000]  Нет, нет, нет.
[45:14.000 --> 45:16.000]  Еще раз.
[45:16.000 --> 45:18.000]  Вот это что такое?
[45:18.000 --> 45:20.000]  Это норма фробениуса.
[45:22.000 --> 45:24.000]  Нет, ну это след,
[45:24.000 --> 45:26.000]  но это в результате вот это...
[45:26.000 --> 45:28.000]  Вы же знаете, что это норма фробениуса
[45:28.000 --> 45:30.000]  вот такой матрицы.
[45:30.000 --> 45:32.000]  А, ну вот оно и написано.
[45:32.000 --> 45:34.000]  Вот это.
[45:34.000 --> 45:36.000]  Норма фробениуса.
[45:36.000 --> 45:38.000]  А норма фробениуса
[45:38.000 --> 45:40.000]  это корень квадрата
[45:40.000 --> 45:42.000]  и сумма квадратов сингулярных чисел.
[45:42.000 --> 45:44.000]  А сингулярные числа
[45:44.000 --> 45:46.000]  в квадрате
[45:46.000 --> 45:48.000]  это собственные значения
[45:48.000 --> 45:50.000]  вот этой матрицы,
[45:50.000 --> 45:52.000]  которая через Ht обозначена.
[45:52.000 --> 45:54.000]  Значит, сумма собственных значений
[45:54.000 --> 45:56.000]  матрицы Ht
[45:56.000 --> 45:58.000]  это в точности квадрат
[45:58.000 --> 46:00.000]  вот этой нормы.
[46:04.000 --> 46:06.000]  Значит, сумма
[46:06.000 --> 46:08.000]  ну сумма собственных значений
[46:08.000 --> 46:10.000]  это и след.
[46:10.000 --> 46:12.000]  То есть след
[46:12.000 --> 46:14.000]  матрицы Ht, это сумма ее собственных значений.
[46:14.000 --> 46:16.000]  И это есть вот
[46:16.000 --> 46:18.000]  квадрат
[46:18.000 --> 46:20.000]  вот этой самой нормы фробениуса.
[46:26.000 --> 46:28.000]  Вот, а
[46:28.000 --> 46:30.000]  след матрицы H
[46:30.000 --> 46:32.000]  это
[46:32.000 --> 46:34.000]  квадрат фробениусовой нормы
[46:34.000 --> 46:36.000]  исходной матрицы A.
[46:38.000 --> 46:40.000]  Ну и теперь вопрос.
[46:40.000 --> 46:42.000]  Знаете вы это?
[46:42.000 --> 46:44.000]  Из вашего курса
[46:44.000 --> 46:46.000]  или этого не было.
[46:46.000 --> 46:48.000]  Как связаны
[46:50.000 --> 46:52.000]  собственные значения
[46:52.000 --> 46:54.000]  ведущей под матрицы
[46:54.000 --> 46:56.000]  с собственными значениями
[46:56.000 --> 46:58.000]  всей матрицы
[46:58.000 --> 47:00.000]  в случае, когда
[47:00.000 --> 47:02.000]  вот эта матрица вся является
[47:02.000 --> 47:04.000]  эрмитовой.
[47:04.000 --> 47:06.000]  Какая здесь связь?
[47:06.000 --> 47:08.000]  Значит, эта связь, это одно
[47:08.000 --> 47:10.000]  из замечательных совершенно
[47:10.000 --> 47:12.000]  наблюдений, известных в теории
[47:12.000 --> 47:14.000]  матрицы.
[47:16.000 --> 47:18.000]  Молчание говорит, что
[47:18.000 --> 47:20.000]  не знаете.
[47:20.000 --> 47:22.000]  Было, ну смотрите.
[47:26.000 --> 47:28.000]  Не-не-не.
[47:28.000 --> 47:30.000]  Вот сейчас.
[47:30.000 --> 47:32.000]  Это вот могу
[47:32.000 --> 47:34.000]  здесь я могу
[47:34.000 --> 47:36.000]  уже стирать.
[47:38.000 --> 47:40.000]  Значит, вот мы имеем
[47:40.000 --> 47:42.000]  возможность
[47:42.000 --> 47:44.000]  вспомнить
[47:46.000 --> 47:48.000]  кое-что из
[47:48.000 --> 47:50.000]  теории матрицы.
[47:50.000 --> 47:52.000]  Еще.
[47:52.000 --> 47:54.000]  Вот смотрите.
[47:54.000 --> 47:56.000]  Вот в нашей теории
[47:56.000 --> 47:58.000]  два утверждения.
[47:58.000 --> 48:00.000]  И мы в общем-то разные
[48:00.000 --> 48:02.000]  инструменты применяем
[48:02.000 --> 48:04.000]  из теории матрицы.
[48:04.000 --> 48:06.000]  Значит, есть
[48:06.000 --> 48:08.000]  замечательный раздел
[48:08.000 --> 48:10.000]  в матричном анализе
[48:10.000 --> 48:12.000]  теории эрмитовых матриц.
[48:12.000 --> 48:14.000]  В этом самом спектральной теории
[48:14.000 --> 48:16.000]  в общем-то речь о
[48:16.000 --> 48:18.000]  собственных значениях эрмитовых матриц.
[48:18.000 --> 48:20.000]  Вот у них столько интересных
[48:20.000 --> 48:22.000]  свойств.
[48:22.000 --> 48:24.000]  Масса.
[48:24.000 --> 48:26.000]  Вот одно из свойств
[48:26.000 --> 48:28.000]  это связь есть
[48:28.000 --> 48:30.000]  с эрмитовой квадратичными
[48:30.000 --> 48:32.000]  формами.
[48:32.000 --> 48:34.000]  Ну наверняка об этой связи вам
[48:34.000 --> 48:36.000]  что-то говорили.
[48:36.000 --> 48:38.000]  И вот есть такое понятие для эрмитовой матрицы
[48:38.000 --> 48:40.000]  понятие инерции.
[48:40.000 --> 48:42.000]  Ну эрмитовая матрица, она имеет вещественные собственные значения.
[48:42.000 --> 48:44.000]  А что такое инерция?
[48:44.000 --> 48:46.000]  Это такая тройка.
[48:46.000 --> 48:48.000]  Число положительных собственных значений,
[48:48.000 --> 48:50.000]  число отрицательных и число нулевых.
[48:50.000 --> 48:52.000]  И есть
[48:52.000 --> 48:54.000]  замечательный закон инерции.
[48:56.000 --> 48:58.000]  Который утверждает, что
[48:58.000 --> 49:00.000]  инерция сохраняется
[49:02.000 --> 49:04.000]  при переходе
[49:04.000 --> 49:06.000]  от одной эрмитовой матрицы
[49:06.000 --> 49:08.000]  к
[49:08.000 --> 49:10.000]  эрмитово-конкурентной матрице.
[49:12.000 --> 49:14.000]  Ну вот такое понятие
[49:14.000 --> 49:16.000]  конкуренции.
[49:16.000 --> 49:18.000]  Значит вот конкуренция.
[49:22.000 --> 49:24.000]  Значит вот есть матрица А,
[49:24.000 --> 49:26.000]  а вы переходите
[49:26.000 --> 49:28.000]  к матрице вот такой вот.
[49:30.000 --> 49:32.000]  И Х здесь просто невыраженная матрица,
[49:32.000 --> 49:34.000]  не обязательно унитарная.
[49:34.000 --> 49:36.000]  Если унитарная, то говорят, что
[49:36.000 --> 49:38.000]  это унитарная конкуренция.
[49:38.000 --> 49:40.000]  А вот здесь просто конкуренция.
[49:40.000 --> 49:42.000]  Вот закон инерции говорит,
[49:42.000 --> 49:44.000]  что конкурентная эрмитовая матрица
[49:44.000 --> 49:46.000]  имеет одну и ту же
[49:46.000 --> 49:48.000]  инерцию.
[49:48.000 --> 49:50.000]  А если инерции эрмитовых матриц
[49:50.000 --> 49:52.000]  одинаковые, то эти матрицы конкурентны.
[49:52.000 --> 49:54.000]  Вот такая есть теорема.
[49:58.000 --> 50:00.000]  Эта теорема на самом деле
[50:00.000 --> 50:02.000]  равносильна
[50:02.000 --> 50:04.000]  еще некоторым чудесным утверждением.
[50:10.000 --> 50:12.000]  Некоторым чудесным утверждением
[50:12.000 --> 50:14.000]  есть так называемые
[50:16.000 --> 50:18.000]  формулы
[50:18.000 --> 50:20.000]  Курнта-Фишера
[50:20.000 --> 50:22.000]  или теорема Курнта-Фишера
[50:22.000 --> 50:24.000]  авариационных свойствах
[50:24.000 --> 50:26.000]  собственных значений.
[50:26.000 --> 50:28.000]  Ну, видимо, не слышали об этом.
[50:28.000 --> 50:30.000]  Ну, бог с ним. Ладно, оставим.
[50:30.000 --> 50:32.000]  Еще одна теорема.
[50:32.000 --> 50:34.000]  Эта теорема
[50:34.000 --> 50:36.000]  о так называемых соотношениях разделения.
[50:36.000 --> 50:38.000]  Вот удивительно важная штука.
[50:38.000 --> 50:40.000]  Соотношение разделения.
[50:40.000 --> 50:42.000]  По существу это равносильно
[50:42.000 --> 50:44.000]  к закону инерции.
[50:44.000 --> 50:46.000]  То есть из законной инерции можно это вывести.
[50:46.000 --> 50:48.000]  Вот я что.
[50:48.000 --> 50:50.000]  Из законной инерции можно вывести
[50:50.000 --> 50:52.000]  соотношение разделения. Правда, можно
[50:52.000 --> 50:54.000]  и другими путями прийти к соотношению
[50:54.000 --> 50:56.000]  разделения.
[50:56.000 --> 50:58.000]  О чем это?
[50:58.000 --> 51:19.000]  Вот если у вас есть Эрмитова матрица H, то есть H-Эрмитова, и в ней вырезали ведущую под матрицу порядка n-1,
[51:19.000 --> 51:45.000]  то давайте я буду писать лямбда и T для матрицы H. Договоримся, что нумеруем лямбда по невозрастанию.
[51:45.000 --> 51:55.000]  Сначала самое большое, но это счастливые числа, их можно упорядочить. Могут быть отрицательные. Самое большое, потом поменьше, поменьше.
[51:55.000 --> 52:08.000]  И n-ное самое маленькое. Собственные значения. Значит вот лямбда и T больше или равно, чем лямбда и T для H, для подматрицы.
[52:08.000 --> 52:20.000]  А вот это собственное значение и T для подматрицы, оно снизу тоже оценивается. И плюс первым собственным значением.
[52:20.000 --> 52:39.000]  Матрица H. И вот это я со отношениями разбираюсь. Собственные значения. Можно их упорядочить как угодно.
[52:39.000 --> 52:53.000]  Предполагаем, что собственные значения матрицы H занумерованы вот таким образом.
[52:53.000 --> 53:02.000]  Ну естественно, надо нумерацию фиксировать. Не для любой нумерации, это правда. Вот такая у нумерации.
[53:02.000 --> 53:21.000]  И аналогичным образом нумеруем собственные значения, ведущие под матрице, порядок, который на единичку меньше.
[53:21.000 --> 53:31.000]  Ну понятно, что вы должны вот единицы, да, n без единиц.
[53:31.000 --> 53:50.000]  Ну, кстати, добавить надо, конечно, и такое, что лямбда M минус первое, для подматрицы снизу оценивается, так и оценивается.
[53:50.000 --> 54:00.000]  Самым младшим собственным значением большой матрицы.
[54:00.000 --> 54:20.000]  Вот если вы нарисуете на вещественной оси собственные значения, только нарисовал все неправильно, нумеруя они так.
[54:20.000 --> 54:28.000]  Вот вопрос, как нумеруем. Здесь лямбда 1, здесь лямбда 2.
[54:28.000 --> 54:39.000]  То где будут собственные значения ведущие под матрицы между? Оттого и название соотношения и разделения.
[54:39.000 --> 54:50.000]  То есть собственные значения ведущие под матрицы разделяются собственными значениями большой матрицы.
[54:50.000 --> 54:57.000]  Но при этом порядок ведущий под матрицы ровно на единичку меньше порядка исходной матрицы.
[54:57.000 --> 55:02.000]  Это есть один из замечательных фактов теории матриц.
[55:02.000 --> 55:09.000]  Соотношение и разделение для собственных значений эрбитовой матрицы и ее ведущей под матрицы.
[55:09.000 --> 55:13.000]  Изучали, не изучали?
[55:13.000 --> 55:22.000]  Ну, надо этот пробел восполнять. Посмотрите, пожалуйста.
[55:22.000 --> 55:29.000]  Практически в любом учебнике по теории матрицы это можно, конечно, прочитать.
[55:29.000 --> 55:36.000]  Ну, вот есть книжка у меня «Маточный анализ и линейный алгебр».
[55:36.000 --> 55:40.000]  Я, правда, подготовил переработанную версию.
[55:40.000 --> 55:46.000]  Ну, есть такая. Можно там прочитать, но, в принципе, это знаменитый факт.
[55:46.000 --> 55:53.000]  Можно взять какую-нибудь книжку по матричному анализу.
[55:53.000 --> 55:57.000]  Хорнед Джонсон, например, есть такой.
[55:57.000 --> 56:01.000]  Первый том на русском языке даже есть.
[56:01.000 --> 56:07.000]  То есть это такая вот фундаментальная вещь.
[56:07.000 --> 56:12.000]  И полезная, кстати. Вычислить-то науку чрезвычайно полезная.
[56:12.000 --> 56:16.000]  А вот то, что я вам говорю, я говорю, а я уже сказал это.
[56:16.000 --> 56:22.000]  А на самом деле, эту теорему можно вывести из закона инерции.
[56:22.000 --> 56:28.000]  И более того, в закон инерции эта теорема, по существу, это эквивалентное утверждение.
[56:28.000 --> 56:35.000]  Но если вы с этим разберётесь, вы всё-таки молодцы.
[56:35.000 --> 56:42.000]  Это один из ключевых утверждений теории Ермитовых матриц.
[56:42.000 --> 56:45.000]  Это соотношение-разделение.
[56:45.000 --> 56:50.000]  И вот эти соотношения-разделения мы сейчас используем.
[56:50.000 --> 56:54.000]  Используем.
[56:54.000 --> 57:04.000]  Ну, кстати говоря, я могу, как я говорю, некоторые главы переработанной книжки я помещаю в папочку.
[57:04.000 --> 57:08.000]  Давайте я помещу, вы можете там почитать.
[57:08.000 --> 57:12.000]  Это будет глава с самым большим номером.
[57:12.000 --> 57:15.000]  Там сейчас какое-то количество глав есть.
[57:15.000 --> 57:23.000]  Ну, я думаю, что тут-то вы справитесь, если у вас желание будет найти, там вы уже найдёте.
[57:23.000 --> 57:27.000]  Нет, подождите, подождите.
[57:27.000 --> 57:32.000]  Мы должны теорему-то до конца довести.
[57:32.000 --> 57:41.000]  Замечательная теорема ещё тем, что повод даёт вспомнить классику, причём нужную классику,
[57:41.000 --> 57:47.000]  которая даже оказалась ей в чём-то незнакомой.
[57:47.000 --> 57:56.000]  Вот, а у нас здесь ведущий под матрицу, только порядок её не n-1, а t.
[57:56.000 --> 57:59.000]  Да?
[57:59.000 --> 58:03.000]  Ну, смотрите как, мы можем же последовательно делать.
[58:03.000 --> 58:13.000]  Вот эти соотношения и разделения вначале применяем для подматрицы n-1, потом n-2 порядка, потом и тогда t дойдём.
[58:13.000 --> 58:15.000]  Правильно?
[58:15.000 --> 58:37.000]  И в результате вот для этой матрицы h-t мы получим вот такие соотношения.
[58:37.000 --> 58:49.000]  Только здесь будет, с конца надо отсчитывать, но я хочу вот что сказать.
[58:49.000 --> 59:06.000]  Лямбда t будет больше или равно, чем лямбда n, лямбда t-1 больше, чем лямбда n-1 от h, ну и так далее.
[59:06.000 --> 59:13.000]  Вот последовательно, применяя соотношения и разделения для ведущих под матрицу порядка трёх на единичку,
[59:13.000 --> 59:19.000]  мы получим некоторую связь соотношения матрицы h-t и матрицы h.
[59:19.000 --> 59:25.000]  И вот эту связь я написал, какую получим в результате.
[59:25.000 --> 59:31.000]  А теперь можно и суммы сравнить.
[59:31.000 --> 59:33.000]  Да?
[59:33.000 --> 59:41.000]  Вот отсюда вытекает, отсюда вытекает ровно то, что нам нужно.
[59:41.000 --> 59:51.000]  Вот сумма вот этих, лямбда t, не лямбда t, сумма всех соотношений,
[59:51.000 --> 01:00:01.000]  давайте, ладно, напишу, лямбда 1, h-t плюс и так далее, лямбда t, h-t, вот этот самый след,
[01:00:01.000 --> 01:00:15.000]  больше или равен, ну а здесь будет лямбда n-t плюс 1, наверное, от h, плюс и так далее.
[01:00:15.000 --> 01:00:20.000]  Нет, а почему же?
[01:00:20.000 --> 01:00:23.000]  Нет, что такое ты?
[01:00:33.000 --> 01:00:36.000]  На самом деле...
[01:00:36.000 --> 01:00:51.000]  А, нет, всё правильно.
[01:00:51.000 --> 01:01:03.000]  Здесь всё правильно, что меня смутило, а не должно было смущать.
[01:01:03.000 --> 01:01:07.000]  А вот это сумма.
[01:01:07.000 --> 01:01:12.000]  Ну, смотрим на нашу оценку.
[01:01:12.000 --> 01:01:18.000]  Ну, t у нас больше или равен n-t?
[01:01:18.000 --> 01:01:21.000]  Как?
[01:01:21.000 --> 01:01:27.000]  Что вы хотели спросить?
[01:01:27.000 --> 01:01:29.000]  Нет, то есть вот это вот...
[01:01:29.000 --> 01:01:31.000]  Вот что это за собственные значения?
[01:01:31.000 --> 01:01:36.000]  Вот сколько?
[01:01:36.000 --> 01:01:41.000]  А, здесь мы от сяшка, нет, здесь вы правильно говорите, h.
[01:01:41.000 --> 01:01:55.000]  Вот, и здесь есть в этой сумме не меньше, чем n-k слагаемых.
[01:01:55.000 --> 01:01:58.000]  Не меньше, чем n-k слагаемых.
[01:01:58.000 --> 01:02:00.000]  Вот в чём.
[01:02:00.000 --> 01:02:04.000]  n-k слагаемых, а это значит, какие собственные значения?
[01:02:04.000 --> 01:02:08.000]  От k, плюс первого, до n-ого.
[01:02:08.000 --> 01:02:12.000]  Это будет оценка снизу.
[01:02:12.000 --> 01:02:14.000]  И оценка снизу, она вот такая.
[01:02:14.000 --> 01:02:20.000]  От k, плюс первого, до n-ого.
[01:02:20.000 --> 01:02:23.000]  Что и требовалось доказать.
[01:02:23.000 --> 01:02:27.000]  То есть вот мы снизу получили...
[01:02:27.000 --> 01:02:30.000]  Ну, если бы в квадрате t лямбда как раз есть.
[01:02:30.000 --> 01:02:35.000]  То есть мы как раз и получили сумму...
[01:02:35.000 --> 01:02:40.000]  Сумму квадратов сингулярных чисел от k, плюс первого, до n-ого.
[01:02:40.000 --> 01:02:41.000]  Да.
[01:02:41.000 --> 01:02:46.000]  Вы не доказаете, что у нас, вот, наша норма по Венесу будет больше равна, чем вот это число.
[01:02:46.000 --> 01:02:52.000]  А как доказать, что это минимум, то, что меньше, чем нигде?
[01:02:52.000 --> 01:02:59.000]  Значит, мы с вами доказали, что, какую бы матрицу B не взяли,
[01:02:59.000 --> 01:03:04.000]  ранг, который не больше, чем k.
[01:03:04.000 --> 01:03:09.000]  Вот оценочка такая.
[01:03:09.000 --> 01:03:12.000]  Всё.
[01:03:12.000 --> 01:03:17.000]  А если вы возьмёте матрицу Аккаты, то будет достигаться эта оценка.
[01:03:18.000 --> 01:03:20.000]  Точно что.
[01:03:20.000 --> 01:03:23.000]  Посчитали минимум.
[01:03:29.000 --> 01:03:32.000]  Нет, ты определенно, однозначно.
[01:03:32.000 --> 01:03:35.000]  Это размерность ядра матрицы B.
[01:03:41.000 --> 01:03:44.000]  Чего не всегда верно?
[01:03:47.000 --> 01:03:48.000]  Нет.
[01:03:48.000 --> 01:03:53.000]  Значит, здесь для любой ведущей под матрицу вот эти соотношения справедливы.
[01:03:53.000 --> 01:03:56.000]  Следствие соотношений разделено.
[01:04:04.000 --> 01:04:07.000]  Минимум больше или равен?
[01:04:07.000 --> 01:04:11.000]  Минимум больше или равен, чем сумма.
[01:04:11.000 --> 01:04:13.000]  Правильно, но не минимум.
[01:04:13.000 --> 01:04:19.000]  Мы доказали, что для любой матрицы B конкретной будет больше или равно, чем сумма.
[01:04:19.000 --> 01:04:24.000]  А если мы по B будем минимизировать, значит, минимум тоже будет больше, чем сумма.
[01:04:24.000 --> 01:04:25.000]  Правильно.
[01:04:25.000 --> 01:04:27.000]  Но почему он равен сумме?
[01:04:27.000 --> 01:04:32.000]  Потому что для некоторой матрицы B эта величина достигается.
[01:04:32.000 --> 01:04:39.000]  Если вы возьмёте B равные Аккаты, то вы получите в точности то, что у нас возникло как оценка.
[01:04:39.000 --> 01:04:42.000]  Вот такое вот здесь рассуждение.
[01:04:42.000 --> 01:04:49.000]  Приглашаю вас его обдумать, обдумать.
[01:04:49.000 --> 01:04:57.000]  И к тому же обязательно познакомьтесь по основательной соотношениям разделения для армиттовых матриц.
[01:04:57.000 --> 01:05:00.000]  Это вам такое задание домашнее.
[01:05:00.000 --> 01:05:09.000]  Мы ещё, я думаю, будут моменты, когда соотношения разделения понадобятся нам.
[01:05:09.000 --> 01:05:17.000]  И точно и в других науках, связанных с вычислениями, это такие очень важные.
[01:05:17.000 --> 01:05:23.000]  Может быть, вы слышали об артагональных многочленах что-нибудь?
[01:05:23.000 --> 01:05:28.000]  Есть тоже наука об артагональных многочленах, и они в эти матрицы огромную роль играют.
[01:05:28.000 --> 01:05:33.000]  У артагональных многочленов, скажем, наотрезки корни вещественные.
[01:05:33.000 --> 01:05:36.000]  И для корней имеют место соотношение разделения.
[01:05:36.000 --> 01:05:40.000]  Такие же, как для армиттовых матриц и подматриц.
[01:05:40.000 --> 01:05:48.000]  И более того, эти соотношения получаются как следствие соотношения разделения для армиттовых матриц и подматриц.
[01:05:48.000 --> 01:05:50.000]  Ну, это так вот.
[01:05:50.000 --> 01:05:55.000]  Если когда-нибудь столкнётесь, а кто-нибудь обязательно столкнётся с применением артагональных многочленов,
[01:05:55.000 --> 01:06:02.000]  вот можете вспомнить о соотношениях разделения, которые мы сегодня обсуждаем.
[01:06:02.000 --> 01:06:05.000]  Значит, ну всё, точка теоремы доказана.
[01:06:05.000 --> 01:06:09.000]  Но я бы не сказал, что это уж такая простая теорема.
[01:06:09.000 --> 01:06:20.000]  Вот, но она связана, она так вот базируется на очень примечательных теоремах из матричного анализа.
[01:06:20.000 --> 01:06:24.000]  И объясняет, и объясняет.
[01:06:24.000 --> 01:06:34.000]  Вот, видите, задачу о приближении малого ранга, по крайней мере, при использовании двух вот этих норм чудесных.
[01:06:34.000 --> 01:06:38.000]  Вот здесь сингулярное разложение даёт вам оптимальные приближения.
[01:06:38.000 --> 01:06:43.000]  То есть, если вы умеете вычислять сингулярные разложения, а это можно делать.
[01:06:43.000 --> 01:06:50.000]  Ну, не прямо сейчас, мы поговорим ещё о том, как вычислять сингулярные разложения.
[01:06:50.000 --> 01:06:57.000]  То вы можете решать задачи об оптимальных приближениях матрицы матрицами заданного ранга.
[01:06:57.000 --> 01:06:59.000]  Какое у нас оптимальное?
[01:06:59.000 --> 01:07:01.000]  Оно окатое.
[01:07:01.000 --> 01:07:03.000]  Окатое?
[01:07:03.000 --> 01:07:06.000]  Обрезанное сингулярное разложение.
[01:07:06.000 --> 01:07:10.000]  Как старших членов? Берёте?
[01:07:10.000 --> 01:07:16.000]  И это то же самое в норме спектральной, то же самое в норме фробени.
[01:07:16.000 --> 01:07:18.000]  Обивительно, да?
[01:07:18.000 --> 01:07:20.000]  По грешности?
[01:07:20.000 --> 01:07:22.000]  Нет.
[01:07:22.000 --> 01:07:27.000]  Вы затрагиваете очень интересный и нетривиальный вопрос.
[01:07:27.000 --> 01:07:32.000]  Значит, вопрос, вот какой.
[01:07:32.000 --> 01:07:40.000]  Если мы здесь вот двоечку уберём, то есть, вот для каких норм это останется?
[01:07:40.000 --> 01:07:42.000]  Да?
[01:07:42.000 --> 01:07:45.000]  Для каких норм это останется?
[01:07:45.000 --> 01:07:51.000]  Мы знаем, для спектральной вермы, для фробениуса вермы, а ещё какие-нибудь есть.
[01:07:51.000 --> 01:07:56.000]  Значит, утверждаю, для произвольной нормы это неверно.
[01:07:56.000 --> 01:08:03.000]  Для произвольной нормы здесь теорему написать не получается.
[01:08:03.000 --> 01:08:10.000]  Но есть большой класс норм, для которых теорема обобщается.
[01:08:10.000 --> 01:08:12.000]  И это есть нетривиальный результат.
[01:08:12.000 --> 01:08:19.000]  Причём так существенно более нетривиальный результат, чем то, что мы получили.
[01:08:19.000 --> 01:08:24.000]  Это так называемую унитарно-инвариантная норма.
[01:08:24.000 --> 01:08:34.000]  То есть, если норма унитарно-инвариантная, то есть, если норма матрицы не меняется при умножении справа и слева на любую унитарную матрицу,
[01:08:34.000 --> 01:08:42.000]  такие нормы называются, то справедливая теорема – вот это самое.
[01:08:42.000 --> 01:08:53.000]  Здесь считать как-то норму А-Ак, это будет зависеть от того, какая конкретно унитарно-инвариантная норма.
[01:08:53.000 --> 01:08:57.000]  Но речь о вычислении нормы диагональной матрицы.
[01:08:57.000 --> 01:09:00.000]  Но вот эта теорема имеет место.
[01:09:00.000 --> 01:09:03.000]  Это теорема Мирского.
[01:09:03.000 --> 01:09:14.000]  Связанная, поскольку мы на физ.техе находимся, можно вспомнить, что довольно много лет преподавал Лицкий на физ.техе.
[01:09:14.000 --> 01:09:20.000]  И с одной из теорем Лицкого связана эта теорема Мирского.
[01:09:20.000 --> 01:09:30.000]  Это такой глубокий результат, требующий довольно больших усилий.
[01:09:30.000 --> 01:09:38.000]  В матричном анализе такая изюминка замечательная.
[01:09:38.000 --> 01:09:48.000]  Ну, если говорить о практике, ну, конечно, спектральная норма, норма Фрагениуса – это самая популярная унитарная норма.
[01:09:48.000 --> 01:09:51.000]  Ну, будем двигаться дальше.
[01:09:51.000 --> 01:09:56.000]  Да, будем двигаться дальше.
[01:09:57.000 --> 01:10:00.000]  В каком направлении?
[01:10:00.000 --> 01:10:10.000]  Ну, давайте все-таки начнем разговор о конкретных вычислительных задачах, о классах вычислительных задач,
[01:10:10.000 --> 01:10:16.000]  относящихся к алгебе, прежде всего к линейной алгебре.
[01:10:16.000 --> 01:10:25.000]  Ну, какие задачи? Все-таки некоторую классику изучать тоже нужно нельзя.
[01:10:25.000 --> 01:10:31.000]  Я пытаюсь вам рассказать о всех свежих вещах. Буду пытаться.
[01:10:31.000 --> 01:10:34.000]  Даже относительно недавних. Я обязательно сделаю.
[01:10:34.000 --> 01:10:43.000]  То есть то, что вы можете услышать в этом курсе, здесь будет присутствовать то, что вы нигде не можете услышать.
[01:10:43.000 --> 01:10:45.000]  Нет еще таких курсов.
[01:10:45.000 --> 01:10:53.000]  Но соответствующие вещи уже проникли активно в жизнь.
[01:10:53.000 --> 01:10:57.000]  В научную и даже в инженерную.
[01:10:57.000 --> 01:10:59.000]  И об этом я буду рассказывать.
[01:10:59.000 --> 01:11:02.000]  Но все-таки есть и такие классические вещи.
[01:11:02.000 --> 01:11:06.000]  Вот есть задача, задача важная.
[01:11:06.000 --> 01:11:10.000]  Решать систему линейных алгебрехических уровней.
[01:11:10.000 --> 01:11:16.000]  Вот мы эту задачу с вами обсудим.
[01:11:16.000 --> 01:11:22.000]  Алгоритмы для решения системы линейных алгебрехических уравнений.
[01:11:28.000 --> 01:11:31.000]  И не такая это маленькая область, как может показаться.
[01:11:31.000 --> 01:11:36.000]  Вот знаете, можно подумать, ну что там с системами линейных уравнений?
[01:11:36.000 --> 01:11:39.000]  Школьник может решить, правда?
[01:11:39.000 --> 01:11:41.000]  И это правда.
[01:11:44.000 --> 01:11:46.000]  И это правда.
[01:11:46.000 --> 01:11:59.000]  Но довольно легко возникают ситуации, с которыми школьник может и не справиться.
[01:11:59.000 --> 01:12:03.000]  Допустим, очень большой мат.
[01:12:03.000 --> 01:12:06.000]  Очень большой мат.
[01:12:06.000 --> 01:12:14.000]  Или, так сказать, даже матцы, которые можно получить в принципе.
[01:12:14.000 --> 01:12:19.000]  То есть можно вычислить их элементы, вроде бы есть формула.
[01:12:19.000 --> 01:12:24.000]  Но размер настолько большой, что вычитать нельзя, поскольку негде запомнить.
[01:12:24.000 --> 01:12:28.000]  Нормально.
[01:12:28.000 --> 01:12:33.000]  Ну, кстати, в инженерных задачах, да и в научных очень просто огромные системы возникают.
[01:12:33.000 --> 01:12:38.000]  И то, что может делать школьник, в принципе, можно бы на компьютере записать.
[01:12:38.000 --> 01:12:40.000]  Метод исключения.
[01:12:40.000 --> 01:12:45.000]  Но он требует огромных ресурсов, и задачу можно не решать.
[01:12:45.000 --> 01:12:52.000]  То есть вопросов здесь на самом деле таких совсем не школьных много.
[01:12:52.000 --> 01:12:56.000]  Но многие вещи можно сделать, как бы так, из термяжных соображений.
[01:12:56.000 --> 01:12:58.000]  Поэтому много.
[01:12:58.000 --> 01:13:06.000]  Вот если просто небольшая система, то в инженерных задачах, понятно, исключаешь неизвестный.
[01:13:06.000 --> 01:13:08.000]  И придешь к какому-то алгоритму.
[01:13:08.000 --> 01:13:12.000]  Обычно говорят, что нет гауса, как принято говорить.
[01:13:12.000 --> 01:13:20.000]  Но если вы хотите анализировать этот алгоритм, вот здесь теория мат. уже нужна.
[01:13:20.000 --> 01:13:28.000]  Если вы хотите придумать более эффективный алгоритм, здесь уже трудно обойтись без теории матрицы.
[01:13:28.000 --> 01:13:34.000]  Значит, вот мы будем обсуждать эту задачу, систему линейных алгоритмических параметров.
[01:13:34.000 --> 01:13:36.000]  Это первое.
[01:13:36.000 --> 01:13:42.000]  В том числе и с разными матрицами, которые имеют специальное происхождение.
[01:13:42.000 --> 01:13:47.000]  И какую-то специфику, что что-то о матрицах известно дополнительное.
[01:13:47.000 --> 01:13:50.000]  То есть это не есть просто самые общие матрицы.
[01:13:50.000 --> 01:14:01.000]  А в приложениях, к счастью, обычно возникают матрицы, обладающие какими-то характерными для приложения свойствами.
[01:14:01.000 --> 01:14:09.000]  То есть вы, конечно, можете сказать, вот есть классическая линейная алгебра, есть методы для матриц общего вида, их можно применять.
[01:14:09.000 --> 01:14:15.000]  Но применение общих методов часто оказывается либо невозможным, либо неэффективным.
[01:14:15.000 --> 01:14:30.000]  А значит, увы, приходится, ну, может быть, не увы, приходится все-таки думать и пытаться понять то, чего прямо не сказано.
[01:14:30.000 --> 01:14:39.000]  А чего прямо не сказано? Не сказано в точности, а какие вот особые свойства связаны с этим приложением.
[01:14:39.000 --> 01:14:50.000]  Вот надо эти свойства еще извлекать и думать, а не получится ли эти свойства применить, как-то превратить их в эффективные методы вычисления.
[01:14:50.000 --> 01:14:56.000]  Вот это есть, так сказать, жизнь современной вычислительной линейной алгебры.
[01:14:56.000 --> 01:14:59.000]  Но помимо задачи, наверное, просто с решением системы.
[01:14:59.000 --> 01:15:03.000]  Есть, конечно, задачи, типа, для меньших квадратов, уже такие, как мессонные задачи.
[01:15:03.000 --> 01:15:13.000]  Задачи приближений. Вот мы о сингулярном разложении или разговоре, как об одном инструменте связанном с задачами приближения.
[01:15:13.000 --> 01:15:16.000]  Есть спектральные задачи вычислять спектры.
[01:15:16.000 --> 01:15:23.000]  Те же сингулярные разложения находят собственные значения, собственные векторы.
[01:15:23.000 --> 01:15:29.000]  Вот об этих задачах, конечно, надо будет поговорить.
[01:15:29.000 --> 01:15:44.000]  Вот и некоторые. И вот то, что совершенно новое, что в нашем курсе будет, это теория и алгоритмы совершенно современные,
[01:15:44.000 --> 01:15:51.000]  в которых векторы и матрицы рассматриваются как тензоры.
[01:15:51.000 --> 01:15:55.000]  Это очень популярным стало.
[01:15:56.000 --> 01:16:06.000]  Здесь вот мы в институте вычислительной математики смогли как-то продвинуться в этом направлении.
[01:16:06.000 --> 01:16:13.000]  Вот и придумать удалось некоторые такие новые на тот момент.
[01:16:13.000 --> 01:16:19.000]  Это в 2009 году было такие тензорные разложения.
[01:16:19.000 --> 01:16:25.000]  Как или иначе, я пока о деталях не говорю, мы будем это обсуждать.
[01:16:25.000 --> 01:16:30.000]  Это связано, это форма представления данных.
[01:16:30.000 --> 01:16:41.000]  То есть матрицы, векторов, некие специальные разложения, с помощью которых можно представлять и векторы, и матрицы.
[01:16:41.000 --> 01:16:51.000]  Вот и все это связано с той самой идеей разделения переменных, которую мы обсуждали в своем сингулярном разложении, мы еще будем обсуждать.
[01:16:51.000 --> 01:16:59.000]  Значит, вот примерно такой вот план на будущее.
[01:16:59.000 --> 01:17:05.000]  Сегодня, по-моему, время у нас с вами вышло.

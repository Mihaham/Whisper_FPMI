[00:00.000 --> 00:11.040]  Итак, ребят, ну, лучший технический вуз, как вы понимаете, техника
[00:11.040 --> 00:12.040]  работает как надо.
[00:12.040 --> 00:15.800]  Итак, сначала пара организационных моментов.
[00:15.800 --> 00:18.320]  Спасибо, что большинство из вас записались на курс,
[00:18.320 --> 00:21.080]  там сейчас 321 анкета.
[00:21.080 --> 00:22.960]  Собственно, касательно семинарский групп.
[00:22.960 --> 00:24.800]  Там, как и предполагалось, половина хочет ходить
[00:24.800 --> 00:27.600]  чисто в лекционную аудиторию на семинары после лекции.
[00:27.600 --> 00:28.600]  Вопросов нет.
[00:28.600 --> 00:31.040]  Это абсолютно рабочий вариант.
[00:31.040 --> 00:33.920]  Семинарские группы, собственно, семинарские занятия предполагаются
[00:33.920 --> 00:39.240]  по вторникам и средам в 18-30 вечером очные, то есть
[00:39.240 --> 00:42.200]  туда можно записаться, мы скинем опросик, чтобы
[00:42.200 --> 00:44.360]  понимать, кто куда, когда пойдет, уже маленькие просто
[00:44.360 --> 00:45.360]  в чатике.
[00:45.360 --> 00:46.360]  Пожалуйста, отметись.
[00:46.360 --> 00:49.320]  Онлайн занятия, скорее всего, будут в районе пятницы,
[00:49.320 --> 00:52.840]  потому что, потому что, потому что так сложилось.
[00:52.840 --> 00:54.400]  То есть на них тоже можно прийти.
[00:54.400 --> 00:55.400]  Вопрос.
[00:55.400 --> 01:04.960]  А кто хочет сегодня ходить в другую семинарскую группу?
[01:04.960 --> 01:09.040]  А вы точно на нее ходить будете?
[01:09.040 --> 01:10.040]  Если да, то можно.
[01:10.040 --> 01:12.440]  Просто ранее получалось так, что ставили семинары
[01:12.440 --> 01:15.080]  параллельно с лекционным потоком, туда приходило
[01:15.080 --> 01:18.080]  два человека, в итоге они просто-напросто переставали
[01:18.080 --> 01:19.080]  работать.
[01:19.080 --> 01:20.720]  Если есть необходимость, можно поставить в понедельник.
[01:20.720 --> 01:24.720]  Хорошо?
[01:24.720 --> 01:26.520]  Да, ходить можно всем по умолчанию.
[01:26.520 --> 01:28.760]  Вопрос в том, кто-то будет в 18-30 по понедельникам
[01:28.760 --> 01:29.760]  ходить не сюда.
[01:29.760 --> 01:30.760]  Тоже вечерня.
[01:30.760 --> 01:35.280]  Смотрите, все семинары по умолчанию будут в 18-30,
[01:35.280 --> 01:36.280]  если не оговорено обратно.
[01:36.280 --> 01:40.280]  Потому что все, как правило, преподаватели, это действующие
[01:40.280 --> 01:42.560]  специалисты, которые работают, и они все это делают после
[01:42.560 --> 01:43.560]  работы.
[01:43.560 --> 01:46.160]  Так, здесь какие-то вопросы есть?
[01:46.160 --> 01:48.720]  Окей, смотрите, тогда еще пара организационных
[01:48.720 --> 01:49.720]  моментов.
[01:49.720 --> 01:52.920]  В репозитории появилась, соответственно, информация
[01:52.920 --> 01:56.280]  про то, про все, плюс-минус, что необходимо.
[01:56.280 --> 01:58.000]  Там появилась примерная программа экзамена, там
[01:58.000 --> 02:00.320]  появилась табличка, где будут лежать все записи
[02:00.320 --> 02:01.320]  лекций.
[02:01.320 --> 02:02.320]  Запись первой лекции там доступна.
[02:02.320 --> 02:05.320]  Второй пока обрабатывается, потому что мы нанимаем
[02:05.320 --> 02:09.760]  нового монтажера, но появится на канале буквально сегодня-завтра,
[02:09.760 --> 02:10.760]  я надеюсь.
[02:10.760 --> 02:14.080]  Там же есть список необходимых знаний, скажем так, необытимых
[02:14.080 --> 02:16.560]  фактов, пререквизитов для курса.
[02:16.560 --> 02:20.480]  Короче, списочек из формата прочитать и понять, знаете,
[02:20.480 --> 02:22.640]  вы из этого списка все или нет.
[02:22.760 --> 02:25.680]  Намек, все, что без звездочки знать должны, наверное,
[02:25.680 --> 02:28.520]  все вот прям совсем, причем сходу в три часа ночи разбуди,
[02:28.520 --> 02:31.600]  чтобы вы это помнили, поэтому, пожалуйста, проверьте сами
[02:31.600 --> 02:32.600]  себя.
[02:32.600 --> 02:33.600]  Проходите.
[02:33.600 --> 02:34.600]  Вот.
[02:34.600 --> 02:43.080]  И что еще, там же появилось первое домашнее задание,
[02:43.080 --> 02:44.080]  его можно решать.
[02:44.080 --> 02:50.200]  Оно максимально простое, прямолинейное, там все описано.
[02:50.240 --> 02:53.400]  Сама проверяющая система сейчас подымется, мне надо
[02:53.400 --> 02:56.000]  деньги закинуть на ту виртуалку, на которую она крутится.
[02:56.000 --> 02:59.440]  Чтобы сдать домашку, вам достаточно будет загрузить
[02:59.440 --> 03:02.920]  домашку, так как указано в инструкции, в проверяющую
[03:02.920 --> 03:05.280]  систему, она вам скажет, сколько у вас баллов, один
[03:05.280 --> 03:08.360]  это сто процентов, короче, вероятность единица, то
[03:08.360 --> 03:09.360]  есть вы все решили.
[03:09.360 --> 03:12.040]  Один значит полный балл, ноль значит не полный балл,
[03:12.040 --> 03:14.360]  нулевой балл, ноль пять значит половину решили,
[03:14.360 --> 03:16.080]  какие тесты у вас не прошли, вам напишут.
[03:16.080 --> 03:17.480]  Вопросы, пожелания, комментарии есть?
[03:20.200 --> 03:27.160]  Окей, надо открыть видимо, надо будет написать видимо
[03:27.160 --> 03:29.200]  более явные инструкции, короче, там как правило
[03:29.200 --> 03:32.920]  всегда есть в папке ноутбук, который называется Assignment,
[03:32.920 --> 03:34.760]  вот это ваша домашка, все остальное, вспомогательные
[03:34.760 --> 03:37.880]  файлы, внутри ноутбука Assignment написано, вся инструкция,
[03:37.880 --> 03:38.880]  что надо делать.
[03:38.880 --> 03:41.440]  Конкретно в данном случае у вас есть ноутбук Assignment,
[03:41.440 --> 03:43.880]  там по сути какие-то вопросы, что-то, что вам необходимо
[03:43.880 --> 03:46.800]  сделать, по факту вам необходимо что, реализовать класс
[03:46.800 --> 03:51.800]  кнн в пай-файлике, а чтобы у вас проверяющая система
[03:51.800 --> 03:55.400]  отработала и дала вам, скажем так, развернутые
[03:55.400 --> 03:57.480]  комментарии, пока что это немного на костылях, но
[03:57.480 --> 04:00.920]  вам нужно весь текст этого файла, когда у вас все работает,
[04:00.920 --> 04:05.080]  запихнуть в template и сдать его, там инструкция написана,
[04:05.080 --> 04:06.080]  не спать.
[04:06.080 --> 04:07.080]  И вы и ноутбук.
[04:07.080 --> 04:15.560]  Пока нет дедлайна, да, после дедлайна можете доздавать,
[04:16.400 --> 04:17.400]  что вы получили до дедлайна.
[04:17.400 --> 04:27.320]  Там написано, чем нельзя пользоваться, то есть к слову,
[04:27.320 --> 04:29.600]  вам говорят написать кнн, использовать кнн на заскалерной
[04:29.600 --> 04:32.840]  нельзя, вам надо его написать, ну и так далее.
[04:32.840 --> 04:38.040]  Ну оно там ограничено каким-то здоровым числом, пока что
[04:38.040 --> 04:40.880]  за всю историю никто его не перебрал, ну то есть там
[04:40.880 --> 04:42.600]  нельзя сделать, полностью не ограничить, там какая-то
[04:42.600 --> 04:45.520]  станция стоит, то ли 100, то ли 1000, короче, у вас раза
[04:45.520 --> 04:48.560]  с пятого, я думаю, зайдет.
[04:48.560 --> 04:51.240]  Ну если не идет там, например, четвертая домашка сложная,
[04:51.240 --> 04:52.240]  она раз с пятнадцатым может зайти.
[04:52.240 --> 04:59.680]  Это нормально, она примерно, это просто то, что сдавали
[04:59.680 --> 05:02.840]  люди весной 21 года, поэтому она примерно.
[05:02.840 --> 05:11.600]  Ну смотрите, то, что вам написал бот, я прошу прощения,
[05:11.600 --> 05:15.240]  я на стену закинул разболовку раньше, чем мы ее согласовали,
[05:15.240 --> 05:16.720]  на это пока не смотрите, я ее сейчас удалю.
[05:16.720 --> 05:21.040]  То, что там написано, это то, что было год назад.
[05:21.040 --> 05:23.960]  Оно сейчас немножко поменялось, поэтому то, что там написано,
[05:23.960 --> 05:24.960]  это неактуально.
[05:24.960 --> 05:27.960]  Ну что ж, еще вопрос-комментарий есть?
[05:27.960 --> 05:32.160]  Окей, смотрите, я на всякий случай напоминаю вот про
[05:32.160 --> 05:34.560]  этот файлик пререквизитов, я про него еще раз в чате
[05:34.560 --> 05:37.880]  напишу, но по сути это то, что вам хорошо бы знать,
[05:37.880 --> 05:41.280]  и это то, что мы с вас спросим где-то в районе, сейчас
[05:41.280 --> 05:44.040]  на сколько, 19 сентября, в районе 3 октября мы скорее
[05:44.040 --> 05:47.880]  всего вас спросим, вот с вас спросим вот эти факты,
[05:47.880 --> 05:50.520]  то есть хорошо бы их вспомнить, запомнить, заботать и так
[05:50.520 --> 05:53.840]  далее, вы из них большинство знаете, но, скажем так, опыт
[05:53.840 --> 05:56.560]  показывает, что многие эти вещи, типа что такое финное
[05:56.560 --> 06:01.160]  преобразование, далеко не сразу люди помнят, а?
[06:01.160 --> 06:06.960]  Да, да, то есть ну представьте себе, что это какой-то потоковый
[06:06.960 --> 06:09.320]  грубо говоря экзамен, кстати да, если будет соответственно
[06:09.320 --> 06:11.400]  переполнение, такое может быть, мы просто еще одну
[06:11.400 --> 06:14.560]  аудиторию возьмем, типа 123 ГК и туда часть выгрузим,
[06:14.560 --> 06:19.960]  ну чтобы не сажать тут людей прям совсем как сосиски,
[06:19.960 --> 06:21.880]  это будет на семинар, то есть лекция пройдет как
[06:21.880 --> 06:24.600]  обычно, потом кусочек семинара мы обсудим то, что необходимо,
[06:24.600 --> 06:26.400]  там коротенький семинар специально сделан, а потом
[06:26.400 --> 06:30.520]  где-то час будет на вот эту контрольную, вот, ну соответственно
[06:30.520 --> 06:35.680]  на нее явка плюс-минус обязательно, то есть неявка в формате
[06:35.680 --> 06:39.000]  заболел-уехал, заболел-уехал и принимается, но это надо
[06:39.000 --> 06:42.120]  тогда отдельно согласовывать, то есть в принципе на посещение
[06:42.120 --> 06:43.880]  прям так по желанию, но на контрольную как вы понимаете
[06:43.880 --> 06:50.720]  надо прийти, да, то есть, не, погодите, вот там это будет
[06:50.720 --> 06:52.600]  конкретно просто в этот день контрольная грубо
[06:52.600 --> 06:54.040]  говоря, можете считать, что она фиксированная для
[06:54.040 --> 06:55.880]  всех, это к семинарской группе не имеет отношения.
[07:06.280 --> 07:10.680]  Решим. Просто я изначально предполагал, что понедельника
[07:10.680 --> 07:13.280]  нет других семинаров, поэтому сейчас вы меня немножко поставили в тупик.
[07:13.280 --> 07:19.920]  Хорошо. Так, окей, еще вопросы есть? Ну ладно, тогда, собственно, вспоминаем,
[07:19.920 --> 07:22.840]  что у нас было в прошлый раз, мы с вами чуть-чуть поговорили про линейную
[07:22.840 --> 07:26.600]  алгебру, вспомнили какие-то базовые понятия, что-то починили,
[07:26.600 --> 07:29.520]  что-то разобрали, где-то вы меня даже поправили, большое спасибо,
[07:29.520 --> 07:32.760]  поправляйте меня абсолютно свободно. Сегодня нам это дело пригодится,
[07:32.760 --> 07:37.040]  мы с вами поговорим про ужас линейную регрессию, но те, у кого была статистика,
[07:37.040 --> 07:40.600]  прекрасно понимают, что это такое, те, кто ходили на лабо по общей физу, тоже
[07:40.600 --> 07:43.240]  прекрасно понимают, что это такое, в принципе, штука максимально простая,
[07:43.240 --> 07:48.920]  но при этом и очень красивая. Итак, сегодня в меню. Во-первых, мы с вами поговорим о том,
[07:48.920 --> 07:53.800]  что такое линейная модель, в принципе, как они работают, зачем они нужны и почему они
[07:53.800 --> 07:59.240]  используются повсеместно. Во-вторых, мы поговорим о том, какие именно подходы к
[07:59.240 --> 08:04.360]  регрессии с точки зрения линейных моделей есть и какие есть подходы для получения аналитического
[08:04.360 --> 08:11.480]  решения и, собственно, какого, градиентного решения. Вот, потом поговорим про теорему Гауса Маркова,
[08:11.480 --> 08:14.960]  классная теорема, одна из немногих теорем машинного обучения, поэтому, пожалуйста,
[08:14.960 --> 08:20.160]  не забивайте на вот те там три-четыре теоремы, которые у нас есть, типа теоремы Экрата Янга,
[08:20.160 --> 08:26.480]  теоремы Гауса Маркова и так далее. Штука классная. Да, на экзамене это тоже понадобится и, в принципе,
[08:26.480 --> 08:30.440]  это понадобится, чтобы понимать. То есть, вот если я говорю, обратите внимание на теорему,
[08:30.440 --> 08:34.480]  да, ее лучше заботать. Причем заботать не формально о тараторике, а понять то,
[08:34.480 --> 08:40.120]  что регулярно люди не понимают чуть такое. Потом поговорим про регуляризацию, по сути,
[08:40.120 --> 08:43.800]  первый раз в таком более-менее явном виде. Сегодня у нас будет регуляризация потихонного,
[08:43.800 --> 08:48.800]  но на самом деле она много где еще применяется и называется по-разному Way Decay и так далее. И
[08:48.800 --> 08:54.880]  поговорим про то, как понять, что ваша модель нехорошая. Сказал бы по-другому, но мы все-таки
[08:54.960 --> 09:02.560]  в университете. Итак, если вопрос есть, задавайте, руку поднимаете, задаете. Поехали. На предыдущей
[09:02.560 --> 09:08.040]  лекции, предыдущей, которая не по линалу, были разговоры у нас про что? Про понятие в машинном
[09:08.040 --> 09:13.720]  обучении, про то, что такое наивный байвский классификатор, про теорему байвса. Коротенько
[09:13.720 --> 09:18.080]  мы поговорили про КНН на прошлом занятии. Это все, в принципе, работало. На всякий случай,
[09:18.080 --> 09:24.640]  эта камера точно меня видит, а то он куда-то туда смотрит. Хорошо, спасибо. Вот. И сегодня мы
[09:24.640 --> 09:28.560]  переходим к линейным моделям. Как вы уже помните, линейная модель это так, кто раздает линейное
[09:28.560 --> 09:35.120]  отображение из пространства параметров, признаков в пространство ответов. Вот то, что мы с вами на
[09:35.120 --> 09:39.400]  прошлые недели разбирали как линейные, по сути, отображения с почвы матричек, это, по сути,
[09:39.400 --> 09:43.840]  все есть линейная модель. Все, что можно представить в виде линейной комбинации наших признаков,
[09:43.840 --> 09:49.280]  является линейной моделью и так далее. Бывают линейные модели в регрессии, понятное дело,
[09:49.280 --> 09:54.640]  есть точки, провели палку. Бывают в пластикации, есть точки, разделили палкой. Бывают линейные модели
[09:54.640 --> 09:59.360]  даже там, где у нас не стоит задача обучения с учителем, например, в задаче снижения размеров.
[09:59.360 --> 10:06.040]  Это unsupervised задача, но на самом деле переходим в supervised режим, то есть наша задача, у нас есть
[10:06.040 --> 10:10.720]  данные в размерности 25, мы хотим использовать не 25 признаков, а 3 признака. Как нам понизить
[10:10.720 --> 10:14.960]  размерность, потеряв наименьшее количество информации. Понятное дело, полностью мы не
[10:14.960 --> 10:19.040]  сможем сохранить всю информацию, потому что у нас размер с 25 на 3 не отображается, если не вырублено.
[10:19.040 --> 10:24.000]  Но, тем не менее. Сегодня мы с вами поговорим про регрессию и классификацию. Это следующая лекция,
[10:24.000 --> 10:29.120]  а снижение размерности это через одну лекцию, там поговорим про PCA. Но, я думаю, про метод
[10:29.120 --> 10:34.400]  главных компонентов тоже из вас многие слышали, правильно? Кто слышал? Понял, не многие слышали.
[10:34.400 --> 10:40.320]  А что такое сингулярное разложение? Может больше слышали? Хорошо, а что такое разложение по собственным
[10:40.320 --> 10:45.600]  векторам? Может больше слышали? О, вот. Ну, по сути, почти все одно и то же. То есть из разложений по
[10:45.600 --> 10:52.440]  собственным векторам можно дойти до PCA, причем достаточно быстро. Ну что ж, и также стоит сказать
[10:52.440 --> 10:57.720]  про линейные модели, что это крайне важная штуковина по двум причинам. Первая, наверное, в доброй
[10:57.720 --> 11:02.560]  половине задач, которые решается машинным обучением, вам достаточно линейная модель. Вот серьезно,
[11:02.560 --> 11:07.240]  вам не нужны убер нейронки, огромные бустинги и так далее, вы берете линейную модель, вставляете
[11:07.240 --> 11:13.320]  ее, она быстрая, устойченная, интерпретируемая, легко реализуется, короче, дешевая, сплошные плюсы
[11:13.320 --> 11:18.400]  и дает хорошее качество. Второе, без понимания того, как работают линейные модели, понять,
[11:18.400 --> 11:22.080]  как работают все эти новомодные нейронки, ну, в принципе, нельзя. То, что нейронки — это
[11:22.080 --> 11:26.440]  эдакие линейные модели на стероидах. В них понапихали нелинейные функции активации,
[11:26.440 --> 11:33.160]  придумали какие-то, скажем так, обоснованные некоторыми свойствами наших данных и какими-то
[11:33.160 --> 11:38.520]  мыслями о симметрии в наших данных преобразованиях. Например, свертки — это, по сути, тоже в некотором
[11:38.520 --> 11:42.640]  смысле линейное преобразование, его можно так представить и так далее. Поэтому понимание
[11:42.640 --> 11:47.000]  линейных моделей — это вообще кругольный, наверное, камень понимания двух третей всего машинного
[11:47.000 --> 11:52.600]  обучения. Поэтому, пожалуйста, не пренебрегайте им. И начнем мы с линейной регрессии. Собственно,
[11:52.600 --> 11:56.440]  что такое линейная регрессия? Это задача регрессии. В данном случае будем рассматривать
[11:56.440 --> 12:00.760]  одномерную, то есть мы просто-напросто предсказываем одну чиселку. Также есть векторная регрессия,
[12:00.760 --> 12:05.440]  линейная регрессия, где мы предсказываем много чиселок, то есть мы отображаем наши признаки в одно
[12:05.440 --> 12:11.560]  число. По сути, у нас есть датсет, пара x и y, x и y — наши объекты, y — целевые переменные,
[12:11.560 --> 12:17.600]  y приходит из R. Если у нас в общем случае это будет векторная регрессия, то есть с из Rk,
[12:17.600 --> 12:24.040]  где Rk — это какая-то камерная векторная штуковина. И, соответственно, наша задача найти некоторую
[12:24.040 --> 12:29.960]  линейную комбинацию наших признаков, чтобы получить ответ. Ну или можно записать вот в
[12:29.960 --> 12:35.360]  таком виде омега 0, это наш свободный член, плюс взвешенная сумма всех x, xкатый — это именно
[12:35.360 --> 12:41.240]  катый признак, не катый объект на всякий случай, но омегк, где у нас всего p признаков. Или что то
[12:41.240 --> 12:47.360]  же самое, так как математики, народ, скажем так, который любит писать все коротко и понятно,
[12:47.360 --> 12:53.480]  очень часто переписывают вот в таком формате. xу, скажем так, виртуально добавляют вот нашему
[12:53.480 --> 12:59.040]  каждому признаку, каждому объекту добавляют пективный признак на первое место, который всегда равен
[12:59.040 --> 13:05.560]  единице. Ну тогда, соответственно, вектор x — это 1, а потом x1, 2, 3, т.т. Зачем это надо? Ну потому что
[13:05.560 --> 13:10.320]  тогда мы можем с вами записать экспонированная омега, вот наша с вами линейная модель. Не надо
[13:10.320 --> 13:15.400]  никого в плюс b и так далее, все максимально красиво, понятно записано. Хорошо? Но еще раз обращай
[13:15.400 --> 13:19.480]  внимание, это чисто для удобства записи, потому что, когда мы начнем говорить про регуляризацию,
[13:19.480 --> 13:26.040]  вот здесь объявится большая проблема, и об нее можно споткнуться. Омега — наш вектор весов,
[13:26.040 --> 13:31.080]  омега 0 — свободный член, соответственно, он может его в себя включить, или же мы можем просто добавить
[13:31.080 --> 13:36.240]  со всей матрицы объект-признак столбец из единичек, и тогда у нас только будет омега-вектор весов,
[13:36.240 --> 13:41.800]  где омега 0 — это краска свободного члена. С ним вроде все понятно. И задача оптимизации, которую мы
[13:41.800 --> 13:46.400]  решаем, это минимизация какого-то функционала, например, функции ошибки, например, средней
[13:46.400 --> 13:52.160]  квадратичной, где у нас есть наши y и есть наша оценка y, то есть предсказание нашей модели. Мы
[13:52.160 --> 13:55.840]  можем с вами минимизировать средние квадратичные отклонения, можем минимизировать средние
[13:55.840 --> 13:59.320]  абсолютные, можете придумать там какую-нибудь квантильную функцию потери, что придумаете,
[13:59.320 --> 14:05.160]  то и будет, и это от вас зависит. На всякий случай. Вот эта конкретная постановка задачи МНК методом
[14:05.160 --> 14:13.600]  наименьше квадратов она решается. Тут вопросы есть, все понятно? 3, 2, 1, хорошо. Ну и соответственно,
[14:13.600 --> 14:18.480]  с этой задачей можно работать максимально просто почему, потому что это один из единственных
[14:18.960 --> 14:25.560]  случаев, где в машинном обучении у нас есть аналитическое решение. Не какая-то там попытка
[14:25.560 --> 14:29.320]  аппроксимировать непонятно что, непонятно как градиентными методами, а просто получить
[14:29.320 --> 14:33.720]  аналитическое решение. На всякий случай, вот этот вывод тоже хорошо бы уметь делать,
[14:33.720 --> 14:37.680]  потому что, во-первых, хорошо бы уметь матрички дифференцировать, во-вторых, на экзамене тоже
[14:37.680 --> 14:44.200]  спросим. Смотрите, вот наша функция, наш функционал, средней квадратичной ошибка, правильно? Ну или,
[14:44.200 --> 14:49.160]  как еще можно назвать, наша функция эмпирического риска может быть записана тем образом. Это квадрат
[14:49.160 --> 14:57.040]  отклонений, правильно? Ну или, что то же самое, мы пытаемся посчитать вектор отклонений и посчитать
[14:57.040 --> 15:02.760]  квадрат его второй нормы. Согласны? Тогда можем это переписать в каком виде. Собственно, как мы с вами
[15:02.760 --> 15:09.320]  на прошлой неделе разбирали, y-xω транспонированный на y-xω. Или, что же самое, вторая норма вектора
[15:09.320 --> 15:14.040]  отклонений, в квадрате. Почему, кстати, в квадрате, как вы думаете, почему бы это еще под квадратный
[15:14.040 --> 15:21.040]  корень не засунуть? Да, так проще считать. А решение задачи оптимизации поменяется, если под корень
[15:21.040 --> 15:29.800]  засунем? Нет, все это понимают. Все помнят, что корень у нас, как бы, какая величина, функция точнее,
[15:31.800 --> 15:38.160]  а везде монотонная. Молодцы, на области определения. Потому что, если вы будете корень из отрицательного
[15:38.160 --> 15:44.920]  числа доставать, то бескомплексной плоскости что-то как-то будет печально. Хорошо. Все, соответственно,
[15:44.920 --> 15:50.400]  вот наш x. На всякий случай, здесь с x-ами абсолютно все равно, или у нас не свободный член, или мы сюда
[15:50.400 --> 15:55.120]  вот включили нашу, нашу столбец из единичек, поэтому у нас свободный член сидит в матрице весов,
[15:55.120 --> 16:00.520]  пока все нормально. Ну и замечательно то, что мы с вами знаем, что в точке у оптима у нас производная
[16:00.520 --> 16:06.080]  равна нулю. Вообще говоря, квадратичная функция потерь парабол у нас выпуклая, правильно? Минимум там
[16:06.080 --> 16:11.320]  один, поэтому мы с вами можем найти решение для данного, для данной оптимизационной задачи.
[16:11.320 --> 16:17.360]  Приравниваем нулю, знаем, что у нас там минимум, там, где она равна нулю, и получается простенький
[16:17.360 --> 16:23.320]  вывод, что ω это x транспонированное x минус 1, x транспонированное y. На всякий случай еще раз
[16:23.320 --> 16:33.520]  проверьте себя вот сейчас визуально, какие размерности должны быть у x и у y. Вот я поэтому
[16:33.520 --> 16:42.120]  говорю сейчас, пожалуйста, внимательно проверьте размерности. Давайте вот сейчас все 10 секунд подумать,
[16:42.120 --> 16:47.600]  я пока горло смочу, а потом соответственно мы с вами скажем, что да как.
[16:47.600 --> 17:08.000]  Размерность в смысле матрицы, вот матрица x у нас в какой размерности, матрица y и матрица ω,
[17:08.000 --> 17:11.800]  соответственно, которую мы проравниваем. Это просто, чтобы вы в уме это все проверили и
[17:11.800 --> 17:15.800]  поняли, что здесь нигде ошибки нет. Потому что когда вы начнете это руками вводить,
[17:15.800 --> 17:19.520]  у третьей, наверное, появятся проблемы с тем, что краски и ничего не сходится, да?
[17:19.520 --> 17:28.360]  Ну, смотрите, здесь как раз-таки вот вопрос, если она вырожденная, вопрос. Пока считаем,
[17:28.360 --> 17:35.640]  что x транспонированное y, не вырожденное, все в порядке. Ну, давайте, хорошо. Собственно,
[17:35.640 --> 17:49.800]  матрица x. Какие у нее сейчас здесь размерности? Ну, что такое m, что такое n? Объектов? Аp,
[17:49.800 --> 17:55.000]  количество признаков. Хорошо, тогда это у нас n на p, тогда x транспонированное x,
[17:55.000 --> 18:01.720]  размерность какая будет? p на p. Все согласны? Эта матрица вам, кстати, ничего не напоминает,
[18:01.720 --> 18:09.840]  ну, вопрос к тем, кто статистику изучал. Хорошо, тогда потом про это. Хорошо, предполагаем,
[18:09.840 --> 18:14.120]  что она у нас не вырожденная, мы ее можем обратить. Потом опять x транспонированное y,
[18:14.120 --> 18:21.920]  p на p, на p на n, правильно? То есть здесь получается p на n, итоговая матрица. А y у нас в какой
[18:21.920 --> 18:28.200]  размерности? y это наши целевые переменные. По сути, это матрица, да, n на 1. Получается,
[18:28.200 --> 18:35.120]  размерность какая остается? p на n на n на 1, p на 1. Ну, короткий, вот наш вектор весов получился.
[18:35.120 --> 18:40.040]  Все правильно, все сошлось, ничего не перепутали. Я почему на это обращаю внимание, когда это начнете
[18:40.040 --> 18:44.240]  в коде писать, а вы начнете это в коде писать, это будет во второй домашке, можно перепутать
[18:44.240 --> 18:47.880]  матрицу, допустим, и транспонировать, тогда размерность матрицы весов окажется какая-то
[18:47.880 --> 18:52.920]  неправильная и так далее. И плюс, собственно, ваш вопрос, а что делать, если у нас свободный
[18:52.920 --> 18:57.600]  член добавился? А нам вообще все равно у нас размерность y зависит от размера обучающей выборки.
[18:57.600 --> 19:04.640]  Размерность x это p на n, n на p точнее, где n это количество объектов, а p количество признаков
[19:04.640 --> 19:11.480]  фиктивным, безфиктивного. Вы можете туда хоть еще 10 добавить, он все равно слопан. Уловили? Все
[19:11.480 --> 19:16.280]  поняли? И заметьте, мы с вами получили аналитическое решение для задачи линейной регрессии
[19:16.280 --> 19:20.080]  со средней квадратической функцией потерь. Все, вот оно у нас есть, на самом деле можете его
[19:20.080 --> 19:24.600]  запомнить, просто полезно его помнить, чтобы резко кому-нибудь ответить на собеседование, вам скажут,
[19:24.600 --> 19:30.360]  ух ты классно, вывести тоже можно за 20 секунд, так что выводить тоже полезно. Но, собственно, вопрос
[19:30.360 --> 19:35.280]  на экране, а что делать, если матрица x transponирована x выраженная? Что такое выраженная матрица? Все
[19:35.280 --> 19:40.560]  помнят, я надеюсь, правильно? Обратить мы ее не можем, если она прям выраженная, но, на самом деле,
[19:40.560 --> 19:45.240]  даже если она плохо обусловленная, то есть ее детерминат близок к нулю, то у нас с обращением
[19:45.240 --> 19:49.640]  будут большие проблемы. Потому что если на детерминат ближе к нулю, то когда мы с вами будем
[19:49.640 --> 19:56.560]  пытаться посчитать обратную матрицу, нам придется делить на определитель, это деление на очень маленькое
[19:56.560 --> 20:01.760]  число, это увеличение вычислительной ошибки и так далее, так далее, так далее. С этим все понятно.
[20:01.760 --> 20:07.240]  Хорошо, что делать? А главное, ладно, что делать, я вам скажу, а в каком случае это может возникнуть?
[20:07.240 --> 20:21.720]  Так, линии зависимых элементов, хорошо. Признаков или объектов? Ну вот, признаков или объектов?
[20:21.720 --> 20:33.000]  То есть у нас, на самом деле, система, как правило, переопределенная, если все хорошо, то есть у нас
[20:33.000 --> 20:39.440]  больше объектов, чем признаков, то есть у нас больше ограничений, чем наших свободных переменных.
[20:39.440 --> 20:45.720]  На самом деле, ответ правильный, если наши признаки линией зависимые, то вот в этой матрице
[20:45.720 --> 20:51.640]  появится, опять же, линия зависимости X transponable X. Вы посмотрите, как она получается. Когда мы с вами
[20:51.640 --> 20:57.000]  считаем X transponable X, по сути что? Мы берем и, по сути, transponable X, мы говорим, вот этот признак
[20:57.000 --> 21:02.200]  равен вот таким величинам у этих объектов, правильно? И другой признак равен таким-то величинам
[21:02.200 --> 21:06.680]  у этих объектов. Если у нас два признака, ну, для простоты, например, одинаковые или они
[21:06.680 --> 21:13.800]  пропорциональны друг другу, то получается, что для всех объектов признак 1 будет там равен 0, 4, 2, 7, 5,
[21:13.800 --> 21:20.400]  а признак 2, который с ним линией зависим, например, будет ровно всегда в два раза больше. Согласны?
[21:20.400 --> 21:27.160]  Соответственно, мы умножаем эти строки сами на себя и получается что? Что у нас, как раз таки,
[21:27.160 --> 21:31.160]  два признака линии зависимые, у нас получается матрица выраженная, потому что мне теперь строки,
[21:31.160 --> 21:35.320]  ну, или столбцы, потому что она, вообще говоря, квадратная линия зависимая, и у нас проблемы с этим.
[21:35.320 --> 21:40.520]  Обратить мы ее не можем. Тобственно, когда у нас появляются с вами зависимые признаки, мы не
[21:40.520 --> 21:46.040]  можем найти решения. Но тут, на самом деле, на это можно посмотреть и с точки зрения просто физического
[21:46.040 --> 21:51.240]  смысла. Смотрите, предположим, у нас с вами есть два признака, которые зависимы друг с другом,
[21:51.240 --> 21:56.720]  например. Для самого выраженного случая, но просто с ним проще, мне не надо в уме какие-то
[21:56.800 --> 22:03.360]  преобразования делать. Представим себе, что у нас в выборке два признака, вес 1 и вес 2. Это абсолютно
[22:03.360 --> 22:09.080]  один и тот же признак. Окей? И мы, на самом деле, знаем, что вес человека влияет на солевую переменную,
[22:09.080 --> 22:16.360]  допустим, с коэффициентом 5. Вот Омега для веса, где она тут, вот здесь, будет равна 5. Но у нас с вами
[22:16.360 --> 22:20.560]  два раза вес присутствует. Значит, соответственно, первые элементы, второй элемент веса суммарно
[22:20.560 --> 22:26.320]  должны делать склад равной 5. Согласны? Но потому что первая половинка делает там склад какой-то,
[22:26.320 --> 22:30.560]  вторая кое-то, вместе они влияют как 5, потому что вес-то один, просто он почему-то был продублирован.
[22:30.560 --> 22:37.240]  А теперь внимание, проблема. Если я возьму веса, у веса, да, веса, параметр значения,
[22:37.240 --> 22:43.840]  Омега 1 для веса 1 будет равно 10, а Омега 2 для веса 2 будет равно минус 5, в сумме у них что
[22:43.840 --> 22:50.840]  получится? 5, 6 и минус 1, 7 и минус 2. У вас получается континенуальное множество решений. У вас
[22:50.840 --> 22:55.440]  континенум решений, вы можете любую пару чисел, которые в сумме дают вам 5, взять и получить
[22:55.440 --> 23:00.520]  абсолютно тот же самый ответ. Так что то, что мы здесь не можем на самом деле найти аналитическое
[23:00.520 --> 23:06.240]  решение, а у нас оно в любом случае не единственное. Так что это на самом деле нормально и физическая
[23:06.240 --> 23:11.000]  обоснованность тоже имеет. Улавливаете пока? То есть линейно-зависимые признаки аналитическое
[23:11.000 --> 23:15.960]  решение мы найти не можем, плюс мы не можем найти. Единственное решение в принципе, его не существует.
[23:15.960 --> 23:25.360]  Классный вопрос и классное предложение. Если признаки линейно-зависимые, их можно выкинуть.
[23:25.360 --> 23:29.800]  Если мы об этом знаем, конечно надо выкинуть. Но проблема в том, что я вам привел пример совсем
[23:29.800 --> 23:34.640]  игрушечный. А на практике нам абсолютно не важно, у нас линейная зависимость, два признака равны
[23:34.640 --> 23:41.040]  друг другу или признак номер 328 это линейная комбинация предыдущих 327. Плюс у нас есть шум,
[23:41.040 --> 23:46.320]  плюс признаков может быть там 10 тысяч. Так что найти какие из них линейно-зависимые друг с
[23:46.320 --> 23:50.760]  другом может быть большой большой проблемой. Просто дорого и мы просто мы не можем этого явно
[23:50.760 --> 23:56.120]  увидеть. Поэтому выкинуть в общем случае нам плыжновато. Окей, с этой проблемой разобрались?
[23:56.120 --> 23:59.840]  Ну что ж, давайте тогда пытаться это чинить. Да?
[24:11.040 --> 24:13.120]  Что разошелись объекты?
[24:13.120 --> 24:24.440]  Вы имеете для одного и того же признаков описания два разных ответа?
[24:24.440 --> 24:33.960]  Ну окей, ну так бывает. Например, ну есть понятие выбраться, где у вас для нормального объекта
[24:33.960 --> 24:37.600]  абсолютно неправильный ответ, например. Или у объекта в принципе неправильный признак
[24:37.600 --> 24:41.520]  описания. С этим придется работать, так как мы если мы не можем их отфильтровать,
[24:41.520 --> 24:45.200]  значит придется строить такие модели, которые устойчивы к наличию выбросов. Правильно тоже
[24:45.200 --> 24:48.840]  поговорим. То есть в общем случае, смотрите, когда мы решаем с вами оптимизационную задачу,
[24:48.840 --> 24:54.440]  вот методом оптимизации в принципе оптимизируем функционалу абсолютно по барабану, что у нас там
[24:54.440 --> 25:00.080]  с данными. Грязное, нечистое. Мы засунули на вход выборку, по сути мы засунули систему линейных
[25:00.080 --> 25:06.240]  уравнений, ответов по сути ограничения мы дали. Оно нашло ответ. Все. Так что если у нас плохие
[25:06.240 --> 25:11.280]  данные, это наша с вами проблема. Мы с этим будем жить. На самом деле принцип, ну его обычно по
[25:11.280 --> 25:15.600]  английски называют garbage in, garbage out, он работает почти везде. Если у вас плохие данные, у вас могут
[25:15.600 --> 25:20.760]  быть хоть идеальные модели, у вас будет плохое предсказание. Так что нам придется как-то чистить
[25:20.760 --> 25:25.640]  данные, строить более устойчивые модели и вообще оценивать насколько плохие у нас данные. Ответил
[25:25.640 --> 25:31.480]  наш вопрос? Супер. Ладно, матрица может быть выраженной. Давайте вам на примере как раз покажу. Вот
[25:31.480 --> 25:36.640]  собственно ситуация, где у нас два признака или там несколько признаков, короче зависимые между
[25:36.640 --> 25:41.200]  собой, в простейшем случае два признака линей независимая. Матрица X и X соответственно выраженная.
[25:41.200 --> 25:45.800]  Вот у нас есть истинное значение omega true, мы с вами на семинаре сегодня ровно проделаем, это оттуда
[25:45.800 --> 25:53.760]  скриншоты. Допустим вот 268, минус 0.52, минус 1.12. Второй-третий признаки между собой зависимы, то есть это
[25:53.760 --> 25:57.480]  почти один и тот же признак, с точностью дам до какого-то маленького-маленького шума. То есть
[25:57.480 --> 26:02.800]  мы все еще можем обратить матрицу X и X, но ошибка у нас большая. Вот аналитическое решение,
[26:02.800 --> 26:07.720]  которое мы получаем буквально формулой за пистом. X и X минус 1, X и Y и так далее. Смотрите,
[26:07.720 --> 26:12.720]  первый член абсолютно правильно определен, с точностью до того, что у нас присутствует небольшой
[26:12.720 --> 26:19.480]  шум данных, то есть 268, два знака после запятой, правильно. Второй-третий член, минус 186, 184, ровно
[26:19.480 --> 26:24.920]  то, о чем я вам говорил. Но обратите внимание, сложите второй-третий признаки, веса точнее их.
[26:24.920 --> 26:35.360]  Получится примерно минус 0.6. Фу, минус 1.06, согласны? Здесь положите. Получится то же самое. У нас
[26:35.360 --> 26:39.400]  одно ограничение, потому что у нас по сути признак один, но он дважды продублирован. У нас одно
[26:39.400 --> 26:43.840]  ограничение, поэтому на их сумму у нас ограничение есть. На каждой подельности у нас ограничений нет.
[26:43.840 --> 26:50.400]  У нас любая комбинация может этому быть равна. Ну, собственно, возникает вопрос, а что с этим делать?
[26:50.400 --> 26:54.600]  Там уже маленький спойлер был, где-то полсекунды. Что делать, если мы с вами не
[26:54.600 --> 27:00.960]  можем эту матрицу обратить? Вообще, что делать с матрицей, если она необратима? Пошуметь?
[27:00.960 --> 27:10.320]  Классно, зашуметь. Но вот мы с вами зашумели матрицу, но мы с вами при этом потеряли
[27:10.320 --> 27:15.400]  достаточно много информации из-за этого, потому что мы шум добавили прям явно туда. А чем больше
[27:15.400 --> 27:19.840]  у нас, грубо говоря, чтобы у нас матрица была точней выраженная, шум должен быть достаточно
[27:19.840 --> 27:23.800]  большой. Потому что если у нас шум будет крайне маленький, то он мало повлияет на определитель,
[27:23.800 --> 27:29.400]  и, соответственно, у вас матрица все еще будет очень близка к выраженной. Что еще можно сделать?
[27:29.400 --> 27:37.600]  Убрать скоррелированные признаки можно, но если мы их знаем. Если у нас там 10 тысяч признаков,
[27:37.600 --> 27:41.760]  и у них зависимости крайне сложные, то их сложновато убрать. Что еще можно сделать?
[27:41.760 --> 27:53.200]  Классный вопрос. А в чем вообще беда? У нас получилось два признака, они в
[27:53.200 --> 27:58.440]  сумме дают что-то там правильное. Они же все равно правильную сумму-то дают. А ответ на самом деле
[27:58.440 --> 28:04.680]  простой. У нас данные на самом деле всегда содержат в себе шум. И, во-первых, у нас вот эти
[28:04.680 --> 28:09.720]  параметры конкретно, значения параметров подобраны под шум в обучающей выборке, и, как правило,
[28:09.720 --> 28:15.920]  очень часто у нас будут получаться значения, которые почти противоположны друг другу. Потому
[28:15.920 --> 28:20.640]  что они как раз подстраиваются под шум, характерное значение шума маленькое относительно признаков,
[28:20.680 --> 28:25.520]  поэтому пытаются настроиться именно на шум, поэтому эти значения большие. Но на новых данных,
[28:25.520 --> 28:30.800]  которых мы не видели на этапе обучения, шум будет другой. Потому что он случайный, он каждый раз
[28:30.800 --> 28:35.240]  случайно генерируется. И у нас может казаться, что вот на этот признак, условно, прилетело значение
[28:35.240 --> 28:42.960]  шума 0.1, а вот на этот не прилетело. И у вас целевая перемена завышена на 186,0 на 10. Потому что мы
[28:42.960 --> 28:48.120]  вот за счет вот этих вот величин начинаем переобучаться под обучающую выборку. То, что у нас
[28:48.120 --> 28:52.880]  два параметра для того, чтобы запомнить только одно значение. Соответственно, у нас по сути один
[28:52.880 --> 29:00.360]  параметр есть, чтобы запомнить что-то специфичное только обучающей выборке. Абсолютно значить? Не,
[29:00.360 --> 29:06.280]  надо уменьшить. Классное замечание. Собственно, мы к нему сейчас придем. Коллеги, смотрите, на это
[29:06.280 --> 29:10.160]  очень полезно смотреть, с какой стороны, и я неспроста вот так медленно это все разжевываю. Я хочу,
[29:10.160 --> 29:13.680]  чтобы вы до этого прямо сами додумались, а не просто я вам это сказал. Мы же с вами изначально
[29:13.680 --> 29:19.560]  сказали, у нас проблем в чем? У нас с вами решений множество, вообще континуум. У нас не единственное
[29:19.560 --> 29:24.080]  решение, правильно? Если у нас не единственное решение, мы бы хотели все-таки одно решение как-то
[29:24.080 --> 29:32.080]  получить. Согласны? Решать другую задачу. Классно. Какую? Смещенную. Ну классно, вы понимаете,
[29:32.080 --> 29:36.840]  о чем речь. Супер. Давайте тогда наложим какое-то дополнительное ограничение на наш вектор весов.
[29:36.840 --> 29:42.520]  Вот тут было классное предложение. Раз у нас присутствует шум, шум случайный, особенно на тестовых
[29:42.520 --> 29:47.440]  данных, тогда давайте потребуем, чтобы, например, этот вектор был наименьший по какой-нибудь норме.
[29:47.440 --> 29:52.120]  Ну, например, тогда, если у вектора маленькая норма, значит у него все члены тоже маленькие.
[29:52.120 --> 29:58.160]  Согласны? Но норма это все равно, если ограничивать норму, ограничивать значение членов. Я согласен.
[29:58.160 --> 30:02.880]  Можно это посмотреть, на самом деле, с другой стороны. Можно посмотреть вот на эту матрицу.
[30:02.880 --> 30:10.040]  На самом деле, как из выраженной матрицы сделать невыраженную. Вот проще, чем зашумить. Мысли
[30:10.040 --> 30:15.280]  более детерминированно. Ну, например, прибавить единичную. Она явно диагональная, тогда у нас
[30:15.280 --> 30:21.440]  получится явно невыраженная матрица. Заметьте, эта матрица квадратная всегда. Согласны? Так что,
[30:21.440 --> 30:26.400]  если мы добавим к ней диагональную матрицу, то мы получим невыраженную матрицу. На самом деле,
[30:26.400 --> 30:31.960]  то, что мы сказали, это плюс-минус одно и то же. И это, по сути, называется регуляризация по-тихому.
[30:31.960 --> 30:36.800]  Мы можем с вами всегда добавить единичную матрицу. Ну, домножим на какой-то коэффициент лямбда. И на
[30:36.800 --> 30:42.360]  самом деле, тогда вот эта матрица у нас явно будет невыраженная. Её всегда можно обратить. Согласны? И
[30:42.360 --> 30:50.880]  это у нас будет решением вот такой задачи оптимизации. Вот этот вывод мы вас сейчас попросим сделать
[30:50.880 --> 30:56.240]  самостоятельно. Это можно вывести на семинаре, но практика краски. Он отличается от вот этого на
[30:56.240 --> 31:01.400]  две строчки, поэтому я вас прошу сделать самостоятельно. Окей? Можете считать частью
[31:01.400 --> 31:06.800]  домашней. Собственно, вот наш функционал. И теперь мы помимо средней квадратической ошибки
[31:06.800 --> 31:15.720]  минимизируем вторую норму вектора весов в квадрате с коэффициентом лямбда. Вот. Что такое лямбда?
[31:15.720 --> 31:22.120]  Лямбда, по сути, это гиперпараметр, который говорит нам, насколько важно для нас, чтобы норма вектора
[31:22.120 --> 31:26.160]  весов была маленькая. То, что мы теперь, по сути, два функционала минимизируем. У нас вот этот
[31:26.160 --> 31:31.760]  функционал хочет лямбду как можно более подходящей, чтобы ошибка на выборке обучающей была минимальной.
[31:31.760 --> 31:36.600]  А этот функционал говорит, что не, слушай, давай-ка Омега у нас будет как можно меньше, потому что
[31:36.600 --> 31:41.360]  иначе штраф большой. По сути, мы пытаемся с вами решить две задачи, которые в разные стороны нас на
[31:41.360 --> 31:47.040]  самом деле тянут. Но здесь на самом деле есть еще один большой плюс. Если у нас ситуация невыраженная
[31:47.040 --> 31:52.200]  и все хорошо, то у нас вот эта задача имеет единственное решение. Все прекрасно. Если у нас
[31:52.200 --> 31:56.680]  решение здесь не единственное, у нас вот эта задача минимизации все равно имеет единственное решение.
[31:56.680 --> 32:01.040]  То есть среди всего континуума значений, которые заставляют одинаковое значение функции потерь,
[32:01.040 --> 32:06.560]  только одна пара даст нам, не одна пара, одно под множество значений весов, даст нам минимальное
[32:06.560 --> 32:11.560]  значение функции ошибки. Но почему? Что такое вторая норма квадрата? Это сумма квадрата. Это выпуклый
[32:11.560 --> 32:28.720]  функционал. Согласны? Согласны. Тишина. У пара был один минимум. Точно? Ну, короче, мы делаем задачу
[32:28.720 --> 32:34.720]  выпуклой, грубо говоря, за счет этой регуляризации. И, собственно, отсюда и появляется вообще у нас
[32:34.720 --> 32:39.600]  в курсе термин регуляризация. На самом деле регуляризацию называют, опять же, от английского языка,
[32:40.280 --> 32:46.360]  ограничивать. Мы, по сути, дополняем нашу задачу некоторыми ограничениями, потому что мы не можем
[32:46.360 --> 32:51.800]  решить исходную задачу единственным образом или таким образом, чтобы это удовлетворяло нашим
[32:51.800 --> 32:56.160]  интересам. Мы не знаем, какое именно решение выбрать, поэтому мы дополнительное ограничение кладем.
[32:56.160 --> 33:00.880]  Например, накладываем, чтобы у нас норма вектора в основном была минимальна. Тогда решение единственное,
[33:00.880 --> 33:05.600]  плюс мы уже знаем, какие свойства ожидать от нормы нашего вектора весов и от вектора весов,
[33:05.600 --> 33:14.640]  соответственно. Смотрите, почему такая функция потерь? Ровно потому, что у нас просто, по сути,
[33:14.640 --> 33:21.560]  добавляется вторая норма вектора весов. Со чем? Со второй нормой вторая норма вектора весов,
[33:21.560 --> 33:26.320]  а лямбда это просто коэффициент, в котором мы ее вкладываем. По сути, это коэффициент регуляризации.
[33:26.320 --> 33:30.400]  Чем больше лямбда? Ну, предположим, мы лямбду стремили в бесконечность. Там где-то вот она очень
[33:30.400 --> 33:34.960]  большое число. Тогда для решения задачи оптимизации, какое решение будет оптимальным?
[33:34.960 --> 33:42.200]  Ну, примерно да, нулевые веса, потому что у нас вклад вот этого члена будет много больше,
[33:42.200 --> 33:48.840]  чем вклад вот этого члена. Если лямбда близка к нулю, то какое решение оптимальное? То,
[33:48.840 --> 33:52.640]  которое позволяет переобучиться под этот функционал. Лямбда это просто наш, грубо говоря,
[33:52.640 --> 33:56.400]  регулятор, насколько важно нам получить невыраженное решение. Все.
[33:56.400 --> 34:06.720]  Эта схема работает всегда в том, что если вы к невыраженной матрице добавить диагонально,
[34:06.720 --> 34:16.240]  она выраженной не станет. О, кстати, кстати, кстати, кстати, кстати, кстати, вот тут квадрат не
[34:16.240 --> 34:21.280]  хватает. Ну, в смысле, лямбда должна быть положительная. В такой формулировке, вот,
[34:21.280 --> 34:26.000]  прошу прощения, с опечатком. Ну, тут или ограничение, что лямбда больше или равна нулю,
[34:26.000 --> 34:31.000]  или, соответственно, здесь надо везде квадрат писать. Окей, чтобы вас не запутать.
[34:31.000 --> 34:44.080]  Не, смотрите, мы реальное значение с вами уже никогда не узнаем почему, потому что если у нас
[34:44.080 --> 34:51.320]  с вами два признака, грубо говоря, дублируют друг друга, то вот у вас 2, вес 1, вес 2. Вы только знаете,
[34:51.320 --> 34:59.080]  что вклад вес 1 и вес 2 вместе равен 1. Нет никакого правильного значения признака веса для веса 1 и
[34:59.080 --> 35:04.080]  вес 2. Поэтому мы с вами, собственно, и разрешаем эту неопределенность, говоря, окей, выберем тот,
[35:04.080 --> 35:18.440]  который доставляет на меньшую норму векторовесов. Если лямбда больше или равна нуля, матрица не станет
[35:18.440 --> 35:26.960]  выражена и почему. Смотрите, что у нас здесь стоит? X транспонировано X. На диагонали что стоит?
[35:26.960 --> 35:34.760]  Что? Это что вообще? X транспонировано X, что задается в точке верения там аналита или нала?
[35:34.760 --> 35:45.200]  Вам слово квадратичная форма ничего не говорит? Ну, смотрите, у нас, собственно, здесь на диагонали
[35:45.200 --> 35:49.880]  будут стоять квадраты X. Мы к ним добавляем строг не отрицательную диагональ все остальной нули.
[35:49.880 --> 36:06.760]  Невыраженная она будет. Так, хорошо. Собственно, еще раз повторяю. Да. Какую? Смотрите, тут можно
[36:06.760 --> 36:10.680]  зайти с двух сторон, собственно, они на самом деле эквивалентны. Мы можем сказать, эта матрица
[36:10.680 --> 36:15.760]  выраженная, сделаем ее невыраженной как? Добавим к ней диагональную матрицу. Ну, например, лямбда,
[36:15.760 --> 36:20.040]  опять же, здесь лямбда не отрицательная, здесь тогда квадрат по-хорошему надо добавить, это
[36:20.040 --> 36:25.480]  опечатка. Тогда матрица невыраженная. Но это эквивалентное решение вот такой задачи. На самом
[36:25.480 --> 36:30.440]  деле я не уверен, как было изначально, то есть можно просто добавить ограничение на нашу норму
[36:30.440 --> 36:35.800]  векторовесов и получится, что есть аналитическое решение, которое ровно такой формат имеет. То есть
[36:35.800 --> 36:42.200]  с двух сторон приходим к одному и тому же. Вот. Ладно. И почему это называется, еще раз, регуляризация?
[36:42.200 --> 36:47.120]  Ограничение, которое мы наложили. На всякий случай, вот эта вот норма вторая векторовесов,
[36:47.120 --> 36:51.840]  это далеко не единственный способ регуляризации, мы с вами их десятку видим. Можно ограничивать
[36:51.840 --> 36:56.720]  количество признаков в модели, можно ограничивать ее структуру в нейронках, глубину дерева в деревьях,
[36:56.720 --> 37:01.760]  другую норму взять, что угодно. Любое ваше априорное предположение, которое вы накладываете на
[37:01.760 --> 37:07.960]  решение задачи, это ваши ограничения, ваши регуляризации. И с регуляризацией, как и со всеми
[37:07.960 --> 37:12.600]  предположениями, работает какое правило? Если у вас хорошее предположение, которое подходит под
[37:12.600 --> 37:18.840]  решение для задачи, вы получаете хороший результат. Если ваши решения противоречат задачи, то от них
[37:18.840 --> 37:28.080]  станет хуй. Поэтому нет такого правила, что с регуляризацией всегда лучше. Окей? Да. Лямду
[37:28.080 --> 37:33.640]  выбирать из соображений того, что у вас есть гиперпараметры, гиперпараметры, как правило,
[37:33.640 --> 37:38.120]  подбираются на кросс-валидации. Про кросс-валидацию мы в конце лекции как раз поговорим. То есть гиперпараметры
[37:38.120 --> 37:43.120]  мы не можем автоматически подобрать, мы их выбираем, например, по качеству поведения нашей модели на
[37:43.120 --> 37:50.640]  валидирующей выборке, то есть на отложенной выборке. Хорошо, еще вопросы тут есть? Окей,
[37:50.640 --> 37:56.880]  я тогда сразу маленький спойлер делаю. Я думаю, многим из вас очевидно, что вторая норма, это далеко
[37:56.880 --> 38:02.480]  не единственная норма, которая здесь может стоять. Согласны? С второй нормой есть только одно
[38:02.480 --> 38:08.960]  важное замечание. Для второй нормы у нас есть аналитическое решение, что с регуляризацией,
[38:08.960 --> 38:13.360]  что без регуляризации. Это, наверное, единственный случай, где в линейной регрессии есть аналитическое
[38:13.360 --> 38:18.280]  решение. Вы можете сюда первую норму подставить, решаться это будет, но только градиентным методами
[38:18.280 --> 38:23.960]  аналитического решения уже не будет. И вторая важная вещь про вторую норму. Теориям Гаусс Маркова.
[38:23.960 --> 38:30.080]  Теориям Гаусс Маркова дает нам очень важные гарантии, на самом деле, и это та причина,
[38:30.080 --> 38:34.880]  почему среднюю квадратичную ошибку вообще любят, в том числе. Вот у нас есть наша целевая
[38:34.880 --> 38:38.800]  переменная. Заметьте, это уже не наша оценка, это вот наше предположение, что есть целевая
[38:38.800 --> 38:44.040]  переменная, которая зависит от XA линейным образом, но в наличии также какой-то шум. Идет?
[38:44.040 --> 38:53.360]  Все согласны. Так, на всякий случай, а кто вообще знает теорию Гаусс Маркова? Никто. Классно. Не,
[38:53.480 --> 39:01.080]  ладно, бывает. Поэтому мы ее разбираем. Еще раз, ε это у нас случайно величины, это наш шум. Данных
[39:01.080 --> 39:06.960]  присутствует шум. Всем окей это, правильно? Наши предположения, во-первых, эти у нас величины
[39:06.960 --> 39:12.360]  независимы, на самом деле, в идеальном случае, когда мы хотим. То есть в идеале наш шум должен
[39:12.360 --> 39:16.840]  быть независимый. Почему? Потому что если наш шум зависимый, то, в смысле, он от переменной как-то там
[39:16.840 --> 39:22.560]  зависит от XA, например, то что-то это на шум не похоже, на самом деле. Но теориям Гаусс Маркова
[39:22.560 --> 39:26.240]  делать, на самом деле, три очень маленьких и простых предположения. Во-первых, у нас шум не
[39:26.240 --> 39:31.040]  смещенный. То есть мат ожидания каждого ε равно нулю. То есть мы можем как завысить наше
[39:31.040 --> 39:38.080]  предсказание, так и занизить. Хорошо? Целевой пример. Во-вторых, у нас есть какая-то дисперсия,
[39:38.080 --> 39:42.520]  которая конечна. То есть у нас нету случайных величин здесь, которые имеют бесконечную дисперсию.
[39:42.520 --> 39:48.120]  Мы не можем получить, как бы, с какой-то значимой вероятностью там отклонения в десятки миллионов.
[39:48.160 --> 39:57.520]  Да? Да, конечно. Хорошо. И третье, собственно, у нас для всех и не равно 0, ковриация между ними
[39:57.520 --> 40:09.560]  равна нулю. Шутка ковриации все помнят? Хорошо. Вар дисперсия варианс. Ну можно еще написать вот
[40:09.560 --> 40:17.720]  такую D, но лотех в ней не умеет. Поэтому написано вот так. Вот, три условия. Все условия понятны,
[40:17.720 --> 40:25.040]  правильно? Еще раз, у нас все ошибки несмещенные относительно нуля. У нас есть конечная дисперсия
[40:25.040 --> 40:36.480]  и ковриация наших ошибок равна нулю. Да, в данном случае давайте будем считать просто
[40:36.480 --> 40:41.440]  фиксированная дисперсия, окей? Которая конечна, главное. На самом деле можно пойти на какие-то
[40:41.440 --> 40:45.520]  обобщения, чтобы даже были разные дисперсии, лишь бы они были все конечны, но пока вот просто
[40:45.520 --> 40:51.440]  равно сигн. Хорошо. И, собственно, в этих предположениях вот это решение, которое мы
[40:51.440 --> 40:57.240]  с вами только что видели. Омега транспонирована х, х транспонирована х-1, х транспонирована у,
[40:57.240 --> 41:03.560]  то есть решение задачи на меньше квадратов является оптимальным среди несмещенных. Говоря
[41:03.560 --> 41:10.640]  по-русски или по-английски best linear unbiased estimator. Наилучшей среди несмещенных. Вопрос,
[41:10.640 --> 41:22.640]  собственно, такой. Во-первых, что значит несмещенных? Без лямбда. Хорошо. А на практике что значит?
[41:22.640 --> 41:32.480]  Мат ожидания кого? Бинго. Наша оценка является несмещенной, у нас же омега с крышкой, что будет?
[41:32.480 --> 41:37.360]  Она же у нас зависит от каких-то случайных величин, правильно? Вот у нас тепсилы сидят,
[41:37.360 --> 41:43.720]  так что это по сути тоже случайная величина. Мат ожидания омеги с крышкой равно омеги. Это
[41:43.720 --> 41:50.400]  значит, что наша оценка несмещенная. Вот. Это решение заставляет нам несмещенную оценку. Это раз.
[41:50.400 --> 41:54.760]  Просто обращаю ваше внимание, потому что иногда начинают говорить, что у нас несмещенная оценка
[41:54.760 --> 42:01.560]  у. Тоже вариант, но в данном случае теорема гласит именно обоим. На что значит всё это наилучшее?
[42:01.560 --> 42:18.240]  Ладно, что такое линейное, понятно? А что про эффективную говорили? Окей, что такое эффективная
[42:18.240 --> 42:31.080]  оценка? Да, на самом деле. Можете ещё рассказать? Опять же наименьшую дисперсию имеет кто? Бинго.
[42:31.080 --> 42:35.840]  Смотрите, наилучшая или оптимальна в середине смещенных. Омега с крышкой, во-первых, несмещенная,
[42:35.840 --> 42:42.120]  то есть мат ожидания омеги, равно омега с крышкой. И у неё наименьшая дисперсия опять же у омеги с
[42:42.120 --> 42:47.640]  крышкой. Не у какой-то там ошибки, у чего-то. Окей? Собственно, у нас есть гарантия. Эта теорема
[42:47.640 --> 42:51.440]  на самом деле оказывается в три строчки. Можем доказательный семинарию или сделать самостоятельно
[42:51.440 --> 42:57.880]  опять же на лекцию. Я стараюсь доказательств теорем не вытаскивать. Это решение у нас является
[42:57.880 --> 43:08.920]  оптимальным в середине смещенных. Окей? Мы-то на практике с вами не знаем реальное значение,
[43:08.920 --> 43:14.720]  но мы с вами знаем выборку. У нас есть выборка. И мы можем получить оценку наших параметров на
[43:14.720 --> 43:19.840]  основании выборки. Вот с такими предположениями, тремя, оценка, которую мы получаем вот отсюда,
[43:19.840 --> 43:30.200]  будет оптимальным. Х и Х? Вот как раз-таки это очень хороший вопрос. Здесь мы предполагаем,
[43:30.200 --> 43:34.520]  конечно, что она обратима. Если она не обратима, то мы это просто посчитать не можем. И вторая
[43:34.520 --> 43:39.360]  проблема, если она, скажем так, обратима, но почти выражена, тут возникает другая проблема,
[43:39.360 --> 43:44.360]  что у нас вычислительная точность наших машин конечна. Поэтому чем ближе она к сингулярной,
[43:44.360 --> 43:48.720]  тем больше у нас здесь будет вычислительная ошибка. То есть это конечно работа, когда мы можем
[43:48.720 --> 43:52.640]  посчитать омегу. Если у нас матрица выраженная, то у нас вообще не существует такой оценки,
[43:52.640 --> 43:59.760]  потому что она не единственная. Хорошо. Ну и теперь, собственно, пара замечаний. Смотрите,
[43:59.760 --> 44:06.800]  возвращаемся назад. Вот у нас с вами минимизация среднеготоритичной ошибки. Задача линейной
[44:06.800 --> 44:12.960]  регрессии. Теория Магауса Маркова работает? Предположение о тех свойствах шумов наших данных.
[44:12.960 --> 44:19.520]  Ещё раз. Минимизируем вот эту функцию ошибки. Теория Магауса Маркова работает,
[44:19.520 --> 44:28.360]  если эти три предположения выполняются? Да, классно. Минимизируем вот такую, вот
[44:28.360 --> 44:44.000]  такой функциональный амперический риск. Теория Магауса Маркова работает? Почему? Ага, классно.
[44:44.000 --> 44:54.760]  А решение у нас какое? Бинго. Ребят, ещё раз. Теория Магауса Маркова гласит, что вот это решение
[44:54.760 --> 44:59.720]  является оптимальным решением для задачи на меньше квадратов. То есть для минимизации среднеготоритичной
[44:59.720 --> 45:06.320]  функции ошибки только. Всё. Если вы добавляете вот сюда любую другую функцию, любой другой
[45:06.320 --> 45:10.040]  функционал, если у вас не прост среднеготоритичная ошибка, у вас уже посылка теория Магауса Маркова
[45:10.040 --> 45:14.840]  неправильно. Вы не задача на меньше квадратов рассматривать. Поэтому теория Магауса Маркова работает
[45:14.840 --> 45:19.280]  тогда только, когда у вас минимизируется среднеготоритичная функция ошибки. В данном
[45:19.280 --> 45:24.120]  случае у вас уже появился второй член. И здесь на самом деле можно увидеть ещё одну вещь. У вас
[45:24.120 --> 45:29.360]  оценка будет априори смещенной. Почему? Потому что для минимизации среднеготоритичной функции
[45:29.360 --> 45:35.400]  ошибки вы явно не хотите минимизировать норму вектора весов. Собственно, у нас добавление
[45:35.400 --> 45:39.640]  регуляризации, как правило, даёт нам смещенное решение, потому что теперь относительно изначального
[45:39.640 --> 45:44.760]  оптимума нашего функционала мы ищем другой оптимум, который обладает заданными нами
[45:44.760 --> 45:55.280]  свойствами. Нет. А как? Мы с вами, смотрите, мы с вами сделали по сути что? У нас была изначальная
[45:55.280 --> 46:00.320]  оптимизационная задача. Мы сказали, что мы не можем её решить либо технически, либо у неё не единственное
[46:00.320 --> 46:06.400]  решение. Мы придумали, по сути, как поставить новую оптимизационную задачу. Вот она теперь, вот её мы
[46:06.400 --> 46:12.840]  решаем. И, собственно, для этой оптимизационной задачи найти решение. Вот её мы решили. Мы не решали
[46:12.840 --> 46:17.520]  другую оптимизационную задачу. Мы не знаем какое у неё решение. У нас нет обратного хода. Мы
[46:17.520 --> 46:34.480]  решили ту задачу, которую мы поставили. Да? Да. Там В с крышкой был набор случайных величин, в
[46:34.480 --> 46:39.480]  теории, так как у вас Х, в общем случае, это всегда реализация, как и Y, реализация случайной величины,
[46:39.480 --> 46:47.880]  вы получили точно так же оценку. Это набор чиселок. Конечно. Смотрите, вы, собственно,
[46:47.880 --> 46:53.160]  и получаете, на самом деле, когда вы средний квадрат ошибки минимизируете, вы считаете
[46:53.160 --> 47:14.360]  краски мат ожидания вот этой величины. Всё правильно. Вот. Добавляя по одному признаку. Да,
[47:14.360 --> 47:23.160]  смотрите, вы предлагаете... Замечательный комментарий, собственно. Коллега ваш предложил
[47:23.160 --> 47:28.000]  отфильтровать признаки, то есть отобрать только те, которые образуют некоторые линии независимые
[47:28.000 --> 47:33.240]  под выборку. Да, так можно делать. Такие подходы тоже есть. Отбора признаков. Итеративные. Минус
[47:33.240 --> 47:37.360]  в том, что когда у нас, опять же, очень много признаков, например, там десятки тысяч, это
[47:37.360 --> 47:40.840]  очень дорого вычислительно. Вам для каждого надо всё равно тогда решить оптимизационную задачу,
[47:40.840 --> 47:46.240]  причём для каждого подможества выбрать оптимальный, повторить. Это просто дорого. Вот.
[47:46.240 --> 47:51.880]  Собственно, а с регуляризацией плюс в чём? Да, мы получаем смещённое решение. Но, во-первых,
[47:51.880 --> 47:55.720]  у нас от лямбды зависит, собственно, насколько сильно оно будет смещённым. Чем меньше лямбда,
[47:55.720 --> 48:00.520]  тем меньше у нас смещение. Во-вторых, решение вы всё равно получаете, которое обладает заданными
[48:00.520 --> 48:05.400]  вами свойствами. Даже если там есть краски зависимые признаки, вы точно знаете, что их
[48:05.400 --> 48:09.640]  суммарный вес будет близок к оптимальному, а каждый вес подельности будет вообще минимальный из
[48:09.640 --> 48:17.240]  возможных. Потому что, начиная в квадрате, он сразу будет выше. Вот и всё. Так, понятно? Ещё раз,
[48:17.240 --> 48:29.000]  что такое оптимальная середина смещённых? Понятно? Обладает наименьшей дисперсией. То есть,
[48:29.000 --> 48:33.680]  любую другую оценку вашей матрицы весов вы можете сделать, но дисперсия вот этой случайной
[48:33.680 --> 48:38.000]  величины это случайный вектор. Потому что он зависит от случайных величин. У неё дисперсия будет
[48:38.000 --> 48:51.080]  выше. Настолько, насколько подходит линейный модель для вашей задачи. То есть, если вы пытаетесь
[48:51.080 --> 48:56.680]  условно параболу линейной моделью описать, то у вас в принципе не очень подходящий класс моделей
[48:56.680 --> 49:02.560]  используется для решения задач. Ещё раз, это теоретический результат. То есть, это обосновывает нам,
[49:02.560 --> 49:08.200]  что в хорошем признаковом описании у нас решение задач на меньше квадратов обладает хорошими
[49:08.200 --> 49:14.640]  свойствами, что это оптимальная оценка средней смещённой. Нет, смотрите, вот речь только про
[49:14.640 --> 49:19.120]  линейные модели. Всё, вот и начальная постановка. Если у нас зависимость не линейной модели,
[49:19.120 --> 49:23.040]  не линейной, никаких опять же договорённостей у нас нет, у нас условия теремии выполняются.
[49:23.040 --> 49:36.360]  Вот, хорошо, да. И да, и нет. Смотрите, ещё раз. А теперь, собственно, когда брать какую норму.
[49:36.360 --> 49:41.280]  Давайте сейчас покажу, что бывает, например, первая норма, что вы понимаете и сами, но какие у неё
[49:41.280 --> 49:46.520]  есть свойства. Но, во-вторых, норм можно брать раз. Собственно, здесь мы подходим с вами опять к
[49:46.520 --> 49:55.640]  очень важным факту. Так, вам видно там справа? Я не сгораживаю. Смотрите, если модель смещённая,
[49:55.640 --> 50:01.160]  значит она решает не ту задачу, которая у вас, вот, тогда она решает, например, не минимизацию,
[50:01.160 --> 50:08.040]  как бы, той функции ошибки. Короче, если у вас модель смещённая, значит она не минимизирует
[50:08.040 --> 50:14.680]  чистой функции ошибки. Смотрите, что такое, давайте я ещё пару тогда этих терминов веду. Вот есть
[50:14.680 --> 50:19.200]  функция ошибки. Её, на самом деле, очень часто путают в интернете друг с другом. Давайте называть так.
[50:19.200 --> 50:24.240]  Функция ошибки — это именно вот ошибка предсказательной нашей модели. То есть насколько мы ошибаемся,
[50:24.240 --> 50:29.760]  предсказывая целевую величину. Вот будет эта функция ошибки. Хорошо? Вот всё вместе можно
[50:29.760 --> 50:34.680]  назвать там, например, функционал имперического риска. Потому что это ошибка плюс какая-то
[50:34.680 --> 50:39.840]  регулярность. Это ровно тот функционал, который мы с вами минимизируем. Несмещённая у нас оценка
[50:39.840 --> 50:43.960]  тогда, когда минимизируем чистую ошибку. Если минимизируем что-то там ещё, то к правилам
[50:43.960 --> 50:47.960]  получаем смещённую. Потому что на этом можно на самом деле посмотреть абсолютно банально со
[50:47.960 --> 50:53.240]  второй стороны. Смотрите. Вот у вас 1 функционал, 2 функционал. Те же помнят, что градиент можно
[50:53.240 --> 50:57.160]  посчитать. И мы на самом деле сейчас про это говорим, можем градиентными методами найти
[50:57.160 --> 51:02.000]  оптимальное значение матрицы Омега. Ну или вектор Омега в данном случае. У вас градиент,
[51:02.000 --> 51:08.960]  производная сумма чему равна? В сумме производных. Поэтому у вас для Омеги будет отсюда идти градиент,
[51:08.960 --> 51:13.600]  который тянет её в сторону оптимальной оценки краски. И отсюда будет идти градиент, который тянет
[51:13.600 --> 51:18.040]  её в ноль. Вот откуда у вас появляется смещение. У вас оценка Омеги по сути съезжает в сторону
[51:18.040 --> 51:29.560]  вот этого градиента. Это совсем на пальцах, если. Ну так это и есть то же самое. У вас то, что Омега
[51:29.560 --> 51:35.320]  с крышкой теперь тянется в сторону нуля, это смещает её относительно мат ожидания. Потому что иначе
[51:35.320 --> 51:44.560]  бы она кратки была в мат ожиданиях, которая вот эту штуку минимизирует. Окей. Так, на всякий
[51:44.560 --> 51:52.120]  случай это тогда тоже можно проделать дополнительно. Так, коллеги, давайте так. Вот, что такое оценка
[51:52.120 --> 51:57.720]  максимального правдоподобия? Кто помнит, вообще знает. Понял, тогда про это можно прямо отдельно
[51:57.720 --> 52:05.520]  до семинара провести, собственно. О чем речь? Мы с вами на самом деле... Ну вот, как раз когда
[52:05.520 --> 52:09.920]  обсудите, мы здесь на семинаре тоже можем обсудить. По сути, когда мы с вами минимизируем среднюю
[52:09.920 --> 52:14.880]  квадратичную ошибку, мы по сути ищем оценку максимального правдоподобия для средней...
[52:14.880 --> 52:21.240]  Когда минимизируем среднюю квадратичную ошибку, мы ищем оценку максимального правдоподобия в
[52:21.240 --> 52:30.120]  предположении, что у нас краски и ошибки нормально распределены. Вот все. Эта краска будет средняя. Нет,
[52:30.120 --> 52:35.680]  это на статистике прямо отдельно выводится где-то полсеминара. Я поэтому и говорю, кто знает, мы это
[52:35.680 --> 52:40.080]  можем на каком-нибудь допсеминаре провести, потому что сейчас у меня записи нет, я это буду из ума
[52:40.080 --> 52:47.840]  выводить минут 20, наверное. Извините. Да? А, это да, сейчас я его потрогаю. Вот, хорошо. Собственно,
[52:47.840 --> 52:52.760]  нормы у нас бывают разные. И возвращаясь к вопросу, а правильно ли, что всегда брать вторую норму и не
[52:52.760 --> 52:58.520]  выпендриваться? Вообще говоря, нет. Потому что выбор того функционала, который вы будете
[52:58.520 --> 53:02.920]  минимизировать, опять же, зависит от вас. Это, на самом деле, одна из самых важных частей в любой задачи
[53:02.920 --> 53:08.000]  машинного обучения. Не уметь там строить классные нейронки, деревья и так далее, ансамбли. Умение
[53:08.000 --> 53:12.840]  правильно из неформальной задачи, ну, например, вам там научу говорит, что надо сделать то-то,
[53:12.840 --> 53:17.480]  или на работе вам говорит, не знаю, менеджер, что надо сделать что-то другое. Короче, как это обычно
[53:17.560 --> 53:42.520]  говорят из бизнес задачи, ну или из научной задачи, формулировать уже математическую
[53:42.520 --> 53:46.840]  это скажу. Минимизация квадратичной ошибки дает нам среднее, минимизация
[53:46.840 --> 53:51.240]  абсолютной ошибки, то есть, например, первые нормы, будет давать нам медиану. Как
[53:51.240 --> 53:54.880] -то доказать краски можем с вами сделать сегодня, например, после там
[53:54.880 --> 53:58.480]  семинара в конце. И, собственно, нормы бывают разные. На практике, как правило, в
[53:58.480 --> 54:02.800]  задаче регрессии применяют в основном две. Ну ладно, три. Первая это квадратичная
[54:02.800 --> 54:06.720]  ошибка, это, наверное, 70 процентов всех случаев. Вторая это средняя абсолютная
[54:06.720 --> 54:12.320]  ошибка, или же их называют МСК и МАЕ на том, что mean squared error, mean absolute error.
[54:12.320 --> 54:17.320]  МАЕ это сумма модуля отклонений, и, соответственно, для нее мы точно так же
[54:17.320 --> 54:21.360]  можем все посчитать, но для нее нет аналитического решения. Градиентная, хотя
[54:21.360 --> 54:25.360]  все еще присутствует. И, собственно, третья функция ошибки, которую часто еще
[54:25.360 --> 54:29.560]  используют, это так называемая квантильная функция ошибки. Короче, там у вас
[54:29.560 --> 54:33.960]  зависимости от того, какой у вас квантиль важен, например, вам сильно важнее там
[54:33.960 --> 54:37.880]  не завышать, чем не занижать. У вас может функция ошибки иметь какой-нибудь вид, типа вот
[54:37.880 --> 54:42.600]  такая галочка, здесь она плавно, растет здесь быстро. Но все понимают, что у нас
[54:42.600 --> 54:47.960]  для модуля график, как вы, просто галка, правильно, отклонения. Причем с углом 90
[54:47.960 --> 54:51.960]  градусов. У вас угол, на самом деле, может быть другой. Такую функцию ошибки тоже
[54:51.960 --> 54:55.240]  можно подобрать, она иногда тоже подходит. Причем этот угол даже является
[54:55.240 --> 55:01.120]  гиперпараметром, ну там можно подбирать и так далее. Вот. Все. Вот у нас есть раз-два
[55:01.120 --> 55:05.800]  именно для функций ошибок. Но также у нас есть и регуляризация, которую точно
[55:05.800 --> 55:09.160]  также можно использовать со второй нормой, с первой, с четвертой, если вам вдруг
[55:09.160 --> 55:13.840]  захотелось, на практике обычно используют, и так далее. И на всякий случай, только
[55:13.840 --> 55:19.000]  вот этот случай MSE без регуляризации работает с теориями Гаусса-Маркова. То, что
[55:19.000 --> 55:22.280]  он в условии теориями Гаусса-Маркова стоит. Мы решаем задачу на меньше квадрат.
[55:22.280 --> 55:27.920]  Все. Линейная модель. Все остальное, как бы, это что-то там другое. Так что, если вы
[55:27.920 --> 55:32.160]  добавили регуляризацию, вы добавили свои ограничения на задачу, но вы ушли от
[55:32.160 --> 55:35.960]  изначальной задачи найти оптимальную оценку, чтобы минимизировать ошибку. Вы что-то
[55:35.960 --> 55:39.800]  другое уже решаете. Поэтому оценка у вас априори смещенная, вы другую задачу
[55:39.800 --> 55:43.040]  решаете. За это вы получаете какие-то хорошие свойства. Например, она у вас более
[55:43.040 --> 55:47.960]  устойчивая. Устойчивые краски под устойчивостью будем понимать следующее.
[55:47.960 --> 55:52.160]  Это такое, так сказать, определение. Оценку будем называть устойчивой, если
[55:52.160 --> 55:56.720]  малое изменение обучающей выборки приводит к малому изменению вектора
[55:56.720 --> 56:00.200]  параметров. То есть, если условно мы на Эпсилон, грубо говоря, изменили выборку,
[56:00.200 --> 56:04.520]  но там шум чуть-чуть поменяли, у нас матрица весов тоже поменялась не больше, чем на Эпсилон,
[56:04.520 --> 56:11.200]  например. Потому что, если у нас, например, наша ситуация выраженная, наша матрица выраженная,
[56:11.200 --> 56:14.840]  регуляризации нет, вы можете чуть-чуть шум поменять. Мы это с вами на семинале опять же
[56:14.840 --> 56:21.560]  сделаем. Здесь может оказаться 50-50, например. Видите, шум там будет меняться на 10-1, 10-2,
[56:22.440 --> 56:28.800]  а здесь будет меняться на 10-2. Вот, значит, оценка неустойчивая. Поняли, согласились,
[56:28.800 --> 56:37.680]  ловили? Вопросы, комментарии? Ну, собственно, как это происходит? Во-первых, если у вас есть
[56:37.680 --> 56:42.840]  возможность на некоторых, на различных выборках ее посчитать, ну, например, у вас там есть две
[56:42.840 --> 56:47.000]  выборки, вы можете посчитать оценку на первой выборке и на второй. Если у вас какие-то признаки
[56:47.000 --> 56:51.200]  получать сильно разные веса, скорее всего, у вас оценка неустойчивая или у вас очень разные
[56:51.200 --> 56:55.040]  выборки. То есть надо или надо проверять распределение у этих выборок по признакам,
[56:55.040 --> 57:00.880]  или, значит, у вас оценка неустойчивая. Второе, если вы, сейчас мы дойдем до градиентов краски,
[57:00.880 --> 57:04.840]  до градиентов меттов, если у вас для каких-то признаков градиенты, для весов признаков
[57:04.840 --> 57:11.360]  градиенты здоровые, и они постоянно туда-сюда меняются, это тоже проблема. Вот, так, на всякий
[57:11.360 --> 57:16.720]  случай, там звук работает, он не сел еще. Супер. Если он вдруг перестанет, вы, пожалуйста, это
[57:16.720 --> 57:25.560]  голосуйте. Хорошо? Ну и, собственно, вот наши сравнения, наши параболы замечательные и наши
[57:25.560 --> 57:31.600]  галки. На всякий случай, внимание, вопрос. У нас же модуль, функция вроде как в нуле недифференцируемая,
[57:31.600 --> 57:40.920]  а что делать? Как мы вообще можем его оптимизировать? Да, то, что у вас вроде как эта функция не совсем
[57:40.920 --> 57:45.560]  дифференцируемая, на практике нас абсолютно не волнует. Почему? У нас производная слева от нуля
[57:45.560 --> 57:49.960]  минус один, производная справа от нуля один, производную в нуле доопределяем нулем и радуемся.
[57:49.960 --> 57:55.960]  Если ошибка ноль, никакого градиента нам не надо. Все, все счастливо, все работает. Соответственно,
[57:55.960 --> 58:02.280]  MSE дает нам blue best linear unbiased estimator, оптимально средний смещенный к оценку, работает с теориями
[58:02.280 --> 58:06.840]  Гаустамаркова соответственно, дифференцируемо, и она чувствительна к шуму. Потому что, если у
[58:06.840 --> 58:11.680]  вас, например, шум достаточно большой, то есть у вас Y, это краски, истинный сигнал, плюс какой-то
[58:11.680 --> 58:16.560]  шум. Чем больше у вас отклонения, парабола вам еще и в квадрат возведет это самое отклонение.
[58:16.560 --> 58:21.280]  Ошибка будет большая. А чем больше ошибка, уже с точки зрения, например, градиентных методов,
[58:21.280 --> 58:28.880]  давайте сюда внимательно посмотрим. Градиент у вас здесь слева будет что? Минус два х, а справа 2х.
[58:28.880 --> 58:36.480]  Согласны? Но если минус два модули х, а справа 2х. Вот. Короче, у вас х там присутствует. То есть,
[58:36.480 --> 58:40.240]  чем больше у вас отклонения, тем больше у вас будет градиент, тем больше у вас каждый объект
[58:40.400 --> 58:45.840]  будет влиять на решение. Она чувствительна к шуму, поэтому. Моя, собственно, у нее что?
[58:45.840 --> 58:51.520]  Отклонения слева дают нам градиент минус один, справа плюс один. Абсолютно все равно сильная там
[58:51.520 --> 58:56.400]  ошибка, слабая. У нас просто не попали точно в цель. Поэтому она гораздо менее чувствительна к
[58:56.400 --> 59:09.160]  шумам. Улавливаете почему? Правильно? Все. Плавно. Тут всем тоже понятно. Что? Еще раз,
[59:09.160 --> 59:13.360]  она не дифференцируема с точки зрения вот мотона. Нам не нужно, чтобы она была дифференцирована
[59:13.360 --> 59:18.640]  всюду. Она дифференцируема на r+, то, что там всегда производная плюс один, на r-, а в нуле
[59:18.640 --> 59:22.720]  просто определяем производную нулем и все. То, что у нее производная как бы не является гладкой
[59:22.720 --> 59:27.180]  функцией, а нам не нужна гладкость. Нам надо, чтобы мы в каждой точке могли посчитать производную. Мы
[59:27.180 --> 59:33.200]  можем. И, соответственно, с регуляризацией. С регуляризацией тоже любопытная вещь. Во-первых,
[59:33.200 --> 59:38.720]  L2-регуляризация — это тот случай, когда у нас есть аналитическое решение, все замечательно. Она
[59:38.720 --> 59:42.100]  дает нам более устойчивое решение, потому что у нас теперь есть единственный вектор, который
[59:42.100 --> 59:49.120]  минимизирует норму вектор весов. Устойчивая, значит, при малом изменении нашей выборки у нас
[59:49.120 --> 59:54.640]  будет мало изменений нашего вектора весов. А вот L1-регуляризация. Она не дифференцируема,
[59:54.640 --> 59:58.720]  но нам, опять же, все равно абсолютно, потому что доопределяем в нуле нулем. То, что она не
[59:58.720 --> 01:00:10.800]  является гладкой, нам это не требуется. Типа того, когда мы в функции ошибки добавляем норму вектора
[01:00:10.800 --> 01:00:17.680]  весов. Можем взять вторую норму, можем взять первую норму. Все, собственно. Как вы понимаете,
[01:00:17.680 --> 01:00:21.840]  третью норму брать немножко странновато, потому что она может быть отрицательной,
[01:00:21.840 --> 01:00:29.920]  нам это вообще не надо. Окей? Ой, простите, третью норму, господи, третью. Ладно, вы меня поняли.
[01:00:29.920 --> 01:00:41.480]  Да. В смысле, с функции ошибки третьей степени не надо брать, я заговорился. Да. Значение весов
[01:00:41.480 --> 01:01:11.000]  или значение параметров. Да. Смотрите, я сейчас на ваш вопрос отвечу, как бы предполагаю. Так,
[01:01:11.320 --> 01:01:17.480]  собственно, L2 регуляризация, как вы понимаете, работает и с МСЕ, и с МОЕ, верно? И с любой другой
[01:01:17.480 --> 01:01:21.320]  функции ошибки, на самом деле. Например, на следующей неделе мы поговорим про задачи классикации,
[01:01:21.320 --> 01:01:25.880]  там у нас будет лог-лоз, логистическая функция потери. Туда точно также можно запихать вторую
[01:01:25.880 --> 01:01:30.600]  норму вектора весов, ничего не поменять, все хорошо. Возникает вопрос, зачем нам L1 регуляризация?
[01:01:30.600 --> 01:01:37.640]  Во-первых, она дает нам чуть другие свойства, потому что, например, она отбирает признаки. Что
[01:01:37.640 --> 01:01:44.440]  это значит, я сейчас скажу. Во-вторых, у нас не всегда есть требования получить наименьше значение
[01:01:44.440 --> 01:01:49.120]  вектора весов, это зависит опять же от ваших предположений. А касательно того, что у вас нет
[01:01:49.120 --> 01:01:54.480]  аналитической формулы, а нам все равно, даже если считать градиентное решение, у вас будет
[01:01:54.480 --> 01:01:58.960]  решение, грубо говоря, не единственное, все равно вы сойдетесь согласно шуму к какому-то решению,
[01:01:58.960 --> 01:02:03.720]  если вы используете выраженную матрицу, просто градиентным методом, но она у вас, собственно,
[01:02:03.720 --> 01:02:07.880]  получится тоже каким-то непонятным, каким-то вот таким может получиться и любым другим,
[01:02:07.880 --> 01:02:12.280]  потому что вы под шум переобучитесь. То, что шум, у вас все равно данных присутствует. А когда вы
[01:02:12.280 --> 01:02:16.920]  используете L2 регуляризацию, вы все равно получите решение, которое гораздо ближе вот сюда. Но не вот
[01:02:16.920 --> 01:02:27.480]  сюда, а тут будет примерно минус 0.7, 0.8 и минус 0.8, они будут гораздо ближе друг к другу. Да, это SL2,
[01:02:27.480 --> 01:02:32.640]  SL1 примерно тоже самое. Заметьте, это тоже функция, по сути, выпуклая, поэтому она все равно дает вам
[01:02:32.640 --> 01:02:37.680]  какое-то единственное решение. Но оно работает чуть по-другому. И здесь обычно начинается какая-то
[01:02:37.680 --> 01:02:46.640]  очень странная разговора. К сожалению, Анимашка почему-то померла. Может, на нее можно посмотреть.
[01:02:46.640 --> 01:02:53.120]  Нет, нельзя, ладно, потом покажу. Ну тут просто это все должно ездить, я вам лучше сейчас на пальцах
[01:02:53.120 --> 01:02:59.360]  объясню. Смотрите, L1 отбирает признаки. Вот это очень такая нестандартная вещь, от нее, к правилу,
[01:02:59.360 --> 01:03:04.000]  все начинает немножко зависать. Что значит L1 отбирает признаки? Я вам давайте сейчас даже
[01:03:04.000 --> 01:03:12.200]  попробую это нарисовать. Благо у нас есть доска. Пока она загружается, собственно, что сделаем? Что
[01:03:12.200 --> 01:03:17.800]  значит L1 отбирает признаки? Давайте пока что подумаем. Вот мы с вами, когда решаем оптимизационную
[01:03:17.800 --> 01:03:23.400]  задачу, нам нужно найти решение, которое минимизирует сумму квадратов отклонений, например, в СМСЕ. Но
[01:03:23.400 --> 01:03:28.640]  при этом мы хотим еще и минимизировать первую норму вектора весов, правильно? Давайте на это посмотрим
[01:03:28.640 --> 01:03:43.880]  с точки зрения градиентных методов. Что? Рисовать на черном фоне классно, мне нравится. По-моему,
[01:03:43.880 --> 01:03:51.560]  так даже лучше видно. Итак, смотрите, вот у нас с вами есть, собственно, первая норма, вектор
[01:03:51.560 --> 01:04:02.680]  омега. То есть это у нас получается сумма омега итой по модулю. Согласны? Давайте мы теперь попробуем
[01:04:02.680 --> 01:04:10.440]  эту штуку минимизировать. Причем градиентный метод. Когда у нас получается d омега первая норма,
[01:04:10.440 --> 01:04:25.240]  по d омега и будет чему равна? Ну по факту это будет плюс один, если омега и больше нуля. Ноль,
[01:04:25.240 --> 01:04:46.760]  если омега равно нулю и минус один, если омега и меньше нуля. Или, что на самом деле эквивалентно,
[01:04:46.760 --> 01:04:59.880]  так просто проще записать, это просто сигнум омега и. Хорошо? О, спасибо. Современные технологии.
[01:04:59.880 --> 01:05:06.960]  Ладно, с этим все согласны. А теперь давайте посмотрим, предположим, что у нас какой-то
[01:05:06.960 --> 01:05:12.080]  признак, например, вообще не влияет на решение. То есть он не нужен, он шумовой. Он неважно там
[01:05:12.080 --> 01:05:17.560]  положительный, отрицательный вес имеет, он на решение не влияет. Тогда что мы получим? У нас,
[01:05:17.560 --> 01:05:22.760]  когда мы с вами пытаемся минимизировать нашу функцию имперического риска, q от омега,
[01:05:22.760 --> 01:05:32.360]  которая в общем случае равна l, это наша функция потерь. Что с тобой не так? Залипло,
[01:05:32.360 --> 01:05:37.440]  наверное. Собственно, у нас есть l, это именно функция ошибки, ошибка предсказания нашей
[01:05:37.440 --> 01:05:45.120]  целевой переменной. Плюс первая норма вектора весов. Согласны? Когда мы с вами считаем производную,
[01:05:45.120 --> 01:05:56.320]  у нас получается dq по d омега. Это что? Это у нас dl по d омега плюс d, кратски, омега 1 по d омега.
[01:05:56.320 --> 01:06:08.720]  С этим все согласны. Давайте добавлю λ. Я сейчас показываю именно тот факт, почему при лямбда равна
[01:06:08.720 --> 01:06:15.200]  1 или лямбда равна 0,5, ничего не поменяется. Если у нас какой-то признак не влияет на функции
[01:06:15.200 --> 01:06:22.880]  ошибки, то что значит будет с этой производной? Она будет равна 0, потому что она от нее не зависит.
[01:06:22.880 --> 01:06:26.600]  Ну или в среднем равна 0, потому что у нас есть шум, одни объекты будут тянуть наверх,
[01:06:26.600 --> 01:06:37.240]  другие вниз, чтобы не засыпали и так далее. Да что с ними не так сегодня? Ой, ладно. Короче,
[01:06:37.240 --> 01:06:52.600]  довки устали. Короче, эта штука примерно равна 0. Нет, все, да ладно, мы почти закончили. Короче,
[01:06:52.600 --> 01:06:59.240]  все согласны, я надеюсь, что вот эта штуковина, она примерно равна 0, если признак не зависит.
[01:06:59.240 --> 01:07:04.440]  Что у нас когда остается от градиента? У нас остается вот этот вклад, а когда вот этот вклад
[01:07:04.560 --> 01:07:09.560]  будет нулевым? То есть когда у нас градиент не будет тянуть данный вес, признака куда-нибудь.
[01:07:09.560 --> 01:07:16.120]  Когда он равен нулю? Потому что всегда когда у нас признак больше нуля, у нас градиент плюс 1,
[01:07:16.120 --> 01:07:21.480]  соответственно антиградиент будет минус 1, всегда когда признак меньше нуля антиградиент будет
[01:07:21.480 --> 01:07:28.540]  плюс 1, то есть у нас градиент будет всегда толкать значение нашего веса в ноль. Да вы меня достали,
[01:07:28.540 --> 01:07:31.420]  «Господи, не вы в смысле о доске?»
[01:07:31.420 --> 01:07:33.760]  Соответственно, всегда, когда значение нашего
[01:07:33.760 --> 01:07:37.000]  признака не равно нулю, веса нашего признака не
[01:07:37.000 --> 01:07:40.600]  равно нулю, если он неважен, то, соответственно, он будет
[01:07:40.600 --> 01:07:43.160]  просто загнан в ноль, потому что вот эта штука компенсировать
[01:07:43.160 --> 01:07:46.360]  это не будет, а эта штука будет его толкать в ноль.
[01:07:46.360 --> 01:07:49.080]  Собственно, ровно поэтому и говорят, что L1 регуляризация
[01:07:49.080 --> 01:07:52.000]  отбирает признаки в смысле того, что если у вас какой-то
[01:07:52.000 --> 01:07:56.240]  признак неважен, он получит на любой вес в конце кону.
[01:07:56.240 --> 01:07:59.820]  То просто, собственно, первая норма загонит в ноль.
[01:07:59.820 --> 01:08:03.220]  Более того, на самом деле, тут даже что можно сказать?
[01:08:03.220 --> 01:08:06.300]  Если у вас вот этот коэффициент регуляризации достаточно
[01:08:06.300 --> 01:08:08.360]  большой, то есть признак все равно имеет там какой-то
[01:08:08.360 --> 01:08:10.820]  мизерный градиент, то есть очень маленький градиент,
[01:08:10.820 --> 01:08:13.740]  он, возможно, как-то очень мало значим для решения
[01:08:13.740 --> 01:08:17.820]  задачи, а здесь коэффициент лямбда большой, то получается,
[01:08:17.820 --> 01:08:21.220]  что вот этот градиент, он же всегда как бы пропорционален
[01:08:21.220 --> 01:08:25.180]  единице, потому что, ну, лямбде, лямбда на единице,
[01:08:25.180 --> 01:08:29.740]  потому что здесь или плюс, или минус один, поэтому
[01:08:29.740 --> 01:08:32.480]  если у вас какой-то признак мало влияет на решение,
[01:08:32.480 --> 01:08:35.280]  допустим, там есть какая-то связь между признаком и
[01:08:35.280 --> 01:08:37.720]  целевой переменной, но она крайне там маленькая,
[01:08:37.720 --> 01:08:39.680]  допустим, там влияет на нее в четвертом знаке после
[01:08:39.680 --> 01:08:40.680]  запятой.
[01:08:40.680 --> 01:08:45.200]  У вас L1 регуляризация с лямбдой больше, чем 10-4, все равно
[01:08:45.200 --> 01:08:47.160]  загонит его в ноль, потому что отсюда у вас градиент
[01:08:47.160 --> 01:08:54.000]  будет порядка 10-4, отсюда порядка единиц, уловили?
[01:08:54.000 --> 01:08:57.000]  Я уловили.
[01:08:57.000 --> 01:08:58.000]  Картиночка.
[01:08:58.000 --> 01:09:01.460]  Ну, визуализировать, так я, собственно, и пытался
[01:09:01.460 --> 01:09:02.460]  вам это визуализировать.
[01:09:02.460 --> 01:09:06.820]  Смотрите, вот вам картиночка, у вас получается, что производной
[01:09:06.820 --> 01:09:10.580]  здесь всегда плюс один, минус один, здесь плюс один,
[01:09:10.580 --> 01:09:13.020]  если у вас производная от функции потери меньше
[01:09:13.020 --> 01:09:15.620]  чем вот эта штука, она ее доминирует, значит, она
[01:09:15.620 --> 01:09:16.620]  загоняет в ноль.
[01:09:16.620 --> 01:09:18.820]  То, что если она вышла из нуля, она сразу получает
[01:09:18.820 --> 01:09:22.340]  пинка в сторону нуля с силой 1, а функция потери с силой
[01:09:22.340 --> 01:09:23.340]  меньше единицы.
[01:09:23.560 --> 01:09:25.600]  Она зайдет именно тогда, где минимум.
[01:09:25.600 --> 01:09:27.600]  Все, он нам больше не нужен.
[01:09:27.600 --> 01:09:28.600]  Да.
[01:09:28.600 --> 01:09:37.560]  А теперь посмотрим на L2, смотрите, L2 у нас какую производную
[01:09:37.560 --> 01:09:38.560]  будет давать?
[01:09:38.560 --> 01:09:39.560]  Два Омега.
[01:09:39.560 --> 01:09:43.800]  Поэтому если Омега около нуля, то у нее градиент становится
[01:09:43.800 --> 01:09:47.120]  уже меньше, чем вот этот самый условно Эпсилон, который
[01:09:47.120 --> 01:09:49.160]  нам дает функция потерь, поэтому она просто загоняет
[01:09:49.160 --> 01:09:51.520]  их близко к нулю, но не ровно в ноль.
[01:09:51.620 --> 01:09:54.300]  L1, собственно, за счет вот этой ступеньки, у нас производная
[01:09:54.300 --> 01:09:57.780]  либо минус 1, 0, 1, у нас ступенька, резкий переход, за счет
[01:09:57.780 --> 01:10:02.300]  этого он ровно в ноль загоняет, а ОL2 плавно убывает производная,
[01:10:02.300 --> 01:10:04.740]  чем ближе к нулю, тем меньше производная, поэтому он
[01:10:04.740 --> 01:10:07.260]  их просто догоняет примерно до нуля, но не выкидывает
[01:10:07.260 --> 01:10:08.260]  полностью.
[01:10:08.260 --> 01:10:11.020]  Именно в этом смысле говорят, что L1 регуляризация отбирает
[01:10:11.020 --> 01:10:12.020]  признаки.
[01:10:12.020 --> 01:10:15.060]  На всякий случай, когда вы используете L1 регуляризации,
[01:10:15.060 --> 01:10:16.940]  вы просто в векторе весов можете получить некоторые
[01:10:16.940 --> 01:10:19.880]  нулевые элементы, но вектор весов будет точно такой
[01:10:19.880 --> 01:10:21.020]  же размеренский.
[01:10:21.020 --> 01:10:26.240]  Это частая ошибка на самом начале работы с машинным
[01:10:26.240 --> 01:10:29.920]  обучением, когда ожидают, что вы применили L1 регуляризацию
[01:10:29.920 --> 01:10:31.840]  и у вас почему-то вектор весов стал меньше.
[01:10:31.840 --> 01:10:33.760]  Нет, конечно, у вас просто такие члены будут равны
[01:10:33.760 --> 01:10:34.760]  нулю.
[01:10:34.760 --> 01:10:38.560]  И именно их мы считаем выкинутыми признаками, ну и после этого
[01:10:38.560 --> 01:10:42.600]  как правило есть еще подход, что после этого вы перезапускаете
[01:10:42.600 --> 01:10:45.240]  вашу модель, оптимизацию, выкинув эти признаки вообще
[01:10:45.240 --> 01:10:48.280]  из обучающей выборки, можете даже после этого уже L2 регуляризацию
[01:10:48.280 --> 01:10:50.580]  взять и уже оставшиеся признаки просто подогнать
[01:10:50.580 --> 01:10:52.580]  поближе к нулю.
[01:10:52.580 --> 01:10:53.580]  Вопросы, пожелания, комментарии.
[01:10:53.580 --> 01:10:59.780]  Все поняли, что L1 делает с признаками, с весами?
[01:10:59.780 --> 01:11:00.780]  Тут тоже.
[01:11:00.780 --> 01:11:04.900]  Зануляет, если они не важны для решения задачи, то есть
[01:11:04.900 --> 01:11:07.420]  если они достаточно не важны относительно коэффициента
[01:11:07.420 --> 01:11:08.420]  лямбда.
[01:11:08.420 --> 01:11:09.580]  И собственно здесь коэффициент лямбда как раз явно видно,
[01:11:09.580 --> 01:11:10.580]  что делает.
[01:11:10.580 --> 01:11:13.060]  Если лямбда большая, то если наш признак влияет
[01:11:13.060 --> 01:11:15.780]  на ошибку, на градиент ошибки меньше, чем на лямбду,
[01:11:15.780 --> 01:11:17.140]  то он будет выкинут.
[01:11:17.140 --> 01:11:19.560]  Если больше, чем на лямбду, то он не будет выкинут.
[01:11:19.560 --> 01:11:20.560]  Все.
[01:11:20.560 --> 01:11:21.560]  Вот.
[01:11:21.560 --> 01:11:24.280]  И это на самом деле штука, я ее потом подвигаю, это
[01:11:24.280 --> 01:11:25.280]  второе объяснение.
[01:11:25.280 --> 01:11:26.280]  Что это такое?
[01:11:26.280 --> 01:11:27.280]  Это шар.
[01:11:27.280 --> 01:11:29.280]  Но во второй норме.
[01:11:29.280 --> 01:11:32.280]  Это шар в первой норме.
[01:11:32.280 --> 01:11:35.240]  Заметьте, у вас вот это, по сути, что здесь, я потом
[01:11:35.240 --> 01:11:37.360]  она подвигается, я пока на пальцах опять же покажу.
[01:11:37.360 --> 01:11:39.200]  Что вот это за эллипс такой?
[01:11:39.200 --> 01:11:41.640]  Это линии уровня квадратичной функции потерь.
[01:11:41.640 --> 01:11:43.920]  То есть линии уровня это что?
[01:11:43.920 --> 01:11:45.920]  Это эквапотенциальная поверхность, то есть на линии уровня
[01:11:45.920 --> 01:11:48.220]  у вас одинаковые значения функции ошибки.
[01:11:48.220 --> 01:11:50.300]  Оптимальное решение у нас где-то вот в этой точке.
[01:11:50.300 --> 01:11:52.540]  Мы именно в нее хотим попасть.
[01:11:52.540 --> 01:11:55.460]  Но когда мы, например, говорим, что рассмотрим все решения,
[01:11:55.460 --> 01:11:59.140]  у которых норма вектор весов первая равна единице,
[01:11:59.140 --> 01:12:02.260]  у нас в среднем вот эта точка, вот эти точки, так как
[01:12:02.260 --> 01:12:04.460]  они у нас по первой норме на шаре лежат, по сути они
[01:12:04.460 --> 01:12:08.100]  в вершинах нашего четырехугольника, они в среднем будут ближе
[01:12:08.100 --> 01:12:10.540]  к центру, чем те, которые лежат между ними.
[01:12:10.540 --> 01:12:14.820]  Опять же, это можно формально доказать, эта штука по хорошему
[01:12:14.820 --> 01:12:15.820]  должна двигаться.
[01:12:15.820 --> 01:12:16.820]  Давайте я вам ее покажу.
[01:12:16.820 --> 01:12:24.440]  Я надеюсь, ссылка еще работает.
[01:12:24.440 --> 01:12:39.680]  Во!
[01:12:40.680 --> 01:12:53.120]  Вот давайте посмотрим, скажем так, за этим, как его.
[01:12:53.120 --> 01:12:55.280]  Бледите за нахождением желтой точки.
[01:12:55.280 --> 01:12:57.280]  Видите, здесь она ездит совершенно свободно.
[01:12:57.280 --> 01:13:01.040]  Она находится всегда на окружности радиуса единицы
[01:13:01.040 --> 01:13:03.380]  от центра и абсолютно все равно у нас есть центральная
[01:13:03.380 --> 01:13:04.380]  симметрия.
[01:13:04.380 --> 01:13:06.320]  Здесь точка почти всегда находится в вершинах.
[01:13:06.320 --> 01:13:10.160]  А это ровно есть та ситуация, когда у вас один из весов,
[01:13:10.160 --> 01:13:13.640]  собственно, оси ω1, ω2, один из весов равен нулю.
[01:13:13.640 --> 01:13:19.960]  Понимаете, потому что его вклад в качестве ошибки
[01:13:19.960 --> 01:13:23.120]  вот здесь гораздо меньше, чем его вклад в качестве
[01:13:23.120 --> 01:13:24.800]  регуляризации здесь.
[01:13:24.800 --> 01:13:25.800]  Только и всего.
[01:13:25.800 --> 01:13:29.880]  Это такая визуальная картинка.
[01:13:29.880 --> 01:13:32.080]  Мне, честно говоря, вот это объяснение было непонятно
[01:13:32.080 --> 01:13:35.000]  наверное первые года 3, как я на него смотрел, с градиентами
[01:13:35.000 --> 01:13:36.000]  было гораздо проще понять.
[01:13:36.000 --> 01:13:40.120]  Но кому-то возможно так понять.
[01:13:40.120 --> 01:13:54.680]  Окей, тут вопрос есть?
[01:13:54.680 --> 01:13:55.680]  Ну почему?
[01:13:55.680 --> 01:13:57.440]  Воспроизводная тогда чему равна?
[01:13:57.440 --> 01:13:58.440]  2 ω всегда.
[01:13:58.440 --> 01:14:02.600]  Так, ребят, сейчас дайте мне еще 5 минут, пожалуйста,
[01:14:02.600 --> 01:14:04.720]  мы тут чуть позже начали, поэтому мы не до конца
[01:14:04.720 --> 01:14:05.720]  уложились.
[01:14:05.720 --> 01:14:06.720]  Что?
[01:14:06.720 --> 01:14:14.680]  Да, да, но ровно поэтому, чем ближе вы к нулю, тем меньше
[01:14:14.680 --> 01:14:16.240]  у вас вклад регуляризации в решение.
[01:14:16.240 --> 01:14:23.800]  Ну да, слушайте, я вас плохо слышу, тут уже пошел, давайте
[01:14:23.800 --> 01:14:24.800]  в перерыве тогда отвечу.
[01:14:24.800 --> 01:14:25.800]  Хорошо.
[01:14:25.800 --> 01:14:28.620]  Ладно, ребят, собственно, бывают различные еще способы
[01:14:28.620 --> 01:14:32.520]  померить качество в регрессии не только МСЕ и МАЕ, например,
[01:14:32.520 --> 01:14:36.440]  есть коэффициент-детерминация R2, есть МОПЕ, средняя абсолютная
[01:14:36.440 --> 01:14:39.000]  процентная ошибка, есть СМОПЕ симметричная, на них
[01:14:39.000 --> 01:14:41.800]  проще на семинаре, посмотрите, они в принципе, это все
[01:14:41.800 --> 01:14:44.120]  мои МСЕ, только чуть-чуть преобразованные.
[01:14:44.120 --> 01:14:47.880]  Ну и последняя, но тем не менее очень важная вещь,
[01:14:47.880 --> 01:14:51.160]  потому что уже второе занятие, скажем так, чисто по машинке,
[01:14:51.160 --> 01:14:54.000]  у нас была еще лекция по линалу, и чтобы все модели
[01:14:54.000 --> 01:14:56.320]  строить, надо уметь оценивать их качество.
[01:14:56.320 --> 01:14:59.520]  И как сказал очень умный, талантливый человек, вы
[01:14:59.520 --> 01:15:02.360]  ровно настолько можете доверять своей модели, насколько
[01:15:02.360 --> 01:15:04.520]  вы доверяете своему пайплайну валидации.
[01:15:04.520 --> 01:15:06.440]  Вот шутку о валидации я как раз сейчас скажу.
[01:15:06.440 --> 01:15:10.400]  Собственно, давайте скажем, что у нас есть выборка,
[01:15:10.400 --> 01:15:12.600]  мы можем решать задачу регрессии, можем решать
[01:15:12.600 --> 01:15:14.360]  задачу бинарной классификации, абсолютно неважно.
[01:15:14.360 --> 01:15:17.760]  У нас есть некоторая модель, которая предсказывает значение
[01:15:17.760 --> 01:15:20.480]  целевой перемены для каждого объекта.
[01:15:20.480 --> 01:15:24.000]  Наша задача – минимизировать вот эту самую функцию имперического
[01:15:24.000 --> 01:15:26.920]  риска, ну или функцию потери, опять же, на практике ее
[01:15:26.920 --> 01:15:29.200]  просто всегда вызывают функции потерь, главное понимать,
[01:15:29.200 --> 01:15:31.840]  что вы в это вкладываете, включает она в себя регуляризацию
[01:15:31.840 --> 01:15:32.840]  или нет.
[01:15:32.840 --> 01:15:33.840]  Собственно.
[01:15:33.840 --> 01:15:35.840]  И у нас может быть три ситуации.
[01:15:35.840 --> 01:15:38.080]  Собственно, недообучение, модель слишком простая
[01:15:38.080 --> 01:15:39.080]  для решения задач.
[01:15:39.080 --> 01:15:41.680]  Собственно, линейная модель, например, пытается, мы
[01:15:41.680 --> 01:15:44.920]  пытаемся ее использовать в явно нелинейной задаче.
[01:15:44.920 --> 01:15:45.920]  Тогда все плохо.
[01:15:45.920 --> 01:15:48.960]  Может быть, хорошая модель, вот то, что мы всегда хотим,
[01:15:48.960 --> 01:15:51.200]  может быть, переобучение, когда модель, по сути, запомнила
[01:15:51.200 --> 01:15:54.800]  ответ на каждом объекте и явно вот такая поверхность
[01:15:54.800 --> 01:15:57.520]  решений как-то больно сложной выглядит для такой задачи.
[01:15:57.520 --> 01:16:00.280]  То есть, что такое оптимальная сложность модели, на самом
[01:16:00.360 --> 01:16:02.360]  деле, можно долго говорить, и это зависит от того, что
[01:16:02.360 --> 01:16:04.360]  мы сформулируем как оптимальная модель.
[01:16:04.360 --> 01:16:09.960]  Но есть вот эта классная картинка из книжки Гудфеллу
[01:16:09.960 --> 01:16:10.960]  и Бенджоу.
[01:16:10.960 --> 01:16:13.240]  И Арен Курвилла, она у нас вторая в списке рекомендованной
[01:16:13.240 --> 01:16:14.240]  литературы.
[01:16:14.240 --> 01:16:17.760]  Собственно, здесь у нас по оси х ошибка, по оси y у
[01:16:17.760 --> 01:16:21.320]  нас, скажем так, сложность модели, капасти, то есть,
[01:16:21.320 --> 01:16:23.240]  количество степеней свободы модели, грубо говоря.
[01:16:23.240 --> 01:16:25.840]  Можете очень грубо это оценить, как количество параметров
[01:16:25.840 --> 01:16:26.840]  модели.
[01:16:26.840 --> 01:16:27.840]  Хорошо?
[01:16:27.840 --> 01:16:28.840]  Сколько данных она может запомнить?
[01:16:28.960 --> 01:16:30.960]  И, собственно, у нас есть две ошибки.
[01:16:30.960 --> 01:16:31.960]  Тиненькая.
[01:16:31.960 --> 01:16:32.960]  Это ошибка на обучающей выборке.
[01:16:32.960 --> 01:16:35.280]  Она у нас снижается примерно к нулю.
[01:16:35.280 --> 01:16:37.920]  Она может достичь нуля, если у нас нету явно шума
[01:16:37.920 --> 01:16:38.920]  в наших данных.
[01:16:38.920 --> 01:16:40.760]  Она не может его достичь, если у нас, например, два
[01:16:40.760 --> 01:16:43.080]  объекта одинаковых с разными предсказаниями.
[01:16:43.080 --> 01:16:44.880]  Тогда не сможешь ничего сделать.
[01:16:44.880 --> 01:16:49.200]  А вот у нас ошибка обобщающая, ошибка обобщения или generalization
[01:16:49.200 --> 01:16:51.720]  error или ошибка на отложенных данных.
[01:16:51.720 --> 01:16:53.760]  Как видите, она до какого-то момента снижается вместе
[01:16:53.760 --> 01:16:56.000]  с ним, а потом начинает краски расти.
[01:16:56.080 --> 01:16:59.440]  Это называют переобучением, когда наша модель переобучается
[01:16:59.440 --> 01:17:01.880]  под обучающую выборку, то есть запоминает специфичные
[01:17:01.880 --> 01:17:03.280]  вещи только для обучающей выборки.
[01:17:03.280 --> 01:17:05.280]  Но это красивая теория.
[01:17:05.280 --> 01:17:08.280]  Тут, на самом деле, есть еще у этой картинки продолжение.
[01:17:08.280 --> 01:17:11.120]  О нем говорят последние годы, и все еще идут научные
[01:17:11.120 --> 01:17:12.120]  изыскания.
[01:17:12.120 --> 01:17:13.120]  Непонятно, что происходит.
[01:17:13.120 --> 01:17:14.120]  Называется она Double Decay.
[01:17:14.120 --> 01:17:17.080]  В некоторых случаях вот эта штука может начать расти,
[01:17:17.080 --> 01:17:18.080]  а потом опять начать падать.
[01:17:18.080 --> 01:17:20.880]  Но это конец второго семестра.
[01:17:20.880 --> 01:17:22.440]  Возможно, про это поговорим, если будет именно как это
[01:17:22.440 --> 01:17:24.240]  обосновать то, что есть просто несколько работ на
[01:17:24.320 --> 01:17:26.480]  эту тему научную, где говорят, ну вообще так бывает.
[01:17:26.480 --> 01:17:27.480]  Вот.
[01:17:27.480 --> 01:17:31.480]  Собственно, недообучение и переобучение.
[01:17:31.480 --> 01:17:32.480]  Проблема в чем?
[01:17:32.480 --> 01:17:34.920]  Если вы модель сделанной слишком простой, усложнить
[01:17:34.920 --> 01:17:36.480]  ее вы всегда можете.
[01:17:36.480 --> 01:17:38.720]  Если слишком сложной, можете ее всегда упростить.
[01:17:38.720 --> 01:17:41.480]  Но, собственно, как понять, насколько ваша модель
[01:17:41.480 --> 01:17:42.480]  переобучена?
[01:17:42.480 --> 01:17:43.480]  Что она недообучена?
[01:17:43.480 --> 01:17:45.560]  В принципе, можно понять достаточно легко, у вас просто
[01:17:45.560 --> 01:17:47.480]  качество в модели недостаточно хорошее.
[01:17:47.480 --> 01:17:49.920]  У вас либо данные плохие, либо модель недообучается.
[01:17:49.920 --> 01:17:51.240]  Именно на обучающей выборке.
[01:17:51.240 --> 01:17:53.760]  Что делать, если модель переобучается?
[01:17:53.760 --> 01:17:56.600]  Вы для этого сначала должны обнаружить переобучение.
[01:17:56.600 --> 01:17:58.640]  Потому что вы не знаете, как ваша модель себя ведет.
[01:17:58.640 --> 01:18:01.320]  Где-то кроме обучающей выборки покажут.
[01:18:01.320 --> 01:18:04.080]  Поэтому можно использовать, скажем так, метод отложенной
[01:18:04.080 --> 01:18:05.080]  выборки.
[01:18:05.080 --> 01:18:08.040]  И я не рекомендую его использовать сейчас, когда мы будем говорить
[01:18:08.040 --> 01:18:09.480]  про какие-то крупные нейронки.
[01:18:09.480 --> 01:18:11.920]  У нас не будет выбора другого, но тем не менее.
[01:18:11.920 --> 01:18:14.840]  Как правило, вот есть такое понятие обучающей выборки
[01:18:14.840 --> 01:18:17.680]  – train, тестовой выборки – test.
[01:18:17.680 --> 01:18:20.480]  И я сразу скажу, что есть, собственно, валидационная
[01:18:20.480 --> 01:18:21.480]  выборка.
[01:18:21.480 --> 01:18:24.200]  Все, чтобы вас сразу не консьюдить, скажем так,
[01:18:24.200 --> 01:18:25.200]  не запутывать.
[01:18:25.200 --> 01:18:29.320]  Собственно, в хорошей нотации тестовая выборка – это то,
[01:18:29.320 --> 01:18:31.720]  что у вас вообще лежит где-то вне, и вы к нему
[01:18:31.720 --> 01:18:34.600]  либо никогда не получите доступа, либо вы получите
[01:18:34.600 --> 01:18:37.080]  к нему доступа, грубо говоря, пройдя точку невозврата.
[01:18:37.080 --> 01:18:39.040]  Ну, словно вот, тестовая выборка – это как будто
[01:18:39.040 --> 01:18:41.800]  вы сдали работу преподавателю на экзамене, и уже преподаватель
[01:18:41.800 --> 01:18:45.720]  вам говорит ответ, хорошо вы решили экзамен или нет.
[01:18:45.720 --> 01:18:47.480]  Вот это ваш тестовая выборка.
[01:18:47.480 --> 01:18:49.640]  Вы не можете сказать преподавателю, а, я здесь ошибся, дайте
[01:18:49.640 --> 01:18:51.560]  я перепишу и опять ему сдать.
[01:18:51.560 --> 01:18:54.640]  Некоторые пытаются, как правило, это не работает.
[01:18:54.640 --> 01:18:56.160]  Валидационная выборка – это все то, что вы делаете
[01:18:56.160 --> 01:18:58.400]  у себя локально, это ваши локальные тесты на ваш код
[01:18:58.400 --> 01:19:00.680]  где-нибудь там на проге, это ваша проверка своей
[01:19:00.680 --> 01:19:03.200]  домашки или попытка соседа попросить проверить вашу
[01:19:03.200 --> 01:19:04.200]  домашку и так далее.
[01:19:04.200 --> 01:19:07.400]  То есть валидационная выборка – это то, что вы сами используете.
[01:19:07.400 --> 01:19:10.760]  Поэтому у нас есть, собственно, train validation test framework.
[01:19:10.760 --> 01:19:13.760]  Train – это то, на чем вы настраиваете параметры в вашей модели.
[01:19:13.760 --> 01:19:15.480]  Вот именно параметр.
[01:19:15.480 --> 01:19:17.960]  Валидация – это то, на чем вы выбираете гиперпараметры
[01:19:18.000 --> 01:19:20.680]  и то, на чем вы проверяете качество в вашей модели.
[01:19:20.680 --> 01:19:23.000]  Обращаю ваше внимание, если вы подбираете параметры
[01:19:23.000 --> 01:19:26.240]  на валидационной выборке, вы уже посредством взаимодействия
[01:19:26.240 --> 01:19:29.540]  самостоятельно с гиперпараметрами, точнее, переобучаетесь
[01:19:29.540 --> 01:19:32.880]  под валидационную выборку и вы можете под нее переобучиться,
[01:19:32.880 --> 01:19:33.960]  если будете долго это делать.
[01:19:33.960 --> 01:19:36.200]  То, что у вас, по сути, утечка данных идет через
[01:19:36.200 --> 01:19:38.760]  вас в ваши гиперпараметры.
[01:19:38.760 --> 01:19:40.620]  Если достаточно долго будете это делать, вы подберете
[01:19:40.620 --> 01:19:43.800]  оптимальный для вашей валидационной выборки, но не факт для
[01:19:43.800 --> 01:19:46.120]  всего распределения, откуда приходят данные.
[01:19:46.120 --> 01:19:51.360]  собственно что можно делать вариант где вы просто от рейна отрубили тест один раз на нем
[01:19:51.360 --> 01:19:57.300]  проверились идея плохая почему то что как минимум вы можете случайно выбрать плохую под
[01:19:57.300 --> 01:20:02.580]  выборку на которую будет тестироваться но например у вас сюда попали только объекты класса 1 тогда
[01:20:02.580 --> 01:20:06.700]  если у вас абсолютно тупой классикатор который умеет предсказывать только объекты класса 1 у вас
[01:20:06.700 --> 01:20:12.820]  100 процентная accuracy у вас все правильно по факту ваш классикатор мусор ну или у вас здесь просто
[01:20:13.820 --> 01:20:18.260]  или здесь у вас только студента например у вас задача там не знаю предсказание среднего дохода и
[01:20:18.260 --> 01:20:22.780]  у вас несколько социальных групп короче как дробить данные там будет различность процедуры
[01:20:22.780 --> 01:20:28.420]  стратесикации нормализация так далее про это мы тоже поговорим собственно как делать пока у вас
[01:20:28.420 --> 01:20:33.620]  маленькие модели это вот примерно до восьмого занятия нашего курса пока мы делаем так мысли так
[01:20:33.620 --> 01:20:38.540]  делают всегда когда маленькие модели когда не слишком много данных когда мы себе подводим можно
[01:20:38.540 --> 01:20:43.020]  собственно или разделить выборку над train validation и тест вы можете либо выбрать
[01:20:43.020 --> 01:20:47.540]  валидацию прям вот очень хорошо проверить все статистики классно пока выборка маленькая
[01:20:47.540 --> 01:20:52.300]  можно сделать сильно проще можно использовать cross валидацию а именно вы сразу иметь какую-то
[01:20:52.300 --> 01:20:56.620]  отложенную выборку вот тест это то что у вас или вам вообще недоступно или вы изначально ее
[01:20:56.620 --> 01:21:02.020]  отложили как правило тест у вас короче вы как правило тест себе не выделяете то есть тест это
[01:21:02.020 --> 01:21:06.500]  либо то что вы будете сдавать там контест либо то что вы издаете на соревнования либо то что вы
[01:21:06.500 --> 01:21:11.060]  завтра тестируетесь на работе на проде и так далее если все упадет плохо что я делал с
[01:21:11.060 --> 01:21:16.180]  валидацией у вас есть все ваши данные обучающие вы их можете побить на различные чанки кусочки и
[01:21:16.180 --> 01:21:23.100]  повторить много раз процедуру выбираем все фолды кроме одного фолт краски в данном случае ну все
[01:21:23.100 --> 01:21:27.900]  куски на котором побили на них обучаемся на последнем валидируемся на всякий случай тут
[01:21:27.900 --> 01:21:32.620]  написано тест волк я когда-нибудь все-таки перепишу валидационный на последнем проверяемся и
[01:21:32.620 --> 01:21:37.220]  повторяем так чтобы качестве валидационного использовался каждый из фолдов который был то
[01:21:37.220 --> 01:21:41.980]  есть по сути мы если у нас есть фолдов 10 раз обучаем нашу модель на различных подвыборках
[01:21:41.980 --> 01:21:47.260]  10 раз тестируем на опять же различных подвыборках валидационных нас получается на самом деле 10
[01:21:47.260 --> 01:21:53.020]  моделей и 10 раз мы посчитали ошибку потом можем усреднить и вот это усредненная ошибка есть наиболее
[01:21:53.020 --> 01:21:59.260]  наверное качественная оценка качества нашей модели да это конечно все умеет собственно мы
[01:21:59.300 --> 01:22:03.900]  с вами этот крас мы вас будем явно просить делать именно так потому что один раз побить пополам
[01:22:03.900 --> 01:22:08.980]  плохая идея плюс это вам позволяет экономить на самом деле данные потому что теперь вы по сути
[01:22:08.980 --> 01:22:13.660]  пробежались протестировались на все обучающие выборки и вы обучились на все обучающие выборки
[01:22:13.660 --> 01:22:19.660]  тут маленькое замечание что делать когда у вас получил 10 моделей на выходе ведь у вас здесь
[01:22:19.660 --> 01:22:25.000]  здесь здесь и здесь получил здесь разных моделей тут есть два варианта первая модель линейная
[01:22:25.000 --> 01:22:31.360]  допустим от линейной регрессии можете банально усреднить их веса и все долго не думая второе
[01:22:31.360 --> 01:22:36.160]  модели какие-то сложные например это какие-то деревья тогда потом одиннадцатый раз обучаете
[01:22:36.160 --> 01:22:39.800]  модель с нужными вам гипер параметрами гипер параметры вы тоже можете подбирать по кросс
[01:22:39.800 --> 01:22:44.880]  валидации просто качестве скоров вы выбираете усредненность и тогда собственно вы просто
[01:22:44.880 --> 01:22:51.080]  потом берете все обучающую выборку нужные вам гипер параметры обучаете модель целиком еще раз
[01:22:51.080 --> 01:22:55.240]  но уже с теми гипер параметрами которые у вас были а как обнаружить пере обучение
[01:22:55.240 --> 01:23:05.920]  нет две вещи первое вы можете так посмотреть вы подкрутили гипер параметры посмотрели на кросс
[01:23:05.920 --> 01:23:11.320]  валидации как поменялся скор стало лучше или хуже на отложенных данных второе вы тем самым
[01:23:11.320 --> 01:23:16.520]  детектируете пере обучение если у вас начинает резко расходиться качество на обучающей выборке
[01:23:16.520 --> 01:23:21.880]  и на валидационной то тут два варианта либо валидационная выборка плохая сильно хуже чем
[01:23:21.880 --> 01:23:26.600]  обучающие либо и пере обучает кросс валидации вас по сути спасает от того что вы пере обучаетесь
[01:23:26.600 --> 01:23:31.680]  то что тьфу от того что она плохая потому что вы пробежались по всем под выборком если на всех
[01:23:31.680 --> 01:23:43.400]  стало хуже значит вы пере обучились под обучающую выборку да конечно то есть по-хорошему если вы
[01:23:43.400 --> 01:23:47.600]  смотрите просто отклонения назначение ошибки вы смотрите на его средний по-хорошему посмотреть
[01:23:47.600 --> 01:23:51.040]  именно на статистике ошибки возможно вас на каких-то классах ошибка больше и так далее
[01:23:51.040 --> 01:23:56.040]  анализировать ошибку более глубоко абсолютно верно но по крайней мере вот базовая вещь вы
[01:23:56.040 --> 01:24:01.120]  по кросс валидации можете выбрать себе так не выбрать а заметить что у вас на отложенной выборке
[01:24:01.120 --> 01:24:05.880]  всегда ошибка сильно выше чем на трейновый на обучающей значит скорее всего слишком сложная
[01:24:05.880 --> 01:24:11.440]  модель она просто-напросто пере обучается под ваши данные и вы находитесь где-то вот здесь надо
[01:24:11.480 --> 01:24:22.320]  делать модель попроще и так далее вот 10 это просто количество фолдов это чем больше тем лучше но
[01:24:22.320 --> 01:24:26.800]  чем больше тем дороже у вас сложность линейно растет со увеличением количества фолдов ранее
[01:24:26.800 --> 01:24:32.080]  вот совсем давным-давно когда данные были маленькие отрава зеленая даже был отдельно
[01:24:32.080 --> 01:24:39.600]  называемый метод л л о о ли ван аут короче оставь один то есть там в качестве трест обучающий
[01:24:39.600 --> 01:24:44.200]  выборке использовал вся выборка с заключением одного объекта соответственно одном объект тестировались
[01:24:44.200 --> 01:24:48.560]  но если у вас выборка размером там миллион то миллион раз обучать вашу модель вы замучаетесь
[01:24:48.560 --> 01:24:54.400]  поэтому условно обычно будет там на 5 10 20 фолдов в зависимости от сложности то есть грубо говоря
[01:24:54.400 --> 01:24:58.400]  сколько вы можете себе позволить именно поэтому кросс валидации с ким-то убер нейронками не
[01:24:58.400 --> 01:25:04.280]  используется потому что обычно там одна модель обучается месяца так два на кластере сгп у если
[01:25:04.280 --> 01:25:11.080]  вы будете это 5 раз делать то это почти год дорого просто дорого вот поэтому когда мы дойдем до
[01:25:11.080 --> 01:25:16.360]  сложных моделей мы краски поговорим еще раз как нам выбирать уже хорошую одну валидационную выборку
[01:25:16.360 --> 01:25:22.200]  там должны совпадать распределение по классам там или каким-то категориальным переменным по ошибке
[01:25:22.200 --> 01:25:26.800]  короче по всему они должны визуально выглядеть как выборки из одной той же из одного это уже
[01:25:26.800 --> 01:25:31.920]  распределение тогда все хорошо ну что ж она это мы подходим к финалу нашей лекции собственно
[01:25:31.920 --> 01:25:37.200]  линейный модели это замечательные модели которые работают половине случаев не как говорит мой
[01:25:37.200 --> 01:25:41.360]  например новочек руководитель это так называется метод палки и веревки очень простой он не может
[01:25:41.360 --> 01:25:44.860]  не работать он может не работать только если у его слишком сложная задача для линии на модели
[01:25:44.860 --> 01:25:49.520]  там ломаться нечему понимание линейных моделей позволит вам работать и с нейронными сетями и
[01:25:49.520 --> 01:25:53.520]  на самом деле с деревьями потому что как мы с вами видим деревья это тоже в некотором смысле ли
[01:25:53.520 --> 01:25:58.760]  неймуте ли просто надо очень странными признаками и собственно стройте валидацию так что вы могли
[01:25:58.760 --> 01:26:03.760]  Если вы не уверены в вашей валидации, значит ровно настолько не уверены в том, что ваша модель вообще делает нефигню.
[01:26:03.760 --> 01:26:09.760]  То есть вот валидация и постановка задачи оптимизации, это, наверное, два таких кругольных камня, вокруг которых строится все остальное.
[01:26:09.760 --> 01:26:12.760]  Если у вас ошибка там или здесь, то остальное бесполезно.
[01:26:12.760 --> 01:26:17.760]  Вот. Ну, на этом лекция завершается, перерыв 15 минут, дальше семинар.

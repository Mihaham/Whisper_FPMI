[00:00.000 --> 00:10.060]  так народ значит сначала я отвечу все запись есть да отвечу на два вопроса
[00:10.060 --> 00:17.280]  первый вопрос какой дедлайн у нас дедлайн по всем заданиям но ориентировочно за неделю
[00:17.280 --> 00:26.440]  до зачетной сессии чтобы мы зачетной сессии уже понимали у кого что отдельных дедлайнов
[00:26.440 --> 00:33.880]  по заданиям нету да то есть вы в принципе можете оставить все на последнюю неделю но это не
[00:33.880 --> 00:44.840]  рекомендуется делать как всегда вот что касается использования солвера на мой взгляд на стэпике
[00:44.840 --> 00:54.600]  самый ну как бы самый простой вариант сейчас это использовать сайпай линпрок оптимайз вот есть
[00:54.600 --> 01:02.600]  такая библиотека сайпай там есть модуль такой в этой библиотеке который называется оптимайз и
[01:02.600 --> 01:15.720]  в этом модуле есть такая функция которая называется линпрок она на вход принимает
[01:15.720 --> 01:22.040]  но я не буду сейчас пересказывать документации этой библиотеке но в общем она на вход принимает
[01:22.040 --> 01:27.440]  матрицы которые описывают задачи линейного программирования вот задача линейного
[01:27.440 --> 01:33.520]  программирования у нас как выглядит да у нас есть неравенство зависящий от х и у этих х
[01:33.520 --> 01:44.880]  коэффициенты можно обозвать там а 1 1 и так далее а 1 n значит там меньше или равно чем b 1 и так
[01:44.880 --> 01:49.120]  далее у вас получается что-то типа системы ну линейных уравнений только не уравнение
[01:49.120 --> 01:53.280]  а неравенство dir chest мы системы линейных уравнений можем записать как произведение
[01:53.280 --> 01:57.820]  матрицы на вектор равняется другому вектору до т mix lovelyould something iranice
[01:57.820 --> 02:03.880]  принципе можем записывается а некоторые матрице помноженные на вектор их транспониров disposition
[02:03.880 --> 02:10.080]  вектор столбец по компонентным не превосходит какого-то вектор какой-то вектор столбец
[02:11.800 --> 02:17.800]  вот так мы записываем неравенство но вот в этой функции линпрок там можно Separately задать
[02:17.800 --> 02:24.480]  неравенство и отдельно задать равенство. Вот там как раз есть соответственно
[02:24.480 --> 02:34.240]  upper bound и upper bound. И есть такая уравненческая часть этой системы.
[02:34.240 --> 02:42.220]  a eq на x равняется b eq, тоже вектор столбит. И вот это такой общий вид
[02:42.220 --> 02:46.500]  задачи. На самом деле, в линейном программировании не обязательно иметь
[02:46.700 --> 02:50.220]  отдельно равенства, отдельно неравенства. Потому что вы всегда можете
[02:50.220 --> 02:53.620]  промоделировать равенство двумя неравенствами в противоположную
[02:53.620 --> 03:01.300]  сторону, правда. А кроме того, вы можете промоделировать неравенства равенством.
[03:01.300 --> 03:05.460]  Ну, не совсем. Промоделировать неравенство равенством, плюс ограничения
[03:05.460 --> 03:11.020]  на знак переменной. Вот я напоминаю, что неравенство вот такое.
[03:11.020 --> 03:15.740]  Можно промоделировать следующим образом. Вы вводите переменную недостатка такого,
[03:15.740 --> 03:25.580]  то есть насколько левая часть действительно меньше, чем правая, раз она не превосходит правой, вот какая разность этих двух частей, и тогда вы обозначаете вот этот вот недостаток
[03:26.580 --> 03:35.460]  переменной y, ну y1, скажем, да, и говорите, что вот если бы у нас еще плюс вот y1 было, тогда бы было точное равенство, равно b1, и
[03:37.580 --> 03:39.580]  y1 по определению
[03:40.420 --> 03:42.420]  неотрицательная величина, то есть
[03:42.460 --> 03:52.220]  задачу линейного программирования с неравенствами можно свести к задаче, где только равенство, а из неравенств только вот такие вот штуки, только ограничения на знаке переменных
[03:53.100 --> 03:55.300]  иногда это бывает удобно, но в вашем случае
[03:56.420 --> 03:58.420]  по-моему и так нормально
[03:59.980 --> 04:04.020]  вот там у нас задачах в основном как раз неравенства сплошные
[04:06.100 --> 04:08.100]  вот это по поводу
[04:08.980 --> 04:10.740]  линпрога
[04:10.740 --> 04:15.500]  но на степике библиотеки чуть-чуть устаревшие, так на годик, на два
[04:16.980 --> 04:22.100]  ну и соображение совместимости там обратно, чтобы ничего не рушилось, но
[04:23.500 --> 04:27.140]  последние фичи вот библиотеки линпрог, я не уверен, что они там есть
[04:28.100 --> 04:29.420]  типа
[04:29.420 --> 04:31.420]  использование солверов совсем
[04:32.580 --> 04:34.300]  мощных
[04:34.340 --> 04:43.700]  значит high GHS такой есть, вот там этого пока нет, но зато там базовая функциональность вся присутствует, используйте это
[04:45.660 --> 04:48.380]  так, чего еще хотел сказать, ну да, дедлайны
[04:48.900 --> 04:50.900]  как делать линейное программирование
[04:51.300 --> 04:59.460]  вроде сказал, если вы пишете на плюсах, то вы можете заморочиться и реализовывать симплекс метод, и он должен работать нормально
[05:00.180 --> 05:05.300]  вот либо вы можете, ну, посмотреть псевдокод симплекс метода, его закодить, то есть за
[05:06.340 --> 05:08.340]  там какой-то
[05:09.140 --> 05:10.340]  код
[05:10.340 --> 05:13.580]  использованный именно в этой части, в части симплекс метода
[05:14.460 --> 05:16.580]  плющить никого никто не будет
[05:18.140 --> 05:20.140]  вообще пишите все самостоятельно
[05:22.220 --> 05:25.020]  у меня есть скрипт, который со степика все выгружает и
[05:25.940 --> 05:27.940]  проверяет это все по метрике
[05:28.300 --> 05:33.980]  НСД, знаете, что такое НСД метрика, я расскажу просто, потому что это интересная идея
[05:35.460 --> 05:41.180]  вот как сравнивать, вообще как сравнивать тексты на совпадение, да, вот как вы представляете там
[05:41.940 --> 05:46.380]  сравнивать на плагиат, да, два текста, вы как бы сравнивали?
[05:48.180 --> 05:53.020]  вы бы не сравнивали? от греха подальше, да, не будем сравнивать, закроем глаза на это
[05:54.420 --> 05:56.420]  а если бы надо было сравнивать, то что бы вы делали?
[05:58.940 --> 06:05.660]  ну, а я имею ввиду алгоритмически, вот есть два текста, там две строки какие-то большие, что бы с ними делали?
[06:07.780 --> 06:11.740]  ну хорошо, расстояние левенштейна, например, вы знаете такую штуку?
[06:12.940 --> 06:21.220]  вот редакторское расстояние, да, сколько символов нужно вынуть из одной строки и сколько в нее вставить, чтобы из первой строки
[06:21.220 --> 06:22.740]  получилась вторая
[06:22.740 --> 06:24.800]  редакторское расстояние, вы даже скорее всего
[06:25.800 --> 06:34.400]  динамическое программирование какое-нибудь прогали, вот в отношении редакторского расстояния, но есть гораздо более интересная и прикольная штука, которая называется ncd
[06:37.920 --> 06:41.720]  это не относится к нашему курсу вообще, мне просто хотелось с вами поделиться
[06:42.760 --> 06:47.200]  идеей, не надо это записывать там и так далее, да, не будет на зачете
[06:47.880 --> 06:52.600]  вот ncd это расшифровывается как normalized compression distance, то есть нормализованное
[06:53.360 --> 06:56.280]  компрессионное расстояние, да, ну compression сжатие
[06:57.160 --> 07:03.320]  а идея очень простая, вот если два текста совсем совпадали бы, то, наверное, если бы вы их
[07:03.880 --> 07:10.120]  сжимали каким-нибудь там zip или rare или 7 zip, то эти два текста они
[07:10.640 --> 07:14.880]  сжатыми были бы по размеру не больше чем один текст, да, если его сжать
[07:14.960 --> 07:20.840]  ну то есть как бы если второй текст не несет никакой новой информации по сравнению по отношению к первому, то он почти
[07:21.600 --> 07:29.560]  забесплатно прибавляется к первому, да, при сжатии, но мы ожидаем, что архиваторы они как-то так должны работать, да, то есть они берут только какую-то вот
[07:30.240 --> 07:34.960]  уникальную часть информации, а повторения все они стараются максимально как-то
[07:36.440 --> 07:40.760]  значит максимально эффективно их закодировать и
[07:42.760 --> 07:47.400]  вот как раз ncd метрика она работает ровно таким образом, мы сжимаем
[07:47.400 --> 07:51.040]  сжимаем текст, ну, допустим, у нас тексты хранятся в
[07:52.080 --> 07:58.680]  значит переменных x и y, значит мы берем просто конкатинацию этих штук, ну, можем как-то это дальше вычесть
[07:59.560 --> 08:02.600]  вычесть, например, сжатый текст x и
[08:03.320 --> 08:05.320]  поделить тоже на
[08:06.680 --> 08:09.240]  comp x, вот получается такая штуковина
[08:09.800 --> 08:16.200]  если, например, x это какой-то большой текст, из которого взят просто кусочек, такой y не очень большой, ну, какой-то
[08:16.200 --> 08:22.800]  абзац, да, то вот эта штука будет практически нулевой, ну, очень маленькой, во всяком случае уж по отношению к
[08:23.800 --> 08:26.600]  объему текста x, она-то будет совсем маленькой
[08:27.240 --> 08:36.040]  вот, ну, вот, пожалуйста, идея такая, как вы можете сжимать, в смысле, мерить расстояние, не особо заморачиваясь вообще про природу данных
[08:36.040 --> 08:38.680]  и эта штука, оказывается, отлично работает на
[08:39.400 --> 08:44.600]  на всяких текстах решений в лотехе, вот, на втором курсе, а во второй курсе
[08:44.600 --> 08:46.600]  ну, вот, вот, на втором курсе фифта
[08:47.320 --> 08:48.760]  значит, у нас там
[08:48.760 --> 08:53.720]  студенты в техе, в лотехе оформляют решение математических задач и мы там на плагиат
[08:54.200 --> 09:01.240]  той же самой метрикой проверяем, очень хорошо работает, ну, то есть, очень грустно работает потом, когда кого-то уловишь на плагиате, пишешь там письмо
[09:01.720 --> 09:09.960]  что, ну, как же так, вы списали, вот, смотрите, вот, и человек что-нибудь пытается объяснить, что, ну, типа, там, на самом деле, там, не знаю
[09:10.120 --> 09:14.120]  я очень торопился, или что-нибудь еще, ну, в общем, короче, очень стыдная ситуация какая-то получается
[09:15.960 --> 09:19.960]  вот, но, как бы, хорошо работает
[09:21.000 --> 09:24.440]  и программный код тоже отлично, на самом деле, этой штукой
[09:25.160 --> 09:33.400]  отрабатывается, ну, наверное, можно что-то эффективнее предложить, конечно, то есть, парстить в абстрактные синтоксические деревья этот код и сравнивать уже
[09:34.040 --> 09:37.960]  абстрактные синтоксические деревья, но до этого я пока не доказал, что это может быть
[09:38.120 --> 09:48.120]  вот, это надо потом еще вложить, годик труда, наверное, чтобы этим заняться
[09:48.120 --> 09:52.120]  ну, ладно, это было лирическое отступление
[09:52.120 --> 09:58.120]  вот такое запугивание, не списывайте, а то иначе вы пойдете под инсидиметрику
[09:58.120 --> 10:06.120]  вот, но, ладно, кто-нибудь кто-то, кто так категорически не боится таких вещей, то-то и так не станет бояться, я думаю
[10:06.280 --> 10:12.280]  а кто хочет писать самостоятельно, и так будет писать самостоятельно, как показывает оба
[10:12.280 --> 10:18.280]  так, теперь вернемся к нашему, к нашей теме, наконец, занятия
[10:18.280 --> 10:26.280]  у нас, мы с вами остановились, по-моему, на том, что сформулировали жадный алгоритм для покрытия матрицы
[10:26.280 --> 10:30.280]  и этот жадный алгоритм, он у нас как действовал
[10:30.440 --> 10:36.440]  он на каждом шаге брал, значит, строчку, которая покрывает
[10:36.440 --> 10:42.440]  у этой строки вес мы обозначали W
[10:42.440 --> 10:54.440]  так, окей, значит, мы брали строчку, какую-то R, у которой максимальное отношение
[10:54.600 --> 11:02.600]  для чего, наверное, минимальное, мне так удобно будет записать, минимальное отношение веса строки
[11:02.600 --> 11:08.600]  к количеству столбцов, которые покрываются этой строкой
[11:08.600 --> 11:12.600]  число покрытых столбцов
[11:12.600 --> 11:22.600]  при этом, естественно, имеется в виду число столбцов, которые не были раньше покрыты, но покрываются на этом шаге конкретно
[11:22.760 --> 11:26.760]  то есть это такая стоимость покрытия одного столбца у нас получается
[11:26.760 --> 11:34.760]  и мы с вами договорились, что на каждом столбце матрицы в момент, когда он покрывается жадным алгоритмом
[11:34.760 --> 11:44.760]  мы будем писать такой Z на этом столбце, то есть вот если, например, строка покрывает нам три столбца
[11:44.760 --> 11:48.760]  то это значит, что мы на этих столбцах
[11:48.920 --> 11:52.920]  я уже не знаю, хочу использовать букву Z, честно говоря, или нет
[11:52.920 --> 11:56.920]  назовем ее буквой Q
[11:56.920 --> 12:04.920]  для этих столбцах будет вес WR натрия, WR натрия, WR натрия, у всех одинаковый
[12:04.920 --> 12:08.920]  который равен как раз вот этой вот величине
[12:09.080 --> 12:15.080]  благодаря тому, что мы это делаем, получается, что в итоге сумма вот этих всех фуитах
[12:15.080 --> 12:25.080]  по всем столбцам и вот единицы до n будет равна как раз весу всех строк, которые были включены жадным алгоритмом
[12:25.080 --> 12:29.080]  на протяжении работы, на протяжении его работы
[12:29.080 --> 12:35.080]  вот как раз вот ровно потому, что это все суммируется к WR
[12:35.240 --> 12:41.240]  и поскольку каждый столбец обязательно на протяжении алгоритма ровно один раз покроется
[12:41.240 --> 12:47.240]  как вот вновь покрытый столбец, то как раз у каждого столбца эта штука не нулевая
[12:47.240 --> 12:51.240]  а просуммировав все мы получим сумму весов всех выбранных строчек
[12:55.240 --> 13:03.240]  Q это то, что раньше я обозначал Z, но мне не хочется писать букву Z, я, пожалуй, напишу Q
[13:05.240 --> 13:13.240]  по определению, по определению это было количество, это был вес строки, которая выбрана
[13:13.240 --> 13:17.240]  поделить на количество столбцов, которые мы покрыли этой строкой
[13:21.240 --> 13:25.240]  то есть если мы, например, выбираем строку, она там покрывает 10 столбцов
[13:25.240 --> 13:31.240]  но вот 7 столбцов уже раньше были покрыты, мы на них уже написали свои кушки
[13:31.400 --> 13:35.400]  а 3 столбца у нас новых, которые мы покрываем
[13:35.400 --> 13:41.400]  вот тогда мы на каждом из этих трех столбцов напишем WR поделить на 3
[13:41.400 --> 13:49.400]  и мы с вами взяли вот эту сумму и сказали, что с одной стороны, вот эта сумма всех кушек
[13:49.400 --> 13:53.400]  по всем столбцам равняется весу жадного решения, просто по определению
[13:55.400 --> 13:59.400]  а с другой стороны, мы эту сумму как-то по-другому хотим оценить
[13:59.560 --> 14:05.560]  как-то по-другому хотим оценить, чтобы связать это все с оптимальным покрытием
[14:05.560 --> 14:11.560]  и, значит, чего мы с вами проделали, я пока что напоминаю в известном смысле
[14:11.560 --> 14:17.560]  мы дальше сказали, что пусть RT строка произвольная
[14:17.560 --> 14:21.560]  может быть она не была выбрана жадным алгоритмом, может быть была
[14:21.560 --> 14:23.560]  вот посмотрим произвольную строчку матрицы
[14:23.560 --> 14:27.560]  и рассмотрим столбцы, которые этой строчкой покрываются
[14:27.720 --> 14:33.720]  я не помню, по-моему, S их обозначали, типа SL и так далее, S1
[14:33.720 --> 14:35.720]  и мы сказали, давайте их занумируем
[14:35.720 --> 14:39.720]  это пока краткое содержание предыдущей серии, как в Санте-Барбаре
[14:39.720 --> 14:43.720]  то есть там нового ничего пока для вас, скорее всего, нету
[14:43.720 --> 14:49.720]  так вот, мы сказали, давайте мы занумируем все столбцы, которые покрываются этой строчкой
[14:49.720 --> 14:55.720]  все столбцы, в которых в этой строке стоят единички в матрице
[14:55.880 --> 15:01.880]  и занумируем их в том порядке, в котором они покрывались жадным алгоритмом
[15:01.880 --> 15:03.880]  жадно алгоритм, в итоге все столбцы покрывают
[15:03.880 --> 15:05.880]  значит, он покрывает и эти столбцы в том числе
[15:07.880 --> 15:11.880]  и естественно, что жадно алгоритм, их покрывает, не обязательно, по одному
[15:11.880 --> 15:13.880]  то есть, он, в какой-то момент, выбирает строчку, которая, может, там
[15:13.880 --> 15:16.880]  первые четыре столбца, в этом списке покрыла
[15:16.880 --> 15:18.880]  потом, какую-нибудь строчку, которая следующий столбец, покрыла
[15:18.880 --> 15:22.880]  потом строчку, которую все остальные столбцы покрыли
[15:22.880 --> 15:24.880]  то есть они, может быть, покрываются какими-то группами
[15:24.880 --> 15:29.800]  группами как они занумерованы там внутри каждой одной группы это не важно главное чтобы вот в
[15:29.800 --> 15:38.360]  целом они шли да по порядку вот так вот покрытие значит что если столбец идет в этом списке правее
[15:38.360 --> 15:45.720]  то это значит что он покрыт не раньше чем тот столбец который идет в нем левее теперь мы с
[15:45.720 --> 15:53.720]  вами дальше сказали вот что рассмотрим момент который покрывается столбец эскатой и к этому
[15:53.720 --> 15:59.960]  моменту у нас не покрытыми являются как минимум сколько столбцов как минимум南 столбцов правда
[15:59.960 --> 16:07.000]  эскатый я с камени Wolf первые но все кто в этом списке идут провели не покрытыми являются как
[16:07.000 --> 16:15.040]  минимум ка столбцов и эти ка столбцов да у нас есть шанс вот в этот момент как когда у нас этот
[16:15.040 --> 16:19.960]  Jesse не покрытый со столбиц все эти столбцы тоже еще не покрыты в этот момент у нас есть
[16:19.960 --> 16:25.840]  шанс покрытия к все за счет того, что мы возьмем как раз таки r-ую строку матрицы.
[16:25.840 --> 16:33.760]  И если мы возьмем сейчас r-ую строку матрицы, то стоимость покрытия вот этих вот столбцов,
[16:33.760 --> 16:43.680]  каждого из них, будет какая? Ну не больше, чем вес строки. Пусть вот эти столбцы покрываются
[16:43.680 --> 16:53.240]  строкой r. Вот если мы сейчас прям возьмем именно строку r, то мы покроем все эти столбцы
[16:53.240 --> 17:04.680]  гарантированно за один раз. А вес у этой строки, ну wr, и стало быть, взяв r-ую строку,
[17:04.680 --> 17:13.800]  мы в расчете на один столбец будем иметь вот такую стоимость покрытия этого столбца.
[17:13.800 --> 17:21.760]  Но жадно алгоритм он может взять любую строку, у которой наименьшее вот это вот число. И стало
[17:21.760 --> 17:27.080]  быть, если мы рассматриваем конкретно тот шаг, на котором гарантированно покрывается вот именно
[17:27.080 --> 17:36.360]  этот столбец, то на этом столбце пушка, которая написана кукатая, будет не больше, чем wr делить
[17:36.360 --> 17:44.600]  на k. То есть жадно алгоритм сейчас покроет sk. Может быть вместе с этими столбцами, взяв строку r,
[17:44.600 --> 17:48.760]  может быть вместе с какими-то другими столбцами, но во всяком случае у жадно алгоритма есть
[17:48.760 --> 17:55.000]  возможность истратить на этот столбец вот такую вот стоимость. Жадно алгоритм старается
[17:55.000 --> 18:00.000]  минимизировать стоимость, следовательно он выберет что-то еще может быть более оптимальное,
[18:00.000 --> 18:07.000]  но уж как максимум вот такое, потому что он здесь минимизирует. Вот, собственно поэтому мы и
[18:07.000 --> 18:19.000]  выписываем такое нерайстое в эту сторону. Ну как вам сказать, да, да, да, то есть кукатая это вообще
[18:19.000 --> 18:26.480]  кушки, это такие константы, которые присваиваются на разных шагах алгоритма столбцам, и мы знаем,
[18:26.480 --> 18:31.760]  что у каждого столбца есть единственный шаг алгоритма, на котором этот столбец получит вот
[18:31.760 --> 18:37.120]  такую пометку, и больше эта пометка меняться уже никогда не будет, потому что столбец только один
[18:37.120 --> 18:42.040]  раз меняет свой статус как бы с непокрытого на покрытый, да, и вот ровно на этом шаге,
[18:42.040 --> 18:48.720]  на котором столбец изменяет свой статус, мы и приписываем этому столбцу свою кушку. И вот как
[18:48.720 --> 18:53.720]  раз мы рассматриваем тот шаг, на котором покрывается именно вот этот вот столбец эскатый,
[18:53.720 --> 19:00.000]  и говорим, что кушка, которая может быть ему приписана, уж заведомо не больше, чем вот такая
[19:00.000 --> 19:06.840]  штука, потому что вот конкретная строка, да, вот есть, если мы ее взяли, то приписали бы ровно
[19:06.840 --> 19:12.560]  такую кушку, но может быть даже возьмем что-то еще лучшее и припишем еще лучшую кушку, ну,
[19:13.040 --> 19:22.000]  если меньше, да, потратим чуть поменьше денег на покрытие этого столбца. Вот, и вот у нас
[19:22.000 --> 19:33.880]  получается вот такая вот, такая штука, у нас получается такая оценка. Ну, исходя из этой оценки,
[19:33.880 --> 19:45.400]  мы с вами сможем оценить вот эту вот сумму. Давайте это проделаем. Давайте рассмотрим
[19:45.400 --> 19:51.640]  оптимальное покрытие матрицы, оптимальное покрытие, с которым нам надо сравниться. Вес
[19:51.640 --> 20:11.840]  этого оптимального покрытия, да, вес опт. Вес опт. Как связаны K и L? Никак.
[20:11.840 --> 20:23.040]  S или, так далее, S1. Покрываются, вот здесь, имеется в виду, просто это те столбцы,
[20:23.040 --> 20:28.160]  на пересечении которых со строкой, стоят единички, то есть столбцы, могущие быть покрыты
[20:28.160 --> 20:35.120]  этой строкой. Да, я просто слово «покрывать», «покрывается», да, я использую в двух, может быть,
[20:35.120 --> 20:41.120]  разных качествах, хотя не часто это дело. Первое качество — это что столбец, в принципе,
[20:41.120 --> 20:45.240]  может быть покрыт, если мы возьмем эту строку. Это просто значит, что на их пересечении стоит
[20:45.240 --> 20:51.520]  единичка в матрице. А второе качество — это что вот этот столбец конкретно перешел из статуса
[20:51.520 --> 20:55.440]  «непокрытого» в статус «покрытый» по ходу жадного алгоритма, вот ровно в тот момент,
[20:55.440 --> 21:02.760]  когда мы взяли какую-то строчку. Вот это я выписал список столбцов, которые просто могут быть покрыты
[21:02.760 --> 21:09.520]  этой строкой, да, в принципе, все столбцы покрывабельны этой строкой. Вот, дальше мы
[21:09.520 --> 21:15.680]  рассматриваем произвольный столбец из этого списка и немножко про него рассуждаем. Это может
[21:15.680 --> 21:20.840]  быть в том числе с l-ты или в том числе с первой, да, вот просто смотрим на произвольное любое k
[21:20.840 --> 21:27.400]  от единички до l и про него рассуждаем. Вот, вот что мы сейчас делаем, да, можно написать здесь,
[21:27.400 --> 21:45.040]  рассмотрим произвольное k от 1 до l. Значит, и говорим, что в момент, когда покрывается столбец
[21:45.040 --> 21:50.880]  s-катой, а он когда-то же покрывается жадным алгоритмом, у нас не покрыты вот как минимум
[21:50.880 --> 21:55.840]  эти столбцы. Если мы выберем эту строчку, то мы точно их все покроем прям с ходу и
[21:55.840 --> 22:04.760]  так далее, так далее. Вот оттуда мы выписываем вот это неравенство. Значит, дальше мы с вами,
[22:04.760 --> 22:11.480]  нам нужно как-то вот сравнить, сравнить с вот этим вот оптимальным покрытием, сравнить с
[22:11.480 --> 22:24.680]  оптимальным покрытием. Давайте с ним сравнимся, давайте запишем, что это такое. Это сумма wr по
[22:24.680 --> 22:32.680]  всем строчкам, принадлежащим оптимальному покрытию, ну просто по определению. Это сумма wr по всем
[22:32.680 --> 22:51.400]  строчкам, принадлежащим оптимальному покрытию. Так, чудесно. Так, давайте мы теперь, давайте мы
[22:51.400 --> 22:59.600]  теперь еще запишем здесь кое-что. Мне сейчас к алгоритму тут хочется перейти. Парам-парам.
[22:59.600 --> 23:10.000]  Запишем вот такую вот штуку. Сумма wr по всем i, вот единица dn. Мы знаем, что это вес жадного
[23:10.000 --> 23:20.400]  покрытия. Она не превосходит вот такой вот штуки. Сумма по всем строчкам из оптимального покрытия
[23:20.400 --> 23:33.080]  здесь я напишу сумму по всем столбцам, покрываемым этой строкой. Вот, но я это напишу вот таким вот
[23:33.080 --> 23:45.960]  образом. Сумма по g от lr до единички. Но для каждой строки r столбцы, которые ей покрываются,
[23:45.960 --> 23:54.440]  они какие-то свои. Вот это l, оно естественно для каждого r свое. И вот здесь я хочу написать вот
[23:54.440 --> 24:01.360]  что. Просуммировать по всем строчкам из оптимального покрытия, а здесь под этой суммой
[24:01.360 --> 24:09.120]  просуммировать по всем столбцам, которые покрываются, покрывабельны этой строкой. Просуммировать
[24:09.120 --> 24:23.080]  соответствующую кушку с индексом s ж. Сейчас давайте разберемся, что здесь происходит. Каждый
[24:23.080 --> 24:30.120]  столбец, он обязательно покрыт оптимальным покрытием, иначе оно не было бы покрытием всей
[24:30.120 --> 24:35.920]  матрицы. То есть для каждого столбца найдется какая-то строка из оптимального покрытия,
[24:36.320 --> 24:46.120]  которой он покрывается. И вот мы теперь говорим, вот здесь вот в этой сумме, эта сумма по всем
[24:46.120 --> 24:53.320]  вообще столбцам матрицы, сумма кушек. А здесь у нас тоже сумма кушек, но такая двойная сумма кушек,
[24:53.320 --> 24:59.120]  в которой мы перебираем все строчки из оптимального покрытия и для каждой строки
[24:59.120 --> 25:05.400]  рассматриваем все столбцы, покрываемые этой строкой. Можем мы между двумя такими суммами
[25:05.400 --> 25:11.160]  поставить знак меньше или равно? Ну в принципе да, просто в этой сумме все кушки учитываются ровно
[25:11.160 --> 25:18.480]  по одному разу, каждый столбец дает вклад ровно в одну такую кушку, в соответствующую сумму. А
[25:18.480 --> 25:26.720]  здесь мы с вами что сделали? Здесь мы сказали, рассмотрим все все строчки, которые у нас в матрице,
[25:26.720 --> 25:34.400]  которые принадлежат оптимальному покрытию. Значит оптимальное покрытие. И для каждой такой
[25:34.400 --> 25:42.320]  строчки R возьмем все столбцы, которые покрываются этой строкой, на пересечении которых с этой
[25:42.320 --> 25:49.960]  строкой стоят единички. И просуммируем вот эту кушку, эту, эту, эту, эту. Потом еще допустим вот
[25:50.400 --> 25:57.420]  строка покрывает вот этот столбец, вот этот столбец, этот столбец, этот, этот. Просуммируем
[25:57.420 --> 26:04.020]  теперь для этой строки еще вот эту кушку, эту, эту, эту, эту. При этом некоторые кушки будут у нас
[26:04.020 --> 26:11.120]  вот в такой двойной сумме повторяться по многу раз. Например вот этот столбец, его кушка она в
[26:11.120 --> 26:15.020]  в эту сумму войдет и для этой строки, и для этой строки
[26:15.020 --> 26:16.020]  с повторением.
[26:16.020 --> 26:17.840]  Ну не беда, ничего страшного.
[26:17.840 --> 26:19.940]  Ну и так дальше.
[26:19.940 --> 26:23.360]  Мы главное знаем, что каждый столбец какой-то строкой
[26:23.360 --> 26:27.040]  из оптимального покрытия точно покрывается, и значит
[26:27.040 --> 26:30.920]  его кушка, кушка этого столбца, она окажется обязательно
[26:30.920 --> 26:32.920]  вот в этой вот двойной сумме.
[26:32.920 --> 26:36.520]  Хотя бы разочек, может быть пару раз, может быть
[26:36.520 --> 26:45.240]  10 раз, да, ну вот хотя бы по разу.
[26:45.240 --> 26:49.080]  Если хотите, я здесь, значит, словами просто могу переписать
[26:49.080 --> 26:53.880]  сумма по всем столбцам, покрываемым Эртой строкой.
[26:53.880 --> 27:04.360]  Здесь написать Qs, да, значит, по столбцам, по столбцам
[27:04.560 --> 27:18.080]  Qs покрываемым строкой R.
[27:18.080 --> 27:22.400]  Но я надеюсь, что на картинке, на такой, это как-то более
[27:22.400 --> 27:23.400]  прозрачно.
[27:23.400 --> 27:27.560]  Перебираем строчки оптимального покрытия, для каждой строчки
[27:27.560 --> 27:31.400]  суммируем кушки всех столбцов, покрываемых вот этой вот
[27:31.400 --> 27:32.400]  строкой.
[27:33.080 --> 27:37.200]  Используем просто определение, что покрытие, это такое множество
[27:37.200 --> 27:41.120]  строчек, что у каждого столбца матрицы без отключения
[27:41.120 --> 27:45.240]  найдется единичка хотя бы с одной строкой, значит,
[27:45.240 --> 27:47.960]  вот этого вот множества, да, а это значит, что каждый
[27:47.960 --> 27:51.200]  столбец обязательно хотя бы разочек, но вот в такую
[27:51.200 --> 27:52.640]  двойную сумму войдет.
[27:52.640 --> 27:55.840]  Найдется такая строка, которая покрывает этот столбец,
[27:55.840 --> 28:00.240]  то есть найдется такая строка, что вот этот столбец,
[28:00.240 --> 28:03.600]  его кушка войдет вот в эту сумму для этой строки.
[28:03.600 --> 28:07.360]  Ну вот, я напишу так, что в принципе вот это вот
[28:07.360 --> 28:10.360]  неравенство вытекает из определения покрытия, то
[28:10.360 --> 28:13.440]  есть вот это неравенство, оно справедливое не только
[28:13.440 --> 28:16.680]  для оптимального покрытия, а вообще для любого абсолютно
[28:16.680 --> 28:22.000]  покрытия матрицы, это было бы справедливо, вытекает
[28:22.000 --> 28:30.920]  и следует из определения, из определения покрытия.
[28:30.920 --> 28:42.120]  Ну вот, еще раз повторю, что это не вытекает не из
[28:42.120 --> 28:44.200]  оптимальности, это ничего общего с оптимальностью
[28:44.200 --> 28:47.120]  не имеет, это просто вытекает из того, что вот это множество
[28:47.120 --> 28:49.720]  опт является покрытием матрицы и больше ни с чего.
[28:49.720 --> 28:55.560]  Ну и из того, что кушки не отрицательные числа.
[28:55.560 --> 29:03.040]  И вот теперь мы можем продолжить вот эту вот штуку, теперь
[29:03.040 --> 29:08.440]  мы можем продолжить вот эту вот цепочку, меньше
[29:08.440 --> 29:09.440]  или равно.
[29:09.440 --> 29:15.960]  Народ, если чего хочется спросить, уточнить, то вы
[29:15.960 --> 29:19.160]  пожалуйста не стесняйтесь, мне не страшно там проговорить
[29:19.160 --> 29:22.440]  еще пару раз другими словами, или еще какую-нибудь картинку
[29:22.440 --> 29:23.440]  нарисовать.
[29:23.440 --> 29:36.560]  Вот, если надумаете, обращайтесь, да, ага, хуито, ну это просто
[29:36.560 --> 29:42.880]  какое-то число, написанное на столбце, ну да, да, это
[29:42.880 --> 29:46.920]  вес строчки жадного, выбранный жадным алгоритмом, который
[29:46.920 --> 29:51.160]  я покрыл этот столбец, поделенный на количество
[29:51.160 --> 29:54.240]  столбцов, которые одномоментно были покрыты этой строкой.
[29:54.240 --> 29:55.240]  Да.
[29:55.240 --> 29:58.520]  Но вот в этом неравенстве, народ, в этом неравенстве
[29:58.520 --> 30:01.440]  это неважно, неважно, как определены куиты на самом
[30:01.440 --> 30:02.440]  деле.
[30:02.440 --> 30:05.760]  В этом неравенстве куиты могут быть произвольными
[30:05.760 --> 30:07.800]  не отрицательными числами.
[30:07.800 --> 30:10.520]  Как бы ни были определены не отрицательные числа
[30:10.520 --> 30:14.840]  на столбцах матрицы, вот эти вот куиты, если опт это
[30:14.840 --> 30:18.280]  множество строчек матрицы, образующих ее покрытие,
[30:18.280 --> 30:20.720]  необязательно оптимальное, вот это неравенство все
[30:20.720 --> 30:24.080]  равно будет выполнено, оно все равно будет выполнено.
[30:24.080 --> 30:28.880]  Значит, это ничего, как бы ничего здесь не следует,
[30:28.880 --> 30:31.520]  никакого rocket science здесь нет, ничего не следует ни
[30:31.520 --> 30:35.960]  из жадности, ни из оптимальности, это очень простое неравенство,
[30:35.960 --> 30:39.040]  вытекающее из определения, что у каждого столбца найдется
[30:39.040 --> 30:43.380]  такая строка, которая его покроет, то есть для каждого
[30:43.380 --> 30:47.980]  номера от единички до n найдется такое r, что соответствующая
[30:47.980 --> 30:50.980]  кушка войдет вот в такую сумму, для r строки, вот
[30:50.980 --> 30:52.180]  и все, что здесь написано.
[30:52.180 --> 31:00.060]  Не, мы естественно используем это неравенство с конкретно
[31:00.060 --> 31:03.420]  определенными кушками, верно оно для любых не отрицательных
[31:03.420 --> 31:08.060]  чисел там куиты, но мы его используем вот с этими.
[31:08.060 --> 31:12.860]  Тут я слышу, что вы поразбирались основательно с тем, что
[31:12.860 --> 31:16.980]  происходит, и это хорошо, значит, ну вот теперь мы
[31:16.980 --> 31:22.420]  с вами видим, что жадное покрытие, вес жадного
[31:22.420 --> 31:24.700]  покрытия это сумма всех кушек вот этих, дальше мы
[31:24.700 --> 31:28.020]  написали вот такое неравенство, а дальше нам нужно оценить
[31:28.020 --> 31:31.380]  его, и мы с вами сейчас оценим вот эту внутреннюю сумму
[31:31.380 --> 31:36.220]  просто для каждого r, как мы это сделаем, о, смотрите,
[31:36.220 --> 31:40.100]  а мы же здесь не зря занимались вот этой вот деятельностью,
[31:40.100 --> 31:42.860]  для произвольной строки матрицы, для произвольной
[31:42.860 --> 31:49.180]  строки матрицы мы понимаем, что кушка катово столбца
[31:49.180 --> 31:53.180]  в этом списке, она не превосходит вот такой величины вес этой
[31:53.180 --> 31:57.900]  строки поделенной на k, спрашивается тогда, а как
[31:57.900 --> 32:08.740]  можно оценить сумму qs, давайте qs, qsk получается, надо мне
[32:08.740 --> 32:16.100]  было здесь описать, qs и так по и от единички до l, вот
[32:16.100 --> 32:20.460]  если просуммировать по всем этим столбцам их ушки,
[32:20.460 --> 32:27.380]  то как мы можем оценить такую штуку, qslt не превосходит
[32:27.380 --> 32:34.220]  wr делить на l, здесь, когда slt покрывается, то вместе
[32:34.220 --> 32:38.900]  с ним не покрыто еще как минимум l столбцов, значит,
[32:38.900 --> 32:48.340]  это не превосходит wr делить на l, для s l-1 его кушка будет
[32:48.340 --> 32:59.180]  оценена wrt делить на l-1, l-1, плюс и так далее, плюс wrt
[32:59.180 --> 33:07.700]  на 1, это когда мы будем оценивать qs-ms1, то есть мы
[33:07.700 --> 33:11.540]  здесь для каждого столбца записали такую оценку, осталось
[33:11.540 --> 33:15.860]  просто просуммировать эти оценки по всем k от единички
[33:15.860 --> 33:19.860]  до l, что мы и делаем, но к счастью получается сумма
[33:19.860 --> 33:23.460]  дробей, у которых одинаковые числители, естественно
[33:23.460 --> 33:27.100]  что мы этот числитель можем вынести за знак суммы,
[33:27.100 --> 33:29.900]  давайте мы это сразу сделаем, а под суммой остается очень
[33:29.900 --> 33:37.580]  чего-то хорошее и нам знакомое, что остается, сумма единицы
[33:37.580 --> 33:45.500]  на k под единицы до l, а мы знаем, как оцениваются
[33:45.500 --> 33:53.340]  такие штуки, а как оцениваются такие суммы, почему это
[33:53.340 --> 33:57.820]  логарифм? Разложение в ряд Тейлора, да, совершенно
[33:57.820 --> 34:01.340]  верно, а если не знать про разложение логарифма
[34:01.340 --> 34:05.020]  в ряд Тейлора, то что мы с вами можем еще сделать?
[34:05.020 --> 34:08.100]  Интегральный метод, например, применить в оценке, знаете
[34:08.100 --> 34:13.980]  интегральный метод для оценивания рядов? Нет? А вы кто-то знает?
[34:13.980 --> 34:18.140]  А? А, вы знаете интегральный признак сходимости, но я
[34:18.140 --> 34:26.780]  здесь, да, база, это полезно, значит, вот нам нужно оценить
[34:26.780 --> 34:31.500]  вот такую сумму, что там с ней происходит, мы для
[34:31.500 --> 34:38.020]  этого представляем ее слагаемое как такие столбики, значит,
[34:38.020 --> 34:43.220]  столбик высотой 1, вот такой вот столбик, дальше столбик
[34:43.220 --> 34:47.860]  высотой 1,2, дальше столбик высотой 1,3, дальше столбик
[34:47.860 --> 34:53.100]  высотой 1,4, ну и так далее, да, и вот у нас с вами появляется
[34:53.100 --> 34:55.820]  вот такая последовательность столбиков, площадь каждого
[34:55.820 --> 35:00.060]  такого столбика это произведение ширины, которая у всех
[35:00.060 --> 35:04.980]  столбиков единичная, на высоту, да, то есть вот, значит,
[35:04.980 --> 35:09.700]  если мы с вами дойдем до к здесь, виноват, до l, да,
[35:09.700 --> 35:13.100]  верхний предел суммы, то вот как раз высота последнего
[35:13.100 --> 35:16.860]  столбика будет единичка на l, и площадь суммарная
[35:16.860 --> 35:19.780]  вот этой вот фигуры, составленной из столбиков, это и будет
[35:19.780 --> 35:22.980]  точное значение вот этой вот суммы, ну а теперь нам
[35:22.980 --> 35:27.340]  остается просто обзавестись функцией единица делить
[35:27.340 --> 35:33.180]  на х, которая проходит через правые части этих столбиков,
[35:33.180 --> 35:36.900]  да, вот эта вот функция единичка делить на х, она проходит
[35:36.900 --> 35:41.980]  через правые границы этих столбиков, 1,2, 1,3, 1,4 и так
[35:41.980 --> 35:46.580]  далее, 1, l, и нам теперь что можно сделать, вот
[35:46.580 --> 35:50.740]  первый столбик мы запишем, как он есть, его площадь,
[35:50.740 --> 35:55.620]  его площадь равна единичке, а для всех остальных столбиков
[35:55.620 --> 35:57.820]  суммарная площадь их не превосходит площади под
[35:57.820 --> 36:01.940]  графиком вот этой функции от единицы до l, то есть интеграла
[36:01.940 --> 36:08.940]  от единицы до l, функция единица на х, вот, ну и поскольку
[36:09.380 --> 36:16.380]  интеграл от единицы на х это логарифм, вот единичке
[36:17.340 --> 36:21.900]  до l, вот, то мы как раз получаем вот эту вот интегральную,
[36:21.900 --> 36:27.380]  вернее, получаем вот эту вот оценку с помощью логарифма,
[36:27.380 --> 36:31.380]  логарифм l, да, ну плюс единичка, единичка никуда не денется.
[36:31.380 --> 36:36.700]  Вот, так что теперь мы можем с вами записать вот здесь
[36:36.760 --> 36:41.260]  вот меньше ли равно, и это уже окончательное такое
[36:41.260 --> 36:45.300]  меньше ли равно, которое нам годиться единица плюс
[36:45.340 --> 36:50.980]  логарифм l, ну что такое l, l это максимальное количество
[36:50.980 --> 36:54.780]  единичек, как универсально оценить это l, да, это максимальное
[36:54.780 --> 36:58.140]  количество единичек столбце, вот виноват, в столбце или
[36:58.140 --> 37:04.740]  в строке, в строке, потому что l это максимальное число
[37:04.740 --> 37:11.180]  покрываемых строкой значит это максимальное количество единичек в строке матрицы да можно здесь написать так и написать
[37:11.860 --> 37:17.100]  максимальное число столбцов покрываемых одной строкой ну я здесь давайте напишу просто логарифм n
[37:17.820 --> 37:21.200]  ладно потому что общее это число столбцов матрицы это n
[37:22.180 --> 37:28.580]  так что ну универсальная оценка у нас получается такая единичка плюс лонарифм n вот и
[37:29.060 --> 37:30.580]  и
[37:30.580 --> 37:37.900]  что мы теперь сделаем вот с этой вот оценкой да мы ее просто подставим сюда для того чтобы оценить вот такую вот сумму
[37:39.660 --> 37:41.660]  вот это вот сумма
[37:43.300 --> 37:48.580]  вот эта сумма мне превосходит вес этой строки помножить на какую-то
[37:49.020 --> 37:53.820]  вещь чину которая не зависит от строк вообще да вот это абсолютно какая-то
[37:54.660 --> 37:56.660]  неизменная вещь которая
[37:56.660 --> 38:02.940]  кульматый куль скорой матрица зафиксировано она уже не зависит от никакого конкретного покрытия да значит
[38:04.260 --> 38:08.140]  мы можем вынести вот это вот единица плюс логарифм
[38:10.820 --> 38:14.680]  давайте здесь звездочку поставлю звездочка
[38:16.500 --> 38:18.500]  вынести единица плюс логарифм
[38:20.420 --> 38:24.100]  вот а тут у нас будет сумма по всем строчкам матрицы
[38:25.100 --> 38:27.100]  входящим в оптимальное покрытие
[38:27.620 --> 38:29.620]  wr
[38:30.500 --> 38:34.380]  да я так вот здесь стоял как раз вот этот множитель я его так раз
[38:35.660 --> 38:37.500]  вынес
[38:37.500 --> 38:41.940]  но и теперь только мы вспоминаем что это оптимальное покрытие у нас это опт
[38:43.820 --> 38:48.700]  все то есть мы теперь знаем что это не что иное как вес оптимального покрытия
[38:55.100 --> 38:59.220]  от чего мы отталкивались мы отталкивались от веса
[39:00.900 --> 39:07.800]  жадного покрытия от суммы всех кушек давайте я напишу напомню что это на самом деле дв гриве и
[39:13.140 --> 39:18.500]  закончили мы с вами вот такой вот штуковины и то есть мы с вами дали некую оценку на показатель
[39:18.940 --> 39:24.180]  ассимации нашего алгоритма во сколько раз жадное покрытие по весу больше оптимально
[39:26.020 --> 39:28.020]  это не
[39:28.060 --> 39:29.500]  константный показатель
[39:29.500 --> 39:36.500]  аппроксимации да то есть это не какая-то там двоечка вот а чем матрица больше тем к сожалению будет аппроксимация хуже
[39:37.220 --> 39:39.220]  но во всяком случае
[39:39.220 --> 39:41.220]  во всяком случае в теории да
[39:41.780 --> 39:43.780]  вот теории может быть все не очень хорошо
[39:44.420 --> 39:47.420]  на практике жадно алгоритм работает очень неплохо
[39:49.020 --> 39:51.420]  но сейчас мы тем не менее предъявим пример
[39:52.380 --> 39:54.300]  который показывает что
[39:54.300 --> 39:58.740]  действительно может здесь вот возникать логарифмичный по размеру матрице
[40:00.420 --> 40:05.300]  саммножитель в худшем случае а мы с вами приводили пример такой матрицы нет не помните
[40:06.820 --> 40:11.460]  не было да вот как раз мы сейчас этим займемся но перед тем как мы этим займемся
[40:12.420 --> 40:17.060]  не знаю если чего-то вот еще могу прокомментировать то пока я не стер вот
[40:17.820 --> 40:20.740]  сейчас могу потом уже сотру будет сложнее
[40:23.340 --> 40:25.340]  интегральный метод стираю
[40:26.500 --> 40:31.860]  приятно стирать какие-то вещи типа интегрального метода да которые не имеют отношения в дискретной оптимизации
[40:34.860 --> 40:36.860]  непрерывчина уйди
[40:37.860 --> 40:39.860]  так
[40:40.860 --> 40:42.820]  ладно это мы тоже сотрем под шумок
[40:43.500 --> 40:45.500]  это тоже сотрем
[40:45.540 --> 40:49.060]  ну а дальше уже неважно раз уж столько всего потерли то и это не жалко
[40:51.140 --> 40:53.140]  все стереть
[40:55.980 --> 40:57.980]  давайте пример приводить пример
[41:00.500 --> 41:02.500]  пример матрицы
[41:05.380 --> 41:07.380]  на который жадно алгоритм
[41:07.900 --> 41:14.300]  ну можете написать лажает но я не буду писать лажает мне неудобно да на который жадно алгоритм работает не оптимально
[41:15.660 --> 41:20.020]  на который жадно алгоритм не оптимален
[41:23.980 --> 41:30.460]  но не оптимален это мягко сказано да не оптимален вот именно в этом смысле что возникает такой по порядку
[41:31.620 --> 41:33.420]  сомножитель
[41:33.420 --> 41:35.180]  давайте рассмотрим
[41:35.180 --> 41:37.180]  такую
[41:37.860 --> 41:40.460]  невысокую матрицу но очень широкую
[41:41.540 --> 41:43.660]  сделаем мы ее организуем мы ее вот так вот
[41:44.620 --> 41:48.020]  возьмем единичку первой строке выписываем первую строку матрицы
[41:49.340 --> 41:51.340]  возьмем единичку и
[41:52.100 --> 41:54.100]  куча нулей
[41:54.100 --> 42:00.220]  сейчас мы с вами поймем сколько нулей а потом еще раз давайте возьмем повторим вот это вот все единичка и
[42:01.900 --> 42:03.900]  куча куча нулей
[42:05.140 --> 42:08.260]  дальше вторую строку выпишем возьмем
[42:09.220 --> 42:17.540]  подставим под единичкой 0 tuning потом напишем две единички и кучу нулей и потом снова две единички и куча нулей до конца
[42:18.420 --> 42:22.220]  потом а угадайте что будет потом попытка не пытка
[42:24.220 --> 42:26.960]  три единички да действительно
[42:28.340 --> 42:30.180]  возьмем три единички
[42:30.180 --> 42:36.140]  чтобы не было одиноко добавим к ним четвертую возьмем четыре единички и куча нулей
[42:36.140 --> 42:43.640]  но 4 единички включают же 3 единички да и то же самое потом возьмем значит 4
[42:43.640 --> 42:49.540]  единички и куча нулей и так дальше а теперь а дальше можете сказать что будет
[42:49.540 --> 42:56.840]  дальше уже понятно что будет не 5 единичек 8 16 и так далее вот у нас
[42:56.840 --> 43:03.020]  получается что строчки они являются такими повторениями до одного и того же
[43:03.020 --> 43:11.820]  вот ну и в конце концов естественно мы берем там какую-то строку с 2 в
[43:11.820 --> 43:21.300]  тепени а единичками значит которые вот здесь вот при как это сказать как это
[43:21.300 --> 43:27.340]  сказать по-русски пэддит по-английски это будет пэддит то есть отступ да вот
[43:27.340 --> 43:33.980]  отступ выполнен нулями в общем короче единички передвы зироус
[43:34.980 --> 43:42.020]  которых два в степени а штук но это еще не вся матрица последние две строки будут
[43:42.020 --> 43:57.180]  такие 0 не давайте так 1 и так далее 1 а здесь нули и наоборот куча нулей это
[43:57.180 --> 44:03.980]  вот единственные две строки которые не по предыдущему правилу делаются да а в
[44:03.980 --> 44:09.420]  этих строках как раз отличается левая и правая половинка всего у нас получается
[44:09.420 --> 44:20.620]  в матрице сколько строчек значит это сколько строчек получается значит здесь
[44:20.620 --> 44:26.820]  у нас 2 стих 2 в первый единичек 2 в квадрате единичек 2 в кубе 2 в
[44:26.820 --> 44:32.180]  степени а плюс один единичек да то есть есть вот уже а плюс один строчек ну еще две строки
[44:32.180 --> 44:43.740]  а плюс три строки такая матрица получается размера а плюс три на уох а сколько же здесь
[44:43.740 --> 44:48.780]  столбцов но количество столбцов определяется просто тем что нам нужно вот эти вот единички
[44:48.780 --> 44:54.300]  друг с другом не перекрываясь поставить да вот так вот по диагонали как бы и количество
[44:54.300 --> 45:01.860]  единичек здесь 2 плюс 2 в квадрате плюс и так далее плюс 2 в степени а а чему равна
[45:01.860 --> 45:13.580]  такая сумма 2 в степени а плюс 1 да получается не два степеня плюс 2 наверное 2 в степени
[45:13.580 --> 45:28.860]  а плюс 2 и минус 2 минус 2 да слушайте похоже вот но геометрическая прогрессия 2 1 плюс 2
[45:28.860 --> 45:38.260]  в квадрате плюс и так далее плюс 2 в степени а плюс 1 вот эта штука равняется 2 в степени
[45:38.260 --> 45:53.700]  а плюс 2 минус 2 вот это и будет стоять тут а потому что здесь у нас два степеня единичек но
[45:53.700 --> 46:01.940]  у нас же две половинки здесь столько и здесь только да вот но вы помните формулы для суммы
[46:01.940 --> 46:13.780]  геометрической прогрессии хорош не да я сам все время забываю поэтому я очень люблю
[46:13.780 --> 46:18.860]  ее заново выводить здесь я не буду конечно ее выводить заново вот просто напомню да что
[46:18.860 --> 46:26.420]  здесь можно добавить вот так вот а вы знаете что такое телескопирование нет телескопирование
[46:26.420 --> 46:33.700]  это сворачивание математического выражения ну когда она допускает за счет такого такой
[46:33.700 --> 46:38.380]  группировки по парной при которой все схлопывается слопывается слопывается друг за другом это как когда
[46:38.380 --> 46:44.900]  вы телескоп складываете звенья телескопа как бы одно на другое тогда налазит ну и телескопическая
[46:44.900 --> 46:49.980]  антенна все то же самое да и в итоге весь телескоп такой длинный он такой раз и в такую штучку
[46:49.980 --> 46:55.700]  сокращается вот здесь вот телескопирование такого выражения как можно устроить добавить
[46:55.700 --> 47:05.220]  сюда 2 в первый и тогда 2 1 плюс 2 1 это будет 2 по второй да 2 плюс 2 4 а 4 плюс 4 это уже 8
[47:05.220 --> 47:12.620]  а 8 плюс 8 это 16 и так далее и вот это вот вот эта спичка она весь этот бигфордов шнур заставляет
[47:12.620 --> 47:19.180]  прогореть и в итоге он бабах и превращается в два степеня плюс два просто ну значит вот
[47:19.180 --> 47:25.340]  исходно этот бигфордов шнур имел длину два степеня плюс два минус два не считая спички
[47:25.340 --> 47:36.100]  которую мы добавили вот так ладненько это можно стереть вот такая у нас получается матрица
[47:36.100 --> 47:44.380]  смотрите какая она не сбалансированная да у нее очень маленькая высота а по ширине
[47:44.380 --> 47:49.780]  она экспоненциально относительно параметра давайте посмотрим как на этой матрице выглядит
[47:49.780 --> 47:59.940]  оптимальное покрытие чему равен опт веса всех строчек считаем единичными то есть вес покрытия
[47:59.940 --> 48:05.980]  это просто мощность покрытия как множество строчек какое оптимальное покрытие этой матрицы
[48:05.980 --> 48:18.860]  предлагаете единичный да да две последние ну как оптимальные вроде одной строкой матрицу не
[48:18.860 --> 48:24.380]  покроешь а двумя последними до первой половины столбцов этой строкой покрывается вторая половина
[48:24.380 --> 48:32.060]  столбцов этой строкой значит у нас получается два и давайте посмотрим как жадно алгоритм будет
[48:32.060 --> 48:40.060]  на ней работать давайте мы посмотрим сколько тут последние две строки сколько у них единичек
[48:40.060 --> 48:47.060]  да вот здесь вот две единички 4 единички 8 единичек бла бла бла 2 в степени а плюс одна единичка а
[48:47.060 --> 48:56.300]  вот здесь у нас сколько единичек 1 плюс 2 плюс 4 и так далее то есть плюс два степень а и того
[48:56.300 --> 49:08.340]  сколько да а плюс один да здесь последняя степень это а значит сумма вплоть до 2 степеня она
[49:08.340 --> 49:19.020]  будет два степеня плюс один потому что в этой строчке до у нас единички стоят только вот в
[49:19.020 --> 49:28.060]  этой половинке вот и ну и в этой строчке все тоже самое два степеня плюс один минус один кто
[49:28.060 --> 49:37.220]  побеждает побеждает с небольшим отрывом побеждает вот этот вот спортсмен да и мы этого
[49:37.820 --> 49:42.940]  вычеркиваем он победила мы его вычеркиваем да ну в общем мы вычеркиваем эту строчку из
[49:42.940 --> 49:49.020]  матрицы как бы мысленно вычеркиваем все столбцы которые ей покрылись что в матрице остается
[49:49.020 --> 49:55.740]  остается та же самая картина но как бы на единичку меньше до параметр все строчки вплоть
[49:55.740 --> 50:02.380]  до той которая содержит 2 степени а единичек а вот в этих строчках в этих строчках остается
[50:02.380 --> 50:07.600]  поскольку единиц, когда мы повычерпим все стропцы, которые покрыты здесь, два
[50:07.600 --> 50:11.920]  степеня минус один. То есть опять-таки каждый из этих строк проиграет вот этой
[50:11.920 --> 50:16.040]  вот строке и так далее, и так далее, и так далее. То есть мы вынуждены будем
[50:16.040 --> 50:21.960]  поперебирать друг за другом все строчки вплоть до самой первой. Мы, конечно, матрицу
[50:21.960 --> 50:26.680]  покроем, но возьмем для этого все строки, кроме последних двух. То есть
[50:26.680 --> 50:36.220]  жадное покрытие имеет размер а плюс один. Вот, но это печальный факт. Ну как
[50:36.220 --> 50:40.860]  печальный? Ну не такой печальный, потому что это вполне соответствует нашей
[50:40.860 --> 50:45.700]  оценке худшего случая о работе жадного алгоритма, правда?
[50:45.700 --> 50:53.860]  Значит, мы понимаем, что если вот это вот n, число столбцов матрицы, то вот это
[50:53.860 --> 51:01.320]  вот величина а плюс один, она имеет порядок. Давайте я напишу тета. Помните
[51:01.320 --> 51:08.060]  обозначение тета? Вот это как бы, о большое, это оценка только сверху, но здесь
[51:08.060 --> 51:11.660]  достаточно бессмысленно писать, что это оценка там сверху. Нам нужно, это же
[51:11.660 --> 51:16.340]  пример, показывающий оценку снизу, что жадный алгоритм, мы можем его заставить
[51:16.340 --> 51:22.560]  работать неоптимально, подав ему неудобную матрицу. Поэтому здесь тета и чего?
[51:22.560 --> 51:28.840]  Логарифм от n, ведь h это логарифмичная величина по отношению к вот такой вот
[51:28.840 --> 51:37.840]  экспоненте. Порядка логарифма n, действительно. Видно, что здесь есть, ну
[51:37.840 --> 51:42.920]  некоторый разрыв в смысле константы, ну в смысле основания логарифма, да, то есть
[51:42.920 --> 51:50.040]  здесь у нас логарифм двоичный на самом деле стоит, если быть точнее, да, и в
[51:50.040 --> 51:54.760]  этой константе чего-то, значит, заглядывать, вот константа в этом тете, а тут у нас
[51:54.760 --> 52:03.200]  логарифм натуральный по основанию 2.7, но что делает? Такой разрыв есть.
[52:03.200 --> 52:09.280]  Принципиально, если p не равно np, организовать решение задачи о покрытии
[52:09.280 --> 52:13.440]  полинамиальным алгоритмом, имеющим константные показатели аппроксимации, мы
[52:13.440 --> 52:26.840]  бы с вами не смогли, то есть это такой известный факт. Замечание. Замечание, если
[52:26.840 --> 52:36.560]  p не равняется np, то для задачи о покрытии,
[52:37.160 --> 52:45.520]  нету алгоритма полинамиального алгоритма
[52:46.520 --> 52:54.400]  с константным показателем аппроксимации.
[52:59.520 --> 53:11.040]  Константным показателем аппроксимации. Так, про это мы с вами сказали, ну и на этом
[53:11.040 --> 53:18.280]  мы заканчиваем работать с задачей о покрытии. Вот, хватит, хватит. И дальше мы с вами
[53:18.280 --> 53:29.040]  переместимся к задаче камевые жора, потом будем рассматривать всякие, значит,
[53:29.040 --> 53:35.640]  схемы приближения полинамиальные. Вот, перед тем, как я все опять не потру и не
[53:35.640 --> 53:40.800]  перейду к задаче камевые жора, вам про нее напоминать и вводить, вот, вы можете
[53:40.800 --> 53:44.800]  мне что-нибудь еще задать, какой-нибудь вопрос.
[53:58.240 --> 54:04.840]  Имеется в виду, вот смотрите, вот когда мы задачу о рюкзаке, например, решали, мы
[54:04.840 --> 54:13.400]  сказали, что можно взять и построить рюкзак, вес которого, стоимость которого не
[54:13.400 --> 54:18.680]  меньше, чем половинка стоимости оптимального рюкзака, да, то есть там у нас тоже мы
[54:18.680 --> 54:22.920]  сравнивали нашу ивристическую стоимость и оптимальную, но между ними
[54:22.920 --> 54:29.840]  разброс был вот этот константный, да, в одну-вторую, а здесь у нас логарифм N это
[54:29.840 --> 54:35.560]  величина, которая может быть сколь угодно большой. Ну, для этого, естественно,
[54:35.560 --> 54:40.200]  матрицу гигантскую придется выдумывать, но тем не менее, вот, все-таки есть
[54:40.200 --> 54:45.800]  качественное различие вот этих двух задач, например, о рюкзаке и о покрытии.
[54:45.800 --> 54:51.120]  Задача о покрытии хуже решается, вот в этом смысле, в смысле приближенного
[54:51.120 --> 54:56.600]  решения, возможности ее приближенной решить.
[54:59.840 --> 55:06.440]  Вообще, чего мы с вами, давайте вы выберите направление, о котором мы дальше пойдем, мы
[55:06.440 --> 55:13.240]  можем дальше пойти рассматривать схемы приближения, то есть такие классные
[55:13.240 --> 55:18.000]  алгоритмы, которые могут с любым, сколько угодно близким к единице, показателям
[55:18.000 --> 55:21.440]  проксимации решить задачу, а можем пойти рассматривать задача Камево-Ижора,
[55:21.440 --> 55:27.240]  такую геометрическую, как точки обойти, значит, по циклу наиболее удобным путем.
[55:27.240 --> 55:34.880]  Вот, вы, Камево-Ижора, ну это наиболее такая громогласная часть населения
[55:34.880 --> 55:45.760]  сказала. Вот, давайте проголосуем, кто за Камево-Ижора? Ага, а кто за схемы приближения?
[55:45.760 --> 56:00.320]  Ага, хорошо, Камево-Ижор, так Камево-Ижор. Значит, задача Камево-Ижора. Я напоминаю, что задача Камево-Ижора
[56:00.320 --> 56:07.240]  или traveling salesperson problem, это задача в выборе гамильтонового цикла в графе наименьшего
[56:07.240 --> 56:13.840]  веса. Помните, что такое гамильтоновый цикл? Это задача в выборе гамильтонового цикла в графе наименьшего
[56:13.840 --> 56:19.200]  веса. Помните, что такое гамильтоновый цикл? Цикл, проходящий через каждую вершину, ровно по одному разу.
[56:19.200 --> 56:31.880]  Выбор кратчайшего гамильтонового цикла.
[56:31.880 --> 57:00.360]  Ну, например, данные точки, и нам требуется их обойти по циклу так, чтобы
[57:00.360 --> 57:08.720]  минимизировать суммарную длину. Ну, например, вот такой маршрут можно предложить. Он, скорее всего,
[57:08.720 --> 57:13.920]  будет действительно оптимальным. Ну, так, смотришь на картинку, кажется, что лучше сложно себе
[57:13.920 --> 57:21.400]  представить. Для любого графа, для произвольного графа гамильтонов цикл, это вот что-то такое же.
[57:21.400 --> 57:28.880]  Это цикл, проходящий по каждой вершине графа без повторений. Все равно, на какой вершине начинать его,
[57:28.880 --> 57:36.840]  построить его очень трудно. Задача, даже определение для заданного графа, есть ли в нем
[57:36.840 --> 57:44.040]  гамильтонов цикл, это импотрудная задача. Задача выбора оптимального гамильтонового цикла в данном
[57:44.040 --> 57:48.400]  графе тоже импотрудная задача. То есть, мы, как всегда, с вами начинаем с того, что это задача
[57:48.400 --> 57:55.120]  трудно решать, а значит обосновываем, что мы в праве решать приближенно. И у задачи ТСП,
[57:55.360 --> 58:02.000]  задача Ками-Воежора, есть три разных варианта, самые общие из которых мы вообще не будем им
[58:02.000 --> 58:09.160]  заниматься, потому что для него сложно предложить сколь угодно реалистичные какие-то схемы
[58:09.160 --> 58:14.440]  приближения. Мы будем с вами рассматривать два вида задач. Это метрическая задача Ками-Воежора.
[58:14.440 --> 58:27.720]  На самом деле, мы на ней как раз сосредоточимся. Это задача на метрическом графе. Задача ТСП
[58:27.720 --> 58:49.920]  на графе с метрической функцией весов. Что это такое? Что такое метрическая функция весов?
[58:49.920 --> 59:00.680]  Знаете, что такое метрика? Да, отлично. А что такое метрика? Какими свойствами она должна
[59:00.680 --> 59:14.040]  обладать, чтобы быть метрикой? Большая нуля, не нравится треугольника, но не обязательно.
[59:14.040 --> 59:22.080]  Симметричная неотрицательная функция, но еще она должна быть равна нулю тогда и только тогда,
[59:22.080 --> 59:28.480]  когда два объекта совпадают. Но самое главное для нас это неравенство треугольника вот здесь.
[59:28.480 --> 59:33.040]  Фактически на все остальное можно забить. Самое нетривиальное в метрике, самое интересное ее
[59:33.040 --> 59:39.240]  свойство, это именно неравенство треугольника. И я его напомню, что когда мы говорим с вами про
[59:39.240 --> 59:46.960]  графы с неравенством треугольника, то это означает, что для любых трех вершин графа A, B, C. Помните,
[59:46.960 --> 59:52.280]  что множество вершин заданного графа G обозначается через V, A, G. Поэтому это я записал.
[59:52.280 --> 01:00:03.880]  Для любых трех вершин A, B, C графа G мы хотим, чтобы вес ребра A, T не превосходил суммы весов
[01:00:03.880 --> 01:00:12.080]  ребер AB и BC. В частности, это означает, что такое ребро должно существовать. То есть,
[01:00:12.080 --> 01:00:18.280]  если существуют ребра AB и BC, то ребро AC тоже существует, и вес его не превосходит вот этого
[01:00:18.280 --> 01:00:23.360]  сила. Из этого следует вообще говоря, что наш граф, если он связанный, то он должен быть полный.
[01:00:23.360 --> 01:00:29.320]  То есть в этом графе мы не задумываемся уже над вопросом, а можно ли пройти напрямик между двумя
[01:00:29.320 --> 01:00:43.120]  вершинами. Всегда можно. То есть считаем граф полным. Полный граф – это такой граф, у которого между
[01:00:43.120 --> 01:00:49.000]  каждой парой вершин есть ребро. Поэтому вопрос о существовании гамильтонового цикла здесь
[01:00:49.000 --> 01:00:56.000]  полностью снимается. Гамильтонов цикл существует всегда, причем их куча. А сколько примерно,
[01:00:56.640 --> 01:01:12.800]  по порядку гамильтоновых циклов у нас в полном графе? Да, все перестановки. Их на самом деле
[01:01:12.800 --> 01:01:20.080]  чуток меньше, как вы понимаете, чем перестановок, потому что одному и тому же циклу, как картинки,
[01:01:20.080 --> 01:01:24.520]  соответствует много перестановок, в зависимости от того, с какой вершины вы начнете его считывать.
[01:01:24.880 --> 01:01:30.180]  И в зависимости от того, в какую сторону вы пойдете считывать его или записывать,
[01:01:30.180 --> 01:01:36.520]  наоборот, начиная с какой-то вершины. То есть точное количество гамильтоновых циклов – это
[01:01:36.520 --> 01:01:46.880]  n факториал делить на 2n. n-1 факториал пополам, но все равно очень много. Факториал – это круче,
[01:01:46.880 --> 01:01:51.280]  чем экспоненты. Это суперэкспоненциальная функция, сверхэкспоненциальная функция.
[01:01:52.280 --> 01:01:57.800]  Ну так и есть. Официальные названия – суперэкспоненциальная функция или
[01:01:57.800 --> 01:02:02.620]  сверхэкспоненциальная функция – это функция, растущая быстрее любой экспоненты с константным
[01:02:02.620 --> 01:02:07.460]  основанием. Субэкспоненциальная функция – функция, растущая медленнее любой экспоненты,
[01:02:07.460 --> 01:02:18.320]  тоже с константным основанием больше единицы. Так, к чему я это все говорил? Забыл. Просто к тому,
[01:02:18.320 --> 01:02:25.880]  что перебором решать эту задачу, наверное, трудно. Вот что я хотел сказать. Ну так вот. Значит,
[01:02:25.880 --> 01:02:31.360]  задача-то существование гамильдонного цикла не стоит больше, но все равно перебором ее не
[01:02:31.360 --> 01:02:37.080]  решишь. Не перебираешь все перестановки, скорее всего, так просто. Но есть еще задача более
[01:02:37.080 --> 01:02:48.840]  специфическая, а именно задача Евклидова за датчиками выезжора. В Евклидовой за датчиками
[01:02:48.840 --> 01:02:54.400]  выезжора вершины графа это прям точки. То есть это вот то, как правило, то, как мы понимаем вот
[01:02:54.400 --> 01:02:59.960]  такие примеры. Когда я на доске рисую какие-то примеры применения алгоритма какого-нибудь для
[01:02:59.960 --> 01:03:04.320]  решения задачек на выезжора, то я же просто точки рисую, чтобы не задумываться, как там веса,
[01:03:04.400 --> 01:03:10.640]  ребер определяются. Просто мы предполагаем, что и раз две точки нарисованы, то между ними всегда
[01:03:10.640 --> 01:03:16.880]  можно провести ребро потенциально, и его вес – это просто длина соответствующего отрезка. То есть мы
[01:03:16.880 --> 01:03:22.600]  буквально ищем коротчайший геометрический маршрут между точками на плоскости. Вот это Евклидова
[01:03:22.600 --> 01:03:28.880]  за датчиками выезжора. Ну а формально можно ее так определить, что вершины графа – это просто
[01:03:28.880 --> 01:03:37.800]  точки под множество Евклидового пространства РД, ну и вес ребра – это Евклидовое расстояние. Вес
[01:03:37.800 --> 01:03:51.040]  ребра АВ – это Евклидовое расстояние между точками. Длина отрезка АВ. По понятным причинам
[01:03:51.040 --> 01:03:57.080]  это очень удобно делать до смотрения примеров, когда не надо думать, как определять веса там,
[01:03:57.680 --> 01:04:03.680]  и так далее. Рисуешь картинку, и все сразу понятно, вроде интуитивно. Мы с вами рассмотрим
[01:04:03.680 --> 01:04:12.640]  несколько алгоритмов для решения за датчиками выезжора, и начнем мы вот с какого. Я его сейчас
[01:04:12.640 --> 01:04:18.560]  просто опишу. Вообще опишу два алгоритма сразу на затравку, чтобы в следующий раз было с чего
[01:04:18.560 --> 01:04:25.520]  начинать. Значит, они очень близкие по духу. Один называется ближайший сосед,
[01:04:25.520 --> 01:04:46.000]  другой называется кратчайшие вставки. Я не буду записывать их словами, ладно,
[01:04:46.000 --> 01:04:53.680]  я просто порисую немножко. Нарисую какой-то пример, и мы с вами поймем, как на этом примере
[01:04:53.680 --> 01:05:02.080]  работал бы алгоритм. Ближайший сосед начинает из произвольной вершины графа. Идет, знаете,
[01:05:02.080 --> 01:05:12.920]  куда? Не догадаетесь. Ближайшего соседа. Идет в ближайшую соседнюю вершину, которая еще не
[01:05:12.920 --> 01:05:18.880]  была посещена, естественно, потому что повторяться мы не имеем права. Отсюда он пойдет сюда,
[01:05:18.880 --> 01:05:26.560]  отсюда он пойдет, ну, скорее всего, сюда, отсюда он пойдет сюда, сюда и так далее.
[01:05:33.520 --> 01:05:39.680]  Давайте я, ну ладно, нет, не буду ничего пытаться переделать. Получается неплохо,
[01:05:40.160 --> 01:05:55.880]  не так уж худо получается. Кратчайшие вставки. Что делают кратчайшие вставки? Алгоритм кратчайших
[01:05:55.880 --> 01:06:03.760]  вставок начинает с того, что выбирает самую близкую пару вершин друг к другу, то есть самое
[01:06:03.760 --> 01:06:10.080]  короткое ребро в графе. Ну, давайте я вот эти вот две вершины нарисую чуть подальше друг от друга,
[01:06:10.080 --> 01:06:15.280]  чтобы у меня самое короткое ребро было вот именно это. Сначала выбираем самое короткое ребро,
[01:06:15.280 --> 01:06:22.080]  потом выбираем вершину, которая ближайшая к этому ребру. Ближайшая к этому ребру это значит
[01:06:22.080 --> 01:06:28.840]  ближайшая к самому близкому концу этого ребра, какая-то вершина. Ну, вот это вот. Да, вот к ней
[01:06:28.840 --> 01:06:32.920]  ближайший конец ребра, он находится на таком расстоянии, и, видимо, это расстояние лучше,
[01:06:32.920 --> 01:06:40.080]  чем у любой другой вершины графа. Вот. Эта вершина добавляется третьей. Третьим будешь,
[01:06:40.080 --> 01:06:49.680]  спрашивают двое ее. Буду, говорит. Ну, я примеры демонстрирую для Евклидова случая, но на самом
[01:06:49.680 --> 01:06:57.480]  деле это для произвольного метрического графа вполне работает. И дальше мы что делаем? Мы находим
[01:06:57.480 --> 01:07:05.000]  ближайшую вершину к вот этому циклу. Какую какая-то вершина? Наверное, вот это. И после того,
[01:07:05.000 --> 01:07:09.320]  как мы зафиксировали эту вершину, мы ее на цикл добавляем. А как мы ее добавляем? Как добавить
[01:07:09.320 --> 01:07:20.680]  вершину на цикл? Раззамкнуть, да. Раззамкнуть-самкнуть. Раз и два, да. Вот. То есть мы удаляем из цикла два
[01:07:20.680 --> 01:07:27.400]  рябра. Одно рябро добавляем два рябра. Дальше ближайшую вершину находим к этому циклу. Какая это
[01:07:27.400 --> 01:07:37.160]  будет вершина? Вот. А самое лучшее. То есть мы... То есть мы... Не, но серьезно. Серьезно. То есть мы
[01:07:37.160 --> 01:07:44.720]  добавляем рябро, которое минимизирует добавленную стоимость. Вот. По всем возможным ребрам, которые из
[01:07:44.720 --> 01:07:51.000]  цикла можно было удалить, вот, минимизируем. Но уже после того, как мы зафиксировали вершину... То
[01:07:51.000 --> 01:07:56.560]  есть важно, что сначала фиксируется, какую вершину мы добавляем, и потом мы уже выбираем наилучший,
[01:07:56.560 --> 01:08:01.920]  наиболее оптимальный способ добавить эту вершину на цикл за счет удаления одного рябра и вставки этой
[01:08:01.920 --> 01:08:09.840]  вершины вместо этого рябра. Вот. Следующая вершина, какую мы вставим? Наверное, вот эту, да. Ну,
[01:08:09.840 --> 01:08:15.040]  наверное, скорее всего, вот так. Следующая вершина, скорее всего, вот эта будет.
[01:08:15.040 --> 01:08:25.760]  Плохое? Что значит плохое? Но рябро удаляется. Наилучшее, какое вы можете удалить? Ну,
[01:08:25.760 --> 01:08:33.600]  может быть... Не, ну, может, какое-то другое будет. Не, ну, я мог бы написать плохую картинку. В любом
[01:08:33.600 --> 01:08:38.920]  случае, этот алгоритм не работает оптимально. То есть у нас задача камевые жоры на метрических
[01:08:38.920 --> 01:08:46.000]  графах и эвклидовая задача камевые жоры, она тоже импотрудная. Вот. То есть здесь все равно все
[01:08:46.000 --> 01:08:53.400]  плохо, не переживайте. Вот. Дальше, ну, вот эту вершину добавляем. Ну, непонятно. Вот так вот или
[01:08:53.400 --> 01:08:58.760]  вот так вот. Ну, давайте представим, что вот так, да. И в конце добавляем вот эту вот вершину. Ну,
[01:08:58.760 --> 01:09:07.880]  допустим, вот так. Опс. Получается вот такой цикл тоже, да? И вот мы с вами в следующий раз
[01:09:07.880 --> 01:09:13.000]  посмотрим на показатели аппроксимации вот этих вот двух алгоритмов. Оба жадные такие по духу,
[01:09:13.000 --> 01:09:17.280]  да? Каждый раз чего-то делаем локально оптимально. Либо в ближайшего соседа идем,
[01:09:17.280 --> 01:09:23.080]  либо в ближайшую вершину к текущему циклу берем и ее добавляем, да? Но показатели аппроксимации
[01:09:23.080 --> 01:09:27.520]  оказываются очень разными в худшем случае у этих двух алгоритмов. У одного константный,
[01:09:27.520 --> 01:09:33.960]  двойка, а у другого логарифмичный по размеру графа. Угадайся, у какого какой. Но я вам не буду
[01:09:33.960 --> 01:09:38.640]  говорить час-ответ для следующего раза. Вот до следующего раза подумайте, ну не подумайте,
[01:09:38.640 --> 01:09:42.840]  просто погадайте, какой из алгоритмов лучше. Спасибо.

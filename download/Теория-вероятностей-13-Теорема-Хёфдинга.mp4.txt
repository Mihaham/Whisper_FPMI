[00:00.000 --> 00:17.360]  Так, будем начинать, да, Колея? Совсем у нас немного, ну ладно. Значит, поздравляю всех с
[00:17.360 --> 00:24.840]  прошедшим праздником, Днём Победы. Символично, что 79 дней назад победоносно завершилась
[00:24.840 --> 00:31.320]  Великодейственная война, и мы сегодня с вами победоносно, надеюсь, завершаем курс теории
[00:31.320 --> 00:44.520]  вероятности. Напомню вкратце, на чем остановились. В прошлый раз мы неким специальным образом ввели
[00:44.520 --> 00:51.320]  вот такую нехитрую с виду функцию, которую назвали имперической функцией распределения. То есть
[00:51.320 --> 00:59.480]  алгоритм такой, это важно как бы, ну, помнить, понимать. Сначала мы омеги поставили в соответствие
[00:59.480 --> 01:04.880]  случайную последовательность, а потом на этой случайной последовательности при фиксированном
[01:04.880 --> 01:12.760]  n, при любом n, ну, построили вот такую функцию. То есть мы к элементарному исходу поставили
[01:12.760 --> 01:21.240]  соответствие функцию. Ну, кстати, если мы ставим числу, точнее говоря, элементарному исходу в
[01:21.240 --> 01:26.360]  соответствие числу мы это называем случайной величиной. Если ставим вектор, называем случайным
[01:26.360 --> 01:31.120]  вектором. Если ставим последовательность, называем случайной последовательностью. А если мы
[01:31.120 --> 01:37.480]  элементарному исходу ставим соответствие в функцию, то это новый для вас объект называется
[01:37.480 --> 01:43.800]  случайным процессом, который вы будете изучать в следующем семестре. Поэтому в
[01:43.800 --> 01:47.580]  некотором смысле, но не в некотором, а, собственно, по определению, империческая
[01:47.580 --> 01:54.000]  функция распределения – это случайный процесс, ну, специфичный, конечно. Вот. И мы с вами
[01:54.000 --> 02:00.080]  сформулировали и начали доказывать теорему Гливенко, которая выглядит следующим
[02:00.080 --> 02:05.880]  образом. В равномерной метрике для, мы оговорились, непрерывных функций
[02:05.880 --> 02:10.660]  распределения, в равномерной метрике империческая функция сходится к так
[02:10.660 --> 02:15.120]  называемой имперической, она называется почти-наверное, то есть с
[02:15.120 --> 02:23.400]  вероятностью единицы. Значит, и еще мы успели в прошлый раз понять, что вот этот
[02:23.400 --> 02:31.280]  супремум, ну, во-первых, это случайная величина. Почему это случайная величина?
[02:31.280 --> 02:36.320]  Потому что вот этот супремум, решение задачи, так сказать, нахождения супремума
[02:36.320 --> 02:42.160]  эквелинтно решению задачи нахождения того же супремума только навсюду плотно
[02:42.160 --> 02:49.120]  множестве рациональных точек. Ну, и там вкратце повторюсь, значит, поскольку любые
[02:49.120 --> 02:55.320]  функции распределения, в том числе и fn и fx в плюс и минус бесконечности равны нулю
[02:55.320 --> 03:00.160]  единицы, соответственно, то точка супремума, ну, грубо говоря, где-то посередине.
[03:00.160 --> 03:04.920]  Поскольку у нас есть, по крайней мере, непрерывность слева, то мы к точке
[03:04.920 --> 03:08.720]  супремума можем построить сходящуюся последовательность. И уж тем более, не
[03:08.720 --> 03:13.480]  тем более, и понятно, что мы ее можем построить из рациональных чисел всю
[03:13.480 --> 03:18.360]  доплотную множество. Поэтому вот этот супремум, это вот этот вот супремум. А
[03:18.360 --> 03:23.120]  здесь это уже супремум по счетному множеству, то есть dn случайно
[03:23.120 --> 03:30.720]  не влечена. Вот на этом мы с вами остановились. Ну, дальше, в общем, не
[03:30.720 --> 03:39.240]  сложно. Давайте вот такое введем множество или большое от k. Это в
[03:39.240 --> 03:49.800]  пространстве элементарных исходов. Туда входят такие омега, что fn от xk
[03:49.800 --> 04:07.300]  сходится к fx от xk. xk у нас это рациональные числа. Мы в прошлый раз тоже об этом
[04:07.300 --> 04:12.400]  сказали. На самом деле, fn это среднеаррифметическая одинаково
[04:12.400 --> 04:18.200]  распределенных случайных величин. Поэтому по теореме, второй теореме
[04:18.200 --> 04:23.560]  Колмогорова, она для каждого x сходится почти, наверное, к своему
[04:23.560 --> 04:29.140]  отожиданию. А мот ожидания fn от x, мы с вами это тоже получили. Это, на самом
[04:29.140 --> 04:36.960]  деле, fx от x. То есть, вот эта сходимость имеет место, почти, наверное, по второй
[04:36.960 --> 04:46.320]  теореме Колмогорова. То есть, это означает, что f от lkt равно единице для
[04:46.320 --> 04:59.920]  любого xk, принадлежащей q. Следующий шаг. Введем множество l, которое
[04:59.920 --> 05:11.000]  есть пересечение всех lкт. xкт принадлежит q. Счетное пересечение множества
[05:11.000 --> 05:17.120]  единичной меры. Какова мера этого множества? Счетное пересечение множества
[05:17.120 --> 05:27.440]  единичной меры. Единица. Поэтому мы получаем, что вероятность l, которая на
[05:27.440 --> 05:37.600]  самом деле есть вероятность пересечения омега таких, что fn от xкт
[05:37.600 --> 05:54.360]  сходится к fx от xкт, равно единице. А что значит это? Какие сюда
[05:54.360 --> 06:01.120]  омега входят? Такие, на которых на всех рациональных числах имеет место вот такая
[06:01.120 --> 06:07.640]  сходимость. На счетном множестве. Ну и давайте возьмем какое-нибудь омега из
[06:07.640 --> 06:13.440]  вот этого множества z. Конкретно для этого омега мы получим последовательность уже
[06:13.440 --> 06:20.080]  не случайных возрастающих функций ограниченных, которые сходятся навсюду
[06:20.080 --> 06:26.240]  плотно множестве к некой функции fx от x. Но отсюда следует, что конечно и для этого
[06:26.240 --> 06:39.520]  выбранного омега supremum по x принадлежащих xk принадлежащих q fn от xk
[06:39.520 --> 06:50.000]  минус fx от xk тоже стремится к нулю. Принадлежащих к бесконечности. Это вот
[06:50.000 --> 06:57.280]  для конкретного омега из z. Но омег здесь в z вероятностной меры единица.
[06:57.280 --> 07:08.800]  Поэтому вот это равно единице, что и означает вот это утверждение. А следовательно
[07:08.800 --> 07:20.000]  и это. Вот собственно доказательства теоремы Гливенко. Здесь нам от некой
[07:20.000 --> 07:26.560]  мороки нас избавило то, что мы считаем функцию fx от x непрерывную. Собственно это и есть то
[07:26.560 --> 07:34.720]  утверждение, как я в прошлый раз говорил, которое доказал Гливенко. Результат Кантелли в общем
[07:34.720 --> 07:41.000]  относится к функциям распределения произвольного вида. Идеологически доказательства то же самое,
[07:41.000 --> 07:48.400]  но только надо повозиться вот в точках разрыва. То есть с идеологической точки зрения это обобщение
[07:48.400 --> 07:57.000]  ну как бы такое естественное вполне. Вот и если говорить, что fx от x произвольная функция
[07:57.000 --> 08:04.440]  распределения, тогда этот результат называется теоремой Гливенко-Кантелли. Предысторию вопроса
[08:04.720 --> 08:11.880]  усиления приоритетов я на прошлой лекции сказал. Вот. Ну оказывается, что это не все свойства вот
[08:11.880 --> 08:20.560]  этой, еще раз повторюсь, очень простенькой на вид функции. Давайте еще одно полезное свойство
[08:20.560 --> 08:39.720]  этой функции найдем. Я ее тут еще раз выпишу. Dn вот это вот такая случайная величина. Ну собственно,
[08:39.720 --> 08:50.000]  кратко говоря, Dn сходится к нулю почти на верное. Вот что мы знаем. Но хотелось бы понять, что-то
[08:50.320 --> 08:57.440]  о распределении этой случайной величины. Смотрите, вот индекс n здесь есть, ну потому что понятно,
[08:57.440 --> 09:03.240]  что от n зависит вот этот супремум. А вот индекса, связанного со случайной величиной,
[09:03.240 --> 09:10.520]  которая относится, здесь в обозначении нет. И это не случайно. Оказывается, что распределение
[09:10.520 --> 09:19.040]  Dn не зависит от случайной величины x. То есть является универсальным. И именно это в дальнейшем в курсе
[09:19.040 --> 09:25.840]  статистики позволяет активно использовать вот эту случайную величину в задачи так называемой
[09:25.840 --> 09:32.240]  проверки гипотез. В статистике этот критерий называется критерием Колмогорова или Колмогорова
[09:32.240 --> 09:41.280]  Смирнова. Ну немножко забегаю вперед, так сказать. Но еще раз просто скажу, что именно тот, ну вообще-то
[09:41.280 --> 09:47.720]  говоря, удивительный факт. С чего бы это вдруг? Тот факт, что распределение Dn не зависит от x,
[09:47.720 --> 09:56.520]  ну в общем вызывает некое удивление с моей точки зрения. Значит, мы давайте этот факт докажем. Но
[09:56.520 --> 10:09.960]  для довольно широкого класса функции распределения, у которых существует обратное. Это небольшое
[10:09.960 --> 10:17.600]  ограничение, потому что в принципе любую функцию распределения мы можем построить сходящуюся к ней
[10:17.600 --> 10:23.640]  по распределению последовательность, каждая из которых будет обладать обратной. Ну чтобы обратная
[10:23.640 --> 10:32.080]  существовала, она и так не убывающая, просто надо, чтобы она константе, да, нигде не равнялась. Вот,
[10:32.080 --> 10:38.280]  значит, то есть это не сильно большое ограничение, которое не принципиально, еще раз повторюсь,
[10:38.280 --> 10:44.560]  построив сходящуюся последность из функции такого класса, мы придем как бы к общему результату.
[10:44.560 --> 10:55.520]  Значит, но перед тем, как доказать это утверждение, обращу внимание на следующий факт. Если он вам знаком,
[10:55.520 --> 11:02.480]  вы мне скажите, я не буду тратить время. Если нет, то значит его докажу. Скажите, пожалуйста, вот если
[11:02.480 --> 11:09.000]  я возьму, ну пусть вот такую функцию с обратной, у которой существует обратное, и функцию распределения
[11:09.000 --> 11:20.040]  подставлю саму случайную величину, что я получу в результате? Ну, думал, может быть, как-то на
[11:20.040 --> 11:27.480]  семинарах касались. Тогда давайте, это, кстати, тоже на вид очень непростой, но крайне важный факт.
[11:27.480 --> 11:33.240]  Значит, ну давайте, это же какая-то случайная величина получилась, функция Борелевская,
[11:33.240 --> 11:38.360]  подставили случайную величину, получилась случайная величина. Чтобы найти функцию распределения,
[11:38.360 --> 11:50.240]  надо вот такую вероятность посчитать. f кси от кси меньше х. Ну, кстати, х теперь уже от 0,1,
[11:50.240 --> 12:00.560]  да? 0,1. Ну, функция распределения сама от 0,1. Так, ну чего, есть обратное, поэтому это вероятность того,
[12:00.560 --> 12:14.320]  что кси меньше f-1 кси от х. А вероятность того, что случайная величина меньше какого-то числа,
[12:14.320 --> 12:24.720]  это же функция распределения по определению, то есть f кси, взятая в точке f-1 кси от х, то есть
[12:24.720 --> 12:33.800]  в точности х. Ну, вот это вот функция распределения вот этой случайной величины. А что это за случайная
[12:33.800 --> 12:46.480]  величина, функция распределения которой равна х от нуля до единицы? Это равномерная на 0,1 случайная
[12:46.480 --> 12:53.240]  величина. То есть мы получаем вот такой, ну, удивительный факт. Если функцию распределения,
[12:53.360 --> 12:59.680]  ну, вот с этим ограничением небольшим подставить саму случайную величину, то мы получим равномерную
[12:59.680 --> 13:08.600]  на 0,1 случайную величину. На 0,1 случайную величину. И, собственно, вот этот вот замечательный факт
[13:08.600 --> 13:16.720]  позволит нам с этой dn, что называется, разобраться. Так, ну, вот давайте теперь чуть
[13:16.720 --> 13:26.240]  поподробнее это распишем dn. Значит, supremum по х. Вместо f-n от х напишу ее определение.
[13:26.240 --> 13:37.840]  Ксикатая мы помним все независимо одинаково распределенные и имеющие функцию распределения
[13:37.840 --> 14:05.000]  f кси. Вот так. И давайте сделаем замену вот такую. х равно f в минус 1 от u, где u уже будет
[14:05.000 --> 14:14.920]  естественно от 0 до 1. Тогда мы получим, что это supremum. Ну, теперь уже по u будет supremum.
[14:14.920 --> 14:30.840]  Пока 1n сумма k от 1 до n. А вот к этому условию применим монотонное преобразование. f кси от
[14:30.840 --> 15:00.800]  х меньше f кси от х. Минус f кси, а вместо х f-1 от u равно supremum по u. Смотрите,
[15:00.800 --> 15:08.520]  f кси от кси это равномерная на 0.1. Мы только что с вами показали. А f кси от х с учетом того,
[15:08.520 --> 15:22.000]  что х равно f-1 у, это получится такое выражение. И вот так напишу u ка т. Это равномерная на 0.1
[15:22.000 --> 15:32.960]  случайная величина. Меньше u. Вот замену произвел. Минус, но это u. Ну и смотрим внимательно,
[15:32.960 --> 15:46.320]  что такое получилось. По определению. Это империческая функция распределения равномерная
[15:46.320 --> 15:52.560]  на 0.1 случайной величины. Вот это империческая функция распределения равномерная на 0.1 случайной
[15:52.560 --> 16:01.960]  величины. А вот это значение функции распределения. То есть на самом деле это мы получили ровно вот такое
[16:01.960 --> 16:10.800]  выражение только в качестве кси равномерная на 0.1. Таким образом, какая бы ни была бы случайная
[16:10.800 --> 16:19.680]  величина кси, случайная величина ДН представляет из себя вот это уклонение в равномерной метрике
[16:19.680 --> 16:27.760]  имперической функции распределения, случайных величин, имеющих равномерное 0.1 распределение,
[16:27.760 --> 16:35.080]  от гипотетической функции, которая в данном случае равна u для равномерного 0.1 распределения. Вот такой
[16:35.080 --> 16:45.640]  замечательный факт. Вот такой замечательный факт. С другой стороны, ну что нам теория Могливенко
[16:45.640 --> 16:53.800]  гласит? Что вот эта ДН сходится почти, наверное, к нулю. Поэтому какое бы ни было распределение
[16:53.800 --> 17:12.600]  этой ДН, ее предельное распределение выглядит вот так. 0. Вот это, так сказать, предел, напишу, f ДН от x.
[17:12.600 --> 17:25.360]  Предел по распределению. Поэтому о распределении асимпатическим случайной величины ДН от нам
[17:25.360 --> 17:32.160]  ничего не говорит, хотя при каждых конкретных Н, пожалуйста, вот она есть, ее можно там посчитать,
[17:32.160 --> 17:41.120]  и в целом можно даже для каждого конкретного Н это распределение построить. Но для того, чтобы
[17:41.200 --> 17:48.080]  результат был более интересен, нужно эту случайную величину ДН промасштабировать соответствующим
[17:48.080 --> 17:57.160]  образом или, как модно сейчас говорить, проскалировать. Оказывается, что если взять корень из Н ДН,
[17:57.160 --> 18:08.000]  то эта уже случайная величина сходится по распределению к некой довольно сложной случайной
[18:08.000 --> 18:14.920]  величине, обозначаемой К Большое, это случайная величина Калмогорова называется, поскольку
[18:14.920 --> 18:21.120]  распределение корень из Н ДН было получено Калмогоровым. Не представляется в виде аналитической
[18:21.120 --> 18:31.080]  функции, представляет из себя довольно хитрый ряд. Но, тем не менее, значит, если вот ДН просто
[18:32.040 --> 18:38.920]  неинтересно ее распределение самой ДН, потому что она все равно сходится к нулю по распределению,
[18:38.920 --> 18:44.960]  теорема Гливенко нам об этом говорит. Но вот если ее вот таким образом отмасштабировать,
[18:44.960 --> 18:51.240]  корень из Н ДН, то получится, так сказать, случайная величина Калмогорова, которая, собственно,
[18:51.240 --> 18:57.520]  и лежит в основе большого количества критериев проверки гипотез. Эта величина затабулирована,
[18:57.520 --> 19:05.240]  в общем, такая важная случайная величина. Вот такие замечательные свойства у этой ну совсем
[19:05.240 --> 19:11.720]  простой на вид функции, имперической функции распределения. И вы с ней еще там главным
[19:11.720 --> 19:20.840]  образом в математической статистике столкнетесь неоднократно. А мы, значит, закончим
[19:20.840 --> 19:31.840]  собственности аремы Гливенко и со свойствами имперической функции. Значит, сейчас мы перейдем,
[19:31.840 --> 19:40.600]  в общем, к последней теме, которая вот с какой проблемой связана. Ну, мы с вами получили достаточно
[19:40.600 --> 19:47.480]  много результатов сходимости, по вероятности, почти наверное, выполнения закона больших чисел,
[19:47.480 --> 19:54.520]  усиленного закона больших чисел и так далее. Но вы знаете, как я уже в общем говорил, как и при
[19:54.520 --> 20:03.560]  использовании каких-то таксотитерационных алгоритмов, нам недостаточно факта сходимости к решению. Ну,
[20:03.560 --> 20:08.880]  точнее говоря, это тоже полезный и важный факт, но с практической точки зрения нам нужно что-то
[20:08.880 --> 20:14.960]  знать о скорости сходимости. Иначе мы не понимаем, где мы находимся. Мы уже в пределе, или если мы
[20:14.960 --> 20:21.000]  N еще увеличим, у нас опять все разойдется и потом где-то сойдется. Ну, кстати, в вычислительной
[20:21.000 --> 20:26.960]  математике такие эффекты бывают, не знаю, знаете. Ну, может быть, даже на эту тему там даже есть
[20:26.960 --> 20:35.280]  один совсем канонический случай. Ну ладно. Вот, связанный, кстати, с именем Академикой Белоцерковского.
[20:35.280 --> 20:46.080]  Вот, значит, поэтому мы сейчас попытаемся чего-то сказать про скорости сходимости тех объектов,
[20:46.080 --> 21:01.280]  средних там, нормальных 8-тики, которые мы с вами изучали. И напомню, что с какими-то там первичными,
[21:01.280 --> 21:05.280]  если можно так выразиться результатами, мы в этой области сталкиваемся.
[21:05.280 --> 21:30.280]  Значит, напомню вам неравенство Чебышева. Если у нас есть последовательность
[21:30.280 --> 21:39.880]  независимых одинаково распределенных величин, то вероятность того, что кси N средняя отклонится от
[21:39.880 --> 21:54.120]  своего от ожиданий на величину больше, чем ε, меньше равна чего? Дисперсия. Кси делить на ε квадрат,
[21:54.120 --> 22:03.120]  но только еще умножить на N. Средняя. Ну, вообще говоря, вот, пожалуйста, пример скорости сходимости.
[22:03.120 --> 22:13.320]  То есть сходимость по вероятности среднего значения к своему от ожиданию имеет порядок 1 на N.
[22:13.320 --> 22:38.840]  И еще мы с вами, когда изучали асимпатическую нормальность, рассматривали неравенство Берри Эссона,
[22:38.840 --> 22:50.200]  только там уже невероятность. Значит, если там выполнено условие того, что некая правильным
[22:50.200 --> 22:55.240]  образом отнормированная случайная величина сходится к стандартному нормальному закону,
[22:55.240 --> 23:03.160]  например, стремясь к бесконечности, то функция распределения... Ну, собственно,
[23:03.160 --> 23:09.480]  сходимость по распределению означает, что поточечно функция распределения сходится к функции
[23:09.480 --> 23:16.160]  распределения нормального закона. Ну, как бы поточечно, на несчетном множестве, как бы не очень,
[23:16.160 --> 23:21.640]  так сказать, понятно. Поэтому на этот счет есть неравенство Берри Эссона, которое говорит нам,
[23:21.640 --> 23:28.640]  что в такой знакомом стандартное обозначение, стандартная нормальная случайная... функция
[23:28.640 --> 23:35.240]  распределения стандартной нормальной случайной величины, греческая в большое. Вот эта вот величина
[23:35.240 --> 23:44.480]  будет меньше математического ожидания, кси центрирована в третьей степени, делить на корень из n
[23:44.480 --> 23:52.760]  сигма в кубе. Сигма с средней квадратичной отклонения. Ну, вот еще как бы один пример оценки
[23:52.760 --> 23:59.520]  скорости сходимости. Это мы вот с вами уже как бы сталкивались. Вот. Но сейчас мы двинемся дальше
[23:59.520 --> 24:09.280]  в результаты, которые, ну, во-первых, имеют существенно более высокие скорости сходимости и,
[24:09.280 --> 24:17.080]  как следствие, ну, более, более, так сказать, с практической точки зрения более важны. Вот.
[24:17.080 --> 24:22.800]  Чтобы к этому перейти, я вам, что называется для затравки,
[24:22.800 --> 24:39.640]  напишу такое неравенство. Значит, dn это вот оно,
[24:39.640 --> 24:53.680]  то есть, уклонение равномерной метрики, вероятность того, что dn будет больше или равно
[24:53.680 --> 25:12.000]  epsilon, меньше или равно 2 на e в степени минус 2n epsilon квадрат. Вот такое неравенство имеет
[25:12.000 --> 25:21.920]  место. То есть, уклонение вот в равномерной метрике имперической функции от гипотетической,
[25:21.920 --> 25:29.920]  это случайно и влечена, которая, естественно, по вероятности в том числе сходится, так сказать,
[25:29.920 --> 25:36.240]  к нулю и имеет место вот такая скорость сходимости. Обратите внимание, что из этого
[25:36.240 --> 25:44.400]  результата следует теорема Гливенко, потому что вот это ничто иное, как быстрая сходимость по
[25:44.400 --> 25:51.680]  вероятности. То есть, вот этот ряд по n сходится, а это достаточно условие того, что dn сходится
[25:51.680 --> 26:02.240]  к нулю почти наверно. Данное неравенство называется неравенство дворецкого Кифера Вольфовица,
[26:02.240 --> 26:10.400]  такое тройное у него название, тройное название. И появилось оно существенно позже теорема Гливенко,
[26:10.400 --> 26:18.080]  естественно, типа лет 30 назад, ну не знаю, 30-40 лет назад. И здесь мы опять видим вот эту штуку,
[26:18.080 --> 26:25.520]  видите, n эпсилон квадрат. Вот она нас будет преследовать в целом по понятным причинам,
[26:25.520 --> 26:39.480]  но давайте, что называется, разберемся по порядку. Так, приведу еще одно неравенство такого же
[26:39.480 --> 26:54.560]  типа. Вот здесь его запишу. Значит, пусть у нас есть серия случайных величин xнкта, они независимые,
[26:54.560 --> 27:08.880]  и с вероятностью единицы все ограничены. То есть, вероятность того, что xнкта по модулю меньше
[27:08.880 --> 27:22.360]  или равно c, равно единице для любого k от единицы до n. Если бы c была маленькая, то мы бы сказали бы,
[27:22.360 --> 27:30.360]  что это как пренебрежима малый в наших терминах. Вот, но здесь она как раз немаленькая. И, значит,
[27:30.360 --> 27:39.880]  стандартное у нас обозначение xн, сумма xнкта крат единицы до n, то есть сумма случайных величин в
[27:39.880 --> 27:44.920]  серии. Ну и здесь это как, видите, как в тензорном учителении. Если индекса нет, значит, по нему
[27:44.920 --> 27:55.040]  просуммировали. Вот. И тогда имеет место вот такое неравенство. xн, только центрированное,
[27:55.040 --> 28:12.760]  больше или равно епсилон, меньше или равно экспонента, минус епсилон квадрат, делить на два дисперсия
[28:12.760 --> 28:29.000]  xн и плюс такое неочевидное слагаемое, епсилон c делить на три. Епсилон c делить на три. Значит,
[28:29.000 --> 28:37.840]  вот это неравенство называется неравенством Бельштейна. И привел я его в связи с тем,
[28:37.840 --> 28:44.760]  что это, пожалуй, первое неравенство из череды вот этих вот неравенств, я не знаю, вот такого
[28:44.760 --> 28:51.320]  типа, если можно так выразиться, с экспоненциальной исходимостью. Значит, это неравенство где-то
[28:51.320 --> 28:59.800]  тридцатые годы, ну, грубо говоря, сто лет, там, может, чуть меньше сто лет, когда эти неравенства
[28:59.800 --> 29:08.360]  были получены Бельштейном. Значит, Бельштейн Сергей Натанович, академик академии наук СССР,
[29:08.360 --> 29:19.720]  ученик Гильберта, кстати, ученик Гильберта. Вот, значит, потом подобные неравенства лет через
[29:19.720 --> 29:25.600]  тридцать-пятьдесят посыпались как из рога изобилия, можно сказать, но все-таки первые, по-видимому,
[29:25.600 --> 29:33.960]  было неравенство Бельштейна. Ну, кстати, если считать, что все хн-каты одинаково распределены,
[29:33.960 --> 29:42.560]  тогда здесь просто n умножить на дисперсию, если здесь вместо хн среднего поставить единица на n,
[29:42.560 --> 29:48.280]  то есть отклонение среднего значения от своего отожидания, то здесь появится тот же самый
[29:48.280 --> 29:58.200]  множитель epsilon квадрат n. Еще раз повторю, который нас будет преследовать, и в целом так
[29:58.200 --> 30:06.920]  физически понятно почему. Вот, значит, ну а теперь давайте чего-то и сами попробуем получить,
[30:06.920 --> 30:14.920]  не все как бы ссылаться на классиков. Так, ну, я тогда вот это напоминание просто было сотру.
[30:18.280 --> 30:42.760]  Так, давайте рассмотрим вот некую случайную величину Xi, про которую мы знаем следующее,
[30:43.320 --> 30:55.440]  что она принадлежит минус 1,1 с вероятностью единицы, и второе ограничение, которое мы наложим,
[30:55.440 --> 31:08.640]  мотожидание Xi равно нулю. Вот, и давайте получим относительно такой случайной величины,
[31:08.800 --> 31:22.520]  там некое неравенство, достаточно универсальное. Так, давайте рассмотрим математическое ожидание
[31:22.520 --> 31:36.040]  е в степени t Xi, t больше нуля будем считать. А, извините, еще до этого еще одно неравенство,
[31:36.040 --> 31:47.120]  прошу прощения, которым мы будем пользоваться. Значит, давайте рассмотрим такую несложную цепочку
[31:47.120 --> 31:54.880]  равенств. Вероятность того, что некая случайная величина Xi больше или равна
[31:54.880 --> 32:01.640]  Эпсилон, равна вероятности того, что Xi минус Эпсилон больше или равно нулю,
[32:01.640 --> 32:12.640]  равно для любого t больше нуля вероятности того, что t умножить на Xi минус Эпсилон
[32:12.640 --> 32:24.520]  больше равно нулю, равно вероятности того, что е в степени t Xi минус Эпсилон больше или равно
[32:24.520 --> 32:32.320]  единице. Ну а теперь смотрите, вот эта величина уже не отрицательная, и к ней можно применить
[32:32.320 --> 32:41.440]  неравенство Маркова и написать меньше или равно математического ожидания е в степени t Xi минус
[32:41.440 --> 32:53.240]  Эпсилон или так написать е в степени минус Эпсилон t на математическое ожидание е в степени t Xi.
[32:53.240 --> 33:04.840]  Но поскольку мы для любого t это делали, то можем написать и чуть по-другому. Вот так. Вероятность
[33:04.840 --> 33:14.440]  того, что Xi больше равна Эпсилон меньше или равно минимуму по t больше нуля е в степени минус
[33:14.440 --> 33:27.760]  Эпсилон t на математическое ожидание е в степени t Xi. Вот это неравенство, ну как бы может быть одна из
[33:27.760 --> 33:36.320]  форм, ну по сути вот это неравенство, называется неравенством Чернова или границы Чернова. Само
[33:36.320 --> 33:44.360]  по себе может быть ничего не дает, но как мы увидим, по крайней мере на одном несложном примере,
[33:44.360 --> 33:54.600]  это удобный инструмент для оценки уклонений. Значит Чернов это американский математик,
[33:54.600 --> 34:01.960]  значит у него не очень американская фамилия по простой причине. Он и получил от своих родителей,
[34:01.960 --> 34:10.120]  которые в начале прошлого века эмигрировали в США из России. Вот, значит вот будем иметь
[34:10.120 --> 34:19.040]  в виду. Ну и давайте теперь как бы дальше двинемся. Я хочу найти математическое ожидание е в степени
[34:19.040 --> 34:30.840]  t Xi. Ну сначала как-то так давайте, может промежуточно. Е в степени t некая х. Это выпуклая функция,
[34:30.840 --> 34:48.400]  да. Поэтому давайте я представлю в таком виде. Е в степени t. Е в степени t. Одна,
[34:48.400 --> 35:12.200]  вторая, один плюс х. Одна, вторая, один плюс х. Минус одна, вторая, один минус х. Правильно,
[35:12.200 --> 35:22.320]  да. Это е в степени t Xi. И в виду выпуклости запишу это следующим образом. Меньше ли равно одна,
[35:22.320 --> 35:42.960]  вторая, е в степени t. Плюс одна, вторая, е в степени t. Правильно, да. Вот. Значит теперь
[35:42.960 --> 35:49.680]  воспользуюсь таким представлением. Вот здесь, ну вместо х я поставлю Xi. И поскольку взял мат
[35:49.680 --> 35:56.640]  ожидания, то давайте возьму мат ожидания здесь, если вместо х поставить Xi. Ну мы помним, что мы
[35:56.640 --> 36:03.440]  рассматриваем случаи, когда мат ожидания Xi равно нулю. Поэтому мы получаем е в степени t Xi. Меньше
[36:03.440 --> 36:16.920]  или равно е в степени t, плюс е в степени минус t делить пополам. Xi пропало, да. Вот. Ну это понятно
[36:16.920 --> 36:24.600]  не для всех все-таки Xi, а для с вероятностью единицы ограниченных. Ну может единицы это частный
[36:24.600 --> 36:32.840]  случай, но в принципе ограниченных. Вот. Ну и давайте дальше. Одна, вторая. Ряд напишу. Т в степени
[36:32.840 --> 36:42.840]  к на к факториал. К от нуля до бесконечности. Плюс минус т в степени к на к факториал. Также
[36:42.840 --> 36:48.720]  к от нуля до бесконечности. Так. Все нечетные степени сократятся, четные встретятся дважды,
[36:48.720 --> 36:59.640]  поэтому это равно сумма t в степени 2k на 2k факториал. К от нуля до бесконечности. Правильно, да.
[36:59.640 --> 37:09.680]  Ну теперь что такое 2k факториал? Это к факториал умножить на k плюс 1, на k плюс 2, на k плюс 3 и так
[37:09.680 --> 37:20.200]  далее. Вот я эти k плюс 1, k плюс 2 и k плюс 3 заменю на двойку. Таким образом я знаменатель уменьшу,
[37:20.200 --> 37:38.640]  дробь величу и поэтому это будет меньше или равно. Т в квадрате в степени k делить на 2 в степени
[37:38.640 --> 37:48.640]  k на k факториал. Ну и нетрудно убедиться, что это е в степени t квадрат пополам. Правильно, да.
[37:48.640 --> 37:56.640]  Ну вот здесь давайте в уголке напишу, будем на него часто ссылаться. Математическое ожидание
[37:56.640 --> 38:09.640]  е в степени t кс меньше или равно е в степени t квадрат пополам. Ну легко мы получили это неравенство,
[38:09.640 --> 38:17.200]  но правда из-за эту легкость мы заплатили там точностью, как станет ясно в дальнейшем. Но тем
[38:17.200 --> 38:30.280]  не менее для того, чтобы получить правильные порядки везде, этого достаточно. Так вот такой имеет место факт.
[38:47.200 --> 39:00.040]  Так, ну а теперь давайте вот что сделаем. Давайте рассмотрим серию, как мы любим,
[39:00.040 --> 39:12.280]  xnk. Это будут независимые случайные величины. Все с вероятностью единица ограничены,
[39:12.280 --> 39:26.920]  xnk меньше или равно единица по модулю равна единице. И математическое ожидание xnk равно 0, k равно 1n.
[39:26.920 --> 39:44.840]  Пока пусть будет так. Вот и давайте рассмотрим. Вот что давайте рассмотрим вероятность того,
[39:44.840 --> 39:59.400]  что xn больше некого ексила. Ну xn это вот оно свернуто по серии. Это звонок, да? Ну давайте
[39:59.400 --> 40:12.080]  отдыхайте с этого момента и продолжим понеравенство Чернова. Ну что продолжим наш труд, коллеги. Итак,
[40:12.080 --> 40:19.560]  вероятность xn больше равно епсилон, xn как бы произвольная. В общем, можем воспользоваться
[40:19.560 --> 40:28.880]  неравенством Чернова и написать меньше или равно. Тут поставлю для любого t, e в степени минус
[40:28.880 --> 40:40.000]  епсилон t, на математическое ожидание e в степени t, а вместо xn поставлю сумму xn катова.
[40:40.000 --> 40:51.120]  xn каты независимые случайные величины, мат ожидания распадается в произведение мат
[40:51.120 --> 41:02.000]  ожиданий, а каждое из со множителей к каждому применю вот эту вот формулу и получу, что это меньше или равно
[41:02.000 --> 41:29.200]  e в степени минус епсилон t, плюс t квадрат пополам, t квадрат пополам, умножить на n.
[41:29.200 --> 41:44.720]  Ну и разумно это взять, поскольку для любого t, давайте это возьмем в точке минимума, минимум
[41:44.720 --> 41:54.720]  этой функции это гипербола ветвями вверх, значит tn минус епсилон равно 0, t равно епсилон делить на n.
[41:54.720 --> 42:09.320]  Правильно, да? Ну и в точке t равно епсилон делить на n, что у нас получается? e в степени
[42:09.320 --> 42:20.160]  минус епсилон квадрат делить на n, плюс t квадрат это епсилон квадрат делить на n квадрат,
[42:20.160 --> 42:33.440]  двойка и умножить на n. Так n1 сокращается и получается e в степени минус епсилон квадрат
[42:33.440 --> 42:45.000]  делить на 2n. Вот такое вот мы с вами получили неравенство. Значит, если у нас есть вот такие
[42:45.000 --> 42:51.920]  величины с вероятностью единиц, ограниченные единицей, с нулевым от ожиданием, то тогда вероятность
[42:51.920 --> 42:57.960]  того, что их сумма будет больше епсилон, ограничится вот такой величиной, e в степени минус
[42:57.960 --> 43:11.200]  епсилон квадрат делить на 2n. Ну а теперь давайте еще одну полезную получим неравенство. 1n xn,
[43:11.200 --> 43:18.920]  то есть средне арифметическое случайный величин к серии, больше или равно епсилон,
[43:18.920 --> 43:29.480]  больше или равно епсилон, равна вероятности того, что xn больше или равно епсилон умножить на n и,
[43:29.480 --> 43:37.160]  подставив вместо епсилон, епсилон умножить на n, получим, что это меньше или равно e в степени
[43:37.160 --> 43:50.000]  минус епсилон квадрат n делить пополам. То есть для таких случайных величин среднее значение
[43:50.000 --> 44:00.800]  отклоняется от мат ожидания, поскольку здесь мат ожидания нулю, в терминах закона больших чисел,
[44:00.800 --> 44:06.600]  среднее значение отклоняется от мат ожидания на величину большую, чем епсилон, с вероятностью
[44:06.600 --> 44:13.280]  меньше, чем e в степени минус епсилон квадрат n пополам. Ну конечно по сравнению с неравенством
[44:13.280 --> 44:19.000]  Чебушева принципиально другое, там скорость ходимости единицы делить на епсилон квадрат n,
[44:19.000 --> 44:33.600]  а здесь e в степени минус епсилон квадрат n. Но это все эффекты, полученные благодаря тому,
[44:33.600 --> 44:39.760]  что мы рассматриваем класс случайных величин, ограниченных с вероятностью единицы. Если это
[44:39.760 --> 44:47.600]  требование убрать, то все, ну так сказать, в целом гораздо хуже. Так, ну и на что еще хочу обратить
[44:47.600 --> 44:54.360]  внимание, прям не знаю, выписать или выписывать, не выписывать. Мы еще можем рассмотреть вероятность
[44:54.360 --> 45:01.440]  того, что xn по модулю меньше или равно минус епсилон. Ну давайте вот здесь вот напишу,
[45:01.440 --> 45:11.280]  чтобы не на слух воспринимать. xn вот это же меньше или равно минус епсилон. Ну понятно,
[45:11.280 --> 45:17.240]  зачем я это делаю, да, чтобы модуль потом поставить. Вот это равно вероятности того,
[45:17.240 --> 45:27.960]  что xn минус xn больше или равна епсилон. Если вы посмотрите на вывод, то это приведет к тому,
[45:27.960 --> 45:34.360]  что здесь будет стоять минус xn. Обращаю ваше внимание, что если xn удовлетворяет вот этим
[45:34.360 --> 45:40.400]  свойствам, то и минус xn удовлетворяет. Поэтому, собственно, все то же самое, получим тот же
[45:40.400 --> 45:47.080]  результат. И в конце концов получаем, давайте вот здесь его запишу, он как бы так будем на него
[45:47.080 --> 46:15.480]  ссылаться. Напишу одностороннее неравенство и двухстороннее для среднего.
[46:17.080 --> 46:36.000]  Попала. Ну понятно, да. Так, значит, вот такую получили штуку скорой сходимости как бы довольно
[46:36.000 --> 46:42.800]  общего вида. Ну отсюда в частности следует, что все последние случайных величин, которые
[46:42.800 --> 46:48.080]  с вероятностью единицы ограничены, их среднее всегда сходится почти на верное, потому что,
[46:48.080 --> 46:56.240]  еще раз повторю, вот такое ограничение гарантирует быструю сходимость по вероятности. Вот. Так,
[46:56.240 --> 47:10.720]  дальше теперь некое обобщение. Некое обобщение. Так, вот это позволю себе стереть. Хотя вот тут,
[47:10.720 --> 47:40.480]  тут уже можно стереть на самом деле. Давайте теперь рассмотрим случайные величины,
[47:40.640 --> 47:57.760]  другой их буквы обозначим, yкт. Пусть их будет н штук. Вот. И, собственно, вот что потребуем,
[47:57.760 --> 48:09.200]  чтобы yкт центрированная, то есть yкт, а хотя, извините, нет, более слабо можем потребовать пока,
[48:09.200 --> 48:21.520]  потребуем, чтобы они тоже были все равномерно ограничены с вероятностью единицы. Вероятность
[48:21.520 --> 48:34.760]  того, что yкт по модулю меньше или равно c, ну пусть будет c, равна единице, k равно 1n.
[48:34.760 --> 48:47.160]  Так, я сейчас, извините, просто, чтобы потом не вводить новых каких-то обозначений, думаю,
[48:47.160 --> 49:11.800]  давайте, ладно, пусть c. Вот, значит, ну и давайте рассмотрим вероятность того, что сумма yкт будет
[49:11.800 --> 49:25.000]  больше или равна некого Эпсилона. Это то же самое yкт центрированных, извините, вот тут уже
[49:25.000 --> 49:34.480]  центрированный, да. Значит, больше или равна Эпсилон. Давайте представим в виде вероятность того,
[49:34.480 --> 49:49.120]  что сумма yкт центрированная делить на c, 1n, больше или равна Эпсилон делить на c. И обратим
[49:49.120 --> 49:55.920]  внимание, что yкт центрированная делить на c обладает тем свойством, что она по модулю меньше
[49:55.920 --> 50:03.760]  единицы, и мат ожидания равно 0. Поэтому вот к этому выражению можно применить полученный нами
[50:03.760 --> 50:15.000]  результат, вот этот вот, и написать, что это меньше или равно е в степени минус Эпсилон квадрат
[50:15.000 --> 50:31.840]  делить на 2nc квадрат, на 2nc квадрат. Вот, или для среднего yкт центрированная средняя, мы еще такое
[50:31.840 --> 50:40.520]  используемое значение, больше или равно Эпсилон, это меньше или равно, чем е в степени минус n Эпсилон
[50:40.520 --> 51:06.120]  квадрат делить на 2c квадрат, на 2c квадрат. Вот. Давайте запишу сюда. Ну, для центрированных запишу.
[51:06.120 --> 51:25.680]  И давайте сразу модуль поставлю. Вот, еще такое мы получили нераненство. Ну,
[51:25.680 --> 51:27.960]  теперь давайте еще чуть-чуть порассуждаем.
[51:56.520 --> 52:04.200]  Давайте теперь вот вместо такого симметричного ограничения по модулю меньше с, давайте считать,
[52:04.200 --> 52:20.480]  что вероятность того, что yкт принадлежит некому отрезку Акт, Бкт равно единице. То есть чуть-чуть
[52:20.480 --> 52:28.480]  так от симметричного перейдем к несимметричному, хотя, так сказать, этот. Все-таки введем такое
[52:28.480 --> 52:45.440]  свойство, что Бкт минус Акт ограничено некой константой С, К равно 1n. То есть распределение может
[52:45.440 --> 52:52.640]  быть на любом интервальчике с концентрированной вероятностью единицы, но длинные этих интервалов
[52:52.640 --> 52:59.440]  все-таки все ограничены некой общей константой. Еще раз повторю, от этого условия не избавиться,
[52:59.440 --> 53:04.640]  в общем случае, если мы не хотим потерять, так сказать, вот такую экспоненциальную скорость
[53:05.640 --> 53:22.240]  Вот так. Теперь такое утверждение. Значит, если yкт принадлежит Акт и Бкт с вероятностью единицы,
[53:22.240 --> 53:33.680]  то мотожидание yкт тоже находится в этих же интервалах. Это понятно, да? Ну,
[53:33.680 --> 53:40.400]  просто интеграл Либега. Вот функция ограничена снизу Акатом, сверху Бктом. Интеграл Либега
[53:40.400 --> 53:56.000]  тоже будет так же выглядеть. И тогда yкт средняя тоже будет принадлежать, в общем случае, этому же
[53:56.000 --> 54:12.560]  интервалу, да? yкт средняя. Так, значит, yкт средняя это yкт минус мотожидание yкт. Значит,
[54:12.560 --> 54:25.760]  что? yкт центрированная. А есть средняя? Прошу прощения. Значит так, yкт, мотожидание yкт,
[54:25.760 --> 54:35.520]  вот это может быть равно A, а вот это может быть равно B, да? Вот, значит, ну в худшем случае,
[54:35.520 --> 54:46.760]  или наоборот, вот это B, а вот это A. Значит, вот это вот по модулю принадлежит Бкт минус Акт.
[54:46.760 --> 55:02.240]  Правильно, да? Нигде не ошибся. Вот. И все это меньше c для любого k. Поэтому мы можем написать.
[55:02.240 --> 55:17.680]  Теперь так. Теперь можем написать так. Собственно, никак особо не можем написать,
[55:17.680 --> 55:35.600]  если не введем. Так, это мы получили. Ладно, давайте тогда будем считать, что все они принадлежат
[55:35.600 --> 55:50.040]  одному и тому же интервалу. Прошу прощения. Прошу прощения. Вот. И тогда мы можем написать
[55:50.040 --> 55:59.000]  следующее, что, вон последнее неравенство, вероятность того, что yкт центрированная,
[55:59.000 --> 56:15.520]  а тут, кстати, я средний это не поставил. Прошу прощения. yкт центрированная средняя по модулю
[56:15.520 --> 56:35.520]  больше или равно ε, меньше или равно 2c, уже меньше или равно B минус A пополам. Правильно, да? Ну, смотрите,
[56:35.520 --> 56:44.120]  значит, в отожидании yкт B плюс A пополам, если у максимальное значение B получается так,
[56:44.120 --> 56:49.360]  и если минимальное значение A получается то же самое с другим знаком. То есть по модулю меньше
[56:49.360 --> 56:57.000]  вот так. И тогда, воспользуясь опять же тем неравенством, мы получим вот такой результат.
[56:57.000 --> 57:12.120]  yкт центрированная средняя больше равно ε, меньше равно 2e, B минус A в квадрате пополам пополам,
[57:12.520 --> 57:28.320]  минус 2n ε квадрат делить на B минус A в квадрате. Тут двойка. Ну, смотрите, по сравнению с этим,
[57:28.320 --> 57:34.960]  как бы скорой сходимости в четвертой степени, да? То есть очень существенная разница. Если здесь
[57:34.960 --> 57:41.440]  какой-нибудь там 10 минус первый, здесь 10 минус первый, то здесь уже будет 10 минус четвертый. То есть
[57:41.440 --> 57:53.360]  очень существенная как бы подвижка. Так вот оказывается, что вот этот результат верен не
[57:53.360 --> 58:02.000]  только для симметричных вот таких случайных величин, а для любых. И вот для любых случайных
[58:02.000 --> 58:15.280]  величин. Эту же именная теорема называется теоремой Хёфтинга. Иногда одну, иногда две буквы пишут.
[58:15.280 --> 58:26.440]  Теорема Хёфтинга. То есть если у вас есть n штук, ну на самом деле одна из, так сказать,
[58:26.440 --> 58:34.320]  интерпретации. Если у вас есть n штук случайных величин, которые от A до B, то уклонение среднего
[58:34.320 --> 58:42.120]  от его мат ожидания на величину больше чем ε ограничивается вот такой штукой. Значит Хёфтинг
[58:42.120 --> 58:56.680]  это финн, финский ученый, но звать его Василий. Почему? Потому что когда Хёфтинг родился,
[58:56.680 --> 59:05.480]  Финляндия была в составе Российской империи. Вот, значит, ну тем не менее он там ни в Советском
[59:05.480 --> 59:10.120]  Союзе, ни в Российской империи не работал, в Америке по-моему работал, но такой Хёфтинг
[59:10.120 --> 59:19.640]  Василии такой имеется в теории вероятности. Значит, итак, это теорема Хёфтинга. Ну, я уже,
[59:19.640 --> 59:27.120]  когда поминал неравенство Бернштейна, говорил, что потом через там 30-50 лет, вообще говоря,
[59:27.120 --> 59:36.920]  появились как изрога изобилия. Вот мы с вами рассмотрели неравенство Хёфтинга, есть еще неравенство
[59:36.920 --> 59:52.880]  Азумы, ну как бы в несколько специфичной области там с мартингалами связано. И такое неравенство
[59:52.880 --> 01:00:03.680]  из этой же серии, неравенство, такая сложная фамилия Магди Армида, неравенство Магди Армида. Вот,
[01:00:03.680 --> 01:00:10.360]  но еще раз повторюсь, все они крутятся вокруг вот этого n-эпсилон квадрат. Ну,
[01:00:10.360 --> 01:00:21.880]  наверное, настал момент сказать, почему, как вы думаете, почему. Потому что мы имеем дело
[01:00:21.880 --> 01:00:27.440]  со средними величинами, тем более финитными, значит, они всегда симпатически нормальны,
[01:00:27.440 --> 01:00:37.200]  а епсилон квадрат n – это, ну, грубая вероятность отклонения нормального распределения на величину
[01:00:37.200 --> 01:00:45.640]  больше чем епсилон стандартно нормального, среднего именно значения, да. Вот, поэтому все крутится
[01:00:45.640 --> 01:00:51.040]  вокруг вот этого n-эпсилон квадрат, который, собственно, еще раз повторюсь, был Бернштейном,
[01:00:51.040 --> 01:00:58.920]  так сказать, там первый раз, так сказать, показано, показано наличие вот таких неравенств. Так,
[01:00:58.920 --> 01:01:10.200]  что еще можно сказать по этому поводу? Ну, вот эти вот неравенства, интерес к ним возник, ну,
[01:01:10.200 --> 01:01:19.280]  относительно недавно. Почему? Потому что до появления вот этих big data, ну, они носили в
[01:01:19.280 --> 01:01:25.160]  значительной степени все-таки теоретический характер. Ну, например, позволяли сделать вывод о
[01:01:25.160 --> 01:01:31.360]  сходимости, почти, наверное, так сказать, быстрая сходимость, да. Но так, вообще-то говоря, до появления,
[01:01:31.360 --> 01:01:39.960]  еще раз повторю, вот этой революции, да, ну, сколько, какими объемами, там, не знаю, измерений,
[01:01:39.960 --> 01:01:47.440]  вычислениями, измерений, да, там, естествоиспытатели, так сказать, оперировали. Ну, тысячи там, условно
[01:01:47.440 --> 01:01:55.800]  говорят, тысячи, это много, а то несколько сотен. Ну, и, так сказать, в этих объемах вот эта величина
[01:01:55.800 --> 01:02:02.400]  все-таки достаточно большая. Чтобы вот эта величина была, ну, какая-то маломальски человеческая,
[01:02:02.400 --> 01:02:10.640]  да, надо, чтобы n-эпсилон квадрат там было, так сказать, ну, не знаю, там порядка единиц хотя бы,
[01:02:10.640 --> 01:02:18.360]  единиц. А это значит, что если епсилон взять какую-нибудь там, там, 10 минус второй, то это уже
[01:02:18.360 --> 01:02:27.080]  n-10 в четвертый, десять тысяч, как бы, измерений. Так сказать, до информационной эпохи с такими
[01:02:27.080 --> 01:02:35.400]  объемами данных, в общем-то, ну, не работали. Вот, значит, поэтому носило вот это вот такой
[01:02:35.400 --> 01:02:40.120]  теоретический, в общем-то, характер. А когда вот появились, так сказать, большие данные,
[01:02:40.120 --> 01:02:48.440]  и там какие-то там, не знаю, там миллионы измерений, вот тогда вот эти вот границы уже стали вполне
[01:02:48.440 --> 01:02:57.920]  рабочие. И вот одна из таких задач, которая, ну, практическую там ценность имеет, да, состоит в
[01:02:57.920 --> 01:03:08.960]  оценке параметра распределения Бернули. Вот когда вы рассматриваете там графы и ребрами
[01:03:08.960 --> 01:03:16.640]  приписываете некие вероятности перехода, там такая модель популярная, да, ну, возникает вопрос,
[01:03:16.640 --> 01:03:23.200]  а как оценить на основании опытных данных, на основании обучающей выборки, надо как-то оценить
[01:03:23.200 --> 01:03:27.680]  вот эту вот вероятность перехода. Ну, вероятность перехода, собственно, как бы вот и оценивается,
[01:03:27.680 --> 01:03:33.080]  исходя вот из неравности такого типа. Что мы здесь получаем, если будем считать, что у нас
[01:03:33.080 --> 01:03:45.480]  yкт там имеет, вот yкт принадлежит распределению Бернули с параметром t. Тогда вероятность того,
[01:03:45.480 --> 01:03:57.200]  что ук средний отклонится от своего мотожидания на величину больше, чем епсилон. Меньше или равна
[01:03:57.200 --> 01:04:07.280]  для распределения Бернули b-а равно единице, да, значит, меньше или равно, чем 2 на е в степени,
[01:04:07.680 --> 01:04:18.200]  вот как раз вот это 2 е квадрат n. То есть вполне рабочая граница, то есть как бы вот можно с этим
[01:04:18.200 --> 01:04:25.160]  работать, уже получать какие-то, при том объеме данных, которые, так сказать, сейчас обрабатываются,
[01:04:25.160 --> 01:04:32.080]  уже можно получать достаточно точные оценки вот этих вероятностей перехода по ребрам графов.
[01:04:32.080 --> 01:04:39.760]  Вот и, собственно, как вот интерес возобновился. А так, еще раз повторюсь, там, 50 лет назад,
[01:04:39.760 --> 01:04:47.160]  образно говоря, значит, ну так было это там, ну много из этого, но носил такое, в общем,
[01:04:47.160 --> 01:04:52.440]  скорее теоретическое значение, не переходило в практическую плоскость. Ну, как я говорил,
[01:04:52.440 --> 01:05:01.000]  сам в начале курса такое в науке бывает, да, так сказать, какие-то наработки, теории, они могут
[01:05:01.000 --> 01:05:09.600]  ждать, там, не знаю, десятилетиями своего часа, да, пока, так сказать, они понадобятся. Ну вот,
[01:05:09.600 --> 01:05:17.480]  эти, этот раздел, так сказать, вот он как бы понадобился при наличии, так сказать, больших данных,
[01:05:17.480 --> 01:05:26.760]  вот. И вот это вот еще раз просто, как такая частная, но очень важная задача, прямо обеду. Вот как
[01:05:26.760 --> 01:05:35.760]  можно строить, получать довольно, так сказать, точные качественные оценки вероятности успеха
[01:05:35.760 --> 01:05:43.160]  в испытании Бернулий, которые интерпретируются как вероятность перехода по ребру графа. Вот. Так.
[01:05:43.160 --> 01:05:56.480]  Что еще по этой теме? Так, вроде ничего не забыл. Единственно, акцентирую еще раз, вот на чем внимание.
[01:05:56.480 --> 01:06:05.080]  Значит, смотрите, мы с вами получили вот этот результат теория Мехёвтинга для частного случая.
[01:06:05.080 --> 01:06:12.400]  Почему мы были вынуждены, так сказать, к частному случаю, так сказать, обратиться? Ну потому что мы
[01:06:12.400 --> 01:06:20.120]  слишком просто, слишком грубо вот это, вот это неравенство получили. Оно очень грубое. Из этого мы
[01:06:20.120 --> 01:06:25.600]  потеряли вот эту, точнее говоря, двойка у нас оказалась вместо числителя, а оказалась знаменателя.
[01:06:25.600 --> 01:06:33.160]  Если бы мы провели более тщательный анализ, воспользовались бы выпуклостью функций, но только
[01:06:33.160 --> 01:06:38.160]  на отрезке АБ, собственно, как это и делается в теории Мехёвтинга, то мы бы как бы аккуратно вот
[01:06:38.160 --> 01:06:46.000]  такой результат получили. Но мы, так сказать, схалтурили немножко вот здесь, поэтому получили
[01:06:46.000 --> 01:06:54.280]  более слабый результат, хотя с точки зрения, как вот еще раз повторюсь, теории вероятности как
[01:06:54.280 --> 01:07:01.280]  естественной научной дисциплины довольно понятно, что так должно быть. Почему? Когда у нас довольно
[01:07:01.280 --> 01:07:08.080]  большие n, вот средние значения, мы же можем их разбить на группы по слагаемым. Каждая из этих групп
[01:07:08.080 --> 01:07:14.320]  будет нормально распределена, это стремится к нормальному распределению и финитно, а нормальное
[01:07:14.320 --> 01:07:19.480]  распределение, оно симметрично, поэтому, так сказать, попадаем в класс симметричных распределений,
[01:07:19.480 --> 01:07:25.600]  в общем, как бы понятно, что так должно быть. Осталось это доказать, вот что Хёвтинг, собственно, и сделал,
[01:07:25.600 --> 01:07:33.400]  да, вот, ну в том числе, так сказать, там как бы не он один, на этом поприще трудился. Вот, значит,
[01:07:33.400 --> 01:07:43.000]  тогда по этой теме, вроде, вроде ничего не забыл, да, вроде ничего не забыл, тогда по этой теме все,
[01:07:43.000 --> 01:07:48.560]  еще раз повторюсь, что это не единственного типа неравенства, их там, ну, несколько,
[01:07:48.560 --> 01:07:57.280]  относящиеся там к разным ситуациям, вот, довольно, как бы, развитые вот эти вот
[01:07:57.280 --> 01:08:04.440]  задачи или неравенства на концентрацию меры. Ну, еще раз повторю, так сказать, порядок у них
[01:08:04.440 --> 01:08:12.800]  у всех один, это n епсилон квадрат. Так, ну, тогда, позволю себе, давайте, вот здесь сотру.
[01:08:12.800 --> 01:08:42.600]  Так, ну, и там хотел бы, наверное, завершить наш курс вот таким
[01:08:42.600 --> 01:08:50.120]  вопросом и соответствующим результатом. Значит, смотрите, мы, когда с вами центральную пределу и
[01:08:50.120 --> 01:09:01.440]  предельную теорему рассматривали, мы получили, что, что, вот, множество такого вида, точнее,
[01:09:01.440 --> 01:09:08.360]  если центральная, предельная, то мы считали, что, не считали, а по условию они все одинаково распределены.
[01:09:08.360 --> 01:09:21.240]  К единице до n делить на n сигма квадрат, корень квадратный, вот так напишу. Вот, вот,
[01:09:21.240 --> 01:09:26.360]  такие вот величины сходятся по распределению к стандартному нормальному закону. Вот,
[01:09:26.360 --> 01:09:32.440]  это центральная предельная теорема. Давайте чуть-чуть, ну, я говорил, что, на самом деле,
[01:09:32.440 --> 01:09:39.120]  есть много результатов, связанных с различными модификациями. Могут они как-то зависеть
[01:09:39.120 --> 01:09:46.800]  специальным образом эти ксикаты и тоже может иметь место, так сказать, сходимости к стандартному
[01:09:46.800 --> 01:09:52.640]  нормальному закону. Еще какие-то вводится, так сказать, специфические там виды. На эту тему много
[01:09:52.640 --> 01:10:00.600]  результатов. И вот одна, как бы, группа результатов относится к случаю, когда количество вот этих
[01:10:00.700 --> 01:10:09.840]  слагаемых случайно, количество слагаемых случайно. То есть вы имеете сумму независимых случайных
[01:10:09.840 --> 01:10:17.520]  величин в случайном количестве. Ну, и вот оказывается, что при разных условиях тоже имеет место
[01:10:17.520 --> 01:10:21.840]  асимптотическая нормальность. Этот, так сказать, при разных схемах эксперимента. Точнее говоря,
[01:10:21.840 --> 01:10:27.920]  при разных способах нормировки. Да, при разных способах нормировки тоже имеют место асимптотическая
[01:10:27.920 --> 01:10:33.760]  нормальность и один из результатов из этой области ну частный случай просто для
[01:10:33.760 --> 01:10:42.520]  понимания значит я вам расскажу это теорема принадлежит Натану Андрею Иксановичу значит
[01:10:42.520 --> 01:10:52.400]  там в прошлом заведующему нашей кафедрой тоже кстати так свете участнику войны значит
[01:10:52.400 --> 01:11:03.200]  полковник в отставке он по моему был да вот значит в применении или в терминах
[01:11:03.200 --> 01:11:09.880]  центральной предельной теоремы давайте вот рассмотрим такую такую постановку вот у нас
[01:11:09.880 --> 01:11:19.760]  есть случайная последовательность в общем случае до бесконечности пусть будет вот значит независимые
[01:11:19.760 --> 01:11:26.080]  случайные величины независимые случайные ну пусть даже раз мы в терминах этой постановки
[01:11:26.080 --> 01:11:31.920]  в частный случай центральной предельной теоремы обобщаем независимо одинаково распределенные
[01:11:31.920 --> 01:11:44.120]  случайные величины дисперсия кси ну сигма квадрата и обозначим и есть у нас дискретная
[01:11:44.120 --> 01:11:52.680]  случайная величина n с функцией распределения которая зависит от некоторого параметра лямбда
[01:11:52.680 --> 01:12:00.080]  от некоторого параметра лямбда вот который обладает единственным свойством требуется чтобы для
[01:12:00.080 --> 01:12:09.680]  любого аргумента для любого n малого функция распределения стремилась к нулю когда лямбда
[01:12:09.680 --> 01:12:18.840]  стремится к бесконечности все дискретные распределения к с которым имели место удовлетворяет
[01:12:18.840 --> 01:12:25.280]  этому свойству только надо правильно параметра лямбда подобрать например если он имеет
[01:12:25.280 --> 01:12:31.680]  биномиальное распределение то таким параметром является количество экспериментов а самой
[01:12:31.680 --> 01:12:36.280]  случайной величиной ну является от количества успехов серии из инструментов как только вы
[01:12:36.280 --> 01:12:41.160]  количестве экспериментов будете стремить бесконечности функции распределения будут
[01:12:41.160 --> 01:12:46.400]  удовлетворять вот этому свойству в общем это не диковинное которое какое-то свойство которое
[01:12:46.400 --> 01:12:55.600]  там надо еще поискать наоборот очень распространенное ну вот и если такое свойство выполнено такой
[01:12:55.600 --> 01:13:06.920]  простой единственное свойство то вот такая сумма ксикатая как единицы до n большого минус а
[01:13:06.920 --> 01:13:17.280]  делить на корень квадратный из n большое sigma квадрат будет стрелиться к стандартному
[01:13:17.280 --> 01:13:24.360]  нормальному распределению но только понятно что не по случайной влечении а вот по этому параметру
[01:13:25.320 --> 01:13:34.960]  ну с практической точки зрения если вы будете если n у вас это биномиальное распределение то если
[01:13:34.960 --> 01:13:40.520]  вы будете увеличивать количество экспериментов или набирать статистику все больше и больше то даже
[01:13:40.520 --> 01:13:46.600]  несмотря на то что n случайная вот эта штука будет сходиться к стандартному нормальному распределению
[01:13:46.600 --> 01:13:57.160]  это вот частный случай теоремы натана вот применённый к обобщению центральной предельной теоремы значит
[01:13:57.160 --> 01:14:07.560]  доказательства особенно в данном случае не сложная но оно идейно как бы и на самом
[01:14:07.560 --> 01:14:17.400]  деле не сложная вот значит давайте рассмотрим вероятность того что вот это наша специальная
[01:14:17.400 --> 01:14:36.480]  случайная влечина будет меньше некоторого x и вычтем из нее вот стандартную функцию распределения
[01:14:36.480 --> 01:14:46.760]  стандартно нормальную значит и представлю этот в таком виде вот эту вероятность по формуле
[01:14:46.760 --> 01:14:49.880]  полной вероятности я выпишу так
[01:15:06.480 --> 01:15:14.760]  меньше x при условии что n большое равно некому n малому и на вероятности n малого
[01:15:14.760 --> 01:15:18.800]  плюс
[01:15:18.800 --> 01:15:32.520]  точнее говоря минус минус f от x и добавлю сюда единицу сумму всех вероятностей что n большое
[01:15:32.520 --> 01:15:47.440]  равно и мало сумма по н единица да теперь смотрим дальше значит смотрите вот это некая
[01:15:47.440 --> 01:15:52.720]  условная вероятность при вот таком событии не нулевой вероятностной меры то есть ну как бы
[01:15:52.720 --> 01:16:01.800]  все корректно да как мы помним условная вероятность это обычная вероятность только взятая на под
[01:16:01.800 --> 01:16:11.480]  множестве событий условия то есть вот наше условие значит это на самом деле можно представить
[01:16:11.480 --> 01:16:22.800]  вот в таком виде секат и минус а ка уже от единицы до n малая то есть вот этого условия делить на
[01:16:22.800 --> 01:16:30.680]  корень n малая сигма квадрат меньше x умножить на вероятность того что
[01:16:30.680 --> 01:16:41.520]  сэкономлю одну строчку напишу здесь минус f от x
[01:16:41.520 --> 01:16:50.960]  на вероятность того что n большое равно n малая ну и дальше просто
[01:17:11.880 --> 01:17:17.280]  дальше просто тогда модуль вот этой вот
[01:17:17.280 --> 01:17:29.000]  еще может быть разобью на сумму значит равно пишу равно
[01:17:29.000 --> 01:17:35.680]  суммы
[01:17:41.520 --> 01:18:08.560]  сумму возьму по n меньше некого n с волной пока не пояснил попозже пояснил и точно такую сумму
[01:18:08.560 --> 01:18:19.600]  по n больше и он с волной вот ну может я там это что-то там скажу не не все запишу тут как бы
[01:18:19.600 --> 01:18:33.200]  достаточно все очевидно смотрите значит для понятности наоборот ту которую не написал она
[01:18:33.200 --> 01:18:42.600]  по очевидней это вот так ну сейчас еще буквально так седьминуту вот смотрите значит
[01:18:42.600 --> 01:18:50.800]  предостаточно больших n вот эта разность мало потому что при не случайно мэн вот эта штука
[01:18:50.800 --> 01:18:57.040]  стремится к нулю при любом фиксированном x да то есть это грубо говоря епсилон можно вынести за
[01:18:57.040 --> 01:19:03.000]  скобки здесь останется вероятность вот такого события которые можно иметь на единицу а вот
[01:19:03.000 --> 01:19:10.480]  в этой сумме это же разность вероятности она по модулю меньше двух двойку можно вынести а то
[01:19:10.480 --> 01:19:23.040]  что здесь останется это функция распределения n большого в точке n с волной да которая стремится
[01:19:23.040 --> 01:19:30.760]  как мы знаем к нулю когда лямба стремится к бесконечности то есть выбрав n с волной такое чтобы
[01:19:30.760 --> 01:19:41.160]  вот эта вероятность была достаточно мало вот а да выбрав н с волной чтобы вот эта вероятность
[01:19:41.160 --> 01:19:47.520]  была достаточно мало но вот эту сумму сделаем маленькой а вот это при достаточно большом n то
[01:19:47.520 --> 01:19:53.280]  есть для n больше n с волной грубо говоря каждый сам множественный будет меньше некоторого
[01:19:53.280 --> 01:20:01.840]  епсилона я понятно объяснил ну так это в принципе не сложно для данного случая все довольно просто
[01:20:01.840 --> 01:20:08.920]  это обобщение центральной предельной теоремы на случай случайного количества слагаемых хочу
[01:20:08.920 --> 01:20:18.200]  заметить что не совсем тут все тривиально видите мы какой выбрали порядок ну или или величину как
[01:20:18.200 --> 01:20:24.040]  отнормировали для сходимости а если бы мы здесь например поставили дисперсию вот этой суммы
[01:20:24.040 --> 01:20:31.120]  случайной ксикатая кат единицы дн большое не и случайно лично на множестве квадрат а дисперсию
[01:20:31.120 --> 01:20:40.040]  вот этой суммы то там вообще бы уже как бы ничего не получилось бы то есть там нормировка должна
[01:20:40.040 --> 01:20:45.240]  выглядеть или или в этой теореме выглядит так и приводит к успеху другая нормировка уже к успеху
[01:20:45.240 --> 01:20:52.920]  может не привести так коллеги ну что значит все мы с вами закончили курс теории вероятности надеюсь
[01:20:52.920 --> 01:21:00.760]  что так сказать там как может быть не сразу но постепенно это у вас перейдет такой активную как бы
[01:21:00.760 --> 01:21:09.440]  фазу довольно длинные истории развития предмета как я вам сказал там порядка 500 лет но на самом
[01:21:09.440 --> 01:21:17.680]  деле я вам хочу сказать все достаточно близко что я имею в виду как это время время на самом
[01:21:17.680 --> 01:21:27.560]  деле гораздо более сжатое чем нам кажется я в свое время имел так сказать честь там несколько
[01:21:27.560 --> 01:21:33.480]  раз здороваться за руку с академиком никольским сергей михалычем который должен до 107 до 107 лет
[01:21:33.480 --> 01:21:40.920]  кстати вот читал мне математический анализ академик никольский естественно ручкался с академиком
[01:21:40.920 --> 01:21:46.080]  бюрнштейн а поскольку они стали в одной академии академик бюрнштейн естественно ручкался с
[01:21:46.080 --> 01:21:52.400]  гильбертом который был научным руководителем так что коллеги я в трех рукопожатиях от гильберта вот
[01:21:52.400 --> 01:22:01.520]  так что все довольно так сказать в этом мире спрессовано и теория вероятности которая там
[01:22:01.520 --> 01:22:09.840]  500 лет развивалась мы с вами видите вот за там сколько там 12-13 лет с этим вот как-то ну не
[01:22:09.840 --> 01:22:19.600]  сказать что так уж всю но прошли все спасибо тем кто доходи дослушал доходил вот потому что так
[01:22:19.600 --> 01:22:24.000]  сказать наверное перед пустой аудитории мне было бы не сильно приятно читать хочу поблагодарить
[01:22:24.000 --> 01:22:33.520]  нашу фею которая весь курс у нас вот добросовестно здесь так сказать отработал оператором все коллеги
[01:22:33.520 --> 01:22:41.120]  значит успехов теперь возможность видеться с вами на экзаменах по случайным процессам или
[01:22:41.120 --> 01:22:42.320]  математической статистике

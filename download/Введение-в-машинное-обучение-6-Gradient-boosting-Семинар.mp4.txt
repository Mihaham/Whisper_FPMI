[00:00.000 --> 00:10.520]  Коллег, один из студентов, я бы хотел вам его задать.
[00:10.520 --> 00:14.720]  Я предлагаю это даже на запись уже включить, все, скринкаст
[00:14.720 --> 00:18.280]  запись экрана тоже пошла, 3-2 раз, 1-2-3, потому что вопрос
[00:18.280 --> 00:19.280]  достаточно актуальный.
[00:19.280 --> 00:24.080]  А именно, вот есть у вас лабораторная работа, там как раз есть
[00:24.080 --> 00:27.080]  задачка применить PCA, куда-то все спроецировать, и тут
[00:27.080 --> 00:30.400]  скорее вопрос не про лабу, а про PCA.
[00:30.400 --> 00:32.560]  Если у вас нет главных компонентов, применяем
[00:32.560 --> 00:35.600]  к самой выборке, к самому датсету.
[00:35.600 --> 00:36.600]  Внимание, вопрос.
[00:36.600 --> 00:40.320]  Вот мы взяли с вами PCA, взяли датсет и выбрали
[00:40.320 --> 00:43.440]  количество главных компонентов, равное количеству признаков
[00:43.440 --> 00:44.440]  в нашем датсете.
[00:44.440 --> 00:48.400]  Условно у нас была 10-мерная выборка, 10 признаков, мы
[00:48.400 --> 00:52.520]  взяли 10 главных компонентов, применили, PCA соответственно
[00:52.520 --> 00:55.520]  выучили все главные компоненты, на них отобразились.
[00:55.600 --> 00:57.840]  Получили ли мы тождественное преобразование или нет?
[00:57.840 --> 00:58.840]  Нет.
[00:58.840 --> 01:03.320]  Кто за да, кто за нет, подумайте, пожалуйста.
[01:03.320 --> 01:12.600]  Вопрос, есть ли метод главных компонентов, в качестве
[01:12.600 --> 01:15.920]  главных компонентов используют столько же главных компонентов,
[01:15.920 --> 01:20.160]  сколько у нас координатных осей в байсе пространства?
[01:20.160 --> 01:21.920]  Получим ли мы тождественное преобразование или нет?
[01:21.920 --> 01:31.120]  Ну вот, коллеги с правой части для меня зала считают,
[01:31.120 --> 01:32.120]  что нет.
[01:32.120 --> 01:34.520]  Коллеги с левой части зала.
[01:34.520 --> 01:46.280]  Ну да, домножаем, вопрос, эта матричка будет единична
[01:46.280 --> 01:48.800]  или нет?
[01:48.800 --> 01:49.800]  Хороший вопрос.
[01:50.040 --> 01:55.200]  Ну давайте, кто за то, что да, оно тождественное?
[01:55.200 --> 01:57.800]  Даже те, кто спрашивали, говорят, что уже нет.
[01:57.800 --> 02:00.800]  Кто за то, что нет?
[02:00.800 --> 02:03.480]  Большая часть аудитории воздержал, я понял.
[02:03.480 --> 02:06.720]  Ну хорошо, давайте те, кто за то, что нет, дадут какое-нибудь
[02:06.720 --> 02:07.720]  обоснование.
[02:07.720 --> 02:08.720]  Ну пожалуйста, вот вы говорили в начале.
[02:08.720 --> 02:15.680]  Ага, классно, пример с овальчиком, работает.
[02:15.680 --> 02:18.280]  Если вы эллипс нарисуете, у вас первая главная компонента
[02:18.280 --> 02:21.360]  будет направлена вдоль первой главной полуаси.
[02:21.360 --> 02:24.080]  Вообще не обязательно, что она совпадает с осью
[02:24.080 --> 02:25.080]  координат.
[02:25.080 --> 02:26.440]  Теперь как это обосновать в общем случае?
[02:26.440 --> 02:29.360]  У вас главные компоненты – это направление наибольшей
[02:29.360 --> 02:32.800]  дисперсии по убыванию, если вы их отсортируете.
[02:32.800 --> 02:35.560]  Соответственно, у вас направление наибольшей дисперсии вовсе
[02:35.560 --> 02:39.000]  не обязательно координатной оси вашего пространства,
[02:39.000 --> 02:40.680]  тем более в том же порядке.
[02:40.680 --> 02:43.480]  Поэтому, конечно же, в общем случае PCA все равно меняет
[02:43.480 --> 02:45.560]  ваше пространство признаковое.
[02:45.560 --> 02:47.400]  По сути, вы его поворачиваете.
[02:47.400 --> 02:49.860]  Так что, если вы примените к выборке PCA и сохраните
[02:49.860 --> 02:52.760]  количество компонентов, равным количеству координат
[02:52.760 --> 02:56.000]  осей вашей выборки, вы просто-напросто в другой
[02:56.000 --> 02:58.400]  базе спирейете в том же пространстве.
[02:58.400 --> 03:00.600]  Вы не потеряете ни йода информации, у вас все останется
[03:00.600 --> 03:04.200]  на месте, но тем не менее база у вас будет другой.
[03:04.200 --> 03:05.200]  Данный вопрос понятен?
[03:05.200 --> 03:06.200]  Супер.
[03:06.200 --> 03:07.200]  Хорошо.
[03:07.200 --> 03:10.800]  Еще какие-то вопросы по предыдущим занятиям есть?
[03:10.800 --> 03:13.240]  Мало ли там что-то еще в голове сло, мы можем с вами
[03:13.240 --> 03:14.240]  их обсудить вначале.
[03:14.240 --> 03:25.240]  Три, два, один, ну ладно, тогда предлагаю продолжать.
[03:25.240 --> 03:29.440]  И сегодня мы с вами поговорим о градиентном бустинге,
[03:29.440 --> 03:33.800]  о ужас, о той технике, которая немножко, скажем так, затмила
[03:33.800 --> 03:37.600]  нейронные сети в начале нулевых, в 2001 году был представлен
[03:37.600 --> 03:41.440]  GBM, Grading Boosting Machine, и за счет этого в том числе нейронные
[03:42.280 --> 03:45.000]  сети на 10 лет ушли в подполье и сидели там, крайне тихо
[03:45.000 --> 03:46.000]  развивались.
[03:46.000 --> 03:48.840]  Собственно, градиентный бустинг это на данный момент
[03:48.840 --> 03:54.360]  наверно некоторая максимально высокая точка развития
[03:54.360 --> 03:58.040]  классических моделей, в каком смысле, он все еще актуален,
[03:58.040 --> 04:00.840]  все еще широко применим, все еще крайне любим и
[04:00.840 --> 04:04.840]  не сдает позиции нейронным сетям в том смысле, в смысле
[04:04.840 --> 04:06.640]  подходу, где мы используем какие-то дифференцируемые
[04:06.640 --> 04:09.240]  преобразования, каскадом их выстраиваем и так далее.
[04:09.440 --> 04:12.120]  То, что нейронные сети и линии на модели, суть примерно
[04:12.120 --> 04:16.360]  одно и то же, можем их отнести примерно в одно семейство.
[04:16.360 --> 04:18.600]  Отдельно могут стоять метрические алгоритмы, тот же самый
[04:18.600 --> 04:21.760]  KNN и так далее, отдельно может стоять дерево и их
[04:21.760 --> 04:22.760]  ансамбли.
[04:22.760 --> 04:25.600]  Сегодня мы с вами поговорим про ансамбли деревьев, про
[04:25.600 --> 04:26.600]  градиентный бустинг.
[04:26.600 --> 04:28.840]  Не обязательно на самом деле деревьев, но исторически
[04:28.840 --> 04:31.440]  он используется в основном на неглубоких деревьях,
[04:31.440 --> 04:33.520]  но опять же это исторически сложилось, его можно и с
[04:33.520 --> 04:34.520]  другими моделями использовать.
[04:35.040 --> 04:39.240]  И сегодня у нас в меню четыре основных шага.
[04:39.240 --> 04:41.880]  Первое, мы с вами постараемся интуитивно понять, что такое
[04:41.880 --> 04:43.800]  бустинг и как он вообще работает.
[04:43.800 --> 04:47.440]  Второе, мы с вами попытаемся математически объяснить,
[04:47.440 --> 04:50.400]  что происходит и как мы можем градиентные методы использовать
[04:50.400 --> 04:54.040]  вроде как деревьями, которые недеференцируемы.
[04:54.040 --> 04:56.200]  Ну а дальше поговорим еще чуть-чуть про другие техники
[04:56.200 --> 04:59.120]  ансамблирования, будь то стекинг, блендинг и про
[04:59.120 --> 05:00.120]  что-нибудь еще.
[05:00.120 --> 05:02.440]  Вопросы сегодня приветствуются, в какой-то момент придется
[05:02.520 --> 05:06.040]  чуть-чуть расширить сознание и понять, что такое градиентный
[05:06.040 --> 05:09.560]  спуск в пространстве моделей, если так грубо говорить.
[05:09.560 --> 05:11.560]  Если у вас был уже функкан, то вам, наверное, будет
[05:11.560 --> 05:12.560]  чуть проще.
[05:12.560 --> 05:14.880]  Если у вас его не было, вам все равно будет нормально.
[05:14.880 --> 05:17.280]  Функкан нам сильно не понадобится, но понимать, что у нас
[05:17.280 --> 05:19.440]  это выражение в разных местах работает полезно.
[05:19.440 --> 05:23.280]  Но перед этим давайте коротенько вспомним, что было на прошлом
[05:23.280 --> 05:24.280]  занятии.
[05:24.280 --> 05:27.600]  Конкретно про random forest поговорим, про случайный лес и про
[05:27.600 --> 05:29.480]  ансамблирование деревьев в лоб.
[05:29.480 --> 05:31.760]  Вот у вас есть куча деревьев.
[05:31.840 --> 05:32.840]  Узяли их, усреднили.
[05:32.840 --> 05:34.520]  Что можно сказать первое?
[05:34.520 --> 05:36.680]  Во-первых, все деревья были построены независимо
[05:36.680 --> 05:37.680]  друг от друга.
[05:37.680 --> 05:38.680]  Правильно?
[05:38.680 --> 05:40.680]  Вы можете одно строить на одной машине, другое
[05:40.680 --> 05:41.680]  на другой.
[05:41.680 --> 05:44.240]  На разных датсетах, на разных признаках, подможествах.
[05:44.240 --> 05:45.240]  Они все независимы.
[05:45.240 --> 05:49.840]  Но тут, если вспомнить, опять же, не знаю, там, электричество,
[05:49.840 --> 05:52.240]  электродинамика, то у нас есть последовательная
[05:52.240 --> 05:55.440]  сеть, какие-то элементы последовательные и параллельные.
[05:55.440 --> 05:58.480]  Вот здесь мы явно все строим параллельно, поэтому оно
[05:58.480 --> 05:59.480]  независимо друг от друга.
[05:59.880 --> 06:01.640]  Можете сразу напроситься вопроса, что если строить
[06:01.640 --> 06:02.640]  последовательно.
[06:02.640 --> 06:03.640]  Этому как раз к бусинку и подойдем.
[06:03.640 --> 06:06.880]  Во-вторых, какие основные свойства у рандом-фореста
[06:06.880 --> 06:10.400]  и ему же подобных бэйгинг-моделей, но рандом-форест, наверное,
[06:10.400 --> 06:13.040]  как некоторая высшая ступень развития этого подхода.
[06:13.040 --> 06:15.160]  Давайте их еще раз повторим.
[06:15.160 --> 06:17.760]  Во-первых, деревья сами по себе хорошо работают
[06:17.760 --> 06:20.920]  с пропусками раз и со скоррелированными признаками два.
[06:20.920 --> 06:23.000]  Рандом-форест это все у них перенимает, потому
[06:23.000 --> 06:25.280]  что скоррелированные признаки абсолютно по барабану мы
[06:25.280 --> 06:27.640]  их раздельно рассматриваем.
[06:27.800 --> 06:28.800]  Пропуски в данных.
[06:28.800 --> 06:31.040]  Если есть пропуск, то мы используем оба под дерева,
[06:31.040 --> 06:33.320]  потом усредняем ответ в зависимости от мощности
[06:33.320 --> 06:36.400]  левого и правого под дерева в смысле объема выборки,
[06:36.400 --> 06:38.360]  который туда и сюда пошел на этапе обучения.
[06:38.360 --> 06:42.360]  И в-третьих, мы можем использовать out-of-back оценку для того,
[06:42.360 --> 06:45.960]  чтобы получать оценку на не увиденных ранее данных.
[06:45.960 --> 06:50.280]  Короче, валютационная выборка бесплатна, нам не нужно
[06:50.280 --> 06:51.800]  делить наши данные на две части.
[06:51.800 --> 06:55.640]  Тогда мы получаем с вами оценку сверху на ошибку
[06:55.640 --> 06:57.440]  модели, потому что если мы работаем out-of-back, у нас
[06:57.440 --> 06:59.160]  лишь часть ансамбля работает.
[06:59.160 --> 07:01.440]  Вот вы здесь можете видеть, что мы здесь все модели,
[07:01.440 --> 07:04.240]  у которых данный объект не попал в обучающую выборку
[07:04.240 --> 07:05.240]  используем.
[07:05.240 --> 07:07.520]  Соответственно, размер ансамбля меньше, эффективность
[07:07.520 --> 07:10.200]  ранним форреста меньше, ниже, поэтому качество
[07:10.200 --> 07:11.200]  тоже ниже.
[07:11.200 --> 07:12.200]  Ошибка выше.
[07:12.200 --> 07:15.000]  Ну и также я говорил коротко в конце занятия, что мы с
[07:15.000 --> 07:18.640]  вами можем использовать другие версии вот этого
[07:18.640 --> 07:22.440]  самого случайного леса, extremely randomized trees, там мы вообще
[07:22.440 --> 07:25.040]  доводим до абсурда случайность, если у нас краски сильно
[07:25.040 --> 07:26.480]  скоррелированы и множество.
[07:26.480 --> 07:30.920]  Или isolation forest, это техника поиска аномалий, как я уже
[07:30.920 --> 07:31.920]  говорил.
[07:31.920 --> 07:33.280]  Повторить про нее еще раз, или все уже все помнят
[07:33.280 --> 07:34.280]  и движемся дальше.
[07:34.280 --> 07:35.280]  Повторить.
[07:35.280 --> 07:39.920]  Ну хорошо, давайте тогда еще раз, тогда это было в
[07:39.920 --> 07:40.920]  конце занятия.
[07:40.920 --> 07:41.920]  Еще раз формулируем.
[07:41.920 --> 07:44.600]  Что такое вот аномалия или же выброс, с точки зрения
[07:44.600 --> 07:46.600]  именно признаков описания, не будем пока смотреть на
[07:46.600 --> 07:47.600]  таргета вообще.
[07:47.600 --> 07:50.200]  Вот у вас есть множество точек, какие-то из них какие-то
[07:50.200 --> 07:53.520]  аномальные, какие-то нестандартные, вы можете какие-то свойства
[07:53.520 --> 07:55.480]  их описать самостоятельно, не глядя на выборку.
[07:55.480 --> 08:01.520]  Ну кто-нибудь, давайте.
[08:01.520 --> 08:11.120]  Далеко от других точек, классно, что-нибудь еще?
[08:11.120 --> 08:12.120]  Легко отделить.
[08:12.120 --> 08:13.120]  Хорошо.
[08:13.120 --> 08:16.480]  Ну по сути, если мы с вами нарисуем какое-то наше множество,
[08:16.480 --> 08:19.160]  мы с вами краски заметим, что у нас наши данные могут
[08:19.160 --> 08:23.200]  образовывать какую-то там достаточно плотную группу,
[08:23.200 --> 08:26.160]  причем она вообще не обязательно какая-то правильная, условно.
[08:26.160 --> 08:28.560]  У нас может быть просто какое-то облако точек круглое,
[08:28.560 --> 08:31.200]  а может быть какая-нибудь вот такая вот крыказиабра
[08:31.200 --> 08:34.920]  непонятной формы от буков КС, и внутри у него у нас
[08:34.920 --> 08:35.920]  накиданы точки.
[08:35.920 --> 08:39.920]  И, допустим, вот эта точка, она в среднем-то не так
[08:39.920 --> 08:42.080]  уж далеко до всех, но она будет выбраться того, что
[08:42.080 --> 08:44.320]  у нас плотность как-то вот таким образом распределена,
[08:44.320 --> 08:47.840]  а здесь у нас краски низкая плотность объектов, а туда
[08:47.840 --> 08:48.840]  кто-то попал.
[08:48.840 --> 08:49.840]  Вот все.
[08:49.840 --> 08:52.920]  Соответственно, выборку мы можем, точнее не выборку,
[08:52.920 --> 08:56.240]  а выбросами или аномалиями мы можем называть те точки,
[08:56.240 --> 08:59.360]  которые визуально выбиваются из вот этого нашего общего,
[08:59.360 --> 09:02.080]  скажем так, распределения, из генеральной совокупности,
[09:02.080 --> 09:03.080]  откуда все пришло.
[09:03.080 --> 09:05.440]  Грубо говоря, если вы нарисуете многомерную функцию плотности,
[09:05.440 --> 09:07.000]  то они краски будут торчать там, где у вас плотность
[09:07.000 --> 09:08.000]  вашего распределения очень низкая.
[09:08.000 --> 09:12.800]  Ну, с простейшим случаем для гауссовского распределения,
[09:12.800 --> 09:14.720]  например, слева-справа, вот там, не знаю, за двумя
[09:14.720 --> 09:16.760]  с половиной стигмами, можете считать, что это у вас какие-то
[09:16.760 --> 09:18.920]  аномалии, так обычно и поступают в статистике.
[09:18.920 --> 09:21.880]  Типа, если у нас там три сигма, это вот почти, наверное,
[09:21.880 --> 09:22.880]  хорошо.
[09:22.880 --> 09:26.880]  Там 99,7 нам уже достаточно, в принципе, из ста.
[09:26.880 --> 09:29.880]  Так вот, isolation forest работает ровно на этом принципе.
[09:29.880 --> 09:32.440]  Давайте попытаемся тогда отделить наши объекты от
[09:32.440 --> 09:35.360]  всего остального и попробуем это сделать наименьшим
[09:35.360 --> 09:36.880]  числом разделений.
[09:36.880 --> 09:39.720]  Тогда мы можем это с вами делать каким образом?
[09:39.720 --> 09:45.920]  Ну, вот чтобы эту точку отделить, можем там 1, 2, 3, 4, все, условно
[09:45.920 --> 09:46.920]  как-то так.
[09:46.920 --> 09:51.640]  Но при этом, если вы попытаетесь отделить ее с самого начала,
[09:51.680 --> 09:53.400]  у вас, наверное, вызовет какие-то проблемы, гораздо
[09:53.400 --> 09:57.320]  проще будет выделить вот такую точку, ее вообще там
[09:57.320 --> 10:02.000]  можно одной, одним ихом отделить и так далее.
[10:02.000 --> 10:04.520]  А те точки, которые сидят здесь внутри, их будет отделять
[10:04.520 --> 10:06.920]  гораздо сложнее, потому что их нужно вообще прям
[10:06.920 --> 10:10.680]  очень точно от всех остальных отводить и так далее.
[10:10.680 --> 10:12.400]  Понятная идея, правильно?
[10:12.400 --> 10:15.520]  То есть, чем меньше точка похожа на другие, тем меньше
[10:15.520 --> 10:18.480]  нам в среднем нужно шагов, чтобы ее откинуть от остальных.
[10:18.480 --> 10:20.800]  Плюс там точно также используется идея того, что мы используем
[10:20.840 --> 10:24.440]  случайно признаков подможества и какие-то там бусттрапированные
[10:24.440 --> 10:25.920]  выборки используем и так далее, чтобы деревья были
[10:25.920 --> 10:26.920]  не похожи.
[10:26.920 --> 10:28.920]  Соответственно, тогда мы имеем для каждой точки,
[10:28.920 --> 10:31.040]  мы строим дерево, например, до какой-то глубины.
[10:31.040 --> 10:34.120]  Для каждой точки мы имеем глубину, на которую мы ее
[10:34.120 --> 10:37.160]  выделили в каждом дереве, или что мы ее вообще не выделили
[10:37.160 --> 10:38.160]  отдельно.
[10:38.160 --> 10:39.440]  И тогда вот скор для каждой точки – это что?
[10:39.440 --> 10:41.640]  Это средняя глубина, на которой она была отделена.
[10:41.640 --> 10:43.840]  Тогда у вас получится, если отранжировать точки вот
[10:43.840 --> 10:46.760]  по этой средней глубине, получится, кто-то сидит слева,
[10:46.760 --> 10:48.560]  все остальные где-то там справа или вообще делены
[10:48.560 --> 10:49.560]  не были.
[10:49.640 --> 10:51.360]  А кто-то сидит слева, мы говорим, ну хорошо, это вроде
[10:51.360 --> 10:52.360]  как аномалия.
[10:52.360 --> 10:55.040]  Понятное дело, это тоже не всегда хорошо подходит
[10:55.040 --> 10:56.040]  и так далее.
[10:56.040 --> 11:00.160]  Ну представь себе, я не знаю, там завернутую, вот представь
[11:00.160 --> 11:03.200]  себе, что у вас все точки лежат на эдаком многообразии,
[11:03.200 --> 11:06.960]  вот у вас есть двумерная плоскость, да, там все точки.
[11:06.960 --> 11:09.160]  Потом вы взяли и ее свернули в рулу, вот коврик для йоги
[11:09.160 --> 11:10.160]  мы так сворачиваем.
[11:10.160 --> 11:12.360]  Теперь это все в сверхмерном пространстве.
[11:12.360 --> 11:14.400]  И теперь там все точки, по сути, лежат вот на этой
[11:14.400 --> 11:16.800]  закрученной поверхности, а некоторые между слоями
[11:16.800 --> 11:17.800]  лежат.
[11:17.960 --> 11:20.200]  В точке зрения расстояния до стальных расстояние
[11:20.200 --> 11:21.200]  будет маленькое.
[11:21.200 --> 11:23.520]  В точке зрения отделения тем же самым изволоченным
[11:23.520 --> 11:26.000]  форстом проблем будет много.
[11:26.000 --> 11:28.280]  Тем не менее, это тоже точки являются аномалиями, они
[11:28.280 --> 11:30.640]  как раз не лежат на вот этой общей гиперповерхности.
[11:30.640 --> 11:33.640]  Это нормально, этот метод не универсальный и он не
[11:33.640 --> 11:34.640]  всегда подходит.
[11:34.640 --> 11:36.640]  Просто чтобы вы понимали, это некая церебряная пуля,
[11:36.640 --> 11:38.840]  что вот мы ее запустили и все работает.
[11:38.840 --> 11:41.960]  Но с этим вот сложным многообразием вообще бывают проблемы.
[11:41.960 --> 11:42.960]  Тут вопросы есть?
[11:45.280 --> 11:47.000]  Три, два, один.
[11:47.000 --> 11:48.000]  Хорошо.
[11:48.000 --> 11:50.160]  Ну и в целом, если у вас есть какое-то желание работать
[11:50.160 --> 11:54.000]  с данными, я надеюсь, у вас оно есть, раз вы ходите
[11:54.000 --> 11:56.960]  даже очень на лекции, то изволоченный, ой, изволоченный,
[11:56.960 --> 11:59.560]  random forest это классный такой подход, который можно в
[11:59.560 --> 12:01.880]  копилку себе положить и всегда его использовать.
[12:01.880 --> 12:02.880]  Одно маленькое но.
[12:02.880 --> 12:07.400]  Пожалуйста, будьте осторожны, если вы пытаетесь его использовать
[12:07.400 --> 12:11.320]  а, с данными, которые упорядочены во времени, потому что вам
[12:11.320 --> 12:14.760]  нужно все-таки четко понимать, что у вас валидация работает
[12:14.760 --> 12:15.760]  как.
[12:15.840 --> 12:17.720]  Хорректно с точки зрения течения времени, потому
[12:17.720 --> 12:20.040]  что вот эта ваша аутов бэк оценка может казаться
[12:20.040 --> 12:22.960]  так, что вы оцениваете, по сути, на прошлых точках,
[12:22.960 --> 12:26.120]  а лес обучался на будущем, если у вас точки упорядочены
[12:26.120 --> 12:27.120]  во времени.
[12:27.120 --> 12:30.760]  Это не очень хорошо, потому что у вас распределение
[12:30.760 --> 12:34.560]  х новый, у новый при условии х, у предыдущего.
[12:34.560 --> 12:36.800]  Если вы пытаетесь построить наоборот х, у предыдущий
[12:36.800 --> 12:39.440]  при условии новых, вы по сути что-то другое делаете,
[12:39.440 --> 12:42.560]  вы нарушаете ход событий и ваша модель учится чему-то
[12:42.560 --> 12:43.560]  не тому.
[12:43.560 --> 12:45.800]  Давайте, пожалуйста, всегда, когда данные упорядочены,
[12:45.800 --> 12:48.360]  будьте очень осторожны с этими всеми случайными
[12:48.360 --> 12:50.000]  подвыбреками и так далее.
[12:50.000 --> 12:51.000]  Тут вопрос есть?
[12:53.000 --> 12:56.000]  Все понятно, все просто, едем дальше, правильно?
[12:56.000 --> 12:59.000]  Или ничего не понятно, ничего не просто спасите, помогите?
[13:07.000 --> 13:09.000]  Вы про изволейшн форс сейчас?
[13:09.000 --> 13:10.000]  Или про кого?
[13:11.000 --> 13:12.000]  Вот про эту штуку.
[13:14.000 --> 13:18.000]  Сейчас, я вопрос не поделал, к сожалению.
[13:22.000 --> 13:26.000]  Когда точки изолированы где-то на маймару?
[13:26.000 --> 13:29.000]  Не, не, я говорил, что изволейшн форс работает не всегда,
[13:29.000 --> 13:32.260]  он все равно смотрит на то, что у вас точка находится
[13:32.260 --> 13:34.200]  как-то подаль от всех остальных.
[13:34.200 --> 13:38.200]  Ну, условно, доведите вот этот пример до абсурда,
[13:38.200 --> 13:40.080]  заверните эту спираль очень сильно, у вас все лежит
[13:40.080 --> 13:43.320]  на спирали, а какие-то точки лежат не на спирале
[13:44.080 --> 13:45.080]  между клоев спирали.
[13:45.080 --> 13:47.080]  В точке зрения какой-то закономерности они выбиваются
[13:47.080 --> 13:48.080]  из этой закономерности.
[13:48.080 --> 13:51.080]  В точке зрения расстояний они все равно рядом со всеми.
[13:51.080 --> 13:54.080]  Поэтому этот мет просто не сможет их отделить.
[13:54.080 --> 13:59.080]  Это нормально, он не рассчитан на все, он в среднем работает
[13:59.080 --> 14:00.080]  неплохо.
[14:00.080 --> 14:01.080]  Половили?
[14:01.080 --> 14:04.080]  Так, больше вопросов нет?
[14:04.080 --> 14:05.080]  Едем дальше.
[14:05.080 --> 14:09.080]  Ребят, вы какие-то скучающие, вы меня пугаете, у вас уже
[14:09.080 --> 14:12.480]  все, подкрадывается сессия, середина семестра все сложно
[14:12.640 --> 14:15.640]  или вам просто скучно?
[14:15.640 --> 14:16.640]  Снег не выпал, рано ботать.
[14:16.640 --> 14:21.640]  А что вы тогда-то делаете?
[14:21.640 --> 14:24.640]  Хорошо, ладно.
[14:24.640 --> 14:28.640]  Ну и давайте тогда чуть-чуть поднимем градус настроения
[14:28.640 --> 14:29.640]  в аудитории.
[14:29.640 --> 14:32.640]  Мы с вами чуть-чуть говорили про bias-variance, я решил его
[14:32.640 --> 14:34.640]  вынести на отдельный, наверное, доп-семинар, чтобы мы про
[14:34.640 --> 14:38.640]  это с вами могли более развернуто поговорить.
[14:38.640 --> 14:41.400]  Да, кстати, на всякий случай онлайн-семинары идут по
[14:41.560 --> 14:43.840]  средам и субботам, все, туда можно ходить, это особенно
[14:43.840 --> 14:47.840]  всем тем, кто в онлайне нас смотрит, приходите, подключайтесь,
[14:47.840 --> 14:50.080]  все ссылки есть в чате, каждый раз они публикуют
[14:50.080 --> 14:51.080]  заранее.
[14:51.080 --> 14:55.000]  Ну, давайте по одному.
[14:55.000 --> 14:57.600]  Там будет то же самое, но с маленькими оговорками
[14:57.600 --> 14:58.600]  в каком-то смысле.
[14:58.600 --> 15:01.360]  Материал опорный, то есть ноутбук и тема, они те
[15:01.360 --> 15:02.360]  же самые.
[15:02.360 --> 15:04.880]  Но, во-первых, там учитывается, что у вас уже есть запись
[15:04.880 --> 15:07.400]  этого семинара и вы можете его посмотреть, то есть там,
[15:07.400 --> 15:09.560]  как правило, разбираются больше вопросы, которые
[15:09.640 --> 15:14.240]  идут уже от аудитории, потому что они пришли уже после
[15:14.240 --> 15:16.320]  лекции, возможно, люди что-то смотрели, обдумали и
[15:16.320 --> 15:17.320]  так далее.
[15:17.320 --> 15:19.600]  Во-вторых, все равно каждый преподаватель чуть по-своему
[15:19.600 --> 15:22.560]  подает материал, например, Валерий, он больше расписывает
[15:22.560 --> 15:24.720]  все руками на планшете, потому что у него есть такая
[15:24.720 --> 15:26.960]  возможность, потому что у него рядом все книжки,
[15:26.960 --> 15:29.360]  у него рядом планшет, у него есть время, это дело,
[15:29.360 --> 15:31.600]  круговоря, заранее вопросы, видите, и так далее.
[15:31.600 --> 15:34.640]  Альбина, я думаю, наоборот, будет делать это все попроще
[15:34.640 --> 15:35.640]  и с красивым визуализажкой.
[15:35.640 --> 15:39.480]  То есть я больше стараюсь захватить теорию и практику,
[15:39.480 --> 15:42.000]  чтобы удержать максимальный баланс между тем и другим.
[15:42.000 --> 15:43.920]  То есть там, грубо говоря, больше уклон в одну или
[15:43.920 --> 15:44.920]  в другую сторону.
[15:44.920 --> 15:47.040]  В принципе, можете посмотреть запись, ей будет лучше.
[15:47.040 --> 15:48.040]  Да.
[15:48.040 --> 15:52.000]  Да, мои семинары остаются до конца симметрии.
[15:52.000 --> 15:54.680]  Ну, с оговоркой, что в какой-то из дней я могу оказаться
[15:54.680 --> 15:57.680]  в отъезде, и тогда меня кто-то заменит.
[15:57.680 --> 16:01.800]  Так, хорошо, ладно, и давайте тогда кортенько поговорим
[16:01.800 --> 16:04.000]  про bias variance.
[16:04.000 --> 16:07.560]  Ладно, давайте про него в конце поговорим, я прессую.
[16:07.560 --> 16:10.240]  Итак, тогда перейдем к бустингу, цель наша сегодняшняя.
[16:10.240 --> 16:11.720]  Что такое бустинг?
[16:11.720 --> 16:15.080]  Ну, понятное дело, что бустинг, опять же, от английского
[16:15.080 --> 16:19.200]  boost, как-то усиливать, улучшать, я думаю, вам знакома библиотека
[16:19.200 --> 16:21.200]  boost, правильно?
[16:21.200 --> 16:24.880]  Ну, по крайней мере, те, кто на плюсах писали, и всем
[16:24.880 --> 16:27.400]  незнакомые, те знают, что это такое.
[16:27.400 --> 16:28.400]  Вот.
[16:28.400 --> 16:31.240]  Давайте тогда подумаем, как нам перейти от идеи
[16:31.240 --> 16:33.640]  параллельности, где у нас все независимо, к идее
[16:33.640 --> 16:35.920]  последовательности наших моделей, чтобы каждое следующее
[16:35.920 --> 16:38.840]  делало предыдущую лучше, тогда у нас получается
[16:38.840 --> 16:42.000]  некоторый каскад моделей, которые все вместе работают
[16:42.000 --> 16:43.000]  как единое целое.
[16:43.000 --> 16:44.000]  Прям замечательно.
[16:44.000 --> 16:48.160]  Но возникает вопрос, а как нам обучать каждую следующую
[16:48.160 --> 16:49.160]  модель?
[16:49.160 --> 16:51.320]  То, что в принципе с тем же самым решающим деревом
[16:51.320 --> 16:53.360]  мы с вами можем сказать, что взяли мы решающее
[16:53.360 --> 16:55.680]  дерево, мы его можем просто обучать до упора, оно всю
[16:55.680 --> 16:59.000]  выборку обучающую запомнит, переобучится под нее, ошибок
[16:59.000 --> 17:01.520]  не будет, исправлять нечем.
[17:01.520 --> 17:03.360]  Почему нам нужно строить каскад модель?
[17:03.600 --> 17:05.680]  Также можно заметить, что вот здесь предлагается
[17:05.680 --> 17:09.160]  самая простая версия, просто линейная комбинация моделей.
[17:09.160 --> 17:11.360]  Как вы думаете, если у нас в качестве базовых моделей,
[17:11.360 --> 17:15.160]  вот эти вот, ашиты, h1 и т.д., hn, будет линейная регрессия?
[17:15.160 --> 17:16.160]  Смысл вообще какой-то имеет?
[17:16.160 --> 17:21.040]  Не имеет, потому что линейная комбинация линейных отображений
[17:21.040 --> 17:22.520]  есть линейное отображение.
[17:22.520 --> 17:23.520]  Все, конец.
[17:23.520 --> 17:25.160]  А если логистическая регрессия?
[17:25.160 --> 17:29.600]  И сверчок такой на фоне.
[17:29.600 --> 17:34.600]  Да, и что?
[17:34.600 --> 17:37.600]  Зачем?
[17:37.600 --> 17:43.360]  Ну, если я сюда просто линейные модели засуну, тоже нейросеть
[17:43.360 --> 17:44.360]  можно обозвать.
[17:44.360 --> 17:51.040]  Ну, линейная регрессия тоже однослойная нейросеть
[17:51.040 --> 17:52.040]  без нелинейности.
[17:52.040 --> 17:56.680]  Ладно, хорошо, смотрите, тогда в конце к этому вопросу
[17:56.680 --> 17:58.840]  вернемся, если коротко, да, логистические регрессии
[17:59.320 --> 18:00.320]  можно даже иногда работать.
[18:00.320 --> 18:05.200]  Хорошо, вот вам выборка, вот вам задачка.
[18:05.200 --> 18:08.640]  Давайте-ка попытаемся ее решить с помощью решающих
[18:08.640 --> 18:12.120]  деревьев, а точнее, решающих деревьев, доведенных до
[18:12.120 --> 18:15.360]  иступления, решающих пней, то есть единых if-ов.
[18:15.360 --> 18:17.520]  Каждом модели будет решающий пень, она говорит, налево
[18:17.520 --> 18:19.280]  или направо, вверх или вниз, больше она что делать не
[18:19.280 --> 18:20.280]  умеет.
[18:20.280 --> 18:22.560]  Как видите, у нас выборка явно не делится ни одним
[18:22.560 --> 18:25.080]  пнем поровну, правильно?
[18:25.080 --> 18:27.880]  Давайте попробуем это сделать последовательностью
[18:27.880 --> 18:28.880]  пней.
[18:28.880 --> 18:31.280]  Ну вот, первый пень мы с вами обучаем, ладно, он
[18:31.280 --> 18:32.280]  все три показал.
[18:32.280 --> 18:34.600]  Хорошо, первый пень мы с вами обучаем, отделили
[18:34.600 --> 18:37.920]  правые две точки и сказали, справа все синие, слева
[18:37.920 --> 18:40.440]  тогда логично, что все красно, тогда у нас появились
[18:40.440 --> 18:43.040]  синие точки, вот эти три рите они специально пожирнее
[18:43.040 --> 18:45.240]  нарисованы, на которых ошибка выше.
[18:45.240 --> 18:46.240]  Согласны?
[18:46.240 --> 18:49.840]  То мы можем сделать, мы можем с вами на каждом шаге
[18:49.840 --> 18:52.560]  перевзвешивать нашу обучающую выборку, отдавая больше
[18:52.560 --> 18:55.960]  вес тем объектам, на которых мы совершили ошибку классификации.
[18:55.960 --> 18:58.120]  Например, регрессия абсолют or все то же самое давайте
[18:58.120 --> 18:59.840]  пока с классификаistiей.
[18:59.840 --> 19:03.460]  На этих точках они стали поменьше, вес маленький,
[19:03.460 --> 19:05.020]  потому что там ошибка маленькая.
[19:05.020 --> 19:08.180]  А на точках слева синих там будто очень больш tanoss
[19:08.180 --> 19:10.400]  в monkey孔- rabbit.
[19:10.400 --> 19:11.380]  Давай Давайте строим вторую модель.
[19:11.380 --> 19:13.640]  Тette есть нас bothering теперь все, что слева, DUO
[19:13.640 --> 19:15.640]  красная eleven справа и о�.
[19:15.640 --> 19:17.020]  У жеön тетти, все.
[19:17.020 --> 19:20.740]  Т Netflix businesses, А절, Laterum,那 эти точки тебе McM arrow
[19:20.740 --> 19:22.640]  Salology по universo прибав Isa.
[19:22.640 --> 19:24.560]  estososto было вот эти точки Dutch & Highman с камперами
[19:24.560 --> 19:29.960]  Строим третью модель. Соответственно, те, кто сверху синие, те, кто снизу красные. Хорошо, супер.
[19:29.960 --> 19:34.420]  И в итоге у нас получаются вот такие три модели, которые, если с какими-то весами
[19:34.420 --> 19:38.800]  сложить, возникает вопрос, с какими весами, то у нас получится вот такая нелинейная
[19:38.800 --> 19:44.880]  разделяющая поверхность. По сути, такая ступенчатая. На самом деле, пример абсолютно игрушечный,
[19:44.880 --> 19:52.040]  но в чем суть? Здесь мы с вами только что увидели, что мы с вами из множества простых моделей,
[19:52.040 --> 19:56.240]  вот у нас три решающих пня, которые могут построить вам только линейную разделяющую
[19:56.240 --> 20:02.080]  поверхность, да более того, она должна быть параллельна оси координатной какой-то. Мы из
[20:02.080 --> 20:07.520]  трех моделей, которые очень простые, получили гораздо более сложную модель. То есть три простых
[20:07.520 --> 20:17.200]  модели объединились в одну более сильную. Логично? В этом... что? Ро-1, Ро-2, Ро-3, это ровно те веса,
[20:17.200 --> 20:22.080]  с которыми мы их объединяем. Откуда их брать, мы сейчас тоже с вами поговорим. Ну мы с вами эти
[20:22.080 --> 20:30.280]  классификаторы, вот у нас есть 1, 2, 3, соответственно. Чего? Ну у вас каждый классификатор предсказывает
[20:30.280 --> 20:36.960]  какую-то чиселку, например, вероятность. Или там, в общем случае, логит. Вы строите линейную
[20:36.960 --> 20:41.880]  комбинацию ваших моделей. Линейная комбинация бывает взвешенной. Вот это веса, с которым вы их
[20:41.880 --> 20:48.680]  добавляете. То есть мы с вами можем из трех получить более сложную модель. Но на самом деле
[20:48.680 --> 20:54.040]  гораздо больше, чем из трех. Вы можете 25 штук построить, но тут 3 достаточно. И давайте теперь
[20:54.040 --> 20:58.360]  вернемся вот к этой вот красивой картинке. Вы ее видели на третьей лекции, на четвертой лекции.
[20:58.360 --> 21:04.600]  Что у нас здесь нарисовано? У нас здесь нарисованы все вот эти наши верхние оценки на истинную,
[21:04.600 --> 21:09.720]  в кавычках, функция потерь, на ошибку классификации. И тут у нас с вами сидела еще экспоненциальная
[21:09.720 --> 21:15.120]  функция потери. Вот она. Давайте попробуем ей заодно и воспользоваться. Когда мы на самом деле
[21:15.120 --> 21:20.000]  переизобретем алгоритм, который был предложен в 97-м, что ли, а допустим, он назывался или 99-м,
[21:20.000 --> 21:26.500]  и сделаем что? Давайте скажем, что у нас функция потерь с вами экспоненциальна. E в степени
[21:26.500 --> 21:32.160]  минус маржина. Логично, тогда она у нас справа убывает, нормально слева она растет экспоненциально.
[21:32.160 --> 21:37.040]  Вон она. То есть чем глубже объект оказывается вне своего класса, тем больше у нас ошибка,
[21:37.040 --> 21:42.280]  причем растет экспоненциально. Сразу можно заметить, что так себе результат, экспоненциальный
[21:42.280 --> 21:45.680]  рост вообще не очень хорошее дело. Примерно везде. Все может сломаться, взорваться,
[21:45.680 --> 21:55.000]  вычислительная точность конечная и так далее. Мы сейчас по сути придумывали, смотрите, мы с
[21:55.000 --> 22:00.920]  вами выбрали пока что простую функцию потерь экспоненциальную. Почему? Сейчас увидите. И для
[22:00.920 --> 22:05.760]  нее как раз попробуем вот это перевзвешивание переизобрести. А потом в общем случае уже просто
[22:05.760 --> 22:13.840]  по стопам Фридмана пройдем, который как раз таки... Фридман, джей. Я надеюсь Фридман. Я в начале
[22:13.840 --> 22:21.200]  лекции говорил имя, сейчас я уже запутался. Просто есть еще классный автор, в MIT ведет лекции,
[22:21.200 --> 22:25.280]  на ютубе у него прикольный подкаст, Лекс Фридман. Сын, собственно, выпускника физтеха,
[22:25.280 --> 22:31.520]  если мне не изменяет память. И у него классный подкаст. По-моему, все-таки тоже Фридман. Ладно,
[22:31.520 --> 22:36.960]  давайте скажем, что у нас с вами функция потерь экспоненциальная, то есть е в степени
[22:36.960 --> 22:43.720]  минус марджин. Хорошо, тогда у нас есть с вами над этом шаге наш алгоритм. Это ансамбль из
[22:43.720 --> 22:49.680]  предыдущих первого по Т-большой алгоритмов. Вот он. И вот наша функция потерь. l от y и
[22:49.680 --> 22:56.960]  предсказание f с крышкой от t. Это что? Экспоненты минус y на f с крышкой от t. Согласны? Пока просто
[22:56.960 --> 23:03.040]  марджин написали. Супер. То есть мы можем это расписать как экспонента минус y на сумму вот
[23:03.040 --> 23:08.840]  этих штук. А что мы с вами знаем про экспоненту от суммы? Ну, логично, что если у нас в степени
[23:08.840 --> 23:14.000]  экспонента стоит сумма, мы это можем разделить на произведение экспонентов. Логично. Работаем.
[23:14.000 --> 23:19.640]  И мы отсюда с вами можем совершенно спокойно взять и вытащить последний элемент. Давайте скажем,
[23:19.640 --> 23:24.400]  что мы сейчас находимся над этом шаге, и у нас есть все модели, кроме последней, обучены. То есть
[23:24.400 --> 23:29.160]  у нас уже есть с первой по Т-большой у нас один модель, который мы обучили, они отлиты в бетоне,
[23:29.160 --> 23:34.840]  их трогать нельзя. И есть последняя hT-большая модель, которую мы сейчас должны обучить. Ну,
[23:34.840 --> 23:39.720]  собственно, вот она. И получается, что на шаге t вот это у нас константа, у нас ничего не
[23:39.720 --> 23:43.840]  меняется. И мы должны минимизировать ровно вот эту штуку. Потому что все предыдущие модели уже
[23:43.840 --> 23:48.560]  обучены в ансамбле. Ну, замечательно. Но раз это константа, получается, что у нас для каждого
[23:48.560 --> 23:54.640]  объекта функция потерь приобретает вот такой вид. Возникает вопрос, зачем нам здесь экспоненциальная
[23:54.640 --> 23:58.840]  функция потерь? Потому что по ней краске очевидно, что такое взвешивание объекта. Смотрите, вот ваш
[23:58.840 --> 24:04.520]  вес. На каждом шаге ошибка нашего алгоритма, который был построен до текущего шага, это и есть
[24:04.520 --> 24:10.120]  вес объекта. Если объект был классифицирован правильно, ошибка у него маленькая, вес маленький. Если
[24:10.120 --> 24:15.080]  ошибка большая, то есть объект был классифицирован неправильно, мы хотим на нем учиться. Понятно,
[24:15.080 --> 24:21.160]  что происходит? Понятно, зачем здесь экспонента? Потому что, если бы не был экспонент, мы не могли
[24:21.160 --> 24:27.880]  вот так факторизоваться и выделить отдельный член. Ну, назвали это дело Adaboost, от слова
[24:27.880 --> 24:33.480]  адаптивный бустинг, adaptive boosting. И вроде как казалось, что вот классно. Мы придумали, как
[24:33.480 --> 24:38.360]  работать, но у Adaboost была куча проблем, он переобучался в лед. У него вот эта экспонента
[24:38.360 --> 24:43.760]  мешала вычислительно всем процессам, и далеко не всегда экспоненциальная функция потерь хорошо
[24:43.760 --> 24:47.640]  подходила. В том числе у нас с вами есть не только задача классифицироваться, а еще есть задача
[24:47.640 --> 24:53.760]  регрессить. Там не очень понятно, как сходу это исправить. Тоже можно, но тем не менее. Но на самом
[24:53.760 --> 24:59.760]  деле в 90-х было множество различных подходов. Adaboost, Brownboost еще был, многие там называли
[24:59.760 --> 25:05.880]  фамилия автора Boost, и так далее. А потом в 2001 году к краске пришел градиентный бустинг, который
[25:05.880 --> 25:11.240]  показал, что все вот это вот многообразие, это на самом деле частный случай общей истории градиентного
[25:11.240 --> 25:18.040]  бустинга. И с ним давайте краски сейчас и попробуем разобраться. Во-первых, я сейчас сразу вас немножко
[25:18.040 --> 25:23.120]  попрошу, наверное, напрячь внимание. Сейчас будет немного больше математики, чем на предыдущих
[25:23.120 --> 25:30.280]  слайдах. И здесь будет, наверное, тема, которая в первом семестре кажется одной из наиболее
[25:30.280 --> 25:36.440]  сложных, наряду с SVM, который часто вызывает вопросы. В этом году мы оттуда почти полностью
[25:36.440 --> 25:41.200]  выполнили двойственную задачу, поэтому он стал кажется очень простым, но тем не менее. И вместе
[25:41.200 --> 25:46.080]  с, не знаю, кому-то первой лекции сложно там всякие правдоподобия вылазить, кому-то всякие
[25:46.080 --> 25:51.280]  нейронки становятся сложными. Короче, давайте сейчас напряжемся на 10-15 минут, все осознаем. Итак,
[25:51.280 --> 25:57.640]  шаг номер один. Как всегда, постановка задачи. У нас с вами есть выборка, пара x, y, причем абсолютно
[25:57.640 --> 26:02.840]  все равно регрессия, классификация, что-нибудь еще там придумайте, неважно. У вас есть множество
[26:02.840 --> 26:08.520]  пар объект-ответ и у вас есть функция потерь. Мы хотели бы с вами найти такую модель, которая
[26:08.520 --> 26:15.040]  достигает оптимума на данной выборке, минимума, нашей функции потерь или что-то же самое,
[26:15.040 --> 26:19.600]  минимума эмпирического риска. Ну, что мы хотим? Мы на самом деле не имеем доступ ко всей
[26:19.600 --> 26:23.360]  генеральной совокупности, ко всему распределению, откуда пришли наши данные. У нас есть только наша
[26:23.360 --> 26:27.200]  выборка, поэтому вместо нашего вот этого вот ожидания по факту мы с вами будем брать что?
[26:27.200 --> 26:31.640]  Среднее по всей выборке. То есть мы хотим получить модель, которая в среднем получает наименьшую
[26:31.640 --> 26:37.200]  ошибку. И пусть у нас с вами не просто какое-то семейство моделей, из которых мы ищем оптимальную,
[26:37.200 --> 26:42.240]  потому что среди всех, типа среди линейных, метрических, деревьев, их ансамблей,
[26:42.240 --> 26:46.680]  непараметрических и так далее сложно найти оптимальную, надо все перебирать. Давайте пусть
[26:46.680 --> 26:50.680]  будет какая-то параметрическая патрика моделей, параметрическая семейство моделей. Что это значит?
[26:50.680 --> 26:56.320]  Что модель параметризуется вот этим вектором тета, неважно что это, это вектор параметров
[26:56.320 --> 27:03.120]  нашей линейной регрессии, множество параметров неровной сети, просто множество плитов вот этих
[27:03.120 --> 27:07.920]  трешолдов и фичей для дерева или что-нибудь еще. Главное, что мы вот эту запихнули все наши
[27:07.920 --> 27:12.920]  параметры, которые мы можем каким-то там образом менять. Градиентным, неградиентным, все нормально.
[27:12.920 --> 27:36.080]  Тут понятно? Вот здесь? Ну, я так скажу, здесь это скорее... Хорошо, здесь, наверное, стоит сказать,
[27:36.080 --> 27:40.680]  что х приходит из какой-то генерально-совокупности и здесь мы, собственно, по нему мат ожидаем. Короче,
[27:40.680 --> 27:47.080]  вот это просто выкиньте и все. Выкиньте второй член. Грубо говоря, это эквалентные записи в том
[27:47.080 --> 27:51.200]  смысле, что у нас с вами х и у приходят из вот этого нашего распределения, откуда пришли данные.
[27:51.200 --> 27:56.280]  Тут просто мы явно это написали. Тут, наверное, да, не совсем корректно написано. Наверное,
[27:56.280 --> 28:02.720]  надо поправить. f это краска. Смотрите, мы говорим, у нас вот что такое оптимальная модель,
[28:02.720 --> 28:08.520]  а которая достигает минимума мат ожидаемости нашей функции потерь. Далее мы говорим, нам ее надо
[28:08.520 --> 28:12.960]  как-то найти, поэтому пусть наша f будет из код параметрических 8s. То есть мы можем каким-то
[28:12.960 --> 28:17.160]  векторам параметров ее определить. Пока у нас ее нет. Пока мы договорились, что...
[28:17.160 --> 28:30.480]  f от... Ну, функция потерь от y и от нашего предсказания. Предсказание. Ну, смотрите,
[28:30.480 --> 28:34.360]  функция потерь у вас обычно что считает? У вас есть ваше истинное значение и предсказанное
[28:34.360 --> 28:39.400]  значение? Средне квадратичная ошибка, разница квадрат, квадрат разницы в течение,
[28:39.400 --> 28:46.680]  log loss, p log q и так далее. То есть это ваша функция потерь. Половили, нет?
[28:46.680 --> 28:56.600]  Так, коллеги, кто-нибудь вообще записывает за вашим вопросом, я в тот раз еще на самом деле
[28:56.600 --> 29:02.600]  просил, если вы вот такие штуки видите, вот да, здесь надо написать f от x. Так будет лучше.
[29:02.600 --> 29:06.360]  Может, пожалуйста, после занятия, если кто-то это лагирует, кидать в чат,
[29:06.360 --> 29:11.080]  тогда это будет все оперативно поправлено. Я, к сожалению, забываю через два с половиной
[29:11.080 --> 29:18.800]  часа, после того, как я это услышал. Спасибо. Хорошо. Опечатки вроде починили. Еще что-то
[29:18.800 --> 29:31.480]  понятно? Не понятно точнее. Вроде все окей. Хорошо. Давайте тогда разобьем нашу вот эту
[29:31.480 --> 29:37.240]  текущую модель краски в том смысле, что мы знаем, что у нас с вами последовательность наших
[29:37.240 --> 29:41.960]  моделек, классикаторов, например, или регрессоров, последовательность алгоритмов, и мы знаем,
[29:41.960 --> 29:49.240]  что на текущем шаге у нас есть, допустим, с 0 по t минус 1 уже обученные все. Вот мы их обучили
[29:49.240 --> 29:53.800]  каким-то образом, не важно, но вспомните мат индукции. То есть тут мы сейчас говорим про шаг
[29:53.800 --> 29:59.800]  индукции. У нас уже что-то обучено, мы хотим еще одну модель дообучить, номер t. То есть нам,
[29:59.800 --> 30:06.640]  по сути, надо для модели на шаге t, ρt это ее вес и θt это ее параметры, решить вот такую задачу
[30:06.640 --> 30:12.880]  минимизации. У нас есть y, истинный, который у нас есть, и собственно f от x, причем с крышкой вот
[30:12.880 --> 30:19.080]  все, что было раньше, плюс ρ на h от x. Вот это вот h от xθ это наша новая модель, которую мы хотим
[30:19.080 --> 30:31.440]  добавить. Чего? Говорите погромче, пожалуйста, я вас плохо слышу. Еще раз, мы с вами, вот это было
[30:31.440 --> 30:36.000]  в общем случае написано, это работает для всего. Теперь мы говорим, давайте будем строить модели
[30:36.000 --> 30:40.480]  последовательно. Каждое следующее будет улучшать результат предыдущего ансамбля. Значит у нас
[30:40.480 --> 30:47.600]  модель общая, вот алгоритм, ансамбль, давайте вот так называть, представим в виде суммы наших
[30:47.600 --> 30:53.720]  моделей. Каждое следующее, это все модели вместе что-то предсказали, получили результат. Согласны?
[30:53.720 --> 31:01.200]  Вот мы с вами пока что прошли t-1 шаг, и у нас есть вот эта вот f с крышкой, она уже каким-то там
[31:01.200 --> 31:05.480]  образом обучена, пока не важно каким, мы собственно сейчас с вами на шаге индукции и покажем как мы ее
[31:05.480 --> 31:10.720]  обучили. Соответственно на новом шаге нам нужно дообучить еще один кусочек ансамбля, вот он.
[31:10.720 --> 31:21.520]  Все согласны? Что? Базу индукции, сейчас мы до нее дойдем. Давайте сначала шаг индукции и пишем потом
[31:21.520 --> 31:27.320]  базу, окей? Если у нас уже есть с вами обученная часть ансамбля, как нам к нему добавить еще один
[31:27.320 --> 31:34.320]  элемент? Вот что мы сейчас делаем, понятно? Хорошо, тогда мы с вами можем разбить в наши функции потерь
[31:34.320 --> 31:41.680]  вот это на две части, наше текущее предсказание и по сути поправка к тому что есть. Верно? Все вот это
[31:41.680 --> 31:49.720]  понимают, правильно? Тут, тут, тут, тут. Нормально. И тогда краски оптимальная, наша tt модель вот f с крышкой t,
[31:49.720 --> 31:57.200]  которую мы должны сюда добавить, это будет ρt на hхθt. Нам надо вот этот дел найти. Теперь внимание в
[31:57.200 --> 32:04.000]  вопрос, как нам найти ρt и tt? Это основной вопрос. И здесь вступают краски в силу гридентный бустинг.
[32:04.000 --> 32:09.280]  Вопрос вы видите уже на экране. А что если бы мы могли использовать гридентный спуск не в
[32:09.280 --> 32:16.600]  пространстве как-то там параметров и так далее, а в пространстве моделей? Что это значит? Помните,
[32:16.600 --> 32:20.680]  вот эту картинку она у нас была на втором занятии, если исключить занятия по повторению линии на
[32:20.680 --> 32:26.680]  регрессии. Это вот у нас линии уровня или же эквипутенциальной поверхности, тут оптимум,
[32:26.680 --> 32:31.680]  вот наша начальная инициализация, вот первый гридентный шаг, второй, третий и так далее. Вот мы
[32:31.680 --> 32:37.840]  куда-то там шагаем. Но мы с вами привыкли это делать где? В пространстве параметров. У нас есть
[32:37.840 --> 32:42.920]  гиперповерхности эквипутенциальной поверхности функции потерь и мы с вами шагаем туда-сюда,
[32:42.920 --> 32:48.480]  пытаясь понизить значение функции потерь. Верно? А кто сказал, что только в пространстве параметров
[32:48.480 --> 32:53.320]  мы можем это делать? Мы это можем делать в пространстве модели и вообще где угодно. На самом деле,
[32:53.320 --> 32:57.720]  я поэтому говорил, что если у вас был функкан, вам будет проще. У вас есть какое-то пространство,
[32:57.720 --> 33:02.520]  а потом есть какие-то эквапутенциальные поверхности, и в котором, допустим, каждая точка
[33:02.520 --> 33:09.160]  соответствует какому-то отображению. В модели это и есть отображение. Можем так сделать? Можем.
[33:09.160 --> 33:15.200]  Остается вопрос, где нам взять вот этот самый гридентный спуск? И вся красота в том, что нам
[33:15.200 --> 33:20.360]  достаточно с вами одного единственного допущения. Давайте договоримся, что вот эта функция потерь,
[33:20.360 --> 33:30.920]  которую мы с вами выбрали, дифференцируема. Хорошо? Чего? Да, например. Вот. Каждая модель — это,
[33:30.920 --> 33:34.120]  по сути, некоторый оператор, сдающий отображение из пространства исходного,
[33:34.120 --> 33:40.760]  в пространство таргетов. Идет? Все, супер. Поэтому я и говорил, что функкан здесь полезен,
[33:40.760 --> 33:44.520]  просто чтобы вы понимали, что пространство операторов тоже можно задать, оно тоже существует.
[33:44.520 --> 33:49.800]  Хоть и не столь привычно. Так вот, единственное ограничение. Пусть вот эта функция потерь
[33:49.800 --> 33:54.880]  дифференцируема. Договорились? Но, я думаю, слово «градиентный бустинг», вот «градиентный» намекает,
[33:54.880 --> 34:02.160]  что где-то там будут градиенты. Нам нужны градиенты. Вот. Функция L — дифференцируема. Что мы тогда
[34:02.160 --> 34:07.720]  можем видеть? Ну, тогда давайте шагом. Вот наша текущая с вами константа, которая предсказывает
[34:07.720 --> 34:13.760]  на каждом шаге. Она у нас есть. Тогда теперь мы с вами что можем сделать? На каждом объекте из обучающей
[34:13.760 --> 34:22.840]  выборки у нас с вами есть истинный ответ. Правильно? И у нас с вами есть что? У нас с вами есть какое-то
[34:22.840 --> 34:29.440]  предсказание нашей текущей модели. Правильно? Ну, давайте я назад лисну. Вот оно. Предсказание
[34:29.440 --> 34:34.160]  нашей текущей модели для каждого х тоже есть. Она же у нас уже обучена. Плюс у нас есть какое-то
[34:35.160 --> 34:45.440]  х. Правильно? Вот давайте вот эту штуку обзавем просто РИтой. В данном случае где-то он у нас. Вот. А, нет. Вот Df от
[34:45.440 --> 34:54.440]  х. Короче, вот это наша добавка. Роа на х тета — это краткий f. Наговорились? Согласитесь, что если вот эта
[34:54.440 --> 35:01.080]  штука дифференцируема, L от Y — f плюс какая-то поправка. Мы же можем с вами по этой поправке
[35:01.080 --> 35:07.080]  продиференцироваться. Это проф — наш свободный член. Вспоминаем производную сложной функции DL по D
[35:07.080 --> 35:15.040]  вот эта поправка. Это что? Это DL по D сумма и D сумма по вот этой штуке. Согласились все? Все согласны. И
[35:15.040 --> 35:19.280]  внезапно у нас получается, что для каждого объекта обучающей выборки, обратите внимание,
[35:19.280 --> 35:25.440]  для каждой пары х и у мы можем считать с вами функцию потерь, предсказание и соответственно
[35:25.440 --> 35:31.320]  значение производной. И получается, что для каждого объекта из нашей обучающей выборки,
[35:31.320 --> 35:37.280]  для каждой пары, у нас есть поправка. Причем это не просто поправка, это ровно то значение градиента,
[35:37.280 --> 35:41.920]  которое показывается. Вот настолько надо здесь поменять предсказание, чтобы стало лучше. Потому
[35:41.920 --> 35:46.280]  что что такое градиент? Направление нескорейшего возрастания функции. Антиградиентное направление
[35:46.280 --> 35:50.760]  нескорейшего убывания. То есть ровно вот так мы должны поменять предсказание на этом объекте,
[35:50.760 --> 35:59.960]  над этом шаге нашего ансамбля, чтобы ошибка ансамбля стала меньше. Вот это понятно? Сейчас
[35:59.960 --> 36:05.040]  еще раз проговорю. То есть мы с вами посчитали производную функцию потерь по предсказанию нашей
[36:05.040 --> 36:12.160]  новой модели, которую мы пока не обучили. Это наш свободный член. Вы ловливаете? Давайте попробую
[36:12.160 --> 36:28.760]  это на доске тоже написать. Вот у вас по сути L. Белый. L у вас зависит от Y этого, потом F от X этого.
[36:28.760 --> 36:40.380]  Вот с крышкой. Это все то, что мы сейчас обучили. Давайте. Вот. Да. X у нас есть. Вот. Взяли один
[36:40.380 --> 36:45.660]  объект, обучающий выборки. Только один. Взяли и с ним работаем. Все. Соответственно, у нас вот
[36:45.660 --> 36:53.820]  эта штука есть. Правильно? Давайте вот эту штуку я просто для удобства обзову кси. Хорошо? Тогда мы
[36:53.820 --> 37:08.860]  говорим с вами DL по DL от Y и кси по D кси и опять же. Чему-то равна. Согласны? То есть это вот та
[37:08.860 --> 37:14.020]  величина, это наш градиент. Это то, как нам надо поменять предсказание, чтобы стало лучше.
[37:14.020 --> 37:20.060]  С этим все согласны? Ну так мы же с вами только что говорили, что мы будем наше предсказание
[37:20.060 --> 37:28.020]  менять ровно вот на эту величину. А вот нам примерно что надо его менять. Все. Мы с вами
[37:28.020 --> 37:33.340]  получили оценку градиента в точке, то есть насколько нам нужно поменять предсказание на каждом объекте.
[37:33.340 --> 37:39.460]  А теперь финтушами. Мы для каждой точки из нашей обучающей выборки или, грубо говоря,
[37:39.460 --> 37:44.260]  для каждой точки в признаковом пространстве, до которой мы знаем значение таргета, мы понимаем,
[37:44.260 --> 37:50.100]  как нужно исправить предсказание. Вот наш градиент для градиент LOS по предсказанию,
[37:50.100 --> 37:55.900]  то есть надо идти по антиградиенту. Верно? Так давайте на него и обучаться. Давайте наша
[37:55.900 --> 38:01.340]  новая модель будет апроксимировать не ответ сам по себе, а антиградиент функции ошибки по
[38:01.340 --> 38:06.780]  предыдущим предсказаниям. То есть наша новая модель, мы когда добавляем новую модель,
[38:06.780 --> 38:11.260]  мы по сути делаем градиентный шаг пространств модели. У нас была какая-то модель, мы добавили
[38:11.260 --> 38:19.020]  новую, которая апроксимирует градиент. Уловили, что произошло? Мы не параметры поменяли, у нас
[38:19.020 --> 38:29.180]  новая модель апроксимирует градиенты, вы ее туда добавляете. Все. Ну смотрите, у вас модель,
[38:29.180 --> 38:35.820]  вот ваш ансамбль, это последовательность моделей. Вот сумма. На новом шаге вы к ней добавляете еще
[38:35.820 --> 38:41.540]  одну модель. Вот это все у вас уже обучено, оно уже где-то есть, оно уже прибито гвоздями. Все,
[38:41.540 --> 38:46.900]  вы его трогать не можете. На N плюс первом шаге вы говорите, хочу найти новую добавку к моему
[38:46.900 --> 38:53.140]  ансамблю, чтобы стало лучше. Как мне эту добавку обучать? Мы ищем антиградиент ошибки по текущим
[38:53.140 --> 38:57.900]  предсказаниям ансамбля, понимаем, что надо подменять предсказания ансамбля вот так и на них
[38:57.900 --> 39:04.740]  учимся. То есть, наш новый элемент ансамбля апроксимирует антиградиент. Предыдущих ошибок
[39:04.740 --> 39:14.940]  ансамбля. Наш новый элемент, ну вот смотрите, вот наша h х тета, вот он ваш, новая модель,
[39:14.940 --> 39:22.020]  которая пока не обучена. Еще раз, вот ваш текущий ансамбль, вот вы к нему добавите
[39:22.020 --> 39:28.380]  РО на h х тета. РО это learning rate просто, это градиентный шаг родмирового. Все, теперь вы
[39:28.380 --> 39:34.580]  нашли антиградиент, говорите, вот мой антиградиент, вот ровно на него вы вашу новую модель,
[39:34.580 --> 39:43.820]  поправку, будете обучать. Кто? Ашмалая, это как раз, смотрите, ашмалая это ваш алгоритм,
[39:43.820 --> 39:48.220]  который вы будете учить. Какой он будет, от вас абсолютно зависит, может быть, линейная модель,
[39:48.220 --> 39:55.220]  кайн и так далее, по барабану. Нет, аш это просто модель из какого-то семейства. Единственное,
[39:55.220 --> 40:00.380]  что вам нужно, вам нужно уметь аш обучать по выборке. То есть, если у вас есть множество пар
[40:00.380 --> 40:11.140]  объектов и ответы, вы можете по ним настроить аш. Ну да, то есть, аш приходит из какого-то
[40:11.140 --> 40:15.020]  параметрического семейства. То есть, вам не надо ее искать везде, например, вы сказали, аш это
[40:15.020 --> 40:20.000]  решающие деревья. Все, тогда у вас для каждого объекта есть х, признаковое описание, есть новый
[40:20.000 --> 40:27.220]  ритый, который оценка антиградиента, на парах х и ритый, вы обучаете ваше новое дерево. Все.
[40:27.220 --> 40:34.620]  Почему именно квадратичное? Смотрите, потому что по сути у вас теперь от задачи регрессия,
[40:34.620 --> 40:39.740]  правильно? И вы сюда можете абсолютно на самом деле ошибку запихнуть. Квадратичные краски потому,
[40:39.740 --> 40:44.060]  что когда вы минимизируете квадрат, вы получаете именно оценку среднего, и у вас там под капотом
[40:44.060 --> 40:54.980]  защиты гауссового распределения. Все. Бинго. Абсолютно неважно, если у вас функция потерь есть
[40:54.980 --> 40:59.460]  дифференцируемая, регрессия, классификация, какой-нибудь там ранжирование придумайте, вам вообще все равно.
[40:59.460 --> 41:04.980]  Есть дифференцированная функция потерь, вы знаете, как сделать ее лучше. Все. Вам вообще не
[41:04.980 --> 41:10.340]  дифференцируемо. Более того, у вас изначально вот эти вот модели, опять же, в чем вся прелесть
[41:10.340 --> 41:13.860]  градиентного бустинга, они могут быть недифференцируемыми. Тот же самый градиентный
[41:13.860 --> 41:18.220]  бустинг над решающими деревьями. Деревья сами по себе градиентной оптимизации не поддаются,
[41:18.220 --> 41:22.940]  каждое дерево это кусочек постоянной функции. Но вы их можете использовать для опроксимации
[41:22.940 --> 41:28.540]  градиентов в каждой точке пространства. Все. То есть у вас, получается, недифференцируемые модели
[41:28.540 --> 41:34.820]  сидят как базовые алгоритмы в ансамбле типа градиентный бустинг. Да.
[41:34.820 --> 41:48.900]  Это не очень логично. А дальше что? Почему это не очень логично?
[41:48.900 --> 41:58.380]  Смотрите, давайте я это напишу тогда на доске. Смотрите.
[41:58.380 --> 42:06.540]  Не, здесь у нас любые модели абсолютно. То есть про деревья я просто говорю,
[42:06.540 --> 42:10.140]  что даже дерево и даже какой-нибудь KNN здесь будет работать. Смотрите,
[42:10.140 --> 42:17.540]  у вас есть текущая модель, вот F от X, крышкой. Это ваша текущая модель. Мы говорим,
[42:17.540 --> 42:23.300]  хочу получить новую модель, которая лучше. Вот это у меня на шаге T, мы хотим получить F,
[42:23.300 --> 42:30.540]  T плюс 1 с крышкой опять же от X. Тогда у нас здесь есть два допущения. Первое, мы говорим,
[42:30.540 --> 42:39.740]  что F, T плюс 1 с крышкой, это что? Это F, T с крышкой от X плюс какое-то РО на какое-то H от X. Ну,
[42:39.740 --> 42:47.780]  с параметрами тета. Вот, чтобы как там было. Согласны? Это мы пока сказали. И теперь, соответственно,
[42:47.780 --> 42:58.380]  вот это наша первая посылка. И второе, мы говорим, как нам лучше всего исправить ошибку нашего
[42:58.380 --> 43:09.580]  текущего ансамбля. Ну, давайте посчитаем. DL от Y и F с крышкой T от X по кому? Ну, даже может
[43:09.580 --> 43:18.260]  T плюс 1 по D краске F с крышкой T плюс 1 от X. Все, вы говорите, как мне лучше всего исправить
[43:18.260 --> 43:23.660]  предсказания? Чистая математика, вот она вам. Вы поняли, как лучше всего исправить предсказания?
[43:23.660 --> 43:29.820]  Ну, теперь добавьте вот вам ваш, по сути, шаг исправления предсказаний. То есть, вроде как
[43:29.820 --> 43:34.300]  логично. Мы хотим оценить, насколько нам нужно подвинуть предсказания на каждом объекте, чтобы
[43:34.300 --> 43:57.620]  стало лучше. Чего? Я понял. Ой, хорошо, давайте посчитаем. У вас L считается от Y и от F с крышкой
[43:57.620 --> 44:08.820]  T плюс РО, H, X и T. Вот с этим согласны? Давайте я для начала вот это обозначу просто как
[44:08.820 --> 44:18.100]  кси, просто чтобы было удобнее писать. Тогда DL Y кси по D кси чему равно? Ну, вот чему-то там равно,
[44:18.100 --> 44:23.660]  мы не знаем, это от функции потери зависит. Хорошо, но мы это дифференцировать умеем. Это у нас есть.
[44:23.660 --> 44:31.980]  То можем дальше сделать. Это у нас есть галка. Дальше мы с вами можем посчитать D кси по кому?
[44:31.980 --> 44:41.820]  По D РО. Ой, ну D тета я забежал вперед. По D РО можем продиференцировать? Чему это будет равно?
[44:41.820 --> 44:59.260]  H X тета. Все, хорошо. Соответственно D кси по D тета. Можем посчитать? Это соответственно будет D кси по D
[44:59.260 --> 45:23.900]  H на D H по D тета. И тут еще РО будет сидеть. Согласны? Вот вы градиенты посчитали. Да,
[45:23.980 --> 45:36.700]  спасибо. Я его просто потом явно написал. Мысли зачем? Не, в смысле, вот вам все,
[45:36.700 --> 45:45.740]  вот ваши все градиенты, откуда они взялись. Вот вы отсюда получили по сути оценку D тета или
[45:45.740 --> 45:53.780]  просто вам достаточно D кси по D H нарисовать, вы знаете чему оно будет равно? У вас будет
[45:53.820 --> 46:02.980]  вот эта штука, умноженная на РО, это ваша D кси по D H. Это то, как надо настроить H. Все. Чего непонятно,
[46:02.980 --> 46:21.380]  ребят, я не понимаю. Ну, ну да. Не, погодите, я вам всего лишь показываю что. Вот у вас эта штука,
[46:21.380 --> 46:28.660]  F от X, правильно? Она в себя включает константную часть, ваше текущее предсказание, вы по константе
[46:28.660 --> 46:33.540]  не можете продиференцировать, она константа. Плюс ваше новое предсказание, которое вы краски
[46:33.540 --> 46:46.540]  будете на него обучать. Так, что я сейчас делал, по шагам. Первое, у нас есть текущая модель,
[46:46.700 --> 46:54.140]  которая предсказывает что-то, правильно? Вот. Сейчас она уже, вот эта часть, она обучена. Вот скажите
[46:54.140 --> 47:00.820]  мне, кто-нибудь может продиференцировать лост по константному предсказанию? Оно константное,
[47:00.820 --> 47:09.940]  вы его поменять не можете. Есть смысл дифференцироваться по константе? Не очень. Хорошо. То есть мы
[47:09.940 --> 47:14.260]  говорим, хорошо, давайте в целом по предсказаниям, считая, что оно может линейно как-то меняться,
[47:14.260 --> 47:18.900]  продиференцируемся. Вот, я только что это писал. Вот, это у вас констант, а это у вас туда краски
[47:18.900 --> 47:26.100]  входят каким-то образом. По нему продиференцируемся и получаем, что вот это та же производная. Потому
[47:26.100 --> 47:31.500]  что у нас есть член, который свободный, он обладает крепким свободным, мы его менять можем. Мы на
[47:31.500 --> 47:46.860]  него краски обучаем кого? Нашу новую модель, которую у нас есть. Есть модель, она делает предсказания.
[47:46.860 --> 47:52.700]  Мы, у вас, вы берете предсказания модели как просто какую-то свободную переменную,
[47:52.700 --> 48:04.180]  кси. Все, вот она. Дл по дкси. Можете посчитать? Можете. В каком месте я вас запутал, ребят?
[48:13.060 --> 48:19.740]  Хорошо, ладно, это было лишнее. Ну, в плане, сейчас ладо для прав, для понимания, скорее вредит. Вы правы.
[48:20.300 --> 48:25.300]  Я всего лишь хотел показать, что это все дифференцируемая операция. То есть, у вас
[48:25.300 --> 48:28.640]  нет того, что вы дифференцируетесь по ком-то константному предсказанию, потому
[48:28.640 --> 48:31.860]  что это смысла не имеет, вы не можете по константе дифференцироваться. У вас здесь
[48:31.860 --> 48:36.620]  ровно зашит свободный член, поэтому вы и имеем право дифференцироваться. Все. Мы, по сути,
[48:36.620 --> 48:41.780]  для простоты понимания еще раз, у вас есть текущее предсказание, обозначаем его свободным
[48:41.780 --> 48:46.380]  членом, дифференцируемся по свободному члену и получаем как надо изменить текущие предсказания,
[48:46.380 --> 48:54.120]  обучаем новую модель ровно на то, как надо его изменить. Логично что? Логично то, что у нас
[48:54.120 --> 48:59.320]  скорее всего модель не сможет идеально аппроксимировать градиент с каждой точки. Поэтому
[48:59.320 --> 49:04.160]  мы с вами не получим идеальную поправку, мы получим какое-то движение в сторону антиградиента
[49:04.160 --> 49:08.160]  ровно как здесь у нас указано. Мы за один шаг не пойдем в оптимум. В том числе потому, что чтобы
[49:08.160 --> 49:12.160]  попасть даже в случае выпуклой задачи в оптимум за один шаг, нам нужны методы второго порядка,
[49:12.160 --> 49:16.980]  применять метод ньютон-равсна и так далее. А егебьяны считать эгесианы никто не любит,
[49:16.980 --> 49:24.740]  особенно на больших водах. Нет, это все для любой задачи, смотрите.
[49:24.740 --> 49:40.780]  Именно поэтому я вам уже пятую лекцию говорю, четвертую. В задачи классикации мы предсказываем
[49:40.780 --> 49:46.520]  вероятность. Все, забудьте про метки класса, это слишком детский сет. У нас там вектор вероятности,
[49:46.520 --> 50:03.920]  по нему можно совершенно дифференцировать. Так, еще вопросы есть? Да. Сейчас, смотрите, мы должны
[50:03.920 --> 50:11.300]  опроксимировать наши антиградиенты. Это уже просто какой-то вектор, в общем случае, в каком-то
[50:11.300 --> 50:16.260]  пространстве, правильно? Если мы решаем задачу, где надо опроксимировать вектор, это какая задача?
[50:16.260 --> 50:24.020]  Регрессии. Регрессию можно решать с помощью минимизации МСЕ, МАЕ, МАЕ и кучу других там всяких
[50:24.020 --> 50:29.140]  функций ошибок. Когда мы с вами минимизируем средне квадратичную ошибку, мы на самом деле делаем
[50:29.140 --> 50:35.240]  оценку максимального правдоподобия, если у нас гауссовский прайор. Ну, это из статов просто
[50:35.240 --> 50:40.600]  вытекает. Оценка максимального правдоподобия при предположении, что у нас нормальное
[50:40.600 --> 50:45.280]  распределение, это называется эквивалентом минимизации средней квадратичной ошибки. Вот,
[50:45.280 --> 50:48.680]  поэтому я и сказал, что по сути это говорит, что у нас где-то там гауссовский прайор. Запихнуть
[50:48.680 --> 51:10.060]  сюда МАЕ, у вас будет лапласовский прайор. Так, у вас тоже был вопрос. Вот, смотрите,
[51:10.060 --> 51:14.100]  что такое РО, давайте вспомним. РО это просто наш размер градиентного шага. Помните, мы с вами,
[51:14.100 --> 51:20.100]  когда говорили про градиентный спуск, у нас там была какая-то тета, это на размер градиента. Вот,
[51:20.100 --> 51:25.940]  у нас функция наша с вами, она умная, она апроксимирует чисто наш градиент. Всё, но она его
[51:25.940 --> 51:30.500]  апроксимирует, она апроксимирует каким-то неидеальным способом. Поэтому теперь мы, зная, что мы
[51:30.500 --> 51:36.820]  шагаем вон туда, мы задаем вопрос, а как именно нам нужно шагнуть в этом направлении оптимально?
[51:36.820 --> 51:41.260]  По сути, мы на каждом шаге находим оптимальный градиентный шаг. Мы выбрали направление, ну и,
[51:41.260 --> 51:44.740]  по сути, норма вектора, тут тоже, на самом деле, как это есть. Но мы смотрим, не надо ли нам его
[51:44.740 --> 51:50.500]  растянуть, только и всего. Возможно, у вас будет отличная оптимальная величина этого шага от 1,
[51:50.500 --> 51:58.460]  тогда вы, соответственно, лучше шагнете в этом направлении. Всё. Да, смотрите, вы сначала
[51:58.460 --> 52:03.780]  апроксимируете антиградиент каким-то там образом, а потом, вы уже зная, что идем в этом направлении,
[52:03.780 --> 52:08.980]  ищите оптимальное значение РО. Потому что РО это просто, грубо говоря, растяжение и сжатие в
[52:08.980 --> 52:17.740]  этом направлении. Куда именно нам шагнуть? Сильно или слабо? Ну, слушайте, по сравнению с тем,
[52:17.740 --> 52:21.460]  что на каждом шаге вы какой-то модель обучаете подобрать какой-то коэффициент, у вас же тут,
[52:21.460 --> 52:27.940]  ну, сами посмотрите, линейный, по сути, всё линейно. Производную вы уже считали, чтобы вот
[52:27.940 --> 52:32.780]  это получить, вам достаточно всего лишь сюда подставить в аналитическое выражение ваши новые
[52:32.780 --> 52:39.420]  предсказания и подобрать порожки производные. Всё. Так что ничего страшного. Не, ну, второй
[52:39.420 --> 52:43.620]  вариант вы можете, если совсем не хочется, может каким-то, не знаю, там линейным поиском пробежаться
[52:43.620 --> 52:59.580]  и всё. Тоже вариант. Да? Сейчас. Что значит? А, смотрите, аргмин, что такое? Ну, мин – задача
[52:59.580 --> 53:05.700]  минимизации. Аргмин – это задача поиска аргументов, при которых достаётся минимум. То есть минимум
[53:05.700 --> 53:10.260]  выражения – это его экстремум, аргмин выражения – это значение аргумента, при котором достигается
[53:10.260 --> 53:14.820]  экстремум. Ну, экстремум – минимум, в смысле argmax, при котором достигается максимум, соответственно.
[53:14.820 --> 53:31.420]  Всё. Хорошо? Да. Вот как. Сначала вы нашли параметр theta, вы знаете, какая модель у вас новая на
[53:31.420 --> 53:36.820]  новом шаге. Всё, модель зафиксировали. Теперь вы говорите, с каким весом мне добавить его в ансамбль?
[53:36.820 --> 54:01.340]  Всё. Я ещё этот вопрос не понимаю. Почему мы в один
[54:01.340 --> 54:11.420]  шаг не нашли и роет эту одновременно? Смотрите, мы с вами с помощью дифференцирования нашли наш
[54:11.420 --> 54:16.180]  градиент. Градиент мы потом аппроксимировали каким-то там образом. Он неточный, это уже оценка
[54:16.180 --> 54:22.060]  градиента, она уже имеет ошибки. Теперь мы говорим, с такой оценкой градиента, насколько нам нужно
[54:22.060 --> 54:28.620]  сместиться? Всё. Если бы у нас с вами был точный градиент, ещё вопрос бы, наверное, имел смысл. У нас
[54:28.620 --> 54:32.540]  вообще оценка градиента, нам не понятно, насколько он вообще хороший. Ну, условно, у вас градиент
[54:32.540 --> 54:39.420]  оценивается трёхслойным деревом в глубиной 3. Он очень грубо оценивается. Вопрос, насколько нам
[54:39.420 --> 54:43.820]  вообще надо идти? Может, нам вообще с таким градиентом идти никуда не надо, потому что слишком
[54:43.820 --> 54:53.940]  паршивая оценка? Всё. Ещё вопросы тут есть? Ну, давайте тогда посмотрим на простой пример. Сразу
[54:53.940 --> 54:59.860]  дисклеймер. В случае регрессии и средней квадратичной ошибки это всё вырождается вообще в детский сад.
[54:59.860 --> 55:05.980]  Это частный случай, так работает не всегда. Хорошо? Поехали. Давайте посмотрим на средней
[55:05.980 --> 55:12.980]  квадратичной ошибку. У нас тогда lost это квадрат отклонения, правильно? То есть 2y с крышкой
[55:12.980 --> 55:19.180]  минус y. y с крышкой наша оценка, y то, что есть. Правильно? Ну, видим, что по сути пропорционально
[55:19.180 --> 55:25.940]  с коэффициентом минус 2, y с крышкой минус y. Верно? А? Пропорционально?
[55:33.940 --> 55:35.580]  Ну, ладно.
[55:38.580 --> 55:45.260]  Ну, наверное, всё-таки пропорционально, потому что я эти слайды составлял. Вывод про это.
[55:49.260 --> 55:54.460]  Так, какой символ вам не нравится? Какой символ вам не нравится?
[55:54.460 --> 56:05.940]  Ребят, нам же основная суть какая? Мы должны условиться об обозначениях. Я хоть горшок на
[56:05.940 --> 56:10.580]  доске нарисовать могу, если вы понимаете, что это такое, абсолютно неважно. Что мы здесь с вами
[56:10.580 --> 56:17.060]  видим? Что у нас градиент по сути пропорционально чему? Реально отклонению нашего предсказания от
[56:17.060 --> 56:24.340]  истинного значения, правильно? То есть мы на каждом шаге с вами будем пытаться оценить что? Мы
[56:24.340 --> 56:29.260]  на каждом шаге будем с вами пытаться краски оценить, насколько мы ошиблись, и каждая новая
[56:29.260 --> 56:33.460]  модель будет предсказывать именно ошибку предыдущей модели в прямом смысле. То есть надо было
[56:33.460 --> 56:38.540]  предсказать 25 овец, мы предсказали 21, надо поправить на 4 овца. Я понять не имею причем здесь
[56:38.540 --> 56:42.820]  овца, мне только что в голову пришли. Извините, я вспомнил мимачик, который я вчера видел.
[56:42.820 --> 56:49.700]  Да, именно, ну то есть смотрите, это именно предсказание в каждой точке. Я еще раз говорю,
[56:49.700 --> 56:53.660]  почему я вот это расписывал, потому что если у вас уже модель фиксирована, все в ней все
[56:53.660 --> 56:57.780]  параметры фиксированы, вы не можете по константе продиференцироваться, чтобы вы понимали,
[56:57.780 --> 57:04.860]  что это значит. Понятно? То есть в случае с линейной регрессией градиентный бустинг вырождается в то,
[57:04.860 --> 57:08.820]  простите, с линейной регрессией, с минимализацией среднего трагичного ошибки, с задачей регрессии,
[57:08.820 --> 57:14.180]  градиентный бустинг вырождается в том, что каждая следующая модель обучается на ошибке предыдущей
[57:14.180 --> 57:21.860]  буквально. Все, он просто, вот вы предсказали, 25 надо было, 30, он учится на 5. Но это работает,
[57:21.860 --> 57:26.380]  потому что у вас производная будет вот такая линейная. Условно с какой-нибудь средней
[57:26.380 --> 57:31.380]  абсолютной ошибкой это уже не работает, потому что производа будет какая? Сигнум вот этой величины.
[57:31.380 --> 57:37.140]  Согласны? Уловили? Ну и давайте посмотрим на классную Демку,
[57:37.140 --> 57:43.740]  которую в далеком 2016 году запилил мой тогда еще коллега, мы с ним в последние
[57:43.740 --> 57:48.260]  годы точно вообще не общались, но Алекс Аргожников классный следователь, классный
[57:48.260 --> 57:54.340]  преподаватель, приложил руку к различным задачкам и преподаванию в ШАДе и многогде. И, в принципе,
[57:54.340 --> 57:58.820]  достаточно... Смотрите, во-первых, вот наше решающее дерево, давайте, наверное, на него
[57:58.820 --> 58:05.260]  сначала посмотрим. Вот у нас с вами гиперповерхность, видите, да, такая волна. Вот что у нас
[58:05.260 --> 58:12.780]  делает решающее дерево, глубины 1, глубины 2, 3, 4, 5, 6. Вот такая вот ступенчатая функция у нас
[58:12.780 --> 58:29.140]  получается. Погодите, пока что одно дерево. Сейчас, или что? Линейная комбинация, да. Погодите,
[58:29.140 --> 58:35.500]  вот же мы с вами только что это писали. Вот, каждое с вами дерево, вы просто добавляете его
[58:35.500 --> 58:40.140]  с весом РО, РО находится в градиентном какой-то оптимизации опять же. Маленькую подзадачу решаю.
[58:40.140 --> 58:53.580]  Погодите, у вас всегда будет ступенчатая функция при сумме, а ступенчатая функция не линейная. Ну,
[58:53.580 --> 59:07.380]  смотрите, давайте я вам это нарисую. Вот у вас одна модель, вот у вас вторая модель. Давайте их
[59:07.380 --> 59:15.980]  просуммируем. Соответственно, тут у нас был 0, тут 1, тут опять 0, 1, 1, 0. Получается у вас будет 0,
[59:15.980 --> 59:26.540]  потом 1, потом 2, потом опять 1. Вот, просуммировал. Ну, вот этого края, правда, нет. Уловили?
[59:26.540 --> 59:33.380]  Короче, вот у нас одно дерево может вот так максимум сделать. Дерево глубиной 6. Возникает
[59:33.380 --> 59:39.780]  вопрос, а что если у нас с вами гораздо больше деревьев, но они все глупенькие? Вот вам градиентный
[59:39.780 --> 59:46.500]  бустинг из 100 решающих деревьев. И давайте посмотрим. Вот у нас 100 деревьев глубиной 3. Согласитесь,
[59:46.500 --> 59:50.620]  кажется, гораздо более плавно описывает нашу вот эту гиперповерхность по сравнению с одним
[59:50.620 --> 59:58.060]  деревом глубиной 6. Ступенник меньше и так далее. Можно даже сделать глубину поменьше и видеть,
[59:58.060 --> 01:00:02.820]  что все равно достаточно плавно он его описывает, хотя, конечно, на сами пике не может добраться,
[01:00:02.820 --> 01:00:07.180]  почему? Потому что глубина дерева слишком маленькая, деревьев маловато. Если увеличим
[01:00:07.180 --> 01:00:12.380]  глубину дерева, даже вон решающие пни, они из-за того, что каждый раз только по одному признаку,
[01:00:12.380 --> 01:00:18.340]  они вторую ось не распознают. Все. Но, соответственно, вот вам два признака уже есть,
[01:00:18.340 --> 01:00:26.140]  вот он хорошо работает. Три, и того краше четыре. Ну, практически на таком, скажем так, удалении
[01:00:26.140 --> 01:00:31.740]  не видно вообще это гладкая поверхность или куча ступенек. По факту это куча ступенек. Но,
[01:00:31.740 --> 01:00:36.220]  соответственно, 100 деревьев глубиной 6. Ну, вот вам замечательная опроксимация той самой вашей
[01:00:36.220 --> 01:00:41.540]  гиперповерхности. Работает. Хотя одно дерево глубиной 6, как видите, даже рядом не лежало,
[01:00:41.540 --> 01:00:49.820]  и тут очень такое все дискретно и не очень красивое. Понятно, что происходит, правильно? Вот. И еще
[01:00:49.820 --> 01:00:54.740]  раз, собственно, возможно, вам тут станет понятно. Вот ваш ансамбль, вот первое дерево, второе дерево,
[01:00:54.740 --> 01:01:01.300]  и так далее. И все их мы обучаем. На четвертом шаге три дерева уже построены. Мы говорим,
[01:01:01.300 --> 01:01:06.860]  что это три дерева плюс четвертое, это наше f от x. Давайте поймем, как нам поправить предсказания
[01:01:06.860 --> 01:01:16.140]  и обучим на этой краске наше четвертое дерево. Все. Вот оно. Вот, смотрите. Вот наша целевая функция.
[01:01:16.140 --> 01:01:20.700]  И теперь, кстати, да, вернемся сразу к краске с вами, к самом начале. Помните, я говорил,
[01:01:20.700 --> 01:01:27.180]  что мы только шаг индукции с вами обозначили. Где взять базу индукции? Фридман, предложивший
[01:01:27.180 --> 01:01:30.500]  гридиэтный бусинг, предложил очень просто. Абсолютно не важно, с чего вы начинаете,
[01:01:30.500 --> 01:01:34.180]  потому что каждая следующая модель будет все равно все поправлять. Давайте всегда у нас в начальной
[01:01:34.180 --> 01:01:40.700]  модели будет просто среднее значение таргета. Констант. Все. Вот у вас слева, вот наш таргет.
[01:01:40.700 --> 01:01:46.700]  Видите, просто ноль. Вот эта гиперпло… вот эта плоскость – это наша текущая оценка. Каждая
[01:01:46.700 --> 01:01:54.780]  следующая модель будет ее исправлять. Чего? Просто среднее значение у. Просто выборочное
[01:01:54.780 --> 01:01:58.140]  среднее. Взяли, да и все. У нас так как нет распределения, у нас интеграл нет, просто
[01:01:58.140 --> 01:02:04.820]  среднее выборочное. Все. Соответственно, вот у нас первое дерево, смотрите. Мы его построили,
[01:02:04.820 --> 01:02:14.980]  с каким-то весом его сюда добавили. Добавили второе дерево. Эх, жалко он нас, эти, не строит нам,
[01:02:14.980 --> 01:02:22.740]  как его называют, остатки. Третье, четвертое, пятое, шестое. А, не, вот, смотрите, справа видно
[01:02:22.740 --> 01:02:29.220]  остатки. Посмотрите, смотрите, слева полностью наш ансамбль, вот он нарисован, и наша целевая
[01:02:29.220 --> 01:02:35.980]  функция. Справа каждая новая дерево и та гиперповершенность, которую дерево должно
[01:02:35.980 --> 01:02:40.980]  опроксимировать, это ровно те самые наши градиенты, антиградиенты. Следите за гиперповершенность
[01:02:40.980 --> 01:02:46.860]  справа. Пока нормально, одно дерево добавили. Заметьте, видите, уже тут появились какие-то
[01:02:46.860 --> 01:02:55.060]  неоднородности, вот их видно. Если идти дальше, видите, у нас уже где появляются, тут у нас почти
[01:02:55.060 --> 01:03:00.020]  все хорошо предсказано, поэтому тут у нас никаких впадин нет, тут почти константа грубая. Вот эта
[01:03:00.020 --> 01:03:05.860]  штука все еще торчит, потому что мы здесь, видите, сильно ошибку имеем, тут градиент большой. Едем
[01:03:05.860 --> 01:03:10.780]  дальше, заметьте, у нас опять гиперповерхность, которую наше дерево опроксимирует, она уже сильно
[01:03:10.780 --> 01:03:19.580]  отличается от исходной, она же больше на шум похоже по центру. 6, 7, 8, 9, 10, вот здесь в конце совсем
[01:03:19.580 --> 01:03:24.700]  хорошо видно. Видите, ошибки у нас большие только в районе пиков, в районе пиков у нас все еще есть
[01:03:24.700 --> 01:03:32.100]  градиенты, в остальных местах у нас скорее какие-то небольшие девиации. И отсюда мы, собственно,
[01:03:32.100 --> 01:03:36.780]  с вами видим, в чем суть градиентного бусинга. Если до этого момента мы с вами всегда, когда
[01:03:36.780 --> 01:03:41.820]  решали задачу какую-либо, мы пытались построить все более-более-более информативное признаковое
[01:03:41.820 --> 01:03:46.020]  описание, мы пытались упростить признаковое описание, придумать новые признаки, придумать
[01:03:46.020 --> 01:03:51.420]  подходящее ядро, сделать нелинейную нашу модель то же самое дерево. Градиентный бусинг заходит с
[01:03:51.420 --> 01:03:56.340]  другой стороны, у нас на каждом шаге признаки те же самые остаются, мы каждый раз упрощаем
[01:03:56.340 --> 01:04:02.180]  таргет. По сути мы говорим, вот смотри, мы уже вот здесь хорошо предсказываем, не обязательно на этом
[01:04:02.180 --> 01:04:06.540]  объекте, вот в этой области хорошо предсказываем, а вот тут у нас все плохо, поэтому надо
[01:04:06.540 --> 01:04:10.940]  фокусировать внимание вот здесь. Вот вам пример, мы в центре уже все достаточно хорошо предсказываем,
[01:04:10.940 --> 01:04:15.580]  а с краев у нас все плохо, поэтому следующие модели будут все больше и больше фокусироваться на краях,
[01:04:15.580 --> 01:04:19.620]  игнорировать центр. Если говорить с точки зрения дерева, дерево будет что-то вроде вот такого,
[01:04:19.620 --> 01:04:24.260]  иметь столбца где-то там слева с краю, а по центру будет просто нулевой констант предсказывать.
[01:04:24.260 --> 01:04:33.780]  Улавливаете? И почему это важно? Потому что когда мы с вами говорим про градиентный бустинг или про
[01:04:33.780 --> 01:04:39.700]  линии на модели, про все это остальное, это не какие-то два абсолютно разных мира, это просто
[01:04:39.700 --> 01:04:44.660]  две крайности. Мы можем либо только работать с признаками, либо только работать с таргетом,
[01:04:44.660 --> 01:04:50.460]  как вы понимаете, можем работать и с тем, и с другим. И поэтому это занятие у нас предшествует введению в
[01:04:50.460 --> 01:04:55.100]  диплеринг, потому что когда мы доберемся с вами до нейронных сетей, мы ровно и открываем глаза на
[01:04:55.100 --> 01:04:59.260]  то, что мы на самом деле можем строить с вами некоторое отображение из пространства объектов,
[01:04:59.260 --> 01:05:05.260]  пространства ответов, и мы менять можем как одно, так и другое, причем последовательно. В чем проблема
[01:05:05.260 --> 01:05:11.340]  градиентного бустинга, если про него говорить? В том, что он строго последовательный, и попав на шаг
[01:05:11.340 --> 01:05:16.660]  n плюс 1, мы теряем возможность поменять предыдущий шаг. Это дерево, если там стоит дерево в качестве
[01:05:16.660 --> 01:05:21.860]  базового алгоритма, в основном дереве. В общем случае, вы все равно построили модель, она фиксирована,
[01:05:21.860 --> 01:05:26.380]  вы от нее считаете градиенты и так далее, вы не имеете права предыдущие модели менять. Поэтому
[01:05:26.380 --> 01:05:30.380]  когда вы попали на n плюс первый шаг, все то, что было до шага n, отлиты в бетоне, больше его не
[01:05:30.380 --> 01:05:35.740]  трогайте. Проблема в том, что это не всегда корректно, и иногда нам хочется что-то в начале поменять,
[01:05:35.740 --> 01:05:40.820]  потому что ошибка в начале, она очень дорога, она потом всегда нам будет аукаться. Нейронные сети
[01:05:40.820 --> 01:05:46.940]  ровно краски этим нам потом и помогут, и мы сможем менять любые шаги, любые преобразования в любой
[01:05:46.940 --> 01:05:51.580]  момент, но об этом на следующей фрексе. Просто чтобы у вас не было вот этого разделения, что вот бустинги,
[01:05:51.580 --> 01:05:55.700]  а тут нейронная сеть, нет, одно к другому очень хорошо подводит. Более того, бустинг появился
[01:05:55.700 --> 01:05:59.900]  после появления нейронных сетей, просто он менее прожорливый с точки зрения данных, поэтому он в
[01:05:59.900 --> 01:06:05.780]  начале нулевых зашел гораздо больше. Нейронные сети к тому моменту показали свой, скажем так,
[01:06:05.780 --> 01:06:10.460]  милый оскал и стали переобучаться в лед на все, что только можно. Градиентный бустинг казался,
[01:06:10.500 --> 01:06:15.440]  что «О, к doo не переобучается», ладно сделали просто побольше ансамбль тоже стал переобычаться,
[01:06:15.440 --> 01:06:19.580]  все опять пригорюнились. То же самое было с нейронными с Templest nut с начала нейронные сети были маленькие
[01:06:19.580 --> 01:06:23.880]  а до ацепты уже накопили большие, казалось, что нейронные сети это идеально, они работают везде
[01:06:23.880 --> 01:06:28.780]  они не переобучаются. Просто потому, что вычислительные мощности на тот момент были слишком маленькие,
[01:06:28.780 --> 01:06:32.620]  чтобы построить достаточно большую сеть, чтобы все переобучилось. Построили большую сеть,
[01:06:32.620 --> 01:06:37.380]  все переобучалось, потом пришел бустинг, все такие БА, бустинг не переобучается –
[01:06:37.380 --> 01:06:44.580]  переобучился. Потом опять нейронные сети в итоге пришли к выводу, что все переобучается, грузь, печаль, беда, обида, но при этом с этим можно бороться.
[01:06:45.220 --> 01:06:47.220]  Хорошо. Вопрос сейчас есть?
[01:06:48.220 --> 01:06:50.380]  Давайте как раз на примере еще вот простеньком
[01:06:52.100 --> 01:06:53.780]  как это называется?
[01:06:53.780 --> 01:06:57.260]  Регрессии. Разберемся. Вот что нам надо. Нам нужны данные,
[01:06:57.780 --> 01:07:04.580]  дифференцированная функция потерь. В чем, кстати, плюс? Вы можете сразу аналитически посчитать производную функцию потерь по предсказаниям один раз и
[01:07:04.740 --> 01:07:09.500]  потом сразу эту формулу эксплуатировать. Вас никто не заставляет на каждом шаге градиенты считать, вам не надо.
[01:07:09.780 --> 01:07:15.660]  У вас аналитическое значение градиента есть, если у вас функция дифференцируема. Вот у нас какая-то семейство алгоритмов,
[01:07:16.100 --> 01:07:21.300]  число итераций и initial value. Давайте просто константой первой модель. У нас будет и все.
[01:07:22.020 --> 01:07:25.980]  Например, вот у нас будет вот такая зависимость. Косинус плюс Эпсилон, то есть каким-то шумом.
[01:07:26.500 --> 01:07:32.260]  Функция потерь будет МСЕ. Опять же, в случае МСЕ все слишком просто. Мы просто предсказываем ошибку предыдущих моделей.
[01:07:32.540 --> 01:07:34.820]  В случае с лог-лостом это уже не работает. Будьте осторожны.
[01:07:35.620 --> 01:07:40.980]  У нас будут решающие деревни глубиной 2, уже не совсем пни, три итерации в начале константа.
[01:07:41.820 --> 01:07:47.460]  Что мы видим? Слева опять же наш ансамбль полностью, справа наш текущий модель.
[01:07:47.980 --> 01:07:54.820]  На первом шаге у нас только константа и, соответственно, ее ошибки. Это уже ошибки модели. Это вот наши эти градиенты.
[01:07:55.900 --> 01:07:59.900]  На втором шаге вот наша добавочная модель, которая на это была обучена.
[01:08:00.460 --> 01:08:06.100]  Вот наш полный ансамбль. Потому что средняя была ноль, соответственно, ноль плюс наша модель. Получилось то же самое.
[01:08:06.660 --> 01:08:12.740]  Посмотрите на наши остатки на третьем шаге. Видите? Вот на что обучается вторая модель. Ну, третья, если с нуля считать.
[01:08:13.420 --> 01:08:15.660]  Это уже далеко не везде похоже на наш Косинус.
[01:08:16.740 --> 01:08:22.460]  Вот общий ансамбль. Вот четвертый шаг. Вот на что похожи наши остатки, на которые учится четвертая модель.
[01:08:22.980 --> 01:08:24.660]  Вот наш полный ансамбль.
[01:08:24.660 --> 01:08:28.340]  То есть каждый раз у нас все ближе и ближе к какому-то шуму становится наш сигнал.
[01:08:28.660 --> 01:08:31.180]  Таргет, на который пытается обучиться наша модель.
[01:08:31.980 --> 01:08:33.140]  Все.
[01:08:33.140 --> 01:08:35.140]  Стал понять, что происходит?
[01:08:36.620 --> 01:08:37.940]  Замечательно.
[01:08:37.940 --> 01:08:44.100]  Ну и как бы пара графиков. Вот вам пример при увеличении числа деревьев для бэддинга.
[01:08:44.820 --> 01:08:50.300]  Просто видим, что он выходит на переобучение в районе там 500 деревьев. Вот для рандом-пороста.
[01:08:50.780 --> 01:08:53.140]  Синький. Он гораздо лучше держится.
[01:08:53.740 --> 01:09:00.580]  Итоговая ошибка у него ниже. То, что деревья менее скоррелированные. У нас больше эффект снижения вот этой дисперсии.
[01:09:00.580 --> 01:09:02.580]  Но тем не менее он тоже уходит куда-то на насыщение.
[01:09:02.980 --> 01:09:09.020]  Вот наш гридентный бустинг. Он выходит на насыщение только в районе тысячи деревьев. Но заметь, итоговая ошибка еще ниже. Почему?
[01:09:09.460 --> 01:09:13.700]  Потому что все равно решающий лес, решающее дерево, оно может переобучиться.
[01:09:14.260 --> 01:09:20.020]  Когда мы с вами усредняем их между собой, мы просто-напросто избавляемся от ошибок остальных деревьев.
[01:09:20.180 --> 01:09:24.100]  Но тем не менее мы итоговую сложность модели усреднением повысить не можем.
[01:09:24.740 --> 01:09:28.580]  Потому что у нас невзвешенные средние. Они просто друг к другу пытаются, грубо говоря, скроховать везде.
[01:09:29.260 --> 01:09:33.540]  В гридентном бустинге каждый следующий модель усиливает предыдущую. Поэтому чем глубже
[01:09:34.140 --> 01:09:38.780]  в лес, чем больше моделей, тем больше у нас с итоговой будет качество, по крайней мере, на обучающей выбраке.
[01:09:39.140 --> 01:09:41.940]  Понятное дело, на тестовый в какой-то момент у нас все переобучится и будет плохо.
[01:09:42.540 --> 01:09:48.620]  Но гридентный бустинг замечательно переобучается. Будьте осторожны. Его можно переобучить в лед. И в чем проблема гридентного бустинга?
[01:09:49.340 --> 01:09:54.540]  Если рэнд и форст вас кое-как страхуют от переобучения в том плане, что переобучилось одно дерево, не беда,
[01:09:54.860 --> 01:09:59.100]  много деревьев, краски, проблемы с переобучением нивелируются, потому что они усредняют друг друга.
[01:09:59.500 --> 01:10:04.660]  С гридентным бустингом, если одно дерево было обычно, ладно, одна модель была обучена где-то в ходе,
[01:10:05.980 --> 01:10:07.980]  все что после нее это мусор.
[01:10:07.980 --> 01:10:11.620]  То, что у вас переобученная модель, повлияет на все оставшиеся гриденты.
[01:10:11.820 --> 01:10:17.780]  Соответственно, если у вас один элемент ансамбля переобучен, весь ансамбль с этого момента и далее это мусор. Он бесполезен.
[01:10:18.300 --> 01:10:22.380]  Поэтому следить за гридентным бустингом надо очень аккуратно и там смотреть на валидацию.
[01:10:27.300 --> 01:10:31.060]  А потому что у вас гридент станет сильно меньше, у вас ошибок стало сильно меньше на обучающей выборке.
[01:10:31.580 --> 01:10:35.740]  То есть вы уже переподстроились под обучающую выборку, у вас гриденты почти все слопнулись.
[01:10:36.260 --> 01:10:38.980]  А следующая модель пытается подстроиться под гридент, которых нет.
[01:10:41.660 --> 01:10:43.660]  Вот.
[01:10:43.660 --> 01:10:50.660]  Как-то так. Ну и вот вам пример краски из начала, вот вам опять две концентрические окружности, очень старая картинка, надо перерисовать.
[01:10:50.660 --> 01:10:53.620]  Короче, снаружи красные точки, в центре синие точки.
[01:10:54.740 --> 01:10:59.380]  Ну, поверьте мне на слово, потом можете посмотреть записи и я перерисую, наверное.
[01:10:59.860 --> 01:11:01.860]  Синие точки в центре, красные по кругу.
[01:11:02.340 --> 01:11:04.740]  Линейная модель одна не способна отделить, правильно?
[01:11:05.260 --> 01:11:07.260]  То, что линия разделимой выборка.
[01:11:07.260 --> 01:11:13.060]  Но мы можем вспомнить, что логистическая регрессия это ни разу не линейное отображение, это сегмойда от линейного отображения.
[01:11:13.860 --> 01:11:18.180]  Поэтому их линейная комбинация уже вполне себе может задавать что-то нелинейное.
[01:11:18.620 --> 01:11:22.500]  Собственно, вот вам гридентный бустинг над логистическими регрессиями.
[01:11:23.220 --> 01:11:26.060]  Замечательно ограничивает центр.
[01:11:28.340 --> 01:11:30.340]  Уловили?
[01:11:31.620 --> 01:11:34.180]  Каждая полоса соответствует какой-то логистической регрессии.
[01:11:34.180 --> 01:11:39.940]  То есть, каждая из них дает линейную разделяющую поверхность, но все вместе они смогли отделить центр от всего остального.
[01:11:40.940 --> 01:11:45.940]  Да, в идеальном случае мы с вами можем три прямых провести и треугольником отделить все.
[01:11:45.940 --> 01:11:49.940]  Но так как у нас там шум присутствует и так далее, вот за 40 шагов он хорошо отделился.
[01:11:49.940 --> 01:11:55.940]  Хотя мы видим с вами, что за первые там 10, на самом деле он уже примерно перестал ошибку ронять.
[01:11:57.940 --> 01:11:59.940]  Вот.
[01:11:59.940 --> 01:12:06.940]  Ну что, гридентный бустинг вас вызвал страх и ненависть в БХИМе.
[01:12:09.940 --> 01:12:15.940]  Вот здесь?
[01:12:15.940 --> 01:12:17.940]  Ну...
[01:12:21.940 --> 01:12:30.940]  Смотрите, это тестовая ошибка, поэтому то, что она себя ведет таким образом, абсолютно нормально, потому что обучаемся на трене, а на тесте валидируем.
[01:12:30.940 --> 01:12:37.940]  То есть, на трене она, как правило, падает достаточно равномерно, если считать по всей выборке.
[01:12:38.940 --> 01:12:44.940]  Если считать по бачам, то она себя будет вести вот так, потому что у вас от бача зависит хорошо или плохо, вы там на этом баче конкретно отработали.
[01:12:46.940 --> 01:12:47.940]  Вот.
[01:12:47.940 --> 01:12:52.940]  Смотрите, тут она, например, сглаженная, во времени, видите, она в среднем-то падает, достаточно стабильно.
[01:12:53.940 --> 01:12:59.940]  Тут сглаживание применено, чтобы мы видели, что вот эти вот плески, они на самом деле в среднем все равно идут вниз.
[01:13:00.940 --> 01:13:02.940]  Ну и коротенькое, наверное, аутро.
[01:13:02.940 --> 01:13:08.940]  Рано-поро Stuff вас работает строго параллельно на уровне деревьев, поэтому вы его можете хорошо парализовать.
[01:13:09.940 --> 01:13:14.940]  И если у вас словно есть 40 потоков, то 40 деревьев можете одновременно построить, будет быстро.
[01:13:14.940 --> 01:13:16.940]  Применять тоже можете быстро.
[01:13:17.940 --> 01:13:23.940]  Градиентный бусинг, он не парализуется влоп, но при этом стоит помнить, что дерево можно тоже строить достаточно эффективно.
[01:13:24.940 --> 01:13:30.940]  Если вы строите, например, градиентный бусинг над деревьями, то каждый дерево может строится, грубо говоря, на два под дерева поделили, они могут параллельно строиться.
[01:13:30.940 --> 01:13:35.440]  строится. Поэтому не стоит думать, что гранитный бустинг уж совсем медленная
[01:13:35.440 --> 01:13:39.820]  штуковина. В современном мире он, конечно, медленнее, чем рано пора, но при этом он
[01:13:39.820 --> 01:13:44.940]  тоже достаточно быстро строится и, как правило, сильно быстрее всяких нейронок.
[01:13:44.940 --> 01:13:51.020]  Окей, сложных нейронок. Ну что, тут какие-то вопросы есть?
[01:13:55.780 --> 01:13:59.940]  Еще раз, я именно и говорил, что если он переобучится, то все плохо.
[01:13:59.940 --> 01:14:04.600]  Именно поэтому в среднем, исторически, глубокие деревья не используются, потому что
[01:14:04.600 --> 01:14:08.780]  неглубокое дерево переобучить сложно, глубокое дерево переобучить легко.
[01:14:08.780 --> 01:14:13.740]  Но опять же, это в среднем, это не то, что только так и никак иначе. Скорее так, если
[01:14:13.740 --> 01:14:16.340]  вы не понимаете, что делать, лучше использовать неглубокие деревья.
[01:14:16.340 --> 01:14:20.420]  С гранитным бустингом и глубоким, и с рано порастым. Если вы понимаете, что вы
[01:14:20.420 --> 01:14:25.120]  делаете, плак вам в руки, барабан на шею. Делать можете угодно. Просто пока у вас
[01:14:25.120 --> 01:14:29.020]  нет понимания, как бы, как в данной задаче отработает тот или иный алгоритм, например,
[01:14:29.020 --> 01:14:32.300]  сильно у вас там шум присутствует или слабый, лучше придерживаются
[01:14:32.300 --> 01:14:38.060]  каких-то общепринятых норм. Вот, как-то так. Короче, моя главная цель была, на
[01:14:38.060 --> 01:14:41.620]  самом деле, чтобы сегодня вы поняли простую вещь, что градиентный бустинг
[01:14:41.620 --> 01:14:48.140]  строит именно на каждом шаге аппроксимацию антиградиента вашей функции потерь по
[01:14:48.140 --> 01:14:52.780]  предсказаниям. По сути, в каждой точке пространства, где есть обучающие значения
[01:14:52.780 --> 01:14:57.060]  парой из обучающей выборки, вы строите ваши антиградиенты, вы аппроксимируете. Вот
[01:14:57.060 --> 01:15:02.620]  это коровая вещь. Все остальное это всякие там свистопляски и так далее. Это осознали?
[01:15:08.500 --> 01:15:11.820]  Хорошо. Ну, смотрите, у меня на самом деле еще простейкинг и блендинг, дополнительные
[01:15:11.820 --> 01:15:16.340]  слайдики. Я, наверное, предлагаю что сделать? Продолжаю даже прямо сейчас сделать перерыв,
[01:15:16.340 --> 01:15:21.300]  потому что не самая легкая была штуковина. Где-то 18-40 мы продолжим тогда, то есть 15 минут
[01:15:21.300 --> 01:15:26.020]  перерыва. И там мы поговорим сначала на практике про деревья и про бустинг, а потом я вам еще про
[01:15:26.020 --> 01:15:29.140]  то, как стакать это все дело расскажу. Все, лекция over.

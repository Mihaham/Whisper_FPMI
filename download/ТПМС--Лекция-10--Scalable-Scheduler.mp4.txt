[00:00.000 --> 00:12.400]  Ну что, давайте начнем. Добрый день, доброе утро. Давайте, прежде чем перейти к теме лекции,
[00:12.400 --> 00:20.120]  она сегодня про планировщик, я небольшое замечание сделаю про текущую домашнюю работу,
[00:20.120 --> 00:27.040]  про задачу Fiberny Mutex, где вы пишете Mutex, Condvar, Weight Group, Произвольный примитив
[00:27.040 --> 00:34.000]  синхронизации. Вот некоторые люди уже заметили, что там есть некоторые сложности, скажем так. То есть
[00:34.000 --> 00:39.760]  у вас все работает, кроме одного небольшого теста, который иллюстрирует очень неприятную ситуацию,
[00:39.760 --> 00:47.040]  очень тонкую ситуацию, которую нужно сначала увидеть своими глазами, разобрать механику
[00:47.040 --> 00:51.840]  детальную, как там что происходит, и, в-третьих, причинить. Вот, пожалуйста, обратите на это внимание,
[00:51.840 --> 00:59.000]  там есть очень тонкая хитрость с лайфтаймами и с разрушением объектов. Вот, есть Mutex, а есть
[00:59.000 --> 01:03.960]  Weight Group, и применение у них несколько разное. Когда мы используем, мы используем Weight Group для
[01:03.960 --> 01:12.160]  того, чтобы дождаться завершения других Fiber'ов. И в одном потоке Fiber сигнализирует последнее,
[01:12.160 --> 01:19.400]  что вот он завершается, говоря Dan, а в другом потоке другой Fiber выходит из Weight'а,
[01:19.400 --> 01:26.120]  завершает свою процедуру, и вот разрушается стековый фрейм, вызывается деструктор этого самого
[01:26.120 --> 01:32.400]  Weight Group. И если неаккуратно один поток, завершая Dan, все еще пишет что-то в разрушаемый объект,
[01:32.400 --> 01:39.760]  то у вас разламывается адрес Sanitizer, он говорит вам, что вы пишете уже разрушенную память. Ну там,
[01:39.760 --> 01:46.080]  в тесте это делается специальным таким маленьким костылем видеолокацией на кучу, чтобы адрес
[01:46.080 --> 01:50.280]  Sanitizer лучше обнаружил. Ну вот, пожалуйста, обратите на это внимание, разберите, и потом
[01:50.280 --> 01:55.920]  попытайтесь починить проблему. Ну и смотрите, она чинится разными способами. Вот есть мнение
[01:55.920 --> 02:00.440]  мое пока на данный момент, что следует исправить эту проблему в Weight Group, потому что именно она
[02:00.440 --> 02:08.200]  про Join Fiber'ов, и именно она про разрушение, в конце концов. Но если вы вдруг решили бонусный
[02:08.200 --> 02:18.640]  уровень, то есть вы решили задачу из экзекутеров Lock-Free Strand, а потом асинхронный Mutex,
[02:18.640 --> 02:26.520]  а потом вы разобрались, где же здесь связь с Mutex, и в задаче Mutex решили бонусный уровень,
[02:26.520 --> 02:31.760]  который про реализацию Lock-Free Mutex, то у вас там такой проблемы не будет. То есть вот прям хорошая
[02:31.760 --> 02:37.400]  реализация Mutex, ее благополучно избегает. Так что вам даже не нужно будет думать про вот эту
[02:37.400 --> 02:44.040]  этот сценарий с разрушением. Ладно, в общем, обращаю ваше внимание, что есть сложное место,
[02:44.040 --> 02:50.480]  и, к сожалению, вот с ним придется столкнуться, и его придется разобрать. Вот я вам пока рекомендую
[02:50.480 --> 02:56.520]  делать это в Weight Group, а чуть позже мы узнаем разные способы, как можно более надежно эту проблему
[02:56.520 --> 03:03.720]  решить. Ну вот так или иначе, о чем задача вот про Mutex? Про то, чтобы придумать приметивы
[03:03.720 --> 03:08.840]  синхронизации, точнее, про то, чтобы реализовать Mutex для начала, это такой первый очевидный ее уровень,
[03:08.840 --> 03:13.160]  разобраться, как именно он устроен в ядре, потому что, по сути, мы пишем то же самое,
[03:13.160 --> 03:17.640]  что и в ядре, и нужно это именно так воспринимать. Мы не пишем что-то другое, мы пишем тот же самый
[03:17.640 --> 03:22.480]  клуб, потому что и под ядром, и под нами один и тот же, в конце концов, компьютер с одними этими же
[03:22.480 --> 03:28.840]  операциями. Следующий уровень – это придумать дизайн хороший для саспенда, то есть увидеть,
[03:28.840 --> 03:36.360]  что Mutex – это совсем не что-то фундаментальное с точки зрения синхронизации. Mutex – это всего лишь
[03:36.360 --> 03:42.560]  такой частный случай вот этого самого общего саспенда, который задача требуется придумать.
[03:42.560 --> 03:47.360]  Просто мы ограничены некоторой изоляцией между ядром операционной системы и пространственным
[03:47.360 --> 03:54.200]  пользователя, а вот в Fiber'ах у нас такого ограничения нет, поэтому мы можем расширить
[03:54.200 --> 04:01.000]  семантику Futex'а и поддержать там произвольную стратегию засыпания и планирования возобновления,
[04:01.000 --> 04:05.560]  которая называется Avator. Вот это следующая очень важная часть задачи – она про дизайн,
[04:05.560 --> 04:13.480]  ну и про API для блокировки, каким оно должно быть. Дальше идет Lock Frame Mutex,
[04:13.480 --> 04:19.120]  который сложный, очень красивый бонусный уровень, и в конце концов, когда вы все это
[04:19.120 --> 04:25.320]  сделаете и вот почините эту проблему с разрушением, вообще заметите ее, у вас получатся довольно
[04:25.320 --> 04:30.000]  хорошие файберы. Ну вот если вы, правда, будете соблюдать все требования и задачи, избавитесь
[04:30.000 --> 04:35.000]  от аллокаций, и в планировщики избавитесь от аллокаций, и в Mutex'ах, в общем, у вас получатся такие
[04:35.000 --> 04:44.680]  аккуратные файберы. А дальше вопрос – насколько они соблюли хорошие? Как бы это проверить?
[04:44.680 --> 04:55.360]  Вот мы их пишем, у нас нет там аллокаций, есть там интрузивность в Mutex. Верно ли,
[04:55.360 --> 05:05.080]  что мы написали хороший, эффективный код? Ну что вы скажете по этому поводу? Мы же, не знаю,
[05:05.080 --> 05:09.880]  может быть, вы взяли и запустили там профайлер с ним, посмотрели, насколько быстро он работает,
[05:09.880 --> 05:28.320]  где он проводит время? Не было у вас такого желания? Нет, разумеется, стеклос будет быстрее,
[05:28.320 --> 05:35.280]  он поэтому и существует, у него мы стеклос платим за одно и выигрываем другое, это разумеется,
[05:35.280 --> 05:41.160]  это очевидно. Но мы не используем стеклос подход, мы используем стеклоподход и делаем это намеренно,
[05:41.160 --> 05:46.920]  так что в рамках этого подхода у нас хорошие файберы получились, они быстро исполняются.
[05:46.920 --> 05:55.760]  Ну вообще-то, как они исполняются, зависит от еще одного компонента, который называется
[05:55.760 --> 06:01.480]  планировщик, правда? Чтобы вообще что-то исполнять, нам нужен шеддлер. Этот шеддлер,
[06:01.480 --> 06:09.360]  планировщик исполняет некоторые задачи, которые в случае с файберами являются такими служебными
[06:09.360 --> 06:13.400]  задачками, которые запускают крутину, делают шаг файбера, потом останавливаются и перепланируют
[06:13.400 --> 06:21.000]  возобновление этого файбера. Так вот, что же можно сказать про наш планировщик? Насколько он будет
[06:21.000 --> 06:24.640]  хорош? Ну потому что все же зависит не только от реализации самих файберов, но и от того,
[06:24.640 --> 06:30.440]  кто их исполняет. Ну и давайте вот, прежде чем говорить, хороший наш планировщик или плохой наш
[06:30.440 --> 06:36.320]  планировщик, что мы вообще понимаем под хорошим? Чего мы ожидаем от хорошего планировщика?
[06:36.320 --> 06:46.080]  Мы ожидаем этого от планировщика, а что мы ожидаем от хорошего планировщика?
[06:46.080 --> 06:53.960]  Ну он же не знает будущего, он же не Кассандра.
[06:53.960 --> 07:07.160]  Ну честность опять, да, честность это хорошее требование, но смотри,
[07:07.160 --> 07:14.600]  я спрашиваю не про то, как должен вести себя там. Понятно, что у планировщика есть какие-то
[07:14.600 --> 07:18.880]  разумные гарантии, которые он обязан соблюдать. Он должен запустить каждую запланированную в него
[07:18.880 --> 07:28.720]  задачу, наверное, да. Я сейчас спрашиваю про то, что такое быстрый планировщик. Вот, это очень
[07:28.720 --> 07:34.800]  хорошее замечание. Вот вы изучаете операционные системы, вы знаете, что там вот есть много
[07:34.800 --> 07:40.440]  компонентов, какая-то сложная логика. Вот вопрос, когда исполняется операционная система в компьютере?
[07:40.440 --> 07:46.080]  Ну если, ну если бы она исполнялась всегда на компьютере с одним ядром, то я боюсь, что такая
[07:46.240 --> 07:50.560]  система была бы бесполезна, потому что исполнялась бы только она. А нам нужно, чтобы исполнялась
[07:50.560 --> 07:58.340]  программа пользователя, а не ядро. Вот, ну то есть есть какие-то ситуации, когда программа,
[07:58.340 --> 08:04.560]  ваша вот полезная нагрузка компьютера обращается к планировщику. Всё остальное время планировщик,
[08:04.560 --> 08:09.760]  ну в смысле планировщик операционной системы, должна не работать. Вот почти всё время должна
[08:09.760 --> 08:14.120]  занимать ваша программа, ваша какая-то полезная нагрузка. Вот это очень разумные ожидания от
[08:14.120 --> 08:18.760]  планировщика. И мы ожидаем от RedPool'а тоже, что вот если мы посмотрим, где эта
[08:18.760 --> 08:23.120]  программа проводит свое время, то мы будем надеяться, что она большую часть
[08:23.120 --> 08:30.000]  времени проводит внутри кода пользователя. Вот то есть, вот здесь. И поменьше внутри
[08:30.000 --> 08:36.800]  методах планировщика. Чем меньше работает планировщик, тем лучше. Согласны? Вот. Ну,
[08:36.800 --> 08:40.960]  потому что работать самого планировщика – это вот как бы обслуживание запуска задачи. Он
[08:40.960 --> 08:44.880]  должен распределять задачи между ядрами процессора. То есть, он должен распределять
[08:44.880 --> 08:50.760]  вычислительные ресурсы. Вот чем меньше его будет, тем, кажется, наш планировщик будет эффективнее.
[08:50.760 --> 08:58.920]  Ну, давайте посмотрим, насколько эффективен наш планировщик. На самом деле, я сейчас покажу не
[08:58.920 --> 09:04.880]  наш планировщик, а мой планировщик, который написан намеренно так же, как и ваш, но он чуть
[09:04.880 --> 09:11.280]  аккуратнее. Там, скажем, нет лишних элокаций, поэтому у вас будет еще медленнее. Ну, в любом случае,
[09:11.280 --> 09:16.200]  у вас будет возможность это исправить еще. Что я запускаю в этом планировщике? Я хочу запустить
[09:16.200 --> 09:24.600]  вот такой код. Ну, какая-то нагрузка. Я беру и запускаю в планировщике 13 групп файберов,
[09:24.600 --> 09:35.240]  которые, где каждая группа в количестве 100 файберов вот такое количество раз захватывает по очереди
[09:35.240 --> 09:40.880]  два мютокса. То есть, у каждой группы по 2 мютокса по 100 файберов, и вот они хватают эти мютоксы,
[09:40.880 --> 09:46.840]  выполняют какие-то простые критические секции. Код, разумеется, он не слишком практичный,
[09:46.840 --> 09:53.040]  вряд ли настоящая программа вот так вот работает. Но вот по этой программе уже можно посмотреть,
[09:53.040 --> 09:58.480]  насколько много времени она занимается планированием, насколько много времени она
[09:58.480 --> 10:01.960]  собственно исполняется. Ну, вот давайте мы и посмотрим.
[10:21.960 --> 10:25.240]  Ну, вот мы собрали профиль, можно посмотреть на него. Я не знаю, видите ли вы что-нибудь на
[10:25.240 --> 10:36.680]  проекторе? Ну, ничего не поделать боюсь, что. Ну, я расскажу. Вот это Flame Graph. Ну, то есть,
[10:36.680 --> 10:43.760]  вот горизонтальные такие колбаски — это время, проведенное в каком-то из вызовов. И вот я смотрю
[10:43.760 --> 10:52.200]  на какой-то thread-threadpool. И вот смотрите, половину времени, 44%, он проводит в лямде пользователя.
[10:52.200 --> 11:00.640]  А где он проводит еще половину времени? Проводит он ее в функции worker-routing. Звучит,
[11:00.640 --> 11:09.600]  выглядит довольно плохо. А что же он делает в worker-routing? Ну, вот он берет mutex. В смысле,
[11:09.600 --> 11:18.280]  он прямо в вызове mutex-lock проводит 8% времени. Еще, ну, примерно столько же в mutex-unlock. А потом
[11:18.280 --> 11:26.560]  еще раз mutex-lock и еще раз mutex-unlock. Ну, вот видите, он половину времени рабочего вашей программы
[11:26.560 --> 11:31.520]  занимается тем, что она вот захватывает и освобождает mutex. В смысле, вот не ваш файберный mutex,
[11:31.520 --> 11:39.920]  который, кстати, написан здесь хорошо. Вот в этом примере он написан как lock-free mutex. Кстати,
[11:39.920 --> 11:47.520]  вот можно и посмотреть, как это влияет. Вот, например, мы запускаем программу threadpool
[11:47.520 --> 11:58.480]  с таким наивным планировщиком. А потом и с lock-free mutex. Потом поменяем на реализацию,
[11:58.480 --> 12:16.320]  которую вы пишете. Посмотрим, что случится. Ну, стало хуже тут, как бы, да, тут заметно,
[12:16.320 --> 12:24.280]  что стало хуже. Одна из причин написать хороший lock-free mutex. Он уже на десятки процентов ускоряет
[12:24.280 --> 12:33.520]  такой, ну, синтетический, но все же пример. Ну, вот mutex мы улучшили даже. Я про это и говорю,
[12:33.520 --> 12:37.440]  что, собственно, вот возьмем, напишем хороший файбер, но под ними вот планировщик, который
[12:37.440 --> 12:45.440]  половину времени проводит в mutex-lock. Давайте теперь я по-другому скажу, что такое хороший или плохой,
[12:45.440 --> 12:52.640]  ну, быстрый или эффективный или неэффективный планировщик. Я бы сказал, что эффективный
[12:52.640 --> 12:59.240]  планировщик – это планировщик, который хорошо масштабируется. Вот наш планировщик пока плохо
[12:59.240 --> 13:05.720]  масштабируется. Я сейчас поясню, что это значит. Под масштабированием вообще глобально система любых
[13:05.720 --> 13:10.960]  распределенных, вот многопоточных понимает такое свойство, что если вы добавляете вашу систему
[13:10.960 --> 13:17.000]  новые вычислительные ресурсы, скажем, для примера вычислительные, то система начинает
[13:17.000 --> 13:22.440]  обслуживать больше запросов в единицу времени пропорционально. Ну, вот вы пишете какой-нибудь,
[13:22.440 --> 13:27.120]  у вас распределенный Mopreduce, который какие-то вычисления проводит, у вас было 100 машин,
[13:27.120 --> 13:32.200]  а вы сделали 200 машин. И вот у вас теперь пропускная способность вашей системы, полезные
[13:32.200 --> 13:37.880]  работы выполняются в два раза больше в единицу времени. Мы, наверное, от планировщика такое хотим.
[13:37.880 --> 13:43.280]  С ростом количества ядер в процессоре, с ростом количества потоков, которые запускают на этих
[13:43.280 --> 13:48.080]  ядрах задачи, кажется, планировщик должен выполнять в два раза больше задачи в единицу времени.
[13:48.080 --> 13:55.320]  Ну, так бы вел себя идеальный планировщик, который бы вот просто линейно масштабировался. А что можно
[13:55.320 --> 13:59.560]  сказать про нашу реализацию нашего планировщика? Является ли она масштабируемой или нет?
[13:59.560 --> 14:12.640]  Верно ли, что мы, добавляя в машину ядра, вот в таком тесте получаем там кратное ускорение?
[14:12.640 --> 14:23.680]  Он не масштабируем. Ну, потому что... Что мы видим в профайле-то? Еще раз. Мы видим, что планировщик
[14:23.680 --> 14:30.840]  половину времени тратит на локи-анлоке-мьютакса. Почему? Ну, потому что в планировщике есть точка
[14:30.840 --> 14:36.000]  contention. Есть вот этот глобальная очередь общая для всех потоков воркеров. И каждый поток,
[14:36.000 --> 14:40.760]  чтобы взять себе задачу, берет и идет к этой общей очереди и захватывает блокировку.
[14:40.760 --> 14:48.960]  А разве количество локов и анлоков в единицу времени зависит от количества ядер? Разве оно
[14:48.960 --> 14:55.880]  растет с ростом числа ядер? Ну, вряд ли, да? Скорее, оно уменьшается с ростом числа ядер. То есть,
[14:55.880 --> 15:01.080]  чем больше нагрузка, тем больше нагрузка на там, как протокол когеретности кэшей, на вот то,
[15:01.080 --> 15:05.840]  чтобы подвинуть одну ячейку из кэша там, кэшлинию из одного кэша в другое, в другое, конечно,
[15:05.840 --> 15:12.600]  но вот вы помните все это, да? Зачем нам нужна была лекция про кэши в этом курсе? Чтобы сказать,
[15:12.600 --> 15:22.800]  что для синхронизации, что при записи в ячейку памяти ядру нужно получить кэшлинию в монопольное
[15:22.800 --> 15:27.800]  владение. Нужно поговорить с другими ядрами, инвалидировать в них копии, получить себе кэшлинию
[15:27.800 --> 15:33.320]  в состоянии modified, эксклюзивный доступ к ней иметь. Поэтому, если у вас есть ядра, и они работают
[15:33.320 --> 15:39.360]  с одними и теми же ячейками памяти, ну, скажем, с одним и тем же флажком в одном и том же мютоксе,
[15:39.360 --> 15:45.440]  то, кажется, все эти ядра работают с ним последовательно. При этом, чем больше ядер,
[15:45.440 --> 15:51.240]  тем больше просто дополнительная нагрузка на кладные расходы, на синхронизацию, на пересылку
[15:51.240 --> 15:57.880]  сообщений, на их обработку. Ну, то есть, мы бы, конечно, хотели, чтобы планировщик работал быстрее
[15:57.880 --> 16:04.680]  с ростом количества потоков, ядер потоков, соответственно, но пропускная способность
[16:04.680 --> 16:14.160]  критической мютокса, она просто ограничена. Вот. И, скажем, помогло бы нам, если бы мы взяли и заменили
[16:14.160 --> 16:21.560]  вот этот самый мютокс, который защищает очередь, на лог-фри очередь, которую мы делали в прошлый раз.
[16:21.560 --> 16:29.960]  Но точно так же это не помогло бы, потому что, смотрите, лог-фри очередь — это снова ячейка,
[16:29.960 --> 16:35.880]  в которой хранится поинтер на голову, и мы там этот поинтер меняли кассом, читали голову, потом
[16:35.880 --> 16:40.520]  пытались перекинуть ее дальше, делали compare exchange. Ну, compare exchange — это снова операция, которая
[16:40.520 --> 16:47.000]  пишет в ячейку памяти, даже если она, даже если неуспешный касс. И снова мы вот, все ядра будут
[16:47.000 --> 16:54.400]  сериализовываться вот на дно этой ячейки, будет contention, будет нагрузка. Понимаете проблемы, да? Вот.
[16:54.400 --> 16:58.920]  Чтобы построить хороший планировщик, эффективный планировщик, который будет масштабироваться,
[16:58.920 --> 17:12.200]  нам нужно применить какой-то другой подход. А это единственное, в смысле, это и есть плата
[17:12.200 --> 17:33.080]  за синхронизацию. Тут совершенно неважно, тратим мы время в сисколах, потому что мы засыпаем,
[17:33.080 --> 17:41.160]  или тратим мы время в преобращении к ячейкам памяти. У нас все равно есть точка contention,
[17:41.160 --> 17:48.240]  у нас все равно есть место, где потоки очень часто сталкиваются друг с другом и упорядочиваются,
[17:48.240 --> 17:53.920]  действуют последовательно. Неважно, какой именно механизм к этому приводит. Тут специфика,
[17:53.920 --> 18:00.520]  смотри, какая. Вот наш тредпул, он, в принципе, хороший тредпул, он нормальный, не нужно про него
[18:00.520 --> 18:04.920]  ничего плохого говорить, просто он не подходит нашей задаче. Вот он подходит для задач, которые
[18:04.920 --> 18:09.480]  выполняют какие-то вычисления тяжелые. Вот задача запустилась, некоторое время поработала,
[18:09.480 --> 18:16.000]  что-то полезное поделала. А у нас здесь задачи очень короткие. И вот почему мы видим такой профиль?
[18:16.000 --> 18:21.540]  Потому что время работы задачи, оно сопостоимое с временем захвата блокировки. Это просто
[18:21.540 --> 18:27.960]  величина одного порядка. И вот ровно поэтому мы на блокировку, именно поэтому здесь уже нужно
[18:27.960 --> 18:33.120]  тратить время на оптимизацию вот этого самого Mutex, а точнее не Mutex оптимизировать, а пытаться его
[18:33.120 --> 18:39.480]  с пути планировщика убрать, чтобы он запускал задачи и при этом не пытался захватывать постоянно
[18:39.480 --> 18:46.480]  для каждой задачи один и тот же разделяемый Mutex. Но вот, кстати, мы сегодня будем разбирать,
[18:46.480 --> 18:53.400]  не кстати, сегодня будем разбирать, как написать такой хороший эффективный планировщик и будем
[18:53.400 --> 19:00.720]  делать это на примере языка Go. Так вот, в языке Go вот до 2012 года был такой вот наивный планировщик,
[19:00.720 --> 19:05.000]  который просто работал, но работал неэффективно, не масштабировался. А потом появился Дмитрий
[19:05.000 --> 19:10.720]  Вьюков. Это такой очень известный человек-операход, который придумал огромное количество всего в
[19:10.720 --> 19:15.320]  синхронизации, придумал огромное количество разных паттернов, трюков. И на аватарке то ли
[19:15.320 --> 19:20.880]  канала, то ли чат у нас Дмитрий Вьюков, вы должны его запомнить. И он написал вот половину runtime
[19:20.880 --> 19:27.320]  Go, которая связана с запуском Go routine. Вот написал планировщик, масштабируемый планировщик. И на всякий
[19:27.320 --> 19:34.040]  случай, ну не на всякий случай, просто посоветую вам, посмотрите его доклад чудесный, 2019 года он
[19:34.040 --> 19:38.800]  выступал на конференции в Петербурге, гидра называется, рассказывал, как этот планировщик устроен. Ну вот,
[19:38.800 --> 19:50.840]  мы сегодня про это и поговорим. И я буду иногда пользоваться его слайдами. Ну о чем он говорит,
[19:50.840 --> 19:55.400]  что вот есть наивный планировщик, когда у вас потоки для того, чтобы исполнять горутины,
[19:55.400 --> 20:00.880]  но горутины, файберы, задачи в нашем случае, в случае Goet задачи это горутины конкретно, в случае нашего
[20:00.880 --> 20:06.600]  планировщика это абстрактные задачи. Вот эти потоки, они сталкиваются на Mutex и вот там упорядочиваются,
[20:06.600 --> 20:11.520]  никакого масштабирования не происходит. Ну lock free, в общем, та же самая история, все равно у нас
[20:11.520 --> 20:20.400]  есть contention на ячейке памяти. Как же побороть проблему с масштабированием? Как же избавиться,
[20:20.400 --> 20:25.520]  ну как же масштабировать, когда у нас есть некоторая точка contention, ну избавиться от нее.
[20:25.520 --> 20:34.080]  Сделать систему распределенной, сделать так, чтобы, ну говорят, шардировать состояние. Так,
[20:34.080 --> 20:40.920]  чтобы каждый поток, каждый forker в threadpool, каждый поток планировщика работал независимо со
[20:40.920 --> 20:47.040]  своим состоянием, со своими ячейками памяти и с другими потоками у него не было необходимости,
[20:47.040 --> 20:53.480]  ну по крайней мере, часто синхронизироваться. Что именно предлагается? Ну давайте просто сделаем
[20:53.480 --> 20:59.440]  вот этот самый распределенный планировщик, шардированный планировщик, где у каждого потока
[20:59.440 --> 21:07.480]  воркера будет своя локальная очередь задачи и он будет работать в первую очередь с ней. Когда
[21:07.480 --> 21:12.560]  он планирует, когда мы планируем задачу и планируем ее из планировщика, ну скажем, мы
[21:12.560 --> 21:20.320]  анлочим mutex в файбере и это приводит к возобновлению другого файбера, который ждал этого mutex.
[21:20.320 --> 21:24.760]  Ну давайте его запланируем не в какую-то глобальную очередь, для которой нам придется
[21:24.760 --> 21:28.760]  брать блокировку или какой-нибудь лог-приписать, но все равно contention будет. Давайте положим ее
[21:28.760 --> 21:36.960]  в локальную очередь. Когда мы будем забирать задачу, посмотрим на локальную очередь. Если
[21:36.960 --> 21:42.640]  там что-то есть, то вот берем и исполняем. Кажется, что тогда вот на быстром пути,
[21:42.640 --> 21:49.040]  когда у всех много работы и она равноверно распределена, то воркеры не будут вообще
[21:49.040 --> 21:54.440]  общаться друг с другом и у них не будет синхронизации и они могут работать из
[21:54.440 --> 22:00.840]  этого физически параллельно. Вот общие ячейки памяти убивают параллелизм. Здесь у нас их
[22:00.840 --> 22:11.680]  может просто не быть. Мы не предсказываем, какие задачи легче, какие тяжелее. В файберах
[22:11.680 --> 22:17.520]  они все не длинные. Если мы в задачах в файберах делаем много работы, то это не… то файберы
[22:17.520 --> 22:19.760]  используются не по назначению. Да.
[22:19.760 --> 22:45.160]  Мало. Ну смотри, да, я не показал. Давай ускорим предпул и посмотрим, как он начнет работать.
[22:45.520 --> 23:00.160]  Ну не все так могут. У вас будет возможность это сделать. Ну там было что-то три секунды,
[23:00.160 --> 23:07.080]  да, отработало все. Три или что-то такое. Ну вот этот планировщик распределенный,
[23:07.080 --> 23:11.400]  шардированный, в котором нет. Ну ладно, я говорю, там есть глобальная очередь,
[23:11.400 --> 23:24.120]  и я сейчас про нее поговорю. Но на быстром пути ее нет. Ну мы же понимаем, как компьютер устроен.
[23:24.120 --> 23:27.640]  Мы послушали лекцию про кэши, мы посмотрели в профайлере, мы видим, мы понимаем, что в нашем
[23:27.640 --> 23:31.880]  планировщике есть контеншн. Мы понимаем, что время захвата задачи в планировщике сопоставим
[23:31.880 --> 23:36.400]  с временем работы самой задачи. Ну вот, все это указывает на то, что нужно планировщик
[23:36.400 --> 23:43.200]  шардировать. Мы это сделали, и вот теперь процедура воркера занимает там 15 процентов. Ну вру,
[23:43.200 --> 23:49.280]  тут еще внутри задачи выполняется код рантайма немного. Ну вот, кстати, здесь видно, что на
[23:49.280 --> 23:54.520]  переключение контекста мы тратим два процента времени. Вот это наша цена за стэкфалка рутины.
[23:54.520 --> 24:04.240]  К этому мы в конце еще вернемся. Ну вот, почему наш код работает быстрее? Потому что вот воркеров
[24:04.240 --> 24:10.320]  стало меньше. Полезная нагрузка была, так и осталась. А планировщик теперь работает незаметнее,
[24:10.320 --> 24:19.280]  потому что он работает на... потому что он не общается... потому что воркеры в планировщике
[24:19.280 --> 24:28.120]  реже обращаются к раздряемым ячейкам памяти. Ну вот, это все довольно естественные вещи,
[24:28.120 --> 24:32.040]  что мы такого ожидали, и так на самом деле и происходит. И сейчас мы будем обсуждать,
[24:32.320 --> 24:40.760]  а как именно такой планировщик сделать. Локальные очереди завести их, наверное, не сложно. В чем
[24:40.760 --> 24:47.840]  сложность такой реализации? В том, что избавляясь от централизованного состояния, от одной общей
[24:47.840 --> 24:54.320]  очереди, мы теряем многие приятные свойства. Ну, например, нам будет сложнее засыпать в
[24:54.320 --> 24:59.680]  ожидании работы, потому что раньше мы посмотрели под мьютоксом на очередь, видим, что она пустая
[24:59.680 --> 25:05.800]  и заснули, значит. А теперь мы не можем так заснуть, потому что у нас состояние децентрализовано,
[25:05.800 --> 25:10.920]  и мы не можем атомарно проверить, что нигде нет задач. Ну, к этому мы придем еще. Другая сложность
[25:10.920 --> 25:16.160]  в том, что нужно балансировать нагрузку аккуратно. Вот пока в таком дизайне совершенно непонятно,
[25:16.160 --> 25:20.320]  почему не получится какой-нибудь перекос, почему на каком-нибудь потоке воркере будет
[25:20.320 --> 25:26.120]  грутина, которая много запускает других грутин, а на другом таких грутин не будет, и в итоге у
[25:26.120 --> 25:33.160]  одного воркера будет больше работы, будет длиннее очередь, и этот воркер, ну, и некоторые грутины не
[25:33.160 --> 25:40.440]  будут исполняться, хотя могли бы. В одной общей очереди все было просто. Если в очереди общие были
[25:40.440 --> 25:47.400]  задачи, значит, все воркеры в тредкуле уже нагружены работой. Здесь это уже не так. Ну, в общем,
[25:47.400 --> 25:52.920]  здесь появляется сложность с балансировкой нагрузки. Ну, я сразу скажу, что у нас все-таки будет
[25:52.920 --> 25:59.000]  общая очередь, которая, в частности, эту проблему решает, и она будет защищена мютоксом, но просто к ней
[25:59.000 --> 26:03.880]  мы будем обращаться редко. Будем, но все-таки редко. Она будет помогать все-таки балансировать
[26:03.880 --> 26:11.720]  нагрузку между этими локальными очередями. Вот. В принципе, можно было бы дальше код писать,
[26:11.720 --> 26:16.960]  но все-таки мы будем говорить про планировщик Go сегодня, а у него есть еще одна неприятная
[26:16.960 --> 26:23.640]  особенность, но не то что неприятная, специфичная для Go особенность в том, что в Go не то чтобы
[26:23.640 --> 26:30.440]  файбер, там есть грутины, и это только грутины. То есть никаких потоков у вас нет, есть только
[26:30.440 --> 26:36.840]  легковесные потоки в виде грутин. И поэтому возникает такая вот проблема, что...
[26:36.840 --> 26:50.920]  секунду, я найду подходящую картинку. Идея следующая. Но проблема следующая, что у вас есть поток
[26:50.920 --> 26:58.040]  планировщика, он исполняет некоторую грутину, некоторые файбер, и эта грутина может, ну,
[26:58.040 --> 27:01.600]  как-то синхронизироваться с другими через специальные примитивы, про которые язык знает,
[27:01.600 --> 27:06.720]  runtime языка знает, там mutex, канал, а может быть у нас сделается SQL и заблокируется.
[27:06.720 --> 27:15.760]  Вот. Проблема в том, что если у нас в планировщике фиксированное число потоков,
[27:15.760 --> 27:26.080]  то мы избавимся от... мы лишимся ядра целого. Поэтому в планировщике Go нужно отдельно обрабатывать
[27:26.080 --> 27:31.800]  случаи, когда поток, когда грутина, которая исполняется в потоке, хочет сделать блокирующий
[27:31.800 --> 27:38.880]  системный вызов. Поэтому в планировщике Go выделяется еще одна сущность. Там есть
[27:38.880 --> 27:47.240]  грутины, там есть потоки, которые эти грутины исполняют, и есть еще сущность, которая называется
[27:47.240 --> 27:58.400]  процессор. Вот, процессор – это очередь задач. Это некоторое состояние, где лежит локальная очередь.
[27:58.400 --> 28:08.640]  И чтобы поток мог исполнять грутины, он должен завладеть некоторым процессором. А если же
[28:08.640 --> 28:16.840]  поток планировщика исполняет грутину, и эта грутина делается SQL, то runtime процессор
[28:16.840 --> 28:25.560]  отпускает и блокируется в системном вызове. А другой поток планировщика подбирает процессор
[28:25.560 --> 28:31.600]  и исполняет дальше его задачи. Нам это не очень актуально, потому что мы используем файберы намеленно,
[28:31.600 --> 28:36.640]  мы понимаем, как их правильно использовать, и не должны делать SQL внутри файберов. Ну, блокирующий
[28:36.640 --> 28:44.720]  SQL, блокирующий поток. В ГОА у вас нет такого выбора потоки, потоки грутины, есть только грутины,
[28:44.720 --> 28:50.200]  поэтому появляется такая дополнительная сложность. Она не по существу, то есть для нас это неважно,
[28:50.200 --> 28:53.640]  для нас можно думать про такую картинку. Просто формально в коде, который мы будем читать,
[28:53.640 --> 29:00.800]  есть несколько сущностей. Вот они называются грутины, процессоры. Грутина – это просто
[29:00.800 --> 29:07.360]  исполняемая задача, по сути. Процессоры – это очередь задач. И 3D, которые называются машинами.
[29:07.360 --> 29:20.320]  М. Ну, смотрите, мы будем читать код на ГО сейчас. И вот в runtime коду на ГО, смотрите,
[29:20.320 --> 29:27.480]  что есть. Есть классы, которые называются… Давайте сейчас мы их найдем. Runtime2, все правильно.
[29:27.480 --> 29:43.960]  Есть грутина. Тут у ГО довольно лаконичный style-гайд. Мы не должны разделять восторги по
[29:43.960 --> 29:48.120]  поводу этого style-гайда, но вот нам приходится сегодня с ними жить. В принципе, вы даже ГО,
[29:48.120 --> 29:51.880]  вы можете не знать, как мы будем читать код, но он похож на C. Если вы все их знаете примерно,
[29:51.880 --> 29:59.880]  то вот в целом считайте, что это такое, что-то подобное. Вот есть структура грутины, есть структура
[29:59.880 --> 30:07.320]  процессора. И смотрите, в процессоре есть очередь. Это локальная очередь. Это массив,
[30:07.320 --> 30:16.840]  в котором лежит 256 пойнтеров, но может лежать 256 пойнтеров на грутины. Вот у каждого потока,
[30:16.840 --> 30:23.880]  который исполняет грутины в планировщике ГО, есть локальная очередь на 256 слотов. И есть два
[30:23.880 --> 30:30.280]  пойнтера Head и Tail, которые вот бегают по ней. Циклический буфер. На семинарах мы разбирали,
[30:30.280 --> 30:34.600]  надалим память, циклический буфер. На лекции мы разбирали циклический буфер. В кышах сегодня
[30:34.600 --> 30:46.200]  он пригодится. Вот, это локальная очередь. Есть еще структура, которая называется М-машина. И есть
[30:46.200 --> 30:54.280]  разделяемое состояние планировщика, которое описывается структурой ShedT, и там есть глобальная
[30:54.280 --> 31:05.760]  очередь. Ну вот, ровно тот дизайн, который описан на этом слайде. Вот это грутины, это процессор,
[31:05.760 --> 31:12.080]  это машина, это разделяемое состояние с глобальной очередью. Здесь 256 слотов,
[31:12.080 --> 31:21.600]  здесь неограниченное количество элементов. Пока понятно? Ну а теперь давайте разбираться,
[31:21.600 --> 31:30.640]  как работает планировщик. Ну вернее, как он работает. А чтобы про это говорить, нужно подумать,
[31:30.640 --> 31:39.520]  а как вообще планировщик вступает в работу, когда он запускается. Во-первых, программа запускает
[31:39.520 --> 31:46.240]  код планировщика, когда она планирует новую работу. Ну скажем, мы отпустили Mutex и разбудили
[31:46.240 --> 31:52.200]  другую грутину. То есть когда мы добавляем работу в планировщик, нужно выполнить код планировщика.
[31:52.200 --> 31:59.960]  А во-вторых, когда текущая грутина по каким-то причинам останавливается, и планировщику нужно
[31:59.960 --> 32:08.800]  выбрать новую грутину для исполнения. То есть добавление работы и поиск новой работы. Вот давайте
[32:08.800 --> 32:13.760]  сначала про добавление работы. Когда мы в планировщик добавляем новую работу,
[32:13.760 --> 32:23.720]  новую грутину или новые файберы для запуска? Перечислите варианты. Запускаем новую грутину,
[32:23.720 --> 32:28.240]  запускаем новые файберы. На всякий случай, я буду сегодня говорить то про ГО, то про наши файберы,
[32:28.240 --> 32:35.560]  в принципе разницы не делаете. Синонимы сегодня нет. Мы запускаем новую грутину. Значит,
[32:36.040 --> 32:41.640]  в планировщике появляется работа, планировщик должен куда-то ее положить, эту работу. Другой
[32:41.640 --> 32:55.880]  пример какой-нибудь. Кажется, работа тогда исчезает и не появляется. Ну мы YIL делаем,
[32:55.880 --> 33:00.520]  например. У нас как бы текущая грутина останавливается, а потом снова добавляется
[33:00.520 --> 33:08.560]  в планировщик. Снова нужно куда-то ее положить. Ну или в общем случае мы будем кого-то. Мы
[33:08.560 --> 33:17.320]  отправили по каналу сообщение о другой грутине. Отправили, и вот эта грутина теперь разбудилась
[33:17.320 --> 33:20.880]  и может выполняться. Нам нужно куда-то в планировщику ее запланировать. Или мы
[33:20.880 --> 33:25.440]  отпустили Mutex, и какая-то грутина сможет его теперь захватить, мы ее разбудили. Ну вот,
[33:25.440 --> 33:30.280]  давайте в рантайме посмотрим, как рантайм себя собственно ведет в таких случаях.
[33:30.280 --> 33:45.880]  Ну давайте начнем с GoShedImpl. GoShed — это функция в рантайме Go, которая эквивалентна YIL. Ну то есть,
[33:45.880 --> 33:51.200]  она перепланирует текущую грутину. Текущая грутина говорит рантайму среди исполнения
[33:51.200 --> 33:55.800]  планировщику, что ей не очень-то нужно сейчас исполняться. Она готова уступить место другим.
[33:55.800 --> 34:03.080]  Вот наш Fiber Yield. Смотрите, что мы делаем в таком случае. Мы говорим glob rank you put.
[34:03.080 --> 34:12.160]  Привыкайте к именам, они похожи на C. glob — глобальная очередь, rank you, put — добавить.
[34:12.280 --> 34:22.040]  glob rank you put — смотрим. Мы под локом планировщика добавляем грутину в глобальную очередь. То есть,
[34:22.040 --> 34:28.280]  смотрите, у нас есть локальные очереди, есть глобальная очередь, и Yield — он очень пессимистичный.
[34:28.280 --> 34:39.680]  Он бросает грутину прямо вот сюда. Не в локальную очередь, а прямо далеко-далеко. А что, если мы
[34:39.680 --> 34:50.160]  посмотрим на новую грутину? Это уже функция rank you put. Не glob rank you put, а просто rank you put.
[34:50.160 --> 34:59.600]  Эта функция, ну пока пропустим эту забавную строчку, и это пропустим, и много всего пропустим. Ну в общем,
[34:59.600 --> 35:06.560]  даже пока я код не показываю, разберёмся с этим чуть позже. Эта функция добавляет грутину в
[35:06.560 --> 35:17.400]  локальную очередь. То есть, в конец локальной очереди. Вот сюда. Когда мы будем другую грутину,
[35:17.400 --> 35:35.560]  то опять мы добавляем её в локальную очередь. Разумно? Разумно. Теперь смотрим, как этот самый
[35:35.560 --> 35:43.120]  rank you put реализован. Ну как реализован глобал rank you put, мы уже посмотрели. Мы под мьютоксом
[35:43.120 --> 35:50.160]  добавляем задачу в очередь, увеличиваем размер очереди. Как реализован этот rank you, чуть позже
[35:50.160 --> 35:57.560]  к этому вернёмся, это важно, кстати. Но пока смотрим на rank you put. Rank you put добавляет задачи в
[35:57.560 --> 36:09.400]  локальную очередь. Локальную очередь, я напомню, это циклический буфер. Это циклический буфер из
[36:09.400 --> 36:22.960]  256 элементов. Что? А как сделать буфер ограниченного размера в ограниченном, в массиве ограниченной
[36:22.960 --> 36:37.800]  ёмкости? Ну, потому что будет странно, если в одной грутине, если у одного воркера в одной его
[36:37.800 --> 36:44.680]  локальной очереди будут скапливаться и скапливаются новые грутины, а у других этих грутин не будет.
[36:44.680 --> 36:49.600]  То есть, мы хотим всё-таки нагрузку балансировать. Ну вот ограничение на локальную очередь, это такое
[36:49.600 --> 36:56.600]  необходимое следствие. Ну, необходимость, потому что нам всё-таки нужно как-то распределять задачу
[36:56.600 --> 37:04.400]  равномерно между воркерами, между машинами. Не должно быть большого перекоса. Вот это ограничение,
[37:04.400 --> 37:14.760]  оно вот про перекос максимально. Окей. Значит, мы должны добавить задачу в циклическую очередь,
[37:14.760 --> 37:23.280]  в конец циклической очереди. Что нужно сделать? Прочитать tail, записать туда в слот по этому индексу
[37:23.280 --> 37:31.440]  pointer на грутину, потом сдвинуть tail вперёд. Да? Ну вот мы так и делаем. Мы читаем head, мы читаем tail,
[37:31.440 --> 37:39.760]  вспоминаете лекцию про кыши и вспоминаете семинар про расстановку memory-order в слабых в циклическом буфере.
[37:39.760 --> 37:44.720]  Если у вас его не было ещё, то требуйте срочно, потому что вот же нам это нужно прямо сейчас
[37:44.720 --> 37:51.320]  планировщики. Мы читаем head, читаем tail. Обращаем внимание, здесь мы читаем tail с relax
[37:51.320 --> 37:56.040]  статомиком, потому что только мы меняем tail. Ну, кто был на семинаре, по крайней мере, на моём
[37:56.040 --> 38:05.320]  последнем, это знают. Смотрим, если в буфере ещё есть элементы, слоты доступные, то хорошо,
[38:05.320 --> 38:14.640]  мы помещаем в индекс tail по модулю длины очереди, длины в смысле капасти, размера очереди,
[38:14.640 --> 38:23.280]  к грутину и двигаем tail вперёд. Ну, я понимаю, что вы, может быть, первый раз видите код на го,
[38:23.280 --> 38:33.680]  особенно вот так, так написанный, но тут не должно быть ничего особенного, удивительного, да? Вроде
[38:33.680 --> 38:38.360]  всё понятно. Вот, ну, смотрите, но если в очереди уже не было слотов, если она переполнилась, там
[38:38.360 --> 38:48.800]  256 элементов, то мы проваливаемся в rank you put slow. И что мы там делаем? Ну, смотрите, у нас
[38:48.800 --> 38:57.840]  переполнилась локальная очередь. Что бы вы сделали в таком случае? Мы бы передвинули задачи в глобальную
[38:57.840 --> 39:03.800]  очередь. Сколько задач? Все? Это было странно, у нас было много работы, теперь у нас стало мало работы,
[39:03.800 --> 39:09.880]  нам ведь теперь не хватает, что же нам делать? Нет. Ну, не нужно двигать мало, в смысле одну,
[39:09.880 --> 39:15.480]  потому что очередь снова быстро переполнится, видимо. Ну, то есть, смотрите, rank you put slow
[39:15.480 --> 39:22.400]  выгружает часть локальной работы в общую очередь, чтобы работа балансировалась. Но эта общая
[39:22.400 --> 39:28.720]  очередь — это общий mutex, точка contention. Нам нужно минимизировать работу с этим mutex. Поэтому
[39:28.720 --> 39:35.400]  мы хотим амортизировать эти расходы на синхронизацию. Мы хотим, в данном случае, передвинуть половину
[39:35.400 --> 39:40.480]  нашей очереди, чтобы и у нас что-то осталось, а с другой стороны, чтобы у других много появилось.
[39:40.480 --> 39:47.680]  То есть, чтобы у нас и много осталось, и был ещё зазор по слотам. Поэтому мы берём, вычисляем
[39:47.680 --> 39:56.280]  размер нашей очереди, берём половину и формируем такую вот пачку. То есть, читаем из нашего локального
[39:56.280 --> 40:05.280]  массива rank you, из вот rank you нашего процессора, которым владеем мы данная машина, поток,
[40:05.280 --> 40:14.800]  worker, и пытаемся эту пачку выгрузить в глобальную очередь. Но мы делаем там это всё довольно хитро,
[40:14.800 --> 40:19.360]  потому что, смотрите, ну, во-первых, нельзя в глобальную очередь класть задачи по одной,
[40:19.360 --> 40:27.200]  потому что мы там много раз возьмём mutex. Нужно положить в rank you сразу пачку,
[40:27.200 --> 40:37.040]  но сделано ещё хитрее. Я забыл, как функция называлась, чёрт возьми.
[40:45.720 --> 40:53.520]  Смотрите, она берёт mutex и работает там за константу, потому что очередь, это на самом деле
[40:53.520 --> 41:07.200]  интрузивная. То есть, смотрите, мы достали из локальной очереди пачку задач groutine,
[41:07.200 --> 41:18.680]  которую мы выгрузим наверх в глобальную очередь. Мы связали их поинтерами, и потом уже мы за
[41:18.680 --> 41:25.040]  константу под mutex добавили сразу много задач в разделяемую очередь. То есть, мы минимизируем
[41:25.040 --> 41:29.640]  время критической секции, которая нам всё-таки тогда необходима. Мы к ней обращаемся редко,
[41:29.640 --> 41:35.840]  когда очередь переполняется, а когда обращаемся, делаем это аккуратно. То есть, уж точно не зовём
[41:35.840 --> 41:43.040]  пут на каждую groutine, и ещё оптимизируем интрузивностью. Но у нас в курсе есть
[41:43.040 --> 41:47.280]  интрузивные контейнеры, вы с ними уже работали, а вот там есть тоже списки, и вы можете
[41:47.280 --> 41:56.960]  конкатинировать их за констант. Это важно. Понятно пока, да? Только смотрите, ещё какой нюанс есть.
[41:56.960 --> 42:03.160]  Когда мы двигаем tail вперёд в ru, когда мы двигаем head вперёд, то есть мы пытаемся забрать из
[42:03.160 --> 42:10.080]  локальной очереди половину элементов. Вот когда мы их забираем, мы почему-то двигаем head вперёд не
[42:10.080 --> 42:17.600]  просто стором, а почему-то кассом. Хотя довольно странно, потому что очередь локальная, и работаем
[42:17.600 --> 42:25.000]  же с ней только мы. Ну, к этому я вернусь ещё, тем более я вам даже не рассказал пока почему,
[42:25.000 --> 42:36.280]  что делает этот код. Я его как-то пропустил. Но пока и ладно, пока неважно. Когда мы добавляем в
[42:36.280 --> 42:42.760]  планировщик новую работу, мы пытаемся добавить эту новую groutine запланированную в конец локальной
[42:42.760 --> 42:48.400]  очереди. Если же локальная очередь переполнилась, если там 256 элементов, то мы выбираем половину
[42:48.400 --> 42:54.040]  этих элементов из локальной очереди, провязываем их ссылками, захватываем минуток с общей очереди
[42:54.040 --> 43:00.640]  и перекладываем половину туда. Вот, значит, половина планировщика рассказана, как планируется новая
[43:00.640 --> 43:09.640]  работа. А теперь самое главное в планировщике, как он новую работу себе выбирает. Вот, планировщик
[43:09.640 --> 43:16.000]  выбирает себе новую работу, когда текущая groutine решила заснуть. И для этого у планировщика есть
[43:16.000 --> 43:25.920]  функция, она называется shadow. Вот, функция. Вот, смотрите, мы вызываем go shed. Мы вызываем
[43:25.920 --> 43:49.680]  go shed. Плохой, необыченный. Вот, я вам всё-таки его покажу. Вот, go shed это функция, которая
[43:49.680 --> 44:00.520]  перепланирует текущую groutine. Вот, мы её вызываем, и что происходит? Мы кладём себя в глобальную очередь,
[44:00.520 --> 44:05.400]  далеко-далеко, а потом вызываем функцию shadow, потому что сейчас нам нечего делать. Нам, в смысле,
[44:05.400 --> 44:12.200]  текущей машине, текущему потоку воркера. Функция shadow, ну, тут много всего происходит,
[44:12.200 --> 44:18.120]  потому что runtime это сложная штука, тут сборка мусора, вытеснения. В конце концов вызывается
[44:18.120 --> 44:24.400]  выбранная groutine. Какая-то groutine выбирается и вызывается, а перед этим она выбирается. И для этого
[44:24.400 --> 44:36.680]  есть функция, которая называется, давайте мы её найдём, она называется find runnable. Нужно найти
[44:36.680 --> 44:43.920]  очередную groutine для исполнения. Ну, давайте думать, что мы делаем. Сначала, разумеется,
[44:43.920 --> 44:51.120]  мы пробуем достать groutine из локальной очереди текущего потока, текущей машины. Ну, зачем нам
[44:51.120 --> 45:04.760]  синхронизироваться с другими? Runqget. И тут снова мы читаем head, читаем tail, читаем groutine из
[45:04.760 --> 45:13.080]  хеда, и почему-то двигаем хед опять кассом. А если не получилось, то ретравимся. Опять непонятно,
[45:13.080 --> 45:16.960]  почему мы так делаем, потому что в циклическом буфере вроде бы, ну, потому что вроде бы один,
[45:16.960 --> 45:25.360]  только один поток работает с этой очередь. Ну, ладно, непонятно, но оставим это на будущее. Вот
[45:25.360 --> 45:30.960]  может быть, да. Вот мы сейчас это и увидим, собственно. Сначала мы идём в локальную очередь,
[45:30.960 --> 45:35.600]  пытаемся брать задачу оттуда. Если получилось, нашлось, то вот здорово, мы сэкономили себе
[45:35.600 --> 45:41.760]  синхронизацию. Вот именно за счёт этого быстрого пути планировщик быстро и работает. Именно поэтому
[45:41.760 --> 45:48.520]  в профиле мы видим мало планировщика и много полезной работы. Но если не получилось, то мы идём в
[45:48.520 --> 46:00.720]  глобальную очередь. И опять, оттуда мы забираем, видимо, не одну задачу, а много. Половину мы
[46:00.720 --> 46:11.800]  берём runQsize, дерем её на число потоков и пытаемся забрать столько. Вот, одну катую, где какая-то число ядер.
[46:11.800 --> 46:29.160]  Хорошо, попробовали, не получилось. Что делать дальше? Ну, дальше нужно учесть, что мы пишем всё-таки
[46:29.360 --> 46:37.560]  runtime язык ОГО. ОГО есть, кроме запуска грутин, есть там таймеры, есть сеть. И вот прямо здесь
[46:37.560 --> 46:45.240]  можно сделать епол. Ну, то есть, какую-то работу, которая может породить какие-то события внешние,
[46:45.240 --> 46:51.600]  в смысле, реагировать на внешние события и запланировать новые задачи. То есть, мы, смотрите,
[46:51.600 --> 46:58.880]  мы поток планировщика. У нас runQ пустая, глобальная runQ тоже пустая. Работы не так много, видимо.
[46:59.000 --> 47:08.320]  Вот почему бы не отвлечься и не нагенерировать событий внешних? Ну вот, если этого не получилось,
[47:08.320 --> 47:17.360]  то смотрите, что нужно делать. Ну, предлагается вуровать у других потоков. Ну, потому что, может быть,
[47:17.360 --> 47:25.400]  ни у кого прям больших излишков нет, но прямо скажем, мы сейчас недоунтируем процессор. Может
[47:25.400 --> 47:32.960]  быть, у кого-то в очереди 100 грутин, а у нас сейчас 0 грутин. Поэтому что мы делаем? Мы...
[47:32.960 --> 47:45.800]  Давайте найдем код. Мы вызываем функцию stillwork. А там мы вот некоторое количество раз берем и
[47:45.800 --> 47:57.200]  обходим другие процессоры, то есть, другие очереди в каком-то случайном порядке, и пытаемся
[47:57.200 --> 48:08.960]  у них захватить задачку. RunQ still. И вот ровно поэтому у нас локальная очередь неоднопоточная. В нее
[48:08.960 --> 48:19.720]  добавляет только один поток, который владеет этой очередью. Поэтому в RunQ put здесь просто store.
[48:19.720 --> 48:27.480]  А вот извлекает из этой очереди несколько потоков. Во-первых, кто этой очереди владеет,
[48:27.480 --> 48:36.200]  а во-вторых, тот, кто из нее ворует. Ну и опять, если уж мы воруем, то это же синхронизация. Лучше
[48:36.200 --> 48:47.440]  делать это пореже. Лучше про запас себе набрать грутин. Поэтому мы пытаемся захватить сразу какое-то
[48:47.440 --> 48:55.240]  количество. Ну половину прям пытаемся своровать. Мы читаем head, чужое читаем tail. Мы передаем
[48:55.240 --> 49:01.720]  буфер, куда нужно переложить сворованные грутины. А дальше мы пытаемся... Смотрите, мы читаем
[49:01.720 --> 49:09.440]  в этот локальный буфер сворованные грутины, а потом берем и пытаемся кассами передвинуть head
[49:09.440 --> 49:14.400]  вперед на n. Если получилось, то мы своровали. Если не получилось, то, видимо, кто-то другой
[49:14.400 --> 49:20.280]  с этой очередью работал, тоже воровал, или просто поток, который владел этой очереди, доставал
[49:20.280 --> 49:27.960]  задачи, грутины. Так что, может быть, нужно претравиться заново. Идея ясна? А вот теперь,
[49:27.960 --> 49:34.600]  смотрите, много тонкостей начинается. Во-первых, обратите внимание, что у нас индексы head и tail,
[49:34.600 --> 49:45.640]  они не закругляются. Они монотонно растут. И мы каждый раз берем по модулю. Понятно ли зачем?
[49:45.640 --> 49:56.920]  Вот тут нельзя написать закругление индексов. Тут важно, что они монотонно растут. Да, потому
[49:57.000 --> 50:02.680]  смотрите, что может получиться. У нас была грутина, которая воровала задачи. Она пытается
[50:02.680 --> 50:11.200]  передвинуть head с нуля до двух, с нуля до десяти. Сорова десять задач. Она прочитала их себе в буфер
[50:11.200 --> 50:19.440]  поэнтеры, а потом пытается передвинуть tail, head кассам. Но между вот этим чтением в буфер и
[50:20.440 --> 50:27.840]  кассам очередь переполнилась, потом пошла по второму кругу, и в итоге head теперь снова стал ноль,
[50:27.840 --> 50:36.400]  и у нас случилось ABA. Мы подумали, что у нас касс успешный, хотя на самом деле очередь
[50:36.400 --> 50:42.600]  сильно изменилась. Так что мы используем здесь монотонные индексы, и ABA у нас не бывает. Понятно?
[50:42.600 --> 50:56.040]  Ну вообще, тут почему-то в runtime я не знаю ответа. Head и tail 32. Они могут переполниться,
[50:56.040 --> 51:06.360]  разумеется. Ну, я не знаю, можно было бы просто 64 сделать, и они бы не переполнились никогда в жизни.
[51:13.600 --> 51:19.160]  Ну, порядочные люди избегают даже такого. Ты прав, Ром, что такого скорее всего не будет,
[51:19.160 --> 51:29.440]  но какая-то… Ну, не знаю, виртуалку поставили на… Ладно, не подойдет. Короче, сборка мусора началась.
[51:29.440 --> 51:37.600]  Там… Смотри, если исправить на 64, то просто станет безопасно всегда. Ну,
[51:37.600 --> 51:49.440]  да ладно, я не спорю. Я не спорю, но в домашней работе напишите, пожалуйста, 64. Вот, а теперь
[51:49.440 --> 52:02.760]  второй очень важный момент. Переполнение будет больным. Ну, видимо, они считают, что не…
[52:02.760 --> 52:17.760]  Теперь следующий важный момент, который касается этого простого кода. Открываем задачу,
[52:17.760 --> 52:30.840]  задачу, которая называется файберы карутина. И там говорят, что с тредпулом нашим… Ну,
[52:30.840 --> 52:38.680]  мы пишем тредпул, используем его для запуска файберов. И при этом говорят, что вот пользователи
[52:38.680 --> 52:43.040]  тредпула, которые не будут запускать файберы, будут использовать функцию submit вместо того,
[52:43.040 --> 52:50.400]  чтобы бросать задачу прямо через мета тредпула. Почему? Потому что тогда можно сделать
[52:50.400 --> 52:55.480]  интрузивность. И про эту интрузивность повторяют в каждой задаче. Вот в экзекьюторах повторяют,
[52:55.480 --> 53:08.760]  говорят… Ну, умоляют уже, можно сказать. Сделайте, наконец, интрузивность, говорят, задача,
[53:08.760 --> 53:18.240]  потому что тогда… Ну, потому что добавление любой задачи в тредпул – это добавление в контейнер
[53:18.240 --> 53:23.480]  некоторой внутренней функции, да, объекта function. Объект function – это type rager. У него
[53:23.480 --> 53:28.560]  фиксированный размер, а лямбда в нем может быть любого размера произвольного типа. Нужно стереть
[53:28.560 --> 53:34.200]  ее тип, нужно положить его на кучу, повесить на нее pointer, сделать динамическую локацию. Так вот,
[53:34.200 --> 53:40.440]  файбером, фьючем, корутином, которые будут у нас в stackless C++ эти динамические локации не
[53:40.440 --> 53:48.880]  нужны. У нас один раз файбер алоцируется сам по себе на куче, и вот он сам и может быть задачей.
[53:48.880 --> 54:10.720]  Вот в хорошем коде… В хорошем коде задача… Задача – это pointer. Это pointer прямо на файбер,
[54:10.720 --> 54:17.680]  который служит задачей. Или это pointer на такую… Или это pointer, который построила функция submit.
[54:17.680 --> 54:25.200]  Она взяла лямбду, алоцировала на куче, поставила pointer туда. Короче говоря, если… Но аккуратная
[54:25.200 --> 54:33.920]  реализация файберов не делает лишних локаций в самом планировщике. Так вот, это было до текущего
[54:33.920 --> 54:39.400]  момента, это была просто аккуратность. А сейчас это необходимость. Посмотрите на этот код в го.
[54:39.400 --> 54:49.240]  Мы здесь читаем из буфера при воровстве. И вот мы читаем из ячейки буфера pointer на
[54:49.240 --> 54:57.040]  грутина. А другие потоки тоже могут воровать же, да? А поток, который владеет этой очередью,
[54:57.040 --> 55:04.160]  может туда писать прямо сейчас. Мы не можем уже в планировщике хранить сложные объекты задачи.
[55:04.160 --> 55:08.560]  Мы не можем функционы хранить, потому что мы не можем их атомарно в одном потоке читать,
[55:08.560 --> 55:16.640]  а в другом писать. Сознали проблему? Здесь важно, что в массиве хранятся просто поэнтеры.
[55:16.640 --> 55:22.040]  И один поток их читает, другой пишет. Ну и мы бы все плюс-плюс написали здесь relaxed atomic.
[55:22.040 --> 55:29.960]  Relaxed atomic от поэнтера. Вот ровно поэтому для того, чтобы такой код эффективно написать,
[55:29.960 --> 55:36.240]  чтобы написать эффективный work steering, нам нужно в планировщике избавиться от STD-фанкшенов
[55:36.240 --> 55:43.560]  и перейти на аллокации задач снаружи пула, перейти на инклюзивные задачи. Пусть пользователь
[55:43.560 --> 55:51.040]  решает, где должна жить задача. На стеке, на куче, каждый раз виллоцировать и переиспользовать,
[55:51.040 --> 55:55.800]  потому что fiber и переиспользуемая задача. В планировщик попадают уже просто поэнтеры.
[55:55.800 --> 56:04.280]  Это важно. Вот тогда можно в планировщике сделать work steering эффективно. Без этого вы просто не
[56:04.280 --> 56:20.480]  напишете вот этот код. Вы эту строчку не сможете написать. Хорошо, я объяснил вам все это. Да,
[56:20.480 --> 56:28.080]  еще не все. Мы попытались, в любом случае у нас будет задача про то, чтобы написать work steering
[56:28.080 --> 56:36.600]  очередь, потом сделать work steering планировщика. Поэтому постепенно мы это еще раз поймем. Итак,
[56:36.600 --> 56:43.520]  мы были find runnable. Мы попробовали взять локальную задачу из локальной. Где мы вообще? Что это? Где
[56:43.520 --> 56:50.920]  я нахожусь? Find runnable, да. Мы пробовали локальную очередь, потом глобальную очередь, потом полить
[56:50.920 --> 56:57.360]  события внешние, потом пытались воровать. Только смотрите, воровать всеми потоками бесполезно,
[56:57.360 --> 57:01.520]  но плохая идея. Представьте, что у вас мало задач и у вас все потоки начинают воровать друг
[57:01.520 --> 57:06.000]  у друга. Это бессмысленно какая-то. Они будут этим заниматься без сно. Life lock получится какое-то.
[57:06.000 --> 57:13.840]  Поэтому в планировщике ограничение, что воруют не больше, чем половина потоков. Если вы
[57:13.840 --> 57:21.520]  переполняете этот лимит, то вы не воруете задачу, вы сдаетесь. И что вы сейчас сдаетесь? Вы
[57:21.520 --> 57:26.640]  пытаетесь уснуть. То есть вы попробовали взять задачу, а очередь у вас пустая. Ну, в смысле,
[57:26.640 --> 57:31.040]  очередь не какая-то локальная, а вот глобальная. Ну, в смысле, в планировщике как будто бы нет
[57:31.040 --> 57:38.600]  задач для вас. И вы готовы уснуть. В чем сложность? В том, что легко было писать такой код, когда у
[57:38.600 --> 57:46.600]  вас были мутокс и кундвары. Мы в блокирующей очереди брали лок, смотрели, если очередь
[57:46.600 --> 57:58.680]  пустая, то кундвар вейт. Да? Очень просто. Проверили пустоту одним вызовом эмпти на контейнере,
[57:58.680 --> 58:04.720]  а потом уснули, отпустили лок, и вы уверены, что пока вы засыпали, ничего не изменилось. Вот тут все
[58:04.720 --> 58:08.920]  намного сложнее в этом планировщике, потому что у вас больше нет одной очереди, нет одного мютокса.
[58:08.920 --> 58:15.840]  У вас состояние сильно децентрализовано, сильно размазано. Вы проверили локальную очередь,
[58:15.840 --> 58:21.120]  потом проверили глобальную очередь, потом проверили другие очереди. Пока вы проверяли,
[58:21.120 --> 58:27.320]  ваши знания тут же устаревают. Поэтому, когда вы решили заснуть, может быть, работа уже появилась,
[58:27.320 --> 58:34.640]  и вам нужно очень аккуратно заснуть. И вот на самом деле это засыпание, это, я бы сказал,
[58:34.640 --> 58:40.840]  самая сложная часть планировщика. Вот это все было очень легко, а вот засыпание там нужно
[58:40.840 --> 58:47.280]  помучиться. Нужно сначала объявить, что мы засыпаем, потом перепроверить, что ничего не появилось нового,
[58:47.280 --> 58:53.320]  а потом все-таки заснуть. И если с момента объявления засыпания задача появилась,
[58:53.320 --> 58:58.080]  то она нас сможет разбудить. Там есть много тонкостей, давайте оставим это на какое-нибудь
[58:58.080 --> 59:04.360]  светлое будущее. Может быть, мы это напишем кто-нибудь из вас, но вообще это сложно. Короче,
[59:04.360 --> 59:16.960]  засыпаем. В принципе, я вам рассказал про планировщика. Добавляем мы сюда,
[59:16.960 --> 59:25.760]  если переполнилось, выгружаем половину сюда. Когда мы забираем работу, мы смотрим сначала
[59:25.760 --> 59:31.880]  сюда. Если здесь пусто, идем сюда, забираем часть задач. Если и здесь пусто, то пытаемся воровать
[59:31.880 --> 59:41.280]  задача. Если нигде ничего нет, засыпаем. При добавлении мы добавляем задачу. Зачем смотреть?
[59:41.280 --> 59:58.760]  Зачем? Глобальная очередь это точка, которая, как раз, балансирует нагрузку более-менее равномерно.
[59:58.760 --> 01:00:07.400]  К ней все обращаются. Так, наверное, тоже можно было бы пробовать. Кажется, что тут проще.
[01:00:07.400 --> 01:00:24.880]  Ну, это немного разные вещи. Это, наверное, самый аккуратный ответ. Ну, мы знаем, потому что если
[01:00:24.880 --> 01:00:31.560]  бы так было лучше, Дмитрий Бюков бы написал именно так. Плохой аргумент, но он, мне кажется,
[01:00:31.560 --> 01:00:41.080]  близко к истине. Итак, алгоритм планировщика понятен, а теперь он утверждается, что он эффективный.
[01:00:41.080 --> 01:00:49.080]  В смысле, тут мало contention. Мы берем mutex на очень короткое время и очень редко. Но утверждается,
[01:00:49.080 --> 01:00:53.840]  что мы построили плохой планировщик, потому что он не то чтобы не эффективный, он нечестный.
[01:00:53.840 --> 01:01:00.720]  В него можно положить задачу, и она никогда не исполнится. Это, наверное, не то, что мы хотим
[01:01:00.720 --> 01:01:06.880]  от планировщика. Причем я даже рассказывал вам сценарий, когда такое получится. Ну, если вы
[01:01:06.880 --> 01:01:15.320]  внимательно следите за происходящим, то можете такой сценарий сконструировать. Вот в планировщике
[01:01:15.320 --> 01:01:20.360]  есть грутина, она где-то лежит. Ну, и программа так написана, что до нее не доходит дела никогда.
[01:01:20.360 --> 01:01:36.240]  Ну, смотрите, у нас грутина сделала yield, и Go ее пессимизировала, отправила глобальную очередь.
[01:01:36.240 --> 01:01:44.280]  Но так получается, что во всех локальных очередях есть работа. Там много грутина обмениваются
[01:01:44.280 --> 01:01:48.880]  данными по каналам и вот постоянно там перепланируются, у каждого ядра есть, чем заниматься. Локальные
[01:01:48.880 --> 01:02:07.200]  очереди не пустеют. А у нас в функции find runnable есть, есть приоритет у локальной очереди.
[01:02:07.200 --> 01:02:17.600]  Поэтому задача, которая попала в глобальную очередь, может никогда и не исполнится. Нужно
[01:02:17.600 --> 01:02:23.560]  это починить. Но вот для этого нужно вернуться в функцию shadow, потому что в функции shadow вызывается
[01:02:23.560 --> 01:02:37.440]  find runnable, а перед этим выполняется еще некоторый код. И смотрите, что тут происходит. Вот мы берем...
[01:02:37.440 --> 01:02:53.200]  Мы берем текущую машину, ее процессор. Shed tick — это количество итераций планировщика. Вот. И если
[01:02:53.200 --> 01:03:01.720]  количество итераций планировщика кратно 61, то мы сначала достаем из глобальной очереди задачу одну.
[01:03:01.720 --> 01:03:15.560]  Вот. А потом уже локальная очередь, потом уже find runnable. В мире есть фундаментальные константы,
[01:03:15.560 --> 01:03:22.120]  физика на них как-то опирается. Вот насколько фундаментальная константа 61. Отвечает
[01:03:22.120 --> 01:03:31.560]  разработчик этого кода. Ну, ответ сложный, конечно. Ответ сложный. Ну, это какое-то число.
[01:03:31.560 --> 01:03:42.840]  Ну, оно простое. Смотрите, вообще магические константы в коде довольно сложно. Вот знаете
[01:03:42.840 --> 01:03:47.440]  вы про click-хаус, самую быструю на свете энергетическую базу данных, которую Алексей Миловидов
[01:03:47.440 --> 01:03:51.880]  в Яндексе написал. Ну вот. Он рассказывал там в одном из своих докладов про интересные штуки.
[01:03:51.880 --> 01:03:55.840]  Там базы данных, мы кладем туда данные, они шардируются между разными машинами, между разными
[01:03:55.840 --> 01:04:04.320]  шардами по хэшу от ключа. А внутри шарда есть хэштаблица какая-то. И вот если хэш-функция,
[01:04:04.320 --> 01:04:10.600]  которая раскладывает данные по шардам, скоррелирует с хэш-функцией в хэштаблице, то ваша хэштаблица
[01:04:10.600 --> 01:04:17.120]  перестанет быть константной. Ее сильно перекоист. Понимаете проблему? Вот. Это такие забавные штуки
[01:04:17.120 --> 01:04:23.360]  возникают в жизни. Так вот. 61 для того, чтобы вот такое странное число, чтобы не коррелировать
[01:04:23.360 --> 01:04:28.600]  с какими-то другими магическими константами в коде. Вот не использовать на всякий случай 61 в своем
[01:04:28.600 --> 01:04:36.160]  коде. Это может быть опасно. Насколько это константы хорошие? Ну вот. Давайте я вам покажу
[01:04:36.160 --> 01:04:49.760]  планировщик Tokyo, framework Tokyo в языке Rust. Вот там тоже есть приоритет. Мы берем сначала из
[01:04:49.760 --> 01:04:59.560]  локальной очереди, а если там нет, из глобальной очереди. Но иногда наоборот. Иногда это когда? Вот.
[01:04:59.560 --> 01:05:06.880]  Ну утверждается, что Дмитрий Юков написал пока лучший известный планировщик и другие языки,
[01:05:06.880 --> 01:05:18.120]  что Kotlin, что Rust. Они его более-менее переписывают. Ну там с какими-то техническими отличиями,
[01:05:18.120 --> 01:05:26.360]  но фундаментальный алгоритм тот же самый. Таким образом, мы немного отсюдим честность. А теперь
[01:05:26.360 --> 01:05:34.680]  вопрос. Вот хороший, но важный вопрос. Насколько нам вообще хочется честности? Насколько нам
[01:05:34.680 --> 01:05:41.480]  хочется FIFO? Потому что это же явно не FIFO, да? Мы там раз в 61 итерацию что-то там попросим.
[01:05:41.480 --> 01:05:47.680]  Вот разумно ли вообще FIFO от планировщика требовать? Чем раньше задача была запланирована,
[01:05:47.680 --> 01:05:54.400]  тем быстрее она, тем раньше она исполнится. Ну давай операция на наши задачи.
[01:05:54.400 --> 01:06:07.280]  Совсем неразумно. Вот FIFO подходит для задач независимых. А наши файберы, они не независимые,
[01:06:07.280 --> 01:06:13.680]  они коммуницируют друг с другом. Они отпускают, там берут общие мютексы, они отправляют друг другу
[01:06:13.680 --> 01:06:21.640]  сообщения по каналу. И вот представим себе, что одна грутина ждала на канале, а другая грутина
[01:06:21.640 --> 01:06:30.200]  отправила сообщения в канал и разбудила эту. И вот мы при добавлении, при отправке сообщения в
[01:06:30.200 --> 01:06:34.680]  канал должны запланировать в планировщик вот эту спящую грутину. Она теперь готова исполнится.
[01:06:34.680 --> 01:06:48.400]  Мы её положим в локальную очередь, в конец. Но зачем мы её туда положим? Чтобы она проснулась и
[01:06:48.400 --> 01:06:58.280]  прочитала сообщения из памяти. Когда мы пишем что-то в память, куда мы пишем? В кэш. И хорошо
[01:06:58.280 --> 01:07:06.440]  мы потом из кэша прочитать. Но мы отправляем сообщения, кладём грутину в конец локальной очереди,
[01:07:06.440 --> 01:07:14.680]  и пока она там маринуется, пока до неё дойдёт очередь, вероятно, данные для этой грутины,
[01:07:14.800 --> 01:07:22.400]  которые она хотела получить, собственно, из кэша уже вытеснет, пока она там ждёт своей очереди,
[01:07:22.400 --> 01:07:27.840]  пока перед ней там ещё 100 других грутин запустится. Вот если мы отправили сообщение
[01:07:27.840 --> 01:07:34.040]  грутине, то выгодно разбудить её прям вот в следующей очереди. Сделать не фифо-планирование,
[01:07:34.040 --> 01:07:43.640]  а лифо. Запустить последнюю добавленную грутину. Потому что мы знаем, что так будет эффективнее по
[01:07:43.640 --> 01:07:49.320]  кэшу, потому что мы ради этого своей планировщикой пишем. Мы знаем, что у нас задачи коммуницируют
[01:07:49.320 --> 01:07:58.160]  друг с другом, они не независимые. Поэтому нужно сделать, видимо, лифо-планирование. Вот это
[01:07:58.160 --> 01:08:10.320]  делается очень забавно. Смотрите, у нас в функции runQput есть флажок next, оно говорит, нужно положить
[01:08:10.320 --> 01:08:17.920]  грутину в локальную очередь, но как бы вот next это означает, что вот она хочет запуститься следующей.
[01:08:17.920 --> 01:08:28.360]  Поэтому, смотрите, в структуре P рядом с runQ есть ещё вот runNext. Такой лифо-слот с длинным
[01:08:28.360 --> 01:08:39.400]  комментарием. Это такой маленький стэк из одного элемента перед очередью. И когда мы планируем
[01:08:39.400 --> 01:08:46.120]  грутину, если у нас стоит runNext, мы добавляем новую грутину в этот слот. Когда мы берем
[01:08:46.120 --> 01:08:54.080]  грутину из локальной очереди, мы сначала смотрим в этот runNext. Если там оказался не null,
[01:08:54.080 --> 01:09:03.520]  то мы берем оттуда и только иначе берем из очереди. Когда мы добавляем грутину в локальную очередь,
[01:09:03.520 --> 01:09:17.640]  то мы, если ее запланировали запуск следующей, то мы пишем в runNext наш новый pointer, и если в
[01:09:17.640 --> 01:09:24.000]  runNext уже что-то было, то мы эту вытесненную следующую грутину помещаем уже в конец runQ.
[01:09:24.000 --> 01:09:29.440]  Понятная идея? Ну, то есть такой маленький лифо-слот, маленький костылик.
[01:09:29.440 --> 01:09:41.080]  Кстати, можно проверить, дает ли это выигрыш. Давайте проверим. У меня есть другой workload,
[01:09:41.080 --> 01:09:49.120]  который про каналы. Тут запускается много пар грутин. Некоторые грутины отправляют
[01:09:49.120 --> 01:10:01.000]  сообщения в канал, а другие селектом дожидаются сообщений из канала. Ну, и я сейчас запущу код.
[01:10:01.000 --> 01:10:09.880]  Нет, я не хочу. Так а зачем мне медленный планировщик? Зачем кому-то медленный планировщик?
[01:10:09.880 --> 01:10:37.080]  Я хочу сделать сейчас вот, вот что. Так, вот код с каналами, с нормальным планировщиком,
[01:10:37.080 --> 01:10:42.880]  вот с этими локальными очередями. Но когда я отправляю сообщения в канал, я в этом коде
[01:10:42.880 --> 01:10:53.400]  бужу fiber, отправляю его в конец локальной очереди. А сейчас давайте я код немного изменю. Вот у меня
[01:10:53.400 --> 01:11:02.560]  executors, они стали немного сложнее, к сожалению, с последней лекции. Я теперь могу сказать
[01:11:02.560 --> 01:11:09.040]  планировщику некоторый хинт. Как ему планировать задачу очередную? Может быть,
[01:11:09.040 --> 01:11:15.600]  а бы как, а может быть, вот, постарайся следующим. Если ты умеешь так, то запланируй ее как
[01:11:15.600 --> 01:11:24.680]  следующую. Это подсказка, это не директива. И я могу в селекте сказать, что я хочу следующим
[01:11:24.680 --> 01:11:29.040]  запустить. Ну, если я бужу грутину на селекте, то пусть она запустится следующей.
[01:11:29.040 --> 01:11:43.800]  Вот это вот такая оптимизация, она вот уже на десятки процентов ускоряет,
[01:11:43.800 --> 01:11:56.720]  просто за счет того, что с кошами лучше работает. Это важно. Но есть проблема. Мы снова сломали
[01:11:56.720 --> 01:12:08.040]  планировщик. Смотрите. Ну, сначала... Нет, у меня планировщик не сломан, но сейчас я его сломаю.
[01:12:08.040 --> 01:12:21.000]  И смотрите вот сюда. Вот такой вот тест. Ну, не тест, а пример. Я беру планировщик с одним
[01:12:21.000 --> 01:12:27.080]  потоком. Запускаю там грутину с двумя каналами. Беру два канала, запускаю первую грутину. Она,
[01:12:27.080 --> 01:12:36.320]  пока не проставлен флажок, ждет из первого, ждет из Y сообщение, кладет в X. Другая грутина ждет
[01:12:36.320 --> 01:12:47.200]  из X кладет в Y. А третья грутина 10 раз делает Y, а потом говорит хватит. Вот. И если мы этот код запустим,
[01:12:47.200 --> 01:13:04.800]  то чем он закончится? Ну, это правда, иначе зачем бы я стал его показывать?
[01:13:09.800 --> 01:13:15.040]  Ну, он не завершается. Почему? Потому что у нас есть один поток, и там выполняется,
[01:13:15.040 --> 01:13:23.000]  допустим, эта грутина. Она кладет сообщение в Y и ждет от X. Блокируется. Но когда она кладет в Y,
[01:13:23.000 --> 01:13:31.480]  она планирует на запуск следующий, вот эту грутину. А она берет значение, перекладывает его в X,
[01:13:31.480 --> 01:13:39.320]  и планирует на запуск эту грутину следующей. И вот они друг друга так вот через лифо-слот
[01:13:39.320 --> 01:13:46.440]  этот по кругу друг друга запускают. Получается такой вот цикл. Как его разорвать?
[01:13:46.440 --> 01:13:58.040]  Ну, предлагается сделать так, что если вы уж зачасти, ну то есть если вы подряд планируете много
[01:13:58.040 --> 01:14:05.120]  итераций через лифо-слот, то остановитесь. Просто поставьте себе какую-то константу,
[01:14:05.120 --> 01:14:12.800]  что вы не хотите слишком часто. 17. Хорошо, когда есть инициативный человек.
[01:14:21.320 --> 01:14:22.240]  Код стал завершаться.
[01:14:22.240 --> 01:14:37.240]  Ну что ж, дважды мы сломали честность. Ну вот как честность. Гарантию, что каждая
[01:14:37.240 --> 01:14:42.920]  запланированная грутина исполнится. И дважды мы ее вот так вот починили. Кстати, можно посмотреть
[01:14:42.920 --> 01:14:50.200]  теперь в этом коде, как все это работает. Ну, в смысле, посмотреть на статистику. Сейчас,
[01:14:50.200 --> 01:15:03.120]  это не этот код. Там по времени вообще смотрится. Я не помню даже константа там или нет.
[01:15:03.120 --> 01:15:15.760]  Вот видите, у меня какое-то количество задач в каналах. Ну там миллионы какие-то. И почти все
[01:15:15.760 --> 01:15:22.920]  они выполняются через лифо, оставшиеся через локальные задачи. Воровства не так уж и много.
[01:15:22.920 --> 01:15:33.560]  А если запустить хороший мютекс, серийный мютекс, как пишут в условии, чтобы это пока не значило,
[01:15:33.560 --> 01:15:47.120]  то смотрите, там лифо вообще не будет, потому что мютекса столько хорош, что ему даже не нужно
[01:15:47.120 --> 01:15:53.320]  лифо. Вот это одна из причин написать хороший мютекс, ну то есть разобраться с этими логфри,
[01:15:53.320 --> 01:16:03.040]  со стрендами и потом вот это все написать. Ну что, давайте какой-то итог подведем. У нас
[01:16:03.040 --> 01:16:11.640]  не осталось времени. Файбером нужен хороший планировщик. Обычный тредпул с разделяемой
[01:16:11.640 --> 01:16:17.640]  очередью подходит для больших независимых задач, но совершенно не подходит для маленьких,
[01:16:17.640 --> 01:16:27.120]  быстрых, зависимых друг от друга задач файберов. Чтобы сделать планировщик быстрым, мы пытаемся
[01:16:27.120 --> 01:16:32.360]  избавиться от contention, от ячеек памяти, с которым мы работаем из разных потоков. Мы шардируем
[01:16:32.360 --> 01:16:35.680]  состояние, делаем локальные очереди, делаем глобальные очереди для балансировки нагрузки,
[01:16:35.680 --> 01:16:42.560]  делаем FIFO-слот для оптимизации по кышам, воруем задачи для балансировки нагрузки. И в общем,
[01:16:42.560 --> 01:16:47.360]  вся эта конструкция начинает работать. Почему это точка оптимума, такой дизайн? Ну вот,
[01:16:47.360 --> 01:16:57.000]  обращусь вот к авторитету. В смысле, люди пробовали и придумали вот такой дизайн,
[01:16:57.000 --> 01:17:09.240]  и кажется, лучше не получается пока сделать. Что для нас нужно, что важно? Аккуратно. Во-первых,
[01:17:09.240 --> 01:17:15.400]  я надеюсь, что эта лекция, она для вас послужит мотивацией, чтобы дальше заняться домашками
[01:17:15.400 --> 01:17:19.560]  про оптимизацию планировщика и вообще пооптимизировать производительность, потому
[01:17:19.560 --> 01:17:25.280]  что кажется, что это интересно. И обращаю ваше внимание, что вот мы здесь оптимизируем
[01:17:25.280 --> 01:17:30.080]  производительность нигде не оптимизируя какие-то там асимптотики или что-то подобное. Вот все
[01:17:30.080 --> 01:17:34.240]  ускорения, они появляются из-за того, что мы знаем, как работает компьютер, как устроена кыши,
[01:17:34.240 --> 01:17:41.680]  как устроена синхронизация. Вот просто знание про кыши, мы дважды сильно ускорили код, мы разбили
[01:17:41.680 --> 01:17:46.280]  состояние планировщика на независимой очереди, и мы вот добавили лифо, и вот этот лифо, маленькая
[01:17:46.280 --> 01:17:54.320]  странная оптимизация, берет и ускоряет код на десятки процентов. Волшебство же. Вот, чтобы
[01:17:54.320 --> 01:17:59.640]  компьютер, чтобы ваша программа быстро работала, вам нужно хорошо понимать ваш многопоточный
[01:17:59.640 --> 01:18:04.640]  компьютер. Ну и последняя статья расхода, про которую в начале мы уже сказали, давайте я еще
[01:18:04.640 --> 01:18:17.600]  раз ее покажу. Это же следующая лекция. Да, следующая лекция возможна. Смотрим в профиль. Вот там есть
[01:18:17.600 --> 01:18:24.800]  разные штуки, и в частности есть переключение контекста. Ну вот тут два процента получилось на вот
[01:18:24.800 --> 01:18:33.600]  эту процедуру с ассемблером. Вот это тот overhead, который можно... вот эти два процента тоже можно
[01:18:33.600 --> 01:18:40.280]  устранить. Это уже не свойство планировщика, это уже свойство наших файберов. Ну, нашей реализации
[01:18:40.280 --> 01:18:47.160]  собственно нашего инструмента для выражения конкурентности файберов. Вот есть другой инструмент
[01:18:47.160 --> 01:18:52.680]  стеклоскорутины в C++, там где вот эти кое вейт, слава кое, return, вы это, возможно, уже видели,
[01:18:52.680 --> 01:19:01.000]  я показывал. Так вот, этот механизм ценой некоторого, скажем так, когнитивного синтактического
[01:19:01.000 --> 01:19:06.800]  overheadа позволяет от переключения контекста избавиться и сэкономить вот эти два процента,
[01:19:06.800 --> 01:19:12.440]  которые здесь неустранимы в этом коде. Ну, видимо, в следующий раз мы этим займемся.
[01:19:12.440 --> 01:19:26.880]  Что ты имеешь в виду? Ну, это я имел в виду как когнитивный синтактический overhead. Ну,
[01:19:26.880 --> 01:19:35.960]  в смысле, что у тебя функции в красе в два цвета, я это имел в виду, если что. Видимо,
[01:19:35.960 --> 01:19:42.720]  я непонятно говорю. Я с тобой согласен, что это фундаментальная вещь. Ну вот, в общем,
[01:19:42.720 --> 01:19:48.200]  есть плюс, есть минус. Мы поговорим об этом, вероятно, через неделю. На сегодня все, спасибо.

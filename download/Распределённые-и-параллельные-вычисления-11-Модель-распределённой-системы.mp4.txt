[00:00.000 --> 00:16.340]  Отлично. Тогда мы готовы начать. Грустно, что вас мало, конечно. И смотрите, что я прочитаю в этом
[00:16.340 --> 00:21.080]  курсе оставшиеся лекции, я расскажу вам про разные системы, про Кавку, про Зукипер,
[00:21.080 --> 00:28.480]  про Кассандру, про Спанер, не знаю, успеем мы или нет поговорить, но сейчас будет отлично.
[00:28.480 --> 00:35.240]  Но сегодня такая вводная лекция, и она предназначена для широкого круга лиц, в том числе
[00:35.240 --> 00:41.600]  людей, которые не слушают мой спецкурс по определенным системам, там где я довольно подробно про все
[00:41.600 --> 00:46.120]  рассказываю сегодня, по замыслу такая обзорная лекция про то, как вообще можно думать про
[00:46.120 --> 00:51.720]  распределенные системы и что о них нужно знать. В смысле, такой теоретико-архитектурный
[00:51.720 --> 00:58.720]  точки зрения совмещенный, если вы ими пользуетесь, но все же вы хотите понимать примерно, что под
[00:58.720 --> 01:04.520]  капотом происходит, с какими гарантиями все работает, какими свойствами системы могут или
[01:04.520 --> 01:10.480]  не могут обладать и как правильно про них говорить. Так вот, в какой степени подробности
[01:10.480 --> 01:18.160]  мне об этом рассказывают, потому что я не совсем понимаю аудиторию свою сегодня.
[01:25.560 --> 01:34.560]  Вопрос непонятен, да? Я хочу узнать, насколько... Слушали ли вы первые лекции моего спецкурса или нет?
[01:34.560 --> 01:44.880]  Да. Вот. По замыслу поет лекция для тех, кто не слушал. Но если вы слушали, то... Я все равно что-то
[01:44.880 --> 01:50.080]  повторю, потому что, не знаю, если запись будет, ее будут смотреть, то будут смотреть не только
[01:50.080 --> 01:55.880]  другие люди, но если у вас есть в принципе желание поговорить о чем-то более детально,
[01:55.880 --> 02:02.160]  я вам не рассказал, или у вас какие-то вопросы с тех пор остались, то будет разумно.
[02:02.160 --> 02:08.000]  Чтобы вы побольше их задавали, чтобы я получал больше обратной связи и понимал,
[02:08.000 --> 02:17.360]  в какие вещи можно было бы углубиться и что-то подробнее проговорить. Вообще сегодня я хотел бы
[02:17.360 --> 02:23.040]  рассказать вам в итоге про, совсем коротко, про результаты, про основные теоретические результаты,
[02:23.040 --> 02:29.040]  про ФЛП, про каптиорема, про то, какими словами люди говорят про определенные системы. Но для
[02:29.040 --> 02:35.040]  начала нужно поговорить, напомнить или рассказать, как вам больше нравится, про то, что мы вообще
[02:35.040 --> 02:40.920]  понимаем под распределенной системой и что нужно любому человеку, который их администрирует и их
[02:40.920 --> 02:49.960]  использует и из них строит инфраструктуру, что про все это нужно знать. Во-первых, нужно конечно
[02:49.960 --> 02:56.480]  же понимать, даже если вы работаете с облаками, даже если вы не ставите физические машины,
[02:56.480 --> 03:03.400]  даже если вы не занимаетесь заказом оборудования, все равно нужно понимать, из каких физических
[03:03.400 --> 03:10.760]  компонентов все в итоге будет выстроено, пусть даже облачного провайдера. Если мы говорим про
[03:10.760 --> 03:18.240]  теорию, и сегодня я хочу рассказать немного про теорию, то теория всегда должна быть согласована
[03:18.240 --> 03:48.240]  с практикой, и поэтому очень важно в самом начале, прежде чем говорить про Зукиппер или про Кавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкавкав
[03:48.240 --> 04:04.240]  к теме, из каких блоков она состоит. Ну, она состоит, понятно, из компьютеров, которые соединены проводами, которым доставляется сообщение, но, как правило, важно, как именно эти провода и эти компьютеры организованы в физическом мире.
[04:04.240 --> 04:17.240]  Ну, вот вы понимаете, что, конечно же, эти компьютеры устанавливаются где-то в датацентрах, и в датацентрах они стоят, во-первых, не по одному, конечно, они стоят в стойках.
[04:17.240 --> 04:39.240]  Эти стойки, ну, это просто такой базовый элемент инфраструктуры в датацентрах, это способ организации машин. Ну, вот, видите, здесь много серверов, они вложены так рядами в такие вот большие шкафы, и между ними тянется очень много проводов, которые соединяют эти компьютеры с коммутатором, который, в свою очередь, соединяет эти компьютеры в одну большую сеть.
[04:39.240 --> 05:02.240]  Казалось бы, это вещь, которую вы не наблюдаете, когда вы работаете со определенными системами, и даже когда вы, не знаю, изучаете какую-то систему, вам правило говорят, что есть дата нода и нейм нода, и вот какая разница, как они стоят в датацентрах.
[05:02.240 --> 05:25.240]  Вот это важно, потому что с понятиями, там, стоек, датацентров, регионов, кластеров, связано понятие фейлордомейна. Фейлордомейн – это компонент физической инфраструктуры, который может полностью выйти из строя из-за какого-то одного правила аппаратного сбоя.
[05:25.240 --> 05:39.240]  Вот вы думаете, что вы обеспечиваете отказоустойчивость, например, в HDFS, храняя реплики одних и тех же данных, одних и тех же фрагментов файлов на разных машинах.
[05:39.240 --> 05:55.240]  Но вот понятно, что этого недостаточно, в общем случае, потому что эти реплики могут находиться в одной серверной стойке, и да, наверное, отказ дисков, особенно крутящихся дисков, можно считать некрелированным сбоем, они происходят независимо.
[05:55.240 --> 06:14.240]  Но если, скажем, у вас есть такой шкаф, где вам стоят две реплики, и вы на них положили копии данных, а выйдет из строя вот этот коммутатор, то вы разом лишитесь нескольких ваших реплик, возникнет так называемый крелированный отказ, и пользователь, возможно, может лишиться своих данных, по крайней мере, на некоторое время.
[06:14.240 --> 06:34.240]  Вот пример того, как знания про физический мир помогают строить более надежные, более доступные распределенные системы, а именно, любая распределенная система, с которой вы работали, и наверняка вы против DFS это говорили, обеспечивает вам гарантию recoverance.
[06:34.240 --> 06:48.240]  То есть, сама система должна знать про то, что не просто компьютеры соединены приводами, а еще они организованы в стойке, если система данные реплицирует, она должна хранить копии в машинах из разных стоек.
[06:48.240 --> 07:17.240]  Ну, тут можно даже, смотрите как, можно даже делать еще чуть-чуть сложнее, не просто размазывать по стойкам, но еще и заботиться о доступности на уровне самой стойки, на аппаратном уровне, а именно, ставить в стойку там не один коммутатор, который связывает эту машину с другим, эту стойку с другими, а сразу несколько, просто потому что коммутаторы, это, как и любые компанентные инфраструктуры, вещь не отказывает, стойчивая.
[07:17.240 --> 07:32.240]  Из-за какого-то аппаратного сбоя или даже бага мы этот компонент можем потерять, и будет замечательно, если мы сможем пережить отказ даже одного коммутатора в стойке, если мы там разместим их пар.
[07:33.240 --> 07:42.240]  Желательно независимых производителей, потому что, понятно, у них могут быть разные сценарии отказов и там какие-то разные баги внутри.
[07:42.240 --> 08:02.240]  Ну а дальше эти стойки в кластере, распределенные системы связываются, эти стойки в этот центре связываются в понятии кластера, ну и когда-то можно было бы думать о распределенной системе, в общем случае, примерно вот так вот.
[08:02.240 --> 08:14.240]  У нас есть отдельные машины, они укладываются в эти стойки, реки, их там десятки, и все это связывается одним свечом, одним коммутатором в один большой кластер.
[08:14.240 --> 08:21.240]  Вот люди так долгое время и жили, но столкнулись с той проблемой, что вот такой дизайн, он не масштабируется.
[08:21.240 --> 08:34.240]  Масштабирование – это еще одна распределенная система, данные наши не помещаются ни в одну, ни в 80, ни в 100, может быть даже ни в тысячу машин, потому что мы храним пятабайты или там сейчас тот же масштаб экзобайты.
[08:34.240 --> 08:44.240]  И очень важно, чтобы сами системы могли расти не только алгоритмически, чтобы в смысле в них не было препятствий к масштабированию.
[08:44.240 --> 08:56.240]  Вот если вы изучали HDFS, то вы знаете, что эта система с некоторыми поправками не может горизонтально масштабироваться, потому что объем методанных ограничен емкостью одной машины.
[08:56.240 --> 09:10.240]  То есть есть препятствия такие архитектурные, которые возникают на уровне дизайна распределенных систем, а есть препятствия архитектурные, которые возникают уровнем ниже, на уровне дизайна кластеров и центров.
[09:10.240 --> 09:20.240]  И вот такой дизайн, он когда-то годился, потому что, во-первых, данных было еще относительно немного, а во-вторых, люди жили как-то изолированно.
[09:20.240 --> 09:29.240]  В смысле, даже если у вас какая-то большая компания и выходит много-много данных, то этим данным хранятся часто, ну так скажем, децентрализовано.
[09:29.240 --> 09:38.240]  В смысле, что у вас много проектов, много сервисов, и каждый сервис наказывает свои машины, разворачивает свои кластера относительно небольшие.
[09:38.240 --> 09:50.240]  Ну и вот нигде в одном месте не возникает необходимости построить какой-то гигантский кластер с большой пропускной способностью любого разреза.
[09:50.240 --> 10:19.240]  Ну вот так было некоторое время, и скажем, так было в Яндексе, если вам интересно, но кажется, где-то с десятых годов, может быть середины, возник такой тренд, что очень неэффективно встроить большие компании, в которых очень много разрозненных сервисов, которые используют разрозненные отдельные кластера для деплоя своих систем.
[10:19.240 --> 10:27.240]  Но ровно потому же, почему неэффективно, скажем, каждому условному бизнесу, каждому стартапу строить свои кластеры и покупать свои физические машины.
[10:27.240 --> 10:46.240]  Гораздо разумнее переиспользовать что-то общее, ну как бы декомпозировать, разделить бизнес-логику какой-то, ну, ваши сервисы от инфраструктуры хранения данных и от инфраструктуры сетевой аппаратной.
[10:46.240 --> 11:01.240]  И люди пришли к дизайну, где они строят очень большие кластера, и машины в этих кластерах просто разделяются между разными сервисами, между разными условными пользователями.
[11:01.240 --> 11:15.240]  У вас может быть огромный кластер, и часть машин его занимают машины, которые обслуживают поисковую систему, а часть машин обслуживают операции в Мэпридьюсе.
[11:15.240 --> 11:28.240]  Ну и там в зависимости от нагрузки в данный момент на поиск или на вычисления, задачи машины можно перебалансировать, между двумя пользователями.
[11:29.240 --> 11:52.240]  Ну вот такая вот фундаментальная очень важная идея, и сейчас везде все это так организовано, ну и, конечно же, вы понимаете, что облака ровно так устроены, облачный провайдер, будь там Amazon, Google или Yandex, дает вам большие кластера и позволяют вам разворачивать свои предложения, свои сервисы поверх этих машин, и все это там гибко очень ракистировать.
[11:52.240 --> 12:05.240]  Ну вот для того, чтобы так делать, в итоге нужно строить вот эти самые большие кластера, и вот просто так взять серверные стойки и объединить их в один большой кластер невозможно, нужно делать что-то сложнее.
[12:05.240 --> 12:33.240]  Ну вот скажем пример дизайна настоящего центра, где не просто там стойки соединены одним свечом, а где используется довольно сложная конфигурация, которая обеспечивает с одной стороны высокую отказоустойчивость, то есть тут нет никакой сетевой коробки, никакого свеча, который бы на себе завязывал всю коммуникацию между любыми машинами.
[12:33.240 --> 12:38.240]  А с другой стороны, у такого дизайна очень высокая выпускная способность.
[12:38.240 --> 12:42.240]  Короче говоря, между любой парой машин довольно много маршрутов.
[12:42.240 --> 12:53.240]  И устроено такие центры, например, на следующем образом. Мы берем много-много стоек, ну вот в данном случае 48, и связываем их между собой вот четырьмя коммутаторами.
[12:53.240 --> 13:03.240]  То есть у нас здесь есть возможность четырьмя способами у любых двух машин из разных стоек четырьмя способами друг с другом поговорить.
[13:03.240 --> 13:09.240]  И вот такой модуль, из которого мы строим дальше большой кластер.
[13:09.240 --> 13:16.240]  Вот если нам хочется расширить кластер, мы добавляем в него такой модуль, а дальше мы соединяем их перпендикулярными плоскостями.
[13:16.240 --> 13:24.240]  Вот у нас есть такие вот поды, в каждом из которых 48 стоек, то есть 48 умножить там на, скажем, 50 машин.
[13:24.240 --> 13:42.240]  И четыре коммутатора, которые связывают эти стойки внутри пода, мы еще перпендикулярными плоскостями ровно такой же вот конструкции связываем между собой.
[13:42.240 --> 13:51.240]  То есть если мы хотим добраться из этой стойки, мы должны попасть сначала в один из этих коммутаторов, то есть выбрать из этих плоскостей,
[13:51.240 --> 13:56.240]  и в этих плоскостях выбрать один из этих коммутаторов для того, чтобы перейти в другую плоскость.
[13:56.240 --> 14:04.240]  Ну вот тут возможно разные на самом деле варианты дизайна, но это такая более-менее стандартная.
[14:04.240 --> 14:13.240]  И она хороша тем, что здесь не требуется никакого специфичного оборудования, то есть здесь используется вполне себе доступный на рынке коммутатор,
[14:13.240 --> 14:20.240]  и не требуется изобретать какую-то волшебную коробку, которая позволит объединить очень-очень-очень много машин.
[14:20.240 --> 14:28.240]  И такую коробку строила бы, я не знаю, какая-нибудь одна компания, и вы бы завязали всю свою инфраструктуру на нее.
[14:29.240 --> 14:33.240]  Вот нет, такой цели нет. Ровно противоположно тому, чего мы хотим.
[14:33.240 --> 14:38.240]  Мы берем более-менее производительные, но доступные, наверное, коммутаторы,
[14:38.240 --> 14:42.240]  и строим из них сеть, которая может расширяться и расширяться более-менее ограниченно.
[14:42.240 --> 14:52.240]  Опять же, это все нужно понимать, потому что мы работаем с...
[14:52.240 --> 14:59.240]  Ну, потому что система сейчас работает в общем пуле ресурсов, на общем пуле машин,
[14:59.240 --> 15:08.240]  и никто сейчас не пытается как-то изолировать свою систему от других.
[15:09.240 --> 15:13.240]  Еще одна причина так делать — это утилизация.
[15:13.240 --> 15:19.240]  Вот если у вас есть отдельные кластера, то понятно, что ваша система не может быть нагружена все время.
[15:19.240 --> 15:27.240]  Как вы находитесь на пике нагрузки, вы должны иметь некоторый запас в случае роста числа пользователей.
[15:27.240 --> 15:33.240]  Поэтому, как правило, ваша система, ваши аппаратные ресурсы не утилизируются полностью.
[15:33.240 --> 15:36.240]  Вы не загружаете до конца процессора, вы не загружаете сеть и так далее.
[15:36.240 --> 15:46.240]  Если вы живете в отдельных кластерах, то получается, что у каждого проекта, у каждого сервиса есть полмашин,
[15:46.240 --> 15:49.240]  и эти машины используются не до конца.
[15:49.240 --> 15:57.240]  Вот если мы их складываем в один большой кластер и с помощью какого-то планировщика,
[15:57.240 --> 16:07.240]  позволяете эти ресурсы большого кула машин разделить между сервисами, то вы можете добиться более высокой утилизации.
[16:07.240 --> 16:11.240]  То есть вы можете балансировать эти ресурсы между теми,
[16:11.240 --> 16:16.240]  отдавать их приоритетизировать тем, у кого сейчас нагрузка есть, а тем, кому она не нужна, у них выбирать.
[16:20.240 --> 16:24.240]  Но если у вас какие-то вопросы есть, то спрашивайте, потому что если вы вдруг это слышали,
[16:24.240 --> 16:32.240]  то, не знаю, может быть вам какие-то детали интересны или какие-то, ну не знаю, что-нибудь еще, что я не говорю, а мог бы.
[16:35.240 --> 16:42.240]  Вот, ну и такая конструкция, она помещается в дата-центр, может быть даже не в один,
[16:42.240 --> 16:48.240]  и на уровне дата-центра нам тоже важна и отказоустойчивость, и масштабируемость.
[16:48.240 --> 16:53.240]  Вот мы хотим расширяться, поэтому мы строим несколько зданий, разумеется,
[16:53.240 --> 16:59.240]  но мы хотим отказоустойчивость, поэтому мы должны позаботиться о резервировании тех компонентов, которые могут отказать.
[16:59.240 --> 17:04.240]  Ну скажем, могут порваться магистральные кайбери, которые соединяют дата-центр с другими дата-центрами,
[17:04.240 --> 17:08.240]  потому что, как правило, реплики вашей системы, если система является высокодоступной,
[17:08.240 --> 17:14.240]  должна размещаться в разных отказах, то есть в разных зданиях, может быть даже на разных континентах.
[17:14.240 --> 17:18.240]  И вот эти здания должны соединять провода в магистральной кайбере,
[17:18.240 --> 17:25.240]  и для того, чтобы ваша система, ваша инфраструктура была устойчива, не знаю, к отказу кайберю,
[17:25.240 --> 17:30.240]  в смысле, к какому-то экскаватору, который вы раскопали, вы соединяете ваш дата-центр несколькими кайберями.
[17:30.240 --> 17:36.240]  И вы, конечно, заботитесь о резервном питании, то есть вы питаетесь от подстанции,
[17:36.240 --> 17:43.240]  скорее всего, у вас есть в вашем здании еще и дизельные генераторы, которые питают,
[17:43.240 --> 17:49.240]  если мы спустимся на уровень вниз, дизельные генераторы, которые служат резервным источникам питания.
[17:49.240 --> 17:59.240]  Ну, не знаю, тут еще на этой картинке витрины мельницы, но это совсем про запас.
[17:59.240 --> 18:09.240]  Вот так примерно устроен дата-центр, и вот в нем мы собираемся деплоить все наши системы,
[18:10.240 --> 18:14.240]  будь-то кавказу, кипер, что угодно, ТФС.
[18:14.240 --> 18:19.240]  И узлы этих систем общаются между собой.
[18:19.240 --> 18:26.240]  Общаются они, как правило, по стандартным протоколам, по протоколу TCP, скажем.
[18:26.240 --> 18:31.240]  И вот почему нужно думать и об этом, если вы пишете распределенный код,
[18:31.240 --> 18:35.240]  потому что нужно понимать, какие гарантии вам эти провода дают.
[18:35.240 --> 18:39.240]  То есть вот вы отправляете сообщение с одного узла до другого узла.
[18:39.240 --> 18:42.240]  Вот что с ним может случиться?
[18:42.240 --> 18:45.240]  Ну, могут случиться очень разные вещи.
[18:45.240 --> 18:50.240]  Могут случиться, не знаю, у вас может поломаться чехсума,
[18:50.240 --> 18:54.240]  потому что где-то в проводе перевернулись какие-то битики,
[18:54.240 --> 18:59.240]  потому что ну просто там не дискретный сигнал передается.
[18:59.240 --> 19:02.240]  Или у вас соединение может порваться,
[19:02.240 --> 19:05.240]  ну конечно сложно, почему это может произойти.
[19:05.240 --> 19:11.240]  Ну а в общем, какие гарантии у вас есть в случае отправки сообщения?
[19:11.240 --> 19:15.240]  Эти гарантии могут, вот без этих гарантий,
[19:15.240 --> 19:18.240]  строить какие-то распределенные, без понимания таких гарантий,
[19:18.240 --> 19:22.240]  строить распределенные протоколы очень сложно.
[19:22.240 --> 19:25.240]  В теории, как правило, говорят про два типа гарантий.
[19:25.240 --> 19:28.240]  Говорят про надежные и ненадежные каналы.
[19:28.240 --> 19:32.240]  Вот ненадежный канал – это канал, в котором вы отправляете сообщения,
[19:32.240 --> 19:36.240]  и оно может доставиться, а может нет.
[19:36.240 --> 19:41.240]  Но если вы будете ретраить отправку бесконечно долго,
[19:41.240 --> 19:45.240]  то вы бесконечно много раз отправите одно и то же сообщение,
[19:45.240 --> 19:49.240]  то оно гарантированно дойдет бесконечно много раз.
[19:49.240 --> 19:52.240]  И имея такую абстракцию, теоретическую абсолютно,
[19:52.240 --> 19:57.240]  можно построить надежный канал, такой, что вы в него отправляете данные,
[19:57.240 --> 20:00.240]  он гарантированно доставляет сообщение адресата,
[20:00.240 --> 20:02.240]  причем доставляет ровно один раз.
[20:02.240 --> 20:09.240]  Такая интуиция, что почему мы считаем, что такой канал можно построить?
[20:09.240 --> 20:13.240]  Потому что у нас есть уникальный идентификатор,
[20:13.240 --> 20:15.240]  который можно генерирует на каждой машине.
[20:15.240 --> 20:19.240]  У нас есть фейерлоз-канал, ненадежный канал,
[20:19.240 --> 20:21.240]  который гарантирует доставку.
[20:21.240 --> 20:24.240]  И вот ретраем мы можем добиться семантики at least once,
[20:24.240 --> 20:27.240]  то есть мы доставим сообщение по крайней мере один раз,
[20:27.240 --> 20:32.240]  а с помощью уникальных идентификаторов получатель может отфильтровать дубли
[20:32.240 --> 20:34.240]  и получить семантику at least once.
[20:34.240 --> 20:39.240]  Ну а дальше в теории алгоритмы строят поверх таких каналов,
[20:39.240 --> 20:41.240]  что на самом деле не очень практично,
[20:41.240 --> 20:44.240]  потому что в реальности вы такой канал построить не можете,
[20:44.240 --> 20:52.240]  потому что вы работаете в физическом мире,
[20:52.240 --> 20:55.240]  и вы работаете с уже существующими слоями инфраструктуры
[20:55.240 --> 20:57.240]  и набором абстракций,
[20:57.240 --> 20:59.240]  и у вас есть TCP-соединения, которые,
[20:59.240 --> 21:01.240]  с одной стороны, являются ненадежным каналом,
[21:01.240 --> 21:03.240]  с другой стороны,
[21:03.240 --> 21:06.240]  беспечить семантику at least once, конечно, не могут,
[21:06.240 --> 21:08.240]  потому что если вы отправляете сообщение,
[21:08.240 --> 21:11.240]  то внутри одного TCP-соединения
[21:11.240 --> 21:16.240]  собственно сам TCP гарантирует, что оно будет доставлено ровно один раз,
[21:16.240 --> 21:21.240]  какой-то фрейм, который вы в TCP-соединении упаковали,
[21:21.240 --> 21:24.240]  а с другой стороны, если TCP-соединение рвется,
[21:24.240 --> 21:27.240]  то никаких гарантий вы тут уже не получаете.
[21:30.240 --> 21:32.240]  Разумно было бы думать об этом так,
[21:32.240 --> 21:35.240]  то есть он настоящий гарантий отправки по сети.
[21:35.240 --> 21:37.240]  Когда вы отправляете сообщение в сеть,
[21:37.240 --> 21:41.240]  то с другой стороны, либо получатель узнает об этом сообщении,
[21:41.240 --> 21:43.240]  либо он его получит,
[21:43.240 --> 21:46.240]  прочитает байты из своего сокета,
[21:46.240 --> 21:49.240]  либо же на стороне отправителя
[21:49.240 --> 21:51.240]  случится разрыв соединения,
[21:51.240 --> 21:54.240]  и отправитель может это как-то обработать.
[21:54.240 --> 21:57.240]  Но гарантировать 10 exactly once невозможно.
[21:57.240 --> 21:59.240]  Если не рвется, то вы не понимаете,
[21:59.240 --> 22:02.240]  получил ли адрес от сообщения или нет.
[22:02.240 --> 22:04.240]  Вы можете его перетраить,
[22:04.240 --> 22:09.240]  а дальше вы можете на этом уровне рассчитывать
[22:09.240 --> 22:11.240]  семантику at least once.
[22:11.240 --> 22:13.240]  То есть каждое сообщение будет доставлено,
[22:13.240 --> 22:15.240]  по крайней мере, один раз.
[22:15.240 --> 22:17.240]  Это уже более разумные допущения,
[22:17.240 --> 22:19.240]  и на уровне сетевой коммуникации
[22:19.240 --> 22:21.240]  стоит пользоваться, конечно, ими.
[22:25.240 --> 22:29.240]  Примерно так организованы вот эти коробки
[22:29.240 --> 22:31.240]  дата-центра и провода.
[22:31.240 --> 22:33.240]  Пара слов про TCP.
[22:33.240 --> 22:36.240]  TCP – это довольно сложная конструкция.
[22:36.240 --> 22:40.240]  С одной стороны, это такая абстракция надежного провода,
[22:40.240 --> 22:42.240]  которая гарантирует доставку сообщения
[22:42.240 --> 22:45.240]  и ровно и без дублей, и без потерь.
[22:45.240 --> 22:47.240]  А с другой стороны,
[22:47.240 --> 22:49.240]  это самостоятельная распределенная система.
[22:49.240 --> 22:52.240]  И когда вы думаете про коммуникацию по TCP,
[22:52.240 --> 22:54.240]  то вы должны учитывать, что, скажем,
[22:54.240 --> 22:56.240]  у одной машины, то есть у вас, у клиента
[22:56.240 --> 22:58.240]  и у севера могут быть разные представления
[22:58.240 --> 23:00.240]  о том, в каком состоянии сейчас соединение.
[23:00.240 --> 23:03.240]  Может быть, вы думаете, что соединение у вас открыто,
[23:03.240 --> 23:05.240]  а север будет думать, что его уже нет,
[23:05.240 --> 23:09.240]  потому что он взял и резко перезагрузился,
[23:09.240 --> 23:11.240]  потому что у него пропало питание.
[23:11.240 --> 23:17.240]  Пока вы не пойдете и не отправите пакет TCP-сегмент
[23:17.240 --> 23:19.240]  на север, вы не узнаете о том,
[23:19.240 --> 23:21.240]  что соединение первое на самом деле.
[23:21.240 --> 23:25.240]  Потому что TCP в проводах не реализован,
[23:25.240 --> 23:28.240]  во всей промежуточной системе инфраструктуры
[23:28.240 --> 23:31.240]  никакого TCP нет, там работают только IP
[23:31.240 --> 23:33.240]  и протоколы более низкого уровня.
[23:33.240 --> 23:37.240]  Поэтому, в общем, я рекомендую статью прочесть,
[23:37.240 --> 23:40.240]  если вы ее не знаете, если я еще не рассказывал,
[23:40.240 --> 23:46.240]  она про то, какие конфигурации могут возникать
[23:46.240 --> 23:48.240]  просто в пределах одного TCP-соединения,
[23:48.240 --> 23:52.240]  как клиент и север могут расходиться в своем представлении
[23:52.240 --> 23:55.240]  о том, как узлы общаются между собой.
[23:59.240 --> 24:01.240]  Это шутка в зуме.
[24:01.240 --> 24:03.240]  Он хочет сказать на просто или просто шумит?
[24:03.240 --> 24:04.240]  Непонятно.
[24:05.240 --> 24:07.240]  Окей.
[24:07.240 --> 24:13.240]  Таким образом, система состоит из узлов,
[24:13.240 --> 24:16.240]  которые объединены в реке, которые объединены сложной,
[24:16.240 --> 24:18.240]  вот это называется, я не сказал,
[24:18.240 --> 24:22.240]  коммутационной фабрикой.
[24:22.240 --> 24:26.240]  Центры бесконечно большие, и в них живут узлы,
[24:26.240 --> 24:29.240]  которые обмениваются сообщениями с помощью
[24:29.240 --> 24:32.240]  относительно надежного транспорта.
[24:32.240 --> 24:37.240]  Ну, а дальше мы собираемся на эти узлы поместить
[24:37.240 --> 24:40.240]  какие-то наши программы, которые будут заниматься,
[24:40.240 --> 24:43.240]  которые будут реализовывать ту или иную распределённую систему,
[24:43.240 --> 24:46.240]  ну, скажем, файловую систему или кивалию хранилища
[24:46.240 --> 24:48.240]  или базу данных или что угодно.
[24:48.240 --> 24:54.240]  И прежде чем мы перейдем к разговору о самих системах,
[24:54.240 --> 25:00.240]  полезно добавить в нашу картинку,
[25:00.240 --> 25:02.240]  которую мы сейчас так медленно рисуем,
[25:02.240 --> 25:04.240]  ещё и клиента.
[25:04.240 --> 25:07.240]  Клиенты – это тоже полноценные участники распределённой системы.
[25:07.240 --> 25:12.240]  И почему важно говорить про клиентов, хотя, казалось бы,
[25:12.240 --> 25:15.240]  они просто вызывают какие-то методы.
[25:15.240 --> 25:22.240]  Во-первых, клиент – это сущность, которая тоже подвержена отказу.
[25:22.240 --> 25:27.240]  И, разумеется, вы внутри системы должны бороться с отказами,
[25:27.240 --> 25:29.240]  у вас не должно быть единой точки отказа,
[25:29.240 --> 25:32.240]  то есть единого компонента, или, не дай бог,
[25:32.240 --> 25:35.240]  даже отдельной машины точно не должно быть,
[25:35.240 --> 25:37.240]  отказ, который приводит к тому,
[25:37.240 --> 25:39.240]  что вся система становится недоступной.
[25:39.240 --> 25:41.240]  Ну, скажем, вот лидер,
[25:41.240 --> 25:44.240]  name-node HDFS является точкой отказа.
[25:44.240 --> 25:47.240]  У нас могут быть тысячи машин, которые хранят данные,
[25:47.240 --> 25:52.240]  но без дела, который менеджит методанно этой системы,
[25:52.240 --> 25:54.240]  хранит списки чанков файлов,
[25:54.240 --> 25:57.240]  вся эта система недоступна для пользователя.
[25:57.240 --> 25:59.240]  Но вот, помимо того, что в системе,
[25:59.240 --> 26:02.240]  в дизайне системы не должно быть такого компонента,
[26:02.240 --> 26:04.240]  на которого все было бы завязано,
[26:04.240 --> 26:06.240]  и он бы не выдерживал отказанной машины.
[26:06.240 --> 26:09.240]  Также, вообще говоря, нужно думать и про отказы клиентов.
[26:09.240 --> 26:13.240]  Вот, скажем, вы пишете сервис блокировок.
[26:13.240 --> 26:17.240]  Сервис блокировок – это такой стандартный компонент в инфраструктуре.
[26:17.240 --> 26:19.240]  Вот эти слайды Google довольно старые,
[26:19.240 --> 26:23.240]  но на самом деле они уже давно сделали что-то разумное.
[26:23.240 --> 26:26.240]  Вот их архитектура, архитектура кейвалюхранилища
[26:26.240 --> 26:28.240]  и тут есть какие-то вспомогательные сервисы.
[26:28.240 --> 26:31.240]  Ну вот, система, которая занимается,
[26:31.240 --> 26:36.240]  которая планирует узлы программы на кластере,
[26:36.240 --> 26:38.240]  есть распределенная файловая система,
[26:38.240 --> 26:40.240]  которая отвечает за хранение данных,
[26:40.240 --> 26:43.240]  и есть сервис блокировок, который выполняет,
[26:43.240 --> 26:47.240]  который предназначен для координации узлов распределенной системы.
[26:47.240 --> 26:51.240]  Но вот эту задачу тоже можно делегировать отдельному компоненту.
[26:51.240 --> 26:53.240]  Так вот, приходит клиент,
[26:53.240 --> 26:56.240]  он берет в этом сервисе блокировок,
[26:56.240 --> 26:58.240]  как можно догадаться, блокировку,
[26:58.240 --> 27:00.240]  и после этого отказывает.
[27:00.240 --> 27:03.240]  Вот если мы можем требовать от того,
[27:03.240 --> 27:07.240]  что от там GFS или от там другой,
[27:07.240 --> 27:10.240]  от самой Bigtable, чтобы в нем отказывать,
[27:10.240 --> 27:12.240]  чтобы он был тейрайсинг отказан,
[27:12.240 --> 27:15.240]  от клиента мы, конечно, отказоустойчивость требовать не можем.
[27:15.240 --> 27:17.240]  Поэтому мы должны позаботиться о том,
[27:17.240 --> 27:22.240]  а как же наша система будет восстанавливаться после сбоя клиента.
[27:22.240 --> 27:24.240]  И там нужны какие-то таймауты,
[27:24.240 --> 27:28.240]  и это там каскадом влечет за собой разные интересные последствия.
[27:28.240 --> 27:31.240]  А именно, если коротко говорить,
[27:31.240 --> 27:34.240]  что распределенные блокировки – это довольно сломанный,
[27:34.240 --> 27:36.240]  довольно странный паттерн,
[27:36.240 --> 27:41.240]  довольно странный механизм координации распределенных систем.
[27:41.240 --> 27:44.240]  Его многие используют, но при этом, что самое нерепое,
[27:44.240 --> 27:47.240]  гарантировать взаимного исключения он не может.
[27:47.240 --> 27:54.240]  Это гарантировать ваш локальный статем юток с вашим многополучным приложением.
[27:54.240 --> 28:00.240]  В общем, про клиентов нужно тоже думать, потому что они отказывают.
[28:00.240 --> 28:03.240]  Про клиентов нужно еще думать,
[28:03.240 --> 28:05.240]  потому что они на самом деле являются
[28:05.240 --> 28:08.240]  полноценными участниками распределенной системы иногда.
[28:08.240 --> 28:16.240]  Потому что они тоже исполняют код вашей системы на самом деле.
[28:16.240 --> 28:21.240]  То есть в простейшем случае клиент просто посылает системе запрос,
[28:21.240 --> 28:23.240]  там по HTTP скажем,
[28:23.240 --> 28:28.240]  но в сложных случаях и у нас будет скоро система ZooKeeper,
[28:28.240 --> 28:30.240]  сервис координации.
[28:30.240 --> 28:34.240]  Это как раз опенсорс-альтернатива вот такому сервису блокировок,
[28:34.240 --> 28:37.240]  который в Google назывался Chabi.
[28:37.240 --> 28:42.240]  Так вот, если вы используете ZooKeeper,
[28:42.240 --> 28:47.240]  то в ZooKeeper вы тоже можете брать условно какие-то блокировки,
[28:47.240 --> 28:50.240]  сделать их сами, но смысл этого не меняется.
[28:50.240 --> 28:53.240]  И в случае смерти клиента, отказа клиента,
[28:53.240 --> 28:57.240]  система должна каким-то образом уметь откатить его действия,
[28:57.240 --> 29:00.240]  освободить его блокировки.
[29:00.240 --> 29:06.240]  И в этом случае от клиента требуется выполнять некоторый протокол,
[29:06.240 --> 29:09.240]  согласованный с протоколом,
[29:09.240 --> 29:13.240]  который работает на узлах самого ZooKeeper.
[29:16.240 --> 29:23.240]  В общем, клиент является здесь полноценным участником системы.
[29:23.240 --> 29:28.240]  Ну и наконец, про клиентов нужно говорить,
[29:28.240 --> 29:31.240]  потому что именно на уровне клиентов,
[29:31.240 --> 29:33.240]  на уровне наблюдаемого поведения,
[29:33.240 --> 29:36.240]  формулируются очень важные свойства систем,
[29:36.240 --> 29:40.240]  а именно в адресу согласованности.
[29:40.240 --> 29:43.240]  Вот для разработчиков распределенной системы эта система –
[29:43.240 --> 29:47.240]  это набор узлов, которые размещаются на кластере
[29:47.240 --> 29:50.240]  и обмениваются сообщениями.
[29:50.240 --> 29:53.240]  Для клиентов распределенная система – это некоторая точка входа,
[29:53.240 --> 29:55.240]  некоторый сетевой адрес,
[29:55.240 --> 29:58.240]  по которому клиент соединяется с какой-то машиной системы
[29:58.240 --> 30:00.240]  и задает у него запросы.
[30:00.240 --> 30:03.240]  И вообще говоря, в хорошей распределенной системе,
[30:03.240 --> 30:09.240]  конечно же, клиент не знает про какие-то нюансы реализации.
[30:09.240 --> 30:15.240]  То есть клиент как клиентская библиотека может про это что-то знать,
[30:15.240 --> 30:20.240]  но клиент всего лишь вызывает методы клиентской библиотеки,
[30:20.240 --> 30:23.240]  говорит, я вот хочу добавить что-то в файл,
[30:23.240 --> 30:25.240]  я хочу записать что-то по ключу,
[30:25.240 --> 30:31.240]  я хочу создать узел в дереве ZooKeeper.
[30:31.240 --> 30:35.240]  Вот такие вот простые операции,
[30:35.240 --> 30:38.240]  и клиент в таком случае.
[30:38.240 --> 30:40.240]  Клиент взаимодействует с системой, по сути,
[30:40.240 --> 30:47.240]  как с некоторым объектом, там, не знаю, таблицей, деревом,
[30:47.240 --> 30:52.240]  файловой системой, который конкурентный в том смысле,
[30:52.240 --> 30:55.240]  что с ним могут работать одновременно разные клиенты.
[30:55.240 --> 30:57.240]  Я отказываю устойчивое,
[30:57.240 --> 31:02.240]  то есть мы не ожидаем, что сбой путьной машины сделает наше дерево ZooKeeper
[31:02.240 --> 31:05.240]  или наша файловая система целиком недоступной.
[31:05.240 --> 31:09.240]  Но при этом клиент не думает, как именно его запросы обслуживаются,
[31:09.240 --> 31:11.240]  он не хочет чем-то думать.
[31:11.240 --> 31:15.240]  Идеально было бы, чтобы для него система представлялась таким вот
[31:15.240 --> 31:18.240]  атомарным, отказоустойчивым, надежным объектом,
[31:18.240 --> 31:21.240]  в который помещается еще неограниченное количество данных,
[31:21.240 --> 31:23.240]  то есть система умеет масштабироваться.
[31:23.240 --> 31:28.240]  Но вот не любая система, не о любой системе можно так думать.
[31:28.240 --> 31:38.240]  И тут возникает понятие, которое носит название модель согласованности,
[31:38.240 --> 31:41.240]  ну или в случае систем с транзакцией модель изоляции.
[31:45.240 --> 31:49.240]  Пользователи, вернее, работают с распределенной системой,
[31:49.240 --> 31:54.240]  как с объектом в разделаемой памяти, условно говоря.
[31:54.240 --> 31:57.240]  Вот адрес системы буквально ими переменный,
[31:57.240 --> 32:00.240]  ну а разные клиенты – это буквально разные потоки.
[32:00.240 --> 32:08.240]  То есть так об этом можно думать, и это будет разумно соответствовать реальности.
[32:08.240 --> 32:14.240]  И, как правило, от разделаемого объекта мы ждем какой-то понятной семантики,
[32:14.240 --> 32:17.240]  как он будет работать в случае конкурентного доступа.
[32:17.240 --> 32:22.240]  Если мы говорим про какие-то объекты внутри многопоточной программы,
[32:22.240 --> 32:24.240]  то мы их защищаем юдоксом, и все понятно.
[32:24.240 --> 32:27.240]  А если мы говорим про распределенную систему,
[32:27.240 --> 32:29.240]  то становится уже менее понятно, потому что…
[32:29.240 --> 32:33.240]  Ну представьте себе, вы клиент…
[32:33.240 --> 32:37.240]  Давайте я покажу вам такие, не то чтобы слайды,
[32:37.240 --> 32:40.240]  но вот я воспользуюсь картинкой, которую я рисовал в другом курсе,
[32:40.240 --> 32:44.240]  потому что тут уже все есть.
[32:44.240 --> 32:47.240]  Вот представьте, что у вас есть определенная система,
[32:47.240 --> 32:51.240]  у нас есть каких-то реплик, они каким-то образом общаются друг с другом.
[32:51.240 --> 32:55.240]  Это клиенту не важно.
[32:55.240 --> 33:00.240]  А клиент просто отправляет сначала запрос на запись в эту систему,
[33:00.240 --> 33:02.240]  получает подтверждение о записи.
[33:02.240 --> 33:04.240]  Он там записал в ячейку памяти единицу,
[33:04.240 --> 33:06.240]  в каком-то ключувке в хранилище единицу.
[33:06.240 --> 33:10.240]  Получил подтверждение от системы, потом пошел к своему соседу,
[33:10.240 --> 33:13.240]  рассказал об этом, и этот сосед отправил систему запрос
[33:13.240 --> 33:16.240]  на чтение и тоже получил ответ.
[33:16.240 --> 33:19.240]  И конечно, второй клиент ожидает, что чтение вернет результат,
[33:19.240 --> 33:23.240]  который сделает клиент.
[33:23.240 --> 33:27.240]  Вот такие естественные ожидания у пользователей от системы.
[33:27.240 --> 33:31.240]  Ну потому что пользователи всю эту распределенность не наблюдают.
[33:31.240 --> 33:36.240]  Но при этом внутри системы обеспечить такую гарантию довольно сложно.
[33:36.240 --> 33:37.240]  Почему сложная?
[33:37.240 --> 33:42.240]  Ну потому что система распределенная, и она должна каким-то образом
[33:42.240 --> 33:47.240]  упорядочивать все записи, все чтения, которые с ней происходят.
[33:47.240 --> 33:51.240]  Каким образом это можно делать?
[33:51.240 --> 33:53.240]  Ну вот можно делать это по-разному.
[33:53.240 --> 33:59.240]  Например, можно упорядочивать все записи, которые с системой происходят,
[33:59.240 --> 34:01.240]  с помощью временных меток.
[34:01.240 --> 34:05.240]  Вот узел получил запись, посмотрел на локальные часы,
[34:05.240 --> 34:09.240]  увидел, что сейчас 13.01, и вот назначил запись в такую временную нетку.
[34:09.240 --> 34:12.240]  Если на другой узел придет другая запись,
[34:12.240 --> 34:15.240]  и эта запись получит другой временную нетку,
[34:15.240 --> 34:18.240]  то вот система поймет, что была запись первая, была запись вторая,
[34:18.240 --> 34:22.240]  они как-то упорядочились, и в системе нужно оставить более свежую запись.
[34:22.240 --> 34:27.240]  Ну и могут возникать какие-то неприятные сценарии,
[34:27.240 --> 34:32.240]  а именно, что пользователи пришли в одном порядке,
[34:32.240 --> 34:36.240]  а часы там на узлах расходились, они были не синхронизированы.
[34:36.240 --> 34:39.240]  И синхронизация часов – это отдельная история, отдельная сложная история,
[34:39.240 --> 34:44.240]  и, к сожалению, история, которая не имеет, скажем, идеального решения.
[34:44.240 --> 34:47.240]  Вы не можете идеально синхронизировать часы.
[34:47.240 --> 34:50.240]  И ваши записи могут переупорядочиться.
[34:50.240 --> 34:53.240]  Или, скажем, вот вы сделали эту запись,
[34:53.240 --> 34:56.240]  эта запись, разумеется, была обработана не одной машиной,
[34:56.240 --> 34:59.240]  каким-то набором машин, каким-то кворумом,
[34:59.240 --> 35:02.240]  и, скажем, вот пользователь записал данные вот сюда.
[35:03.240 --> 35:08.240]  А после этого он сообщил своей записи другому пользователю,
[35:08.240 --> 35:11.240]  а пользователь прочитал, и прочитал их там с другой машины.
[35:11.240 --> 35:13.240]  И запись не увидел.
[35:15.240 --> 35:18.240]  Вот пользователь не может, не знает про то,
[35:18.240 --> 35:21.240]  как система устроена внутри, не знает про то,
[35:21.240 --> 35:25.240]  что она там временные мерки выбирает, или что какие-то собирают кворумы.
[35:25.240 --> 35:28.240]  Пользователь лишь наблюдает,
[35:29.240 --> 35:33.240]  лишь выполняет операции, порождает вот такие вот отрезки,
[35:33.240 --> 35:36.240]  начало операции, завершение операции, может обмениваться сообщениями
[35:36.240 --> 35:39.240]  с другими клиентами, с другими пользователями,
[35:39.240 --> 35:42.240]  и отправлять новые запросы. То есть он порождает вот такие вот,
[35:42.240 --> 35:45.240]  что называется, конкурентными историями системой.
[35:45.240 --> 35:50.240]  И он хочет, чтобы эти конкурентные истории были разумны в некотором смысле.
[35:50.240 --> 35:54.240]  То есть понятно ему клиенту, который не знает про то,
[35:54.240 --> 35:57.240]  как система реплицирована там, устроена изнутри.
[35:57.240 --> 36:02.240]  И вот ровно в этом месте появляется очень естественное принятие модели согласованности.
[36:02.240 --> 36:06.240]  Модель согласованности говорит, а как же пользователь может думать
[36:06.240 --> 36:09.240]  об исполнении своих операций внутри системы.
[36:09.240 --> 36:13.240]  Это понятие не про то, как система устроена внутри.
[36:13.240 --> 36:17.240]  Это система про то, как пользователь наблюдает поведение этой системы.
[36:18.240 --> 36:22.240]  То есть мы здесь декомпозируем клиентов, гарантии клиента
[36:22.240 --> 36:26.240]  от децентрализации.
[36:26.240 --> 36:29.240]  И самая сильная и самая естественная гарантия,
[36:29.240 --> 36:32.240]  которую вы как правило хотите получить, называется
[36:32.240 --> 36:36.240]  либо линьеризуемость, либо строгая сириализуемость.
[36:36.240 --> 36:39.240]  Вот это две главные гарантии, которые вы хотите иметь,
[36:39.240 --> 36:42.240]  когда вы работаете с определенными системами.
[36:43.240 --> 36:47.240]  Линьеризуемость означает, что несмотря на то, что все системы
[36:47.240 --> 36:52.240]  можно работать конкурентно, любое исполнение операций над этой системой
[36:52.240 --> 36:57.240]  можно представить себе в виде последовательного исполнения,
[36:57.240 --> 37:02.240]  где все операции происходят в некотором порядке друг за другом
[37:02.240 --> 37:08.240]  и при этом сохраняется относительный порядок неконкурирующих операций.
[37:08.240 --> 37:14.240]  То есть если пользователь выполнил операцию О1
[37:14.240 --> 37:21.240]  и после этого, возможно, другой пользователь выполнил операцию О2,
[37:21.240 --> 37:29.240]  то система исполнила их как будто бы в некотором глобальном порядке,
[37:29.240 --> 37:32.240]  но обязательно О1 исполнила до О2.
[37:35.240 --> 37:36.240]  Почему это важно?
[37:36.240 --> 37:41.240]  Почему мы хотим такого атомарного наблюдаемого поведения
[37:41.240 --> 37:44.240]  или, строго говоря, линьеризуемого поведения?
[37:44.240 --> 37:48.240]  Потому что очень удобно пользователю не думать про конкурентность.
[37:48.240 --> 37:52.240]  А почему мы хотим, чтобы система соблюдала порядок операции в реальном времени?
[37:52.240 --> 37:56.240]  То есть если вы сделали операцию О1, она завершилась до начала другой операции,
[37:56.240 --> 37:59.240]  то эти записи будут системой в таком же порядке.
[37:59.240 --> 38:04.240]  Потому что пользователь все-таки имеет какие-то ожидания
[38:04.240 --> 38:08.240]  про порядок выполнения операции внутри системы.
[38:08.240 --> 38:13.240]  В данном случае пользователь сделал запись, отправил сообщение другому клиенту,
[38:13.240 --> 38:14.240]  второй клиент сделал чтение.
[38:14.240 --> 38:18.240]  Разумеется, второй клиент ожидает, что он увидит результаты этой записи.
[38:18.240 --> 38:19.240]  Почему?
[38:19.240 --> 38:23.240]  Потому что между этим чтением и этой записью есть причинность.
[38:23.240 --> 38:26.240]  Ну или это называется отношение happens before.
[38:26.240 --> 38:29.240]  Мы говорили о нем в прошлом, это и весной.
[38:29.240 --> 38:34.240]  Конечно же, пользователь не знает про предшествование операции в реальном времени,
[38:34.240 --> 38:39.240]  потому что, опять же, я говорю, задача синхронизации часов не решается идеально.
[38:40.240 --> 38:45.240]  Но пользователь знает, что вот это чтение случилось, потому что случилась эта запись.
[38:45.240 --> 38:52.240]  Но беда в том, что система не знает про то, что между этим чтением и этой записью есть такая зависимость.
[38:52.240 --> 38:59.240]  Причинность реализуется, коммуникация возникает за пределами системы.
[38:59.240 --> 39:04.240]  Но при этом сама система должна эту коммуникацию, эту причинность учесть.
[39:05.240 --> 39:11.240]  Поэтому она дает гарантию не в терминах после причинности, которая ей неизвестна, а в терминах реального времени.
[39:11.240 --> 39:18.240]  То есть система вам говорит, что если вы завершили запись, а после этого начали чтение,
[39:18.240 --> 39:22.240]  то ваше чтение увидит, по крайней мере, сделанную запись, а может быть, что-то более свежее.
[39:25.240 --> 39:27.240]  Вот это такая фундаментальная гарантия.
[39:27.240 --> 39:34.240]  И, скажем, для Key Value хранилищ или для систем, которые реализуют отдельные объекты, это гарантия линиализуемости.
[39:34.240 --> 39:38.240]  А в случае, если мы говорим про систему с транзакциями, про базу данных, про систему,
[39:38.240 --> 39:45.240]  где можно, скажем, потрогать сразу несколько строчек таблицы, то это называется строгая сериализуемость.
[39:47.240 --> 39:54.240]  То есть реализуемость и сериализуемость – это гарантия атомарного поведения нашей системы.
[39:54.240 --> 40:03.240]  Ну и плюс дополнительная гарантия про соблюдение предшествования аберраций клиентов,
[40:03.240 --> 40:13.240]  про соблюдение happens before через соблюдение порядка аберраций в реальном времени.
[40:15.240 --> 40:19.240]  Ну вот, это такая фундаментальная диаграмма всех возможных гарантий.
[40:19.240 --> 40:21.240]  Тут находятся более славые гарантии.
[40:21.240 --> 40:26.240]  Но они нам, наверное, не очень интересны, потому что все-таки современные системы, хорошие системы,
[40:26.240 --> 40:28.240]  чаще всего линиализуемые.
[40:28.240 --> 40:33.240]  Вот правая ветка – она про гарантии относительно операций над отдельными объектами.
[40:33.240 --> 40:39.240]  Правая ветка и левая ветка – про гарантии зарядции транзакций.
[40:39.240 --> 40:43.240]  Как правило, вы работаете либо с транзакциями, либо с отдельными операциями.
[40:43.240 --> 40:48.240]  И вот вы используете либо эту ветку гарантии, либо эту ветку гарантии.
[40:48.240 --> 40:51.240]  Вопрос. Как же такие гарантии достигаются?
[40:51.240 --> 40:56.240]  То есть как строятся линиализуемые системы, которые, несмотря на то, что внутри реплицированы,
[40:56.240 --> 41:00.240]  достигают вот такого атомарного наблюдаемого поведения?
[41:00.240 --> 41:05.240]  Для этого есть некоторый фундаментальный примитив, который называется atomic broadcast.
[41:05.240 --> 41:09.240]  И в следующий раз у нас будет система зукипер.
[41:09.240 --> 41:13.240]  Так вот, она вот на этом примитиве и построена.
[41:13.240 --> 41:17.240]  Атомик broadcast – это такое фундаментальное понятие.
[41:17.240 --> 41:22.240]  Это примитив коммуникации в распространенной системе.
[41:22.240 --> 41:25.240]  Примитив коммуникации между ее узлами.
[41:25.240 --> 41:30.240]  Вот с одной стороны между каждой парой узлов есть TCP-соединение.
[41:30.240 --> 41:33.240]  И оно там дает какие-то гарантии.
[41:33.240 --> 41:39.240]  Гарантию можно считать, что TCP-соединение строго говоря даёт гарантию exactly once,
[41:39.240 --> 41:43.240]  но поскольку соединение может порваться и вы должны будете его переустанавливать,
[41:43.240 --> 41:46.240]  то эта гарантия превращается в at least once.
[41:46.240 --> 41:50.240]  Так вот, для того чтобы строить линиализуемые системы,
[41:50.240 --> 41:54.240]  то есть системы, которые ведут себя как одно атомарное целое,
[41:54.240 --> 42:01.240]  нужно поверх такой ненадежной сети, которая не гарантирует exactly once доставку,
[42:01.240 --> 42:04.240]  построить систему, которая… построить примитив коммуникации,
[42:04.240 --> 42:06.240]  который дает больше гарантий.
[42:06.240 --> 42:09.240]  Вы можете отправить сообщения другим узлам.
[42:12.240 --> 42:16.240]  Два разных узла могут отправить два разных сообщения.
[42:16.240 --> 42:20.240]  Но вот примитив коммуникации Atomic или Total Reordered Broadcast
[42:20.240 --> 42:25.240]  гарантирует, что если два сообщения были отправлены,
[42:25.240 --> 42:28.240]  то отправлены всем узлам разом,
[42:28.240 --> 42:31.240]  но если два сообщения были отправлены разом,
[42:31.240 --> 42:37.240]  то узлы, которые доживут до получения этих сообщений,
[42:37.240 --> 42:39.240]  получат их в одном и том же порядке.
[42:41.240 --> 42:46.240]  То есть, это примитив, который позволяет вам сделать такую широкомещательную рассылку
[42:46.240 --> 42:49.240]  с гарантией порядка доставки сообщений.
[42:51.240 --> 42:54.240]  И если у вас такой примитив есть,
[42:54.240 --> 42:57.240]  если вы можете сделать такую так иначе,
[42:57.240 --> 43:00.240]  И если у вас такой примитив есть,
[43:00.240 --> 43:04.240]  если вы можете независимо с разных узлов
[43:04.240 --> 43:08.240]  отправлять всем другим узлам сообщение,
[43:08.240 --> 43:11.240]  эти сообщения доставляются в одном и том же порядке,
[43:11.240 --> 43:14.240]  вы можете на самом деле решать задачу репликации,
[43:14.240 --> 43:20.240]  то есть вы можете хранить копии состояния вашей системы,
[43:20.240 --> 43:23.240]  которые будут согласовано меняться,
[43:23.240 --> 43:27.240]  согласовано переживать одну и ту же серию апдейтов.
[43:27.240 --> 43:30.240]  Вот приходит клиент вашу систему,
[43:30.240 --> 43:32.240]  он скажем, в ней что-то пишет,
[43:32.240 --> 43:34.240]  переписывает какую-то строчку вашей таблице,
[43:34.240 --> 43:36.240]  своей таблице,
[43:36.240 --> 43:38.240]  для этого отправляется команду в систему,
[43:38.240 --> 43:39.240]  какая-то реплика ее получает,
[43:39.240 --> 43:40.240]  не важно какая,
[43:40.240 --> 43:42.240]  она иницирует бродкаст,
[43:42.240 --> 43:47.240]  и когда этот бродкаст доставит эту команду
[43:47.240 --> 43:49.240]  на эту самую реплику,
[43:49.240 --> 43:52.240]  то она применяется к данным пользователя
[43:52.240 --> 43:54.240]  и пользователь получает ответ.
[43:54.240 --> 43:57.240]  Другой клиент может прийти на другую реплику
[43:57.240 --> 43:59.240]  с другой командой, с другой записью,
[43:59.240 --> 44:02.240]  и эти записи вообще конкурируют во времени,
[44:02.240 --> 44:05.240]  но за счет того, что на каждую реплику
[44:05.240 --> 44:07.240]  эти две записи доставятся в одном и том же порядке
[44:07.240 --> 44:10.240]  просто силами этого бродкаста,
[44:10.240 --> 44:12.240]  мы получим гарантию,
[44:12.240 --> 44:14.240]  что все реплики нашей системы двигаются
[44:14.240 --> 44:16.240]  через одну и ту же серию апдейтов,
[44:16.240 --> 44:18.240]  проходит через одну и ту же серию апдейтов,
[44:18.240 --> 44:20.240]  а значит, пребывает в согласованном состоянии,
[44:20.240 --> 44:24.240]  и мы таким образом получаем линьеризуемость.
[44:29.240 --> 44:32.240]  Если мы хотим сделать линьеризуемость,
[44:32.240 --> 44:35.240]  то вот такой фундаментальный принцип
[44:35.240 --> 44:38.240]  мы, как правило, используем в нашей реализации
[44:38.240 --> 44:40.240]  приметив под названием Atomic Broadcast.
[44:45.240 --> 44:47.240]  А каким образом этот Atomic Broadcast
[44:47.240 --> 44:49.240]  можно построить?
[44:50.240 --> 44:52.240]  Вот тут возникает, наверное,
[44:52.240 --> 44:55.240]  главная задача, которая в теории распределенных систем
[44:55.240 --> 44:57.240]  существует, это задача консенсуса.
[44:57.240 --> 45:00.240]  Задача консенсуса звучит очень просто.
[45:00.240 --> 45:02.240]  У вас есть n узлов,
[45:02.240 --> 45:05.240]  и они должны договориться об общем выборе.
[45:05.240 --> 45:08.240]  Просто каждый узел получает сообщение, запускается,
[45:08.240 --> 45:10.240]  и должен поговорить с другими узлами,
[45:10.240 --> 45:12.240]  и завершиться, и выбрать некоторые значения.
[45:14.240 --> 45:15.240]  Все, что нужно сделать,
[45:15.240 --> 45:17.240]  выбрать одно из предложенных значений,
[45:17.240 --> 45:19.240]  все узлы, которые доживут до выбора,
[45:19.240 --> 45:21.240]  должны выбрать одно и то же,
[45:21.240 --> 45:23.240]  ну и алгоритм должен завершаться.
[45:23.240 --> 45:26.240]  Вот такая совершенно примитивная,
[45:26.240 --> 45:28.240]  тривиальная постановка задача
[45:28.240 --> 45:31.240]  оказывается эквивалентна
[45:31.240 --> 45:34.240]  по сложности задачи реализации Atomic Broadcast.
[45:34.240 --> 45:37.240]  А Atomic Broadcast – это вот приметив,
[45:37.240 --> 45:39.240]  с помощью которого достигается главная гарантия,
[45:39.240 --> 45:44.240]  главная модель согласованности линьеризуемости.
[45:44.240 --> 45:47.240]  Вот если вы умеете решить такую простую задачу,
[45:47.240 --> 45:53.240]  то вы можете добиться линьеризуемого поведения вашей системы
[45:53.240 --> 45:58.240]  и таким образом от пользователя скрыть всю распределенность.
[46:01.240 --> 46:04.240]  Это очень важный, очень фундаментальный факт,
[46:04.240 --> 46:08.240]  что мы всю сложность распределенных систем,
[46:08.240 --> 46:12.240]  ну фактически не всю половину сложности,
[46:12.240 --> 46:15.240]  свели к одной простой задаче.
[46:17.240 --> 46:19.240]  И вот тут уже не нужно думать про клиентов,
[46:19.240 --> 46:23.240]  тут уже не нужно думать про какие-то Happens Before,
[46:23.240 --> 46:24.240]  которые там возникают.
[46:24.240 --> 46:26.240]  Если вы просто можете научить Enz Love
[46:26.240 --> 46:29.240]  выбирать общее значение и завершаться при этом,
[46:29.240 --> 46:32.240]  то вы можете более-менее добиться любого,
[46:32.240 --> 46:35.240]  вы можете добиться линьеризуемого
[46:35.240 --> 46:38.240]  или строгости реализуемого поведения сверху.
[46:43.240 --> 46:45.240]  Ну и про задачу Consensus,
[46:45.240 --> 46:47.240]  это конечно долгая история,
[46:47.240 --> 46:50.240]  ее невозможно там за одну лекцию, за полчаса рассказать,
[46:50.240 --> 46:53.240]  но с задачей Consensus связаны два фундаментальных результата.
[46:53.240 --> 46:55.240]  И результаты они про то,
[46:55.240 --> 46:57.240]  что задача решается довольно тяжело.
[46:57.240 --> 46:59.240]  Первый результат состоит в том,
[46:59.240 --> 47:02.240]  что для того, чтобы задача решилась,
[47:02.240 --> 47:05.240]  для того, чтобы вы могли завершаться,
[47:05.240 --> 47:09.240]  сделать выбор и при этом не нарушать Agreement,
[47:09.240 --> 47:12.240]  то есть не расходиться во мнениях,
[47:12.240 --> 47:14.240]  не расходиться в выборах,
[47:14.240 --> 47:17.240]  вам требуется довольно много узлов.
[47:17.240 --> 47:21.240]  То есть если вы готовы переживать отказы при этом,
[47:21.240 --> 47:23.240]  то вам требуется довольно много узлов.
[47:23.240 --> 47:26.240]  А именно, если вы хотите пережить
[47:26.240 --> 47:31.240]  f отказов задачи Consensus, то есть в вашей системе,
[47:31.240 --> 47:35.240]  то вам требуется, по крайней мере,
[47:35.240 --> 47:38.240]  два f узлов, два f плюс один узол.
[47:38.240 --> 47:42.240]  То есть если вы хотите хранить сообщения,
[47:42.240 --> 47:44.240]  хранить данные пользователя,
[47:44.240 --> 47:47.240]  какую-нибудь его запись, строчку в таблице
[47:47.240 --> 47:50.240]  и переживать отказ любого диска,
[47:50.240 --> 47:52.240]  то вам нужны три копии.
[47:52.240 --> 47:54.240]  Если вы хотите хранить данные,
[47:54.240 --> 47:56.240]  переживать отказ двух дисков,
[47:56.240 --> 47:59.240]  то вам нужно уже пять копий данных и так далее.
[47:59.240 --> 48:02.240]  Сразу замечу, что задача серьезно упрощается,
[48:02.240 --> 48:04.240]  и вам не нужен никакой Consensus,
[48:04.240 --> 48:07.240]  если ваши данные мутабельны.
[48:07.240 --> 48:11.240]  И это такое очень важное замечание,
[48:11.240 --> 48:13.240]  вы с самого начала должны понять,
[48:13.240 --> 48:15.240]  у вас данные мутируются или нет.
[48:15.240 --> 48:17.240]  Если мы говорим про кивалию хранилища
[48:17.240 --> 48:20.240]  или про базу данных, то вы, конечно, там делаете апдейты.
[48:20.240 --> 48:23.240]  А если мы говорим про файл, про файловую систему,
[48:23.240 --> 48:27.240]  то, может быть, в API файловой системы у вас нет апдейтов.
[48:27.240 --> 48:29.240]  Может быть, вы просто добавляете данные к файлу,
[48:29.240 --> 48:31.240]  делаете апенды.
[48:31.240 --> 48:33.240]  Это довольно разумное ограничение,
[48:33.240 --> 48:36.240]  потому что оно с системой снимает огромную уголовную боль,
[48:36.240 --> 48:38.240]  а именно необходимость решать задачу Consensus,
[48:38.240 --> 48:41.240]  в большом масштабе.
[48:41.240 --> 48:43.240]  Но если у вас данные мутабельны,
[48:43.240 --> 48:45.240]  если вы пишете кивалию хранилища,
[48:45.240 --> 48:47.240]  или если вы пишете базу данных,
[48:47.240 --> 48:50.240]  что одно и то же, то Consensus вам необходим,
[48:50.240 --> 48:53.240]  потому что иначе это единственный способ,
[48:53.240 --> 48:56.240]  в котором вы можете добиться линейализуемого поведения,
[48:56.240 --> 48:58.240]  то есть вскрыть распределенность от пользователя
[48:58.240 --> 49:03.240]  и дать ему очень понятную модель поведения вашей системы.
[49:03.240 --> 49:05.240]  Первое ограничение, которое касается задачи Consensus,
[49:05.240 --> 49:11.240]  в том, что требуется довольно большая избыточность.
[49:11.240 --> 49:13.240]  То есть для двух реплик, 5 узлов,
[49:13.240 --> 49:15.240]  для трех реплик, 7 узлов и так далее.
[49:15.240 --> 49:17.240]  Это первое ограничение.
[49:17.240 --> 49:19.240]  Второе ограничение.
[49:19.240 --> 49:21.240]  Оно еще более...
[49:21.240 --> 49:25.240]  Это ограничение, в общем-то, понятно,
[49:25.240 --> 49:27.240]  если вы знаете про систему кворумов.
[49:27.240 --> 49:29.240]  В любом случае, если вы даже не знаете,
[49:29.240 --> 49:31.240]  поговорим об этом,
[49:31.240 --> 49:33.240]  когда будем разбирать систему Кассандра,
[49:33.240 --> 49:39.240]  то сбил смысле.
[49:45.240 --> 49:47.240]  По-другому.
[49:47.240 --> 49:49.240]  Есть первое ограничение про то,
[49:49.240 --> 49:52.240]  что если система должна переживать f отказов
[49:52.240 --> 49:55.240]  и завершаться, и не нарушать общего выбора,
[49:55.240 --> 49:58.240]  то требуется 2 f плюс 1 реплика, по крайней мере.
[49:58.240 --> 50:00.240]  Второе ограничение.
[50:00.240 --> 50:02.240]  Оно звучит еще более шокирующе.
[50:02.240 --> 50:04.240]  Это, наверное, самое известное ограничение,
[50:04.240 --> 50:06.240]  которое существует в распределенных системах.
[50:06.240 --> 50:08.240]  Это FLP-тиарема.
[50:08.240 --> 50:10.240]  FLP-тиарема говорит о том,
[50:10.240 --> 50:12.240]  что задачи Consensus,
[50:12.240 --> 50:16.240]  ее невозможно гарантированно решить,
[50:16.240 --> 50:18.240]  даже если у вас, в принципе,
[50:18.240 --> 50:20.240]  возможен только один сбой.
[50:20.240 --> 50:22.240]  То есть даже если вам пообещали,
[50:22.240 --> 50:24.240]  что в системе откажет не более одного узла,
[50:24.240 --> 50:26.240]  что, конечно, в реальности.
[50:26.240 --> 50:28.240]  Невозможно.
[50:28.240 --> 50:30.240]  В смысле, получить такое обещание.
[50:30.240 --> 50:32.240]  То даже в таких условиях
[50:34.240 --> 50:36.240]  все равно задача не решается.
[50:36.240 --> 50:38.240]  В том смысле, что
[50:38.240 --> 50:40.240]  если ваш алгоритм Consensus
[50:40.240 --> 50:42.240]  не нарушает agreement,
[50:42.240 --> 50:44.240]  то есть никогда его ответы не расходятся,
[50:44.240 --> 50:46.240]  выборы узлов не расходятся,
[50:46.240 --> 50:48.240]  то в этом алгоритме непременно существует LifeLog.
[50:50.240 --> 50:52.240]  То есть сценарии,
[50:52.240 --> 50:54.240]  где узлы бесконечно долго
[50:54.240 --> 50:56.240]  не могут сделать выбор.
[50:58.240 --> 51:00.240]  Все это, конечно, справедливо
[51:00.240 --> 51:02.240]  не в любой теоретической модели,
[51:02.240 --> 51:04.240]  а справедливо в модели,
[51:04.240 --> 51:06.240]  где мы предполагаем
[51:06.240 --> 51:08.240]  асинхронность
[51:08.240 --> 51:10.240]  или частичную синхронность.
[51:10.240 --> 51:12.240]  То есть мы не делаем предположений
[51:12.240 --> 51:14.240]  о скорости наставки сообщений.
[51:14.240 --> 51:16.240]  Мы не требуем, чтобы они все
[51:16.240 --> 51:18.240]  гарантированно доставляли
[51:18.240 --> 51:20.240]  за некоторое ограниченное время.
[51:20.240 --> 51:22.240]  Как правило, конечно же,
[51:22.240 --> 51:24.240]  сеть в датацентрах
[51:24.240 --> 51:26.240]  устроена довольно эффективно.
[51:26.240 --> 51:28.240]  То есть если мы говорим про тайминги,
[51:28.240 --> 51:30.240]  то внутри датацентра можно ожидать
[51:30.240 --> 51:32.240]  латентности отправки сообщений
[51:32.240 --> 51:34.240]  в пределах миллисекунд
[51:34.240 --> 51:36.240]  или сотен микросекунд,
[51:36.240 --> 51:38.240]  что довольно быстро.
[51:40.240 --> 51:42.240]  Но это вероятностная гарантия.
[51:44.240 --> 51:46.240]  Вам не гарантируют,
[51:46.240 --> 51:48.240]  что всегда 100% сообщений
[51:48.240 --> 51:50.240]  будут доставляться за ограниченное время.
[51:52.240 --> 51:54.240]  Если вы это допускаете,
[51:54.240 --> 51:56.240]  а любая промышленная система
[51:56.240 --> 51:58.240]  работает именно в таких предположениях,
[51:58.240 --> 52:00.240]  то вы подвергаете себя
[52:00.240 --> 52:02.240]  эффекту FLP-тиаремы.
[52:02.240 --> 52:04.240]  И таким образом
[52:04.240 --> 52:06.240]  ваш консенсус может не завершаться.
[52:06.240 --> 52:08.240]  То есть ваш Atomic Broadcast
[52:08.240 --> 52:12.240]  может не доставлять сообщения.
[52:12.240 --> 52:14.240]  И для вашей распиленной системы
[52:14.240 --> 52:16.240]  это означает,
[52:16.240 --> 52:18.240]  что ваша распиленная система
[52:18.240 --> 52:20.240]  может не отвечать пользователю.
[52:20.240 --> 52:22.240]  Она не нарушает...
[52:22.240 --> 52:24.240]  Если консенсус не нарушает
[52:24.240 --> 52:26.240]  требования общего выбора,
[52:26.240 --> 52:28.240]  то это в свою очередь
[52:28.240 --> 52:30.240]  на уровне Atomic Broadcast
[52:30.240 --> 52:32.240]  не приводит к нарушению
[52:32.240 --> 52:34.240]  общего порядка доставки.
[52:34.240 --> 52:36.240]  Это в свою очередь не приводит к тому,
[52:36.240 --> 52:38.240]  что не приводит к нарушению
[52:38.240 --> 52:40.240]  линьализуемого поведения.
[52:40.240 --> 52:42.240]  Но при этом это недоступна система.
[52:44.240 --> 52:46.240]  Вот такой вот фундаментальный факт,
[52:46.240 --> 52:48.240]  что если система линьализуема,
[52:48.240 --> 52:50.240]  то она использует Atomic Broadcast,
[52:50.240 --> 52:52.240]  то есть решает задачу консенсуса
[52:52.240 --> 52:54.240]  или серию задач консенсуса
[52:54.240 --> 52:56.240]  для того, чтобы с помощью этой серии
[52:56.240 --> 52:58.240]  договориться о порядке
[52:58.240 --> 53:00.240]  доставки сообщений.
[53:00.240 --> 53:02.240]  Но поскольку все эти задачи
[53:02.240 --> 53:04.240]  решаются по синхронной модели,
[53:04.240 --> 53:06.240]  то на нас действует теарема FLP.
[53:06.240 --> 53:08.240]  И теарема FLP говорит,
[53:08.240 --> 53:10.240]  что в нашей реализации
[53:10.240 --> 53:12.240]  обязан быть лайфлог,
[53:12.240 --> 53:14.240]  обязан быть сценарий,
[53:14.240 --> 53:16.240]  где из-за неудачного порядка
[53:16.240 --> 53:18.240]  доставки сообщений в сети
[53:18.240 --> 53:20.240]  алгоритм никак не может завершиться,
[53:20.240 --> 53:22.240]  никак не может обработать
[53:22.240 --> 53:24.240]  запрос пользователей.
[53:30.240 --> 53:32.240]  Это первый способ
[53:32.240 --> 53:34.240]  говорить про какие-то
[53:34.240 --> 53:36.240]  теоретические гарантии.
[53:36.240 --> 53:38.240]  Второй способ – он гораздо
[53:38.240 --> 53:40.240]  более популярный, потому что он
[53:40.240 --> 53:42.240]  не требует знаний про
[53:42.240 --> 53:44.240]  какие-то внутренние детали,
[53:44.240 --> 53:46.240]  про теорию задачи консенсуса,
[53:46.240 --> 53:48.240]  про бродкасты и все такое.
[53:48.240 --> 53:50.240]  Это
[53:50.240 --> 53:52.240]  засуждение в духе
[53:52.240 --> 53:54.240]  каптеаремы.
[53:56.240 --> 53:58.240]  Вот есть
[53:58.240 --> 54:00.240]  такой подход
[54:00.240 --> 54:02.240]  к классификации распределенных систем,
[54:02.240 --> 54:04.240]  который в принципе
[54:04.240 --> 54:06.240]  про то же самое,
[54:06.240 --> 54:08.240]  но совершенно другими словами.
[54:08.240 --> 54:10.240]  Когда мы говорим
[54:10.240 --> 54:12.240]  про распределенную систему, мы должны сказать,
[54:12.240 --> 54:14.240]  какие гарантии надают пользователю,
[54:14.240 --> 54:16.240]  как она переживает отказы
[54:16.240 --> 54:18.240]  и
[54:18.240 --> 54:20.240]  как она ведет себя, когда пользователи
[54:20.240 --> 54:22.240]  с ней работают конкурентно.
[54:22.240 --> 54:24.240]  Особенно интересно, как система
[54:24.240 --> 54:26.240]  переживает один специальный вид отказов,
[54:26.240 --> 54:28.240]  а именно партишены.
[54:28.240 --> 54:30.240]  Вот у вас могут
[54:30.240 --> 54:32.240]  строить отдельные узлы – это одна проблема.
[54:32.240 --> 54:34.240]  Еще у вас может
[54:34.240 --> 54:36.240]  ломаться сеть. Ломаться таким образом,
[54:36.240 --> 54:38.240]  что она разваливается на два сегмента
[54:38.240 --> 54:40.240]  и
[54:40.240 --> 54:42.240]  внутри одного сегмента
[54:42.240 --> 54:44.240]  узлы могут общаться друг с другом
[54:44.240 --> 54:46.240]  и могут работать с клиентами.
[54:46.240 --> 54:48.240]  А в другом сегменте,
[54:48.240 --> 54:50.240]  внутри одного сегмента, а между сегментами
[54:50.240 --> 54:52.240]  коммуникации нет.
[54:52.240 --> 54:54.240]  Скажем прямо,
[54:54.240 --> 54:56.240]  внутри DC с такой топологией
[54:56.240 --> 54:58.240]  довольно сложно получить партишен,
[54:58.240 --> 55:00.240]  но если у вас
[55:00.240 --> 55:02.240]  есть несколько DC, которые соединены
[55:02.240 --> 55:04.240]  к магистральными каверами,
[55:04.240 --> 55:06.240]  то такая ситуация уже вполне естественна.
[55:06.240 --> 55:08.240]  И довольно любопытно,
[55:08.240 --> 55:10.240]  как система может
[55:10.240 --> 55:12.240]  на такое поведение сети
[55:12.240 --> 55:14.240]  реагировать.
[55:14.240 --> 55:16.240]  Есть специальный сценарий
[55:16.240 --> 55:18.240]  для этого.
[55:18.240 --> 55:20.240]  Есть специальное имя для этого
[55:20.240 --> 55:22.240]  Speedbrain.
[55:22.240 --> 55:24.240]  Для поведения системы,
[55:24.240 --> 55:26.240]  когда система разваливаясь
[55:26.240 --> 55:28.240]  на два сегмента,
[55:28.240 --> 55:30.240]  начинает в этих сегментах действовать независимо.
[55:30.240 --> 55:32.240]  И если
[55:32.240 --> 55:34.240]  вы можете
[55:34.240 --> 55:36.240]  в одну часть
[55:36.240 --> 55:38.240]  один клиент,
[55:38.240 --> 55:40.240]  работающий с одним сегментом,
[55:40.240 --> 55:42.240]  в него данные записать,
[55:42.240 --> 55:44.240]  а с другой стороны
[55:44.240 --> 55:46.240]  другой клиент из другого сегмента
[55:46.240 --> 55:48.240]  может данные прочитать, то понятно,
[55:48.240 --> 55:50.240]  что второй клиент запись первого увидеть
[55:50.240 --> 55:52.240]  не может, потому что просто коммуникации
[55:52.240 --> 55:54.240]  между полушариями системы нет.
[55:54.240 --> 55:56.240]  Вот такой сценарий
[55:56.240 --> 55:58.240]  называется Speedbrain.
[55:58.240 --> 56:00.240]  И понятно, что вот здесь есть
[56:00.240 --> 56:02.240]  некоторые фундаментальные трейдов,
[56:02.240 --> 56:04.240]  как система себя ведет.
[56:06.240 --> 56:08.240]  Вот интуиции по поводу
[56:08.240 --> 56:10.240]  поведения системы в данном случае
[56:10.240 --> 56:12.240]  выражают в правилах в виде каптиоремы.
[56:12.240 --> 56:14.240]  Говорят, что есть
[56:14.240 --> 56:16.240]  consistency – это
[56:16.240 --> 56:18.240]  гарантия,
[56:18.240 --> 56:20.240]  это, собственно, поведение системы,
[56:20.240 --> 56:22.240]  как оно описано пользователю.
[56:22.240 --> 56:24.240]  Вот линейризуемость –
[56:24.240 --> 56:26.240]  это модель согласованности.
[56:26.240 --> 56:28.240]  Consistency model.
[56:28.240 --> 56:30.240]  Есть availability –
[56:30.240 --> 56:32.240]  это свойство, что система отвечает
[56:32.240 --> 56:34.240]  на запросы, которые вы в нее посылаете.
[56:34.240 --> 56:36.240]  И третья буква
[56:36.240 --> 56:38.240]  в каптиореме – это
[56:38.240 --> 56:40.240]  partition tolerance, то есть
[56:40.240 --> 56:42.240]  устойчивость к отказу, способность системы
[56:42.240 --> 56:44.240]  каким-то образом реагировать на partition в сети.
[56:44.240 --> 56:46.240]  И вот говорят, что
[56:46.240 --> 56:48.240]  можно классифицировать
[56:48.240 --> 56:50.240]  системы следующим образом,
[56:50.240 --> 56:52.240]  что системы не могут достигать
[56:52.240 --> 56:54.240]  всех трех свойств, то есть они не могут
[56:54.240 --> 56:56.240]  переживать партишены,
[56:56.240 --> 56:58.240]  оставаясь абсолютно доступными
[56:58.240 --> 57:00.240]  и сохраняя согласованность.
[57:00.240 --> 57:02.240]  Поэтому, когда вы говорите про конкретные системы,
[57:02.240 --> 57:04.240]  вы должны охарактеризовать,
[57:04.240 --> 57:06.240]  какой выбор они делают,
[57:06.240 --> 57:08.240]  какие буквы они выбирают.
[57:08.240 --> 57:10.240]  Грубо говоря, есть системы,
[57:10.240 --> 57:12.240]  которые
[57:12.240 --> 57:14.240]  разумно предположить,
[57:14.240 --> 57:16.240]  что есть три типа системы.
[57:16.240 --> 57:18.240]  Все три свойства недоступны,
[57:18.240 --> 57:20.240]  потому что понятно, что если у вас
[57:20.240 --> 57:22.240]  система разделилась на две части,
[57:22.240 --> 57:24.240]  и они не общались между собой,
[57:24.240 --> 57:26.240]  то нельзя одновременно быть согласованным
[57:26.240 --> 57:28.240]  и обслуживать, отвечать на каждый запрос.
[57:28.240 --> 57:30.240]  Но поэтому нужно выбрать
[57:30.240 --> 57:32.240]  какие-то две буквы из трех,
[57:32.240 --> 57:34.240]  и вот скажем, у нас
[57:34.240 --> 57:36.240]  будут системы,
[57:36.240 --> 57:38.240]  которые выбирают consistency
[57:38.240 --> 57:40.240]  и которые выбирают доступность.
[57:40.240 --> 57:42.240]  То есть у вас есть такой фундаментальный тредов
[57:42.240 --> 57:44.240]  между доступностью системы и согласованным поведением.
[57:44.240 --> 57:46.240]  Но в такой формулировке
[57:46.240 --> 57:48.240]  это довольно странная теория,
[57:48.240 --> 57:50.240]  но это вообще не теоремы, конечно же,
[57:50.240 --> 57:52.240]  в этом собственно и проблема
[57:52.240 --> 57:54.240]  этой самой CAP.
[57:54.240 --> 57:56.240]  Такая просто интуиция,
[57:56.240 --> 57:58.240]  которая следует из очевидного дизайна
[57:58.240 --> 58:00.240]  от определенных систем.
[58:00.240 --> 58:02.240]  И вообще формулировать ее нужно немного иначе.
[58:02.240 --> 58:04.240]  Сказать, что...
[58:04.240 --> 58:06.240]  Ну не то чтобы вы выбираете любые две буквы,
[58:06.240 --> 58:08.240]  потому что partition...
[58:08.240 --> 58:10.240]  Если consistency и доступность
[58:10.240 --> 58:12.240]  это свойство вашей системы,
[58:12.240 --> 58:14.240]  то partition это просто некоторая данность
[58:14.240 --> 58:16.240]  в физическом мире.
[58:16.240 --> 58:18.240]  Partition могут возникать.
[58:18.240 --> 58:20.240]  Поэтому когда говорят про CAPTIOREMO,
[58:20.240 --> 58:22.240]  говорят, что в системе бывают partition
[58:22.240 --> 58:24.240]  неизбежно.
[58:24.240 --> 58:26.240]  Поэтому ваша система должна выбирать,
[58:26.240 --> 58:28.240]  как она себя в случае partition ведет.
[58:28.240 --> 58:30.240]  Либо она способна обслуживать пользователей
[58:30.240 --> 58:32.240]  с обеих сторон partition,
[58:32.240 --> 58:34.240]  и тогда она объявляется доступной.
[58:34.240 --> 58:36.240]  Или же она
[58:36.240 --> 58:38.240]  должна быть согласованной,
[58:38.240 --> 58:40.240]  то есть линеризуемой.
[58:42.240 --> 58:44.240]  И это означает,
[58:44.240 --> 58:46.240]  что, видимо,
[58:46.240 --> 58:48.240]  в какой-то части partition
[58:48.240 --> 58:50.240]  она обслуживать пользователей не может.
[58:50.240 --> 58:52.240]  И тут нет никакого общего рецепта,
[58:52.240 --> 58:54.240]  в смысле, как делать правильно.
[58:56.240 --> 58:58.240]  Решение, то есть выбор
[58:58.240 --> 59:00.240]  между consistency и доступностью
[59:00.240 --> 59:02.240]  зависит от
[59:02.240 --> 59:04.240]  просто вашей задачи,
[59:04.240 --> 59:06.240]  от ваших требований.
[59:06.240 --> 59:08.240]  Мы поговорим про Cassandra.
[59:08.240 --> 59:10.240]  Cassandra — это open-source реализация
[59:10.240 --> 59:12.240]  системы Dynamo,
[59:12.240 --> 59:14.240]  киволюхранилище,
[59:14.240 --> 59:16.240]  с помощью которого Amazon строил свой интернет-магазин.
[59:16.240 --> 59:18.240]  Так вот,
[59:18.240 --> 59:20.240]  Amazon не было необходимости
[59:20.240 --> 59:22.240]  строить систему с высокой доступностью.
[59:22.240 --> 59:24.240]  В системе Dynamo
[59:24.240 --> 59:26.240]  хранились корзины с товарами.
[59:26.240 --> 59:28.240]  И, ну, корзины с товарами —
[59:28.240 --> 59:30.240]  это все-таки не банковский счет.
[59:30.240 --> 59:32.240]  Если в случае partition
[59:32.240 --> 59:34.240]  из этой корзины
[59:34.240 --> 59:36.240]  проходит какой-то товар, потом снова там появится,
[59:36.240 --> 59:38.240]  ну, это не смертельно.
[59:38.240 --> 59:40.240]  Главное, чтобы пользователь понимал,
[59:40.240 --> 59:42.240]  что он покупает, когда он нажимает на «покупить».
[59:42.240 --> 59:44.240]  С другой стороны, есть система,
[59:44.240 --> 59:46.240]  при которой выбор
[59:46.240 --> 59:48.240]  очевиден в другую сторону.
[59:48.240 --> 59:50.240]  Если мы, скажем,
[59:50.240 --> 59:52.240]  храним данные пользователей,
[59:52.240 --> 59:54.240]  какие-то, не знаю, его письма, или счета,
[59:54.240 --> 59:56.240]  или что-то подобное, то мы не можем позволить
[59:56.240 --> 59:58.240]  себе потерять эти письма
[59:58.240 --> 01:00:00.240]  или сделать деньги недоступными.
[01:00:00.240 --> 01:00:02.240]  В смысле, сделать...
[01:00:08.240 --> 01:00:10.240]  Нарушить изоляцию транзакций над
[01:00:10.240 --> 01:00:12.240]  счетами пользователя.
[01:00:12.240 --> 01:00:14.240]  Мы не можем себе позволить
[01:00:16.240 --> 01:00:18.240]  испортить данные,
[01:00:18.240 --> 01:00:20.240]  которые он пишет в свою,
[01:00:20.240 --> 01:00:22.240]  в нашу базу, в наше облако,
[01:00:22.240 --> 01:00:24.240]  пусть даже ценой
[01:00:24.240 --> 01:00:26.240]  некоторой недоступности.
[01:00:26.240 --> 01:00:28.240]  Вот если мы облачный провайдер,
[01:00:28.240 --> 01:00:30.240]  то мы не можем предоставить ему...
[01:00:30.240 --> 01:00:32.240]  Мы можем предоставить два типа систем,
[01:00:32.240 --> 01:00:34.240]  но мы не можем за него
[01:00:34.240 --> 01:00:36.240]  выбрать в любом случае.
[01:00:38.240 --> 01:00:40.240]  Вот если вам нужна доступность,
[01:00:40.240 --> 01:00:42.240]  то вы, как правило, не используете консенсус.
[01:00:44.240 --> 01:00:46.240]  Если вам нужна доступность,
[01:00:46.240 --> 01:00:48.240]  то вы должны строить систему без консенсуса,
[01:00:48.240 --> 01:00:50.240]  потому что консенсус еще раз
[01:00:50.240 --> 01:00:52.240]  ограничивает вас в доступности, когда возникает партишка.
[01:00:52.240 --> 01:00:54.240]  Я говорил, что
[01:00:54.240 --> 01:00:56.240]  для того, чтобы решить задачу консенсуса,
[01:00:56.240 --> 01:00:58.240]  вам нужно иметь
[01:00:58.240 --> 01:01:00.240]  доступными, по крайней мере, половину узлов.
[01:01:02.240 --> 01:01:04.240]  Но вот партишка наставляет
[01:01:04.240 --> 01:01:06.240]  систему
[01:01:06.240 --> 01:01:08.240]  с большей частью и меньшей частью.
[01:01:08.240 --> 01:01:10.240]  И в меньшей части консенсус не решается,
[01:01:10.240 --> 01:01:12.240]  а вместе с ним не решается задача
[01:01:12.240 --> 01:01:14.240]  на тумбе Quest,
[01:01:14.240 --> 01:01:16.240]  а вместе с ним не решается репликация,
[01:01:16.240 --> 01:01:18.240]  и система блокируется.
[01:01:20.240 --> 01:01:22.240]  Так что если вы работаете
[01:01:22.240 --> 01:01:24.240]  с какой-то системой,
[01:01:24.240 --> 01:01:26.240]  то вы должны понимать
[01:01:26.240 --> 01:01:28.240]  во-первых, на совсем грубом
[01:01:28.240 --> 01:01:30.240]  высоком уровне,
[01:01:30.240 --> 01:01:32.240]  как она позиционирует
[01:01:32.240 --> 01:01:34.240]  себя в смысле каптиоремы,
[01:01:34.240 --> 01:01:36.240]  и если она позиционирует себя
[01:01:36.240 --> 01:01:38.240]  как система доступная,
[01:01:38.240 --> 01:01:40.240]  то, видимо, из этого следует,
[01:01:40.240 --> 01:01:42.240]  что она внутри не использует консенсус,
[01:01:42.240 --> 01:01:44.240]  не использует Atomic Broadcast,
[01:01:44.240 --> 01:01:46.240]  и это означает,
[01:01:46.240 --> 01:01:48.240]  что она не может вам
[01:01:48.240 --> 01:01:50.240]  предоставить высокой модели согласованности.
[01:01:52.240 --> 01:01:54.240]  А это значит, что эта система
[01:01:54.240 --> 01:01:56.240]  неизбежно заставляет вас
[01:01:56.240 --> 01:01:58.240]  думать про свою реализацию.
[01:01:58.240 --> 01:02:00.240]  Вот мы будем говорить про
[01:02:00.240 --> 01:02:02.240]  Кассандру и Динамо,
[01:02:02.240 --> 01:02:04.240]  и для того, чтобы понять,
[01:02:04.240 --> 01:02:06.240]  какие гарантии вы получаете,
[01:02:06.240 --> 01:02:08.240]  вы обязаны разобраться,
[01:02:08.240 --> 01:02:10.240]  как внутри система
[01:02:10.240 --> 01:02:12.240]  реплицирует данные.
[01:02:12.240 --> 01:02:14.240]  Если же вы видите,
[01:02:14.240 --> 01:02:16.240]  что ваша система...
[01:02:16.240 --> 01:02:18.240]  Если вы видите в документации,
[01:02:18.240 --> 01:02:20.240]  что ваша система позиционирует себя
[01:02:20.240 --> 01:02:22.240]  не как AP,
[01:02:22.240 --> 01:02:24.240]  а как consistency
[01:02:24.240 --> 01:02:26.240]  плюс partition tolerance,
[01:02:26.240 --> 01:02:28.240]  то есть она выбирает случай partition
[01:02:28.240 --> 01:02:30.240]  в consistency, то вы понимаете,
[01:02:30.240 --> 01:02:32.240]  что эта система использует консенсус,
[01:02:32.240 --> 01:02:34.240]  она видимо предоставляет вам
[01:02:34.240 --> 01:02:36.240]  высокую модель изоляции
[01:02:36.240 --> 01:02:38.240]  или высокую модель согласованности,
[01:02:38.240 --> 01:02:40.240]  но в то же время
[01:02:40.240 --> 01:02:42.240]  эта система в случае partition
[01:02:42.240 --> 01:02:44.240]  в одной части
[01:02:44.240 --> 01:02:46.240]  становится полностью недоступной,
[01:02:46.240 --> 01:02:48.240]  ну и в принципе
[01:02:48.240 --> 01:02:50.240]  в ней возможны лайфлоги,
[01:02:50.240 --> 01:02:52.240]  то есть какие-то периоды,
[01:02:52.240 --> 01:02:54.240]  когда система недоступна, хотя все ее узлы
[01:02:54.240 --> 01:02:56.240]  могут быть даже живы,
[01:02:56.240 --> 01:02:58.240]  просто в силу асинхронности сети
[01:02:58.240 --> 01:03:00.240]  и в силу FOPTR.
[01:03:02.240 --> 01:03:04.240]  Такой вот фундаментальный трейдов,
[01:03:04.240 --> 01:03:06.240]  который заложен в любой системе
[01:03:08.240 --> 01:03:10.240]  и фундаментальный трейдов
[01:03:10.240 --> 01:03:12.240]  в смысле между consistency
[01:03:12.240 --> 01:03:14.240]  и availability
[01:03:14.240 --> 01:03:16.240]  и на уровне реализации,
[01:03:16.240 --> 01:03:18.240]  на уровне уже чего-то конкретного
[01:03:18.240 --> 01:03:20.240]  этот трейдов заключается
[01:03:20.240 --> 01:03:22.240]  в выборе или в отказе
[01:03:22.240 --> 01:03:24.240]  от использования
[01:03:24.240 --> 01:03:26.240]  atomic broadcast и задачи консенсуса.
[01:03:28.240 --> 01:03:30.240]  То есть если
[01:03:30.240 --> 01:03:32.240]  cap-теорема
[01:03:32.240 --> 01:03:34.240]  еще раз правильно
[01:03:34.240 --> 01:03:36.240]  спозиционирую ваше
[01:03:36.240 --> 01:03:38.240]  понимание, cap-теорема
[01:03:38.240 --> 01:03:40.240]  это всего лишь такая интуиция,
[01:03:42.240 --> 01:03:44.240]  какое предпочтение
[01:03:44.240 --> 01:03:46.240]  делать система в своем дизайне.
[01:03:46.240 --> 01:03:48.240]  А под капотом уже
[01:03:48.240 --> 01:03:50.240]  действуют совсем другие
[01:03:50.240 --> 01:03:52.240]  теоремы
[01:03:52.240 --> 01:03:54.240]  и другие задачи,
[01:03:54.240 --> 01:03:56.240]  и под капотом мы говорим про задачу
[01:03:56.240 --> 01:03:58.240]  atomic broadcast,
[01:03:58.240 --> 01:04:00.240]  то есть упраточную доставку сообщений
[01:04:00.240 --> 01:04:02.240]  про то, что эта задача
[01:04:04.240 --> 01:04:06.240]  atomic broadcast решается
[01:04:06.240 --> 01:04:08.240]  с помощью задачи консенсуса,
[01:04:08.240 --> 01:04:10.240]  а задача консенсуса в некоторых
[01:04:10.240 --> 01:04:12.240]  ограничениях просто не решается
[01:04:12.240 --> 01:04:14.240]  и решается
[01:04:14.240 --> 01:04:16.240]  без каких-то очень сильных гарантий,
[01:04:16.240 --> 01:04:18.240]  то есть без гарантий завершения.
[01:04:20.240 --> 01:04:22.240]  И вот все это связано
[01:04:22.240 --> 01:04:24.240]  напрямую с моделью согласованности,
[01:04:24.240 --> 01:04:26.240]  которую система вам предоставляет.
[01:04:26.240 --> 01:04:28.240]  То есть как вы можете думать
[01:04:28.240 --> 01:04:30.240]  о ее поведении?
[01:04:30.240 --> 01:04:32.240]  Вот если вы все это в голове свяжете,
[01:04:32.240 --> 01:04:34.240]  то станет примерно понятно,
[01:04:34.240 --> 01:04:36.240]  как можно из гарантии
[01:04:36.240 --> 01:04:38.240]  системы сразу понимать какие-то
[01:04:38.240 --> 01:04:40.240]  фундаментальные принципы ее дизайна
[01:04:40.240 --> 01:04:42.240]  и наоборот понимать, что какие-то
[01:04:42.240 --> 01:04:44.240]  гарантии система вам дать не может.
[01:04:44.240 --> 01:04:46.240]  Если она является
[01:04:46.240 --> 01:04:48.240]  высокодоступной, то это означает, что
[01:04:48.240 --> 01:04:50.240]  она не может использовать консенсус
[01:04:50.240 --> 01:04:52.240]  и это значит, что она будет
[01:04:52.240 --> 01:04:54.240]  рано или поздно
[01:04:54.240 --> 01:04:56.240]  и будет для вас неатомарной.
[01:04:58.240 --> 01:05:00.240]  Ну вот это такой
[01:05:00.240 --> 01:05:02.240]  совсем короткий экскурс
[01:05:02.240 --> 01:05:04.240]  в
[01:05:04.240 --> 01:05:06.240]  основные базворды,
[01:05:06.240 --> 01:05:08.240]  которые нужно знать,
[01:05:08.240 --> 01:05:10.240]  при работе с определенными системами.
[01:05:10.240 --> 01:05:12.240]  И я бы еще
[01:05:12.240 --> 01:05:14.240]  к этому у нас осталось немного времени,
[01:05:14.240 --> 01:05:16.240]  я бы еще раз здесь сказал про то,
[01:05:16.240 --> 01:05:18.240]  как примерно
[01:05:18.240 --> 01:05:20.240]  о системе можно
[01:05:20.240 --> 01:05:22.240]  думать не в смысле
[01:05:22.240 --> 01:05:24.240]  задач, модели согласованности
[01:05:24.240 --> 01:05:26.240]  и возможных отказов
[01:05:26.240 --> 01:05:28.240]  и отказоустойчивости
[01:05:28.240 --> 01:05:30.240]  в смысле архитектуры.
[01:05:30.240 --> 01:05:32.240]  Вот это такие два взгляда, которые
[01:05:32.240 --> 01:05:34.240]  требуются для того, чтобы
[01:05:34.240 --> 01:05:36.240]  системой просто аккуратно
[01:05:36.240 --> 01:05:38.240]  пользоваться. Какие гарантии она дает,
[01:05:38.240 --> 01:05:40.240]  какие задачи она внутри решает с одной
[01:05:40.240 --> 01:05:42.240]  стороны, а с другой
[01:05:42.240 --> 01:05:44.240]  какие
[01:05:44.240 --> 01:05:46.240]  слои выделены
[01:05:46.240 --> 01:05:48.240]  в ее архитектуре.
[01:05:48.240 --> 01:05:50.240]  Смотрите,
[01:05:50.240 --> 01:05:52.240]  как бы система ваша не была устроена
[01:05:54.240 --> 01:05:56.240]  и какой бы модель данных
[01:05:56.240 --> 01:05:58.240]  она вам не предоставляла,
[01:05:58.240 --> 01:06:00.240]  будь то key value хранилищ,
[01:06:00.240 --> 01:06:02.240]  будь то база данных,
[01:06:02.240 --> 01:06:04.240]  будь то распределенная файловая система,
[01:06:06.240 --> 01:06:08.240]  в какой-то момент
[01:06:08.240 --> 01:06:10.240]  вы должны
[01:06:10.240 --> 01:06:12.240]  где-то хранить данные.
[01:06:12.240 --> 01:06:14.240]  То есть система распределенная, конечно же,
[01:06:14.240 --> 01:06:16.240]  но рано или поздно
[01:06:16.240 --> 01:06:18.240]  вы спускаетесь в отдельные жесткие диски
[01:06:20.240 --> 01:06:22.240]  и должны там
[01:06:22.240 --> 01:06:24.240]  хранить и модифицировать
[01:06:24.240 --> 01:06:26.240]  большое количество данных.
[01:06:26.240 --> 01:06:28.240]  Причем на этих конкретных дисках
[01:06:28.240 --> 01:06:30.240]  уже эти данные будут
[01:06:30.240 --> 01:06:32.240]  обновляться точечно
[01:06:36.240 --> 01:06:38.240]  по-другому скажу,
[01:06:38.240 --> 01:06:40.240]  что вот в конце концов
[01:06:40.240 --> 01:06:42.240]  на самом низком уровне архитектуры вам нужно
[01:06:42.240 --> 01:06:44.240]  будет решать задачу хранения данных
[01:06:44.240 --> 01:06:46.240]  и сложность в этой задаче
[01:06:46.240 --> 01:06:48.240]  в том, что
[01:06:48.240 --> 01:06:50.240]  чтобы позволить
[01:06:50.240 --> 01:06:52.240]  точечный апдай данных
[01:06:52.240 --> 01:06:54.240]  скажем, если вы храните key value хранилищ
[01:06:54.240 --> 01:06:56.240]  то вот вы храните это key value хранилищ на диске
[01:06:58.240 --> 01:07:00.240]  и при этом
[01:07:00.240 --> 01:07:02.240]  делать это эффективно
[01:07:02.240 --> 01:07:04.240]  с учетом того, что диски
[01:07:04.240 --> 01:07:06.240]  не умеют
[01:07:06.240 --> 01:07:08.240]  случайный доступ.
[01:07:08.240 --> 01:07:10.240]  Вот диски, которые вращаются
[01:07:10.240 --> 01:07:12.240]  они в принципе не умеют прыгать
[01:07:12.240 --> 01:07:14.240]  быстро между отдельными секторами.
[01:07:14.240 --> 01:07:16.240]  Для этого диску нужно повернуться
[01:07:16.240 --> 01:07:18.240]  и передвинуть считывающую головку
[01:07:18.240 --> 01:07:20.240]  на другой трак.
[01:07:20.240 --> 01:07:22.240]  Это вот единицы миллисекунд.
[01:07:22.240 --> 01:07:24.240]  Вот на этом уровне возникает такая задача
[01:07:24.240 --> 01:07:26.240]  как делать это эффективно
[01:07:26.240 --> 01:07:28.240]  и как делать это надежно.
[01:07:28.240 --> 01:07:30.240]  Как переживать, скажем,
[01:07:30.240 --> 01:07:32.240]  рестартную машину в любой момент времени.
[01:07:32.240 --> 01:07:34.240]  Это вот самый низкий слой
[01:07:34.240 --> 01:07:36.240]  архитектуры любой системы.
[01:07:36.240 --> 01:07:38.240]  Абсолютно любой.
[01:07:38.240 --> 01:07:40.240]  Дальше вы поднимаетесь на следующий
[01:07:40.240 --> 01:07:42.240]  уровень. Это уровень репликации.
[01:07:42.240 --> 01:07:44.240]  И вот уровень репликации
[01:07:44.240 --> 01:07:46.240]  это ровно тот уровень,
[01:07:46.240 --> 01:07:48.240]  на котором возникают
[01:07:48.240 --> 01:07:50.240]  все гарантии, в смысле модели
[01:07:50.240 --> 01:07:52.240]  согласованности.
[01:07:52.240 --> 01:07:54.240]  И на котором
[01:07:56.240 --> 01:07:58.240]  появляется
[01:07:58.240 --> 01:08:00.240]  трейдов между доступностью
[01:08:00.240 --> 01:08:02.240]  и согласованностью.
[01:08:02.240 --> 01:08:04.240]  И вот это тот уровень,
[01:08:04.240 --> 01:08:06.240]  на котором фиксируется отказово-устойчивость вашей системы.
[01:08:06.240 --> 01:08:08.240]  То есть сколько отказов
[01:08:08.240 --> 01:08:10.240]  она способна переживать.
[01:08:10.240 --> 01:08:12.240]  Ну ладно, не совсем, но и на этом уровне тоже.
[01:08:12.240 --> 01:08:14.240]  Вот
[01:08:14.240 --> 01:08:16.240]  вся
[01:08:16.240 --> 01:08:18.240]  все те базворды,
[01:08:18.240 --> 01:08:20.240]  которые я произносил,
[01:08:20.240 --> 01:08:22.240]  вот у меня был откаст, консенсус, модели согласованности.
[01:08:22.240 --> 01:08:24.240]  Это все вот этот уровень репликации.
[01:08:24.240 --> 01:08:26.240]  Его задача
[01:08:26.240 --> 01:08:28.240]  взять отдельные жесткие диски
[01:08:28.240 --> 01:08:30.240]  и отдельные хранилища данных
[01:08:30.240 --> 01:08:32.240]  на этих жестких дисках
[01:08:32.240 --> 01:08:34.240]  и сделать
[01:08:34.240 --> 01:08:36.240]  из этих нескольких дисков
[01:08:36.240 --> 01:08:38.240]  нескольких машин,
[01:08:38.240 --> 01:08:40.240]  которые сами по себе
[01:08:40.240 --> 01:08:42.240]  не отказово-устойчивые, разумеется.
[01:08:42.240 --> 01:08:44.240]  Сделать некоторую отказово-устойчивую
[01:08:44.240 --> 01:08:46.240]  сущность.
[01:08:46.240 --> 01:08:48.240]  Сделать одну машину,
[01:08:48.240 --> 01:08:50.240]  которая как будто бы не отказывается
[01:08:50.240 --> 01:08:52.240]  с помощью томикбродкаста.
[01:08:52.240 --> 01:08:54.240]  То есть решить задачу консенсуса
[01:08:54.240 --> 01:08:56.240]  и решить задачу репликации.
[01:08:56.240 --> 01:08:58.240]  И вот эта задача,
[01:08:58.240 --> 01:09:00.240]  решение этой задачи более-менее
[01:09:00.240 --> 01:09:02.240]  называется
[01:09:02.240 --> 01:09:04.240]  реплицированным автоматом.
[01:09:06.240 --> 01:09:08.240]  Реплицированный автомат позволяет вам
[01:09:08.240 --> 01:09:10.240]  реплицировать
[01:09:10.240 --> 01:09:12.240]  некоторое состояние совершенно
[01:09:12.240 --> 01:09:14.240]  произвольное между несколькими машинами.
[01:09:14.240 --> 01:09:16.240]  Для этого там решается задача.
[01:09:16.240 --> 01:09:18.240]  Реплицируется блок изменений.
[01:09:18.240 --> 01:09:20.240]  Он реплицируется с помощью томикбродкаста.
[01:09:20.240 --> 01:09:22.240]  Потом несколький модель
[01:09:22.240 --> 01:09:24.240]  решает задачу консенсуса.
[01:09:24.240 --> 01:09:26.240]  И вот если вы используете такой дизайн,
[01:09:26.240 --> 01:09:28.240]  то на этом уровне у вас появляется как раз
[01:09:28.240 --> 01:09:30.240]  с одной стороны согласованность,
[01:09:30.240 --> 01:09:32.240]  а с другой стороны вы теряете
[01:09:32.240 --> 01:09:34.240]  доступность в случае,
[01:09:34.240 --> 01:09:36.240]  в меньшей части partition в случае
[01:09:36.240 --> 01:09:38.240]  разделения сетей на два сегмента.
[01:09:40.240 --> 01:09:42.240]  Если же вы Cassandra или Dynamo,
[01:09:42.240 --> 01:09:44.240]  то вы используете другие протоколы,
[01:09:44.240 --> 01:09:46.240]  более слабые протоколы для репликации,
[01:09:46.240 --> 01:09:48.240]  и в этом случае вы теряете
[01:09:48.240 --> 01:09:50.240]  согласованность.
[01:09:50.240 --> 01:09:52.240]  Но при этом вы можете обслуживать записи и чтения
[01:09:52.240 --> 01:09:54.240]  даже в случае partition.
[01:09:56.240 --> 01:09:58.240]  Следующий уровень репликации любой системы
[01:09:58.240 --> 01:10:00.240]  это слой
[01:10:00.240 --> 01:10:02.240]  шардирования.
[01:10:02.240 --> 01:10:04.240]  Вот у вас может быть
[01:10:04.240 --> 01:10:06.240]  очень много таблиц
[01:10:06.240 --> 01:10:08.240]  в системе, очень много данных
[01:10:08.240 --> 01:10:10.240]  в таблицах, очень большие файлы,
[01:10:10.240 --> 01:10:12.240]  очень
[01:10:12.240 --> 01:10:14.240]  много файлов.
[01:10:14.240 --> 01:10:16.240]  И все это, конечно же, не помещается
[01:10:16.240 --> 01:10:18.240]  в отдельную машину. Вы берете
[01:10:18.240 --> 01:10:20.240]  много машин,
[01:10:20.240 --> 01:10:22.240]  делите ваши данные
[01:10:22.240 --> 01:10:24.240]  таблице
[01:10:24.240 --> 01:10:26.240]  или файловую систему каким-то образом
[01:10:26.240 --> 01:10:28.240]  на части.
[01:10:28.240 --> 01:10:30.240]  С файлами системы это сложно делать, но вот
[01:10:30.240 --> 01:10:32.240]  когда мы говорим про таблицы, то есть про кивали
[01:10:32.240 --> 01:10:34.240]  у хранилища и про базу данных,
[01:10:34.240 --> 01:10:36.240]  то шардировать довольно естественно.
[01:10:36.240 --> 01:10:38.240]  Можно по диапазонам строчек.
[01:10:38.240 --> 01:10:40.240]  Вот мы эти данные шардируем,
[01:10:40.240 --> 01:10:42.240]  раскладываем их по разным машинам
[01:10:44.240 --> 01:10:46.240]  и таким образом
[01:10:46.240 --> 01:10:48.240]  мы позволяем системе
[01:10:48.240 --> 01:10:50.240]  потенциально горизонтально
[01:10:50.240 --> 01:10:52.240]  масштабироваться.
[01:10:54.240 --> 01:10:56.240]  Если мы пытаемся соотнести это
[01:10:56.240 --> 01:10:58.240]  с предыдущими уровнями, то вот
[01:10:58.240 --> 01:11:00.240]  картинка примерно это устроена.
[01:11:00.240 --> 01:11:02.240]  Если мы пишем базу данных,
[01:11:02.240 --> 01:11:04.240]  то под капотом это будет
[01:11:04.240 --> 01:11:06.240]  кивали у хранилища. Это кивали у хранилища
[01:11:06.240 --> 01:11:08.240]  делится на диапазоны
[01:11:08.240 --> 01:11:10.240]  ключей.
[01:11:10.240 --> 01:11:12.240]  Эти диапазоны ключей реплицируются
[01:11:12.240 --> 01:11:14.240]  на разных машинах и реплицируются
[01:11:14.240 --> 01:11:16.240]  с помощью Turing Broadcast консенсусом.
[01:11:18.240 --> 01:11:20.240]  При этом каждая физическая машина в кластере
[01:11:20.240 --> 01:11:22.240]  она отвечает за
[01:11:22.240 --> 01:11:24.240]  репликацию
[01:11:24.240 --> 01:11:26.240]  сразу многих диапазонов
[01:11:26.240 --> 01:11:28.240]  ваших данных.
[01:11:32.240 --> 01:11:34.240]  После этого уровня
[01:11:34.240 --> 01:11:36.240]  идут уже транзакции
[01:11:36.240 --> 01:11:38.240]  и идет слой распыленных запросов.
[01:11:42.240 --> 01:11:44.240]  Так же как CAPTIOREMA
[01:11:44.240 --> 01:11:46.240]  в фундаментальном смысле
[01:11:46.240 --> 01:11:48.240]  CAPTIOREMA в фундаментальном смысле
[01:11:48.240 --> 01:11:50.240]  что она заставляет вас делать выбор
[01:11:50.240 --> 01:11:52.240]  между
[01:11:52.240 --> 01:11:54.240]  согласованностью и доступностью
[01:11:54.240 --> 01:11:56.240]  то есть между применением консенсуса
[01:11:56.240 --> 01:11:58.240]  и отказом от консенсуса
[01:11:58.240 --> 01:12:00.240]  так же
[01:12:00.240 --> 01:12:02.240]  вот эта диаграмма
[01:12:02.240 --> 01:12:04.240]  вот эти слои, этот дизайн
[01:12:04.240 --> 01:12:06.240]  он достаточно фундаментален
[01:12:06.240 --> 01:12:08.240]  и так или иначе прослеживается в любой распределенной системе.
[01:12:10.240 --> 01:12:12.240]  Вот из таких кубиков вы в конечном итоге
[01:12:12.240 --> 01:12:14.240]  ее собираете.
[01:12:14.240 --> 01:12:16.240]  Тут конечно есть
[01:12:16.240 --> 01:12:18.240]  разные нюансы, разные
[01:12:18.240 --> 01:12:20.240]  разные способы
[01:12:20.240 --> 01:12:22.240]  в этом дизайне что-то оптимизировать.
[01:12:22.240 --> 01:12:24.240]  Это отдельная большая история.
[01:12:24.240 --> 01:12:26.240]  Но в целом
[01:12:26.240 --> 01:12:28.240]  если вы говорите про
[01:12:28.240 --> 01:12:30.240]  Кавку
[01:12:30.240 --> 01:12:32.240]  или вы говорите про Кассандру
[01:12:32.240 --> 01:12:34.240]  или вы берете спанер
[01:12:34.240 --> 01:12:36.240]  то в любой подобной системе
[01:12:36.240 --> 01:12:38.240]  все равно будут
[01:12:38.240 --> 01:12:40.240]  вот такие вот слои.
[01:12:40.240 --> 01:12:42.240]  Где-то транзакций не будет, не будет SQL
[01:12:42.240 --> 01:12:44.240]  но по крайней мере вот такие подзадачи
[01:12:44.240 --> 01:12:46.240]  решаются довольно изолировано
[01:12:46.240 --> 01:12:48.240]  и о них нужно и думать
[01:12:48.240 --> 01:12:50.240]  изолировано, потому что нужно
[01:12:50.240 --> 01:12:52.240]  декомпозировать разные компоненты системы
[01:12:52.240 --> 01:12:54.240]  по задачам, которые они решают.
[01:12:54.240 --> 01:12:56.240]  Вот задачи принципиально такие
[01:12:56.240 --> 01:12:58.240]  и одна из них выстраивается над другой.
[01:13:06.240 --> 01:13:08.240]  Ну вот примерно так
[01:13:08.240 --> 01:13:10.240]  можно думать про распределенную
[01:13:10.240 --> 01:13:12.240]  систему в целом.
[01:13:12.240 --> 01:13:14.240]  То есть брать любую систему
[01:13:14.240 --> 01:13:16.240]  и вот раскладывать ее по слоям
[01:13:16.240 --> 01:13:18.240]  архитектуры, по гарантиям, которые
[01:13:18.240 --> 01:13:20.240]  она доставляет, по задачам
[01:13:20.240 --> 01:13:22.240]  которые внутри нее решаются или нет
[01:13:22.240 --> 01:13:24.240]  и таким образом
[01:13:24.240 --> 01:13:26.240]  получать довольно неплохое представление
[01:13:26.240 --> 01:13:28.240]  об отказу устойчивости
[01:13:28.240 --> 01:13:30.240]  и о поведении системы.
[01:13:32.240 --> 01:13:34.240]  Просто потому что вот такой дизайн
[01:13:34.240 --> 01:13:36.240]  и такие ограничения
[01:13:36.240 --> 01:13:38.240]  они касаются любой системы
[01:13:38.240 --> 01:13:40.240]  как бы она не была устроена
[01:13:40.240 --> 01:13:42.240]  какую бы задачу она не решала.
[01:13:42.240 --> 01:13:44.240]  И уже вот на понимание
[01:13:44.240 --> 01:13:46.240]  этой общей архитектуры
[01:13:46.240 --> 01:13:48.240]  и общих ограничений
[01:13:48.240 --> 01:13:50.240]  нужно конкретную систему отображать.
[01:13:52.240 --> 01:13:54.240]  Мы собственно в последующих занятиях
[01:13:54.240 --> 01:13:56.240]  так и будем делать. Если мы будем
[01:13:56.240 --> 01:13:58.240]  говорить про Кассандру, то мы будем
[01:13:58.240 --> 01:14:00.240]  разбираться, а как именно
[01:14:00.240 --> 01:14:02.240]  она не используя
[01:14:02.240 --> 01:14:04.240]  консенсус
[01:14:04.240 --> 01:14:06.240]  пытается все-таки обеспечить
[01:14:06.240 --> 01:14:08.240]  согласованность и как это позволяет
[01:14:08.240 --> 01:14:10.240]  достичь доступности.
[01:14:10.240 --> 01:14:12.240]  И если мы говорим про
[01:14:12.240 --> 01:14:14.240]  Spanner, то каким образом
[01:14:14.240 --> 01:14:16.240]  он обеспечивая согласованность
[01:14:16.240 --> 01:14:18.240]  все же пытается достичь
[01:14:18.240 --> 01:14:20.240]  доступности.
[01:14:20.240 --> 01:14:22.240]  В общем, это разговор
[01:14:22.240 --> 01:14:24.240]  про следующие занятия, там где
[01:14:24.240 --> 01:14:26.240]  мы будем говорить про конкретные системы
[01:14:26.240 --> 01:14:28.240]  разбирать конкретный дизайн, свойства.
[01:14:28.240 --> 01:14:30.240]  Ну а пока
[01:14:30.240 --> 01:14:32.240]  с обводной частью, наверное, все.

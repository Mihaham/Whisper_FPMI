[00:00.000 --> 00:15.080]  все отлично так значит в прошлый раз мы с вами ввели характеристическую функцию на самом деле
[00:15.080 --> 00:23.560]  это просто преобразование фурье наряду с характеристической функцией рассматривается еще
[00:23.560 --> 00:35.600]  вот такие похожие на похоже на нее функция то есть она как характеристической только комплексные
[00:35.600 --> 00:43.120]  экспоненты нет это как комплексной да и нету видите вот это функция называются характеристической
[00:43.120 --> 00:49.240]  функцией моментов или производящие функции моментов так сказать в иностранной литературе у
[00:49.240 --> 00:58.880]  нее есть название специальная мгф функция moment generation function ну и собственно чем она любопытна
[00:58.880 --> 01:06.360]  по большому счету ничем свойства у ней схоже с свойством характеристической функции но в
[01:06.360 --> 01:18.000]  частности ее катая производная в нуле в точности равна к этому моменту если он существует функция
[01:18.000 --> 01:27.000]  моментов от суммы независимых случайных величин равна произведению этих моментных функций ну
[01:27.000 --> 01:33.080]  и там остальные свойства типа изменения при линейном преобразовании и так далее все похоже
[01:33.080 --> 01:42.080]  ну вот возникает вопрос зачем она нужна тем более что у нее есть такое свойство или точнее
[01:42.200 --> 01:48.740]  возникает вопрос зачем нужна характеристическая функция если есть такая же значит дело в том что
[01:48.740 --> 01:54.800]  характеристическая функция как мы знаем всегда ограничена по модулю единицей и всегда существует
[01:54.800 --> 02:01.100]  для любой случайной величины характеристическая функция а-t это еще равномерно непрерывная
[02:01.100 --> 02:08.800]  функция а вот функция моментов может вставать не всегда или не при всех t и она не ограничена то
[02:08.800 --> 02:17.300]  То есть в теоретической точки зрения моментная функция менее удобна, чем характеристическая функция.
[02:17.300 --> 02:23.300]  Но с другой стороны, моментную функцию можно вводить в курсы для студентов,
[02:23.300 --> 02:27.300]  которые не знают теории функций комплексного переменного.
[02:27.300 --> 02:33.300]  Это тоже с педагогической и методологической точки зрения полезно,
[02:33.300 --> 02:40.800]  потому что в общем свойства-то в основном те же, а про комплексно-значную плоскость можно не говорить.
[02:40.800 --> 02:49.800]  И еще наряду с функцией моментов, еще бывает частенько, но это вопрос удобства.
[02:49.800 --> 02:56.800]  Используют функцию математическое ожидание t в степени кси.
[02:56.800 --> 03:03.800]  Мы вообще знаем такую как бы функцию, что это такое, мы как называем ее?
[03:03.800 --> 03:11.800]  Ну в общем случае, там сумма ряда там t в степени к или фетка.
[03:11.800 --> 03:15.800]  Производящая функция, это производящая функция.
[03:15.800 --> 03:26.800]  Ну и тоже в иностранной литературе она имеет собственное название pgf probability generation function.
[03:26.800 --> 03:32.800]  То есть функция генерящая вероятности. Откуда это название берется?
[03:32.800 --> 03:37.800]  Если кси у нас принимает значение, ну натуральные числа,
[03:37.800 --> 03:43.800]  то тогда производящая функция обладает таким свойством.
[03:43.800 --> 03:53.800]  Ее k-тое производное в нуле равно k-факториал на вероятность того, что кси равно k.
[03:53.800 --> 03:58.800]  Иногда бывает удобно, но вот чем имеет место.
[03:58.800 --> 04:06.800]  Ну вернемся к моментной функции, о которой мы, так сказать, вот тут поминули ее.
[04:06.800 --> 04:14.800]  И отмечу, что есть у нее еще одно полезное свойство, чисто техническое, но важное.
[04:14.800 --> 04:22.800]  И давайте мы как бы рассмотрим на конкретном примере, чем нам порой может помочь моментная функция.
[04:22.800 --> 04:27.800]  Значит, давайте припомним гамма распределения,
[04:27.800 --> 04:34.800]  которая зависит от двух параметров и задается своей плотностью.
[04:41.800 --> 04:45.800]  α, λ, х все больше нуля.
[04:46.800 --> 04:53.800]  Ну и задача найти характеристическую функцию гамма распределения.
[04:53.800 --> 05:00.800]  Ну что мы должны сделать? Мы должны эту функцию умножить на e в степени и tx и проинтегрировать.
[05:00.800 --> 05:07.800]  А что будет под интегралом? Тригинометрическая функция умножена на степенную функцию, умножена на экспонент.
[05:07.800 --> 05:14.800]  Довольно сложная штука. Ну можно, конечно, безусловно, но получается сложновато.
[05:14.800 --> 05:20.800]  Поэтому давайте мы для начала найдем моментную функцию гамма распределения.
[05:22.800 --> 05:26.800]  Я тут без параметров буду писать.
[05:44.800 --> 05:51.800]  И сделаю такие немудренные преобразования.
[06:14.800 --> 06:17.800]  Правильно, да?
[06:21.800 --> 06:29.800]  Ну вот это с этим сокращается. Вот она лямбда в степени α, а вот он e в степени xt.
[06:29.800 --> 06:36.800]  Вот отсюда сразу видно, когда существует функция моментов.
[06:36.800 --> 06:41.800]  Для гамма распределения она существует, но не для всех t.
[06:41.800 --> 06:46.800]  А для каких t? Вот для таких t.
[06:51.800 --> 06:55.800]  Вот для таких t она существует.
[06:55.800 --> 07:00.800]  И если лямбда минус t больше 0, чем уровень вот этот интеграл?
[07:12.800 --> 07:19.800]  Коллеги, это интеграл от плотности случайной величины, взятый по всему пространству.
[07:20.800 --> 07:26.800]  Еще раз. Чему равен? Единицы.
[07:26.800 --> 07:33.800]  Потому что вот под интегралом ничто иное, как гамма распределения с параметрами α и лямбда минус t.
[07:33.800 --> 07:38.800]  Правильно, да? И отсюда мы получаем, собственно, вот ответ.
[07:38.800 --> 07:44.800]  Вот это моментная функция, но в области лямбда минус t больше 0.
[07:44.800 --> 07:49.800]  А теперь давайте заметим такую вещь, я ее вот здесь запишу.
[07:49.800 --> 07:58.800]  Если в моментную функцию чисто формально подставите t, то получится характеристическое.
[07:58.800 --> 08:01.800]  Видите, да?
[08:01.800 --> 08:13.800]  Ну я подставлю сюда чисто формально и напишу, фиксия t равно лямбда на лямбда минус i t в степени альфа.
[08:13.800 --> 08:19.800]  Могу я так? Вытирую функции комплексного переменного?
[08:19.800 --> 08:24.800]  А вы ее еще не проходите?
[08:24.800 --> 08:27.800]  Понятно.
[08:27.800 --> 08:33.800]  Ну тогда скажу, вы в будущем это узнаете. Ну по крайней мере комплексную экспоненту вы знаете, да?
[08:33.800 --> 08:42.800]  Значит, смотрите, вот эти функции от комплексной переменной, у них есть такое любопытное свойство.
[08:42.800 --> 08:46.800]  Ну, правда, некий класс очень широкий называется аналитические функции.
[08:46.800 --> 08:52.800]  У него есть такое свойство, если у вас две функции совпадают на какой-то сходящейся последовательности,
[08:52.800 --> 08:58.800]  то есть значение двух функций совпадает на какой-то сходящейся последовательности или на каком-то интервальчике,
[08:58.800 --> 09:02.800]  то эти функции равны вообще во всей комплексной плоскости.
[09:02.800 --> 09:07.800]  Но для вас это просто формальная подстановка.
[09:07.800 --> 09:11.800]  То есть законность этой подстановки это уже, ну, как бы некая теория.
[09:11.800 --> 09:14.800]  Для вас это формальная подстановка.
[09:14.800 --> 09:22.800]  Подставили ИТ и получили характеристическую функцию гамма-распределения.
[09:22.800 --> 09:26.800]  Ну и отсюда всякие интересные выводы в принципе следуют.
[09:26.800 --> 09:39.800]  Например, мы с вами вводили показательные распределения как гамма-распределения с параметрами 1 лямбда.
[09:39.800 --> 09:51.800]  Ну и чему тогда равна характеристическая функция экспоненциального распределения?
[09:51.800 --> 09:57.800]  Ну просто α равно 1, подставляем, получаем лямбда на лямбда минус ИТ.
[09:57.800 --> 10:01.800]  Лямбда на лямбда минус ИТ.
[10:01.800 --> 10:10.800]  А теперь давайте будем считать, что альфа у нас натуральная, альфа равно некому n.
[10:10.800 --> 10:26.800]  Тогда характеристическая функция гамма-распределения с параметрами n и лямбда равно лямбда на лямбда минус ИТ в степени n.
[10:26.800 --> 10:29.800]  Вот из этой формулы это следует.
[10:30.800 --> 10:37.800]  Что отсюда, какой вывод мы делаем глядя на это соотношение?
[10:41.800 --> 10:54.800]  Что сумма независимых случайных величин, имеющих экспоненциальное распределение в количестве n штук имеет гамма-распределение с параметрами n лямбда.
[10:54.800 --> 11:11.800]  То есть отсюда мы сразу получаем, что гамма альфа лямбда равна сумма эксп лямбда ж, ж равно 1 до n.
[11:11.800 --> 11:16.800]  Ну вот вывод, который без характеристических функций довольно сложно было бы получить.
[11:16.800 --> 11:22.800]  Хотя еще раз повторю, аппарат характеристических функций – это техническое средство, не более того.
[11:23.800 --> 11:29.800]  Теперь напомню вам, что мы еще вводили хи квадрат с n степенями свободы.
[11:29.800 --> 11:31.800]  Что это такое было?
[11:33.800 --> 11:35.800]  Гамма с какими параметрами?
[11:38.800 --> 11:40.800]  n пополам, 1 вторая.
[11:43.800 --> 11:45.800]  Напоминаю вам это.
[11:46.800 --> 12:02.800]  Тогда характеристическая функция хи квадрат с n степенями свободы, ну просто подставляю 1 вторая, 1 вторая минус и t в степени n пополам.
[12:09.800 --> 12:13.800]  Гамма от n да, совершенно справедливо, спасибо.
[12:14.800 --> 12:22.800]  Ну еще тут для красоты, числитель знаменатель домножает на двойку, получается 1 делить на 1 минус 2 и t.
[12:26.800 --> 12:29.800]  Хи квадрат с одной степенью свободы, что такое?
[12:29.800 --> 12:32.800]  Ну это надо просто n подставить 1.
[12:36.800 --> 12:40.800]  И что, какой вывод можем сделать, глядя на эти две формулы?
[12:41.800 --> 12:57.800]  Что хи квадрат с n степенями свободы это сумма хи квадрат независимых величин с одной степенью свободы в количестве n штук.
[13:02.800 --> 13:04.800]  Ну и это еще не все.
[13:05.800 --> 13:15.800]  На самом деле хи квадрат с одной степенью свободы это известная нам случайная величина, но я сразу анонсирую.
[13:16.800 --> 13:23.800]  Оказывается, что хи квадрат с одной степенью свободы это квадрат стандартной нормальной случайной величины.
[13:24.800 --> 13:27.800]  Ну как это получить?
[13:28.800 --> 13:33.800]  Ну давайте выпишем функцию плотности для хи квадрат с одной степенью свободы.
[13:34.800 --> 13:40.800]  Это значит сюда нужно подставить альфа n пополам и лямбда 1 вторая.
[13:41.800 --> 13:46.800]  Точнее говоря альфа 1 вторая, поскольку с одной степенью свободы.
[13:46.800 --> 13:51.800]  Ну и получаем эту плотность, вот так я напишу, ее плотность.
[13:52.800 --> 14:00.800]  Это единица на корень из двух, гамма от 1 и 2 чему равна?
[14:02.800 --> 14:12.800]  Корень из пи, х в степени минус одна вторая, внесу сюда х и на е в степени минус х пополам.
[14:17.800 --> 14:21.800]  Если вы, ну не знаю, давайте сделаем как бы.
[14:24.800 --> 14:31.800]  Давайте найдем функцию плотности вот этой случайной величины квадрат стандартного нормального распределения.
[14:32.800 --> 14:39.800]  Вот так хи напишу, fх от х это вероятность, точнее говоря, f большой функции распределения.
[14:40.800 --> 14:50.800]  Это вероятность того, что стандартная квадрат стандартной нормальной величины окажется меньше х, причем х больше нуля, так с кубочками заметим.
[14:51.800 --> 15:02.800]  Это равно вероятности того, что n01 будет меньше корень из х и больше минус корень из х.
[15:03.800 --> 15:15.800]  Это есть функция стандартного нормального распределения в точке корень из х, минус f01 в точке минус корень из х.
[15:16.800 --> 15:19.800]  Ну и для того, чтобы получить плотность, надо просто продифференцировать.
[15:20.800 --> 15:22.800]  Если вы это сделаете, то получите точно такую плотность.
[15:23.800 --> 15:31.800]  И это означает, что х квадрат с одной степенью это квадрат стандартной нормальной случайной величины.
[15:32.800 --> 15:38.800]  И отсюда мы и делаем вывод, который вам будет полезен в дальнейшем.
[15:39.800 --> 15:44.800]  Это представимый стихи квадрат степени n вот такой суммой.
[15:49.800 --> 15:55.800]  Точнее говоря, для вас будет важно, что вот такая сумма имеет х квадрата распределения.
[15:55.800 --> 16:21.800]  На таком весьма простом примере мы получили различные свойства, которые, в общем, так сходу и не поймешь.
[16:22.800 --> 16:28.800]  А скажите, пожалуйста, х квадрат с двумя степенями свободы, какой имеет распределение?
[16:34.800 --> 16:37.800]  Для этого вот сюда нужно поставить n равно 2.
[16:43.800 --> 16:49.800]  Если вы поставите n равно 2, то получите вот эту формулу, где λ одно и второй.
[16:49.800 --> 16:58.800]  То есть сумма квадратов двух стандартных нормальных случайных величин имеет показательное распределение с параметром 1 и 2.
[16:59.800 --> 17:12.800]  Вот такие всякие соотношения, которые так с легкостью получаются с использованием аппарата характеристических функций, а без него получается сложно.
[17:12.800 --> 17:17.800]  Давайте пойдем дальше еще нам немножко.
[17:42.800 --> 17:56.800]  Напомню вам распределение каши, которое имеет два параметра. Я напишу для 0 и 1.
[17:57.800 --> 18:13.800]  Или, ладно, напишу сразу а и сигма, которые имеют вид 1 делить на π сигма, 1 плюс х минус а делить на сигма, все в квадрате.
[18:14.800 --> 18:21.800]  Ну, давайте считать сигма больше нуля, а если это не так, то модуль сигма тут надо ставить, вот здесь.
[18:22.800 --> 18:26.800]  Ну, давайте считать сигма больше нуля.
[18:27.800 --> 18:36.800]  Ну и так сказать, стандартное распределение каши с параметрами 0 и 1, соответственно, имеет вид 1 делить на π 1 плюс х квадрат.
[18:37.800 --> 18:41.800]  Чему равно математическое ожидание этой случайной величины?
[18:41.800 --> 18:56.800]  Оно не существует, и именно оно не равно плюс или минус бесконечности, то есть вот интеграл либега вот от этой функции х dx не существует.
[18:57.800 --> 19:01.800]  В положительной части он плюс бесконечности, в отрицательной части минус бесконечности.
[19:02.800 --> 19:12.800]  Вот это такое свойство, которое всегда упоминают и как примерное распределение, у которого нет мат ожидания, приводит распределение каши.
[19:14.800 --> 19:27.800]  Что это за распределение? Ну вот, например, отношение двух независимых стандартных нормальных случайных величин имеет распределение каши.
[19:28.800 --> 19:37.800]  Например, вот, нас интересует характеристическая функция, ну мы хотим получить характеристическую функцию.
[19:38.800 --> 19:46.800]  Значит, ну попробуем, так удачно у нас получилось с моментной функцией, давайте моментную функцию сначала попробуем.
[19:57.800 --> 20:04.800]  Чему этот интеграл равен?
[20:16.800 --> 20:21.800]  Как е в степени t умножить tх, вот же определение.
[20:28.800 --> 20:38.800]  Ну интеграл равен бесконечности, причем существенно для любого t, только при t равном нулю, как и любая характеристическая функция, он равен единице.
[20:39.800 --> 20:52.800]  Вот, а так он равен бесконечности, то есть здесь нет никакого интервалчика из t, для которых существует моментная функция, поэтому этот фокус не пройдет, как у нас было в прошлый раз.
[20:52.800 --> 21:02.800]  Вот, в принципе, давайте я напишу характеристическая функция, ну стандартная характеристическая функция,
[21:03.800 --> 21:09.800]  у меня такая легко запоминающаяся характеристическая функция, минус модуль t, е в степени, минус модуль t.
[21:10.800 --> 21:18.800]  Наиболее так элегантно и просто она получается с использованием аппарата функции комплексного переменного,
[21:19.800 --> 21:28.800]  то есть надо просто взять интеграл е в степени tх, но вы этого делать не умеете, поэтому я собственно это опускаю часть,
[21:29.800 --> 21:34.800]  но тем не менее можно получить эту характеристическую функцию просто обратным преобразованием в фурье,
[21:35.800 --> 21:45.800]  но ввиду громоздкости делать не буду, кто как бы интересуется обратное преобразование в фурье можете взять и получить вот эту вот функцию.
[21:45.800 --> 21:55.800]  А можно сделать, ну точнее говоря взять от этой функции обратное преобразование в фурье и получить вот эту функцию, вот по какому пути нужно пойти.
[21:56.800 --> 22:04.800]  Так, значит, вот характеристическая функция, ну какой, так же как и для нормального распределения,
[22:05.800 --> 22:11.800]  у распределения каши есть такое свойство, устанавливаемое, ну тоже вот по такой типа схеме.
[22:11.800 --> 22:20.800]  То есть если у вас есть стандартная случайная величина каши с параметрами 0,1, то сигма на случайную величину плюс a,
[22:21.800 --> 22:30.800]  вы получаете распределение каши с параметрами a сигма. Все понимают как это показать, да? Тогда я это опущу.
[22:30.800 --> 22:40.800]  Вот, значит, имеет место такой факт. Ну и давайте напишем, чему равна характеристическая функция уже для произвольного распределения каши.
[22:41.800 --> 22:49.800]  Она равна e в степени i, t, a на характеристическую функцию.
[22:49.800 --> 23:02.800]  Она равна e в степени i, t, a на характеристическую функцию в точке сигма t.
[23:03.800 --> 23:11.800]  Ну мы вроде договорились сигма больше нуля считать, тогда это минус сигма модуль t.
[23:12.800 --> 23:16.800]  Минус сигма модуль t.
[23:16.800 --> 23:26.800]  Ну и отсюда видно из характеристической функции, что сумма независимых случайных величин, имеющие распределение каши,
[23:27.800 --> 23:32.800]  ну с разными параметрами, там a житая, сигма житая, тоже будет иметь распределение каши.
[23:33.800 --> 23:52.800]  То есть если мы рассмотрим вот такую a житая, сигма житая, ну ж от единицы до некоего k, то она будет принадлежать каши с параметрами сумма a житая, сумма сигма житая.
[23:53.800 --> 24:00.800]  Это всем понятно, да? Характеристическая функция суммы, произведение у характерических сумм функции умножаем.
[24:00.800 --> 24:07.800]  Здесь собираются a житые, здесь собираются сигма житые и значит вот такой факт имеет место.
[24:08.800 --> 24:19.800]  Вот это значит то, что касается распределения каши и соотношений, которые нам позволяют получить характеристические функции.
[24:20.800 --> 24:27.800]  Значит для случайных величин аппарата характеристических функций мы на этом закончим.
[24:27.800 --> 24:34.800]  Остальные там примеры довольно тривиальные, типа нахождения характеристической функции, там не знаю, Плассоновского, например, распределения.
[24:35.800 --> 24:38.800]  Ничего интересного нет, поэтому не будем тратить время.
[24:39.800 --> 24:50.800]  А сейчас мы рассмотрим еще один существенный важный пример применения характеристических функций, но только для случайных векторов.
[24:51.800 --> 24:52.800]  Случайных векторов.
[24:52.800 --> 24:53.800]  Случайных векторов.
[24:54.800 --> 24:56.800]  Значит с вашего позволения вот это сотру.
[24:56.800 --> 25:24.800]  И чуть-чуть преобразую определение характеристической функции.
[25:24.800 --> 25:37.800]  Определение характеристической функции вектора это функция, аргументом которой является вектор той же размерности, что и сама случайно влечена.
[25:38.800 --> 25:41.800]  И вот здесь вместо тксиста стоит т транспонированная кси.
[25:42.800 --> 25:44.800]  Ну то есть если чуть подробнее.
[25:44.800 --> 25:56.800]  Вот такое определение.
[25:57.800 --> 26:11.800]  Ну для нас важно, что тут существует тоже взаимодназначное соответствие.
[26:11.800 --> 26:19.800]  То есть каждый векторный функции характеристической функции соответствует случайный вектор однозначно.
[26:20.800 --> 26:29.800]  Есть аналогичные свойства там с интегрированием, причем тут уже смешанные, производные, частные, но не будем на этом останавливаться.
[26:30.800 --> 26:31.800]  Особо нет на это времени.
[26:32.800 --> 26:47.800]  Вот и мы сейчас применим вот это определение характеристической функции для случайного вектора для введения в рассмотрение так называемого обобщенного нормального вектора.
[26:48.800 --> 26:49.800]  Обобщенный нормальный вектор.
[26:50.800 --> 26:54.800]  Ну сначала немножко вернемся назад, сами вспомним.
[26:54.800 --> 27:00.800]  Значит у нас нормальный вектор имеет два параметра.
[27:02.800 --> 27:09.800]  Это математическое ожидание вектора кси, математическое ожидание вектора и кавалиционную матрицу.
[27:10.800 --> 27:18.800]  Кавалиционная матрица, напомню, это математическое ожидание, кси центрированное, на кси центрированное транспонированное.
[27:18.800 --> 27:25.000]  интеллижено-транспонированная. Это матрица k на k. Если k, это у нас размерность вектора.
[27:25.000 --> 27:30.600]  И мы нормальный вектор, увеличились его плотность.
[27:30.600 --> 27:52.640]  Единица делить на 2p в степени k пополам. Корень квадратный из детермината Rc. Для нормального
[27:52.640 --> 27:59.240]  вектора, который мы называем невырежденным, Rc это положительно определенная матрица.
[27:59.240 --> 28:06.680]  Ну и соответственно, детерминат у нее положительный. А здесь экспонента минус,
[28:06.680 --> 28:25.240]  x минус m это вектора, Rx минус m пополам. Вот так мы ввели нормальный случайный вектор через
[28:25.240 --> 28:35.400]  его плотность. Припоминаете, да? Так, ну а теперь давайте сделаем вот что. Пока это еще
[28:35.400 --> 28:41.920]  не характеристические функции, подготовительная работа, просто факт, который будет нам нужен.
[28:55.240 --> 29:01.720]  Давайте рассмотрим невырожденное преобразование. Это равно некая матрица b на x.
[29:01.720 --> 29:10.800]  B невырожденная матрица. Линейное невырожденное преобразование. Ну и давайте просто по правилам
[29:10.800 --> 29:17.160]  замены переменных под интегралом, получим плотность этого нового вектора. Ну, во-первых,
[29:17.160 --> 29:31.240]  x равно b минус 1 это. Значит, модуль якобиана равен, детерминат b минус 1 единица делить на
[29:31.240 --> 29:38.760]  детерминат b по модулю. Но я это перепишу, дальше станет понятно почему. Вот таким образом,
[29:38.760 --> 29:47.760]  единица делить на дет б, детерминат b, на детерминат b транспонированная, корень квадратный.
[29:47.760 --> 30:04.320]  Избавился от модуля. Понятно, да? Так, значит, давайте посмотрим, как у нас преобразуется плотность. Ну,
[30:04.320 --> 30:11.080]  во-первых, значит, в новых переменах, которые мы назовем у, это будет bх. Ну,
[30:11.080 --> 30:19.200]  а соответственно, повторяюсь, x это будет b минус 1у. Ну и давайте плотность преобразуем.
[30:19.200 --> 30:36.520]  Значит, я напишу fn, только тут уже напишу mθ rθ от y равна единица на 2πk пополам. Тут
[30:36.520 --> 30:47.560]  напишу det rx, детерминат b, на детерминат b транспонированного, так длинновато получается,
[30:47.560 --> 31:02.680]  умножить на экспоненту. Ну и тут честно все заменяем. x заменяем на b минус 1 y минус mx,
[31:02.680 --> 31:19.720]  rx в минус 1, тут транспонированная, b в минус 1, y минус mx делить пополам. Давайте я пока
[31:19.720 --> 31:31.960]  поработаю ровно вот с этим выражением. Транспонирую b в минус 1 вынесу, получу y минус b
[31:32.680 --> 31:52.520]  mx транспонированная, b в минус 1 транспонированная, rx минус 1, b в минус 1, y минус bmx пополам.
[31:52.520 --> 31:56.880]  b на mx это что такое?
[32:02.680 --> 32:18.840]  Ну берем от ожидания от обоих сторон, здесь получается bmx, а здесь что? mθ, да? Это,
[32:18.840 --> 32:27.360]  не тета, это. Значит равно, тогда вот я вот это выражение еще-еще преобразовываю, y минус m,
[32:27.360 --> 32:42.960]  это транспонированная, вот эту штуку я таким образом сверну, b на rx, b транспонированная в
[32:42.960 --> 32:57.160]  минус 1, на y минус m это делить пополам согласно с такими преобразованиями.
[33:12.960 --> 33:42.240]  И получаем. Сейчас запишу, поясню. m это r это от y равно единицы делить на 2 pi
[33:42.240 --> 34:05.160]  в степени k пополам, на корень квадратный из детерминанта b rx b экспонента минус,
[34:05.160 --> 34:29.640]  а транспонирование надо, да. Значит минус y минус m это транспонированная b rx b транспонированная в минус 1,
[34:29.640 --> 34:50.840]  y минус m это пополам. Ну и мы видим, что если исходить из того, что m это, это bmxy, а r это,
[34:50.840 --> 35:01.920]  это b rx b транспонированная, то получается, что при линейном преобразовании, невыразденном,
[35:01.920 --> 35:07.160]  нормальный случайный вектор переходит в нормальный случайный вектор. Правильно?
[35:07.160 --> 35:20.480]  Нет вопросов? Ну иногда въедливый слушатель задает такой вопрос.
[35:20.520 --> 35:30.280]  Когда мы здесь писали функцию плотности изначально, Туэр у нас имела вполне конкретный смысл, это
[35:30.280 --> 35:36.560]  матрица с кавериацией. Здесь, конечно, можно так обозначить, но будет ли вот эта матрица,
[35:36.560 --> 35:47.040]  матрица кавериации. Ну, очевидно, что да. Если сюда вместо кси подставить b в минус 1 это,
[35:47.120 --> 36:01.480]  b в минус 1 это, то вы получите ту же самую матрицу. Точнее говоря, давайте, r это,
[36:01.480 --> 36:08.920]  это по определению должно быть математическое ожидание, это транспонированное, это на это
[36:08.920 --> 36:15.600]  центрированное транспонированное, вместо это подставляем bxy транспонированное, ну и тогда
[36:15.600 --> 36:25.240]  получается, что вот эта кавериационная матрица как раз равна br, кси b транспонированное. То есть
[36:25.240 --> 36:30.560]  действительно при невыразденном линейном преобразовании нормальный вектор переходит
[36:30.560 --> 36:36.200]  в нормальный вектор, причем его параметры изменяются вот таким образом у нового вектора.
[36:37.120 --> 36:50.760]  Это предварительно у нас. Давайте я вот сюда запишу и оставлю эту доску,
[36:50.760 --> 37:01.720]  чтобы нам было напоминание. Это равно bxy, тогда получается нормальный вектор с параметрами bmx и
[37:02.680 --> 37:13.480]  кавериционной матрицей brx, b транспонированно. Вот просто будем, запомним это. Так, пошли дальше.
[37:13.480 --> 37:27.240]  Значит, давайте опять возьмем невыразденный нормальный вектор. Это. Невыразденный нормальный
[37:27.240 --> 37:34.120]  вектор означает, что матрица кавериаций у него положительно определена, и еще она к тому же
[37:34.120 --> 37:44.880]  симметрическая. Отсюда следует, что существует такая артагональная матрица U, такая, что U,
[37:44.880 --> 37:58.040]  R это на U транспонированная, будет равно диагональной матрице sigma, которую запишем так sigma1 в
[37:58.040 --> 38:09.400]  квадрате, sigma k в квадрате, а здесь везде нули. Правильно, да? Существует такая матрица. Ну а раз
[38:09.400 --> 38:18.760]  такая матрица существует, то давайте ведем вектор, новый вектор. Этот у нас это, да? Ведем
[38:18.760 --> 38:29.800]  новый вектор кси по правилу U на это. Ну, отдохните и продолжим. Так, ну теперь, собственно, как это,
[38:29.800 --> 38:35.320]  попробуем воспользоваться аппаратом характеристических функций. Давайте найдем
[38:35.320 --> 38:45.960]  характеристическую функцию вектора кси. phi кси от t по определению, это математическое ожидание
[38:45.960 --> 39:02.480]  E в степени и сумма t житая кси житая, ж равно от единицы до k. Но у вектора кси матрица кавериации
[39:02.480 --> 39:12.200]  диагональная. Что отсюда следует для нормального вектора? Что его компоненты независимы. Поэтому вот
[39:12.200 --> 39:25.800]  это не что иное, как произведение характеристических функций, характеристических функций по определению E в
[39:25.800 --> 39:36.440]  степени и т житая кси житая, ж равно от единицы до k. Вот именно то свойство, что для нормального вектора
[39:36.440 --> 39:41.800]  из некоррелированности компонентов следует их независимость, позволяет нам так записать. Ну,
[39:41.800 --> 39:51.040]  а характеристическую функцию нормального распределения мы ведь знаем, да? Это будет произведение E в
[39:51.040 --> 40:04.080]  степени и т житая м житая. Я пока индекс и упущу, ну уберу, чтобы не писать тут кучу индексов. Умножить
[40:04.080 --> 40:18.840]  на е в степени минус сигма житая в квадрате t квадрат пополам. Правильно? Вот это вот
[40:18.840 --> 40:26.120]  характеристическая функция нормальной стандартной величины с дисперсией сигма житая им от ожиданий
[40:26.120 --> 40:39.400]  м житая. А это я перепишу в такой форме. Е в степени и т транспонированная м кси минус т транспонированная
[40:39.400 --> 40:57.680]  е т пополам. Правильно, да? Вот. Ну а теперь, как вы, наверное, догадываетесь, мы вернемся к
[40:57.680 --> 41:12.840]  характеристической функции вектора это. Как мы это сделаем? Да по определению мы напишем фи это в
[41:12.840 --> 41:24.080]  точке t, это математическое ожидание и транспонированное это, а вместо это мы напишем у транспонированной кси.
[41:24.080 --> 41:33.920]  Матрица артагональная, поэтому обратная равна транспонированной. Ну или чуть покрасивее е в
[41:33.920 --> 41:43.840]  степени и у т транспонированная на кси. И мы видим, что характеристическая функция случайной величины
[41:43.840 --> 41:51.120]  это, это характеристическая функция случайной величины кси, только взятая в точке у t. Ну и поставим это.
[41:51.200 --> 42:03.200]  Вот сюда подставляем. Е в степени и вместо т транспонированного пишем t транспонированная
[42:03.200 --> 42:12.200]  u-транспонированная mx-и минус t-транспонированная, это будет у нас
[42:12.200 --> 42:18.000]  t-транспонированная u-транспонированная
[42:18.600 --> 42:30.000]  sigma ut пополам равно e в степени i t-транспонированная
[42:30.000 --> 42:39.080]  u-транспонированная mx-и что такое матрица артагональна обратная равна
[42:39.080 --> 42:52.680]  диагонали у транспонированная mx-и это что такое это м это вектор ну минус а
[42:52.680 --> 43:00.680]  у транспонированная sigma u глядя на то что на это чему равна у транспонированная
[43:00.680 --> 43:20.560]  sigma r это вот ну совершенно вот наш ответ вот характеристическая функция
[43:20.560 --> 43:27.560]  невырожденного нормального распределения а теперь следующий шаг вы видите что в
[43:27.560 --> 43:33.600]  этой формуле нету r-1 и детерминанта нету это означает что мы можем себе
[43:33.600 --> 43:39.600]  позволить такое обобщение и учитывая наличие взаимооднозначного соответствия
[43:39.600 --> 43:44.280]  между множеством характеристических функций и функции распределения мы можем
[43:44.280 --> 43:49.120]  сказать так давайте нормальным вектором назовем любой который имеет вот такую
[43:49.120 --> 43:56.520]  характеристическую функцию если r это положительно определенное ну тогда это
[43:56.520 --> 44:01.000]  как бы наш обычно не вырожденный вектор а если не положительно определенное то это
[44:01.000 --> 44:04.920]  другой вектор но он тоже нормальный по нашему определение мы называем его
[44:04.920 --> 44:10.280]  нормально только вот такое определение называется обобщенный нормальный вектор
[44:10.280 --> 44:15.120]  который включает себя как вырожденный так и не вырожденное распределение
[44:15.120 --> 44:29.560]  ну поскольку эта матрица к вариации то она конечно всегда больше или равна нулю но есть
[44:29.560 --> 44:40.200]  случаи когда она равна нулю это соответствует тому если у вас ну понятно между между строками
[44:40.200 --> 44:45.280]  матрицы коллекционной матрицы есть там линейная связь да вот но тем не менее мы
[44:45.280 --> 44:52.160]  даем такое определение сюда укладываются и наши ну так сказать обычные не вырожденные
[44:52.160 --> 44:57.400]  нормальные распределения еще целый класс вырожденных нормальных распределений все это
[44:57.400 --> 45:03.760]  обобщенный нормальный вектор но теперь следующий шаг давайте введем линейное
[45:03.760 --> 45:14.560]  прообразование по правилу му равно f это только вот теперь f произвольная матрица ну например
[45:14.560 --> 45:21.960]  какая-нибудь вот такая к л ну у меня здесь л меньше кану это не обязательно или может
[45:21.960 --> 45:27.360]  и больше как главное что они могут быть неравны это может быть не квадратная матрица произвольная
[45:27.360 --> 45:36.160]  матрица вот ну и давайте найдем характеристическую функцию это но в нового вектора мю
[45:36.160 --> 45:45.560]  я здесь не буду писать тэ напишу другую букву например с почему ну потому что уже другая
[45:45.560 --> 45:53.400]  размерность видите у нас у той характеристической функции было к размерность там ну аргумента а
[45:53.400 --> 46:00.240]  здесь она после вот такого при на перемножение станет л поэтому я другую букву использую ну
[46:00.240 --> 46:10.360]  и по определению это математическое ожидание е в степени и с транспонированная мю правильно
[46:10.360 --> 46:24.560]  да ну дальше мне каса логика понятно вместо мю пишем f это и пишем что это математическое
[46:24.560 --> 46:38.200]  ожидание е в степени и f транспонированная с транспонированная это не трудно проверить
[46:38.200 --> 46:44.600]  что по размерности подходит то есть вот это f транспонированная с будет иметь размерность
[46:44.600 --> 46:52.760]  к как у нас исходный вектор имеет ну и получается что это на самом деле вот здесь напишу вот это
[46:52.760 --> 47:06.080]  получается равна f мю только взятая в точке f транспонированная с ну и давайте подставим так
[47:06.120 --> 47:08.480]  вот вот это позволить и стереть
[47:08.480 --> 47:36.160]  вот она на f
[47:38.480 --> 47:54.280]  так f мю точки с равно подставляем е в степени и вместо т транспонированного подставляем
[47:54.280 --> 48:09.160]  с транспонированная f это мю это извините это минус это все так сказать в показателе
[48:09.160 --> 48:27.760]  экспоненты и с транспонированная f и это f транспонированная с пополам ну и все те же
[48:27.760 --> 48:37.240]  правила что такое f на мат ожидания это это мат ожидания мю поэтому это есть е в степени и
[48:37.320 --> 48:48.200]  с транспонированная на мат ожидания мю минус f rn f с транспонированная на
[48:48.200 --> 49:00.600]  r мю уж какая получилась выразенная не выразена пополам то есть любое преобразование обобщенного
[49:00.600 --> 49:08.440]  нормального вектора остается в классе этих обобщенных нормальных векторов в зависимости
[49:08.440 --> 49:14.640]  от того как вы строили эти все преобразования вот это r мю может быть выраженной не выраженной
[49:14.640 --> 49:20.880]  матрицей ну например может иметь размерность 1 если вы все компоненты нормального случайного
[49:20.880 --> 49:27.360]  вектора не выраженного складываете у вас получается размерности 1 это р и будет не равно 0 то есть
[49:27.360 --> 49:32.480]  получится несмотря на то что образование преобразование сильно выраженная итоговый
[49:32.480 --> 49:38.000]  вектор получится ну не выраженной стандартной нормальной но тем не менее все эти преобразования
[49:38.000 --> 49:45.040]  укладывается в класс обобщенного нормального распределения который задается своей
[49:45.040 --> 49:53.920]  характеристической матрицей вот ну и помимо того что это такой красивый как мне кажется пример
[49:53.920 --> 50:03.520]  использования и и ну идеи или технологии характеристических функций это еще ну важно
[50:03.520 --> 50:09.320]  с практической точки зрения потому что нормальное распределение это но один из основных классов с
[50:09.320 --> 50:17.320]  которыми работают и теория вероятности и случайный процесс и математическая статистика так
[50:23.920 --> 50:34.200]  ну раз оно записывается матрица умножить на вектор это линейная вам как это так
[50:34.200 --> 50:42.480]  определение преобразования не но если определить преобразование сохраняется размерность тогда это
[50:42.480 --> 50:48.760]  не преобразование но я просто честно говоря как-то у меня так в голове не отложилось что слово
[50:48.760 --> 50:55.000]  преобразование только сохранением размерности но если так пусть пусть будет так так все тогда
[50:55.000 --> 51:01.200]  с этим заканчиваем и переходим ну по большому счету последней теме нашего курса
[51:01.200 --> 51:16.680]  который из-за нехватки времени будет немножко сжатый к сожалению но по крайней мере я надеюсь что
[51:16.840 --> 51:25.480]  все так сказать идеи мы сможем с вами усвоить и изложить значит следующий объект который мы
[51:25.480 --> 51:34.080]  будем изучать это случайные последовательности но в качестве вводной напомню значит у нас было
[51:34.080 --> 51:44.160]  отображение из-за мега в 1 мы это называли случайной величиной было отображение из-за мега в рк мы это
[51:44.160 --> 51:51.480]  называли случайный вектор размера стека а если омега отображается в эр бесконечности то мы
[51:51.480 --> 51:59.400]  называем случайной последовательностью то есть каждый омега ставится в соответствие счетное
[51:59.400 --> 52:08.640]  число функции вот таких случайных величин точнее говоря не просто функции случайных величин вот
[52:08.640 --> 52:15.000]  если есть такое отображение то мы имеем дело уже со случайными последовательности что про
[52:15.000 --> 52:25.120]  случайные последовательности для начала нам нужно знать ну давайте рассмотрим множество омега таких
[52:25.120 --> 52:44.040]  что инфинум кси к от омега меньше некого x но это вроде бы объединение множество типа омега такое
[52:44.040 --> 52:59.080]  что кси к от омега меньше x правильно да если инфинум меньше значит какой-то найдется при этом
[52:59.080 --> 53:07.120]  омега который меньше и если омега одному из этих принадлежит то значит и не смешь вот так вот что
[53:07.920 --> 53:16.120]  отсюда следует отсюда следует что инфинум кси катах это случайная величина измеримая функция
[53:16.120 --> 53:26.540]  совершенно аналогично супрэну там сильно аналогично супремум ксикаты это тоже измеримая функция то
[53:26.540 --> 53:36.320]  есть случайно и влечена отсюда следует что например инфинум по n супрэмум пока больше равно
[53:36.320 --> 53:49.880]  n ксикатова тоже измеримая величина это что такое это верхний предел ксикаты и нижний предел
[53:49.880 --> 53:56.880]  аналогично измеримая функция вот как бы с этими объектами вот в этом смысле можем работать мы не
[53:56.880 --> 54:03.280]  вводим здесь некий аналог функции распределения потому что это была бы функция со счетным
[54:03.280 --> 54:08.760]  числом измерений и там возникают серьезные теоретические проблемы разрешенные в свое время
[54:08.760 --> 54:15.280]  колмогоровым и которые лежат в основе определения случайного процесса этот объект который вы будете
[54:15.280 --> 54:23.920]  дальше изучать нам пока вот нужно знать что вот нам понадобится инфы мы супремум и верхние нижние
[54:23.920 --> 54:34.160]  пределы вот ну раз есть последовательности то следующее что возникает желание так сказать
[54:34.160 --> 54:39.320]  какую аналогию провести математическом анализе чем из последовательности делаем
[54:39.320 --> 54:48.760]  главным образом пределы ищем да ищем пределы но на числовой оси там как бы все просто да у
[54:48.760 --> 54:54.840]  нас определение предела простое но в основе этого определения лежит то что в off
[54:54.840 --> 55:02.300]  пределы чтоб то ни было да функции или число начиная с некоторые номера лежит ну все вся
[55:02.300 --> 55:06.860]  под последовательность начинает с этого номера хвост последовательности лежит но для этого надо
[55:06.860 --> 55:13.920]  определить расстояние что значит в окрестности в анализе это более менее ну не то что просто
[55:13.920 --> 55:20.640]  что просто abyssal однозначно, особенно никак не определишь, а вот вероятности здесь есть разные
[55:20.640 --> 55:27.860]  взгляды на эту текстатематику. И мы, вообще говоря, определяем четыре типа сходимости на
[55:27.860 --> 55:34.920]  случайных последовательностях. Вот я их здесь запишу. Первое, это сходимость по распределению.
[55:34.920 --> 55:44.040]  Кси n-ное сходится кси по распределению, обозначается это буквой D, distribution. Ну n
[55:44.040 --> 55:57.440]  стремится к бесконечности. Если f кси n-ное от x сходится к f кси от x слабо, то есть поточечно,
[55:57.440 --> 56:08.200]  слабо. Но, еще есть одно, еще одна оговорочка. Во всех точках непрерывности f кси предельной
[56:08.200 --> 56:13.640]  функции. То есть, если предельная функция у нас разрывная, это значит, что сходится, ну там,
[56:13.640 --> 56:19.160]  там к части, к дискретной, например, случайной величине, то в точках разрыва не обязательно
[56:19.160 --> 56:24.120]  поточенная сходимость. Но во всех точках непрерывности она должна быть. Если есть,
[56:24.120 --> 56:29.600]  то мы говорим, что кси n-ое сходится к си по распределению. И, кстати, когда я тут писал,
[56:29.600 --> 56:36.360]  вот уже стер, там, например, что х квадрат с n степенями свободы равно сумме х квадрат
[56:36.360 --> 56:42.320]  с одной степенями свободы, там, конечно, над знаком равно надо было писать букву D. То есть,
[56:42.320 --> 56:53.120]  равны в смысле распределения. Вот такой тип сходимости мы вводим первым. Вторым мы вводим
[56:53.120 --> 56:58.640]  такой тип сходимости. А, ну и почему эта сходимость понятна, потому что, ну вот это мера близости
[56:58.640 --> 57:04.840]  в пространстве не прерыве, в пространстве ограниченных, не убывающих функций. Значит,
[57:04.840 --> 57:21.120]  для любого епсилон больше нуля, а точнее говоря, сначала, что вводим, да, какую. Значит, кси n сходится к
[57:21.120 --> 57:31.760]  си по вероятности, буквой P, probability, естественно. Если для любого епсилон больше нуля, вероятность
[57:31.760 --> 57:40.080]  того, что кси n отклонится от кси по модулю больше, чем епсилон, стремится к нулю, при n стремяется к
[57:40.080 --> 57:48.160]  бесконечности. Но этот тип сходимости вам, в принципе, знаком. Как он называется в общем случае?
[57:48.160 --> 57:56.960]  Сходимость по мере. Ну, то есть, это обычная сходимость по мере, но поскольку у нас мера
[57:56.960 --> 58:02.440]  всегда вероятностная, то мы называем сходимостью по вероятности. А так это сходимость по мере.
[58:02.440 --> 58:13.200]  Третий тип сходимости, который мы вводим, кси n сходится к си порядка R и обозначается,
[58:13.200 --> 58:21.720]  вот тут R в скобочках берется. Кси n сходится к си порядка R, если математическое ожидание
[58:21.720 --> 58:36.880]  кси n-е минус кси по модулю в степени R стремится к нулю. Ну, можно, конечно, там в погоне за
[58:37.480 --> 58:44.680]  R любые рассматривать, но мы будем рассматривать, иметь в виду, всегда R больше равно единице и
[58:44.680 --> 58:53.280]  здесь есть два, так сказать, собственных имени. Если R равна единице, то такую сходимость называют
[58:53.280 --> 59:04.040]  сходимостью как в среднем, а если R равно 2, то называют сходимостью в средне квадратичном. Вот,
[59:04.040 --> 59:10.360]  значит, это третий тип сходимости. Ну и четвертый тип сходимости. Это сходимость почти
[59:10.360 --> 59:17.800]  наверно или, так сказать, в классической математике функциональном анализе аналог сходимости почти
[59:17.800 --> 59:28.400]  всюду. Но здесь она называется сходимостью Pn, почти наверно, или с вероятностью единица,
[59:28.400 --> 59:35.480]  это термины, так сказать, полные синонимы, сходимость почти наверно или с вероятностью
[59:35.480 --> 59:48.280]  единица. Есть ли вероятностная мера Омега таких, что кси n от Омега сходится кси от Омега, равна единица?
[59:48.280 --> 01:00:05.120]  Понятно определение? Ну немножко трудно оно дается обычно, поэтому еще два слова буквально. Ну вот
[01:00:05.120 --> 01:00:10.120]  что это такое? Вот вы берете некая Омега, как только вы зафиксировали, ваша последность
[01:00:10.120 --> 01:00:16.360]  случайно превращается в числовую, и она сходится вот к этому кси от Омега. Перебирайте все такие
[01:00:16.360 --> 01:00:21.640]  Омега, на которых выполнено то условие. Если их вероятность на меру единица, значит имеет
[01:00:21.640 --> 01:00:27.760]  место сходимость с вероятностью единицы. А если на каком-то значимом множестве Омега это не
[01:00:27.760 --> 01:00:37.800]  выполнено, значит нет сходимости с вероятностью единицы. Вот, значит, почему почти наверно,
[01:00:37.800 --> 01:00:44.160]  то есть для чего дополнительный термин не почти всюду, да? Почти всюду все-таки речь идет о
[01:00:44.160 --> 01:00:53.320]  конкретном, конкретной мере множества Лебеговой на отрезке, ну там на напрямой. А здесь может быть,
[01:00:53.320 --> 01:00:59.120]  в принципе, любая. Здесь как-нибудь там какая-нибудь одна точка, которая содержит в себе там значительную
[01:00:59.120 --> 01:01:05.240]  часть меры. С точки зрения сходимости, почти всюду это почти всюду, а с точки зрения теории вероятности,
[01:01:05.240 --> 01:01:11.640]  это не сходимость почти, наверное. Вот, ну так небольшие, так сказать, в терминах отличия. Вот,
[01:01:11.640 --> 01:01:20.120]  значит, эти типы сходимости неравнозначны, но заметно связаны. И я сейчас напишу, как они связаны.
[01:01:20.120 --> 01:01:27.280]  Значит, самый слабый тип сходимости, давайте я это напишу вот на этой доске, мы будем сюда
[01:01:27.280 --> 01:01:34.840]  обращаться. Хотя, так сказать, самый слабый тип сходимости, это сходимость по распределению,
[01:01:34.840 --> 01:01:53.560]  которую мы назвали 1, он следует из сходимости 2, то есть по вероятности. Сходимость 2 следует
[01:01:53.560 --> 01:02:09.280]  из сходимости 3, то есть порядка R. Сходимость 2 следует из сходимости 4, то есть почти, наверное,
[01:02:09.280 --> 01:02:24.840]  почти, наверное. И, собственно, все. Но есть еще ряд связей, которые как-то имеют место при
[01:02:24.840 --> 01:02:31.120]  некоторых условиях. Например, из сходимости по распределению следует сходимость почти,
[01:02:31.120 --> 01:02:37.560]  наверное, если xi, то есть предельная функция, предельная случайно влечена, это константа,
[01:02:37.560 --> 01:02:44.000]  то есть число вырожденное случайно влечено. Вот если предельная случайно влечена вырождена,
[01:02:44.000 --> 01:02:48.720]  например, ноль, то сходимость по вероятности, это же действие на сходимости почти, наверное.
[01:02:48.720 --> 01:03:01.440]  Сходимость почти, наверное, следует из сходимости по вероятности, если она,
[01:03:01.440 --> 01:03:09.480]  так называемая, быстрая. То есть для любого епсилон больше нуля не только вот эта вот вероятность
[01:03:09.480 --> 01:03:15.600]  стремится нулю, а этот ряд стремится к нулю. То есть этот ряд сходится, быстрая сходимость.
[01:03:15.600 --> 01:03:27.680]  Вероятность xi-n-xi больше епсилон, сумма по n меньше бесконечности. То есть если сходимость
[01:03:27.680 --> 01:03:34.320]  быстрая, то есть не просто вот эта вероятность стремится к нулю, она настолько быстро стремится
[01:03:34.320 --> 01:03:39.760]  к нулю, что этот ряд сходится, то тогда этого достаточно для сходимости почти, наверное.
[01:03:39.760 --> 01:03:50.720]  Аналогично и сходимости порядка r. Следует сходимость почти, наверное, тоже, если сходимость
[01:03:50.720 --> 01:03:59.320]  быстрая, то есть сумма математических ожиданий xi-n-xi в степени r меньше бесконечности.
[01:03:59.320 --> 01:04:18.400]  И наконец, со сходимости 2 следует сходимость порядка r, если вероятность того, что xi-n меньше
[01:04:18.400 --> 01:04:28.160]  или равно m равна единице для любого n. То есть с вероятностью единица все xi-n-ы ограничены. Вот
[01:04:28.200 --> 01:04:36.440]  если они все ограничены, то тогда сходимости по вероятности следует сходимость порядка r.
[01:04:36.440 --> 01:04:48.560]  Ну и еще имеет место, как бы к этому относится, теория Мариса, знакомая вам, которая гласит,
[01:04:48.560 --> 01:04:59.240]  вот здесь я, так сказать, напишу, что если xi-n-ая сходится к xi по вероятности, то бишь по мере,
[01:04:59.240 --> 01:05:05.800]  то отсюда следует, что существует подпоследовательность n-катая, такая,
[01:05:05.800 --> 01:05:13.720]  что xi и n-катая сходятся к xi в наших терминах почти, наверное, почти всюду с вероятностью
[01:05:14.200 --> 01:05:26.160]  Так, ну вот, значит, нам предстоит поработать с этой таблицей, так сказать, убедиться. Ну и давайте
[01:05:26.160 --> 01:05:34.800]  начнем, как говорится, дорогу осилит идущий. Ну давайте сначала из 2.1, то есть сходимости по
[01:05:34.800 --> 01:05:41.640]  вероятности следует сходимость по распределению. Ну здесь по-разному можно доказывать, но раз уж мы
[01:05:41.640 --> 01:05:47.800]  характеристические функции с вами изучили, давайте, значит, этим воспользуемся. Для того,
[01:05:47.800 --> 01:05:58.120]  чтобы доказать сходимость по распределению, извините, чтобы не забыть, существует эквивалентное
[01:05:58.120 --> 01:06:05.320]  определение сходимости по распределению, эквивалентное, которое выглядит так, для любой
[01:06:05.320 --> 01:06:24.960]  непрерывной, ограниченной функции там phi от x, математическое ожидание phi от xi n-е
[01:06:24.960 --> 01:06:32.920]  сходится к математическому ожиданию phi от xi. Если выполна такое свойство, то это часто берется
[01:06:32.920 --> 01:06:39.640]  за определение сходимости по распределению. Ну, понятное дело, что если речь о том же сходимости,
[01:06:39.640 --> 01:06:48.400]  то они эквивалентны, но вот это вот для понимания предметом мы доказывать это не будем, но имейте
[01:06:48.400 --> 01:06:57.360]  в виду. Итак, из 2.1 сходимость по распределению эквивалентна сходимости характеристических
[01:06:57.360 --> 01:07:04.400]  функций. Давайте для каждого t по точной сходимости. Давайте вот это phi xi n-е от t
[01:07:04.400 --> 01:07:15.680]  минус phi от t по модулю. Пишу сразу меньше или равно интеграла e в степени i t xi n от
[01:07:15.680 --> 01:07:33.320]  омега минус e в степени i t xi от омега по модулю p d омега. Равно. Разбиваю на два интеграла.
[01:07:33.320 --> 01:07:46.680]  Xi n от омега минус xi от омега меньше или равно epsilon плюс интеграл xi n от омега минус
[01:07:46.680 --> 01:07:56.760]  xi от омега больше epsilon. Ну и давайте с каждым из них отдельно разберемся. Если вот в этой области
[01:07:57.120 --> 01:08:08.400]  мы смотрим. Вот эта функция, разность, непрерывная функция. Поэтому для любого
[01:08:08.400 --> 01:08:17.080]  epsilon, если epsilon вот это мало, то и вот эта функция, модуль тоже достаточно мало. Обозначим
[01:08:17.440 --> 01:08:28.160]  вот этот интеграл превратится в некое дельта от epsilon на p d омега. Вот взятый вот по этой области.
[01:08:28.160 --> 01:08:35.960]  Ну понятно, что это меньше или равно дельта от epsilon, которая стремится к нулю, когда epsilon
[01:08:35.960 --> 01:08:44.040]  стремится к нулю. Понятно, да, с этим? Вторая область. Ну вот эта разность заведомо меньше
[01:08:44.040 --> 01:08:56.120]  двух. Поэтому вот этот вот интеграл меньше или равен два интеграла p d омега xi n минус
[01:08:56.120 --> 01:09:06.760]  xi по модулю больше и epsilon. А вот это что такое? Вероятность какого события? Это две вероятности
[01:09:06.760 --> 01:09:14.920]  того, что xi n минус xi по модулю больше и epsilon, правильно? А вот эта штука стремится к нулю,
[01:09:14.920 --> 01:09:21.040]  потому что у нас есть сходимость по мере, по вероятности. Ну всё, таким образом,
[01:09:21.040 --> 01:09:26.120]  всё это, так сказать, может быть сделано, сколько угодно, малым. И это означает сходимость
[01:09:26.120 --> 01:09:32.480]  характеристических функций и, как следствие, сходимость функций распределения. Опять же,
[01:09:32.520 --> 01:09:37.680]  во всех точках непрерывности предела. Вот мы, когда характеристические функции с вами смотрели,
[01:09:37.680 --> 01:09:41.560]  тоже на этот эффект обращали внимание. Так.
[01:09:41.560 --> 01:10:07.920]  Так, ну следующее, что нам малой кровью дастся, это из 3 в 2.
[01:10:07.920 --> 01:10:28.680]  3 в 2. Мы пишем вероятность того, что xi минус xi больше и epsilon равна вероятности того,
[01:10:28.680 --> 01:10:38.520]  что xi n минус xi в степени r больше и epsilon в степени r, а это меньше или равно, чем математическое
[01:10:38.520 --> 01:10:45.600]  ожидание xi n минус xi в степени r делить на epsilon в степени r неравенство Чебышева, Маркова.
[01:10:45.600 --> 01:10:53.760]  Или это уже Чебышева мы называли. Вот. Эта штука по условию стремится к нулю, значит и это стремится к нулю.
[01:10:53.760 --> 01:11:10.680]  Так. Следующее, что мы без особых проблем получим, это из 1 в 2, когда xi равно константе.
[01:11:10.680 --> 01:11:25.360]  Из 1 я вот тут в 2 так отрывистый чертой, что это не всегда. Нам по большому счету надо доказать,
[01:11:25.360 --> 01:11:33.280]  что при наличии сходимости по распределению вероятность того, что xi минус а, некое число,
[01:11:33.480 --> 01:11:41.680]  по модулю больше и epsilon будет стремиться к нулю. Но мы докажем обратное, рассмотрим вероятность
[01:11:41.680 --> 01:11:49.120]  события xi n минус а меньше или равно epsilon и докажем, что это к единице стремится.
[01:11:49.120 --> 01:12:01.920]  Значит, это равно у нас вероятности того, что xi n меньше или равно a плюс epsilon и больше или
[01:12:01.920 --> 01:12:13.520]  равно a минус epsilon. Это вероятность больше или равна вероятности xi n больше или равно
[01:12:13.520 --> 01:12:23.400]  а минус epsilon, но зато меньше, строго меньше, а плюс epsilon. Область с узелем вероятности
[01:12:23.400 --> 01:12:38.280]  меньше. А вот это уже f xi n в точке a плюс epsilon минус f от xi n в точке a минус epsilon.
[01:12:38.280 --> 01:12:49.440]  Поскольку xi n сходится к a по вероятности по распределению, а это значит, что функция
[01:12:49.440 --> 01:13:00.080]  распределения к xi n сходится вот к такой функции. Вот это a, вот функция распределения
[01:13:00.080 --> 01:13:09.040]  константа. A плюс epsilon это предел единицы, а a минус epsilon это ноль, то бишь равно единице.
[01:13:09.040 --> 01:13:18.480]  Но это означает, что вот эта вероятность стремится к единице, точнее говоря не равно,
[01:13:18.480 --> 01:13:36.720]  а стремится к единице. Понятно, да? Так, ну еще за 5 минут мы, наверное, успеем вот это доказать,
[01:13:36.720 --> 01:13:42.520]  что если ограниченные последовательности, то исходимости вероятности следует исходимость порядка
[01:13:42.520 --> 01:14:12.360]  r. Так, пишем. Математическое ожидание к xi n
[01:14:12.360 --> 01:14:21.000]  минус xi в степени r, что это такое по определению, это интеграл модуль к xi n от омега минус
[01:14:21.000 --> 01:14:36.240]  xi от омега в степени r pd омега равно. Разбиваю такой наш излюбленный прием, к xi n от омега
[01:14:36.240 --> 01:14:49.680]  минус к xi от омега меньше равно epsilon, плюс интеграл к xi n от омега минус к xi от омега больше
[01:14:49.680 --> 01:15:07.120]  epsilon. И здесь я напишу еще раз. Так, с этим все просто интегралом. Под интегральное выражение
[01:15:07.120 --> 01:15:13.720]  меньше epsilon, мера нормированная единицу, значит эта штука сколь угодно малая. Выбором epsilon.
[01:15:13.720 --> 01:15:24.760]  Смотрим вот сюда. Я напишу следующее. Меньше или равно, так, извините, немножко вот так
[01:15:24.760 --> 01:15:36.840]  коряло. Меньше или равно supremum по омега к xi n от омега минус к xi от омега в степени r на
[01:15:36.840 --> 01:15:47.240]  меру вот такого множества. А это ничто иное, как вероятность того, что к xi n минус к xi будет
[01:15:47.240 --> 01:15:58.320]  больше epsilon. Вот этой штуке supremum существует? Ну смотрите, все хn с вероятностью единицы меньше
[01:15:58.320 --> 01:16:11.960]  или равно m. Понятно. А вот это-то, про предел, что можно сказать? Это предел по вероятности. А здесь
[01:16:11.960 --> 01:16:18.240]  вот как раз удачно теоремы Рисова воспользоваться. Поскольку к xi n сходится к xi по вероятности,
[01:16:18.240 --> 01:16:26.800]  существует подпоследовательность единичной меры такая, что к xi nkt от омега минус к xi от омега
[01:16:26.800 --> 01:16:36.680]  ну там, точнее говоря, к xi nkt от омега стремится к xi от омега и вероятностная мера равна единице.
[01:16:36.680 --> 01:16:43.480]  Вот для любого из этих омега, которые здесь вероятностная мера единица, это же числовая
[01:16:43.480 --> 01:16:49.280]  последовательность все члены, которые меньше или равно m. Это предел. Поэтому на множестве единичной
[01:16:49.280 --> 01:16:56.000]  меры для каждого из этих омега к xi от омега тоже меньше m. Поэтому вот эта вся штука просто меньше
[01:16:56.000 --> 01:17:05.800]  2m справедливости ради в степени r. На вот эту вероятность, вот эта вероятность стремится к нулю,
[01:17:05.800 --> 01:17:11.200]  поскольку мы исходим из того, что имеет место сходимость по вероятности. Вот эта штука ограничена.
[01:17:11.200 --> 01:17:17.600]  Все, мы как бы доказали то, чего хотели. Значит, давайте я вот здесь то, что мы заказали,
[01:17:18.560 --> 01:17:31.160]  зачеркну, чтобы в следующий раз можно было вернуться. А то, что осталось доказать,
[01:17:31.160 --> 01:17:37.480]  нам для этого потребуется критерий сходимости почти на верное. И если у меня есть минута,
[01:17:37.480 --> 01:17:43.720]  то я его запишу просто, чтобы мы с этого начали в следующий раз.
[01:17:47.600 --> 01:18:00.920]  Значит, для того, чтобы последовательность xn сходилась к xi почти на верное, необходимо
[01:18:00.920 --> 01:18:12.400]  и достаточно, чтобы вероятность вот такого события, supremum, xi kt – xi, supremum берется
[01:18:12.400 --> 01:18:21.840]  пока больше или равно некого n. Больше epsilon стремился к нулю при n, стремящемся к бесконечности.
[01:18:21.840 --> 01:18:32.400]  Может, он нам даже знаком, да? Поскольку тут существует разница в мере. Быстро пробежимся,
[01:18:32.400 --> 01:18:37.080]  там не такое сложное доказательство. Все, значит, в следующий раз начинаем с этого,
[01:18:37.080 --> 01:18:42.360]  и тогда, значит, закончим вот эту табличку. Спасибо.

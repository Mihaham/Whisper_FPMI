[00:00.000 --> 00:18.200]  Так, коллеги, напомню, значит, на чем мы остановились в прошлый раз. Мы остановились на теореме
[00:18.200 --> 00:25.360]  Линберга-Леве. Вот я написал в двух формулировках, чтобы была перед глазами в терминах серии
[00:25.360 --> 00:34.520]  случайных величин. Вот условия Линберга, следствие, ну, я одно написал, асимпатическая
[00:34.520 --> 00:42.440]  нормальность. И вот я написал, как я это назвал, в ретро-форме, как она, собственно, была предложена
[00:42.440 --> 00:49.640]  изначально Линбергам. Ну, следствие тоже. Просто немножко по-другому записывается та величина,
[00:49.640 --> 00:56.600]  которая симпатически стремится к N01. Это сумма ксикатых минус акатых. Акаты – это мы от ожидания.
[00:56.600 --> 01:05.920]  Делить на BN. BN в квадрате – это сумма дисперсии. Вот это мы сами в прошлый раз установили. Но тут же
[01:05.920 --> 01:15.240]  нам, можно сказать, таким призом бесплатным достается следующий факт. Значит, как мы знаем,
[01:15.240 --> 01:22.400]  сходимость по распределению – это поточечная сходимость во всех точках непрерывности предельной
[01:22.400 --> 01:29.080]  функции. Поскольку функция распределения N01 непрерывна, это значит, что в данном случае
[01:29.080 --> 01:37.640]  сходимость по распределению означает сходимость поточечна во всех точках. Причем, обращаю ваше
[01:37.640 --> 01:51.640]  внимание, что сходимость у нас происходит к функции, которая ограничена и монотонно убывающая.
[01:51.640 --> 02:00.600]  Вообще говоря, это означает, что сходимость у нас будет равномерная. То есть мы получаем равномерную
[02:00.600 --> 02:07.600]  сходимость к функции распределения N01 в функции распределения вот этой вот случайной величины.
[02:07.600 --> 02:14.920]  Функция распределения этой случайной величины при любом N, это же функция распределения, она тоже
[02:14.920 --> 02:21.440]  ограничена и не убывающая. Последительность ограниченных не убывающих функций, сходящихся поточечно,
[02:21.440 --> 02:29.520]  сходится в равномерной метрике. Вот этот факт мы дополнительно получаем и еще сегодня про него
[02:29.520 --> 02:38.160]  вспомним. Но, кстати, то же самое верно и на всюду плотном множестве. Если у нас на всюду плотном
[02:38.160 --> 02:46.160]  множестве имеет место поточная сходимость, то у нас и равномерная сходимость тоже имеет место на этом
[02:46.160 --> 02:53.360]  же всюду плотном множестве. Если речь идет о сходимости не убывающих, еще раз повторю, ограниченных
[02:53.360 --> 02:58.720]  функций, потому что в противном случае это не так. Контр-примеры в мотоналисе существуют.
[02:58.720 --> 03:06.560]  Вот, ну теперь давайте несколько важных частных случаев. Значит, первый, можно сказать,
[03:06.560 --> 03:12.520]  хрестоматийный частный случай из теоремы Линберга-Леви, это так называемая центральная
[03:12.520 --> 03:23.720]  предельная теорема. ЦПТ сокращенно. Центральная предельная теорема. Значит, пусть у нас есть
[03:23.720 --> 03:30.720]  последовательность ксиенная независимых, но теперь одинаково распределенных случайных величин,
[03:30.720 --> 03:38.640]  таких что дисперсия кси с одной стороны меньше бесконечности, но с другой стороны строго больше
[03:38.640 --> 03:52.560]  нуля. Тогда асимпатическая нормальность, то есть выполнение вот этого свойства. Асимпатическая
[03:52.560 --> 04:00.520]  нормальность. Вот, значит, мы с вами уже с частным случаем этой теоремы в курсе сталкивались. Напомните
[04:00.520 --> 04:07.320]  мне, пожалуйста, что за теорема у нас была про асимпатическую нормальность еще достаточно
[04:07.320 --> 04:17.680]  там несколько лекций назад. Давайте как-то общую картину формируем. Какую мы теорему с вами доказывали?
[04:17.680 --> 04:26.120]  Муаврова пласа. Она относилась к асимпатической нормальности последствия независимых одинаково
[04:26.120 --> 04:33.360]  распределенных случайных величин, но конкретного распределения. Ксиенная имели какое распределение?
[04:33.360 --> 04:48.460]  Бернули. Вот. Вот как бы более-более общий случай. Его, кстати, тоже можно доказать с использованием
[04:48.460 --> 04:53.640]  аппарата характеристических функций, как мы доказывали с вами теорему Мауэрла пласа. Ну,
[04:53.640 --> 04:59.960]  уж раз у нас есть условия Линдберга, ну давайте просто его проверим для вот такой ситуации.
[04:59.960 --> 05:09.920]  Воспользуюсь, так сказать, ретроформой вот этой. Так, значит, для независимых одинаково
[05:09.920 --> 05:17.760]  распределенных случайных величин bn в квадрате это n на дисперсию Сигма. Ну, а мат ожиданий у
[05:17.760 --> 05:27.200]  не всех одинаковые равно к там некому а, да? Ну, давайте вот перепишем bn в квадрате n дисперсия
[05:27.200 --> 05:40.200]  кси. Здесь все будут интегралы одинаковые, то есть это n штук. Вот таких интегралов х минус а,
[05:40.200 --> 05:49.440]  нет индекса, потому что одинаковые мат ожиданий у всех в квадрате. На d f кси от x. Тут индекс
[05:49.440 --> 05:54.520]  тоже пропадает, потому что они все одинаково распределены. И область интегрирования у них
[05:54.520 --> 06:07.600]  у всех одинаково. Ну, смотрите, n сокращается, остается вот такой интеграл, что мы про него
[06:07.600 --> 06:15.440]  знаем. Что если у него бы были бы пределы интегрирования от минус бесконечности до бесконечности,
[06:15.440 --> 06:25.920]  то чему бы равнялся интеграл? Что это такое по определению? А? Дисперсия. Случайно увлечена
[06:25.920 --> 06:31.840]  минус мат ожидания в квадрате и ее мат ожиданий. Это была бы дисперсия, которая по условию конечна.
[06:31.840 --> 06:40.400]  Поэтому если мы интегрируем ее на хвости, как так сказать, да, при bn. А, извините,
[06:40.400 --> 06:51.280]  здесь уже можно написать явно, прошу прощения, для большей ясности это будет корень из n, корень
[06:51.280 --> 06:59.440]  из дисперсии кси. Подставлю сразу. Ну и тут видно, что когда для любого tau больше 0a стремится к
[06:59.440 --> 07:05.920]  бесконечности, интеграл остается только вот хвостиков. Сам полностью интеграл конечен,
[07:05.920 --> 07:10.840]  потому что равен дисперсии, значит, стремится к нулю. Париан стремяющийся к бесконечности.
[07:10.840 --> 07:19.840]  И мы получаем, что в частном случае независимых одинаково распределенных случайных величин
[07:19.840 --> 07:27.120]  выправлено условия Линдберга, следовательно, имеет место асимпатическая нормальность. Вот это,
[07:27.120 --> 07:33.960]  пожалуй, самая известная теорема, связанная с асимпатической нормальностью. Центральная
[07:33.960 --> 07:41.400]  предельная теорема. Она ничего ими не носит, поскольку разными, независимо, в разное время,
[07:41.400 --> 07:47.520]  разными способами доказывалась как некий фундаментальный факт. Вот. Но факт действительно,
[07:47.520 --> 07:54.840]  во-первых, фундаментальный, а во-вторых, естественно, научный. Я думаю, что, по крайней мере,
[07:54.840 --> 08:01.760]  когда я был студентом, нам на первом курсе по физике показывали такой опыт. Такая доска с
[08:01.760 --> 08:09.880]  гвоздями, закрытой стеклом, и туда сыплются шарики. Вам показывают такой опыт? И получается кривая
[08:09.880 --> 08:16.760]  гауса, нормальное распределение. Это, собственно, демонстрация того, что сумма большого числа
[08:16.760 --> 08:23.400]  случайных возмущений, каждая из которых небольшая, приводит к асимпатической нормальности. Есть,
[08:23.400 --> 08:31.720]  как бы, более поэтичные примеры. Вот если взять какую-нибудь средневековую Европу, такую глубокую,
[08:31.720 --> 08:43.520]  где есть какие-нибудь мраморные или гранитные ступеньки, то вот если на них посмотреть,
[08:43.520 --> 08:54.080]  то они стерты вот таким образом. Вот это тоже кривая гауса. Сотни лет терли, отклонение всегда
[08:54.080 --> 09:01.720]  как бы случайное, небольшое, накладывалось. Вот получилась кривая гауса. То есть асимпатическая
[09:01.720 --> 09:09.320]  нормальность, я еще раз хочу сказать, это не абстрактный математический факт, это объективный
[09:09.320 --> 09:19.800]  закон, проявляющийся в природе, как любой другой объективный закон. Там, условно говоря, при
[09:19.800 --> 09:29.120]  увеличении температуры, газ расширяется. Так и вот это вот. Если вы будете накапливать независимые
[09:29.120 --> 09:35.560]  факторы, каждая из которых не играет решающей роли, это именно условия пренебрежимой малости,
[09:35.680 --> 09:42.640]  то вы получите в результате нормальное распределение. Броновское движение является тоже таким классическим
[09:42.640 --> 09:48.760]  примером нормального распределения, точнее говоря, асимпатически нормального распределения.
[09:48.760 --> 09:58.640]  Вот, значит, центральная предельная теорема. Еще одно важное и полезное следствие, то есть это
[09:58.640 --> 10:06.920]  достаточно условия асимпатической нормальности. Еще одно важное, с практической точки зрения важное,
[10:06.920 --> 10:13.000]  с теоретической точки зрения важное, достаточно условие. Это так называемое условие Липунова.
[10:13.000 --> 10:30.800]  Условие Липунова. Значит, достаточно условия в форме Липунова. Пусть у нас есть такое число,
[10:30.800 --> 10:41.400]  дельта больше нуля, со следующими свойствами. Сумма математических ожиданий кси КТ минус а КТ в
[10:41.400 --> 10:53.240]  степени 2 плюс дельта, К от единицы до n, делить на bn в степени 2 плюс дельта, стремится к нулю,
[10:53.240 --> 11:01.560]  прям стремяющих к бесконечности. Вот, предположим, мы нашли такое дельта, то тогда асимпатическая
[11:01.560 --> 11:10.920]  нормальность имеет место. Ну, не сказал явно, но здесь укажу. Ксиенная, как обычно, независимые
[11:10.920 --> 11:17.080]  случайные величины. Независимые случайные величины. Ну и подразумевается, конечно,
[11:17.080 --> 11:23.520]  что моменты вот этого порядка абсолютно и существуют. Давайте это условие докажем,
[11:23.520 --> 11:31.600]  тоже опираясь на условия Линдберга, как я говорю, в ретрофор. Давайте его собственно запишем,
[11:31.600 --> 11:34.760]  еще раз перепишем, чтобы удобно было.
[11:53.520 --> 12:08.760]  И давайте, ой, извините, что это меня на верхние индексы потянуло. И давайте обратим внимание на
[12:08.760 --> 12:17.880]  то, что в области интегрирования х минус а КТ по модулю делить на тау бн больше единицы. Поэтому
[12:17.880 --> 12:26.920]  я могу смело написать, что это меньше или равно единица на бн в квадрате, сумма КТ единица до
[12:26.920 --> 12:41.920]  н интеграл. Здесь напишу х минус а КТ по модулю в степени 2 плюс дельта, а здесь напишу тау в
[12:41.920 --> 12:56.160]  степени дельта, бн в степени дельта. Понятно, да? Здесь сносочку х минус а КТ по модулю делить на
[12:56.160 --> 13:08.880]  тау бн в степени дельта, которая больше нуля, больше единицы, ну может равно. Вот так вот, да? Равно,
[13:08.880 --> 13:22.600]  а область интегрирования здесь важна, она у нас еще пока вот такая. Так, ну напишу,
[13:22.600 --> 13:34.600]  меньше или равно, меньше или равно. Под интегральное что-то теряю. Под интегральное
[13:34.600 --> 13:42.200]  выражение не отрицательное, поэтому если мы расширим область интегрирования до всей оси,
[13:42.200 --> 13:49.080]  интеграл только увеличится, поэтому меньше или равно. Тут сразу сделаю такие преобразования.
[13:49.080 --> 14:01.720]  А здесь останется сумма интегралов х минус а КТ в степени 2 плюс дельта по модулю по всей
[14:01.720 --> 14:20.400]  области df КТ от х, КТ единицы dn. Правильно, да? Вот это что по определению? Ну вот же оно.
[14:20.400 --> 14:30.480]  Правильно? Математическое ожидание кси КТ минус а КТ в степени 2 плюс дельта. Ну и поэтому
[14:30.480 --> 14:38.480]  пишу единица на тау в степени дельта, которая ни на что не влияет, и, собственно, условия Липунова.
[14:48.480 --> 14:58.200]  Если вот это стремится к нулю, то выполнено условие Линдберга и, следовательно,
[14:58.200 --> 15:07.000]  имеет место симпатичная нормальность. Значит, первое, зачем мы про это помним, так сказать,
[15:07.000 --> 15:16.120]  и еще такое именное имеет значение. С точки зрения практических вычислений, вычисления вот этой дроби
[15:16.120 --> 15:23.840]  Липунова, так называемой, оказывается проще, чем проверка условия Линдберга. Это техническая
[15:23.840 --> 15:32.280]  составляющая, но есть еще глубокая теоретическая составляющая. Вот для случая дельта равно одному,
[15:32.280 --> 15:43.760]  вот эту, собственно, вот это называют дробью Липунова, которая имеет просто вид такой ксикатая
[15:43.760 --> 15:56.960]  минус аккатая в третий, а в знаменателе будет сумма дисперсий ксикатых в степени 3 вторых.
[15:56.960 --> 16:05.720]  Ну вот, если вот это стремится к нулю, то, во-первых, асимпатическая нормальность,
[16:05.720 --> 16:18.120]  вот эту дробь Липунова, прям у нее специально есть обозначение L3. Зачем оно нужно? Оказывается,
[16:18.120 --> 16:29.640]  что имеет место вот такое довольно сильное неравенство, что в равномерной метрике f это
[16:29.640 --> 16:46.520]  n от x отклонится от f 0.1 от x supremo по x на величину, не превышающую некую константу на L3.
[16:46.520 --> 16:57.760]  Ну, когда у нас n увеличивается, мы знаем, что имеет место поточная сходимость. Более того,
[16:58.560 --> 17:04.160]  мы знаем, что имеет место равномерная сходимость, то есть сходимость такой метрики имеет место.
[17:04.160 --> 17:13.520]  Но какова скорость этой сходимости? Вот результат. Кстати, константа c меньше единицы, типа 0,8.
[17:13.520 --> 17:26.280]  Она тоже вычислена. Ну и дальше вот какое замечание. По странному стечению обстоятельств,
[17:26.280 --> 17:33.360]  вот этот результат Липунова как-то не очень популярен, в учебниках не поминается, в том числе
[17:33.360 --> 17:47.400]  и как бы таких больших. Но его частный случай, когда x независимо одинаково распределенные
[17:47.400 --> 17:56.800]  случайные величины, очень даже популярен в этом случае, как нетрудно убедиться. L3 равно,
[17:56.800 --> 18:04.360]  принимает вид. Все одинаково распределены, значит здесь n штук математических ожиданий,
[18:04.360 --> 18:13.840]  кси центрированных по модулю в кубе. Дисперсии тоже все равны, это значит n корень из n,
[18:13.840 --> 18:22.520]  дисперсия кси в степени 3 вторых. Вот этот n сокращается, ну и остается, собственно,
[18:22.520 --> 18:29.240]  вот такая величина со скоростью единицы на корень из n, стремяющаяся к нулю. Это когда кси n
[18:29.240 --> 18:34.480]  независимо одинаково распределенные величины. И вот неравенство для независимо одинаково
[18:34.480 --> 18:42.240]  распределенных величин, вот с такой величиной L3, называется неравенством Берри Эссона.
[18:50.240 --> 18:58.040]  Берри Эссона, хотя является частным случаем неравенства Липунова и появилось лет на 50 позже.
[18:58.040 --> 19:05.520]  Ну, тем не менее, вот это в большинстве учебников есть, а вот собственно исходника в большинстве
[19:05.520 --> 19:13.120]  учебников нет. Ну, тем не менее, это такое лирическое отступление. Условие Липунова
[19:13.120 --> 19:21.680]  для симпатической нормальности выглядит вот так. Практически используется в большинстве случаев,
[19:21.680 --> 19:29.280]  когда дельта равна единице, и тогда выглядит вот так. Ну и скорость вот этой равномерной исходимости,
[19:29.280 --> 19:36.560]  которая так здесь имеет место, вот она имеет порядок единицы на корень из n для независимых
[19:36.560 --> 19:51.320]  одинаково распределенных случайных величин. Пошли дальше. Наверное, я теперь тут почти
[19:51.320 --> 20:19.960]  все могу стереть. Ну, здесь несколько слов надо сказать. Значит, смотрите,
[20:20.080 --> 20:25.800]  мы с вами установили, получили достаточно фундаментальные законы симпатической нормальности
[20:25.800 --> 20:33.280]  для серий случайных величин, но еще на прошлой лекции мы с вами получали другое предельное
[20:33.280 --> 20:42.160]  распределение. Какое? Теорема у нас была соответствующая, ненормальная, так сказать.
[20:42.160 --> 20:51.560]  Какое было у нас предельное распределение? Пуассона. То есть мы с вами доказали сходимость
[20:51.560 --> 21:01.000]  неких серий к распределению пуассона. Ну, теперь возникает вопрос. В распределении пуассона мы
[21:01.000 --> 21:08.680]  тоже брали серию Бернульских случайных величин. С одной стороны, Бернульские случайные величины
[21:08.680 --> 21:14.840]  асимпатически нормальные. Это теорема маврового пласа или, так сказать, центральная предельная
[21:14.840 --> 21:21.400]  теорема. С другой стороны, так сказать, те же Бернульские случайные величины асимпатически
[21:21.400 --> 21:36.520]  имеют распределение пуассона. В чем дело? Как так? Весь вопрос в том, как мы переходили
[21:36.520 --> 21:45.240]  к пределу. Для асимпатической нормальности характерная форма вот. Вы берете, например,
[21:45.240 --> 21:52.680]  одинаково распределенные случайные величины и определенным образом их нормируете. А в распределении
[21:52.680 --> 21:59.080]  пуассона, если вы посмотрите по условию, у нас с ростом n менялось распределение вот этих секатах.
[21:59.080 --> 22:06.200]  Вероятность успеха становилась еще меньше и меньше таким образом, что n умножить на pn
[22:06.200 --> 22:12.720]  стремилась к лямдо. Разные предельные переходы получились разное распределение. То есть никакого
[22:12.720 --> 22:24.080]  здесь противоречия или парадокса нет. Причем надо сказать, что и нормальное распределение,
[22:24.080 --> 22:33.560]  распределение пуассона это довольно часто встречающиеся в жизни. Про примеры
[22:33.600 --> 22:39.320]  нормального распределения я вам сказал, а про распределение пуассона приведете мне
[22:39.320 --> 22:47.600]  какой-нибудь пример. Какой физический эксперимент, ну там мы описываем, мы это хорошо теоретически
[22:47.600 --> 22:51.600]  подтверждается, описывается распределением пуассона. Знаете какой-нибудь?
[22:51.600 --> 23:06.960]  Длина свободного пробега имеет показательное распределение, экспоненциальное. А пуассонское
[23:06.960 --> 23:13.440]  распределение имеет количество частиц, зарегистрированный счетчиком Гейгера. Это
[23:13.440 --> 23:20.920]  вот классический, так сказать, физический процесс, который и описывается моделью пуассонское
[23:20.920 --> 23:26.840]  распределение и очень хорошо и соответствует. И, грубо говоря, имеется теоретические основания,
[23:26.840 --> 23:34.400]  чтобы это распределение было именно такое. Вот так, значит сейчас я вернусь немножко назад, за мной
[23:34.400 --> 23:41.040]  был должок вторая теорема Колмогорова усиленного закона больших чисел. Мы это сделаем, потому что
[23:41.040 --> 23:46.800]  он нам сегодня понадобится для дальнейшего. Итак, значит вторая теорема Колмогорова.
[23:46.800 --> 24:04.800]  Напоминаю, значит, если ксиен средняя, значит ксиенная теперь это независимо одинаково
[24:04.800 --> 24:10.600]  распределенные случайные величины. Во второй теореме Колмогоров, в первой теореме это просто
[24:10.600 --> 24:15.520]  независимые случайные величины, во второй это независимо одинаково распределенные. Значит,
[24:15.520 --> 24:26.000]  если ксиенная сходится к А почти на верное или для того, чтобы необходимой достаточно существование
[24:26.000 --> 24:35.400]  математического ожидания кси равного А. Если среднее значение к чему-то сходится, то это и есть
[24:35.400 --> 24:40.240]  мат ожидания, но, разумеется, независимо одинаково распределенных случайных величин. И если у вас
[24:40.240 --> 24:47.680]  есть мат ожидания, то ксиенная средняя сходится к нему. Вот, собственно, утверждение о теореме
[24:47.680 --> 24:57.800]  Колмогорова. Есть более слабый, как аналог этой теоремы, относящийся к другому типу сходимости и
[24:57.800 --> 25:07.680]  только в одну сторону достаточно условия. Что это за теорема? Коллеги, я вас спрашиваю, чтобы как-то
[25:07.680 --> 25:15.440]  побудить у вас некую целостную картину сформировать. Это была теорема Хинчина,
[25:15.440 --> 25:23.280]  которая утверждала, что для системы независимо одинаково распределенных случайных величин
[25:23.280 --> 25:29.440]  существование мат ожидания является достаточным условием выполнения закона больших чисел,
[25:29.440 --> 25:37.440]  не усиленного, а просто закона больших чисел. Надо сказать, что теорема Хинчина, частный случай
[25:37.440 --> 25:44.680]  теоремы Колмогорова, в том числе и потому, что из сходимости почти, наверное, следует сходимость
[25:44.680 --> 25:52.960]  по вероятности. Но, как сказать, она нам сегодня сослужит некую добрую службу, упростит некие
[25:52.960 --> 26:01.600]  наши рассуждения, поэтому вспомним про нее. Итак, ну давайте-то, кстати, начнем. Так, первое,
[26:01.600 --> 26:12.760]  существует математическое ожидание кси меньше бесконечности. Первое утверждение, которое я отсюда
[26:12.760 --> 26:19.360]  делаю или напоминаю вам, что и мат ожидания модуль кси существует и меньше бесконечности,
[26:19.360 --> 26:25.640]  правильно? Поскольку в интеграле ли бега, модуль и сама функция одновременно либо интегрируемы,
[26:25.640 --> 26:32.480]  либо не интегрируемы. То есть, это мы уже начали доказательств, это не условие, это я напомнил. Так,
[26:32.480 --> 26:42.160]  ну и вот, собственно, теперь можно сказать ключевая идея доказательства, принадлежащая Колмогорову и
[26:42.160 --> 26:49.560]  введенному им понятию усеченных случайных величин. Ведем случайные величины ксиенная со
[26:49.560 --> 26:58.480]  звездочкой по следующему правилу. Ксиенная со звездочкой в точности равно ксиен, если
[26:58.480 --> 27:13.680]  ксиен по модулю меньше n и нулю иначе. Ну, то есть, ксиенная по модулю больше равно n.
[27:13.680 --> 27:29.520]  Вот, вот как-то ключевая идея доказательства. Что мы сейчас сделаем? Мы сейчас с вами докажем,
[27:29.520 --> 27:35.880]  опираясь на первую теорему Колмогорова, что вот для таких случайных величин выполнен усиленный
[27:35.880 --> 27:42.520]  закон больших чисел. Обращаю ваше внимание, что они уже не одинаково распределены, вот эти случайные
[27:42.520 --> 27:56.280]  величины, но остались независимы, разумеется. Так, для этого давайте возьмем дисперсию кси n со
[27:56.280 --> 28:02.520]  звездочкой. Эта дисперсия, конечно, меньше от ожидания ксиен со звездочкой в квадрате,
[28:02.520 --> 28:18.840]  а этом от ожидания есть интеграл от x квадрат df кси от x, взятый по области x по модулю меньше n.
[28:18.840 --> 28:32.280]  Правильно? Ну вот здесь вот прием, которым мы будем сегодня неоднократно пользоваться,
[28:32.280 --> 28:40.200]  мы разобьем этот интеграл на сумму интегралов такого вида x по модулю меньше k плюс 1,
[28:40.200 --> 28:54.760]  больше или равно k, k от 0 до n минус 1, x квадрат df кси от x. Понятно, почему здесь n пропало,
[28:54.760 --> 29:08.440]  кстати? Здесь было, здесь не стало. Ну понятно, значит хорошо. Так, разобьем на такую сумму и вот
[29:08.440 --> 29:20.880]  дальше напишем следующее. Меньше или равно k равно 0, n минус 1, k плюс 1 в квадрате по свойствам
[29:20.880 --> 29:37.000]  интеграла Либега на df кси от x, x меньше k плюс 1, больше или равно k. Вот этот интеграл,
[29:37.000 --> 29:43.440]  вот этот интеграл, это же на самом деле вероятность того, что случайная уличина
[29:43.440 --> 29:56.760]  кси меньше k плюс 1, больше равно k. Правильно, да? Нам сегодня много раз надо будет с этим,
[29:56.760 --> 30:02.160]  как бы, такими интегралами работать в разных эпостасиях, поэтому я введу для него короткое
[30:02.160 --> 30:11.320]  обозначение. Вот так вот k, k плюс 1. Слева закрыт интервал, справа открыт. Видите, да? То есть
[30:11.320 --> 30:17.500]  вот этот интеграл, вероятность такого события, я буду просто вот так обозначать. Это чисто
[30:17.500 --> 30:24.040]  обозначение, чтобы чуть-чуть сократить записи. Ну и таким образом просто перепишу, чего у нас
[30:24.040 --> 30:32.600]  получилось. У нас получилось, что дисперсия ксиенного созвездочка меньше или равна сумме k от 0
[30:32.600 --> 30:50.640]  до n минус 1, k плюс 1 в квадрате, k, k плюс 1. Вот. Так, перехожу сюда.
[31:02.600 --> 31:28.240]  Теперь рассмотрим вот такую сумму. Дисперсия кси n созвездочкой делить на n квадрат.
[31:28.240 --> 31:49.240]  И что такое? К чему это мне? Это от чего условия? Ну вот такой ряд появляется. Где? Ой, коллеги,
[31:49.240 --> 31:55.560]  что-то у вас как-то как стирается все к следующей лекции. Это первая тярема Колмогорова. Если такой
[31:55.560 --> 32:03.720]  ряд сходится, то последность ксиенная почти, наверное, сходится. Для него выполнен усиленный
[32:03.720 --> 32:14.960]  закон больших чисел. Ну, пишу, меньше или равно единица на n квадрат, а вместо дисперсии ксиенного
[32:14.960 --> 32:31.120]  вот напишут эту штуку. k равен 0 до n минус 1, k плюс 1 в квадрате, k, k плюс 1. Вот. Значит,
[32:31.120 --> 32:38.040]  вот получили некий ряд. Все его члены не отрицательные. То есть, если он сходится,
[32:38.040 --> 32:43.600]  то он сходится абсолютно. Если сходится, если ряд сходится абсолютно, то в нем можно как угодно
[32:43.880 --> 32:49.320]  переставлять члены. От этого сумма ряда не изменяется. Ну и таким образом, если после
[32:49.320 --> 32:54.440]  перестановки членов мы получим сходящийся ряд, это как раз и будет означать, что вот этот ряд
[32:54.440 --> 33:03.640]  тоже сходится. После произвольной как нам удобно перестановки членов. Значит, давайте я вот здесь
[33:03.640 --> 33:11.720]  вот чуть подробнее напишу, чтобы как бы так не в голове все прокручивать такие крокодилы. Значит,
[33:11.720 --> 33:20.720]  n равно 1. Давайте посмотрим, что получается. Получается 1 на 1 в квадрате. Когда n равно 1,
[33:20.720 --> 33:35.040]  k принимает только одно значение 0. Значит, это будет 0,1. Теперь, если n равно 2, то будет 1 на 2 в
[33:35.040 --> 33:44.720]  квадрате. Первая слагаемая вот в этой сумме тоже будет, ну так напишу здесь, 1 в квадрате. Там
[33:44.720 --> 33:53.160]  все-таки k плюс 1 в квадрате. Значит, 1 в квадрате 0,1 плюс, когда n равно 2,
[33:53.160 --> 34:05.240]  еще член появляется 1, это значит будет 2 в квадрате на 1, 2. Вот так. И так далее.
[34:05.240 --> 34:16.120]  Ну, теперь хочу вот так перепаковать. Давайте я сейчас запишу и мы посмотрим, прав ли я.
[34:16.120 --> 34:40.760]  Сумма k равно от 0 до бесконечности, k плюс 1 в квадрате k, k плюс 1 на сумму 1 на n квадрат,
[34:40.760 --> 34:50.240]  вот тут главное правильно начать. n равно k плюс 1 до бесконечности. Посмотрите, пожалуйста,
[34:50.240 --> 35:03.640]  правильно, да? Сделал я все. Вот этот ряд вот так перепаковал. Здесь начнется 1 в квадрате 0,1 и
[35:03.640 --> 35:14.160]  квадраты, обратные квадраты начиная с единицы. Следующий ряд начнется 2 в квадрате на 1, 2 и
[35:14.160 --> 35:23.080]  обратные квадраты уже с 2. То есть то, что у нас k плюс 1. Вот с k плюс 1 обратные квадраты пошли.
[35:23.080 --> 35:40.280]  Так, теперь вот такая немудренная аналитика. Выделим член k равно 0. Тогда это будет значит 0,1,
[35:40.280 --> 35:51.320]  0,1. А вот здесь будет ряд единицы на n квадрат от единицы до бесконечности. Чему такой ряд равен?
[35:51.320 --> 36:00.400]  Совершенно справедливо. Но на самом деле нам даже не сильно надо, но как бы хорошо,
[36:00.400 --> 36:09.120]  что помните. Ну и плюс то, что осталось k от единицы до бесконечности, k плюс 1 в квадрате,
[36:09.120 --> 36:22.080]  к, к плюс 1, сумма единицы на n квадрат, n равно k плюс 1 до бесконечности. Ну теперь первый ряд
[36:22.080 --> 36:35.240]  будет начинаться с двойки. Так, значит смотрите, простенькая картиночка. Единица на n квадрат,
[36:35.240 --> 36:49.200]  на x квадрат, единица на x квадрат. Ну вот, например, когда вот k, а вот k плюс 1. Вот нам
[36:49.200 --> 36:56.840]  нужно k плюс 1 в квадрате. Это вот такой столбик. Дальше пошло так далее. Ну и собственно видно,
[36:56.840 --> 37:10.000]  что вот этот ряд, когда k начинается с двойки, он, ну напишу, меньше или равен,
[37:10.000 --> 37:21.480]  перепишу. Так, π квадрат на 6 я заменю на тройку, поймете почему. На тройку заменил,
[37:21.480 --> 37:34.120]  на большую величину. Вот, значит здесь остается k от 1 до бесконечности, k плюс 1 в квадрате,
[37:34.120 --> 37:51.480]  k, k плюс 1. А вот здесь это будет интеграл от k до бесконечности dx на x квадрат. Согласны? Вот.
[37:51.480 --> 38:09.760]  Так, так, так, наверное. Не знаю, куда переместиться. Можно сюда? Мы уже использовали то, что нам нужно здесь.
[38:21.480 --> 38:40.280]  Так, значит меньше или равно, напишу здесь. Ну или сейчас пока, ну да, меньше или равно. Меньше или
[38:40.280 --> 39:01.080]  равно 3 на вот этот наш объект 0,1 плюс сумма k от 1 до бесконечности, k плюс 1 в квадрате делить на k и умножить на k,
[39:01.080 --> 39:11.680]  k плюс 1. Понятно, да, откуда единица на k взялась? Потому что это интеграл в районе единицы на k.
[39:11.680 --> 39:25.560]  k больше единицы у нас. Ну, кстати, как, если мы даже не помнили, что сумма обратных квадратов это
[39:25.560 --> 39:34.920]  пи квадрат делить на 6, то мы знаем, что ряд, который начинается с 2, он меньше единицы. Ну и первый
[39:34.920 --> 39:41.760]  член этого ряда единица, то есть вот это как бы двойка заведома. Мы заменили на тройку. Так,
[39:41.760 --> 39:49.800]  это звонок, да? Ну давайте вот на этом месте остановимся. И так, значит, мы с вами получили,
[39:49.800 --> 39:56.760]  что вот этот ряд с учетом вот этого свойства и вот этой вот оценки, которую мы как бы нарисовали,
[39:56.760 --> 40:06.800]  он имеет вот такой вид. Отдохните. Так, коллеги, чтобы двинуться дальше, вот такое напишу тривиальное
[40:06.800 --> 40:15.960]  утверждение, что k плюс 1 в квадрате делить на k меньше или равно k плюс 3. Для любого k больше
[40:15.960 --> 40:24.840]  или равно единицы. Поэтому воспользуюсь здесь этим и напишу, меньше или равно. Эту троечку
[40:24.840 --> 40:37.440]  сразу вытащу. Меньше или равно 3 суммы вот этих наших множеств типа k, k плюс 1, k от нуля до
[40:37.440 --> 40:57.040]  бесконечности, плюс k, k плюс 1, k от единицы до бесконечности. Вот эта сумма чему равна?
[40:57.040 --> 41:10.640]  Единицы. Да, значит, это вероятность того, что модуль нашей случайной величины попадет или от нуля
[41:10.640 --> 41:19.360]  до единицы, это одного до двойки, от двойки до тройки, это единицы. Вот поэтому пишу это пока
[41:19.360 --> 41:36.800]  напишу равно. Тройка плюс, а вот здесь напишу сумма k. Вернусь, что это такое. Это dfc от x по области
[41:36.800 --> 41:44.360]  x меньше k плюс 1 больше или равно k. И знаете, пожалуй, чтобы два раза не переписывать...
[41:44.360 --> 41:58.960]  Нет, нет, нет. Было k плюс 1 в квадрате делить на k, мы заменили на k плюс 3.
[41:58.960 --> 42:15.400]  Прошу прощения. Конечно, прошу прощения. Так вот, чтобы два раза не переписывать,
[42:15.400 --> 42:26.920]  я все-таки здесь поставлю меньше или равно, а здесь напишу с k равного нуля. Можно было и равно,
[42:27.000 --> 42:34.120]  потому что с k равным нулю это 0. Ну ладно. А теперь пишу следующее. Меньше или равно
[42:34.120 --> 42:53.720]  3 плюс сумма k от нуля до бесконечности, интеграл, модуль x, dfc от x, x по модулю меньше k плюс 1
[42:53.720 --> 43:06.280]  больше или равно k. А вот это что такое? Совершенно справедливо. Это математическое ожидание модуля
[43:06.280 --> 43:14.680]  psi, который у нас существует и меньше бесконечности. Таким образом, мы с вами показали,
[43:14.680 --> 43:22.520]  что вот это вот, вот это вот, начали мы отсюда. Вот эта сумма меньше бесконечности при условии,
[43:22.520 --> 43:30.720]  что существует мат ожидания. И тогда по первой теореме Колмогорова мы можем утверждать следующее.
[43:30.720 --> 43:35.680]  Сотру вот, вот это могу стереть, да? Думаю.
[43:53.520 --> 44:00.280]  Вот в такую форме запишу первую теорему Колмогорова.
[44:00.280 --> 44:11.120]  psi n со звездочкой средняя минус математическое ожидание psi n со звездочкой средняя,
[44:11.120 --> 44:19.520]  сходится к нулю почти, наверное, при n, стремящемся к бесконечности. Вот факт,
[44:19.520 --> 44:31.920]  который мы с вами сейчас доказали. Так, а теперь давайте исследуем вот такой ряд. Вероятность того,
[44:31.920 --> 44:45.360]  что psi n со звездочкой не равно psi n. n от единицы до бесконечности. И исследуем его на счет
[44:45.360 --> 45:07.200]  сходимости. Вот вопрос в этом. Вот это? Это вероятность того, что случайная величина
[45:07.200 --> 45:12.920]  принадлежит от минус бесконечности до бесконечности. Потому что вот это вот,
[45:12.920 --> 45:24.480]  одна из форм этой записи, это вероятность того, что psi меньше k плюс 1 больше равно k. Ответил наш
[45:24.480 --> 45:43.240]  вопрос. Так, значит, кто мне скажет, зачем, вот почему меня это заинтересовало? Такая сумма. Отвечу. Если
[45:43.240 --> 45:51.520]  такой ряд сходится, то по Lemme-Barrelli-Cantelli это означает, что psi n со звездочкой не равно
[45:52.400 --> 46:01.800]  конечное число раз с вероятностью единицы. Припоминайте Lemme-Barrelli-Cantelli. Если я это докажу,
[46:01.800 --> 46:11.440]  то я могу утверждать, что с вероятностью единицы ряды psi n со звездочкой и просто psi n отличаются
[46:11.440 --> 46:17.440]  лишь на конечном числе членов. А это вообще означает, что у них одинаковое среднее
[46:17.440 --> 46:24.000]  арифметическое. При этом стремясь к бесконечности. Вот, поэтому вот этот факт мне сильно поможет.
[46:24.000 --> 46:38.240]  Давайте я с ним разберусь. Так, значит, вероятность того, что psi n со звездочкой не равно psi n, это на самом
[46:38.240 --> 46:52.680]  деле вероятность того, что кси по модулю будет больше или равна n. Правильно, да? Здесь я тоже
[46:52.680 --> 47:03.200]  воспользуюсь разбиением на вот такие блоки. Нет, именно кси. Потому что все независимые одинаково
[47:03.200 --> 47:11.320]  распределены. Я могу поставить кси n, но в исходные последствия. А зависимость от n неявная. Вот она.
[47:11.320 --> 47:19.280]  Кси n со звездочкой это зависит от n, поэтому как-то это должно зависеть от n, так это зависит от n через
[47:19.280 --> 47:30.560]  вот эту. Так, это тогда будет сумма. Тут просто мне не ошибиться. Значит, вот этот наш объект очень
[47:30.560 --> 47:45.920]  удобный для записи k, k плюс 1, k равно от n до бесконечности. Правильно, да? n, n плюс 1, n плюс 1, n плюс 2 и так далее.
[47:45.920 --> 47:55.120]  Кси больше или равно n. Вот. Так, ну и тогда, собственно, давайте ряд теперь из этого составим.
[47:55.120 --> 48:16.560]  Кси n это равно ряд n равно 1 до бесконечности. Сумма k, k плюс 1, k равно n до бесконечности. Ну,
[48:16.560 --> 48:25.360]  вот тут вот тоже картинку нарисую. При n равном единице это будет 1, 2, плюс 2, 3, плюс и так далее.
[48:25.360 --> 48:38.680]  Это n равно 1. n равно 2. Это будет вот 2, 3, плюс и так далее. То есть это на самом деле сумма n равно
[48:38.680 --> 48:48.720]  1 до бесконечности или k равно 1 до плюс бесконечности. Вот давайте как-то привыкли к этому индексу,
[48:48.720 --> 49:02.520]  который разбивает у нас случайно в личину k. k от 1 до бесконечности, k на k, k плюс 1.
[49:02.520 --> 49:14.600]  Да? Правильно? И вот сюда смотрим, мы уже такую штуку делали. Вот она,
[49:14.600 --> 49:24.640]  то же самая сумма, поэтому все это меньше, ну пусть или равно, в от ожидания модуль кси. Правильно?
[49:24.640 --> 49:36.440]  Нет вопросов. Так, ну вот собственно существенный факт. Ну а теперь немножко, как говорится,
[49:36.440 --> 49:45.400]  поднапряжемся. Вот это теперь можно уже стирать. Мы получили, чего хотели.
[50:06.440 --> 50:14.840]  Ну может просто чтобы было понятно, я вот здесь внизу напишу, вероятность номера омега таких,
[50:14.840 --> 50:28.480]  что кси n со звездой от омега не равно кси n от омега, конечное число раз, вот здесь,
[50:28.480 --> 50:43.840]  раз, равна единице. Вот словами там или как-то полусловами записано утверждение
[50:43.840 --> 50:52.440]  Лемма-Борреля-Кантелли. Вот. Ну и значит смотрите, берем омега из вот этого множества единичной
[50:52.440 --> 51:09.960]  меры и что нам известно, что кси n со звездочкой от омега сходится к, точнее говоря, не сходится,
[51:09.960 --> 51:23.120]  минус, минус математическое ожидание кси n со звездочкой, среднее, среднее вот здесь,
[51:23.120 --> 51:33.080]  среднее, сходится почти, наверное, к нулю. Правильно, да? Для всех омега из вот этого
[51:33.080 --> 51:43.280]  множества единичной меры, это сходимость почти, наверное. Кси n со звездочкой, средняя и просто
[51:43.280 --> 51:50.160]  кси n от омега имеют один и тот же предел, поэтому кси n средняя для этого же омега,
[51:50.160 --> 52:03.760]  минус вот эту штуку тоже сходится к нулю, почти, наверное, до приема стремящейся к бесконечности.
[52:03.760 --> 52:13.360]  Ну, можно повозиться и поискать вот этот предел приема стремящейся к бесконечности, вот этот вот.
[52:13.360 --> 52:21.320]  Математическое ожидание у средненных случайных величин одинаково распределенных. Но здесь мы
[52:21.320 --> 52:29.240]  можем поступить и по-другому, воспользовавшись теоремой Хинчина. По теореме Хинчина мы знаем,
[52:29.240 --> 52:36.200]  что независимые одинаково распределенные случайные величины, если существует мат ожидания,
[52:36.200 --> 52:47.320]  сходится по вероятности мат ожиданию кси. Значит, вот эта штука не может сходиться ни к чему другому,
[52:47.320 --> 52:55.280]  как к мат ожиданию кси, которая равно а, правильно? Потому что, если бы она сходилась к чему-то другому,
[52:55.280 --> 53:03.360]  то у нас кси n средняя сходилась бы к некому числу, отличному от мат ожидания, почти,
[53:03.360 --> 53:09.840]  наверное, а значит, сходилась бы к этому же числу и по вероятности. А по вероятности она сходится к мат
[53:09.840 --> 53:29.680]  ожиданию кси этой теоремы Хинчина. Поэтому кси n средняя, почти, наверное, сходится к числу а,
[53:29.680 --> 53:46.960]  которая равно мат ожиданию, почти, наверное. Итак, если существует мат ожидания, то последствия
[53:46.960 --> 53:53.360]  независимо одинаково распределенных случайных величин сходится к некому числу, сходится к своему
[53:53.360 --> 54:03.040]  мат ожиданию. Теперь давайте предположим, что нам известно, что кси n сходится к некоему
[54:03.040 --> 54:09.360]  числу а, который мы не знаем, равно мат ожиданию, неравном мат ожиданию, и мы, так сказать, поймем,
[54:09.360 --> 54:24.800]  к чему она может сходиться. Так, ну давайте, так вот буду по модулю два так вытирать все.
[54:39.360 --> 54:56.120]  Пусть нам известно теперь, что кси n средняя сходится почти, наверное, к некоему числу а.
[54:56.120 --> 55:09.560]  Ну тогда я вот так напишу. Кси n делить на n равно сумма ксикатых k от 1 до n делить на n
[55:09.560 --> 55:21.880]  минус сумма ксикатых k от 1 делить на k от 1 до n минус 1 делить на n. Вот что это такое.
[55:21.880 --> 55:36.800]  Ну еще вот это второе, как это, вычитаемое, умножу на n минус 1 делить на n минус 1, то есть на
[55:36.800 --> 55:48.000]  единицу. Значит, смотрите, вот эта штука по условию сходится к а, почти, наверное. Вот это
[55:48.000 --> 55:56.600]  выражение сумма n минус 1 член делить на n минус 1 тоже сходится к а, почти, наверное,
[55:56.600 --> 56:03.040]  при n стремящейся к бесконечности. Ну и остается множитель n минус 1 делить на n,
[56:03.040 --> 56:10.400]  который при n стремящейся к бесконечности стремится к единице. Поэтому вот кси n делить на n сходится
[56:10.400 --> 56:27.400]  к нулю, почти, наверное. Правильно, да? А теперь вот такое утверждение. Если кси n делить на n сходится
[56:27.400 --> 56:43.760]  к нулю, почти, наверное, то вот такой ряд меньше бесконечности. Откуда это следует?
[56:43.760 --> 56:59.280]  Это вторая часть Lema-Barrelli-Cantelli или теоремы. Поскольку кси n все независимы, то если бы этот
[56:59.280 --> 57:07.340]  ряд расходился, то это бы означало, что событие кси n делить на n больше единицы встречается
[57:07.340 --> 57:14.100]  бесконечное число раз с вероятностью единицы. Но если кси n делить на n с вероятностью единицы
[57:14.100 --> 57:21.780]  сходится к нулю, то у нее не могут встречаться члены сколь угодно далеко, которые больше единицы.
[57:21.780 --> 57:31.740]  Согласны? Таким образом, из того, что кси n делить на n почти, наверное, сходится к нулю и кси n
[57:31.740 --> 57:40.920]  независимой, следует сходимость вот такого ряда. Ну теперь я делаю следующее. Пишу бесконечность
[57:40.920 --> 57:51.580]  больше. Такой чисто формальный член вероятность того, что кси по модулю больше равно нулю, это единица,
[57:51.580 --> 58:00.620]  просто единичку. Ну и плюс вот этот сходящийся ряд n равно единицы до бесконечности, вероятность того,
[58:00.620 --> 58:15.660]  что кси n по модулю больше или равно n. Но теперь мне будет удобно загнать все под общую сумму n от
[58:15.660 --> 58:23.620]  нуля до бесконечности, вероятность того, что кси по модулю, индекс n здесь убрал, больше или равно n.
[58:23.620 --> 58:36.740]  Понятно, да? Почему индекс убрал? Так, мы уже такую штуку делали, вот она, она, только будет немножко
[58:36.740 --> 58:45.220]  здесь по-другому, поскольку n с нуля будет начинаться, здесь будет 0, 1 плюс, здесь будет 1, 2 плюс и
[58:45.220 --> 59:04.100]  так далее. То есть это, если я все перепакую, то это будет сумма k плюс 1, k, k плюс 1, k от нуля до
[59:04.100 --> 59:22.660]  бесконечности. Посмотрите так. Так, да? Вот. Ну а теперь уже, честно говоря, стандартный прием k от нуля до
[59:22.660 --> 59:35.660]  бесконечности, k плюс 1, интеграл х по модулю меньше k плюс 1, больше или равно k, df кси от x,
[59:35.660 --> 59:53.860]  больше или равно сумма k от нуля до бесконечности, интеграл модуль х, df кси х, область интегрирования
[59:53.860 --> 01:00:02.780]  х меньше k плюс 1, больше или равно k. Так, это мы уже с вами, с таким объектом сталкивались, это чего?
[01:00:02.780 --> 01:00:18.860]  Это математическое ожидание модуль кси. Видите, да? Вот. Итак, если к чему-нибудь,
[01:00:18.860 --> 01:00:23.580]  к какому-то числу сходится, почти, наверное, последствия независимых одинаково распространённых
[01:00:23.580 --> 01:00:31.140]  случайных величин, то обязательно существует мат ожидания. Ну и опять, так сказать, тот же фокус
[01:00:31.140 --> 01:00:40.660]  с теоремой Хинчина. Итак, ксиенная средняя сходится к а, почти, наверное. Ксиенная средняя по теореме Хинчина,
[01:00:40.660 --> 01:00:47.740]  раз есть мат ожидания, мы это доказали, сходится по вероятности в мат ожидания кси. Ну и отсюда
[01:00:47.740 --> 01:00:59.820]  следует, что а равно кси, мат ожидания кси. Вот теорема Калмагорова. Вторая. Завершили доказательства.
[01:00:59.820 --> 01:01:13.540]  Нет вопросов. Ну, что скажешь, немножко длинновато и где-то нудновато. Ну вот,
[01:01:13.540 --> 01:01:21.700]  тем не менее, вот такое красивое доказательство с использованием, ещё раз повторю, введённой
[01:01:21.700 --> 01:01:28.660]  Калмагоровым специального, так сказать, приёма усечённыхся случайных величин. Вот,
[01:01:28.660 --> 01:01:40.420]  так сказать, получили ответ. Всё. Тогда, значит, я стираю всё и мы двигаемся дальше. Собственно,
[01:01:40.420 --> 01:01:47.940]  нам осталось не так уж и много. Так, следующая лекция 10-го числа у нас последняя, как и планировали.
[01:01:47.940 --> 01:01:57.260]  Ну, точнее говоря, не как планировали, а как распорядилась судьба. Вот. Так,
[01:01:57.260 --> 01:02:05.020]  ну это пока оставлю. Может быть, так сказать, кто-то ещё смотрит, размышляет и записывает. Так,
[01:02:05.020 --> 01:02:17.420]  значит, следующая наша цель – это доказательства теоремы Глевенко. Это тоже фундаментальная
[01:02:17.900 --> 01:02:26.820]  теорема теории вероятностей. Глевенко – наш соотечественник, выпускник Московского
[01:02:26.820 --> 01:02:32.780]  университета. К сожалению, умер там в раннем возрасте, в сорок с небольшим лет. Но оставил
[01:02:32.780 --> 01:02:39.820]  после себя две именные теоремы. Одну в теории вероятностей, а вторая в теории, как-то назвать,
[01:02:39.980 --> 01:02:47.460]  а там логики исчислимости, вычислимости и исчислимости. Вот. Значит, то есть в разных
[01:02:47.460 --> 01:02:55.180]  областях две именно теоремы. Ну, прямо скажем так сказать, грассмейстерский результат. Вот. Но
[01:02:55.180 --> 01:03:03.100]  до теоремы Глевенко нам нужно изучить ещё один очень, как бы, такой важный объект, с которым вы
[01:03:03.100 --> 01:03:10.380]  там неоднократно столкнетесь в последующем. И я, может, где-то так сказать, ну, по крайней мере,
[01:03:10.380 --> 01:03:17.660]  на уровне терминов дам намётки. Значит, напомню, как мы вводили случайную последовательность.
[01:03:17.660 --> 01:03:30.580]  Каждое Омега мы ставили в соответствие кси-н от Омега. Каждой такой последовательности мы можем
[01:03:30.580 --> 01:03:39.260]  поставить в соответствие такие серии. Кси-один от Омега. Первая серия. Вторая кси-один от Омега.
[01:03:39.260 --> 01:03:49.660]  Кси-два от Омега. И, наконец, там какой-нибудь кси-к от Омега. Ой, кси-один от Омега. Так далее кси-к от Омега.
[01:03:49.660 --> 01:03:58.460]  То есть, если есть Омега, то вот ставим в соответствие такие серии. Через случайную последовательность.
[01:03:58.460 --> 01:04:11.740]  И каждой такой серии мы можем поставить в соответствие некую функцию. Это будет f1 от x. Вот здесь будет f2 от x. А вот здесь будет fk от x.
[01:04:11.740 --> 01:04:33.820]  Некую функцию. По следующему правилу. fn от x равно 1n сумма. А, прошу прощения, это важный факт. Вот мы говорим
[01:04:33.820 --> 01:04:44.460]  о последствиях независимых одинаково распределенных случайных величин. Пометьте, это важно. Независимые одинаково распределенные случайные величины.
[01:04:44.460 --> 01:04:59.900]  Функция fn строится по правилу. 1n индикаторная функция события. x kt меньше x. Ну и k от единицы до n. Я ее прям обведу в рамочку.
[01:04:59.900 --> 01:05:14.700]  Несмотря на такую простоту, это очень важный объект. Принципиально важный для математической статистики. Понятно, да, как он строится?
[01:05:14.700 --> 01:05:25.780]  Есть Омега, получается последовательность. Из этой последовательности строим серии. Вот такие по правилу, так сказать, треугольника.
[01:05:26.180 --> 01:05:34.180]  И каждой такой серии ставим соответствия уже функцию от x. Вот последующему правилу.
[01:05:34.180 --> 01:05:49.220]  Значит, эта функция называется империческая функция распределения. Это термин. Империческая функция распределения.
[01:05:50.180 --> 01:06:01.300]  И она обладает рядом замечательных свойств. Одной из этих свойств составляет содержание теоремы Гливенко, к которой мы, так сказать, подойдем.
[01:06:01.300 --> 01:06:14.020]  А пока давайте вот на что обратим внимание. Давайте посмотрим. Вот зафиксируем x, любой, для любого x, но зафиксируем.
[01:06:14.020 --> 01:06:19.060]  Давайте найдем предел, почти, наверное, fn от x.
[01:06:25.060 --> 01:06:27.060]  Есть идея?
[01:06:27.060 --> 01:06:44.740]  Ну, смотрите, все каты одинаково распределены, поэтому вот эти все индикаторные функции, это тоже случайные величины, одинаково распределенные.
[01:06:44.740 --> 01:06:54.500]  Значит, и мы имеем с вами дело среднеарифметическое одинаково распределенных случайных величин.
[01:06:57.060 --> 01:07:09.380]  Значит, по второй теореме Колмогорова, если это к чему исходится, то к чему? К мотожиданию вот этой индикаторной функции.
[01:07:17.140 --> 01:07:24.580]  А это что такое? А это вероятность того, что x меньше x. А это что такое?
[01:07:25.540 --> 01:07:27.540]  Это функция распределения.
[01:07:31.540 --> 01:07:36.500]  И вот первое удивительное для такой простой функции свойство.
[01:07:36.500 --> 01:07:45.860]  Эта функция для каждого фиксированного x, почти, наверное, сходится к функции распределения, которая, может быть, нам неизвестна.
[01:07:46.820 --> 01:07:57.780]  Мы с вами, когда говорили о всяких физических законах, связанных с концентрацией меры, мы там говорили, что средние значения вокруг мотожидания.
[01:07:57.780 --> 01:08:02.180]  То есть у нас есть как бы экспериментальный способ найти мотожидание.
[01:08:02.180 --> 01:08:12.260]  Там нормированные какие-то величины тем или иным способом концентрируются вокруг соответствующего закона распределения по асоновского или нормального.
[01:08:12.340 --> 01:08:19.220]  Но вообще говоря, случайная величина в нашей теории отождествляется всего функцией распределения.
[01:08:19.220 --> 01:08:32.340]  Таким образом, империческая функция распределения представляет нам экспериментальную базу для того, чтобы найти функцию распределения неизвестной нам случайной величины.
[01:08:32.340 --> 01:08:37.780]  Вот какое-то явление природное может быть описано случайной величиной.
[01:08:37.860 --> 01:08:48.660]  Причем, возвращаясь, грубо говоря, к самому началу нашего курса, есть, особенно в квантовой физике, вещи, которые принципиально так описываются.
[01:08:48.660 --> 01:08:51.620]  Это не модель, они как бы по сути такие.
[01:08:53.620 --> 01:08:57.860]  И у нас просто есть механизм. Мы ничего не знаем, какая распределена случайная величина.
[01:08:57.940 --> 01:09:09.860]  Но если у нас есть экспериментальные данные, они независимы, одинаково распределены, это, кстати, из одного и того же природного явления, или эксперименты их черпаем,
[01:09:09.860 --> 01:09:21.860]  то мы можем почти, наверное, с вероятностью единица построить оценку функции распределения, которую называют в данном случае гипотетической.
[01:09:21.860 --> 01:09:29.860]  Империческая функция распределения для любого х почти, наверное, сходится к гипотетической функции распределения.
[01:09:29.860 --> 01:09:41.860]  Вот такой вот крайне важный факт, который открывает нам возможность экспериментального описания случайной величины.
[01:09:41.860 --> 01:09:49.860]  Повторюсь еще раз, ведь случайная величина – это не число, это ее функция распределения. Если вы знаете функцию распределения, вы все знаете о случайной величине.
[01:09:52.860 --> 01:09:56.860]  Но это не все свойства этой случайной величины.
[01:09:56.860 --> 01:10:01.860]  Следующая.
[01:10:01.860 --> 01:10:29.860]  Ну, например, у вас есть там показания счетчика Гейгера, там, не знаю, 100 у вас счетчиков или 200, и они там, грубо говоря, стоят в соседних комнатах.
[01:10:29.860 --> 01:10:40.860]  Вы получаете значение, вы знаете модель, что это случайная величина, количество частиц зарегистрированное.
[01:10:40.860 --> 01:10:49.860]  Вы берете эти числа и по такой формуле для каждого х, с каким-то шагом, строите эту функцию.
[01:10:49.860 --> 01:10:53.860]  Я ответил наш вопрос?
[01:10:54.860 --> 01:11:05.860]  То есть если это рассматривать как экспериментальные данные, а не случайные величины, ну вот это и дает нам, грубо говоря, модель эксперимента.
[01:11:05.860 --> 01:11:18.860]  Мы считаем, что мы наблюдаем некие случайные величины, ну исходим из того, да, и хотим знать функцию распределения, а наблюдаем мы, ну, просто, так сказать, какие-то, ну, разрозленные числа, грубо говоря.
[01:11:18.860 --> 01:11:32.860]  И если мы вот по такой схеме их соберем, вот такую функцию, то при достаточно большой мэн мы получим достаточно хорошее, пока не говорю в каком смысле, приближение неизвестной нам функции распределения.
[01:11:32.860 --> 01:11:39.860]  А эта функция получается в каждый раз для отдельного эксперимента своя? Ну, то есть в каком смысле она даже случайная оказывается?
[01:11:39.860 --> 01:11:53.860]  Она оказывается случайная, но вот это почти наверно означает, что при настремляющей бесконечности вы получите функцию, сходящуюся в обычном смысле, как, ну, числовая последовательность.
[01:11:53.860 --> 01:11:57.860]  В этом же смысл сходимости почти наверно.
[01:11:58.860 --> 01:12:05.860]  Что вероятность таких наборов случайных величин, что вот этой сходимости не будет, равна нулю.
[01:12:05.860 --> 01:12:15.860]  То есть, ну, как и любое явление с вероятностью ноль, оно существует в теоретической модели, но никогда не встречается в практике, да.
[01:12:15.860 --> 01:12:26.860]  Как условно говоря, есть некая вероятность нулевая или там очень маленькая, что все молекулы в этой комнате соберутся в той половине, и мы все задохнемся.
[01:12:26.860 --> 01:12:32.860]  Но ни одного такого факта неизвестно, это не происходит никогда, потому что вероятность этого события ноль.
[01:12:32.860 --> 01:12:38.860]  Хотя, как математическое событие в вероятностном пространстве, оно существует.
[01:12:38.860 --> 01:12:43.860]  Вот, поэтому она не случайная все-таки.
[01:12:43.860 --> 01:12:52.860]  Ну, вот вы, когда считаете численным методом что-то, получаете решение, сходящее с итерационно к истинному, да.
[01:12:52.860 --> 01:12:59.860]  Если у вас просто известно, что имеет место сходимость, вы же не знаете в какой момент остановиться, да.
[01:12:59.860 --> 01:13:07.860]  Вы уже достигли предела, или сейчас вокруг какой-то кривой концентрируется, потом все опять разлетится, а потом, кстати, опять начнет сходиться.
[01:13:07.860 --> 01:13:12.860]  В этом смысле и детерминированный эксперимент тоже случайен, да.
[01:13:12.860 --> 01:13:16.860]  Вот в этом смысле, если речь о предельных соотношениях.
[01:13:16.860 --> 01:13:20.860]  Ну, сходимость почти наверно, это она и есть.
[01:13:20.860 --> 01:13:24.860]  То есть, если вы будете увеличивать тен, то вы получите последовательность,
[01:13:24.860 --> 01:13:31.860]  которая как обычная числовая последовательность для некоторых х, будет сходиться вот к этой величине.
[01:13:31.860 --> 01:13:37.860]  И в этом смысле она не хуже и не лучше численного метода или какого-то приближения.
[01:13:37.860 --> 01:13:40.860]  Приближения же тоже они асимпатические все.
[01:13:41.860 --> 01:13:46.860]  Ну, значит, следующее свойство этой функции,
[01:13:46.860 --> 01:13:52.860]  которая собственно и составляет содержание теоремы Гливенко, выглядит так.
[01:13:52.860 --> 01:14:02.860]  Значит, supremum по х fn от х
[01:14:02.860 --> 01:14:08.860]  минус fx от х сходится к нулю
[01:14:08.860 --> 01:14:13.860]  почти наверно, при н-стриме и бесконечности.
[01:14:13.860 --> 01:14:20.860]  То есть, сходимость не только точечная, а в метрике равномерной сходимости.
[01:14:20.860 --> 01:14:24.860]  Эта функция сходится равномерно к функции распределения.
[01:14:24.860 --> 01:14:28.860]  На самом деле мы об этом, наверное, немножко догадываемся,
[01:14:28.860 --> 01:14:32.860]  потому что fn, это формально функция распределения,
[01:14:32.860 --> 01:14:39.860]  она неубывающая, ограниченная и сходится она к некой функции, пусть непрерывной.
[01:14:39.860 --> 01:14:44.860]  Ну, тогда из поточной сходимости следует сходимость в равномерной метрике,
[01:14:44.860 --> 01:14:49.860]  но нам нужно будет немножко повозиться там, как бы аккуратными быть в этом смысле.
[01:14:49.860 --> 01:14:56.860]  Значит, ну, а теперь тоже об некой исторической справедливости.
[01:14:56.860 --> 01:15:03.860]  Вот эта теорема в большинстве учебников называется теоремой Гливенко-Кантелли.
[01:15:03.860 --> 01:15:06.860]  Теорема Гливенко-Кантелли.
[01:15:06.860 --> 01:15:13.860]  И с этим связана такая, ну, не совсем прозрачная история, как мне кажется.
[01:15:13.860 --> 01:15:18.860]  Дело в том, значит, эта теорема была опубликована в 1933 году,
[01:15:18.860 --> 01:15:24.860]  а это был такой исторический период, когда, как бы,
[01:15:24.860 --> 01:15:30.860]  ну, было такое выражение, статистика, буржуазная наука и так далее.
[01:15:30.860 --> 01:15:34.860]  То есть некоторые выдающиеся работы наших соотечественников
[01:15:34.860 --> 01:15:37.860]  опубликовались в зарубежных журналах.
[01:15:37.860 --> 01:15:38.860]  Вот.
[01:15:38.860 --> 01:15:43.860]  Значит, ну, кстати, работа Колмогорова, тоже связанная с империческим функцией распределения,
[01:15:43.860 --> 01:15:47.860]  тоже была опубликована, так сказать, там во французском журнале,
[01:15:47.860 --> 01:15:50.860]  как говорится, теперь и концов не найдешь.
[01:15:50.860 --> 01:15:51.860]  Вот.
[01:15:51.860 --> 01:15:57.860]  Но, тем не менее, вот, значит, Гливенко сформулировал эту теорему
[01:15:57.860 --> 01:16:01.860]  для непрерывных случайных величин.
[01:16:01.860 --> 01:16:04.860]  То есть когда f от xi непрерывный случайный, и доказал.
[01:16:04.860 --> 01:16:08.860]  Ну и, собственно, так сказать, опубликовал.
[01:16:08.860 --> 01:16:15.860]  Ну и вот каким-то странным образом именно в этом же журнале, там, в том же 1933 году
[01:16:15.860 --> 01:16:20.860]  Контель опубликовал свою работу, обобщающую теорему Гливенко
[01:16:20.860 --> 01:16:23.860]  на случай произвольных распределений.
[01:16:23.860 --> 01:16:28.860]  То есть, там, ну, за исключением там сингулярных, которые мы не рассматриваем.
[01:16:28.860 --> 01:16:29.860]  Вот.
[01:16:29.860 --> 01:16:35.860]  Идея доказательства, собственно, была такая же, как и Гливенко,
[01:16:35.860 --> 01:16:39.860]  только, ну, требуется более тщательная возня с точками разрыва,
[01:16:39.860 --> 01:16:43.860]  непрерывные слева, которые у нас, как функции распределения.
[01:16:43.860 --> 01:16:48.860]  Поэтому, ну, с точки зрения приоритетов, эта теорема называется
[01:16:48.860 --> 01:16:56.860]  теоремой Гливенко-Контелли, но, учитывая, что доказательства Контелли
[01:16:56.860 --> 01:17:00.860]  идейно, еще раз повторю, было такое же, как и у Гливенко,
[01:17:00.860 --> 01:17:04.860]  просто он там тщательно повозился там с точками разрыва.
[01:17:04.860 --> 01:17:10.860]  Ну вот, наверное, все-таки так, наверное, не зря в названии теоремы
[01:17:10.860 --> 01:17:15.860]  Гливенко стоит первый, а Контелли второй, хотя, вроде бы, более общий случай.
[01:17:15.860 --> 01:17:16.860]  Вот.
[01:17:16.860 --> 01:17:22.860]  Значит, вот эта вот теорема Гливенко, Гливенко-Контелли.
[01:17:22.860 --> 01:17:30.860]  Значит, ну, давайте начнем чуть-чуть, как бы так сказать, и идейно подведем.
[01:17:30.860 --> 01:17:38.860]  Давайте вот это обозначим так, некое дн-ное, просто обозначение пока, дн-ное.
[01:17:38.860 --> 01:17:43.860]  И давайте посмотрим, что означает дн-ное сходится к нулю почти наверно.
[01:17:43.860 --> 01:17:47.860]  Это, вообще-то говоря, означает, что дн-ное случайная величина, да?
[01:17:47.860 --> 01:17:52.860]  Потому что мы эти определения давали исходя из того, что имеем дело
[01:17:52.860 --> 01:17:53.860]  с случайными величинами.
[01:17:53.860 --> 01:17:57.860]  Ну, посмотрите, здесь Супремум берется по несчетному множеству.
[01:17:57.860 --> 01:18:03.860]  И наша теория, такие объекты, ну, как бы они автоматически в нее не включаются.
[01:18:03.860 --> 01:18:09.860]  Но, как бывает, в частности, там, в теории случайных процессов,
[01:18:09.860 --> 01:18:15.860]  которые вы будете изучать в следующем году, там, события
[01:18:15.860 --> 01:18:20.860]  на континуальном множестве, ну, специальным образом, в каких-то задачах,
[01:18:20.860 --> 01:18:23.860]  сводятся к событию там на счетном множестве.
[01:18:23.860 --> 01:18:29.860]  Но, в частности, в данном случае, вот, что я имею в виду?
[01:18:29.860 --> 01:18:33.860]  Что, ну, не буду два раза переписывать, а напишу так.
[01:18:33.860 --> 01:18:43.860]  Дн, на самом деле, можно представить в виде Супремум по х-катам,
[01:18:43.860 --> 01:18:49.860]  которые принадлежат рациональным числам,
[01:18:49.860 --> 01:18:57.860]  ФН от х-кат, минус Фкси от х-кат.
[01:18:57.860 --> 01:19:01.860]  Согласны?
[01:19:01.860 --> 01:19:07.860]  Ну, если здесь есть Супремум, значит, существует какая-то последовательность
[01:19:07.860 --> 01:19:10.860]  из х, которая сходится к той точке Г-экстремум.
[01:19:10.860 --> 01:19:14.860]  Ну, поскольку это всю доплотное множество, то мы и из рациональных чисел
[01:19:14.860 --> 01:19:17.860]  можем построить последовательность, сходящуюся ровно туда.
[01:19:17.860 --> 01:19:23.860]  Ну, кстати, обратите внимание, Супремум не достигается в точке плюс-минус бесконечность,
[01:19:23.860 --> 01:19:27.860]  потому что в точке плюс-минус бесконечность все функции распределения,
[01:19:27.860 --> 01:19:32.860]  и ФН, и Фкси произвольные, равны нулю бесконечности.
[01:19:32.860 --> 01:19:33.860]  Ноль получается.
[01:19:33.860 --> 01:19:36.860]  Поэтому точка Супремума, она где-то внутри.
[01:19:36.860 --> 01:19:44.860]  Вот. И тогда, естественно, так сказать, можно воспользоваться фактом того,
[01:19:44.860 --> 01:19:47.860]  что множество рациональных чисел всюду плотно,
[01:19:47.860 --> 01:19:52.860]  и, так сказать, получить, что ДН можно представить не в таком, а в таком виде.
[01:19:52.860 --> 01:19:58.860]  А это уже Супремум счетного числа вот таких случайных величин,
[01:19:58.860 --> 01:20:02.860]  и это случайная величина, для которой вполне конкретно,
[01:20:02.860 --> 01:20:06.860]  вполне корректно определить понятие сходимости почти наверно.
[01:20:06.860 --> 01:20:09.860]  Понятно, да, до этого места?
[01:20:09.860 --> 01:20:13.860]  Нам осталось совсем немного, ну и в следующий раз мы закончим.
[01:20:13.860 --> 01:20:17.860]  Это тоже, как сказать, очень красивая теорема в том смысле,
[01:20:17.860 --> 01:20:22.860]  что она очень глубокая, но совсем несложная,
[01:20:22.860 --> 01:20:25.860]  ну идея доказательства очень несложная.
[01:20:25.860 --> 01:20:29.860]  Все, коллеги, тогда давайте до следующего раза. Спасибо.

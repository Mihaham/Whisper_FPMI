[00:00.000 --> 00:09.000]  Уважаемые коллеги, у нас сегодня будет заключительная лекция в рамках курса математика больших данных.
[00:09.000 --> 00:19.000]  Я попросил Алексея Наумова эту лекцию провести. Алексей оказался у него в конце года, достаточно там сейчас много дел.
[00:19.000 --> 00:26.000]  И доклад будет делан Дениил Тяпкин. Дениил Тяпкин очень перспективный, очень активный молодой ученый.
[00:26.000 --> 00:31.000]  У нас уже несколько статей на топовой конференции. Вообще у нас он уже выступал и очень здорово.
[00:31.000 --> 00:38.000]  Мне кажется, что это хорошо, что у нас сейчас будет такая возможность Дениила послушать.
[00:38.000 --> 00:43.000]  И рассказ будет про математику обучения с подкреплением. Это будет заключительная тема.
[00:43.000 --> 00:52.000]  И на этом основной цикл заканчивается. Дальше уже будет контрольная и сдача курса, сдача проектов и так далее.
[00:52.000 --> 00:58.000]  Но замечу, что элементы reinforcement learning могут войти в контрольную, которая будет в следующем среду.
[00:58.000 --> 01:04.000]  Вот, собственно, мы начинаем. Формат стандартный. Сначала полуторачасовая лекция, потом часовая.
[01:04.000 --> 01:08.000]  Через небольшой перерыв. Дениил, пожалуйста, начинайте.
[01:22.000 --> 01:32.000]  Дениил, меня слышно было?
[01:32.000 --> 01:40.000]  Да, вас было слышно.
[01:40.000 --> 01:47.000]  Так, я не верю, что Дениила нас слышит. Дениил, вы нас слышите? Меня слышно?
[01:47.000 --> 02:07.000]  Можно начинать, говорю.
[02:07.000 --> 02:11.000]  Не коллеги, не палатки.
[02:11.000 --> 02:17.000]  А, вот сейчас слышно. Сейчас слышно.
[02:17.000 --> 02:19.000]  Но презентацию зато не видно.
[02:37.000 --> 02:47.000]  Коллеги, подскажите, видно ли презентацию?
[02:47.000 --> 02:49.000]  Да, да, сейчас видно.
[02:49.000 --> 02:56.000]  Все отлично. Да, к сожалению, я из-за неполадок пропустил представление от Александра Роднировича.
[02:56.000 --> 03:02.000]  Поэтому я сразу перейду к своему докладу.
[03:03.000 --> 03:09.000]  Алексей попросил меня прочитать эту лекцию по математике машинного обучения.
[03:09.000 --> 03:15.000]  Поэтому как бы по названию курса мы будем в первую очередь ориентироваться на некоторую математику.
[03:15.000 --> 03:20.000]  По-другому эту лекцию можно назвать приложение концентрации меры в обучении с подкреплением.
[03:20.000 --> 03:24.000]  И в первую очередь речь будет как раз об этом.
[03:24.000 --> 03:29.000]  То есть немножко напомню, что такое обучение с подкреплением.
[03:29.000 --> 03:35.000]  В данном случае мы хотим обучить некоторого агента тайно замениться средой.
[03:35.000 --> 03:38.000]  Агент находится в некотором состоянии СТ.
[03:38.000 --> 03:41.000]  Он делает некое действие АТ.
[03:41.000 --> 03:44.000]  Действие это допустим пойти влево, пойти вправо.
[03:44.000 --> 03:47.000]  Что-то такого духа.
[03:47.000 --> 03:54.000]  После чего окружение, нечто называемое окружение, реагирует на это действие.
[03:54.000 --> 04:01.000]  Это дает следующее состояние, в котором будет находиться агент и некоторую награду, которую он получает.
[04:01.000 --> 04:05.000]  И так агент пытается максимизировать сумму наград, которую он получает.
[04:05.000 --> 04:12.000]  Концепт довольно простой, но понятно, что здесь очень много подводных камней.
[04:12.000 --> 04:17.000]  В первую очередь хочется оппозиции такого окружения.
[04:17.000 --> 04:24.000]  То есть для этого мы расскажем некоторые базовые вещи про марксиспроцепция решений,
[04:24.000 --> 04:26.000]  как называемые МДП.
[04:26.000 --> 04:35.000]  И докажем оценки на регресс для некоторого магического алгоритма обучения с подкреплением.
[04:35.000 --> 04:38.000]  Начну с таких базовых определений.
[04:38.000 --> 04:42.000]  Марксиспроцепция решений или MarkovetitionProtos или МДП.
[04:42.000 --> 04:47.000]  Это некий кортеж из, в этом случае, 5 элементов.
[04:47.000 --> 04:54.000]  С – это процесс состояния, то есть всего может состояние, где может находиться агент.
[04:54.000 --> 04:57.000]  А – это действие, которое может совершать агент.
[04:57.000 --> 05:00.000]  То есть пойти влево, пойти вправо, и все в таком духе.
[05:00.000 --> 05:03.000]  П – это переходное ядро.
[05:03.000 --> 05:09.000]  То есть если агент находится в состоянии С и делает действие А, то в какое состояние после этого попадет?
[05:09.000 --> 05:14.000]  Вот это определяется, основываясь на текущем состоянии и выбранном действии.
[05:14.000 --> 05:21.000]  И это ядро – это в некомерном состоянии, которое ассоциируется с следующим состоянием, которое получается.
[05:21.000 --> 05:28.000]  Также есть еще награда, которая зависит только от текущего состояния действия.
[05:28.000 --> 05:30.000]  Будем для просты считать, что она детерминирована.
[05:30.000 --> 05:32.000]  На самом деле это, конечно, не так.
[05:32.000 --> 05:38.000]  То есть награда обычно случайна, но для просты картинки будем считать, что она детерминирована.
[05:38.000 --> 05:43.000]  И мы будем рассматривать в случае конечного горизонта планирования.
[05:43.000 --> 05:47.000]  То есть после вот этого аж большого шагов игра начинается заново.
[05:47.000 --> 05:49.000]  То есть допустим, игра шагов.
[05:49.000 --> 05:51.000]  С – это количество возможностей в состоянии доски.
[05:51.000 --> 05:57.000]  А – это возможное действие, куда поставить, допустим, какую-нибудь коня, все вот эти возможные установки.
[05:57.000 --> 06:00.000]  А П в этом случае будет некая детерминированная вещь.
[06:00.000 --> 06:02.000]  Он будет не случайным.
[06:02.000 --> 06:06.000]  Она будет просто стоять там единицей в неком состоянии, о котором мы прийдем в следующее.
[06:06.000 --> 06:09.000]  А функция наград уже будет как-то правильно дизайниться.
[06:09.000 --> 06:13.000]  Есть вообще большая наука про дизайн наград, как правильно это делать.
[06:13.000 --> 06:17.000]  И горизонт – это максимальная длина игры в жахматы.
[06:17.000 --> 06:24.000]  Там, не знаю, словно 100 шагов, после 100 шагов игра заканчивается, объявив ничья, допустим.
[06:26.000 --> 06:29.000]  Соответственно говоря, есть простые среды.
[06:29.000 --> 06:33.000]  Допустим, вот такой вот мини-грид среда, которая забрана слева.
[06:33.000 --> 06:40.000]  Цель агента в этом случае треугольничка – это найти зеленую награду, зеленую клетку.
[06:40.000 --> 06:47.000]  У него награда для всех состояний действий нулевая, кроме случая, когда он достигает зеленой клетки.
[06:47.000 --> 06:49.000]  Туда у него награда ничная.
[06:49.000 --> 06:53.000]  Есть более сложные примеры, когда процесс состояния очень большое.
[06:53.000 --> 06:55.000]  Это, допустим, игры в Atari.
[06:55.000 --> 07:02.000]  То есть здесь процесс состояния – это просто все возможные картинки, все возможные состояния доски.
[07:02.000 --> 07:08.000]  По сути говоря, не доски, а экрана самой игровой приставки.
[07:08.000 --> 07:13.000]  И действие – это подвинуть влево, подвинуть вправо нашу платформу.
[07:13.000 --> 07:18.000]  И награда, соответственно говоря, это в точности количества заработанных очков для выполнения действия.
[07:20.000 --> 07:29.000]  Сам процесс обучения в MDP устроен так, что у нас есть, допустим, табулярное MDP, то есть слишком большое.
[07:29.000 --> 07:35.000]  Для большого количества состояний нужна всякая высокая наука про опроксимацию и все прочее.
[07:35.000 --> 07:37.000]  Ну, для нас это довольно простой случай.
[07:37.000 --> 07:41.000]  Есть некоторый горизонт планирования, как я уже говорил ранее.
[07:41.000 --> 07:43.000]  С – это число состояний, А – число действий.
[07:43.000 --> 07:45.000]  Все конечное для простоты.
[07:45.000 --> 07:50.000]  А сам процесс обучения, как я до этого описывал, у нас есть какое-то количество эпизодов.
[07:50.000 --> 07:54.000]  Это количество партий в шахматах, скажем.
[07:54.000 --> 07:58.000]  Внутри одного эпизода у нас есть, допустим, десятый шаг нашей партии.
[08:00.000 --> 08:06.000]  STH – это состояние доски в игре T в момент времени H.
[08:07.000 --> 08:10.000]  Дальше выбирается некоторое действие – ATH.
[08:11.000 --> 08:13.000]  И, собственно говоря, выбирается…
[08:13.000 --> 08:16.000]  Следующее состояние – оно поделяется средой.
[08:16.000 --> 08:20.000]  То есть поделяется неким вот этим ядром P, который мы не знаем.
[08:20.000 --> 08:22.000]  Но если бы мы его знали, все было бы просто.
[08:22.000 --> 08:24.000]  В нашем случае все не так просто.
[08:24.000 --> 08:28.000]  И мы получаем после этого некоторую награду вместе с состоянием,
[08:28.000 --> 08:30.000]  которую для простыбы считать известной.
[08:30.000 --> 08:34.000]  Опять же, есть вариации на тему, когда она случайная, еще в таком духе.
[08:34.000 --> 08:36.000]  Как выбирается действие?
[08:36.000 --> 08:38.000]  Здесь выбирается согласно некоторой политике P.
[08:38.000 --> 08:42.000]  То есть к некоторому правилу как выбирать действие.
[08:44.000 --> 08:47.000]  Высокая наука предписывает, что можно…
[08:47.000 --> 08:54.000]  Высокая наука предписывает, что можно это вот P выбирать не от каких-то распределений на действия, а от терминированных.
[08:54.000 --> 08:57.000]  То есть некий результат, который можно доказать.
[08:57.000 --> 09:01.000]  Мы его примем бездокачественно, доказываться несложно.
[09:01.000 --> 09:05.000]  И цель агента – это найти такую политику P,
[09:05.000 --> 09:07.000]  которая максимизирует value-функцию.
[09:07.000 --> 09:11.000]  На самом деле, это небольшой обман, что политика P зависит от шага H.
[09:11.000 --> 09:15.000]  То есть, условно, в конце игры она выгоднает некие жадные шаги,
[09:15.000 --> 09:18.000]  которые дают какую-то максимальную награду прямо сейчас.
[09:18.000 --> 09:20.000]  А в начале мы хотим действовать более стратегически.
[09:20.000 --> 09:24.000]  То есть при маленьких H наши шаги должны как-то вести в зону с большой наградой.
[09:26.000 --> 09:30.000]  То есть, допустим, вот в этом примере, если у нас была бы еще какая-нибудь добытная награда маленькая,
[09:30.000 --> 09:34.000]  то под конец игры нам выгодна просто пойти в эту маленькую награду.
[09:34.000 --> 09:38.000]  Если же у нас начало игры, нам выгодно развернуться и пойти в сторону большой.
[09:39.000 --> 09:46.000]  Вопрос такой, а вот у нас как бы меняется состояние S с шагом?
[09:47.000 --> 09:52.000]  Да, у нас каждый шаг выбирает новое состояние, и мы дальше уже от него шагаем.
[09:52.000 --> 09:55.000]  То есть у нас статируется новое состояние ST H+,
[09:55.000 --> 09:58.000]  и в следующий раз у нас будет политика P H+,
[09:58.000 --> 10:04.000]  которая по этому состоянию определяет следующее действие, которое мы хотим выбрать.
[10:04.000 --> 10:06.000]  Да, я как раз хотел спросить.
[10:06.000 --> 10:11.000]  То есть у нас получается вот S тоже зависит от сделанных нами шагов,
[10:11.000 --> 10:16.000]  и мы как бы все в совокупности учитываем при политике следующей.
[10:17.000 --> 10:19.000]  Да, да. Все так.
[10:20.000 --> 10:24.000]  То есть это такой, скажем так, это сеттинг теорический,
[10:24.000 --> 10:27.000]  потому что на практике, можно назвать дисконтированный сеттинг,
[10:27.000 --> 10:30.000]  когда у нас игра считается бесконечно долгой,
[10:30.000 --> 10:34.000]  но у нас следующее действие имеет меньшую стоимость.
[10:34.000 --> 10:36.000]  Все, что будет в будущем.
[10:36.000 --> 10:39.000]  Там профессионально, некоторым дисконтирующим фактором.
[10:39.000 --> 10:42.000]  Для теории гораздо удобнее сеттинг кобизонический,
[10:42.000 --> 10:46.000]  там, где у нас для каждого H, ну по сути можно считать, что меняется состояние.
[10:46.000 --> 10:50.000]  То есть все возможное там партии на первом шаге, все возможно партии на втором шаге.
[10:50.000 --> 10:52.000]  Шаг можно, допустим.
[10:54.000 --> 10:57.000]  То есть вот мы играем вот такую игру,
[10:57.000 --> 10:59.000]  скажем так, что у нас меняется состояние,
[10:59.000 --> 11:01.000]  она может на самом деле стать тем плюсом,
[11:01.000 --> 11:03.000]  но политика у нас изменится.
[11:03.000 --> 11:07.000]  У нас политика, это не она функция из состояния в действии,
[11:07.000 --> 11:10.000]  а набор из H функции на самом деле.
[11:11.000 --> 11:14.000]  Ну да, просто мне, просто я как-то хотел
[11:15.000 --> 11:17.000]  следить за тем, что у нас возможно,
[11:17.000 --> 11:20.000]  чтобы мы в модели не учитывали
[11:20.000 --> 11:22.000]  будущее знание в прошлом,
[11:22.000 --> 11:26.000]  чтобы мы не моделировали так
[11:26.000 --> 11:28.000]  нашу систему,
[11:28.000 --> 11:30.000]  но, наверное, так все и делается.
[11:30.000 --> 11:33.000]  Да, то есть эта политика, она не знает
[11:33.000 --> 11:36.000]  будущее состояние, они простомплируют в такой момент выбора в действии,
[11:36.000 --> 11:41.000]  поэтому все получается довольно некой в мысли справедливо.
[11:43.000 --> 11:46.000]  Соответственно говоря, дальше у нас есть эта value функция,
[11:46.000 --> 11:49.000]  то есть это сумма наград начинает с шага H.
[11:49.000 --> 11:53.000]  То есть мы хотим максимизировать наш сумму наград на первом шаге в идеале,
[11:53.000 --> 11:56.000]  но политика у нас каждый раз меняется,
[11:56.000 --> 11:59.000]  поэтому мы хотим на самом деле найти такую последность действий,
[11:59.000 --> 12:02.000]  чтобы начало состояния S
[12:02.000 --> 12:04.000]  максимизировать сумму наград.
[12:04.000 --> 12:07.000]  Ровно то, что я описывал на вот этой красивой схеме,
[12:07.000 --> 12:11.000]  которая бывает в каждом общем докладе по обучению с нащуплением.
[12:12.000 --> 12:15.000]  То есть мы находим состояние S на шаге
[12:15.000 --> 12:18.000]  на момент времени H внутри одного эпизода.
[12:18.000 --> 12:21.000]  То есть если H маленький равно 10,
[12:21.000 --> 12:24.000]  то после 10 шагов шагмата мы говорим, что пусть доска такая,
[12:24.000 --> 12:27.000]  то на какую награду мы можем получить,
[12:27.000 --> 12:30.000]  если будем поддерживать стратегию P, начиная с этого момента времени.
[12:30.000 --> 12:33.000]  Вот эта политика P у нас разная здесь,
[12:33.000 --> 12:36.000]  поскольку значит, что каждое следующее состояние,
[12:36.000 --> 12:39.000]  каждое следующее действие выбирается согласно политике P
[12:39.000 --> 12:42.000]  от предыдущего действия, а следующее действие
[12:42.000 --> 12:45.000]  S H плюс один, допустим, оно будет согласно
[12:47.000 --> 12:49.000]  этому ядру
[12:50.000 --> 12:54.000]  для предыдущего состояния S H
[12:54.000 --> 12:57.000]  и действия H, которое выбирается из политики.
[12:59.000 --> 13:01.000]  Вот концепт какой-то такой.
[13:01.000 --> 13:04.000]  То есть мы ходим, мы играем, мы выбираем шаги,
[13:04.000 --> 13:07.000]  и мы хотим правильное правило выбора шагов
[13:07.000 --> 13:12.000]  для каждого возможного состояния, для каждого шага внутренней партии.
[13:15.000 --> 13:18.000]  Вместе с value функцией,
[13:18.000 --> 13:21.000]  функцией полезности, можно найти action value функцию,
[13:21.000 --> 13:23.000]  то есть Q функцию.
[13:23.000 --> 13:26.000]  Q функция – это объект, который закрепляет
[13:26.000 --> 13:29.000]  на шаге H не только состояние, но еще и действие.
[13:29.000 --> 13:32.000]  То есть мы считаем, что получилось бы дальше,
[13:32.000 --> 13:35.000]  если бы мы играли согласно политике P,
[13:38.000 --> 13:41.000]  и какую сумму награду мы получили в этом случае.
[13:42.000 --> 13:45.000]  Опять же, задача найти политику,
[13:45.000 --> 13:48.000]  чтобы максимизировать все, что угодно.
[13:50.000 --> 13:53.000]  Value функцию и Q функцию можно вместе
[13:53.000 --> 13:56.000]  клеить в неком смысле помощи так называемых уровней Белмана.
[13:56.000 --> 13:59.000]  То есть Q функция – это награда на текущем шаге,
[13:59.000 --> 14:02.000]  но оно и понятно, почему оно должно быть наградой на текущем шаге,
[14:02.000 --> 14:05.000]  потому что у нас здесь первая слагаемая в неком смысле закреплена.
[14:05.000 --> 14:09.000]  То есть reward на F при H3 равном H маленькому
[14:09.000 --> 14:12.000]  он у нас уже известен,
[14:12.000 --> 14:15.000]  потому что мы знаем текущий состояние, текущие действия.
[14:15.000 --> 14:18.000]  А все, что дальше, на самом деле,
[14:18.000 --> 14:21.000]  определяется под этим от ожидания.
[14:21.000 --> 14:24.000]  То есть вот это вот ПВ,
[14:24.000 --> 14:27.000]  по-хорошему В должны быть скобки вокруг вот этого ПВ.
[14:27.000 --> 14:30.000]  Может это действие, это марковская ядра на функцию.
[14:30.000 --> 14:33.000]  В таком от ожидания, это Value функция в следующем состоянии,
[14:42.000 --> 14:45.000]  которые мы уже знаем.
[14:45.000 --> 14:48.000]  Я не очень понимаю,
[14:48.000 --> 14:51.000]  то ли у меня проблемы с интернетом,
[14:51.000 --> 14:54.000]  то ли Даниил подвис, коллегия,
[14:54.000 --> 14:57.000]  если вы меня слышите, можете сказать.
[14:57.000 --> 15:00.000]  У вас так же ощущения?
[15:00.000 --> 15:03.000]  Я слышу вас, я не слышу.
[15:03.000 --> 15:06.000]  Да, Даниил,
[15:06.000 --> 15:09.000]  у вас какие-то проблемы со связи?
[15:09.000 --> 15:14.960]  какие-то проблемы со связи? мы вас сейчас вы отрубились в видео. вы что-то притормозил.
[15:14.960 --> 15:22.320]  можете повторить и снова расшарить презентацию? да конечно. вот Bellman equation,
[15:22.320 --> 15:31.160]  где вы начали объяснять марковский оператор. вот тут было уже все, не слышно. да конечно.
[15:31.160 --> 15:38.800]  то есть вот этот оператор это так называемые действия ядра на функцию. то есть по сути
[15:38.800 --> 15:44.680]  говоря это вот. можно просто смотреть на эту формулу. то есть мы сэмплируем действие s'
[15:44.680 --> 15:52.360]  нашего ядра и усредняем все возможные состояния функции. то есть вот эта штука это от ожидания по
[15:52.360 --> 15:57.640]  следующему состоянию при условии, что мы сейчас сидим в состоянии s и делаем действие a.
[15:57.640 --> 16:04.840]  такое усреднение по будущему состоянию. то есть если бы у нас не было бы действий,
[16:04.840 --> 16:11.880]  то это было бы некое стандартное действие теории марковских цепей. то есть от ожидания по
[16:11.880 --> 16:17.080]  следующему состоянию. так как у нас в следующем состоянии зависит не только от состояния
[16:17.080 --> 16:23.040]  начать действия, то получается, немножко не стандартная нотация, что у нас здесь два
[16:23.040 --> 16:31.840]  аргумента есть. то есть мы по сути некому смыслу их умножаем. это ядро p на f и смотрим точки s'
[16:31.840 --> 16:40.680]  примерно выглядит так. то есть на самом деле эта штука это в точности некоторое следствие
[16:40.680 --> 16:51.840]  свойства условного манемического ожидания. это можно очень трудно доказать. то есть будем
[16:51.840 --> 16:57.120]  доказывать по индукции. наша биозидическая структура позволяет нам легко делать индукции.
[16:57.120 --> 17:06.160]  то есть для h плюс один мы здесь все определяем равно нулю. и в функцию и ко функцию. просто по
[17:06.160 --> 17:11.520]  определению. потому что если у нас h маленькое равно h плюс один, у нас сумма по нулю слагаемых,
[17:11.520 --> 17:21.280]  она будет конечно равно нулю. поэтому для h плюс один все верно. теперь мы записываем
[17:21.280 --> 17:26.320]  определение ко функции. а просто в таком от ожидания. затем по линейному от ожидания выносим
[17:26.320 --> 17:35.360]  первое слагаемое. у нас остается вот такая сумма. а затем мы можем сказать, что у нас просто
[17:35.360 --> 17:41.720]  мот ожидания, а давайте еще внутрь запишем условно мот ожидания. это ничего не изменит, потому что мы
[17:41.720 --> 17:49.520]  будем от ожидать по этому h плюс один. но мы можем заметить, что вот эта штука это в точности
[17:49.520 --> 18:03.440]  валют функции. по определению. поэтому можно записать это как валют функцию и получить, что ко функции это
[18:03.440 --> 18:10.640]  reward плюс мот ожидания валют функции в следующем шаге. а действие ядра мы приняли ровно так. то есть
[18:10.640 --> 18:17.840]  можно на самом деле об этом думать именно вот так, что вот это действие pvp с точностью вот
[18:17.840 --> 18:27.120]  такая запись на предпоследней строчке. мот ожидания vp h плюс один при условии, что предыдущее
[18:27.120 --> 18:39.760]  состояние это sh и действие это a h. то есть на втором уровне Боумана, который определяет vp через q,
[18:39.760 --> 18:44.960]  он тоже пишет довольно просто. мы просто используем power property, то есть
[18:44.960 --> 18:52.640]  кископическое свойство условно мот ожидания. просто здесь добавляем на h, потому что почему бы и нет,
[18:52.640 --> 19:00.880]  мы все равно по нему промот ожидаем и замечаем, что внутри мы получаем в точности q функцию. после этого
[19:00.880 --> 19:07.680]  мы видим, что наша q функция при условии закрепленного состояния, следующее действие
[19:07.680 --> 19:16.800]  определяется однозначно из политики p. мы получаем, что v функции вращается через q функцию. то есть вот эта
[19:16.800 --> 19:23.960]  запись, это в точности некая fancy запись условных мот ожиданий, чтобы v функции вращались через q функции
[19:23.960 --> 19:30.680]  наоборот. это мега удобно. это мега удобно еще по причине того, что можно сделать некое расширение
[19:30.680 --> 19:36.840]  этой штуки для оптимальной политики. то есть оптимальная политика, она максимизирует value функцию
[19:36.840 --> 19:45.560]  для всех возможных состояний, для всех возможных шагов h. для конечного размера, конечного множества
[19:45.560 --> 19:50.800]  состояний и построить довольно не трудно, нужно просто решить вот эти оптимальные уровни bellman.
[19:50.800 --> 20:02.120]  то есть здесь опять же наша политика, это будет то действие, на котором этот максимум достигает.
[20:02.120 --> 20:07.240]  то есть что такое оптимальный уровень bellman? у нас q функция пришла через оптимальную v функцию
[20:07.240 --> 20:14.320]  в следующем шаге точно так же, как и в обычном уровне bellman. но v функция считает через q функцию как максимум.
[20:14.320 --> 20:18.560]  то есть мы выбираем наиболее жадное действие, взаимодействие на наши оценки на q функцию,
[20:18.560 --> 20:24.520]  которая приносится на максимальную награду. и утверждается, что эти уравнения ровно определяют
[20:24.520 --> 20:34.600]  оптимальную v функцию. если не должно быть этого индекса h, это некая опечатка. но идея такая, что
[20:34.600 --> 20:40.440]  просто если мы возьмем максимум вместо взятия в политике, то мы по всей этой конструкции можем
[20:40.440 --> 20:46.760]  построить и оптимальную политику, и у нас есть оптимальная q и v функция, что замечательно. это
[20:46.760 --> 20:52.800]  доказывается используя обычный уровень bellman довольно несложно. то есть первое, что мы можем заметить,
[20:52.800 --> 21:00.300]  что наша q функция это вот supremo по всем политикам. наша q функция для p. а для q p можно
[21:00.300 --> 21:07.400]  поставить оптимальную уровню bellman. тогда все вынесется сюда направо. после чего мы можем
[21:07.400 --> 21:14.000]  воспользоваться, допустим, тиремой Bepa-levia, потому что все у нас здесь ограничено, все у нас
[21:14.000 --> 21:19.720]  замечательно. у нас v функция ограничена, поэтому supremo можно передащить само дожидание.
[21:19.720 --> 21:33.560]  с действием midrp. а это supremo это в точности p в s звездой. далее, чтобы получить v функцию как
[21:33.560 --> 21:39.480]  максимум функций, просто уходим с тем, что если мы максимизируем политику, то это то же самое,
[21:39.480 --> 21:48.000]  что for bellman это просто qp h от s и некоторое действие в основном на политике. давайте по
[21:48.000 --> 21:53.040]  нему промаксимизируем, по действию, которое у нас здесь есть. после чего у нас получится два
[21:53.040 --> 21:59.960]  максимума, можем их переставить и получить точность оптимальной уровни bellman. то есть,
[21:59.960 --> 22:07.520]  казалось бы, все хорошо, просто решаем уравнение, начиная с конца, потому что v с звездой h плюс 1
[22:07.520 --> 22:15.320]  большого, оно известно, все замечательно. но проблема в том, что мы не знаем ядро. и как бы все
[22:15.320 --> 22:21.760]  интересные моменты обучения сцепления начинаются как раз с момента, что дети мы не знаем ядро и мы не
[22:21.760 --> 22:25.320]  умеем из него допущенно сэмплировать. то есть мы не умеем сэмплировать для произвольного
[22:25.320 --> 22:31.840]  nsa. ну давайте посэмплируем, получим хорошую оценку и решим уровень bellman. замечательный подход,
[22:31.840 --> 22:39.000]  получается в правильном порядке. но в нашем случае мы будем рассматривать так называемое
[22:39.000 --> 22:43.800]  анонимное обучение сцепления, когда можно выходить только траекториями. у нас есть некая политика
[22:43.800 --> 22:49.320]  api, мы можем по ней пройтись траектории, получить какие-то данные, в основе этих данных улучшить
[22:49.320 --> 22:58.320]  политику. мы не можем допустим обученно улучшить качество ядра для какого-нибудь
[22:58.320 --> 23:03.960]  отдаленного действия, в которое мы редко были. нужно как-то придумать сначала политику, которая его
[23:03.960 --> 23:13.320]  посетит, а только потом его как-то использовать. то есть вот в этом framework у нас есть некая
[23:13.640 --> 23:18.760]  политика api. то есть api 1 это некая изначальная политика. допустим, он случайно действует,
[23:18.760 --> 23:24.440]  потому что мы ничего не знаем о среде. на втором шаге мы уже какие-то знания получили о среде,
[23:24.440 --> 23:30.560]  мы можем ее немножко улучшить и так далее. а в чем главный пафос всей этой обосновки в том,
[23:30.560 --> 23:35.400]  что мы должны, эта политика api не просто делать максимально жадные действия относительно текущих
[23:35.400 --> 23:41.720]  представлений о среде, но еще как-то правильно исследовать нашу среду. то есть у нас получается
[23:41.720 --> 23:46.640]  некий trade-off между тем, что эта политика api, с другой стороны, должна быть довольно хорошая,
[23:46.640 --> 23:51.520]  потому что у нас в нашей мере качества, в нашем rig-rate, она есть прямо вот невязка
[23:51.520 --> 23:57.160]  между оптимальной v-функцией и v-pt. но и при этом мы должны исследовать, чтобы следующие политики
[23:57.160 --> 24:05.400]  были еще лучше. прежде чем как-то погрузиться в эту тему, как раз таки Стофим говорит в начале,
[24:05.400 --> 24:10.960]  что доклад было правильно назвать применение констрации меры обучения с подкреплением. я напомню
[24:10.960 --> 24:17.400]  некоторые классические результаты констрации меры. первый классический результат, который нам
[24:17.400 --> 24:24.280]  понадобится, это неравенство зума кердинга. то есть пусть у нас есть некая последовательность моркови
[24:24.280 --> 24:33.440]  разности. то есть нечто с конечным от ожидания, что измеримость некой фильтрации заранее закрепленной,
[24:33.440 --> 24:42.240]  и мот ожидания этого уйн при условии предыдущей фильтрации равно 0. то есть, скажем, если уйн просто
[24:42.240 --> 24:49.280]  аид исключенной величины строевого мот ожидания, то это отлично подходит. и для таких мобилоразностей
[24:49.280 --> 24:54.720]  можно написать неравенство зума кердинга. то есть если все уйн ограничены, то на самом деле они
[24:54.720 --> 25:01.920]  довольно хорошо констрируются от ожидания, которого было 0. то есть в точности это здесь
[25:01.920 --> 25:11.360]  подтверждает, что вероятность того, что сумма ук будет больше 1 на t, она шкалируется как квадрат,
[25:11.360 --> 25:16.640]  то есть некая опечатка. то есть она шкалируется как квадраты этих штук сумма квадратов,
[25:16.640 --> 25:26.720]  если у нас было бы среднее, и еще мы поделим все на n, и допустим у граничной единицей изначальная,
[25:26.720 --> 25:33.880]  то есть пусть yk это 1 делить на n некой другие, xk скажем, тогда эти ck квадрата это будет 1 делить на n
[25:33.880 --> 25:40.400]  квадрат и сумма n штук. у нас получается специальный констракт. к сожалению, я пропустил квадрат,
[25:40.400 --> 25:46.720]  довольно важен, но в следующем неразистое оно есть, что при помощи неразистое суммы кердинга
[25:46.720 --> 25:52.800]  можно доказать некий более могучий факт, что если у нас есть некая функция, которая уничтожает
[25:52.800 --> 25:59.160]  некоторому свойство граничной разности bound difference property, то говорит то, что функция не
[25:59.160 --> 26:04.320]  сильно чувствительна в каждой отдельной координате. то есть меняя одну координату, мы можем изменить
[26:04.320 --> 26:09.200]  значение функции не больше, чем на некое cit, которое имеет точно такую же роль, как и здесь, что у
[26:09.200 --> 26:17.840]  нашей yk не больше, чем ck, то есть есть примерно такая же штука. тогда эта функция f отлично консервируется
[26:17.840 --> 26:24.320]  в своем отожидании, тоже экспоненциально, если мы все у средней провели. если наши cit
[26:24.320 --> 26:31.840]  какие-то маленькие, допустим 1 на n, то у нас получается правильная констракция.
[26:31.840 --> 26:42.720]  вероятно говоря, используя этот замечательный факт, неразистое могдярмидо, мы можем получить
[26:42.720 --> 26:49.440]  вот такой результат на констракцию для 1 норм. давайте к нему вернемся чуть позже, чтобы было
[26:49.440 --> 26:54.920]  понятно, зачем он нужен. то есть давайте вернемся, собственно, к количеству крепления.
[26:54.920 --> 27:02.120]  перед нами стоит задача исследования, как я говорил ранее, что у нас в нашем пределении regret
[27:02.120 --> 27:12.920]  у нас сумма ошибок нашей политики для каждой из этих политик pt. в чем проблема в том, что если
[27:12.920 --> 27:18.320]  мы будем шагать pt слишком жадно, то следующие политики будут плохие, и эта сумма regret будет
[27:18.320 --> 27:24.360]  большой. сумма этих невязок. если мы будем исследовать в начале много, чтобы потом была хорошая
[27:24.360 --> 27:30.680]  политика, то в начале у нас будет большие эти невязки. нужно как-то это правильно балансировать.
[27:30.680 --> 27:39.400]  есть вариант, как балансирование происходит неправильно, допустим, есть action grid исследования.
[27:39.400 --> 27:45.000]  то есть action жадно, у нас есть некая оценка на q-функция, допустим, получена уровень bellman,
[27:45.000 --> 27:50.880]  если мы вместо изра поставим его at. какая-нибудь оценка на q-функцию правильная, которая по моему
[27:50.880 --> 28:00.040]  ожиданию как хорошую q-функцию. и мы сравниваем, что epsilon действует случайно, чтобы исследовать
[28:00.040 --> 28:07.320]  среду. 1-epsilon действует жадно, согласно текущим председаниям от 3d. тогда если даже эти epsilon
[28:07.320 --> 28:13.760]  неким правильным образом уменьшать в зависимости от t, то regret получается сублинейный, что уже неплохо,
[28:13.760 --> 28:23.760]  но не самый лучший. и более того, здесь на самом деле спрятана вот под этим большим экспоненциальной
[28:23.760 --> 28:30.320]  зависимость от изменения состояния. если мы возьмем такую среду, то есть среда так называемая, и много
[28:30.320 --> 28:37.280]  названий этой среды, это можно назвать chain, может быть reverse swim. но в чем идея? мы находим состояние с 1. у нас
[28:37.280 --> 28:41.560]  есть два действия. у нас есть действие пойти налево, что у нас всегда удается, либо действие пойти
[28:41.560 --> 28:51.160]  направо. пойти направо у нас получается с трудностью 1.9 на 1.7. то есть мы можем проиграть, тогда мы
[28:51.160 --> 28:59.360]  снова вернемся налево. правильная стратегия это всегда идти направо. мы идем-идем-идем-идем-идем направо,
[28:59.360 --> 29:06.200]  пока не назовем это наградой 1. если он отдаст его назад, неважно, мы еще раз делаем. еще проблема
[29:06.760 --> 29:14.440]  потому что на сцене с2 он будет пробовать пойти налево. все-таки с какой-то вероятностью. он будет
[29:14.440 --> 29:22.520]  пробовать пойти налево и тем самым сдаться свой прогресс по достижению этой награды. даже если он хоть
[29:22.520 --> 29:26.440]  один раз дойдет до правой награды и узнает, что там есть хорошая награда и больше идти некуда,
[29:26.440 --> 29:34.520]  начнут всегда идти туда, то F2D будет нас сбивать с пути, будем очень часто идти назад. и таким
[29:34.520 --> 29:41.640]  образом у нас получается очень плохое количество состояний. даже для F равно 100 это уже что-то не
[29:41.640 --> 29:49.720]  очень приемлемо. и вот такое вот балансировать исследование и использование данных получается
[29:49.720 --> 29:56.480]  довольно плохим. как делать правильно? на самом деле правильно неким образом использовать
[29:56.480 --> 30:02.560]  так называемые оптимистические оценки ко функции. то есть использовать принципы оптимизма
[30:02.560 --> 30:08.720]  предсовывая неопиневеленность. то есть пусть наши средние оценки наших ко функций обозначены синей
[30:08.720 --> 30:16.400]  чертой. то есть действовать жадно действует согласно максимальной этой ко функции. если мы будем действовать
[30:16.400 --> 30:22.880]  жадно, надо шагать по этой синей черте. мы не постепенно сходимся к кусо-звездой каждым новым шагом,
[30:22.880 --> 30:33.560]  но мы на самом деле никогда не будем после этого дергать за левую ручку. у нас есть 3 действия. для
[30:33.560 --> 30:41.520]  каждой из них есть некая uppercut and bounce. то есть некая верхняя квантиль. то есть мы с большой вероятностью
[30:41.520 --> 30:48.240]  нашу реальную кусо-звездой находим где-то ниже этой uppercut and bounce. и ее другой важный свой
[30:48.240 --> 30:53.760]  то, что она постепенно концентрируется вокруг кусо-звездой. то есть с каждым дополнительным
[30:53.760 --> 30:59.520]  использованием этого действия наши границы, с левой справа, начинают схлопываться вокруг
[30:59.520 --> 31:06.320]  кусо-звездой. но в текущий момент у нас есть такие вот такие оценки. данных мало, имею что имеем.
[31:06.320 --> 31:13.280]  если мы будем действовать жадно согласно среднему, то мы никогда не будем использовать первое действие.
[31:13.280 --> 31:18.680]  просто потому что текущая оценка среднего она точно меньше чем оптимальная кусо-звездой,
[31:18.680 --> 31:26.600]  к которой мы будем сходить. здесь даже вот здесь немножко не поместилась счет надпись,
[31:26.600 --> 31:33.320]  но здесь нижняя оценка на вот это второе действие, она лежит выше этого среднего. то есть если мы
[31:33.320 --> 31:38.880]  будем действовать согласно среднему, то мы наверняка будем пытаться делать только второе действие,
[31:38.880 --> 31:47.760]  тем самым у нас будут плохие оценки. то есть у нас наше UCB будет постепенно уменьшаться кусо-звездой.
[31:47.760 --> 31:55.880]  если мы будем действовать согласно среднему, то у нас сейчас схлопливается во втором действии,
[31:55.880 --> 32:02.400]  мы будем только лишь его дергать. возможно треть. но третья тоже у него средняя, тоже ниже кусо-звездой
[32:02.400 --> 32:07.840]  для второго. поэтому средние будут постепенно опускаться для второго действия. и мы никогда не
[32:07.840 --> 32:13.320]  будем дергать ни первого ни третья, хотя там реально кусо-звездой она больше. то есть нам следовало
[32:13.320 --> 32:19.760]  бы или пробовало первое и третье действия. если мы действуем согласно UCB, то мы на самом деле
[32:19.760 --> 32:26.040]  некогда не можем пролететь мимо оптимального действия. то есть они будут постепенно сконцентрироваться
[32:26.040 --> 32:31.960]  в группу кусо-звездой, но UCB никогда не может стать меньше, чем кусо-звездой просто по
[32:31.960 --> 32:40.240]  строению. поэтому мы не можем его пролететь. некого смысла. оно будет постепенно сходиться в кусо-звездой,
[32:40.240 --> 32:50.160]  но при этом, если вот для второго действия у нас какой-то момент, наша UCB SA2, она станет ниже,
[32:50.160 --> 32:56.240]  чем кусо-звездой для первого. просто потому что оно должно сходиться. тогда мы будем дергать за первое действие.
[32:56.240 --> 33:05.240]  вот. то есть концепт немножко запутанный, а в предыдущем примере это выглядит примерно так.
[33:05.240 --> 33:15.440]  то есть давайте сначала определим некую в функцию для некого ядра p-штрих. то есть это ядро
[33:15.440 --> 33:21.840]  p-штрих может отличаться от того, с чем мы реально играем, но для некоторой модели p-штрих мы делим
[33:21.840 --> 33:29.840]  функцию вот так. тогда мы хотим действовать так, а поделим для моделей некое дополнительное множество.
[33:29.840 --> 33:39.360]  такое, что там содержится реальная p. это близко к этой идее, что мы хотим, чтобы наша кусо-звездой всегда
[33:39.360 --> 33:47.320]  была ниже этого UCB, что у нас все возможные функции, которые мы имеем в виду, лежат здесь. где-то вот здесь.
[33:47.320 --> 33:56.320]  тогда что мы можем сделать? тогда мы можем играть в политику следующим образом. мы находим некое
[33:56.320 --> 34:08.480]  самооптимистичное ядро p-штрих из множества, для которого вот в функции для текущей политики максимально.
[34:08.480 --> 34:14.360]  и затем играем вот эту оптимальную политику для самого оптимистичного ядра. то есть из-за того,
[34:14.400 --> 34:22.880]  что в этом множестве живет просто p, то в функция в новой модели она будет всегда больше, чем оптимальная
[34:22.880 --> 34:34.360]  в функции. оптимальная в функции для модели просто p или реальной модели. вот. то есть концепция
[34:34.360 --> 34:38.920]  какой-то такой, что у нас есть некое множество моделей, в котором точно содержится p. и мы
[34:38.920 --> 34:43.800]  пытаемся играть вот этой верхней квантили всегда. мы пытаемся выбирать действия, которые оптимистичны,
[34:43.800 --> 34:48.680]  которые жадно согласны верхней квантили. на этой картинке это будет соответствовать тому, что сначала
[34:48.680 --> 34:53.160]  мы будем дергать, допустим, за третий ручек. вот эта вот верхняя граница будет постепенно сужаться,
[34:53.160 --> 34:59.160]  она будет сужаться, сужаться, сужаться. потом она перескочит, станет ниже, чем вторая граница.
[34:59.160 --> 35:05.080]  будем будем использовать действия А2. будут уменьшаться, уменьшаться, уменьшаться. стоит меньше, чем третья.
[35:05.080 --> 35:11.080]  они друг друга будут постепенно выбирать только эти два действия. а затем они станут меньше, чем вот это
[35:11.080 --> 35:21.600]  UCB. чем UCB SA1. а после этого мы будем использовать это действие. оно тоже будет как-то уменьшаться. но в
[35:21.600 --> 35:30.000]  конечном итоге, после того, как мы много надергаем за А2 и А3, использовать эти действия, то их UCB станут
[35:30.000 --> 35:36.720]  меньше, чем UCB SA1. и мы будем точно все знать, что оптимальное действие было первым.
[35:36.720 --> 35:44.560]  мы пытаемся так сходиться сверху, чтобы совершенно не пропустить правильное действие. в этом случае
[35:44.560 --> 35:52.160]  мы стараемся не пропустить правильную политику. есть ли вопросы в текущем моменте?
[36:01.000 --> 36:11.480]  я понимаю, вопросов нет. тогда давайте разберемся, как строить этот CDELTA. звучит все как-то
[36:11.480 --> 36:16.460]  немножко странно. есть какие-то додавительные множества, как его строить. но в самом деле его можно
[36:16.460 --> 36:24.400]  построить следующим образом. давайте рассмотрим некая плесклышка. это самая разумная оценка модели
[36:24.400 --> 36:34.480]  по текущим данным. то есть у нас как-то наш агент ходил по нашему MDP. он в действии SA переходил
[36:34.480 --> 36:42.120]  в S3 какое-то количество раз. это НТ S3 при условии SA. давайте просто, чтобы посчитать это под
[36:42.120 --> 36:52.720]  крышкой, просто поделим часов переходов из SA в S3 на часов всех посещений SA. а если мы ничего не
[36:52.720 --> 37:00.080]  знаем про эту текшен пару SA, то просто будем ходить случайно. просто наша под крышкой будет как-то
[37:00.080 --> 37:08.000]  случайно нас разграстывать. на самом деле можно еще также показать, что вот эта оценка это будет
[37:08.000 --> 37:13.840]  оценка максимального продоподобия при известных данных о переходах. при известных вот этих вот
[37:13.840 --> 37:24.240]  SHT'. то есть мы проходили какой-то T игр и после этого мы можем написать такую оценку
[37:24.240 --> 37:30.480]  максимального продоподобия на P с крышкой. если мы будем сходить согласно допустим оптимального уровня
[37:30.480 --> 37:35.880]  BOMA на согласно P с крышкой, потому что когда сильно его, нам могут допустим не повести
[37:35.880 --> 37:46.680]  в эту ситуацию. то есть у нас P с крышкой для какой-то политики 2 ВП для этой модели будет большим,
[37:46.680 --> 37:53.400]  кем-то поврется, потому что ему повезло так. но вообще говоря, так не бывает. то есть если мы
[37:53.400 --> 38:00.200]  будем шагать больше, то подсветов это среднее станет меньше. но мы никак не можем влиять условно
[38:00.200 --> 38:05.080]  повезло-неповезло с темпами из модели. они уже есть. по всей данной мы больше ничего не можем сделать.
[38:05.080 --> 38:12.600]  поэтому можно верить на самом деле модель так, что у нас эта P с крышкой это некая самая правильная
[38:12.600 --> 38:19.240]  вещь, самая правильная модель. мы ничего больше не можем сказать. но давайте скажем, что реально
[38:19.240 --> 38:26.520]  модель с крышкой недалеко находится. как бы P с крышкой должно сходиться к P, если мы стоплируем из SA,
[38:26.520 --> 38:34.040]  если мы находимся в состоянии SA, находимся в состоянии S, дергаем ручку A, выбираем действие A,
[38:34.040 --> 38:42.040]  то мы получим в состоянии S штрих. и вот эта вещь при бесконечном частотопочине должна сходить
[38:42.040 --> 38:49.560]  к правильной модели. значит скорее всего, наше правильное дурительное множество должно нагреть
[38:49.560 --> 38:56.160]  где-то вокруг нашего P с крышкой. и это по факту формализуется вот с помощью вот этой леммы.
[38:58.160 --> 39:06.280]  то есть наше множество моделей, это как раз-таки множество таких переходных ядер P штрих, для всех
[39:06.280 --> 39:16.880]  состояний P-P с крышкой не больше чем вот эта B дельта, где beta delta оно скалируется как 1
[39:17.080 --> 39:23.720]  на корень Z. то есть постепенно это множество будет сужаться. и при этом мы точно знаем,
[39:23.720 --> 39:33.040]  что внутри этого множества живет настоящая модель. и ровно для этого нам нужна концентрация
[39:33.040 --> 39:41.760]  для 1 норма. мы хотим показать, что реальная модель, мотоожидание D P с крышкой будет удовлетворять
[39:41.760 --> 39:51.840]  вот такому нерайд. в этом моменте я хочу вернуться к пропущенному моменту, как писать концентрацию
[39:51.840 --> 39:58.200]  на эту послышку, почему вот эта B дельта не гнустит правильно. если к этому моменту какие-то вопросы.
[40:12.760 --> 40:16.920]  окей, я понимаю, вопросов нет скорее всего. либо все понятно, либо ничего не понятно.
[40:16.920 --> 40:21.240]  в обоих случаях я возможно помочь ничем не могу прямо сейчас.
[40:21.240 --> 40:30.600]  давайте я еще раз расскажу этот принцип, как работает агритмический рельт.
[40:30.600 --> 40:40.560]  мы на самом деле можем сказать так, что мы по этому множеству C дельта T, это не это выпукло множества
[40:40.560 --> 40:47.040]  в нашем среднем случае. поэтому вот эту максимизацию проводить легко. ну как легко?
[40:47.040 --> 40:53.880]  не фундаментально сложно. в предвестной модели можно считать B P и C дельта B. поэтому вот эту
[40:53.880 --> 41:01.440]  максимизацию можно как-то делать. как ее делать? вопрос отдельный. этот алгоритм, он из 60-го года,
[41:01.440 --> 41:07.560]  уже вышло очень много чего гораздо лучше, но он довольно простой для качества, поэтому я
[41:07.560 --> 41:13.760]  выбрал его. то есть выполнять его на практике это, конечно, абсолютно бессмысленно-беспощадно занятие.
[41:13.760 --> 41:24.120]  но мы можем посчитать вот вот некого максимум взять по этому множеству и затем на тему
[41:24.120 --> 41:31.200]  оптимальную политику. можно наоборот. можно максимум переставить. то есть мы для панели P4 находим
[41:31.200 --> 41:36.680]  оптимальную политику на уровне Балман. то есть мы находим ВС звездой H от действия S
[41:36.680 --> 41:49.520]  при условии P штрих. при этой модели известной. а затем мы находим такую модель P штрих,
[41:49.520 --> 41:54.920]  которая максимизирует ВС звездой. наверное даже более правильно думать об этом так,
[41:54.920 --> 41:59.920]  что представить эти два максимума местами, а потом взять оптимальную политику для вот
[41:59.920 --> 42:05.440]  этой самой оптимистичной модели, для которой максимизируется ВС звездой. это считается
[42:05.440 --> 42:12.560]  примерно вот так политика. зачем нам нужен оптимизм? так называемый. это будет видно из
[42:12.560 --> 42:23.040]  качества частично. по факту это принцип того, что мы не хотим пройти мимо ВС звездой при уменьшении
[42:23.160 --> 42:30.560]  давительных интервалов на кустах звездой. то есть принцип говорит какой-то такой. и качество
[42:30.560 --> 42:38.680]  множества цдельта мы выбираем вот такой или один шар вокруг оценки максимально правдоподобия.
[42:38.680 --> 42:47.840]  и вот результат, что оно концентрируется скоростью 1 дизель на корень СН. там еще есть S плюс
[42:47.880 --> 42:55.440]  алгоритм. вот это S плюс алгоритм, оно как раз таки получается из не раз это концентрация для 1 нормы,
[42:55.440 --> 43:03.080]  которая я вот пропустил вот здесь. вот оно. если его формулировать, формально оно выглядит так.
[43:03.080 --> 43:11.880]  пусть у нас есть n векторов, сенсированных, независимых, ограниченных в 1 норме. почему бы и нет?
[43:11.880 --> 43:18.760]  тогда для каждого дельта, хотя бы 1 минус дельта, у нас верного такого не нравится.
[43:18.760 --> 43:30.360]  раз на самом деле у нас разделена ошибка такая фуктуация, и это по сути говоря будет оценком от
[43:30.360 --> 43:40.320]  ожидания. не будет ли тут под алгоритм еще фактора, зависящего от n? то есть там алгоритм n на дельта,
[43:40.320 --> 43:46.520]  вот именно такое неравенство имеет место или все-таки под алгоритмом n должно быть? вот у меня
[43:46.520 --> 43:52.000]  какое-то ощущение из того, что и то, что я видел, мне попадалось. там все-таки под алгоритм еще n
[43:52.000 --> 43:59.040]  должно быть. может такое быть? конкретно в этом образцом неравенства кажется нет. давайте проверим
[43:59.040 --> 44:04.440]  сейчас, но мне казалось, что здесь его нет, просто потому что алгоритм 1 на дельта это просто
[44:04.440 --> 44:11.960]  макдярмид. в макдярмиде никакого бытия на n под дельта не резет, но конкретно в применении его для
[44:11.960 --> 44:16.880]  алгоритма crl действительно там будут какие-нибудь n под этим алгоритмом, потому что мы будем делать
[44:16.880 --> 44:23.120]  мучение union bound. в этом будет чуть позже. да-да-да, это я понимаю, я просто к тому, что вообще говоря,
[44:23.120 --> 44:27.000]  иногда такие неравенства называют неравенство хейовдинга в гильбертовом пространстве или не в
[44:27.000 --> 44:32.160]  гильбертовом банаховом пространстве, и ими занимались, например, немировские юди, у них есть такого
[44:32.160 --> 44:38.320]  типа неравенства, и вот у них, если я правильно помню, за счет того, что это все-таки 1 норма,
[44:38.320 --> 44:46.160]  и это как бы не гильбертово пространство, там появлялись какие-то такие логарифмы от размерности,
[44:46.160 --> 44:51.680]  но это я сейчас, так сказать, 100% говорить не буду, но если вы знаете, как это доказать с помощью
[44:51.680 --> 44:59.800]  макдярмида, то здорово. может быть, действительно и нет, но так с входа не очевидно. с входа не очевидно,
[44:59.800 --> 45:05.720]  давайте посмотрим, давайте сейчас проведем немножко погальшества, время чуть-чуть есть,
[45:05.720 --> 45:13.480]  что мы хотим проверить, что на самом деле наша 1 норма удовлетворяет этому свойству 1, этому
[45:13.480 --> 45:21.320]  определению, в самом деле, на скопорте, отчасти на векторах х. если мы это проверим, то у нас
[45:21.320 --> 45:25.960]  автомический очконтраст вокруг среднего. после этого остается отдельно значка, как оценим среднее,
[45:25.960 --> 45:35.320]  но именно от дельты ничего больше зависеть не будет. возможно, я здесь мог упустить что-нибудь в этой
[45:35.320 --> 45:41.320]  экспоненте, но давайте проверим. давайте рассмотрим вот эту f, как я говорил, как просто эту 1 норму.
[45:41.320 --> 45:49.800]  надо сходить по не разу треугольника любого и другого х, то есть если мы поставим эти x не случайными,
[45:49.800 --> 45:58.600]  у этой функции f, после того, что вот это вот f от x1 и далее xn, это вот такая сумма. что здесь?
[45:58.600 --> 46:06.840]  можно здесь прибавить и отнять под нормой x'k. почему бы и нет? в таком случае эта сумма x1 xn,
[46:06.840 --> 46:18.600]  то есть мы прибавили вот такую разность xk'-xk под нашей нормой.
[46:18.600 --> 46:27.000]  если мы используем не разу треугольника, вытащили отдельно слагаемую разность. если мы вычтем xk,
[46:27.000 --> 46:36.440]  то в точности получится сумма всех, кроме xk, еще прибавим xk'. то есть точка ниже. вот эту штуку
[46:36.440 --> 46:40.840]  мы можем еще раз использовать не разу треугольника. используется свойством того, что все наши xk-ты
[46:40.840 --> 46:52.280]  не больше чем b по 1 норме. и собственно говоря получается вот так. если у нас есть оценка не с 1
[46:52.280 --> 46:58.600]  нормы, а с 2 нормы, то чтобы перевести это в неравенство по 1 норме, нужно запустить размерность. можно где-то так.
[46:58.600 --> 47:09.240]  и после чего мы можем просто применить макдярмиду к этому всему. то есть у нас есть вполне дифференс попортия.
[47:09.240 --> 47:18.800]  когда у нас надо 2b для всех x, тогда мы можем перевести эту экспоненту, то есть что это экспонент равно
[47:18.800 --> 47:33.360]  дельте и в таком случае получить 2t квадрат делить на 2bn равно дельте. ну и казалось бы тут
[47:33.360 --> 47:43.480]  никаких проблем нет. да согласен. и получается здесь такой фуктационный член. еще нужно
[47:43.480 --> 47:51.800]  мотождание оценить. а мотождание можно оценить заменив 1 норму на 2 норму. заменив неравенство
[47:51.800 --> 47:57.720]  эквивалентности норм. то есть как известно в конечном мерном пространстве все нормы эквивалентны и более того
[47:57.720 --> 48:03.520]  можно выписать константы явно при известной размерности. на данном случае мы знаем, что 1 норма не больше чем
[48:03.520 --> 48:10.680]  корень из d размерности на 2 норма. поэтому сначала подменяем 1 норму на 2 норму и
[48:11.680 --> 48:20.400]  еще применяем ентенна сразу же. то есть мы загоняем все это под корень. но
[48:20.400 --> 48:26.400]  теперь квадрат 2 нормы это штука довольно понятная, а просто сумма квадрата. у нас здесь получается
[48:26.400 --> 48:34.840]  сумма квадрата этих координат нашего вот этого xk. все это в квадрате просто потому что они квадрат
[48:34.840 --> 48:42.560]  суммирования. но у нас все xk они независимы. поэтому на самом деле этот квадрат можно внести как бы
[48:42.560 --> 48:50.880]  под сумму. они все независимы, значит не коррелированы. значит просто вот это вот мотождание суммы ратов по
[48:50.880 --> 48:58.600]  вот этим k это то же самое сумма дисперсии. или же сумма вторых моментов, потому что мотождание равно 0.
[48:58.600 --> 49:08.200]  поэтому получаем что мотождание 1 нормы не больше чем скорень из дизмерности умножить на такую сумму
[49:08.200 --> 49:18.000]  мотождания квадратов координат, которую можно свернуть обратно вот сюда. а дальше услуживается тем, что 2 норма не больше
[49:18.000 --> 49:25.080]  чем 1 норма. то есть у нас есть оценка на 1 норму для x, но у нас есть оценка на 2 нормы для x. поэтому вот эта штука
[49:25.080 --> 49:35.880]  она не больше чем b квадрат. b квадрат под корнем просто b, поэтому корень из dn. вот и все. то есть
[49:35.880 --> 49:42.360]  доказательство вот такого простого неравенства из двух частей. сначала мы избавляемся от флуктуации,
[49:42.360 --> 49:49.800]  заменяем вот эту статистическую штуку на мотождание. на тему оценки мотождания выложите кулинастный норм
[49:49.800 --> 49:58.560]  и с некими свойствами дисперсии. в чем главный пафос? как я уже говорил, что здесь зависимость от размерности
[49:58.560 --> 50:06.720]  она здесь есть под корнем. это плохо, но хорошо то, что она разделена от этой флуктуации, от алгоритма 1 дельта.
[50:06.720 --> 50:14.480]  то есть у нас нет, допустим, произведения размерности на алгоритм 1 дельта. в некоторых случаях довольно важно,
[50:14.480 --> 50:21.040]  если мы хотим работать с маленькой дельтой в большой размерности. на самом деле мы можем в независимости с ними работать.
[50:21.040 --> 50:28.400]  пока у нас это алгоритма доминирует, мы можем уменьшать дельты сколько угодно много, без существенной потери в опроксимации.
[50:28.400 --> 50:39.680]  теперь возвращаясь к алгоритму srl, мы хотим показать, что наше p, то есть мотождание это p с крышкой,
[50:39.680 --> 50:47.200]  будет находиться вот в таком увеличительном множестве. в чем тут главная проблема?
[50:47.200 --> 50:58.000]  проблема тут в том, что вот эта nt относительно не что-то случайное. то есть у нас какие-то случайные вещи,
[50:58.000 --> 51:06.920]  верхняя оценка говорит случайная, а эта штука тоже случайная. в чем довольно не тривиально случайно,
[51:06.920 --> 51:14.680]  потому что у вас проход по траекториям довольно не тривиальный. и казалось бы здесь как-то сложно
[51:14.680 --> 51:23.240]  еще выйдет, но тут уже помогает union bound. давайте сначала рассмотрим множество srl.
[51:23.240 --> 51:28.200]  здесь у нас говорится, что для всяких srl у вас верного такого не нравится.
[51:28.200 --> 51:39.000]  давайте разделим эти srl по одному. если мы получим вот такую штуку с равностью 1-10,
[51:39.000 --> 51:50.440]  то здесь даже это t не особо нужно именно здесь, но это неважно. у нас получится верность таких событий хотя бы такой,
[51:50.440 --> 51:59.160]  то при помощи union bound мы можем в ничке ездить для всех srl. если у нас это событие
[51:59.160 --> 52:06.080]  у нас с очень большой вероятностью, то мы можем заплатить это srl под алгоритмом по сути и получить
[52:06.080 --> 52:15.080]  одновременно для всех seduction parts. теперь вопрос в том, как доказать такую штуку. тут уже хотя бы у нас нет
[52:15.080 --> 52:21.720]  разного состояния, уже неплохо, но все равно есть неприятная nt. а для этого мы на самом деле
[52:21.720 --> 52:32.520]  делаем некий трюк, который в литературе на группе 1 называется reward tape. в этом случае мы скажем так,
[52:32.520 --> 52:39.400]  что у нас вот эта nt это некое случайное число. мы хотим для всех t по этим случайным числом,
[52:39.400 --> 52:44.360]  а если мы докажем просто для всех возможных n, которые могут здесь получаться в этой правой части,
[52:44.360 --> 52:50.200]  и соответственно они будут здесь. вместо этого t, которое на кладочное количество
[52:50.200 --> 52:55.000]  сэмплов используется для оценки под вспышкой, а давайте заменим это на что-то более конкретное.
[52:55.000 --> 53:02.560]  просто заведем такую оценку ровно по n sample, где n это просто фиксирует на констанции.
[53:02.560 --> 53:11.880]  посмотрим вот такое событие. для всех n, где n это возможно количество сэмплов, их может быть
[53:11.880 --> 53:17.880]  сильно больше, чем nt на последний момент времени. вот эта n маленькая, которая допустим максимально
[53:17.880 --> 53:27.080]  ровно t аж, она может сильно превышать вот это вот n в верхнем ныксом t. но это на самом деле не проблема.
[53:27.080 --> 53:34.720]  просто потому что мы не сильно много за это платим. все мы поделим вот такие модели, построенные по n
[53:34.720 --> 53:46.000]  sample. то есть у нас есть для каждой selection пары, пусть у нас есть некий набор состояния x1,
[53:46.000 --> 53:52.480]  x2 и т.д. xk, который мы получили, сапплировав вот этого p. тогда во время самой игры мы на самом деле
[53:52.480 --> 53:57.360]  можем не сапплировать каждый раз заново. просто у нас есть вот эта вот лента со следующими
[53:57.360 --> 54:01.720]  состояниями. мы просто берем следующий элемент с этой лентой. эти две модели абсолютно кевалетные,
[54:01.720 --> 54:09.320]  потому что мы не смотрим на эту ленту выбора решения. на эту ленту можно смотреть только среда.
[54:09.320 --> 54:17.440]  а я не тратил сагнировать совсем новый xk и взять следующий с этой лентой. поэтому у нас есть
[54:17.440 --> 54:29.880]  некая лента с нашими состояниями xk, с 1 и т.д. xth. может быть довольно много потенциально. и мы по
[54:29.880 --> 54:37.920]  вот этим первым xм строим оценку под скрышкой от n скобочек. это будет в точности такая же
[54:37.920 --> 54:47.760]  правильная оценка. если nм равно nт, то эти две оценки точно совпадают. то есть под скрышкой t
[54:47.760 --> 54:58.440]  и под скрышкой от nt это одно и то же. тогда мы докажем некое большее событие, что для всякого n
[54:58.440 --> 55:08.160]  у нас есть концентрация. для всякого вот этого n. то есть для этого мы требуем, чтобы у нас
[55:08.160 --> 55:13.400]  была концентрация неких случайных индексов. это вот это событие внутреннее. а если мы требуем,
[55:13.400 --> 55:26.160]  чтобы для всех возможно. ну конечно вот эта e с крышкой будет больше чем e. поэтому мы получаем
[55:26.160 --> 55:37.640]  все что надо. а для этой штуки мы уже можем сказать, что окей, а давайте снова сделаем
[55:37.640 --> 55:50.520]  union bound, снова разобьем все для каждого прессированного n. и мы хотим сделать вот такую штуку. но на самом
[55:50.520 --> 55:56.240]  деле это штука о точности концентрация индицентрируемых векторов в 1 норме. то есть
[55:56.240 --> 56:02.760]  на отождание вот этого слогамма под 1 нормой, на диратус этих моделей равно 0. если мы рассмотрим
[56:02.760 --> 56:11.800]  вот такие y, то есть это 1 на n на индикатор минус вот этот p h3. на отождание этой штуки равно 0
[56:11.800 --> 56:19.080]  просто по построению как мы эти xk строили. тогда мы можем просто применить нашу концентрацию
[56:19.080 --> 56:27.520]  для 1 нормы. и получается что надо. то есть мы получаем, что для каждого прессированного n вот это 1 норма
[56:27.520 --> 56:38.120]  не больше чем b дельта с вероятностью хотя бы 1 делит на дельта s a t h. делаем union bound по n
[56:38.120 --> 56:50.440]  по всем возможным от 1 до t h. получаем событие e с крышкой. дальше наше e оно вложено в e с крышкой.
[56:50.440 --> 56:59.000]  поэтому мы имеем вот это вот e s a. значит делаем union bound по 7 s a и получаем то что надо. получаем ровно вот это событие.
[56:59.000 --> 57:14.120]  вот этот трюк он как бы довольно важен и в бандитах и в обученном креплении. именно сведение в некоем
[57:14.120 --> 57:21.160]  смысле к id случая. тогда id случая даже как-то и не пахнет. как-то все выбирается случайным образом,
[57:21.160 --> 57:27.160]  но нам это не важно. если мы просто рассмотрим что-то больше, больше состояний, больше что-нибудь
[57:27.160 --> 57:32.800]  такого духа, то никаких проблем не будет. просто потому что мы будем делать union bound по каждому из них.
[57:32.800 --> 57:36.880]  есть ли вопрос к этому моменту?
[57:36.880 --> 57:43.480]  вроде нет.
[57:43.480 --> 57:53.000]  окей. тогда давайте уже сам результат объявим. то есть это выкручивающий рель, который еще раз напомнишь что он делает.
[57:53.000 --> 58:08.040]  мы для каждой модели п штрих умеем считать высот звездой. мы будем считать максимум по моделям,
[58:08.040 --> 58:14.520]  максимум по политикам vp. максимум по политику это оптимальная v функция, которая считается в
[58:14.520 --> 58:23.400]  уровне Беомена, который напомню выглядит вот так. вот эти уравнения очень легко считать если мы
[58:23.400 --> 58:31.880]  знаем p, просто считаем с конца. сначала считаем для h большого, потом у нас есть v функция следующего шага,
[58:31.880 --> 58:41.160]  но просто считаем действие ядра. просто умножение матриц делается замечательно быстро,
[58:41.160 --> 58:48.840]  возможно с другими операциями. дальше мы считаем высот звездой и снова шагаем дальше, уменьшаем h.
[58:48.840 --> 58:53.520]  мы посчитали оптимальную v функцию, оптимальную политику для этой модели.
[58:53.520 --> 59:05.560]  мы посчитали высот звездой, а затем мы находим такую политику p, такую модель p с крышкой из этого
[59:05.560 --> 59:12.200]  вдавительного множества, которая максимирует эти оптимальные v функции. это делается возможно
[59:12.200 --> 59:22.120]  тривиально, но хорошо что тут у нас все выпокоено, поэтому это делаться в принципе может, но довольно
[59:22.120 --> 59:29.240]  сложно, но можно. а затем мы берем оптимальную политику для этого оптимического шага. затем играем один
[59:29.240 --> 59:36.760]  эпизод этой политикой, у нас пересчитываем нашу множество власти дельта и повторяем все действия.
[59:36.760 --> 59:45.480]  это очень время затратно, но мы можем доказать для этого алгоритма такого сенсона регрета,
[59:45.480 --> 59:52.720]  уже с неким правильным шкалированием по количеству эпизодов. то есть регрета от алгоритма это то,
[59:52.720 --> 01:00:00.600]  что до алгоритма корень из h в кубе, x² at. скажу, что x² тут не оптимален, можно улучшать, и это уже
[01:00:00.600 --> 01:00:08.720]  давным-давно сделали, но для этого алгоритма можно построить такую ассоциацию. еще есть слагаемо второго
[01:00:08.720 --> 01:00:14.840]  порядка, это тоже не оптимально, можно делать значимость по s линейной, но это на самом деле особо не важно.
[01:00:14.840 --> 01:00:19.960]  то есть важно первое слагаемое, что для регрета корня ст. в сравнении с х2н гриде довольно заметное
[01:00:19.960 --> 01:00:27.800]  улучшение с 2х3 до корня ст. вот здесь мы тоже будем пользоваться некой концентрацией, но на самом
[01:00:27.800 --> 01:00:35.320]  деле самое главное мы уже доказали. самое главное это вот эта лемма. она обеспечивает довольно большую
[01:00:35.320 --> 01:00:40.320]  часть успеха, это доказательство. то есть оптимизм очень важен для доказательства.
[01:00:40.320 --> 01:00:50.680]  да, то есть мы сначала будем скажем, что пусть у нас событие из этой леммы выполнено,
[01:00:50.680 --> 01:00:56.400]  что у нас все дополнительные множества корректны, в смысле что реальная модель живет в дополнительном множестве
[01:00:56.400 --> 01:01:09.400]  для всех t. затем мы можем, вот то что я говорю о конструкции, построить v с чертой th, это будет
[01:01:09.400 --> 01:01:17.600]  максимум по штриху, максимум по политикам vp vph. то есть это точности то, почему мы здесь берем
[01:01:17.600 --> 01:01:26.720]  арк максимум. то есть если убрать здесь арк, то это будет точности v с чертой. вот если приставить
[01:01:26.720 --> 01:01:32.960]  2 максимума местами, вообще ровно такую штуку. максимум по политикам vp vph, а это максимум по моделям.
[01:01:32.960 --> 01:01:37.920]  но эту штуку можно как-то посчитать. манификция таблик точно существует, потому что c компакт,
[01:01:37.920 --> 01:01:47.200]  эта штука сочная линия на самом деле. вообще все прекрасно. и пусть с чертой аналогично считаем
[01:01:47.200 --> 01:01:53.200]  просто вот такую максимум паку функции. и также насчет того, что вот это может создать так компакт,
[01:01:53.200 --> 01:02:01.000]  можно найти модель, на котором она достигается, назовем ее v с чертой. это нам понадобится. то есть v с чертой
[01:02:01.000 --> 01:02:05.760]  на самом деле считают через уровень Белмана для этой модели v с чертой, через оптимальный уровень Белмана.
[01:02:05.760 --> 01:02:12.880]  и наша политика, это будет жадная политика на сильных хуй с чертой, потому что следует за уровнем Белмана.
[01:02:12.880 --> 01:02:20.520]  можно брать такую политику, можно брать другую. но зачем? когда мы можем брать жадно. то есть здесь вот я об этом говорю,
[01:02:20.520 --> 01:02:27.920]  что предположено, что вот наша pt, она жадная в значении ко функции. это будет действительно то, что нам нужно,
[01:02:27.920 --> 01:02:32.920]  это будет одной из возможностей выбора оптимальной политики для компакта звездой, нашей p с чертой.
[01:02:32.920 --> 01:02:44.400]  возьмем вот такие дельты. дельта у нас по сути разница между ошибкой и опроксимацией. то есть мы хотим,
[01:02:44.400 --> 01:02:51.120]  чтобы наша верхняя оценка опроксимировала vp для нашего этого шага. насколько они далеки друг от друга,
[01:02:51.120 --> 01:03:01.520]  это точность сущности дельта th, именно на шаге, на котором мы играем. затем, на самом деле нам
[01:03:01.520 --> 01:03:09.920]  интересно для h равно 1, но для этого количества хорошо считать для всех маленьких h. разница,
[01:03:09.920 --> 01:03:15.920]  которую определили регрета, потому что регрета это сумма каких-то невязок между высот звездой и vpt.
[01:03:15.920 --> 01:03:23.360]  мы можем просто прибавить и отнять v с чертой. мы по общению первым оставляем и вторым оставляем.
[01:03:23.360 --> 01:03:28.800]  но просто по построению этого v с чертой, здесь что мы говорим? мы говорим, что v с чертой это
[01:03:28.800 --> 01:03:38.960]  максимум. мы знаем, что у нас есть p, наша реальная модель. поэтому максимум всегда больше,
[01:03:38.960 --> 01:03:45.000]  чем высот звездой для реальной модели. в нашем случае мы просто обращаем его в высот звездой. без
[01:03:45.000 --> 01:03:50.520]  выбора модели, если модели нет, это считается в реальной модели, к которой мы играем. поэтому
[01:03:50.520 --> 01:03:55.800]  это сагамма не больше нуля. у нас есть только estimation error, к которому мы начали этим дельтам.
[01:03:55.800 --> 01:04:07.520]  главный пафос всего происходящего в том, что если мы выберем такие уровни bellman, то действия,
[01:04:07.520 --> 01:04:13.600]  которые выбираются там, они не согласовываются с этой политикой. то есть они как-то идут в разнобой,
[01:04:13.600 --> 01:04:22.080]  потому что политика не знает ничего о высот звездой во время вычинения, только начинает узнавать. а v с чертой и v
[01:04:22.080 --> 01:04:27.080]  pt, они в некотором смысле эволюционируются согласно негде же правилам. то есть pt это жадность
[01:04:27.080 --> 01:04:33.240]  сильно q с чертой. ну и даже можно считать, что pt это жадность сильно просто q pt. вернее не жадная,
[01:04:33.240 --> 01:04:42.160]  просто наша v pt это q функция, посчитанная в этом состоянии выбрана дальше. надеюсь, я этим и
[01:04:42.160 --> 01:04:48.760]  пользуюсь, что v с чертой это q с чертой для этого действия выбранного. потому что эта политика
[01:04:48.760 --> 01:04:55.920]  жадная, та политика выбиралась жадной по q с чертой. ну и при этом просто по уровням bellman для
[01:04:55.920 --> 01:05:04.400]  нашей политики p, наша v pt это q pt для этих двух действий, которые мы выбираем. когда мы пишем
[01:05:04.400 --> 01:05:14.480]  уровень bellman, награды сокращаются. дело несколько прибавить и отнять. один из главных трюков во всей
[01:05:14.480 --> 01:05:21.040]  математике может поделить прибавить и отнять формула теора неравенствовательно. все что нужно для
[01:05:21.040 --> 01:05:27.080]  того, чтобы делать математику. вот в этом случае можно прибавить и отнять из них несколько солгаемых,
[01:05:27.080 --> 01:05:34.840]  и вводит такое разложение. первое солгаемое это ошибка модели. насколько наша p с чертой
[01:05:34.840 --> 01:05:43.720]  предсказывает реальные p. второе солгаемое, небольшой спойлер, это будет некая маркетиговоразность,
[01:05:43.720 --> 01:05:53.960]  потому что st h++1 оно простемплировано из модели с выбранными такими параметрами. это p это
[01:05:53.960 --> 01:06:01.160]  мат ожидания. это буквально мат ожидания первого солгаемого с х чертой по всем возможным st h++1.
[01:06:01.160 --> 01:06:08.880]  второе солгаемое имеет такую же структуру. еще у нас есть сама дельта. мы дельту убросили через дельту только
[01:06:08.880 --> 01:06:17.640]  с большим шагом h. мы таким образом немножко перевели ошибку дальше. следующий h, а для h большого у нас
[01:06:17.640 --> 01:06:25.040]  все ноль. поэтому все прекрасно. вот первое, что мы можем сделать, это посчитать ошибку модели.
[01:06:25.040 --> 01:06:33.840]  ошибку модели можно посчитать просто по нерасту гельдера. просто мы разлагаем ошибку на это
[01:06:34.840 --> 01:06:48.560]  между v с чертой и моделью state action пары. значит наша 10 ядра не больше
[01:06:48.560 --> 01:06:57.320]  произведения. не больше чем произведения норм. на случай 1 норма и бесконечная норма.
[01:06:57.320 --> 01:07:05.480]  а наша v с чертой не больше чем h, потому что у нас награда не больше единиц. дальше здесь
[01:07:05.480 --> 01:07:10.520]  мы тоже можем приводить ст h не раз в треугольник. у нас получается p с чертой минус p с крышкой и
[01:07:10.520 --> 01:07:21.400]  под крышкой минус p. но ровно так по дизайну нашего длительного множества эти оба
[01:07:21.400 --> 01:07:31.360]  оставляем не больше чем beta delta. мы получили удвоенную ошибку, удвоенный радиус нашего
[01:07:31.360 --> 01:07:37.800]  длительного множества. мы получаем, что нашу дельту мы разлагаем вот таким образом. у нас
[01:07:37.800 --> 01:07:45.280]  есть аппроксимация модели, ошибка. у нас есть два марксингальна салагаемого и следующая дельта.
[01:07:48.400 --> 01:07:59.840]  если мы свишем весь эгрет, это сумма v с чертой и v пт. это небольшая сумма дельты. дальше мы
[01:07:59.840 --> 01:08:08.200]  работаем только с дельтами и начинаем их раскручивать по шагам h. если мы продолжим
[01:08:08.200 --> 01:08:14.080]  раскручивать сумму этих дельт, которые есть последняя слагаемая на нижней строчке дальше,
[01:08:14.080 --> 01:08:21.320]  то мы получим ровно такую структуру. наш эгрет это не больше чем сумма ошибок аппроксимации
[01:08:21.320 --> 01:08:33.200]  модели во все моменты времени t и на всех шагах h для вот этого счетчика nt. и сумма этих марксигал разности.
[01:08:37.200 --> 01:08:41.600]  и в принципе количество активистических алгоритмов, то есть недоятельная эссерия,
[01:08:41.600 --> 01:08:48.800]  устроена примерно так, что у нас в конечном итоге получается некая ошибка аппроксимации
[01:08:48.800 --> 01:08:57.280]  модели или сумма бонусов, что будет ведущим слагаемым. и какие-то марксигалы, которые совсем
[01:08:57.280 --> 01:09:05.400]  мелочь, которые как-то суммируются, грубо азума хюдинга или бенштейна. но в принципе
[01:09:05.400 --> 01:09:13.560]  все эти два слагаемых маленьких, главное слагаемая первая. мы можем оценить,
[01:09:13.560 --> 01:09:22.920]  расспомнив что наша бодельта, я напомню, отсюда это 1 делить на корнез n не чисто, то есть как корнез s
[01:09:22.920 --> 01:09:32.560]  плюс алгоритм. корнез s плюс алгоритм делить на число посещений, делить на n.
[01:09:32.560 --> 01:09:44.000]  у нас получается вот то, что ровно есть сверху. если мы просто расспишем определение бодельта,
[01:09:44.000 --> 01:09:52.280]  но еще здесь немножко забило на том, что здесь была сумма. в этом случае сумма не больше
[01:09:52.280 --> 01:09:59.120]  чем произведение, потому что все довольно маленькое. тельта довольно маленькая, поэтому алгоритм большой,
[01:09:59.120 --> 01:10:07.800]  больше единица. это грубо, но для наших целей сойдет. и остается вот такая вот сумма,
[01:10:07.800 --> 01:10:15.040]  они делят на корень nt. на самом деле мы можем перегруппировать правильно.
[01:10:18.000 --> 01:10:25.440]  если мы выделим одну straight action пару, и будем считать сколько раз мы вылетят в эту сумму,
[01:10:26.360 --> 01:10:31.140]  на самом деле это число будет вот так постепенно расти. при первом посещении будет просто единство,
[01:10:31.140 --> 01:10:44.260]  при втором посещении это будет 1 делить на корнез одного и так далее.
[01:10:44.260 --> 01:10:50.260]  у нас будет nt, а в итоге все равно 3.
[01:10:50.260 --> 01:10:58.260]  И всего количество раз слова присутствует, а на этой текшн паре в сумме это n для t большого от n.
[01:10:58.260 --> 01:11:08.260]  Затем мы можем сделать самый простой трюк, заменить сумму на интеграл, оценивать это сверху.
[01:11:08.260 --> 01:11:16.260]  Дальше, прооптимизировав по nt, мы можем получить, что эта сумма 1 на корень из этих n.
[01:11:16.260 --> 01:11:22.260]  Эта сумма анализируется как корень из nt, nt большого.
[01:11:22.260 --> 01:11:26.260]  Эта сумма во второй формуле.
[01:11:26.260 --> 01:11:30.260]  Это будет 1 плюс корень из nt большого.
[01:11:30.260 --> 01:11:36.260]  Но вот эта сумма по всем текшн парам, 1 плюс вот эта штука,
[01:11:36.260 --> 01:11:44.260]  1 просто выносится как SA, а второе на самом деле максимизируется, когда все nt большого равны между собой.
[01:11:44.260 --> 01:11:50.260]  Получается ровно вот эта вот 2 умножена корень из SATH.
[01:11:50.260 --> 01:11:55.260]  Затем, если мы еще вспомним, что если мы замутили на S и H,
[01:11:55.260 --> 01:12:01.260]  у нас получится точности вот эта вот ведущая слагаемая из регрета.
[01:12:01.260 --> 01:12:08.260]  А оставшая слагаемая это какая-то мелочёпка, просто нужно определить правильную фильтрацию, чтобы сказать, что это маркетовая разница.
[01:12:08.260 --> 01:12:12.260]  В этом случае правильная фильтрация, да будет по сути фильтрация по времени.
[01:12:12.260 --> 01:12:20.260]  То есть у нас есть какие-то эпизоды t, значит со нашей фильтрацией мы будем обуславливаться
[01:12:20.260 --> 01:12:28.260]  на все наши посещения, для всех эпизодов более ранних, для всех шагов, которые у нас были.
[01:12:28.260 --> 01:12:35.260]  И для последнего эпизода, для всех этих h, для всех шагов внутреннего эпизода, до того, на котором мы сейчас находимся.
[01:12:35.260 --> 01:12:41.260]  То есть по сути говоря, у нас есть как-то время, мы обуславливаемся на всё, что было до этого момента.
[01:12:41.260 --> 01:12:44.260]  Если мы так сделаем, мы получаем маркетовую разницу.
[01:12:44.260 --> 01:12:47.260]  Которая по нерадостному зуму хердинга.
[01:12:47.260 --> 01:12:49.260]  Получается какой-то такой вид.
[01:12:49.260 --> 01:12:53.260]  Я не стал выбирать точные константы, потому что они нам не интересны.
[01:12:53.260 --> 01:12:56.260]  Но идейная слагама будет как-то так.
[01:12:56.260 --> 01:13:02.260]  Важно здесь корень из t не везутся до размерности, то бишь f.
[01:13:02.260 --> 01:13:08.260]  И вот эта штука с разностью 1-2 дельта зрищется вот так.
[01:13:08.260 --> 01:13:13.260]  После чего мы можем всё просуммировать и получить нужный отец.
[01:13:13.260 --> 01:13:19.260]  Давайте я еще раз по этому всему пройдусь.
[01:13:19.260 --> 01:13:22.260]  По структуре доказательства.
[01:13:22.260 --> 01:13:24.260]  И структуре алгоритма.
[01:13:24.260 --> 01:13:26.260]  То есть как выглядит наш алгоритм?
[01:13:26.260 --> 01:13:30.260]  На самом деле понятнее кажется вот здесь.
[01:13:30.260 --> 01:13:36.260]  Получается, что мы определяем некие такие b чертой, q чертой, как вот такие максимумы.
[01:13:36.260 --> 01:13:41.260]  То есть у нас некий кусок звезды для любописного модели считается несложно.
[01:13:41.260 --> 01:13:44.260]  Мы промаксимизируем внутри наших длительных множеств.
[01:13:44.260 --> 01:13:48.260]  А затем играем шаду с сильной атакой чертой.
[01:13:48.260 --> 01:13:51.260]  Вот наша политика.
[01:13:51.260 --> 01:13:53.260]  Как является c дельта?
[01:13:53.260 --> 01:14:00.260]  c дельта это некий шар в один норме, можно это назвать так.
[01:14:00.260 --> 01:14:04.260]  Вокруг оценки максимально правдоподобие.
[01:14:04.260 --> 01:14:07.260]  То есть самая разумная оценка, которую можно сделать.
[01:14:07.260 --> 01:14:12.260]  Вот где-то рядом с ней должна находиться реальная модель.
[01:14:12.260 --> 01:14:19.260]  И при помощи концентрации меры мы можем выписать точно это b дельта.
[01:14:19.260 --> 01:14:22.260]  Получаем на самом деле точную скорость сходимости, а как оно сходит.
[01:14:22.260 --> 01:14:25.260]  Как этот длительный провал сужается со временем.
[01:14:25.260 --> 01:14:27.260]  Но вот там всегда будет c и p.
[01:14:27.260 --> 01:14:31.260]  Поэтому вот эта v чертой, которая у нас здесь определяется,
[01:14:31.260 --> 01:14:36.260]  когда мы строили политику, она будет всегда больше, чем w и c звездой для реальной модели.
[01:14:36.260 --> 01:14:44.260]  Это важно, это позволяет нам в некотором смысле закаплить то есть оценку на регресс.
[01:14:44.260 --> 01:14:51.260]  То есть делать так, чтобы мы могли распручивать наш регресс по уровню Белмана.
[01:14:51.260 --> 01:14:55.260]  Без этого доказывается сложно.
[01:14:55.260 --> 01:14:58.260]  Очень сложно, но в принципе можно.
[01:14:58.260 --> 01:15:05.260]  Есть работа над чему, как делать это без оптимизма на качестве на регресс.
[01:15:05.260 --> 01:15:08.260]  То есть оптимизм это то, что q с чертой больше, чем q с звездой.
[01:15:08.260 --> 01:15:10.260]  И v с чертой больше, чем v с звездой.
[01:15:10.260 --> 01:15:14.260]  Тогда как нам доказать центральный регресс на этой штуке.
[01:15:14.260 --> 01:15:16.260]  Сначала пользуемся оптимизмом.
[01:15:16.260 --> 01:15:20.260]  Меняем v с чертой на v с звездой.
[01:15:20.260 --> 01:15:23.260]  И v с чертой и v с звездой эволюционируют по одному принту.
[01:15:23.260 --> 01:15:27.260]  Выбираются одно и то же действия, такие же есть q функции.
[01:15:27.260 --> 01:15:29.260]  И все в прочем дует.
[01:15:29.260 --> 01:15:31.260]  Значит можно стать уровнем Белмана.
[01:15:31.260 --> 01:15:37.260]  Потому что v с чертой это v функция для некоторой модели, для некоторой политики.
[01:15:37.260 --> 01:15:41.260]  Точно ровно политика PT там используется.
[01:15:41.260 --> 01:15:46.260]  То есть v с чертой это v функция для политики PT, но в другой модели.
[01:15:46.260 --> 01:15:51.260]  Дальше мы пишем уровень Белмана, прибавить и отнять.
[01:15:51.260 --> 01:15:53.260]  Разлагаем на 4 слагаемых.
[01:15:53.260 --> 01:15:56.260]  Первая слагаемая это ошибка модели.
[01:15:56.260 --> 01:16:00.260]  То есть насколько далеко p с чертой от p.
[01:16:01.260 --> 01:16:03.260]  Вторая слагаемая.
[01:16:03.260 --> 01:16:08.260]  Это нечто маркетингальное, потому что мы хотим дальше подключить дельты.
[01:16:08.260 --> 01:16:11.260]  Не от ожидания неких дельт, а сами дельты.
[01:16:11.260 --> 01:16:15.260]  Поэтому мы как-то подальше подключим дальше, прибавить и отнять.
[01:16:15.260 --> 01:16:18.260]  Это v с чертой в следующем состоянии.
[01:16:18.260 --> 01:16:21.260]  И v пт в следующем состоянии.
[01:16:23.260 --> 01:16:29.260]  Дальше эти два слагаемых, они кажут довольно мелочью, просто потому что у нас будет сумма их.
[01:16:29.260 --> 01:16:31.260]  Кажут, что гаммаодельность может быть большим.
[01:16:31.260 --> 01:16:36.260]  Допустим, какое-то не типичное состояние выпало, и все, и все плохо.
[01:16:36.260 --> 01:16:42.260]  Но если мы просуммируем, а в игрете мы ровно получаем сумму,
[01:16:42.260 --> 01:16:45.260]  то эта сумма будет влиять мало на итоговую оценку.
[01:16:45.260 --> 01:16:48.260]  Логичность просто с кси.
[01:16:48.260 --> 01:16:52.260]  И дельта просто начинаем отключивать дальше по тому же самому принципу.
[01:16:54.260 --> 01:16:57.260]  Ошибка модели считается просто 11 Гельдера,
[01:16:57.260 --> 01:17:01.260]  потому что мы знаем, как далеко p с чертой от p живет в 1 норме.
[01:17:03.260 --> 01:17:05.260]  Они максимум как бы...
[01:17:05.260 --> 01:17:07.260]  У нас это по сути некий шарик,
[01:17:08.260 --> 01:17:10.260]  есть вокруг p с крышкой.
[01:17:11.260 --> 01:17:14.260]  p и p с чертой могут лежать на разных катах этого шарика,
[01:17:14.260 --> 01:17:18.260]  поэтому расстояние между ними не больше, чем 2 на радиошарик, то есть диаметр.
[01:17:20.260 --> 01:17:22.260]  Еще h это...
[01:17:22.260 --> 01:17:25.260]  Сколько большая может быть эта v функция с чертой?
[01:17:26.260 --> 01:17:28.260]  Для нашего ошибка дельта t,
[01:17:28.260 --> 01:17:34.260]  ошибка оценки нашей с чертой реальной модели,
[01:17:34.260 --> 01:17:38.260]  она не больше, чем эта ошибка аппроксимации модели
[01:17:38.260 --> 01:17:42.260]  плюс 2 маркенгальных члена и плюс следующая дельта.
[01:17:42.260 --> 01:17:46.260]  Дальше можно продолжать писать неравенство для всех h
[01:17:48.260 --> 01:17:50.260]  и получить, что сейчас вот такая сумма.
[01:17:51.260 --> 01:17:53.260]  Первая сумма – это сумма ошибок в модели.
[01:17:54.260 --> 01:17:58.260]  Вторая сумма – это что-то маленькое,
[01:17:58.260 --> 01:18:02.260]  потому что это не что с нулевым от ожидания, что мы просуммировали,
[01:18:02.260 --> 01:18:05.260]  но маленькое, если мы поделим это, допустим, на t.
[01:18:05.260 --> 01:18:10.260]  Уж на t h, но h у нас по порядку меньше, чем t, как правило.
[01:18:11.260 --> 01:18:15.260]  Эта штука будет маленьким, то есть последние 2 совгамы будут довольно маленькими
[01:18:16.260 --> 01:18:18.260]  в сумме во второй строчке.
[01:18:19.260 --> 01:18:22.260]  И главная совгама – это сумма ошибок в модели.
[01:18:23.260 --> 01:18:27.260]  Ну, если правильно перегруппировать совгамы,
[01:18:27.260 --> 01:18:29.260]  то становится понятно, как эта штука себя ведет.
[01:18:31.260 --> 01:18:33.260]  Если просто следить за отдельной стрит-экшен парой,
[01:18:33.260 --> 01:18:36.260]  мы про нее будем думать про 1 s a,
[01:18:36.260 --> 01:18:42.260]  сколько раз она здесь возникает и с какими весами,
[01:18:42.260 --> 01:18:44.260]  то мы получим ровно вот этот англо справа.
[01:18:44.260 --> 01:18:47.260]  Здесь, на самом деле, скорее равенство, а не неравенство.
[01:18:47.260 --> 01:18:49.260]  Но неравенство точно верно.
[01:18:49.260 --> 01:18:50.260]  Вот.
[01:18:50.260 --> 01:18:52.260]  Может быть, эта еничка может...
[01:18:52.260 --> 01:18:54.260]  Скажем так, если мы не поделим стрит-экшен пару,
[01:18:54.260 --> 01:18:56.260]  у нас не будет здесь енички вообще.
[01:18:57.260 --> 01:19:00.260]  Но будем считать вам еничку,
[01:19:00.260 --> 01:19:04.260]  потому что в лучшем случае мы посадим все стрит-экшен пары.
[01:19:04.260 --> 01:19:06.260]  Все s a.
[01:19:07.260 --> 01:19:09.260]  Тогда заменяем...
[01:19:13.260 --> 01:19:15.260]  суммы как просто корм из NT.
[01:19:16.260 --> 01:19:18.260]  Затем сумма всем стрит-экшен парам
[01:19:18.260 --> 01:19:21.260]  максимизируется, когда все NT равны между собой.
[01:19:21.260 --> 01:19:23.260]  То есть при условии того, что
[01:19:23.260 --> 01:19:25.260]  сумма всех NT равно TH,
[01:19:26.260 --> 01:19:28.260]  то эта сумма корней максимизируется,
[01:19:28.260 --> 01:19:30.260]  когда они все равны между собой.
[01:19:30.260 --> 01:19:33.260]  Потом, в общем, оценку вот такую s a TH.
[01:19:36.260 --> 01:19:38.260]  Еще двойка, потому что
[01:19:38.260 --> 01:19:40.260]  интеграль не нравится неточная.
[01:19:40.260 --> 01:19:43.260]  Вот здесь тоже что-то может вылезти.
[01:19:46.260 --> 01:19:48.260]  Чтобы вылезти в такую интеграль.
[01:19:48.260 --> 01:19:51.260]  То есть у нас получается ведущая слагаемая в регрете,
[01:19:51.260 --> 01:19:53.260]  если мы учтем эти a TH,
[01:19:53.260 --> 01:19:55.260]  по которой у нас много материалов.
[01:19:57.260 --> 01:20:00.260]  Вторая слагаемая это какая-то мелочевка
[01:20:02.260 --> 01:20:04.260]  некая сумма морского разности,
[01:20:04.260 --> 01:20:07.260]  которая лично консенсируется около 0 согласно зуме Хердин.
[01:20:09.260 --> 01:20:12.260]  И таким образом мы получаем вот эта первая слагаемая.
[01:20:13.260 --> 01:20:15.260]  Это будет как раз таки слагаемая
[01:20:18.260 --> 01:20:20.260]  вот эта ведущая,
[01:20:20.260 --> 01:20:22.260]  которая внизу справа.
[01:20:23.260 --> 01:20:25.260]  Это вот как ведет в себя эта сумма
[01:20:25.260 --> 01:20:28.260]  один делит на счетчике в общем случае.
[01:20:28.260 --> 01:20:30.260]  Вторая слагаемая это просто
[01:20:30.260 --> 01:20:32.260]  вот эта вот s a,
[01:20:32.260 --> 01:20:34.260]  которая здесь тоже снизу берется.
[01:20:34.260 --> 01:20:36.260]  И третья слагаемая,
[01:20:36.260 --> 01:20:38.260]  это маркегальная слагаемая.
[01:20:38.260 --> 01:20:40.260]  У нас первая слагаемая доминирует третья,
[01:20:42.260 --> 01:20:44.260]  поэтому у нас получается такая сумма.
[01:20:44.260 --> 01:20:48.260]  Алгоритмы сейчас спрятаны под O с tilde,
[01:20:49.260 --> 01:20:53.260]  поэтому тут все спрятано под O с tilde,
[01:20:53.260 --> 01:20:55.260]  но степень алгоритмов,
[01:20:55.260 --> 01:20:57.260]  если здесь можно отследить,
[01:20:57.260 --> 01:20:59.260]  это корень алгоритма максимум.
[01:21:00.260 --> 01:21:02.260]  Поэтому это не особо проблема.
[01:21:03.260 --> 01:21:05.260]  И мы получаем такую основу на рекрете.
[01:21:08.260 --> 01:21:10.260]  Есть ли какие-нибудь вопросы?
[01:21:13.260 --> 01:21:15.260]  Коллеги, пожалуйста, есть ли вопросы?
[01:21:20.260 --> 01:21:21.260]  Видимо нет.
[01:21:21.260 --> 01:21:23.260]  Но у нас сейчас как раз по времени
[01:21:23.260 --> 01:21:25.260]  должна быть пауза где-то,
[01:21:25.260 --> 01:21:27.260]  ну может еще пять минут.
[01:21:27.260 --> 01:21:29.260]  И потом еще часовой доклад.
[01:21:29.260 --> 01:21:31.260]  У вас как сейчас идет это?
[01:21:31.260 --> 01:21:33.260]  Где-то точку паузу можно поставить?
[01:21:33.260 --> 01:21:35.260]  Или пока еще рано?
[01:21:35.260 --> 01:21:39.260]  Я думаю, что в течение минут 50 можно закончить.
[01:21:39.260 --> 01:21:41.260]  Да-да, отлично.
[01:21:41.260 --> 01:21:43.260]  Давайте так и сделаем.
[01:21:45.260 --> 01:21:47.260]  Давайте немножко просуммируем,
[01:21:47.260 --> 01:21:49.260]  что здесь было,
[01:21:49.260 --> 01:21:51.260]  что в принципе мы получили по итогу.
[01:21:51.260 --> 01:21:54.260]  Мы получили, что при помощи концентрации меры
[01:21:54.260 --> 01:21:57.260]  можно получать оценки на регрет
[01:21:57.260 --> 01:21:59.260]  для алгоритмов обучения сопротивления.
[01:21:59.260 --> 01:22:01.260]  То есть в принципе все,
[01:22:01.260 --> 01:22:03.260]  что из математики мы использовали,
[01:22:03.260 --> 01:22:05.260]  это качество, это прибавительность,
[01:22:05.260 --> 01:22:07.260]  можно поделить и не раз концентрация.
[01:22:07.260 --> 01:22:09.260]  То есть ничего здесь больше в принципе не нужно.
[01:22:09.260 --> 01:22:11.260]  Это как бы главная математическая идея,
[01:22:11.260 --> 01:22:13.260]  которая здесь используется
[01:22:13.260 --> 01:22:15.260]  для деконстрации меры.
[01:22:15.260 --> 01:22:18.260]  И она здесь используется просто повсеместно.
[01:22:18.260 --> 01:22:21.260]  Все время вылезают какие-нибудь маркетингауы,
[01:22:21.260 --> 01:22:23.260]  которые нужно оценивать.
[01:22:23.260 --> 01:22:25.260]  Все время вылезает что-то такого духа,
[01:22:25.260 --> 01:22:27.260]  может вылезать что-то более сложное,
[01:22:27.260 --> 01:22:29.260]  но довольно редко.
[01:22:29.260 --> 01:22:33.260]  И в принципе на самом деле возникает вопрос,
[01:22:33.260 --> 01:22:35.260]  а оптимальна ли оценка,
[01:22:35.260 --> 01:22:37.260]  которую мы получили на регрет?
[01:22:37.260 --> 01:22:39.260]  На самом деле нет, на самом деле можно сделать лучше.
[01:22:39.260 --> 01:22:41.260]  А лучше делается даже более вычисленно
[01:22:41.260 --> 01:22:43.260]  дружелюбным способом.
[01:22:43.260 --> 01:22:45.260]  Давайте у нас есть ровняний Бэмман,
[01:22:45.260 --> 01:22:47.260]  оптимальный, слева.
[01:22:47.260 --> 01:22:49.260]  Мы у них не знаем модель.
[01:22:49.260 --> 01:22:51.260]  И единственное, что мы можем сделать,
[01:22:51.260 --> 01:22:53.260]  это, называемый статистический плагин estimate,
[01:22:53.260 --> 01:22:55.260]  мы можем поставить под крышкой.
[01:22:55.260 --> 01:22:57.260]  Но при этом мы знаем,
[01:22:57.260 --> 01:22:59.260]  что под крышкой не работает.
[01:22:59.260 --> 01:23:01.260]  То есть под крышкой оно будет сходить куда-то не туда,
[01:23:01.260 --> 01:23:03.260]  просто потому что тп в генерации данных
[01:23:03.260 --> 01:23:05.260]  зависит от самой этой оценки.
[01:23:05.260 --> 01:23:07.260]  Если оценка плохая, данных делаются плохие,
[01:23:07.260 --> 01:23:09.260]  а так остается плохой.
[01:23:09.260 --> 01:23:11.260]  Горбачин, горбачаут.
[01:23:11.260 --> 01:23:13.260]  Во всей красе.
[01:23:13.260 --> 01:23:15.260]  Но вместо этого мы можем поощрять
[01:23:15.260 --> 01:23:17.260]  агента исследовать.
[01:23:17.260 --> 01:23:19.260]  Дополнительным образом.
[01:23:19.260 --> 01:23:21.260]  То есть можно делать эксенгриде,
[01:23:21.260 --> 01:23:23.260]  а эксенгриде плохо. Давайте по-другому.
[01:23:23.260 --> 01:23:25.260]  Будем добавлять некий exploration bonus,
[01:23:25.260 --> 01:23:27.260]  нашу оценку на ку-функцию.
[01:23:27.260 --> 01:23:29.260]  То есть если мы посетили
[01:23:29.260 --> 01:23:31.260]  as a selection bar мало раз,
[01:23:31.260 --> 01:23:33.260]  то это exploration bonus большой.
[01:23:33.260 --> 01:23:35.260]  То есть мы будем в любом случае
[01:23:35.260 --> 01:23:37.260]  выбирать это действие,
[01:23:37.260 --> 01:23:39.260]  даже если на него оценка довольно плохая
[01:23:39.260 --> 01:23:41.260]  по первым думам с алгамма,
[01:23:41.260 --> 01:23:43.260]  то есть по ревордам плюс
[01:23:43.260 --> 01:23:45.260]  с крышкой на В чертой.
[01:23:45.260 --> 01:23:47.260]  Если по этим штукам оценка у нас плохая,
[01:23:47.260 --> 01:23:49.260]  но exploration bonus хороший,
[01:23:49.260 --> 01:23:51.260]  то сразу туда пойдем.
[01:23:53.260 --> 01:23:55.260]  Все равно выберем это действие
[01:23:55.260 --> 01:23:57.260]  в этом состоянии.
[01:23:57.260 --> 01:23:59.260]  Наша политика это ARC максимум
[01:23:59.260 --> 01:24:01.260]  по Q чертой.
[01:24:01.260 --> 01:24:03.260]  Если же у нас
[01:24:03.260 --> 01:24:05.260]  бонус маленький,
[01:24:05.260 --> 01:24:07.260]  но большая вот эта алгамма,
[01:24:07.260 --> 01:24:09.260]  которая идет из уровня Баумана,
[01:24:09.260 --> 01:24:11.260]  то тоже хорошо.
[01:24:11.260 --> 01:24:13.260]  Тут гарантированно хорошие награды есть.
[01:24:15.260 --> 01:24:17.260]  И что из этого получается,
[01:24:17.260 --> 01:24:19.260]  как выбираются эти бонусы,
[01:24:19.260 --> 01:24:21.260]  чтобы у нас был этот принцип оптимизма,
[01:24:21.260 --> 01:24:23.260]  чтобы куча всего было больше
[01:24:23.260 --> 01:24:25.260]  чем куча звездой,
[01:24:25.260 --> 01:24:27.260]  чтобы у нас сработал этот первый шаг
[01:24:27.260 --> 01:24:29.260]  до качества,
[01:24:29.260 --> 01:24:31.260]  самый первый,
[01:24:31.260 --> 01:24:33.260]  когда мы выкинули вот эту ошибку
[01:24:33.260 --> 01:24:35.260]  после звездой минус Q чертой.
[01:24:35.260 --> 01:24:37.260]  И мы получили разность
[01:24:37.260 --> 01:24:39.260]  двух value функций,
[01:24:39.260 --> 01:24:41.260]  которые некому другому соответствуют.
[01:24:43.260 --> 01:24:45.260]  Которые расключиваются
[01:24:45.260 --> 01:24:47.260]  одинаковым образом.
[01:24:51.260 --> 01:24:53.260]  И алгоритм получше называется
[01:24:53.260 --> 01:24:55.260]  UCBVI,
[01:24:55.260 --> 01:24:57.260]  для upper content bounce reiteration.
[01:24:57.260 --> 01:24:59.260]  В 2017 году он получает
[01:24:59.260 --> 01:25:01.260]  вот такую оптимальную оценку на регресс.
[01:25:03.260 --> 01:25:05.260]  Здесь еще в таком
[01:25:05.260 --> 01:25:07.260]  более общем стетинге,
[01:25:07.260 --> 01:25:09.260]  зависит еще от шага h,
[01:25:09.260 --> 01:25:11.260]  но это абсолютно не важно.
[01:25:11.260 --> 01:25:13.260]  То есть эти бонусы,
[01:25:13.260 --> 01:25:15.260]  как они выбираются,
[01:25:15.260 --> 01:25:17.260]  либо как h делить на корень
[01:25:17.260 --> 01:25:19.260]  числа посещений,
[01:25:19.260 --> 01:25:21.260]  либо как использовать
[01:25:21.260 --> 01:25:23.260]  оценку на дисперсию.
[01:25:23.260 --> 01:25:25.260]  То есть в этом случае мы
[01:25:25.260 --> 01:25:27.260]  получим качество использования разницы хевдинга,
[01:25:27.260 --> 01:25:29.260]  чтобы получить оптимизм.
[01:25:29.260 --> 01:25:31.260]  В этом случае мы используем разницу берштейна.
[01:25:31.260 --> 01:25:33.260]  Но потом еще есть большая говно
[01:25:33.260 --> 01:25:35.260]  больше с этим дисперсиями,
[01:25:35.260 --> 01:25:37.260]  потому что это не просто дисперсия
[01:25:37.260 --> 01:25:39.260]  с этой звездой в реальной модели,
[01:25:39.260 --> 01:25:41.260]  а у вас и модель неправильная, и оценка
[01:25:41.260 --> 01:25:43.260]  неправильная, и вообще все неправильное, оно пипит.
[01:25:43.260 --> 01:25:45.260]  Поэтому это все можно скорректировать.
[01:25:47.260 --> 01:25:49.260]  Он делится на число посещений,
[01:25:49.260 --> 01:25:51.260]  и можно получить, что этот алгоритм
[01:25:51.260 --> 01:25:53.260]  оптимальен
[01:25:53.260 --> 01:25:55.260]  на число посещений или грэпов.
[01:25:55.260 --> 01:25:57.260]  Точно алгоритмах
[01:25:57.260 --> 01:25:59.260]  в этой науке никто
[01:25:59.260 --> 01:26:01.260]  не заботится,
[01:26:01.260 --> 01:26:03.260]  потому что объекты гораздо сложнее,
[01:26:03.260 --> 01:26:05.260]  чем в условных бандитах.
[01:26:05.260 --> 01:26:07.260]  То есть эти велифункции не раскручиваются,
[01:26:07.260 --> 01:26:09.260]  и все это делать довольно сложно.
[01:26:09.260 --> 01:26:11.260]  Как безумим баунда здесь обходится,
[01:26:11.260 --> 01:26:13.260]  как в бандитах не особо понятно.
[01:26:13.260 --> 01:26:15.260]  И что еще может быть дальше
[01:26:15.260 --> 01:26:17.260]  с этим сделано?
[01:26:17.260 --> 01:26:19.260]  То есть можно использовать на самом деле
[01:26:19.260 --> 01:26:21.260]  алгоритм, который не связан с оптимизмом.
[01:26:21.260 --> 01:26:23.260]  Когда мы вместо бонуса
[01:26:23.260 --> 01:26:25.260]  добавляем шум в гаусс,
[01:26:25.260 --> 01:26:27.260]  мы получаем
[01:26:27.260 --> 01:26:29.260]  зашумленную оттенку
[01:26:29.260 --> 01:26:31.260]  уровня Белмана,
[01:26:31.260 --> 01:26:33.260]  и магическим образом это работает.
[01:26:33.260 --> 01:26:35.260]  То есть можно делать
[01:26:35.260 --> 01:26:37.260]  пейст рель,
[01:26:37.260 --> 01:26:39.260]  posterior sampling for revolving.
[01:26:39.260 --> 01:26:41.260]  Делаем вместо пейст-крышкой
[01:26:41.260 --> 01:26:43.260]  модель,
[01:26:43.260 --> 01:26:45.260]  просто берем некую базовскую модель
[01:26:45.260 --> 01:26:47.260]  на все наши модели,
[01:26:47.260 --> 01:26:49.260]  возможно, и сэмплируем
[01:26:49.260 --> 01:26:51.260]  как-то из апостелюрного
[01:26:51.260 --> 01:26:53.260]  распределения
[01:26:53.260 --> 01:26:55.260]  и решаем такие уровни Белмана.
[01:26:55.260 --> 01:26:57.260]  Этот алгоритм
[01:26:57.260 --> 01:26:59.260]  отлично работает
[01:26:59.260 --> 01:27:01.260]  на практике,
[01:27:01.260 --> 01:27:03.260]  но, к сожалению, для него пока неизвестно
[01:27:03.260 --> 01:27:05.260]  никаких технических результатов.
[01:27:05.260 --> 01:27:07.260]  Конкретно я и Алексей
[01:27:07.260 --> 01:27:09.260]  занимались таким
[01:27:09.260 --> 01:27:11.260]  сеттингом совмещения
[01:27:11.260 --> 01:27:13.260]  техник оптимизма
[01:27:13.260 --> 01:27:15.260]  и эфирандомизации
[01:27:15.260 --> 01:27:17.260]  по очереди два алгоритма.
[01:27:17.260 --> 01:27:19.260]  То есть один алгоритм – это bias UCBVI.
[01:27:19.260 --> 01:27:21.260]  Он тоже следствует
[01:27:21.260 --> 01:27:23.260]  такой идеи,
[01:27:23.260 --> 01:27:25.260]  но у него нет бонусов.
[01:27:25.260 --> 01:27:27.260]  Вместо бонусов мы тоже пользуемся базовской моделью
[01:27:27.260 --> 01:27:29.260]  и берем верхнюю клонтиль
[01:27:29.260 --> 01:27:31.260]  по возможным моделям.
[01:27:31.260 --> 01:27:33.260]  Есть также оптимистический posterior sampling.
[01:27:33.260 --> 01:27:35.260]  В этом случае мы вместо клонтиль
[01:27:35.260 --> 01:27:37.260]  берем максимум
[01:27:37.260 --> 01:27:39.260]  по просмотрованных моделей
[01:27:39.260 --> 01:27:41.260]  из апостелюрного распределения.
[01:27:41.260 --> 01:27:43.260]  Мимо прочего можно
[01:27:43.260 --> 01:27:45.260]  развивать эту
[01:27:45.260 --> 01:27:47.260]  оптимистическую идею дальше
[01:27:47.260 --> 01:27:49.260]  и говорить, что в любом случае это слишком плохо,
[01:27:49.260 --> 01:27:51.260]  потому что мы не покрываем
[01:27:51.260 --> 01:27:53.260]  каких-нибудь атарий, ничего разумного мы не покрываем.
[01:27:53.260 --> 01:27:55.260]  Поэтому
[01:27:55.260 --> 01:27:57.260]  хочется сказать, что
[01:27:57.260 --> 01:27:59.260]  допустим у нас кофункция
[01:27:59.260 --> 01:28:01.260]  скажем так, у нас
[01:28:01.260 --> 01:28:03.260]  как-то апрактируется какой-то
[01:28:03.260 --> 01:28:05.260]  нейросеть или там
[01:28:05.260 --> 01:28:07.260]  какие-нибудь веревые методами
[01:28:07.260 --> 01:28:09.260]  оптимальная кофункция.
[01:28:09.260 --> 01:28:11.260]  Что можно туда про это говорить и сказать?
[01:28:11.260 --> 01:28:13.260]  Здесь тоже есть довольно развитая история,
[01:28:13.260 --> 01:28:15.260]  где помимо кастрации меры
[01:28:15.260 --> 01:28:17.260]  еще возникает
[01:28:17.260 --> 01:28:19.260]  теория проксимации.
[01:28:19.260 --> 01:28:21.260]  И помимо прочего, можно пытаться
[01:28:21.260 --> 01:28:23.260]  придумывать, как историческое соображение
[01:28:23.260 --> 01:28:25.260]  общается на реальные алгоритмы,
[01:28:25.260 --> 01:28:27.260]  на реальные игры в атаре
[01:28:27.260 --> 01:28:29.260]  и все прочее.
[01:28:29.260 --> 01:28:31.260]  Это уже более инженерная история,
[01:28:31.260 --> 01:28:33.260]  но тем не менее, я считаю, что довольно
[01:28:33.260 --> 01:28:35.260]  интересно и довольно
[01:28:35.260 --> 01:28:37.260]  поучительное правило.
[01:28:37.260 --> 01:28:39.260]  На этом
[01:28:39.260 --> 01:28:41.260]  у меня все.
[01:28:41.260 --> 01:28:43.260]  Спасибо.
[01:28:43.260 --> 01:28:45.260]  Да, пожалуйста, вопросы.
[01:28:45.260 --> 01:28:47.260]  Спасибо за доклад, Данил.
[01:28:47.260 --> 01:28:49.260]  Есть ли вопросы, коллеги?
[01:28:51.260 --> 01:28:53.260]  Ну вот, хорошо мы познакомились
[01:28:53.260 --> 01:28:55.260]  как бы в библиотеке
[01:28:55.260 --> 01:28:57.260]  с нерастными концентрациями,
[01:28:57.260 --> 01:28:59.260]  как это все работает.
[01:29:05.260 --> 01:29:07.260]  Будем в курсе.
[01:29:07.260 --> 01:29:09.260]  Но каких-то большого количества
[01:29:09.260 --> 01:29:11.260]  примеров мы не разбирали.
[01:29:11.260 --> 01:29:13.260]  Нераста Макдермита
[01:29:13.260 --> 01:29:15.260]  вообще очень полезная машина
[01:29:15.260 --> 01:29:17.260]  в обучении и здорово, что оно
[01:29:17.260 --> 01:29:19.260]  всплыло в конце
[01:29:19.260 --> 01:29:21.260]  курса.
[01:29:21.260 --> 01:29:23.260]  Ребят, есть ли какие-то вопросы
[01:29:23.260 --> 01:29:25.260]  Данилу?
[01:29:29.260 --> 01:29:31.260]  Так, ну
[01:29:31.260 --> 01:29:33.260]  не знаю, видимо
[01:29:33.260 --> 01:29:35.260]  вопросов нет.
[01:29:35.260 --> 01:29:37.260]  Вот.
[01:29:37.260 --> 01:29:39.260]  Нет вопросов. Давайте благодарим Данила
[01:29:39.260 --> 01:29:41.260]  за сделанный доклад,
[01:29:41.260 --> 01:29:43.260]  лекцию прочитанную. Он был
[01:29:43.260 --> 01:29:45.260]  интересный. Как раз правильные
[01:29:45.260 --> 01:29:47.260]  детали были проговорены.
[01:29:47.260 --> 01:29:49.260]  Ну и тогда, наверное,
[01:29:49.260 --> 01:29:51.260]  все. На этом курс заканчивается.
[01:29:51.260 --> 01:29:53.260]  Ну и дальше следите
[01:29:53.260 --> 01:29:55.260]  за новостями. Данил, большое
[01:29:55.260 --> 01:29:57.260]  спасибо. Спасибо, что откликнулись.
[01:29:57.260 --> 01:29:59.260]  Спасибо.
[01:29:59.260 --> 01:30:01.260]  Да, хорошо.
[01:30:01.260 --> 01:30:03.260]  Вроде тогда все. Да, коллеги,
[01:30:03.260 --> 01:30:05.260]  спасибо. До свидания.
[01:30:05.260 --> 01:30:07.260]  До свидания.

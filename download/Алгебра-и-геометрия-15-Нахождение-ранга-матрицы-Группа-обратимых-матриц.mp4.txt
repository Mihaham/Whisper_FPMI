[00:00.000 --> 00:14.880]  Так, добрый день, прошу прощения за опоздание. Давайте мы с вами будем продолжать. Мы закончили в прошлый раз
[00:14.880 --> 00:28.360]  на том, что доказали теорему ранги матрицы и определили таким образом ранг матрицы.
[00:28.360 --> 00:37.600]  Сегодня мы выясним, давайте все-таки это сниму, пока я далеко от вас, сегодня мы выясним каким
[00:37.600 --> 00:49.360]  образом этот ранг искать, но перед этим хочется еще одну характеризацию ранга сообщить. Для этого нам
[00:49.360 --> 01:04.760]  потребуется небольшое определение. Матрица A размера n на n, обратите внимание, я говорю про квадратную
[01:04.760 --> 01:25.560]  матрицу, называется невырожденной, если ранг этой матрицы равен n. Больше чем n он, естественно,
[01:25.560 --> 01:31.160]  быть не может, потому что в ней всего лишь навсего n строк или n столбцов, правильно? Когда он
[01:31.160 --> 01:36.440]  достигает максимального возможного значения, матрица называется невырожденной. Сразу хочу сказать,
[01:36.440 --> 01:43.200]  это понятие реже применяется к прямоугольным матрицам, если применяется, то имеется в виду,
[01:43.200 --> 01:48.400]  конечно же, что матрица невырождена, когда она максимального возможного ранга для этого размера,
[01:48.400 --> 01:55.720]  то есть сколько это будет, если это будет n на k? Минимум из n и k, правильно? Потому что ранг
[01:55.720 --> 02:00.600]  все равно в любом случае не больше чем n и не больше чем k, когда он достигает,
[02:00.600 --> 02:05.360]  тогда иногда тоже говорят, что матрица невырождена максимального возможного ранга.
[02:05.360 --> 02:23.640]  Итак, значит теорема о базисном миноре. Что такое базисный минор, я сейчас скажу. Итак,
[02:23.640 --> 02:32.640]  пусть A это некоторая матрица, не обязательно квадратная, возможно прямоугольная, ранга R,
[02:32.640 --> 02:53.520]  тогда в A есть невырожденная подматрица размера R на R.
[02:53.520 --> 03:07.160]  Давайте мы сразу оставим небольшое замечание, чтобы стало понятно, почему именно это факт нам
[03:07.160 --> 03:14.200]  интересен. Естественно, если мы из матрицы выкинем строчку, строчный ранг этой матрицы не может
[03:14.200 --> 03:19.600]  увеличиться, он может только уменьшиться, правильно? То же самое со столбцовым рангом
[03:19.600 --> 03:25.120]  при выкидывании из столбца, ну а ранг у нас один и тот же, значит что строчный, что столбцовый.
[03:25.120 --> 03:33.680]  В результате очевидное замечание при выкидывании строк из столбцов ранг не увеличивается, правильно?
[03:33.680 --> 03:53.780]  Ранг не увеличивается, если матрица была ранга R, то при выкидывании из нее строки столбцов ее
[03:53.780 --> 04:00.320]  ранг не может стать больше, чем R, то есть невырожденной матрице размера R плюс 1 вы уже точно не
[04:00.320 --> 04:06.440]  получите, правильно? Потому что тогда у нее ранг был бы R плюс 1, а это невозможно. То есть вот этот
[04:06.440 --> 04:12.480]  теоретическим наибольший ранг, который можно получить выкидыванием строки столбцов, и вот нам
[04:12.480 --> 04:29.160]  говорят, что оказывается, матрица R на R обязательно найдется. Значит, невырожденной под матрице R плюс 1
[04:29.160 --> 04:45.240]  на R плюс 1 не найдется. Ну вот я сформулировал теорему о базисном миноре в том виде, в котором
[04:45.240 --> 04:54.680]  ее очень часто формулируют, на самом деле мы докажем несколько более сильный факт. Давайте я
[04:54.680 --> 05:03.000]  его даже сформулирую, потому что иногда полезно пользоваться именно им. Что значит, что A это
[05:03.000 --> 05:09.480]  матрица ранга R? Это значит, что в ней найдется система из R строк, которая будет линейно-независимой,
[05:09.480 --> 05:16.520]  правильно? И найдется R столбцов, образующих линейно-независимую систему. Вот оказывается,
[05:16.520 --> 05:22.400]  если вы такие найдете, R строк и R столбцов, то на них пересечение обязательно будет та матрица,
[05:22.400 --> 05:42.760]  которая нам нужна. В этом и состоит усиление. Значит, еще раз, в матрице ранга R на пересечении R
[05:42.760 --> 05:49.760]  линейно-независимых строк, давайте я формулирую кратко, но мы понимаем, что формально это означает R
[05:49.760 --> 06:01.640]  строк, образующих линейно-независимую систему. И R линейно-независимых столбцов обязательно,
[06:01.640 --> 06:10.240]  в пересечении, да, грамотно говорить, в пересечении R линейно-независимых строк и R линейно-независимых
[06:10.240 --> 06:24.520]  столбцов обязательно образуется невырожденная подматрица. Вот я еще раз, я это подчеркнул на
[06:24.520 --> 06:31.480]  доске, давайте я еще раз это словами скажу, условие, что ранга R важно. Если вы возьмете не R,
[06:31.480 --> 06:37.440]  а, скажем, R-1 линейно-независимую строку и R-1 линейно-независимый столбец, вам никто
[06:37.440 --> 06:43.680]  не гарантирует, что подматрица на пересечении будет невырожденной, только когда мы берем максимально
[06:43.680 --> 06:58.120]  возможное количество. И, естественно, этим мы с вами сейчас будем пользоваться. Мы можем взять R-1
[06:58.120 --> 07:03.800]  независимую строку и R-1 независимый столбец, на пересечение не обязана получиться невырожденная
[07:03.800 --> 07:09.800]  подматрица. Там может получиться матрица меньшего ранга, чем R-1. А вот если мы берем именно столько,
[07:09.800 --> 07:17.840]  каков ранг матрицы, вот тогда обязательно это будет. Вот, доказательства. Ну вот давайте я
[07:17.840 --> 07:25.320]  доказательства оформлю на несколько формальном языке матриц, но я объясню, что за ним скрывается.
[07:25.320 --> 07:39.160]  Значит, давайте мы ведем обозначение. А это наша матрица. Вот я рисую всю матрицу А. В ней есть
[07:39.160 --> 07:52.120]  какие-то R-линейно-независимых строк. Давайте мы обозначим через B подматрицу из R-линейно-независимых
[07:52.120 --> 08:04.320]  строк. Грубо говоря, вот у нас B. Эти строки могут идти не подряд, но давайте я схематично это буду
[08:04.320 --> 08:24.200]  рисовать. C это подматрица из R-линейно-независимых столбцов. Вот у нас C. Ну а D это на их пересечении
[08:24.200 --> 08:34.880]  та самая подматрица R на R, про которую нам нужно доказать, что она не вырождена. Их пересечение. Нам
[08:34.880 --> 08:43.120]  хочется доказать, что D не вырожденная подматрица. Значит, давайте разбираться вот с чем. Как мы будем
[08:43.120 --> 08:52.320]  пользоваться тем, что строк у нас именно R, а никак не меньше. Очень просто. Мы с вами берем R-линейно-независимых
[08:52.320 --> 08:59.840]  строк. Если мы к этим R-строкам добавим еще одну строку матрицы A, получится уже зависимая система,
[08:59.840 --> 09:08.600]  правильно? Потому что мы выбрали ранг именно строк. Это означает, что через строки B, мы с вами это
[09:08.600 --> 09:23.640]  уже говорили, выражаются все строки матрицы A. Как это нам записать? Мы уже говорили, чтобы нам записать,
[09:23.640 --> 09:31.640]  что строки A выражаются через строки B, мы можем написать, что A это матрица B, умноженная на какую-то
[09:31.640 --> 09:43.880]  матрицу с какой стороны? Слева, правильно? Дайте я L умножить на B скажу. Когда я матрицу L умножаю на
[09:43.880 --> 09:52.480]  строки на матрицу B, у меня получаются в полученной матрице линейные комбинации строк матрицы B. И,
[09:52.480 --> 09:57.400]  естественно, как мы с вами в прошлый раз уже говорили, любые такие линейные комбинации мы
[09:57.400 --> 10:03.360]  получить можем, то, что строки A выражаются через строки B, записывается ровно вот таким вот образом.
[10:03.360 --> 10:17.600]  Аналогично, мы с вами знаем, что через столбцы матрицы C выражаются все столбцы матрицы A, то есть мы
[10:17.600 --> 10:27.600]  можем написать A равно C умножить на какую-то другую матрицу, правильно? Значит, давайте я еще раз
[10:27.600 --> 10:43.080]  напомню интуиции. В строках матрицы L что стоит? Коэффициенты, с которыми выражаются строки A через
[10:43.080 --> 10:51.560]  вот эти вот базерсные строки, правильно? Мы говорим, что строка A это строка L умножить на матрицу B. То есть мы
[10:51.560 --> 10:56.640]  взяли строки матрицы B с коэффициентами из строчки L. Вот здесь вот стоят те самые коэффициенты,
[10:56.640 --> 11:03.720]  с помощью которых мы выражаем строки A через строки B. Аналогично в столбцах R стоят ровно
[11:03.720 --> 11:16.080]  такие вот коэффициенты. Ну а теперь глядите, что мы можем сказать. Мы можем сказать, что если строки A
[11:16.080 --> 11:25.720]  выражаются через строки B с какими-то коэффициентами, то, естественно, строки матрицы C будут выражаться
[11:25.720 --> 11:31.560]  через строки матрицы D с теми же самыми коэффициентами, правильно? Строки матрицы C это
[11:31.560 --> 11:36.840]  же просто части строк матрицы A, под строки, как бы, правильно? Естественно, они с теми же коэффициентами
[11:36.840 --> 11:45.920]  будут выражаться через строчки D. Отсюда я могу написать, что строки C выражаются с теми же коэффициентами
[11:45.920 --> 11:53.280]  через строки D. Я объяснил смысл эти коэффициентов из L, и поэтому вот то, что я написал, и значит с теми
[11:53.280 --> 12:04.320]  же коэффициентами. Так, и так, строки C выражаются через строки D, а значит, здесь я могу заменить C на L, D.
[12:04.320 --> 12:20.560]  Извините, равно просто, правильно? Здесь вот равно. Матрица A это L на D на R. Но мы с вами доказывали в
[12:20.560 --> 12:27.120]  прошлый раз, что когда я буду брать ранг произведения матриц, он не превосходит ранга любого из
[12:27.120 --> 12:34.600]  совмножителей, правильно? И значит, ранг A не превосходит ранга любого из этих трех
[12:34.600 --> 12:46.160]  совмножителей, меня интересует ранг D. Но D это под матрица A. Мы с вами говорили, что ранг D не может
[12:46.160 --> 12:57.920]  превосходить ранга A, правда? Ну, значит, ранг D равен рангу A, что мы и хотели доказать.
[12:57.920 --> 13:16.400]  А, можно на правую, да, спасибо. Вот, значит, наша теорема уже доказана. Еще раз, смысл этой теоремы,
[13:16.400 --> 13:24.480]  давайте я смысл доказательств еще раз вкратце намечу. Мы из матрицы D можем получить матрицу
[13:24.480 --> 13:30.960]  A вот каким образом. Сначала мы получаем C, беря линейные комбинации строчек D, правильно? От этого
[13:30.960 --> 13:36.960]  ранг не меняется, не добавляется. После этого мы добавляем столб C, беря линейные комбинации
[13:36.960 --> 13:42.960]  столбцов C. При всех этих операциях ранг матрицы не увеличивается. Ну, значит, ранг A равен рангу D.
[13:42.960 --> 13:50.800]  Можно вот это доказательство сформулировать еще и таким образом. Так, ну что ж, давайте двигаться
[13:50.800 --> 14:03.520]  дальше. Следующий вопрос, разумеется, практически как найти ранг матрицы. Ответ на этот вопрос дают
[14:03.520 --> 14:17.120]  следующие два утверждения. Утверждение первое. Я опять же таки сначала сформулирую не совсем
[14:17.120 --> 14:23.160]  полную версию, потом я его уточню, потому что уточнение тоже очень полезно. Итак, ранг матрицы
[14:23.160 --> 14:40.640]  не меняется при элементарных преобразованиях строк. Разумеется из столбцов тоже, правильно?
[14:40.640 --> 14:49.240]  Поскольку строки из столбцы у нас аналогичны, если я буду брать элементарные преобразования строк,
[14:49.240 --> 14:55.920]  или если я буду делать элементарные преобразования столбцов, ранг матрицы не меняется. Значит,
[14:55.920 --> 15:05.640]  более точно давайте я сформулирую вот что. При элементарных преобразованиях строк
[15:05.640 --> 15:27.920]  не меняются, внимание, линейные зависимости столбцов. Что это означает? Если в матрице А какие-то
[15:27.920 --> 15:33.440]  столбцы были зависимы с какими-то коэффициентами, то есть можно было взять их линейную комбинацию с
[15:33.800 --> 15:40.080]  коэф и получить ноль, то после элементарного преобразования эти же столбцы будут зависимы
[15:40.080 --> 15:50.720]  с теми же самыми коэффициентами. Но естественно, из вот этого более точного утверждения следует
[15:50.720 --> 15:57.040]  первое, если столбцы были линейно зависимы, они после элементарного преобразования останутся
[15:57.040 --> 16:02.200]  линейно зависимыми. Если столбцы были линейно независимы, они остаются линейно независимыми
[16:02.200 --> 16:10.200]  после преобразования, потому что линейные зависимости не меняются, они остаются, не исчезают и не появляются.
[16:10.200 --> 16:18.200]  Ну и значит, если было R линейных независимых столбцов, то и останется R линейных независимых столбцов, но не больше.
[16:18.200 --> 16:27.200]  Значит, первое утверждение следует из второго. Давайте мы это запишем, да?
[16:27.200 --> 16:35.200]  Из второго сразу, но сказать это стоит. А второе доказать уже очень просто.
[16:35.200 --> 16:49.200]  Значит, пусть столбцы A зависимы с какими-то коэффициентами.
[16:49.200 --> 17:01.200]  Давайте мы все эти коэффициенты запишем в один столбец. Гамма – это столбец коэффициента.
[17:01.200 --> 17:04.200]  Как нам это вкратце записать?
[17:04.200 --> 17:14.200]  Что столбцы матрицы A линейно-зависимы, столбец коэффициентов – это гамма.
[17:14.200 --> 17:27.200]  Что значит взять линейную комбинацию столбцов A с коэффициентами гамма?
[17:27.200 --> 17:36.200]  Взять эти столбцы, умножить каждый на какую-то гамму И, сложить. Долго объясняем, да?
[17:36.200 --> 17:49.200]  Проще говоря, если я A умножу на гамму, это же и есть взять линейную комбинацию столбцов A с коэффициентами из гаммы.
[17:49.200 --> 17:54.200]  Получится нулевой столбец.
[17:54.200 --> 18:02.200]  Ну пусть теперь S – это какая-то матрица элементарного преобразования.
[18:02.200 --> 18:11.200]  То есть мы матрицу A, применяя элементарное преобразование, заменяем на матрицу S на A.
[18:11.200 --> 18:21.200]  Но раз A гамма было равно нулю, то и SA на гамму тоже равно нулю, правда?
[18:21.200 --> 18:27.200]  Это и означает, что линейная зависимость осталась той же самой.
[18:27.200 --> 18:35.200]  То есть линейная зависимость сохранилась.
[18:35.200 --> 18:44.200]  Все линейные зависимости сохранились, почему не появилось новых? Ровно так же.
[18:44.200 --> 18:51.200]  Если у преобразованной матрицы есть какая-то линейная зависимость столбцов,
[18:51.200 --> 18:59.200]  то есть если мы можем SA умножить на какой-то столбец гамму и получить ноль,
[18:59.200 --> 19:03.200]  то мы можем совершить обратное элементарное преобразование.
[19:03.200 --> 19:09.200]  Мы знаем, что все элементарные преобразования обратимы, все их матрицы тоже обратимы, правильно?
[19:09.200 --> 19:14.200]  Мы можем домножить на S-1.
[19:14.200 --> 19:17.200]  А это есть A гамма.
[19:17.200 --> 19:22.200]  Таким образом мы понимаем, что если линейная зависимость столбцов сейчас есть,
[19:22.200 --> 19:26.200]  то она и до преобразования тоже была.
[19:26.200 --> 19:31.200]  А значит, линейные зависимости сохраняются.
[19:31.200 --> 19:36.200]  Итак, линейные зависимости сохраняются, ранг матрицы сохраняется,
[19:36.200 --> 19:41.200]  поэтому мы вполне можем применить к матрице метод гауса
[19:41.200 --> 19:48.200]  и привести ее хотя бы к ступенчатому виду, дальше не надо.
[19:48.200 --> 19:55.200]  А дальше вступает в силу второе утверждение,
[19:55.200 --> 20:10.200]  что ранг ступенчатой матрицы равен числу ее ступенек.
[20:10.200 --> 20:20.200]  То есть числу не нулевых строх, правильно?
[20:20.200 --> 20:25.200]  Ну и после этого утверждения алгоритм нахождения ранга матрицы становится уже ясен.
[20:25.200 --> 20:39.200]  Берем произвольную матрицу, приводим ее к ступенчатому виду, количество ступенек – это ответ, правильно?
[20:39.200 --> 20:45.200]  Давайте мы это на всякий случай запишем.
[20:45.200 --> 20:55.200]  Значит, приведя матрицу к ступенчатому виду,
[20:55.200 --> 21:03.200]  элементарными преобразованиями строк, естественно, найдем ее ранг.
[21:03.200 --> 21:09.200]  Обратите еще раз внимание, к упрощенному виду приводить не надо достаточно к ступенчатому, правильно?
[21:09.200 --> 21:13.200]  Осталось доказать утверждение.
[21:13.200 --> 21:20.200]  А когда у матрицы ранг равен количеству не нулевых строк ее?
[21:20.200 --> 21:23.200]  Когда они все линейно независимы?
[21:23.200 --> 21:28.200]  Ранг же – это наибольший размер независимой системы, которую мы можем выбрать.
[21:28.200 --> 21:33.200]  Больше чем не нулевых строк мы не можем выбрать независимых строк, правильно?
[21:33.200 --> 21:39.200]  Значит, нам нужно просто доказать, что эти строки линейно независимы.
[21:39.200 --> 21:42.200]  Итак, давайте мы введем какие-нибудь обозначения.
[21:42.200 --> 21:46.200]  Пусть у нас А – это ступенчатая матрица.
[21:46.200 --> 21:49.200]  Давайте мы как-нибудь ступеньки обозначим.
[21:49.200 --> 21:56.200]  Здесь А – первая строка, и столбец с номером G1.
[21:56.200 --> 22:01.200]  Во второй строке главный элемент имеет номер G2.
[22:01.200 --> 22:03.200]  И так далее.
[22:03.200 --> 22:12.200]  В пятой строке главный элемент имеет номер G с индексом K.
[22:12.200 --> 22:17.200]  Нам надо доказать, давайте мы это запишем,
[22:17.200 --> 22:25.200]  что все не нулевые строки линейно независимы.
[22:31.200 --> 22:39.200]  Давайте мы предположим, что это не так, и существует линейная зависимость.
[22:39.200 --> 22:45.200]  Давайте я ее запишу следующим образом.
[22:45.200 --> 22:49.200]  Сумма по И от 1 до K.
[22:49.200 --> 22:55.200]  Гамма Ит – это коэффициент зависимости, умножается на Ит-ую строчку матрицы А.
[22:55.200 --> 23:01.200]  Напоминаю, что Ит-ая строчка мы договаривались обозначать вот так вот, и со звездочкой.
[23:01.200 --> 23:07.200]  Предположим, что они линейно зависимы, то есть эта линейная комбинация равна нулевой строке.
[23:07.200 --> 23:11.200]  0 – это, естественно, нулевая строка.
[23:11.200 --> 23:19.200]  И не все Гамма Ит нули.
[23:19.200 --> 23:27.200]  Мы предположили, что у нас есть зависимость, то есть эта линейная комбинация нетривиальна.
[23:27.200 --> 23:33.200]  Но тогда можно выбрать самые первые, не ноль.
[23:33.200 --> 23:39.200]  Выберем наименьшее.
[23:39.200 --> 23:45.200]  И такое, что Гамма Ит не ноль.
[23:45.200 --> 23:51.200]  Ну давайте вот на рисунке, пусть у меня это И будет двойкой для порядка.
[23:51.200 --> 23:59.200]  Мы знаем, что если я возьму линейную комбинацию элементов вот этого столбца, столбца с номером G2 в данном случае,
[23:59.200 --> 24:03.200]  то есть G Ит-ая в нашем в общем случае.
[24:03.200 --> 24:09.200]  Если я возьму линейную комбинацию вот этих вот элементов с коэффициентными Гаммами, Гамма Итами, то я получу ноль, правильно?
[24:09.200 --> 24:13.200]  Потому что я линейную комбинацию всех строк беру, получаю ноль.
[24:13.200 --> 24:15.200]  А как это будет выглядеть?
[24:15.200 --> 24:21.200]  Первые элементы будут умножаться на Гамма с номерами меньшими, чем И, то есть теми, которые нули, правда?
[24:21.200 --> 24:27.200]  Этот не нулевой элемент умножится на Гамма Ит, который не ноль и даст нам не ноль.
[24:27.200 --> 24:31.200]  Следующие элементы здесь равны нулю, потому что у нас матрица ступенечная.
[24:31.200 --> 24:37.200]  Давайте мы запишем это формально.
[24:37.200 --> 24:45.200]  В столбце с номером G Ит-ая получим следующую вещь.
[24:49.200 --> 24:51.200]  Значит, что ноль
[24:51.200 --> 24:59.200]  Эта сумма по Слушайте, И плохо, да?
[24:59.200 --> 25:07.200]  Давайте я сумму буду писать не по И, а по какому-нибудь Т.
[25:07.200 --> 25:15.200]  Т от одного до К, Гамма Ит-ая, Гамма Т-ая, А с индексом Т, G Ит-ая.
[25:15.200 --> 25:19.200]  Именно, да, я беру столбец с номером G Ит.
[25:19.200 --> 25:27.200]  Ну, формально я могу написать так. Сумма по Т от единицы до И минус одного вот этих вот товарищей.
[25:31.200 --> 25:39.200]  Плюс Гамма Ит на А и G Ит, да?
[25:39.200 --> 25:48.200]  И плюс сумма по Т от И плюс одного до К того же самого.
[25:49.200 --> 25:55.200]  Это вот я как раз разбил сумму на верхнюю часть, вот этот элемент и нижнюю часть, правильно?
[25:55.200 --> 26:00.200]  Первая сумма равна нулю, потому что Гаммы равны нулю, по нашему выбору.
[26:00.200 --> 26:04.200]  Последняя сумма равна нулю, потому что Ашки равны нулю.
[26:04.200 --> 26:07.200]  Вот этот вот товарищ не равен нулю.
[26:07.200 --> 26:15.200]  Мы выбрали И, так что Гамма Ит-ая не ноль, и А и G Ит-ая это главный элемент в еды строке, то есть он тоже не ноль.
[26:15.200 --> 26:20.200]  Значит, вся эта сумма отлична от нуля. Противоречие.
[26:35.200 --> 26:39.200]  Можно было бы и так, но и так, по-моему, достаточно просто, правильно?
[26:39.200 --> 26:44.200]  В принципе, да, можно было бы и так с упрощенным видом, доказательство бы еще упростилось.
[26:44.200 --> 26:46.200]  Но то он и упрощенный.
[26:47.200 --> 26:54.200]  Замечательно, и мы с вами ранг матрицы умеем искать.
[26:54.200 --> 27:09.200]  Более того, заметьте, пожалуйста, что наш алгоритм, вот глядите, нам говорят, что если ранг матрицы равен R,
[27:09.200 --> 27:22.200]  то существует система, ну, существует линейно-независимая система из R столбцов.
[27:22.200 --> 27:28.200]  И на самом деле мы ее этим алгоритмом тоже найдем.
[27:28.200 --> 27:38.200]  То есть мы найдем не только ранг матрицы, но и то, какие именно столбцы в исходной матрице будут линейно-независимыми.
[27:38.200 --> 27:42.200]  Мы сможем указать эти R линейно-независимых столбцов.
[27:42.200 --> 27:54.200]  А именно это будут главные столбцы ступенчатой матрицы.
[27:58.200 --> 28:06.200]  Если мы возьмем ступенчатую матрицу и возьмем ее главные столбцы, почему они будут образовывать линейно-независимую систему?
[28:06.200 --> 28:10.200]  Потому что они по-прежнему будут образовывать ступенчатую матрицу, правильно?
[28:10.200 --> 28:18.200]  Они будут образовывать ступенчатую матрицу с R ступеньками, и поэтому они будут по-прежнему линейно-независимыми.
[28:18.200 --> 28:21.200]  Эта матрица будет иметь ранг R.
[28:21.200 --> 28:28.200]  А в матрице A какие столбцы мы можем указать?
[28:28.200 --> 28:35.200]  Столбцы с теми же самыми номерами, потому что мы знаем, что линейные зависимости столбцов не меняются, правильно?
[28:35.200 --> 28:47.200]  Раз в полученной, в приведенной к ступенчатому виду матрице A, столбцы с такими номерами линейно-независимы, то и в исходной матрице A столбцы с теми же самыми номерами были линейно-независимыми.
[28:47.200 --> 29:03.200]  И столбцы означат столбцы с теми же номерами в матрице A.
[29:03.200 --> 29:17.200]  В частности, если угодно, это способ искать базисные столбцы в любой матрице.
[29:17.200 --> 29:24.200]  В частности, если вам заданы, например, несколько векторов в пространстве своими координатами,
[29:24.200 --> 29:28.200]  то базис своих линейной оболочки вы можете найти тем же самым способом.
[29:29.200 --> 29:35.200]  Это вы просто можете записать их координаты в одну матрицу и найти ее базисные столбцы.
[29:35.200 --> 29:53.200]  В частности, если у вас есть, давайте я сюда даже перейду, v1 и так далее, vm,
[29:54.200 --> 29:59.200]  заданы вам, если вам заданы несколько векторов своими координатами в базисе,
[30:01.200 --> 30:06.200]  скажем, e умножить на s, но это я и говорю, что эти векторы заданы координатами в каком-то базисе,
[30:06.200 --> 30:12.200]  потому что каждый столбец матрицы s даст нам координатный столбец одного из этих векторов, правильно?
[30:12.200 --> 30:31.200]  То базис линейной оболочки v1 и так далее vm образуют векторы, давайте мы это проговорим,
[30:31.200 --> 30:46.200]  координатные столбцы которых будут базисными в s, то есть если мы возьмем матрицу s,
[30:46.200 --> 30:52.200]  возьмем ее ранг, он естественно будет равен, напоминаю на всякий случай,
[30:52.200 --> 30:58.200]  ранг s равен размерности линейной оболочки столбцов s,
[30:58.200 --> 31:02.200]  ну то есть размерности линейной оболочки соответствующих векторов,
[31:02.200 --> 31:07.200]  если мы возьмем столько независимых столбцов матрицы s,
[31:07.200 --> 31:14.200]  соответствующие векторы будут образовывать базис вот этой вот линейной оболочки,
[31:14.200 --> 31:19.200]  ну а такие столбцы в s мы только что искать научились,
[31:19.200 --> 31:24.200]  знаете, вот подобный класс практических задач у нас с вами уже решается.
[31:24.200 --> 31:35.200]  Давайте уж немножко дополним нашу теорию линейных уравнений,
[31:35.200 --> 31:42.200]  теперь мы с вами можем сформулировать несколько теоретических утверждений,
[31:42.200 --> 31:50.200]  связанных с этими системами,
[31:50.200 --> 31:58.200]  дополнение про системы линейных уравнений, как ранг матрицы может прилагаться туда,
[31:58.200 --> 32:11.200]  ну например, вот так, теорема, пусть у нас есть однородная система линейных уравнений,
[32:11.200 --> 32:17.200]  мы ее записываем в матричном виде, как ax равно нулю, с n переменными,
[32:17.200 --> 32:27.200]  ну то есть столбец x имеет длину n, мы с вами знаем, что тогда ее решения образуют
[32:27.200 --> 32:32.200]  под пространство, в пространстве всех столбцов, давайте мы его обозначим через u,
[32:32.200 --> 32:40.200]  u это пространство ее решений,
[32:40.200 --> 32:50.200]  тогда размерность этого пространства это n-ранг A,
[32:50.200 --> 32:56.200]  количество переменных минус ранг матрицы A,
[32:57.200 --> 33:05.200]  доказательства, ну давайте мы сформулируем, что же такое размерность u,
[33:05.200 --> 33:15.200]  размерность u равно количеству столбцов,
[33:15.200 --> 33:19.200]  ну, количеству базисных векторов в базисе, правильно,
[33:19.200 --> 33:25.200]  а базисные вектора это точности столбцы фундаментальной матрицы,
[33:25.200 --> 33:31.200]  правильно, мы говорили, что фундаментальная матрица, как раз ее столбцы это базис пространства решений,
[33:31.200 --> 33:37.200]  количество столбцов фундаментальной матрицы,
[33:37.200 --> 33:42.200]  что такое количество столбцов фундаментальной матрицы, сколько их там будет,
[33:42.200 --> 33:50.200]  вот в терминах нашего решения системы линейных уравнений,
[33:50.200 --> 34:00.200]  спасибо, это мы только что уже написали, как его выразить через ступенчатый вид нашей матрицы,
[34:00.200 --> 34:08.200]  неправда, количество свободных переменных, правильно,
[34:08.200 --> 34:14.200]  количество столбцов фундаментальной матрицы это количество свободных переменных,
[34:14.200 --> 34:20.200]  и мы знаем, что через свободные переменные все переменные выражаются правильно в решении,
[34:20.200 --> 34:26.200]  ровно однозначно, и у нас там в матрице даже единичная матрица стояла на местах свободных переменных,
[34:26.200 --> 34:32.200]  поэтому, конечно же, количество столбцов фундаментальной матрицы,
[34:32.200 --> 34:38.200]  это то же самое, что количество свободных переменных,
[34:38.200 --> 34:44.200]  количество это, естественно, n, главных переменных,
[34:46.200 --> 34:52.200]  ну а главных переменных у нас по одной на ступеньку,
[34:54.200 --> 34:58.200]  поэтому количество главных переменных это количество ступеней,
[34:58.200 --> 35:05.200]  в ступенчатом виде нашей матрицы А, здесь у нас получается n минус количество ступенек,
[35:05.200 --> 35:14.200]  в ступенчатом виде матрицы А, то есть в той ступенчатой матрице,
[35:14.200 --> 35:22.200]  которой мы А приведем. Ну а мы с вами знаем, что количество ступенек – это ранг А.
[35:22.200 --> 35:29.200]  Здесь у нас получается n-ранг А.
[35:29.200 --> 35:34.200]  Размерность вот этого самого пространства мы с вами знаем.
[35:38.200 --> 35:51.200]  Продолжаем разговор. Ещё одна важная, хотя и сейчас для нас уже простая, теорема.
[35:51.200 --> 36:00.200]  Первая теорема была о однородных системах, сейчас будет о неоднородных.
[36:00.200 --> 36:11.200]  Теорема носит имена двух математиков – Кроникер и Капелли.
[36:11.200 --> 36:16.200]  Часто называют ее теорема Кроникер и Капелли, это два разных человека.
[36:16.200 --> 36:26.200]  Если мы рассмотрим неоднородную систему, то иногда она будет совместна и иногда нет.
[36:26.200 --> 36:32.200]  Теорема не в этом. Теорема о том, когда она совместна.
[36:32.200 --> 36:42.200]  Система АХ равно B совместна тогда и только тогда, когда ранг матрицы системы, то есть матрица А,
[36:42.200 --> 36:51.200]  равен рангу расширенной матрицы системы, то есть матрицы А с приписанной к ней столбцом B.
[36:57.200 --> 37:07.200]  Доказательства можно вести на разных языках, но давайте я даже на языке системы линейных уравнений.
[37:12.200 --> 37:22.200]  Давайте мы приведем расширенную матрицу нашей системы к ступенчатому виду.
[37:22.200 --> 37:25.200]  Элементарными преобразованиями строк, естественно.
[37:25.200 --> 37:34.200]  Если мы эту матрицу приведем к ступенчатому виду, то и матрица А тоже приведется к ступенчатому виду автоматически, правильно?
[37:34.200 --> 37:46.200]  Тогда А тоже приведется к ступенчатому виду.
[37:50.200 --> 37:52.200]  Ну и что тогда это значит?
[37:52.200 --> 37:58.200]  Тогда ранг А равен рангу расширенной матрицы.
[38:00.200 --> 38:02.200]  Как это условие понять?
[38:04.200 --> 38:09.200]  Это означает, что в матрице А будет столько же ступенек, сколько в расширенной матрице.
[38:09.200 --> 38:15.200]  Точнее, ступенчатом виде матрицы А будет столько же ступенек, сколько в ступенчатом виде расширенной матрицы.
[38:15.200 --> 38:20.200]  А значит новые ступеньки в столбце свободных членов не появятся, правильно?
[38:21.200 --> 38:28.200]  Нет ступеньки в столбце свободных членов.
[38:28.200 --> 38:52.200]  Ну а это, как мы с вами говорили, в точности означает, что система совместна, потому что мы с вами говорили, что вот единственное противоречие, которое у нас может возникнуть, это когда мы получаем уравнение 0 равно не нулевой константе, то есть как раз ступеньку в столце свободных шлемов.
[38:52.200 --> 38:56.200]  И таким образом наша теорема уже доказана.
[38:56.200 --> 39:06.200]  На всякий случай давайте я скажу, что другое доказательство этой теоремы можно получить из более глубокого смысла системы уравнений.
[39:06.200 --> 39:15.200]  Что означает решить эту систему? Это означает найти, как столбец В выражается через столбцы матрицы А.
[39:15.200 --> 39:21.200]  Желающие могут довести это до другого доказательства теоремы Кроники Ракапелли.
[39:21.200 --> 39:25.200]  Так, ну давайте мы продолжим.
[39:25.200 --> 39:30.200]  Про ранг матрицы мы более-менее наверное сказали все, что хотели.
[39:30.200 --> 39:37.200]  И в общем теперь мы про ранг, про систему линейных уравнений, в общем про метод Гаусса скорее даже.
[39:37.200 --> 39:43.200]  Знаем достаточно, чтобы разобраться с еще одним вопросом.
[39:43.200 --> 39:55.200]  Давайте мы выясним, какие матрицы, у каких матриц существует обратное, то есть какие матрицы обратимы.
[39:55.200 --> 40:13.200]  Итак, мы выясняем, какие матрицы обратимы.
[40:13.200 --> 40:21.200]  Говоря научным языком, давайте я заодно напомню это обозначение.
[40:21.200 --> 40:32.200]  Иначе говоря, мы сейчас с вами ищем, знаете что, мы с вами ищем кольце матриц размера N на N.
[40:32.200 --> 40:37.200]  Мы знаем, что это кольцо, их можно складывать, вычитать и перенажать друг с другом, правильно?
[40:37.200 --> 40:49.200]  Кольце матриц мы ищем группу обратимых элементов. Да, это квадратные матрицы, только квадратные матрицы бывают обратимыми.
[40:49.200 --> 40:54.200]  Сейчас я даже это напомню. Что значит, что матрица обратима?
[40:54.200 --> 41:07.200]  Это значит, что матрица A обратима, это значит, что существует A в минус 1 такая, что их произведение в любом порядке равно E.
[41:07.200 --> 41:14.200]  Но если матрица A будет не квадратной, то вот такие матрицы не могут иметь даже одного и того же размера.
[41:14.200 --> 41:24.200]  Поэтому уж если матрица обратима вот в этом смысле, а это стандартный смысл, то она обязана быть квадратной.
[41:24.200 --> 41:32.200]  Итак, мы с вами ищем вот такую вот группу. Давайте я сразу веду обозначение для нее стандартное.
[41:32.200 --> 41:40.200]  Стандартное обозначение для этой группы выглядит вот так вот.
[41:40.200 --> 42:01.200]  F это наше поле, N это размер наших матриц, буквы же L стоят для следующих двух слов, которые вы можете считать английскими, хотя изначально, насколько я понимаю, они все-таки считались французскими.
[42:01.200 --> 42:13.200]  Как мы скоро с вами поймем, это общее линейное преобразование n-мерного пространства.
[42:13.200 --> 42:15.200]  Оттуда берутся эти слова.
[42:15.200 --> 42:23.200]  Вот пока что g, l, n, t в стандартное обозначение для группы обратимых матриц размера n на n.
[42:23.200 --> 42:32.200]  Ну и мы хотим его, мы хотим элементы этой группы описать, и вот вам сразу несколько таких описаний.
[42:32.200 --> 42:50.200]  Пусть A это матрица размера n на n над полем F, тогда равносильны следующие условия.
[42:50.200 --> 43:05.200]  Первое условие A не вырождено, но, напоминаю, это означает, что ранг A равен вот тому самому n.
[43:05.200 --> 43:23.200]  Второе условие, матрица A элементарными преобразованиями строк приводится к единичной матрице.
[43:23.200 --> 43:31.200]  Можно совершить элементарные преобразования строк с матрицей A, получив единичную.
[43:31.200 --> 43:40.200]  Третье условие, матрица A, есть произведение нескольких элементарных матриц.
[43:50.200 --> 43:55.200]  Ну и четвертое и пятое условие, до которых мы как раз и хотели добраться.
[43:56.200 --> 44:14.200]  Четвертое условие, матрица A обратима, и пятое условие, матрица A обратима слева или справа.
[44:14.200 --> 44:26.200]  Что это означает? Обратимость матрицы слева означает, что существует матрица B, такая, чтобы A равно E.
[44:26.200 --> 44:35.200]  Обратимость справа означала бы, что существует матрица B, такая, чтобы AB равно E.
[44:35.200 --> 44:45.200]  Все эти 5 условий равносильны. В частности, я обращу внимание на несколько полезных равносильностей.
[44:45.200 --> 44:52.200]  A обратима тогда и только тогда, когда A не вырождено, тогда и только тогда, когда A произведение элементарных матриц.
[44:52.200 --> 44:57.200]  Любую матрицу можно разложить в произведении элементарных.
[44:57.200 --> 45:06.200]  Ну и вот пятый пункт тоже полезен. Он говорит нам, что если вы выяснили для матрицы квадратной, что она обратима с одной стороны,
[45:06.200 --> 45:14.200]  то автоматически тот самый обратный будет обратным с обеих сторон к этой матрице. Это тоже порой полезно.
[45:16.200 --> 45:23.200]  Ну давайте доказывать на самом деле с высот той теории, с которой мы с вами уже познакомились.
[45:23.200 --> 45:28.200]  Сейчас все шаги будут достаточно простыми. Мы сейчас быстренько пройдем по циклу.
[45:28.200 --> 45:33.200]  То есть скажем, что из каждого условия следует следующее, а из пятого следует первое.
[45:43.200 --> 45:51.200]  Мы с вами говорили, что в любом кольце группа обратимых элементов – это группа, а не пустая она, потому что там есть нейтральный элемент.
[45:54.200 --> 45:58.200]  Тогда же и говорили, собственно. Правильно?
[45:59.200 --> 46:05.200]  Вот так. Как у нас из первого свойства следует второе? Пусть матрица не вырождена.
[46:09.200 --> 46:14.200]  Ранг A равен N. Приведем ее к упрощенному виду.
[46:23.200 --> 46:32.200]  Ну естественно, как обычно, когда мы это говорим, мы подразумеваем, что мы это делаем элементарными преобразованиями строк.
[46:35.200 --> 46:44.200]  А что такое этот упрощенный вид? У нас и ранг матрицы равен N. Это значит, что у нас будет N-ступенник, правильно?
[46:45.200 --> 46:53.200]  И N главных переменных. Главных переменных, ну то есть главных столбцов.
[46:56.200 --> 47:02.200]  N, то есть все столбцы главные, правильно? И они выглядят как столбцы единичной матрицы.
[47:03.200 --> 47:10.200]  Ну значит у нас и получилась единичная матрица. Этот упрощенный вид – это E.
[47:11.200 --> 47:16.200]  Просто каждый главный столбец у нас. Мы знаем, как выглядит в упрощенном виде.
[47:19.200 --> 47:22.200]  А кроме главных столбцов у нас ни на что места и не осталось.
[47:23.200 --> 47:26.200]  Таким образом, этот переход у нас уже доказан.
[47:26.200 --> 47:35.200]  Так, доказываем из 2-го пунктов 3-ти. Пусть мы A каким угодно способом привели к виду E – элементарными преобразованиями строк.
[47:38.200 --> 47:42.200]  Что это означает? Что означает применить к матрице элементарные преобразования строк?
[47:43.200 --> 47:45.200]  Это нужно ножить ее слева, а не вверх.
[47:45.200 --> 47:46.200]  Строк.
[47:50.200 --> 47:54.200]  Что это означает? Что означает применить к матрице элементарные преобразования строк?
[47:55.200 --> 47:58.200]  Это нужно ножить ее слева на элементарную матрицу, правильно?
[47:59.200 --> 48:08.200]  Значит, если мы A привели единичные матрицы элементарными преобразованиями строк,
[48:08.200 --> 48:12.200]  это значит, что мы A домножили на какие-то элементарные матрицы.
[48:13.200 --> 48:16.200]  S1, потом на S2, потом и так далее, потом на SKT.
[48:17.200 --> 48:19.200]  Так, чтобы получилось е.
[48:22.200 --> 48:26.200]  Но мы с вами знаем, что все элементарные матрицы обратимы.
[48:28.200 --> 48:33.200]  И поэтому это равенство мы сейчас можем домножить на обратное к ним.
[48:34.200 --> 48:36.200]  В каком порядке мы будем это делать?
[48:38.200 --> 48:40.200]  Сначала на SKT в минус 1 домножим, правильно?
[48:41.200 --> 48:43.200]  Потом на SKT в минус 1 домножим и так далее.
[48:44.200 --> 48:47.200]  И когда мы это все домножим, мы получим, что A есть.
[48:48.200 --> 48:52.200]  S1 в минус 1, S2 в минус 1 и так далее, SKT в минус 1.
[48:54.200 --> 49:00.200]  Но мы с вами знаем не только то, что элементарные матрицы обратимы, а что обратные тоже элементарны.
[49:01.200 --> 49:03.200]  Поэтому третий пункт мы с вами уже доказали.
[49:04.200 --> 49:06.200]  Все это элементарные матрицы.
[49:09.200 --> 49:16.200]  Так, да, я там тоже должен был поставить такой пустой треугольничек, что этот пункт мы доказали.
[49:21.200 --> 49:24.200]  Так, из третьего в четвертое.
[49:25.200 --> 49:28.200]  Но мы снова пользуемся тем, что элементарные матрицы обратимы, правильно?
[49:28.200 --> 49:42.200]  Если A это произведение каких-то элементарных матриц, то конечно же A в минус 1 это будет произведение их обратных.
[49:43.200 --> 49:45.200]  В каком порядке? В обратном, правильно?
[49:46.200 --> 49:47.200]  Мы с вами уже говорили.
[49:48.200 --> 49:49.200]  S1 в минус 1.
[49:50.200 --> 49:57.200]  Эти все, можно было так сказать, что раз эти все товарищи лежат в нашей группе, то естественно их произведение тоже лежит.
[49:58.200 --> 50:00.200]  Часть доказательства того, что это группа.
[50:05.200 --> 50:15.200]  Иначе говоря, мы это с вами получили из того, что мультипликативная группа кольца – это группа.
[50:19.200 --> 50:21.200]  Так, самая сложная импликация.
[50:27.200 --> 50:28.200]  Очевидно, правильно?
[50:33.200 --> 50:37.200]  Уж если матрица обратима, то она обратима и слева, и справа.
[50:37.200 --> 50:38.200]  И справа.
[50:43.200 --> 50:46.200]  Наконец последний переход, который нам сейчас замкнет весь этот цикл.
[50:54.200 --> 51:01.200]  Если мы с вами знаем, что скажем BA равно E, для обратимости справа рассуждение будет естественно аналогично,
[51:02.200 --> 51:07.200]  то мы можем применить неравенство о рангах для произведения матриц.
[51:08.200 --> 51:12.200]  Мы с вами знаем, что ранг E не превосходит ранга каждого из совножителей.
[51:13.200 --> 51:15.200]  В частности, ранга A.
[51:16.200 --> 51:21.200]  Но ранг E естественно равен N, а ранг A естественно не превосходит N.
[51:25.200 --> 51:27.200]  Значит ранг A равен N.
[51:27.200 --> 51:32.200]  Если матрица обратима хотя бы с одной стороны, это уже означает, что она не вырождена.
[51:33.200 --> 51:35.200]  Для AB равна E будет то же самое.
[51:36.200 --> 51:42.200]  Все, мы с вами доказали, что эти пять пунктов равносильны, и можем этим свободно пользоваться.
[51:45.200 --> 51:48.200]  Поскольку был такой вопрос, да если бы его и не было, то я бы все равно оставил.
[51:49.200 --> 51:52.200]  Давайте я оставлю такое упражнение.
[51:52.200 --> 52:08.200]  Давайте мы предположим, что матрица A прямоугольная, типа обратима с левой и справа.
[52:09.200 --> 52:16.200]  То есть пусть для этой матрицы существуют матрицы B и C, сейчас станет понятно в каких размерах,
[52:17.200 --> 52:27.200]  такие что B это обратная матрица для A слева, C это обратная матрица для A справа.
[52:28.200 --> 52:31.200]  Обратите, пожалуйста, внимание, я не пишу, что все это равно одному и тому же,
[52:32.200 --> 52:36.200]  потому что здесь у меня написаны единичные матрицы разных размеров.
[52:37.200 --> 52:43.200]  Здесь у меня размеры какие? У матрицы A каст толпцов, вот у этой матрицы тоже будет каст толпцов.
[52:43.200 --> 52:49.200]  Значит, е, это е размера k на k, вот это е будет размера n на n.
[52:53.200 --> 52:59.200]  Даже в таком случае это возможно только когда матрица квадрат.
[52:59.200 --> 53:19.200]  Ну а разговор о обратимых матрицах стоит, конечно же, закончить алгоритмом для нахождения обратной матрицы.
[53:29.200 --> 53:51.200]  Ещё раз, мы с вами знаем давным-давно, что если у матрицы, если у элемента группы есть левый обратный и правый обратный, то они совпадают.
[53:52.200 --> 53:55.200]  Раз B это левый обратный, то она же будет и правая.
[53:55.200 --> 54:01.200]  То есть, если вы нашли левую обратную, то AB тоже будет равно е.
[54:02.200 --> 54:04.200]  Это вот так, для квадратных матриц, естественно.
[54:07.200 --> 54:15.200]  Ну и давайте поймём, как, исходя из всего этого великолепия, находить A в минус 1.
[54:16.200 --> 54:19.200]  На самом деле, вот здесь вот этот алгоритм уже написан.
[54:20.200 --> 54:23.200]  Для ясности давайте мы его сформулируем вот как.
[54:24.200 --> 54:33.200]  Давайте мы возьмём матрицу A и запишем вот какую матрицу A с приписанной к ней матрицей E.
[54:36.200 --> 54:44.200]  И затем эту матрицу элементарными преобразованиями строк.
[54:44.200 --> 54:56.200]  Если A в минус 1 существует, то значит, мы эту матрицу элементарными преобразованиями строк можем привести к такой, в которой в левой половине стоит E, правильно?
[54:57.200 --> 55:00.200]  Это вот наш второй пункт. Приведём к матрице E.
[55:03.200 --> 55:06.200]  Левую часть, правая часть приведётся к какой-то матрице B.
[55:07.200 --> 55:16.200]  Тогда, на самом деле, B это A в минус 1.
[55:17.200 --> 55:20.200]  Потому что, глядите, давайте посмотрим просто на наше доказательство.
[55:21.200 --> 55:29.200]  Мы сказали, что A привелась к матрице E какими-то элементарными преобразованиями, домножением на какие-то элементарные матрицы.
[55:30.200 --> 55:35.200]  Тогда A это произведение их обратных, а A в минус 1 у нас стоит ровно здесь, правильно?
[55:37.200 --> 55:40.200]  Но давайте мы это проговорим.
[55:48.200 --> 56:04.200]  Если мы применили какие-то элементарные преобразования к матрице A, получили E, то вот то, на что мы её домножили, это, разумеется, A в минус 1 хотя бы потому что левая обратная к A, правильно?
[56:05.200 --> 56:07.200]  Как мы с вами только что говорили.
[56:10.200 --> 56:12.200]  Ну а, что это означает?
[56:13.200 --> 56:27.200]  Когда мы применяли элементарные преобразования к вот этой вот матрице, S1, когда мы применяем это к этой матрице, мы, по сути дела, применяем их и к матрице A, и к матрице E.
[56:27.200 --> 56:37.200]  То есть, в первой половинке у нас получилось E, а в правой половинке у нас как раз получилось то, что будет из E. Получится из E применение вот этих вот матриц.
[56:38.200 --> 56:40.200]  То есть, просто это произведение, правильно?
[56:40.200 --> 56:57.200]  Вот таким образом вот этот метод нам даёт обратную матрицу.
[56:58.200 --> 57:08.200]  Желающие могут на этот метод посмотреть и с другой стороны, вот разные взгляды на эти процессы бывают полезными, поэтому давайте я объясню этот же процесс чуть с другой стороны.
[57:09.200 --> 57:13.200]  Мы справа от черты приписывали с вами столбцы свободных членов, правильно?
[57:14.200 --> 57:21.200]  Можно рассмотреть этот процесс как решение систем нескольких линейных уравнений.
[57:22.200 --> 57:28.200]  A на X равно первому столбцу единичной матрицы, A на X равно второму столбцу единичной матрицы и так далее.
[57:29.200 --> 57:39.200]  И когда мы A приводим к единичному виду, то здесь стоят как раз решения этих систем, которые и образуют матрицу A-1.
[57:40.200 --> 57:42.200]  Можно воспринять это вот в таком вот ключе.
[57:43.200 --> 57:53.200]  Так, наверное, на этом про обратные матрицы я уже сказал всё, что хотел, и мы с вами переходим к новой теме.
[57:54.200 --> 58:01.200]  Снова от матриц мы переходим к более абстрактным понятиям, к понятиям линейных пространств.
[58:04.200 --> 58:16.200]  И следующая наша тема – сумма и пересечение подпространств.
[58:16.200 --> 58:24.200]  Если у нас есть, давайте я опишу глобальную ситуацию, в которой мы находимся.
[58:25.200 --> 58:39.200]  Если у нас есть какое-то V, это пространство, линейное пространство над полем F,
[58:40.200 --> 58:46.200]  и пусть U1 и U2 – это два его подпространства.
[58:47.200 --> 58:57.200]  Мы с вами будем сейчас по сути дела выяснять какие-то факты относительно того, как эти подпространства расположены друг относительно друга.
[58:58.200 --> 59:08.200]  И первый шаг в этом – это если у нас есть два подпространства в одном и том же пространстве V, мы можем построить два новых подпространства.
[59:10.200 --> 59:18.200]  Утверждение первое – если мы их пересечем, то тоже получим подпространство V.
[59:23.200 --> 59:30.200]  Доказательства. Давайте мы, напоминаю, нам всегда нужно проверять, если у нас есть подпространство.
[59:31.200 --> 59:36.200]  Первым делом нам нужно проверить, что оно не пусто, если мы хотим, чтобы у нас получилось подпространство.
[59:37.200 --> 59:45.200]  Это пересечение, разумеется, не пусто, поскольку в нем есть 0, правильно?
[59:50.200 --> 59:56.200]  Ну и дальше нам нужно проверить, напоминаю, что оно замкнуто относительно операций.
[59:57.200 --> 01:00:04.200]  Ну и эта проверка, разумеется, непосредственна, правильно?
[01:00:05.200 --> 01:00:15.200]  Если A и B лежат в пересечении, это значит, что они лежат в каждом из наших пространств.
[01:00:16.200 --> 01:00:20.200]  A и B лежат в U1 и лежат в U2.
[01:00:20.200 --> 01:00:27.200]  Ну это значит, что сумма этих векторов лежит тоже в U1 и лежит в U2.
[01:00:28.200 --> 01:00:31.200]  Ну то есть эта сумма лежит в их пересечении.
[01:00:35.200 --> 01:00:43.200]  Естественно, аналогично, если мы возьмем произвольный скаляр и произвольный вектор из пересечения,
[01:00:44.200 --> 01:00:50.200]  то альфа на А тоже будет лежать в первом пространстве и во втором, а значит, и в их пересечении.
[01:00:51.200 --> 01:00:57.200]  Итак, любые два подпространства можно пересечь и получить новое подпространство.
[01:00:58.200 --> 01:01:06.200]  Мы с вами привыкли к тому, что к пересечению существует двойственная дополнительная операция, которая называется объединение.
[01:01:07.200 --> 01:01:10.200]  Но объединение подпространств подпространством будет очень редко.
[01:01:10.200 --> 01:01:15.200]  Желающие могут подумать когда и только.
[01:01:16.200 --> 01:01:17.200]  Вот это вот и стоит проверить.
[01:01:18.200 --> 01:01:23.200]  Поэтому вместо объединения у нас возникает другая операция.
[01:01:27.200 --> 01:01:32.200]  Давайте я даже напишу определение, хотя в принципе это определение мы уже давали.
[01:01:33.200 --> 01:01:46.200]  Пусть у1 и у2 это подпространство В, тогда их сумма это их сумма.
[01:01:48.200 --> 01:01:57.200]  Ну то есть, напоминаю, мы с вами вот так вот договорились обозначать множество всех сумм элемента отсюда и элемента отсюда, правильно?
[01:01:58.200 --> 01:02:03.200]  У1 лежит в У2.
[01:02:10.200 --> 01:02:24.200]  Ну и естественно, определение это я дал, стоит доказать, что сумма двух подпространств также является подпространством В.
[01:02:28.200 --> 01:02:38.200]  Ну давайте мы сразу докажем, то, что оно не пусто, очевидно, правильно?
[01:02:39.200 --> 01:02:45.200]  Почему очевидно? Потому что ноль это сумма нуля и нуля, правда?
[01:02:45.200 --> 01:02:56.200]  Опять же таки, проверяем замутность относительно операций.
[01:02:57.200 --> 01:03:06.200]  Давайте мы возьмем два вектора в У1 плюс У2, давайте я вот так вот напишу сразу, и сразу распишу их по компонентам.
[01:03:06.200 --> 01:03:18.200]  Если вектор А лежит в этой сумме, это значит, что он представляется в виде А1 плюс А2, где каждый из слагаемых лежит в соответствующем подпространстве, правильно?
[01:03:19.200 --> 01:03:25.200]  И вектор В, лежащий там, также раскладывается вот в таком вот случае, в таком вот виде.
[01:03:25.200 --> 01:03:51.200]  То есть если у нас в У1 плюс У2 лежат вот такие вот векторы, где АИТ и БИТ лежат естественно у ИТ, то сумма этих векторов мы ее можем перегруппировать следующим образом А1 плюс Б1 и плюс А2 плюс Б2.
[01:03:51.200 --> 01:04:04.200]  Ну и разумеется, поскольку У1 было подпространством, то вот эта вот сумма лежит в У1, поскольку У2 было подпространством, то вот эта вот сумма лежит в У2. То есть это тоже лежит в сумме.
[01:04:05.200 --> 01:04:15.200]  Естественно, совершенно аналогично проверяется, даже проще, проверяется замутность относительно умножения на скаляр.
[01:04:15.200 --> 01:04:32.200]  Если я А1 плюс А2 умножу на скаляр, это будет А1 плюс А2. Этот товарищ лежит в У1, этот товарищ лежит в У2.
[01:04:33.200 --> 01:04:37.200]  И мы наше утверждение догадали.
[01:04:38.200 --> 01:04:44.200]  Итак, мы с вами ввели два понятия суммы и пересечения.
[01:04:45.200 --> 01:04:49.200]  Ну, стоит, наверное, все-таки заметить.
[01:04:51.200 --> 01:04:59.200]  Пересечение мы можем брать нескольких подпространств, и понятное дело, что это не зависит ни от чего.
[01:05:00.200 --> 01:05:18.200]  Но давайте я на всякий случай все-таки скажу, что сумма подпространств – это операция ассоциативная, ну и коммутативная, естественно.
[01:05:19.200 --> 01:05:37.200]  То есть если я буду складывать, например, три подпространства, то мне можно не ставить здесь скобки, потому что все равно это будет множеством всех сумм вот таких вот троек векторов.
[01:05:38.200 --> 01:05:44.200]  И значит, как я здесь скобки не расставляю, все равно получится вот такое вот множество.
[01:05:44.200 --> 01:05:49.200]  Поскольку сложение векторов ассоциативно, то и сложение подпространств тоже.
[01:05:50.200 --> 01:05:54.200]  Это, кстати, вот общее свойство, которое часто стоит замечать.
[01:05:55.200 --> 01:06:03.200]  Если у нас есть какие-то свойства операции с векторами, то они очень часто наследуются операциями с множествами векторов, вот такими как здесь.
[01:06:03.200 --> 01:06:13.200]  Давайте я на самом деле это оформлю в качестве утверждения.
[01:06:14.200 --> 01:06:21.200]  Другая характеризация суммы. Давайте я немножко по-другому опишу, что такое сумма.
[01:06:21.200 --> 01:06:37.200]  Пусть у1 это подпространство, порожденное каким-то множеством s1, у2 это подпространство, порожденное каким-то множеством s2.
[01:06:38.200 --> 01:06:43.200]  Ну, s1, s2 это какие-то подмножества в, разумеется, правильно?
[01:06:44.200 --> 01:07:01.200]  Тогда сумма наших двух подпространств, можно сказать, что это линейная оболочка объединения, чтобы связать уж наконец сумму с тем объединением, о котором мы говорили,
[01:07:02.200 --> 01:07:08.200]  или что то же самое, это линейная оболочка объединения вот этих вот порождающих множеств.
[01:07:13.200 --> 01:07:23.200]  Доказательство очень простое.
[01:07:28.200 --> 01:07:36.200]  Напоминаю, что такое линейная оболочка какого-то множества. Это самое маленькое подпространство, которое содержит это множество, правильно?
[01:07:36.200 --> 01:07:43.200]  А с другой стороны, это набор, это множество всех линейных комбинаций элементов вот того, что у нас тут написано, правда?
[01:07:44.200 --> 01:07:58.200]  Вот, из всего этого должно быть очевидно следующее. Во-первых, эта линейная оболочка лежит в этой линейной оболочке,
[01:07:58.200 --> 01:08:03.200]  просто потому что мы сюда добавили еще каких-то векторов, правильно?
[01:08:04.200 --> 01:08:18.200]  S1 содержится в U1, S2 содержится в U2. Дальше, вторая линейная оболочка содержится в сумме наших U1 и U2. Почему?
[01:08:18.200 --> 01:08:25.200]  Потому что в сумме U1 и U2 содержится U1. Это правда?
[01:08:26.200 --> 01:08:33.200]  Как нам получить любой вектор из U1? Прибавить к нему 0, правда? И содержится U2.
[01:08:34.200 --> 01:08:43.200]  Это подпространство, содержащее вот это множество, правильно? А значит, содержит его линейную оболочку просто по определению нашей линейной оболочки.
[01:08:43.200 --> 01:08:48.200]  Линейная оболочка – самое маленькое подпространство. Это содержится в U1 плюс U2.
[01:08:49.200 --> 01:08:56.200]  Ну а с другой стороны, U1 плюс U2 содержится вот в такой вот линейной оболочке,
[01:08:57.200 --> 01:09:01.200]  ибо векторы вот эти вот – линейная комбинация элементов S1 и S2, правда?
[01:09:02.200 --> 01:09:10.200]  Векторы из U1 – это линейная комбинация элементов из S1. Векторы из U2 – это линейная комбинация элементов U2.
[01:09:10.200 --> 01:09:15.200]  Значит, векторы из суммы – это линейная комбинация всего вот этого вот объединения.
[01:09:16.200 --> 01:09:21.200]  Векторы U1 – это линейная комбинация элементов…
[01:09:22.200 --> 01:09:28.200]  Так, векторы из U1 – это линейная комбинация элементов из S1.
[01:09:29.200 --> 01:09:33.200]  Ну и все. Из этих включений, разумеется, следует, что все они равны.
[01:09:33.200 --> 01:09:40.200]  Так, ну что ж, сумму мы с вами определили и даже описали ее свойства.
[01:09:41.200 --> 01:09:45.200]  Естественно, ну давайте я в качестве следствия это напишу.
[01:09:49.200 --> 01:09:58.200]  Если у Иты это линейная оболочка S1, то, разумеется, и сумма наших комбинаций,
[01:09:58.200 --> 01:10:09.200]  то, разумеется, и сумма наших К, скажем, подпространств – это будет линейная оболочка объединения этих множеств.
[01:10:10.200 --> 01:10:16.200]  Его следствие следует оттуда, поэтому я даже, наверное, не буду этого доказывать.
[01:10:16.200 --> 01:10:34.200]  А следствие отсюда заключается в том, что размерность суммы мы можем оценить как сумму размерностей у Итых.
[01:10:46.200 --> 01:11:01.200]  Как это следует оттуда? Мы можем в качестве S1 взять самое маленькое множество, которое порождает у Иты, то есть базис у Итым.
[01:11:02.200 --> 01:11:10.200]  Если в качестве S1 взять базис у Итым, в любом подпространстве есть базис,
[01:11:10.200 --> 01:11:22.200]  то мы с вами получим, что сумма наших подпространств U1 и т.д. плюс УКТ.
[01:11:26.200 --> 01:11:34.200]  Так, давайте я вот это, то, что у меня написано справа, обозначу через D, например.
[01:11:35.200 --> 01:11:44.200]  D U1 и т.д. плюс D UKT. Мы получим, что сумма наших подпространств порождена D векторами.
[01:11:47.200 --> 01:11:53.200]  Ну а раз подпространство порождено D векторами, то его размерность не больше, чем D.
[01:11:58.200 --> 01:12:03.200]  Хотя бы потому, что из этих векторов можно выбрать базис в этом пространстве.
[01:12:04.200 --> 01:12:10.200]  Значит, размерность U1 и т.д. плюс UKT не превосходит D.
[01:12:15.200 --> 01:12:20.200]  Так, продолжаем разговор.
[01:12:26.200 --> 01:12:31.200]  А это мы сейчас как раз будем выяснять, ну точнее не сейчас, а больше на следующей лекции.
[01:12:31.200 --> 01:12:36.200]  Этот вопрос мы как раз очень подробно выясним.
[01:12:37.200 --> 01:12:46.200]  Если в этом неравенстве достигается равенство, это специальное понятие, о котором мы будем сейчас достаточно много говорить.
[01:12:47.200 --> 01:12:53.200]  Но перед тем, как к нему ровно перейти, я сделаю замечательнее.
[01:13:02.200 --> 01:13:10.200]  Если сумма U1 и U2 совпадает с суммой U1 и U3, разумеется, из этого внимания не следует, что U2 равно U3.
[01:13:22.200 --> 01:13:25.200]  Давайте придумаем какой-нибудь простой пример.
[01:13:25.200 --> 01:13:33.200]  Давайте возьмем двумерную плоскость и возьмем на ней три одномерных подпространства.
[01:13:34.200 --> 01:13:42.200]  Подпространство в двумерной плоскости это прямая, проходящая через ноль, по сути дела, правильно?
[01:13:43.200 --> 01:13:54.200]  Сумма любых двух разных прямых, это будет вся плоскость, потому что она будет спорождаться вот такими вот двумя векторами, правильно?
[01:13:55.200 --> 01:14:03.200]  Здесь U1 плюс U2 и U1 плюс U3 и U2 плюс U3, все это будет вся наша плоскость.
[01:14:09.200 --> 01:14:15.200]  Это все V2. В то же время эти подпространства, естественно, разные.
[01:14:16.200 --> 01:14:32.200]  Ну и давайте я все-таки, особенно раз меня спросили, дам определение и начнем с этого, с этим работать понятием, но основная работа с тем понятием, которое я определю, конечно, будет завтра.
[01:14:32.200 --> 01:14:40.200]  Итак, определение. Пусть U1 и так далее, Укатая, это подпространство В.
[01:14:43.200 --> 01:14:57.200]  Мы хотим взять их сумму. И вот их сумма называется, вот давайте я сейчас избыточность немножко в речи скажу.
[01:14:58.200 --> 01:15:03.200]  Я мог бы сказать, их сумма называется прямой, но у нас бывает слово прямая в другом смысле.
[01:15:04.200 --> 01:15:13.200]  Поэтому давайте я скажу, их сумма называется прямой суммой, если выполнено вот какое свойство.
[01:15:13.200 --> 01:15:28.200]  Глядите, мы с вами уже знаем, что любой вектор из суммы этих подпространств представляется в виде суммы k-векторов по одному из этих подпространств, правильно?
[01:15:29.200 --> 01:15:35.200]  Так вот, сумма прямая, если такое представление для каждого вектора из их суммы единственное.
[01:15:35.200 --> 01:15:53.200]  То есть для любого вектора V из этой суммы существует единственный набор U1 из U1, U2 из U2 и так далее, Укатая из Укатая, сумма которых равна V.
[01:15:53.200 --> 01:16:08.200]  Таких, что V это U1 плюс и так далее плюс Укатая.
[01:16:08.200 --> 01:16:31.200]  Значит, обозначение для этой прямой суммы, если сумма подпространств прямая,
[01:16:31.200 --> 01:16:40.200]  то она записывается следующим образом.
[01:16:41.200 --> 01:16:53.200]  То есть, по сути дела, когда я пишу вот такое вот выражение, я говорю, что мы рассматриваем сумму этих подпространств, и мы уже знаем, что она прямая.
[01:16:53.200 --> 01:17:02.200]  Естественно, вот так вот ее записать тоже можно, но вот эта зависть подчеркивает, что эта сумма не простая, а прямая.
[01:17:03.200 --> 01:17:13.200]  Ну и давайте, чтобы на определении не заканчивать, у нас еще одна минута есть вроде как, да? Нету? Есть.
[01:17:13.200 --> 01:17:36.200]  Что докажем утверждение? Сумма подпространств прямая тогда и только тогда, когда существует единственный набор векторов из подпространств уитых, дающий в сумме нулевой вектор.
[01:17:36.200 --> 01:17:50.200]  Иначе говоря, нам говорят, что любой вектор должен представляться единственным образом, а утверждение говорит, что это достаточно проверить для нулевого вектора, правильно?
[01:17:51.200 --> 01:17:58.200]  Если нулевой вектор представляется единственным образом вот в таком вот виде, то и любой вектор представляется единственным образом.
[01:17:59.200 --> 01:18:08.200]  Слева направо. Следствие очевидно, правильно? Раз уж любой вектор представляется, то и нулевой тоже.
[01:18:09.200 --> 01:18:17.200]  Справа налево. Если какой-то вектор представился двумя способами, давайте вот я напишу эти два способа.
[01:18:17.200 --> 01:18:28.200]  У1 пишет и т.д. плюс укатая и У1 штрих плюс и т.д. плюс укатая штрих. Естественно, каждый из слагаемых лежит в соответствующем подпространстве.
[01:18:29.200 --> 01:18:44.200]  Вот это вот лежит в У1, это лежит в У1 тоже. То нам достаточно вычесть одно из этих представлений из другого.
[01:18:44.200 --> 01:18:58.200]  Давайте мы вычтем правое из левого и получим, что У1 минус У1 штрих плюс У2 минус У2 штрих плюс и т.д. плюс укатая минус укатая штрих.
[01:18:59.200 --> 01:19:07.200]  Это ноль. Если мы предположили, что ноль представляется единственным способом, то что же это за способ?
[01:19:07.200 --> 01:19:19.200]  Это все нули, правильно? Вот этот вот единственный способ, это представление в виде суммы нескольких нулей, потому что нулитов уитых точно есть, правда?
[01:19:20.200 --> 01:19:35.200]  Ну и мы с вами получили, что уитая минус уитая штрих это ноль, поскольку мы предполагаем, что ноль представляется единственным способом, естественно все эти слагаемые лежат в соответствующих подпространствах.
[01:19:36.200 --> 01:19:44.200]  Ну а это значит, что уитая равно уитому штриху. Вот мы и проверили, что эти представления совпадают.
[01:19:46.200 --> 01:19:52.200]  Утверждение доказано, более глубокие свойства прямой суммы. Следующий раз на сегодня все.

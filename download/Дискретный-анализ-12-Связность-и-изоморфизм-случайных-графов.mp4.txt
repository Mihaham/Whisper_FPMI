[00:00.000 --> 00:12.840]  Так, а лекции-то у нас по-прежнему пафосные. Все очень содержательно, интересно, и сегодня мы
[00:12.840 --> 00:20.340]  продолжим разбираться с компонентами связанности, причем вы находитесь в таком удивительном
[00:20.340 --> 00:28.400]  моменте, когда я впервые читаю доказательства одной из частей некоторой теоремы, которую раньше
[00:28.400 --> 00:36.200]  я не доказывал совсем. Но благодаря тому, что мы часть теории графов перенесли в первый
[00:36.200 --> 00:42.400]  семестр, мне удается туда включить еще это. Это важно, это красиво очень, но просто до сих пор
[00:42.400 --> 00:49.640]  не успевалось. Значит, что мы доказали в прошлый раз? Давайте я напомню. Мы в прошлый раз доказали,
[00:49.640 --> 01:02.680]  что если вероятность ребра случайного графа имеет вид t логариф men поделить на n, то при t
[01:02.680 --> 01:18.680]  большем единице асимптатически почти, наверное, g от np связан, а при t меньшем единице асимптатически
[01:18.680 --> 01:32.840]  почти, наверное, g от np не связан. Вот так. Это мы доказали в прошлый раз. Но на самом деле это даже
[01:32.840 --> 01:38.920]  не самый, может быть, главный факт о случайных графах и их связанности, хотя очень важно. Я
[01:38.920 --> 01:45.560]  пытался продемонстрировать практическую составляющую этого дела, но вот есть еще один очень
[01:45.560 --> 01:51.240]  важный факт, который я обычно оставлял полностью без доказательства, а сегодня постараюсь доказать
[01:51.240 --> 02:02.840]  наполовину. Значит, пусть p теперь это c поделить на n, то есть выскакивает, не выскакивает, а наоборот
[02:02.840 --> 02:09.600]  уходит, выпадает это рассмотрение с множества логариф men. Ну то есть эта вероятность ребра совсем
[02:09.600 --> 02:18.400]  как бы маленькая, и понятно, что в этом случае случайный граф, конечно, связан не будет. Это
[02:18.400 --> 02:25.600]  понятно? Тут даже не c меньше единицы, это вообще в логариф men раз меньше. Конечно, связанности не
[02:25.600 --> 02:33.080]  будет. Тем не менее, опять имеет место фазовый переход через единичку. c меньше единицы и c
[02:33.080 --> 02:44.200]  больше единицы это принципиально разные вещи. Оказывается, что если c больше единицы, то
[02:44.200 --> 02:55.480]  асимптотически почти, наверное, не так, то существует такая... Нет, давайте, наверное, я все-таки с c меньше
[02:55.480 --> 03:02.120]  единицы начну, а то слишком громоздко будет виноват. Давайте, если c меньше единицы здесь сначала
[03:02.120 --> 03:14.280]  будет, значит, если c меньше единицы, то существует такая константа бета, большая нуля, то асимптотически
[03:14.280 --> 03:34.800]  почти, наверное, число вершин каждой связанной компоненте, ну случайного графа, конечно же,
[03:34.800 --> 03:44.920]  от np не больше, чем бета помножить на логариф men. То есть не просто граф разваливается, но он,
[03:44.920 --> 03:49.200]  конечно, разваливается, потому что мы находимся в таком режиме относительно первой теоремы,
[03:49.200 --> 03:56.240]  но он разваливается на крошечные компоненточки. Если всего n вершин, то в каждой компоненте не
[03:56.240 --> 04:08.200]  больше, чем бета логариф men. Очень существенная вещь, он совсем развален. Так, а второй пункт — это
[04:08.200 --> 04:18.080]  что происходит при c большем единице. Тут чуть более громоздко. Если c больше единицы, то существует,
[04:18.080 --> 04:25.720]  давайте, снова бета больше нуля, и существует гамма из интервала от нуля до единицы. Тут важно,
[04:25.720 --> 04:40.600]  что строгий интервал такие, что асимпатически, почти, наверное, g от np есть ровно одна компонента,
[04:40.600 --> 04:56.480]  не менее чем гамма n вершинами, а все остальные компоненты имеют не больше, чем бета лог n вершин.
[04:56.480 --> 05:21.080]  Остальные компоненты имеют не больше, чем бета логариф men вершин. В смысле,
[05:21.080 --> 05:30.680]  каждая из них имеет не больше, чем бета логариф men вершин. Каждая из них. Есть ровно одна гигантская
[05:30.680 --> 05:37.120]  компонента, в которой вершин порядка общего числа вершин в графе. Вот эта вот ровно одна
[05:37.120 --> 05:42.800]  компонента, она действительно официально совершенно называется гигантской компонентой случайного
[05:42.800 --> 06:04.280]  графа. Понятно говорю, да? Но это сильный ход. То есть, если в пункте один традиционная сарделька,
[06:04.280 --> 06:14.440]  обозначающая множество вершин, заполняется уже не как сарделька, а как огурец, такими крошечными
[06:14.440 --> 06:21.320]  компонентами размер, каждый из которых не превосходит логарифма по порядку, то здесь в пункте
[06:21.320 --> 06:28.040]  два возникает гигантская компонента, но я ее вот как-нибудь так нарисую, вы должны понимать,
[06:28.040 --> 06:33.080]  что вот эта гамма, она, конечно, может быть очень близка к нулю, но слава богу, она единая для
[06:33.080 --> 06:40.520]  всех n. То есть, когда растет число вершин, размер компонента растет пропорционально, но я не утверждаю,
[06:40.520 --> 06:49.120]  что гамма близка к единице. Чем ближе c к единице, тем, конечно, ближе гамма к нулю. Это, я надеюсь,
[06:49.120 --> 06:57.320]  интуитивно хотя бы понятно. Чем дальше c от единицы, тем жирнее становится гигантская компонента.
[06:57.320 --> 07:03.840]  Математически она гигантская в том смысле, что ее размер линейен по числу вершин, а это крошечные
[07:03.840 --> 07:09.960]  компоненты, а не логарифмические. Кстати, друзья, вы помните, когда мы занимались
[07:09.960 --> 07:17.160]  хроматическим числом случайного графа, вот этот режим тоже был в некотором смысле нами изучен,
[07:17.160 --> 07:29.560]  правда, я там ничего не доказал, но вы не помните. Я говорил так, что если c меньше единицы, а p равняется
[07:29.560 --> 07:37.480]  c поделительно, то это в скобках, это я напоминаю то, что было когда-то, то асимпатически почти
[07:37.480 --> 07:43.920]  наверное хроматическое число случайного графа не превосходит тройке, даже равно тройке.
[07:43.920 --> 07:52.960]  И пояснением к этому было, что все связанные компоненты этого графа это деревья или унициклические
[07:52.960 --> 08:03.360]  графы. Сейчас вспоминаете, нет? Нет, не помните, кто-то помнит. Ну было такое. Сейчас выясняется
[08:03.360 --> 08:12.320]  дополнительная вещь, вот эти деревья, унициклические графы, они крошечные. Возможно, знание, которое мы
[08:12.320 --> 08:18.600]  сегодня создадим, я это докажу, вот пункт один я докажу, поможет вам решить вот это как упражнение.
[08:18.600 --> 08:30.560]  Доказывать я это все равно не буду, но можете попробовать. Что говорите? Ну это да, это некая задача со
[08:30.560 --> 08:37.920]  звездочкой, да, да. А вам уже это выдали? Ну хорошо, да, это задача со звездочкой, а я вам подсказываю,
[08:37.920 --> 08:43.640]  что можно попробовать ее решить, например, зная вот этот результат. Ну может быть как-то по-другому,
[08:43.640 --> 08:50.320]  то есть не обязательно так, но этот результат может помочь. Вот, но сам по себе результат очень
[08:50.320 --> 08:56.080]  содержательный. Есть такая, понимаете, как бы эволюция случайного графа, эволюция прямо в смысле,
[08:56.080 --> 09:04.320]  как мировая история эволюционирует. Я люблю в этом месте рисовать такую как бы развлекательную
[09:04.320 --> 09:12.400]  картинку. Вот у нас есть два порога, один поделить на n и логариф men поделить на n, которые определяют
[09:12.400 --> 09:20.360]  как бы качественный скачок в истории мира. Ну мир, это как формируется граф, конечно. Значит,
[09:20.360 --> 09:25.440]  если мы находимся сильно ниже вот этого порога, первого про который мы только сегодня узнали,
[09:25.440 --> 09:33.520]  то у нас фактически имеет вместо феодализм, потому что граф распадается на крошечные вот такие
[09:33.520 --> 09:40.200]  вот феодики. Соответственно, то, что происходит вот в этой части, от одной n и до логариф men
[09:40.200 --> 09:47.760]  поделить на n, можно квалифицировать как появление империи. Гигантская компонента,
[09:47.760 --> 09:53.280]  это своего рода империя, вокруг которой все остальное, это, извините, не биполярный мир там
[09:53.280 --> 10:00.480]  никакой, там одна империя, а вокруг вот это логарифмическое охвосте, которое захватывает
[10:00.480 --> 10:05.920]  империя, захватывает мировое господство после перехода через вот этот, соответственно, порог.
[10:05.920 --> 10:12.960]  Ну потому что все, не остается никаких охвостей, а империя захватывает весь граф, он становится
[10:12.960 --> 10:21.200]  связным. Вот так вот это я обычно интерпретирую. На самом деле, в теории случайных графов, которые
[10:21.200 --> 10:26.800]  описывают какие-то более, может быть, даже реальные процессы, идущие в мире с какие-нибудь там сети
[10:26.800 --> 10:33.240]  интернет или экономических взаимодействий между банками, наличием транзакций и так далее. Вот такие
[10:33.240 --> 10:38.560]  вот фазовые переходы, они прям вот играют роль при оценке рисков, например, возникновения каких-нибудь
[10:38.560 --> 10:52.560]  финансовых кризисов. Добрый день. Я не знаю, я пришел, он тут стоит. Я пришел, да, было открыто.
[10:52.560 --> 11:00.560]  Нет, ну как никого нет, там были студенты еще, я уже вторую лекцию читаю подряд. Я потом сдам, конечно.
[11:08.560 --> 11:18.440]  Так, друзья, понятно все, да? То есть, да, это играет роль вообще во всей теории, очень существенная. Ну хорошо,
[11:18.440 --> 11:24.280]  я нацелился за сегодня, может быть, не только это сделать, но за сегодня уж точно доказать вот эту
[11:24.280 --> 11:32.320]  часть. А вторая останется без доказательства в курсе, ну раньше все оставалось, а сейчас вот так.
[11:32.320 --> 11:39.840]  Там метод как бы новый, поэтому я точно хочу это рассказать и всегда хотел новый по сравнению с
[11:39.840 --> 11:45.600]  тем, что мы делали. То есть, это не неравенство Маркова, там не неравенство Чебышова, а некий,
[11:45.600 --> 11:52.640]  ну в каком-то смысле ветвящийся процесс. У вас, конечно, никаких процессов еще не было, у вас
[11:52.640 --> 11:58.560]  только вероятность началась. Процессы будут ровно через год, Даша, больше они начнутся через год,
[11:58.560 --> 12:04.160]  весной третьего курса, но ничего из них не надо знать, не переживайте, я вам сейчас все расскажу,
[12:04.160 --> 12:10.640]  все будет понятно. Вот давайте для начала, прежде чем формально прописывать доказательства, возьмем
[12:10.640 --> 12:15.000]  просто какой-нибудь граф, какой-нибудь не обязательно случайный, просто конкретный граф,
[12:15.000 --> 12:27.680]  пока на нем нет никакого распределения. Я не знаю, нарисуем граф. Пусть в нем есть несколько
[12:27.680 --> 12:34.000]  связанных компонентов, вот пример графа, такой граф. Возьмем какую-нибудь его вершину, любую
[12:34.000 --> 12:45.440]  совершенно, и будем вести такой процесс, будем считать, что эта вершина живая, а все остальные
[12:45.600 --> 12:55.300]  вершины, ну вообще все остальные вершины, но не мертвые нет, пока не мертвые, нейтральные,
[12:55.300 --> 13:11.920]  да. Дайте считать, что она живая, а все остальные нейтральные. Вот из множества нейтральных вершин
[13:11.920 --> 13:18.000]  вершин, мы выбираем всех соседей вершины В в графе.
[13:18.000 --> 13:25.000]  На этом примере видно, что соседей два выбрали, обозвали
[13:25.000 --> 13:28.240]  их живыми, а вот эту таки кокнули.
[13:28.240 --> 13:34.920]  Но это такой процесс размножения гибели, ветвящийся процесс.
[13:34.920 --> 13:38.240]  Ну знаете, классический ветвящийся процесс, это
[13:38.240 --> 13:42.160]  просто, у вас есть организм в начале всех времен, он
[13:42.160 --> 13:47.600]  порождает какое-то количество потомков, а сам умирает,
[13:47.600 --> 13:50.600]  кто-то из его потомков порождает еще какое-то количество
[13:50.600 --> 13:54.440]  потомков, сам умирает, и так вот процесс идет, он
[13:54.440 --> 13:55.440]  может выродиться.
[13:55.440 --> 14:00.080]  Я понятно говорю, но я не хочу подробно рассказывать
[14:00.080 --> 14:02.320]  про классические процессы, это вам расскажут через
[14:02.320 --> 14:05.600]  больше чем год, но смысл совершенно понятен.
[14:05.760 --> 14:11.800]  Вот теперь у нас две живых, одна мертвая вершина, а все
[14:11.800 --> 14:15.400]  остальные нейтральные, вот все остальные пускай нейтральные.
[14:15.400 --> 14:20.960]  Теперь мы выбираем любую из живых, ну например выбираем
[14:20.960 --> 14:29.400]  вот эту, и смотрим ее соседей, ну это правда да, смотрим
[14:29.400 --> 14:34.120]  ее соседей среди нейтральных, но знаете тут какой-то организм
[14:34.120 --> 14:38.120]  родился и никого не породил, ну бывает к сожалению такое
[14:38.120 --> 14:42.120]  в природе, ну а в графе это не так трагично смотрится,
[14:42.120 --> 14:44.360]  у нее просто нет соседей, но важно, что я говорю про
[14:44.360 --> 14:47.880]  среди нейтральных, это важно для описания уже случайного
[14:47.880 --> 14:50.800]  процесса, с которым мы будем работать, среди нейтральных
[14:50.800 --> 14:56.120]  нет, ну кокаем ее, напрасно прожила свою жизнь, что
[14:56.120 --> 14:59.720]  поделать, у нас остается вот эта живая вершина,
[14:59.720 --> 15:02.400]  ну у нее слава богу среди тех вершин, которыми по-прежнему
[15:02.400 --> 15:06.200]  мы считаем нейтральными, есть целых две соседки,
[15:06.200 --> 15:10.280]  вот ее кокаем, эти все три мертвые, видите они уже
[15:10.280 --> 15:19.120]  мертвые стали, а эти две живые да, пока еще живые,
[15:19.120 --> 15:21.720]  пока еще живые, ну у этой живой вершины есть хоть
[15:21.720 --> 15:25.760]  одна соседка среди нейтральных, нейтральных, а ну нейтральные
[15:25.760 --> 15:29.760]  есть, но туда не попасть, соседок нет, да, то есть
[15:29.760 --> 15:33.680]  у нее среди нейтральных соседок нет, кок мертвой
[15:33.680 --> 15:38.520]  стала, кок мертвой стала, и все, что случилось на выходе,
[15:38.520 --> 15:42.160]  мы умерщвили все вершины в одной компоненте связанности,
[15:42.160 --> 15:44.560]  а именно в той, к которой относится выбранная нами
[15:44.560 --> 15:51.180]  исходная вершина В, согласны, ну то есть это такой возможный
[15:51.180 --> 15:57.520]  процесс обхода всей компоненты, ну можно по-другому как-то
[15:57.520 --> 15:59.920]  было определить, например, не из нейтральных, выбирать
[15:59.920 --> 16:04.600]  там из живых, но вот мы выбираем из нейтральных, так, друзья,
[16:04.600 --> 16:06.080]  всем понятно, что происходит?
[16:06.080 --> 16:08.240]  Теперь давайте считать, что граф случайный, все вот
[16:08.240 --> 16:11.760]  мы переходим прямо к доказательству, я объяснил, как процесс
[16:11.760 --> 16:15.920]  устроен на конкретном примере, теперь у нас случайный граф,
[16:15.920 --> 16:20.840]  и мы тем не менее фиксировали какую-то вершину В из нашей
[16:20.840 --> 16:23.120]  знаменитой сардельки, вон там целых два рисунка
[16:23.120 --> 16:27.880]  этой сардельки, В это вершины, сарделька, вот, берем какую-то
[16:27.880 --> 16:32.760]  вершину и запускаем такой же процесс, ну какой, давайте
[16:32.760 --> 16:46.680]  Y с индексом T будем обозначать число живых вершин, Z с индексом
[16:46.680 --> 16:51.760]  T, нет, давайте Z с индексом T чуть позже, нейтральные
[16:51.760 --> 16:57.120]  обозначим N с индексом T, так, T это время, то есть есть
[16:57.120 --> 17:00.080]  начальный момент времени потом, следующий шаг, ну
[17:00.080 --> 17:02.920]  вы видели эти шаги, T это время, сейчас я напишу и
[17:02.920 --> 17:09.720]  начало и рекурсию, N, T это число нейтральных, нейтральных
[17:09.720 --> 17:16.320]  вершин, ну и давайте действительно Z, T, это не число мертвых,
[17:16.320 --> 17:28.280]  это число потомков, ну то есть соседей, конечно, выбранной
[17:28.280 --> 17:31.320]  живой вершины, то есть процесс прямо в точности пойдет так,
[17:31.320 --> 17:36.200]  как я его описал, Z, T это число потомков выбранной
[17:36.200 --> 17:47.800]  на шаге с номером T выбранной живой вершины, ну на самом
[17:47.800 --> 17:50.280]  деле проще всего сказать, ну давайте я все-таки начну
[17:50.280 --> 17:54.440]  с того, что Y0 конечно равно единице, потому что мы стартовали
[17:54.440 --> 17:58.240]  с вершины V, вот она живая в начальный момент времени,
[17:58.240 --> 18:03.320]  ну и еще просто сказать, просто сказать, что YT, количество
[18:03.320 --> 18:08.600]  живых вершин, это сколько их было на предыдущем шаге,
[18:08.600 --> 18:14.640]  плюс сколько породила выбранная нами живая вершина, и минус
[18:14.640 --> 18:21.240]  это самая вершина, которая перестала быть живой, так
[18:21.240 --> 18:27.700]  и еще можно вот так сказать, Z с индексом T имеет биномиальное
[18:27.700 --> 18:38.340]  распределение с параметрами N с индексом T минус 1 и P,
[18:38.340 --> 18:40.900]  ну NT минус 1 это случайная величина, то есть такое
[18:40.900 --> 18:44.680]  условное конечно распределение, но если зафиксировано
[18:44.680 --> 18:47.900]  количество нейтральных вершин на предыдущем шаге,
[18:47.900 --> 18:51.580]  то конечно число потомков любой живой вершины имеет
[18:51.580 --> 18:56.900]  биномиальное распределение вот с такими параметрами,
[18:56.900 --> 18:59.820]  обозначение понятно или не очень, ну то есть это
[18:59.820 --> 19:04.020]  случайная величина, которая может принимать любое значение
[19:04.020 --> 19:09.420]  от нуля до вот этого числа, а P, ну это вероятность успеха
[19:09.420 --> 19:15.700]  как всегда, ну то есть вероятность того, что ZT равняется какому-то
[19:15.700 --> 19:20.060]  K, давайте я напишу аккуратно, что ZT равняется какому-то
[19:20.060 --> 19:28.180]  K, это C из NT по K на P вкатый на Q в степени NT, ой NT минус
[19:28.180 --> 19:35.940]  1, NT минус 1 минус K, немножко не лезет, понятно, но обычное
[19:35.940 --> 19:41.180]  биномиальное распределение, как у нас жизнь устроена,
[19:41.180 --> 19:45.940]  вот мы зафиксировали какую-то живую вершину, есть какое-то
[19:45.940 --> 19:51.820]  количество нейтральных вершин, и на случайном
[19:51.820 --> 20:00.380]  графе число потомков, это надо каждое потенциальное
[20:00.380 --> 20:03.260]  ребро, которое могло бы из этой вершины ввести в множество
[20:03.260 --> 20:06.140]  нейтральных, протестировать на то, ведет она туда или
[20:06.140 --> 20:09.500]  не ведет, ведет с вероятностью P, не ведет с вероятностью
[20:09.500 --> 20:13.780]  Q, 1 минус P, поэтому получается биномиальное распределение
[20:13.780 --> 20:18.100]  с такими параметрами, сейчас все понятно?
[20:18.100 --> 20:19.100]  Так.
[20:19.100 --> 20:28.420]  Слушайте, а вы меня, между прочим, не спросили, что
[20:28.420 --> 20:31.500]  будет при C равном единице, и в прошлый раз из вас это
[20:31.500 --> 20:32.740]  пришлось клещами тянуть.
[20:32.740 --> 20:40.340]  Нет, но на самом деле при C равном единице там люди
[20:40.900 --> 20:44.500]  очень много чего, но в данном случае это настолько действительно
[20:44.500 --> 20:47.700]  громоздко и непонятно, что я решил это вообще не комментировать.
[20:47.700 --> 20:51.660]  В этой теории я давал комментарии в прошлый раз, там при C равном
[20:51.660 --> 20:52.660]  единице все понятно.
[21:10.340 --> 21:28.180]  Так, ну ладно, давайте докажем вот такую лему, она простая,
[21:28.180 --> 21:30.340]  как Y с индексом A.
[21:30.340 --> 21:33.060]  Сейчас, подождите, прежде чем лемма, забыл сказать
[21:33.060 --> 21:35.100]  кое-что.
[21:35.100 --> 21:39.060]  Давайте считать, что вот этот процесс Y, T, количество
[21:39.060 --> 21:43.500]  живых вершин, он в какой-то момент ведь принимает значение
[21:43.500 --> 21:44.500]  ноль.
[21:44.500 --> 21:45.500]  Впервые.
[21:45.500 --> 21:52.820]  Что в этом случае происходит, как вы считаете?
[21:52.820 --> 21:56.620]  Все останавливается, мы выделили компоненту связности,
[21:56.620 --> 21:57.620]  правильно?
[21:57.620 --> 22:01.700]  Содержащую исходную вершину V, правильно?
[22:01.700 --> 22:04.460]  Но давайте будем считать, что этот процесс формально
[22:04.460 --> 22:07.300]  продолжается дальше, но просто для удобства будем
[22:07.300 --> 22:10.220]  определять и дальше Y, T просто через такие случайные
[22:10.220 --> 22:11.220]  величины.
[22:11.220 --> 22:12.220]  Это будет полезно технически.
[22:12.220 --> 22:15.540]  То есть, когда процесс вырождается, все, первый
[22:15.540 --> 22:17.940]  раз выразился, отлично, мы нашли компоненту.
[22:17.940 --> 22:20.460]  Но сами вот эти случайные величины мы определим и
[22:20.460 --> 22:21.460]  дальше тоже.
[22:21.460 --> 22:26.420]  Ну так же, да, вот этой рекурсией, да, то есть, там был ноль,
[22:26.420 --> 22:29.900]  к нему прибавляется какой-то случайное Z, T, там минус
[22:29.900 --> 22:30.900]  один.
[22:30.900 --> 22:31.900]  Какое Z, T?
[22:31.900 --> 22:34.900]  Ну вот такое.
[22:34.900 --> 22:36.180]  Не хотите?
[22:36.180 --> 22:39.900]  Ну я вот хочу так.
[22:39.900 --> 22:42.460]  Это перестает описывать, да, это перестает описывать,
[22:42.460 --> 22:44.580]  на всякий случай просто я ее доопределю, так это
[22:44.580 --> 22:47.020]  будет технически полезно.
[22:47.020 --> 22:48.020]  Ну хорошо.
[22:48.020 --> 22:51.820]  Я утверждаю, что Y, T имеет вот такое распределение,
[22:51.820 --> 23:02.220]  это бином параметрами n-1, 1, минус 1, минус p в степени
[23:02.220 --> 23:07.060]  t, дальше к этому прибавляется единица и вычитается t.
[23:07.060 --> 23:11.060]  Ну то есть, мы берем вот эту константу, в каком-то
[23:11.060 --> 23:13.980]  смысле константу, если t зафиксирована, то это константа.
[23:13.980 --> 23:18.460]  И к этой константе прибавляем вот такую биномиальную случайную
[23:18.460 --> 23:19.460]  величину.
[23:19.460 --> 23:24.460]  Я утверждаю, что это и есть случайная величина Y, T.
[23:24.460 --> 23:26.380]  Так, ну смотрите.
[23:26.380 --> 23:34.380]  Вообще, очевидно, что n, T, это я, кстати, не написал,
[23:34.380 --> 23:41.980]  но напишу здесь, это n минус t минус Y, T.
[23:41.980 --> 23:44.580]  Давайте подумаем, очевидно это или нет.
[23:44.580 --> 23:45.580]  Вдруг вообще неверно.
[23:45.580 --> 23:54.300]  Да, если t равно 0 нам, например, очень хорошо подставлять,
[23:54.300 --> 23:57.860]  здесь будет 0, а здесь будет 1, n минус 1, когда у нас дана
[23:57.860 --> 24:01.060]  только одна исходная вершина нейтральных, n минус 1 штука.
[24:01.060 --> 24:04.860]  Правильно?
[24:04.860 --> 24:12.020]  Ну вообще говоря, достаточно тогда, так, с этим согласились,
[24:12.020 --> 24:18.620]  тогда доказать вот это утверждение, это то же самое,
[24:18.860 --> 24:27.260]  что доказать вот такое утверждение, что вот это есть n минус 1,
[24:27.260 --> 24:35.460]  1 минус p в степени t, просто вот так, что число нейтральных
[24:35.460 --> 24:39.940]  вершин, это просто бином от n минус 1, 1 минус p в степени
[24:39.940 --> 24:40.940]  t.
[24:40.940 --> 24:46.660]  Сейчас, почему это так, давайте, я еще сам не понял.
[24:47.580 --> 24:51.060]  Я знаю, что так должно быть, я просто это понимаю хорошо.
[24:51.060 --> 24:54.180]  Теперь давайте осознаем, почему это так.
[24:54.180 --> 24:56.940]  Ну надо аккуратно осознать, это просто на самом деле,
[24:56.940 --> 24:57.940]  но надо осознать.
[24:57.940 --> 25:01.540]  Значит, смотрите, допустим, мы доказали, что n с индексом
[25:01.540 --> 25:09.860]  t, это вот такой бином, тогда, тогда, Y с индексом t, это же
[25:09.860 --> 25:15.140]  n минус t минус наоборот nt, ну так перекувырнем, почему
[25:15.140 --> 25:16.140]  бы нет.
[25:16.220 --> 25:21.820]  Мы доказали, что nt, это вот такой бином, но давайте
[25:21.820 --> 25:30.540]  напишем так, это n минус 1 минус nt, плюс 1 минус t.
[25:30.540 --> 25:33.180]  Согласны, что пока я сделал совершенно стандартное,
[25:33.180 --> 25:34.180]  понятное преобразование.
[25:34.180 --> 25:41.100]  1, тут минус 1, все сходится, то есть 1 минус t уже вот оно,
[25:41.100 --> 25:42.100]  очень удачно.
[25:42.100 --> 25:45.420]  Ну а тут-то тоже все понятно, смотрите, вы из количества
[25:45.420 --> 25:50.380]  испытаний вычитаете количество успехов вот в такой схеме
[25:50.380 --> 25:51.380]  Бернули.
[25:51.380 --> 25:56.140]  Потому что я понятно выражаю, схема Бернули, испытания,
[25:56.140 --> 25:58.300]  были эти слова все?
[25:58.300 --> 25:59.300]  Не нужно пояснять.
[25:59.300 --> 26:03.500]  Если мы верим, что нейтральные вершины имеют вот такое
[26:03.500 --> 26:06.900]  распределение, это значит, что фактически число нейтральных
[26:06.900 --> 26:10.620]  вершин, это число успехов в схеме из n минус 1 испытания
[26:10.620 --> 26:13.060]  Бернули вот с такой вероятностью успеха.
[26:13.140 --> 26:15.620]  Если из числа испытаний вычесть это количество,
[26:15.620 --> 26:20.060]  но это будет число неудач, но вероятность неудачи,
[26:20.060 --> 26:23.460]  соответственно, это 1 минус вот эта величина.
[26:23.460 --> 26:26.860]  Ну и в другую сторону, естественно, все так же получается.
[26:26.860 --> 26:31.380]  Я вывел из того, что nt такая вот биномиальная, то что
[26:31.380 --> 26:35.300]  yt такая как нам нужна, но обратная, очевидно, верно,
[26:35.300 --> 26:38.820]  потому что все действия, которые мы совершали, они
[26:38.820 --> 26:39.820]  обратимые.
[26:43.060 --> 26:47.380]  Мертвые не входят, и что?
[26:47.380 --> 26:52.220]  Вот они вычитаются, вот это t это как раз вычитаются
[26:52.220 --> 26:53.220]  мертвые вершины.
[26:53.220 --> 26:56.220]  Есть еще вопросы?
[26:56.220 --> 26:57.780]  Вроде все хорошо.
[26:57.780 --> 27:01.940]  Вопрос правильный, да, вот это вычитание t это как
[27:01.940 --> 27:03.340]  раз вычитание мертвых вершин.
[27:03.340 --> 27:07.940]  Так, вроде получилось.
[27:07.940 --> 27:11.500]  Ну давайте докажем этот факт, подчеркнутый факт,
[27:11.500 --> 27:12.820]  скажем просто по индукции.
[27:12.820 --> 27:29.300]  n0 это мы знаем, n-1, и о чудо, это правда бином от n-1 и 1-p в
[27:29.300 --> 27:30.300]  нулевой степени.
[27:30.300 --> 27:36.340]  Ну что такое 1-p в нулевой степени, это единица.
[27:36.340 --> 27:39.980]  Бином от n-1 с вероятностью единицы это просто n-1.
[27:40.460 --> 27:43.420]  Если мы каждый объект выбираем с вероятностью 1, всякий раз
[27:43.420 --> 27:48.780]  успех случается с вероятностью 1, ну успехов и будет с определенностью
[27:48.780 --> 27:55.220]  просто n-1 штука, то есть это база индукции t равно 0.
[27:55.220 --> 27:58.540]  Ну там t равно единица также точно просто устроена.
[27:58.540 --> 28:01.020]  Ну а дальше давайте сделаем шаг.
[28:01.020 --> 28:09.100]  Вот у нас nt, это еще раз перепишем, n-t-yt, а дальше к y применим
[28:09.220 --> 28:12.700]  ту рекурсию, которая у нас вот здесь записана, вот эту
[28:12.700 --> 28:13.700]  рекурсию.
[28:13.700 --> 28:28.380]  Значит у нас, я поднимусь наверх, так у нас получится
[28:28.380 --> 28:49.260]  n-t-yt-1, да, минус zt, да, минус zt и плюс 1, так да, и плюс 1.
[28:49.260 --> 29:05.660]  Давайте так это перепишем, n-t-1-y с индексом t-1 и минус
[29:05.660 --> 29:10.140]  zt, по-моему все правильно, да, минус t, плюс 1, вот так я загнал,
[29:10.140 --> 29:14.460]  плюс 1, да, минус t-1, но это что такое?
[29:14.460 --> 29:23.020]  Это nt-1, к которому можно применять предположение
[29:23.020 --> 29:28.860]  индукции, но давайте не сразу, это nt-1, а вот это, смотрим
[29:28.860 --> 29:35.180]  еще раз сюда, это бином от nt-1 и p, то есть вот тут
[29:35.180 --> 29:43.900]  вот бином от того же nt-1 и p, то есть мы из количества
[29:43.900 --> 29:50.860]  испытаний nt-1 вычитаем число успехов на этом количестве
[29:50.860 --> 30:07.580]  испытаний, но это есть естественно бином от nt-1 и 1-p, правильно?
[30:07.580 --> 30:11.580]  Мы из количества всех испытаний выкидываем количество
[30:12.140 --> 30:14.860]  успехов в этих испытаниях, у нас получается количество
[30:14.860 --> 30:20.140]  неудач, правильно?
[30:20.140 --> 30:24.140]  Из всех испытаний выкидываем количество успехов, получается
[30:24.140 --> 30:32.140]  количество неудач, но вероятность неудач это 1-p, ну да, лучше
[30:32.140 --> 30:36.860]  писать, конечно, тильно, но знаете, вообще совсем
[30:36.860 --> 30:40.540]  хорошо писать не так, а, наверное, хорошо писать
[30:40.540 --> 30:44.380]  равняется дальше какую-нибудь букву, там, это или что-нибудь
[30:44.380 --> 30:48.780]  такое, где это и дальше тильда вот это, ну, наверное,
[30:48.780 --> 30:52.780]  так будет более корректно, но это чересчур, тильда,
[30:52.780 --> 30:55.740]  тильда, хорошо, тильда, но смотрите, что такое бином
[30:55.740 --> 31:01.260]  от nt-1 и 1-p, если мы знаем, что само nt-1 по предположению
[31:01.260 --> 31:11.340]  индукции это бином от n-1 и 1-p в t-1 степени, то есть
[31:11.340 --> 31:17.980]  мы берем n-1 испытание и ищем в нем успехи вот с такой
[31:17.980 --> 31:23.500]  вероятностью, а потом на множестве этих успехов
[31:23.500 --> 31:28.220]  снова вот с такой же вероятностью, с такой же, с такой вероятностью
[31:28.220 --> 31:32.940]  производим снова поиск успехов, для этого они были
[31:32.940 --> 31:35.340]  неудачами, теперь они стали успехами с вероятностью
[31:35.340 --> 31:42.300]  1-p, будто или понятно, вот здесь мы говорим, есть nt-1
[31:42.300 --> 31:47.980]  испытание и вероятность, ну пусть успеха, 1-p, мы сжимаем
[31:47.980 --> 31:52.060]  в 1-p раз как-то, ну в каком-то смысле, среднее значение
[31:52.060 --> 31:56.620]  брать, а мы знаем уже, что найти само nt-1, это значит
[31:56.700 --> 31:59.660]  произвести испытание Бернули, просто на фиксированном
[31:59.660 --> 32:03.820]  количестве объектов, вот с такой вероятностью успеха,
[32:03.820 --> 32:06.700]  но эти действия независимы от этих, значит вероятности
[32:06.700 --> 32:17.420]  надо перемножить, сейчас понятно сказал, нет, но вот
[32:17.420 --> 32:24.620]  у нас есть сколько-то испытаний, само их количество, это число
[32:24.620 --> 32:29.100]  успехов вот в такой схеме, теперь мы к этому количеству
[32:29.100 --> 32:34.540]  снова применяем схему, но действия в рамках этой
[32:34.540 --> 32:38.700]  схемы от действий, которые вот здесь осуществлялись
[32:38.700 --> 32:44.300]  не зависят, поэтому вероятности успеха просто перемножаются,
[32:44.300 --> 32:49.100]  выбрать отсюда сколько-то успехов с такой вероятностью,
[32:49.100 --> 32:52.140]  это все равно, что выбрать отсюда сколько-то успехов
[32:52.220 --> 32:59.340]  с такой вероятностью, умноженной на вот эту, понятно, ну все,
[32:59.340 --> 33:10.060]  мы получили то, что хотели, то есть вот это N-1 и 1-p в тепени
[33:10.060 --> 33:18.780]  t, индукция завершена, индукция получилась, сейчас звонок
[33:18.780 --> 33:23.500]  будет, дайте нам тогда устроим перерыв, заметьте, что здесь
[33:23.500 --> 33:27.020]  то уже никаких случайных величин нет, мы аккуратненько
[33:27.020 --> 33:30.780]  по индукции убедились в том, что нейтральные вершины
[33:30.780 --> 33:36.060]  имеют вполне себе куда оно делось, понятное распределение,
[33:36.060 --> 33:40.540]  вот оно, тут все зафиксировано, при фиксированном N, фиксированном
[33:40.540 --> 33:44.860]  T, все зафиксировано, то есть пользуясь вот этими индуктивными
[33:44.940 --> 33:49.420]  действиями, мы в конце концов получили неусловное распределение,
[33:49.420 --> 33:51.980]  распределение при условии того, какое значение примет
[33:51.980 --> 33:56.220]  предыдущая случайная величина, а безусловное, именно, что
[33:56.220 --> 33:59.980]  как бы мы не зафиксировали эту величину, в итоге получится
[33:59.980 --> 34:02.620]  вот такое вот распределение, для этого и была применена
[34:02.620 --> 34:07.100]  индукция, ну да, продумаете это еще, потом можем еще
[34:07.100 --> 34:10.060]  подискутировать, я думаю все будет понятно, ничего
[34:10.060 --> 34:13.020]  такого страшного нет, в итоге у нас получились вполне
[34:13.020 --> 34:17.340]  недетерминированные распределения, то есть yt не опирается больше
[34:17.340 --> 34:20.060]  ни на каких предшественниц, оно имеет вот именно такое
[34:20.060 --> 34:24.860]  распределение с понятными параметами, и вот если мы
[34:24.860 --> 34:27.980]  сейчас это до какой-то степени поняли, дальше поймем, лучше
[34:27.980 --> 34:32.540]  когда будем глубже разбираться, то этим можно очень хорошо
[34:32.540 --> 34:33.980]  и красиво воспользоваться.
[34:43.020 --> 34:58.780]  Так, сейчас воспользуемся, вот у нас какая-то вершина
[34:58.780 --> 35:02.780]  v, я еще раз напоминаю, она уже была где-то здесь написана,
[35:02.780 --> 35:07.460]  вот она, ну я еще раз напоминаю, что мы стартовали с какой-то
[35:07.460 --> 35:14.580]  вершины нашего графа, вероятность того, что мощность компонента
[35:14.580 --> 35:21.940]  содержащей эту вершину больше какого-то t, ну то есть
[35:21.940 --> 35:26.780]  что процесс остановится впервые позже, чем в момент
[35:26.780 --> 35:32.740]  времени t, t от v это компонента связанности, содержащая
[35:32.740 --> 35:37.260]  вершину v, вот вероятность того, что количество вершин
[35:37.260 --> 35:45.540]  в этой компоненте больше чем t, вероятность того,
[35:45.540 --> 35:49.540]  что мощность этой компоненты больше чем t, она не больше
[35:49.540 --> 35:58.260]  чем вероятность того, что yt больше нуля, вот в этом
[35:58.260 --> 36:05.780]  месте я пользуюсь тем, что процесс продлил формально,
[36:05.780 --> 36:12.700]  потому что понимаете, что значит yt больше нуля, я
[36:12.700 --> 36:15.740]  нигде здесь не пишу, что t это первый момент времени,
[36:15.740 --> 36:19.740]  в который yt больше нуля или последний момент времени,
[36:19.740 --> 36:22.920]  когда он еще больше нуля, yt может оказаться больше
[36:22.920 --> 36:26.900]  нуля в более широком множестве случаев, когда мы продлеваем
[36:26.900 --> 36:30.420]  этот процесс, он тоже может потом случайно оказаться
[36:30.420 --> 36:37.020]  больше нуля, поэтому я пишу меньше либо равно, я этим
[36:37.020 --> 36:41.020]  пользуюсь явно, так, теперь что значит, что yt, смотрите
[36:41.020 --> 36:50.780]  на лему, больше нуля, это с какой величине у нас одно
[36:50.780 --> 36:53.140]  вероятностное пространство, это множество графов на
[36:53.140 --> 36:59.420]  n вершинах, а я понял, да, пусть вот дано какое-то
[36:59.420 --> 37:01.980]  t, мы просто оценим сейчас вероятность, а потом подберем
[37:01.980 --> 37:05.980]  это t так, как нам нужно, да-да-да, сейчас вот t это
[37:05.980 --> 37:10.060]  просто параметр оценки, вот ввели какое-то t и посмотрим,
[37:10.060 --> 37:13.420]  с какой вероятностью количество вершин в компоненте связанности
[37:13.420 --> 37:17.300]  содержащей v больше, чем вот это t, ну какое это t,
[37:17.300 --> 37:20.620]  любое t, с какой вероятностью размер компонента больше,
[37:20.620 --> 37:24.180]  чем это t, с вероятностью точно не больше, чем вероятность,
[37:25.180 --> 37:40.100]  нуля, а это есть вероятность того, что binom от n-1 1-1-p в степени
[37:40.100 --> 37:46.860]  t больше, чем t-1, я вот это 1-t, которое прибавляется
[37:46.860 --> 37:51.020]  там, перекинул вправо, так, ну сейчас, наверное, все понятно,
[37:51.020 --> 38:06.860]  можно я вот так, больше либо равно t, тоже самое, забыл скобочку, да, вот эту скобочку забыл,
[38:06.860 --> 38:12.700]  так, ну быть больше строго, чем t-1 для целого числа, это то же самое,
[38:12.700 --> 38:18.500]  чтобы быть не меньше, чем t, у нас тут целое число с t, целое число больше,
[38:18.500 --> 38:25.420]  чем t-1 тогда и только тогда, когда оно больше либо равняется t, теперь, ну давайте так,
[38:25.420 --> 38:37.700]  это меньше либо равно, тут я перепишу, вероятность того, что binom от n и 1-1-1-p в степени t больше
[38:37.700 --> 38:53.540]  либо равняется t, хр, ну почему хр, потому что одно дело посчитать успехи на n-1 испытании и
[38:53.540 --> 38:58.220]  сравнить их вот так именно с t, а другое дело на n, но, наверное, вероятность того,
[38:58.220 --> 39:03.860]  что за n бросаний станет больше либо равно t, опять скобку не закрыл, станет больше либо равно t,
[39:04.080 --> 39:08.820]  это вероятность больше, вообще говоря, чем вероятность того же самого за n-1 бросаний,
[39:08.820 --> 39:16.940]  монетки. Так, и то же самое касается вероятности, если мы вот эту вероятность оцениваем сверху,
[39:16.940 --> 39:25.580]  то и всю большую вероятность мы тоже оценим сверху, чем больше вероятность успехов в каждом
[39:25.580 --> 39:31.940]  испытании, тем больше, конечно, вероятность того, что успехов будет много, значит мы это оценим как,
[39:31.940 --> 39:37.060]  но я думаю все присутствующие знают, как можно оценить 1-p в степени t снизу.
[39:37.060 --> 39:49.940]  1-pt, да? Ну то есть, в общем, все это не больше, вот давайте так, это не больше, чем pt. Это не
[39:49.940 --> 40:03.460]  больше, чем pt, поэтому все вместе не больше, чем вероятность, с которой binom от npt больше
[40:03.460 --> 40:13.220]  не выровняется t. Так, дорогие друзья, сейчас я в очередной раз у вас спрошу, вы теорему
[40:13.220 --> 40:22.540]  муавролаплас или центральную предельную теорему знаете? Центральную предельную знаете? Я знаю,
[40:22.540 --> 40:32.920]  я могу напомнить. Ну, потому что ей хотелось бы воспользоваться, в принципе, можно здесь
[40:32.920 --> 40:38.260]  сделать такое же исхищрение, как я делал, помните, когда оценивал уклонение пьяницы от кабака.
[40:38.260 --> 40:45.780]  Можно не применять cpt, но я не могу непосредственно сослаться на то ухищрение, то есть нужно тоже
[40:45.780 --> 40:52.780]  передоказывать. Давайте я вам для разнообразия расскажу вот это не через уклонение, которое там
[40:52.780 --> 41:00.500]  с чосинусами какими-то хитрыми, а через cpt. Заодно лучше заиграет теорема, которую часть из вас знает,
[41:00.500 --> 41:06.100]  часть еще нет, но я ее напомню, она простая. Классический факт, без него вы никогда не
[41:06.100 --> 41:17.100]  останетесь. Нет, она простая в том смысле, что без нее вообще никак, то есть это совершенно
[41:17.100 --> 41:24.860]  классический факт, который играет огромную роль в приложениях и теории. Доказательства ее довольно
[41:24.860 --> 41:31.660]  сложные, но здесь-то она будет применяться к биномиальным величинам. Могу, в общем, для биномиальной
[41:31.660 --> 41:38.380]  ее написать. Там это более просто доказывается, но тоже требует выкладок в духе всяких формул
[41:38.380 --> 41:46.820]  стирлинга и так далее. Обычно в наших курсах, насколько я понимаю, отдельно для биномиальных
[41:46.820 --> 41:51.900]  не доказывают, выводят прям как следствие с cpt. Ну давайте я напомню, что такая центральная
[41:51.900 --> 41:59.700]  предельная теорема в самом простом своем обличии. У нас есть последовательность случайных величин,
[41:59.700 --> 42:10.900]  бесконечная, которые независимая и одинаково распределены. Независимая и одинаково распределена.
[42:10.900 --> 42:20.260]  Ну еще, конечно, надо сказать, что мат ожидания каждой из них это какой-нибудь а, дисперсия
[42:20.260 --> 42:26.300]  каждой из них это какой-нибудь сигма квадрат. Конечно, большая нуля, то есть это невырожденные
[42:26.300 --> 42:33.740]  случайные величины с конечной дисперсией. Вот она обозначена сигма квадрат. Значит,
[42:33.740 --> 42:44.620]  тогда утверждается, что вероятность, с которой кси1 плюс и так далее плюс кси n минус на поделить
[42:44.620 --> 42:54.700]  на корень из n сигма квадрат. Можно вот так написать просто. Ну, например, меньше либо
[42:54.700 --> 43:04.220]  равняется x при n стремящемся к бесконечности сходится к 1 поделить на корень из 2p. Так,
[43:04.220 --> 43:14.180]  от минус бесконечности до x не в степени минус t квадрат полам dp. Это называется плотность
[43:14.180 --> 43:27.740]  стандартного нормального распределения. Сейчас, я кокнул? Нормально, точно? Нормальное распределение
[43:27.740 --> 43:39.220]  должно быть нормально. Так, ну на самом деле, то мы делаем, вот здесь у нас ведь тоже сумма
[43:39.220 --> 43:49.460]  независимых случайных величин. Что такое бинон? Какими-то там параметрами скажем n и pt. Что это
[43:49.460 --> 43:57.420]  за случайная величина? Это сумма кси1 плюс и так далее плюс кси n, где вот эти случайные величины
[43:57.420 --> 44:05.380]  независимые и очень просто одинаково распределены. А именно, они принимают значение 0 с вероятностью pt,
[44:05.380 --> 44:15.580]  1 с вероятностью pt и 0 с вероятностью 1 минус pt. Успех неудача. Так, это, я надеюсь, понятно,
[44:15.580 --> 44:21.780]  вроде мы это сегодня однократно говорили. Но успехов в схеме из-за испытаний вернули с вероятностью
[44:21.780 --> 44:29.860]  успеха pt. То есть, мы находимся вот в этой ситуации, в ситуации центральной предельной теории. Так,
[44:29.860 --> 44:39.540]  мат ожидания какое? pt, да, вот здесь pt. Мат ожидания, это просто pt такой случайной величины,
[44:39.540 --> 44:45.980]  это очевидно. Ну, а здесь будет npt соответственно вычитаться. То есть, чтобы нам привести наше
[44:45.980 --> 44:55.860]  выражение к виду, который вот здесь фигурирует, надо слева и справа вычесть npt. Ну, надо вычесть
[44:55.860 --> 45:02.180]  n раз математическое ожидание каждого из слагаемых, чтобы хотя бы числитель был похож на то,
[45:02.180 --> 45:06.540]  что фигурирует в центральной предельной теории. То есть, я пишу вот так. Это вероятность,
[45:06.540 --> 45:24.180]  с которой binom от n и pt минус npt больше либо равняется t минус npt. Просто сделала тождественное
[45:24.180 --> 45:30.180]  преобразование, слева и справа вычил npt, чтобы подогнать левую часть к тому виду,
[45:30.180 --> 45:35.660]  который фигурирует в cpt, в центральной предельной теории. Но надо еще и поделить
[45:35.660 --> 45:46.060]  на корень из n sigma квадрат. Так, ну сейчас поделим. Так, какая у нас дисперсия у каждой из ксиитов?
[45:46.060 --> 46:00.540]  Кокнул, нет? Да, понятно. Но вот какая дисперсия у ксиитов? Как посчитать дисперсию? Дисперсия
[46:00.540 --> 46:10.780]  это вот так минус вот так. Вот это дисперсия, правильно? Одна из формул, которая кажется здесь
[46:10.780 --> 46:17.180]  удобнее. Если ксиитов возводить в квадрат, что-то поменяется в сравнении с ксиитом? Нет. Поэтому
[46:17.180 --> 46:27.180]  это pt минус pt в квадрате. Это pt минус pt в квадрате. Ну или как более общепринято писать pt на 1
[46:27.180 --> 46:33.700]  минус pt? Ну более общепринято писать p на q. Но у нас сейчас роль p играет pt, а q, соответственно,
[46:33.700 --> 46:43.860]  это 1 минус pt. Я внятно выражаюсь? Вот. Ну то есть надо как написать? Господи, как бы мне от этого
[46:43.860 --> 46:48.900]  бинового проклятого избавиться? Может какую-нибудь букву его обозначить? Это, например. Уже надоело
[46:48.900 --> 46:59.620]  его переписывать каждый раз. Это минус npt поделить на корень. Так, на корень из какой же бяки
[46:59.620 --> 47:09.420]  мы делим? Вот из этой. Но только на n еще надо умножить и будет нам полный катапсис. 1 минус pt.
[47:09.420 --> 47:21.340]  Так, больше либо равняется ну совсем поганой бяки t минус np. Да никакой поганой бяки. Вообще смысл
[47:21.340 --> 47:26.820]  то теоремы центральной предельной чувствуете, понимаете? Что очень простой и естественный. Мы
[47:26.820 --> 47:32.820]  взяли случайную величину, складывающуюся как сумму независимых одинаковых факторов,
[47:32.820 --> 47:40.060]  отцентрировали ее, как говорят, то есть догнали ее среднее в ноль, и отнормировали так, чтобы
[47:40.060 --> 47:46.420]  дисперсия получилась единица. То есть у нас стоит здесь случайная величина со среднем 0 и дисперсией 1.
[47:46.420 --> 47:52.940]  И теорема утверждает просто, что распределение этой случайной величины, функция распределения
[47:52.940 --> 47:58.620]  этой случайной величины очень похожа на функцию распределения стандартного нормального, тоже со
[47:58.620 --> 48:06.660]  среднем 0 и дисперсией 1. В этом же смысл. Ну здесь вот они так выразились чуть-чуть громоздко, но ничего
[48:06.660 --> 48:17.580]  в этом страшного нет. Здесь тоже npt под корнем 1 минус pt. Так, и большая скобка закрывается. Теперь
[48:17.580 --> 48:26.860]  настает некоторый момент истины, потому что как сходится? Я думаю, что в том варианте центральной
[48:26.860 --> 48:33.780]  предельной теоремы, которые вам доказывают в стандартном курсе, именно в общем виде сходится как-то
[48:33.780 --> 48:43.260]  и сходится. Но если здесь стоит тема испытаний Бернули, то есть биномиальная случайная величина,
[48:43.260 --> 48:50.980]  то сходимость равномерная. И на самом деле сходимость равномерная и в общей CPT тоже, в некоторых
[48:50.980 --> 48:56.340]  дополнительных условиях. Может быть вам в курсе это и говорят, я не знаю. Но в общем, вот эта
[48:56.340 --> 49:03.020]  сходимость в нашем случае равномерная. Это очень важно. Почему это важно? Потому что у нас справа
[49:03.020 --> 49:13.900]  стоит не какой-то фиксированный x, как здесь, а x, который зависит от n. И я вообще не могу,
[49:13.900 --> 49:19.980]  строго говоря, писать, что это стремится и дальше сюда ставить функцию, зависящую тоже от n. Я
[49:19.980 --> 49:28.980]  могу писать вот так, имея в виду, что тот интеграл, который появится справа, будучи поделенным на вот
[49:28.980 --> 49:35.460]  это выражение, стремится к единице. И на самом деле даже будучи вычтенным из этого, тоже стремится
[49:35.460 --> 49:41.620]  к единице, потому что имеет место равномерная сходимость. Вот эта сходимость в нашем случае
[49:41.620 --> 49:50.620]  равномерная. Сейчас понятно? Я пишу тильда и рисую вот такой интеграл. Ну, я надеюсь, что вы
[49:50.620 --> 49:56.260]  понимаете, здесь у нас было неравенство такое, поэтому мы интегрировали dx. Здесь неравенство в
[49:56.260 --> 50:03.180]  другую сторону, поэтому интегрировать мы, естественно, будем от правого вот этого предела. Здесь будет
[50:03.180 --> 50:16.100]  1 на корене с 2p интеграл. Вопрос какой-то. t-npt. Громоздко выглядит, смысл очень простой. Вот я
[50:16.100 --> 50:24.700]  переживаю, вроде много слов произношу, а на самом деле все, что происходит, очень просто. npt 1-pt,
[50:24.700 --> 50:30.700]  здесь плюс бесконечность наоборот, ну и е в степени минус наоборот. x давайте писать,
[50:30.700 --> 50:39.780]  квадрат dx, чтобы не путать с нижним пределом, который сейчас от буквы t зависит. Ладно, друзья,
[50:39.780 --> 50:45.820]  вот если вы хотите, ну не катарсис, так хотя бы просто понимание того, что произошло. В чем суть,
[50:45.820 --> 50:51.260]  а может и катарсис? Вот у вас сейчас прояснится, почему нам важно, что c меньше единицы, может уже
[50:51.260 --> 50:58.380]  у кого-то прояснилось. У нас же c меньше единицы. Я утверждаю, что это принципиально важно для того,
[50:58.380 --> 51:06.260]  чтобы дальше что-то получилось. Давайте я напишу вот это t-npt поделить на корень,
[51:06.260 --> 51:18.100]  вот там npt на 1-pt. Господи, помилуй, кажется какая-то бяка. Но у нас p это c поделить на n. Вот это вы
[51:18.100 --> 51:37.180]  помните еще, что p это у нас c поделить на n, поэтому npt это ct х, то есть вот здесь написано t-ct и
[51:37.180 --> 51:49.420]  делится это на корень из ct, ну умножить на какую-то фигню. Умножить на 1-ct деленное на n. Вот так,
[51:49.420 --> 51:54.220]  то есть вот эта штука, стремящаяся к единице под корнем, она ни на что не влияет, это фигня.
[51:54.220 --> 52:02.780]  Ну в принципе я, наверное, даже могу как-нибудь оценить. Сейчас давайте подумаем, это меньше
[52:02.780 --> 52:14.180]  единицы, да? Больше единицы? Плохо, да? Это больше, чем если... А, так нет, это же хорошо.
[52:14.180 --> 52:22.300]  Так, друзья, я кокнул вас вот этим своим размышлением или ничего? Я очень хочу от этой
[52:22.300 --> 52:27.820]  бяки избавиться, чтобы вам приятнее на все это было глядеть, но могу не избавляться. Я хочу
[52:27.820 --> 52:33.260]  что сказать? Я хочу сказать, что мы интегрируем вот от этой величины, а если бы мы интегрировали
[52:33.260 --> 52:43.020]  не от такой, а вот от такой, то что ближе к плюс бесконечности исходное или вот такая?
[52:43.020 --> 52:49.820]  Исходное ближе к плюс бесконечности. Ну значит соответствующий интеграл меньше,
[52:49.820 --> 52:57.220]  чем интеграл, в который подставлена вот такая штучка, правда? Ну приятнее глядеть. Давайте
[52:57.220 --> 53:07.300]  я так и напишу. Это не превосходит интеграла от t-ct на корень просто из ct до плюс бесконечности
[53:07.300 --> 53:17.940]  1 на корень из 2p, е в степени минус х квадрат пополам dx. Ну вот тут принципиально важно,
[53:17.940 --> 53:24.980]  я уже пытался это начать говорить, что c меньше 1. Именно потому что c меньше 1,
[53:24.980 --> 53:34.500]  t-ct, куда стремиться при t растущем? Чем больше t, тем куда это ближе к плюс бесконечности. То
[53:34.500 --> 53:44.540]  есть это функция, стремящаяся к нулю. Успели, да, уследили. Какой скорости? Вот это вы понимаете
[53:44.540 --> 53:51.900]  или нет? Какой примерно скоростью, я не прошу прям вот точно оценить, но какой примерно скоростью,
[53:51.900 --> 53:58.740]  вот эта величина, в которой сейчас идет интегрирование, это величина, которая имеет вид,
[53:58.740 --> 54:05.060]  ну давайте я как-нибудь константу обозначу. Тут 1 минус c поделить на корень из c. Вот давайте
[54:05.060 --> 54:13.220]  это 1 минус c, t за скобку, поделить на корень из c, обозначим как-нибудь. Ну альфа, например,
[54:13.220 --> 54:23.100]  хорошо? Так, это будет альфа корень из t. Альфа положительная, потому что c меньше 1. Значит,
[54:23.100 --> 54:29.300]  тут стоит альфа корень из t и идет интегрирование до плюс бесконечности очень-очень быстро убывающей
[54:29.300 --> 54:37.460]  функции. Я утверждаю, что это стремится к нулю, но это я не знаю, просто он совсем такой аналитический,
[54:37.460 --> 54:44.860]  простой вроде факт, примерно как e в степени минус, ну какую-нибудь еще букву надо написать,
[54:44.860 --> 54:53.420]  альфа штрих умножить на t. Весь этот интеграл не превосходит e в степени минус альфа штрих на t,
[54:53.420 --> 55:01.700]  где альфа штрих тоже положительная. Но я просто фактически подставляю альфа корней из t вот сюда и
[55:01.700 --> 55:08.460]  получаю вот этот альфа штрих. То есть весь интеграл доминируется своим первым, ну как сказать,
[55:08.460 --> 55:14.620]  слагаемым. Интеграл это же сумма такая, мы берем самое большое слагаемое, а дальше они убывают,
[55:14.620 --> 55:21.460]  убывают с бешеной скоростью, и вся эта сумма доминируется своим первым слагаемым. Вот я все,
[55:21.460 --> 55:26.340]  что утверждаю. Давайте расценим это как простое упражнение, которое я не буду сейчас решать,
[55:26.340 --> 55:33.940]  что мне лень просто, а вам, может быть, будет полезно. Нормально так? Ну попробуйте,
[55:33.940 --> 55:38.380]  по крайней мере, если не будет получаться, будем обсуждать. По-моему, должно получиться. Я вроде
[55:38.380 --> 55:43.540]  объяснил интуицию, функция очень быстро убывает, интеграл это сумма вот этих площадей. Мы берем
[55:43.540 --> 55:49.540]  значение в самом первом кусочке, а дальше сумма она доминируется вот этим первым слагаемым кусочком.
[55:49.540 --> 55:57.140]  Фуф! Что у нас получилось? У нас получилась вероятность того, что компонента связности,
[55:57.140 --> 56:03.500]  содержащая данную вершину, имеет больше чем t вершин сама. Эта вероятность маленькая,
[56:03.500 --> 56:09.340]  она не превосходит e в степени минус альфа штрих на t, где альфа штрих как-то зависит от исходного c.
[56:09.340 --> 56:22.780]  Понятно? Ну хорошо, остался последний, извините за тавтологию, штрих, но не над альфой, а в доказательстве.
[56:22.780 --> 56:35.740]  Сейчас будет подлинный катарсис. Сейчас мы полетим на крыльях части.
[56:35.740 --> 56:44.860]  У вас, наверное, еще какая-то легкая, но у меня тоже. Так, смотрите, вероятность того,
[56:44.860 --> 56:57.260]  что существует v такая, что модуль c от v больше чем t, ну это очевидно не больше чем n на e в степени
[56:57.260 --> 57:02.780]  минус альфа штрих t, правильно? Потому что вершина n штук, а существование, это как всегда объединение
[57:02.780 --> 57:22.140]  событий. Ну так пусть или положим пусть t равняется, как нам, например, 2 поделить на альфа штрих
[57:22.140 --> 57:35.900]  логариф натуральный n. Ну, например, 2 поделить на альфа штрих логариф натуральный n. Тогда пишем меньше
[57:35.900 --> 57:45.620]  либо равно, или равно даже, равно, почему меньше? Но n умножить на e в степени минус альфа штрих
[57:45.620 --> 57:54.980]  сокращается, то остается 2 логарифмен. Ну это, извините, 1 поделить на n. Это стремится к нулю.
[57:54.980 --> 58:06.060]  Но значит с вероятностью стремящейся к единице для любого v размер компонента связности,
[58:06.060 --> 58:14.060]  которая содержит эту вершину, не превосходит 2 поделить на альфа штрих логарифмен. Обозначаем
[58:14.060 --> 58:20.180]  вот эту штуку буквой бета, которая фигурировала в формулировке теоремы, и получаем ее результат.
[58:20.180 --> 58:27.860]  Существует бета, зависящая только от c. Вот, например, такая 2 поделить на альфа штрих,
[58:27.860 --> 58:35.180]  что все компоненты связанности, с какой бы вершины мы ни стартовали, имеют количество вершин,
[58:35.180 --> 58:40.220]  не превосходящее бета лог n. Ну в данном случае логариф натуральный. Я вроде так и писал.
[58:40.220 --> 58:55.220]  Кажись, доказал. А, она стремится к единице, конечно. Я сказал, но не дописал. Это стремится к нулю,
[58:55.220 --> 59:01.700]  а это ее отрицание. Ну, стремится к единице. А симпатически почти, наверное, каждая компонента
[59:01.700 --> 59:09.620]  случайного графа имеет не больше, чем бета логарифмен вершин. Так, друзья, тут было две тонкости
[59:09.620 --> 59:16.220]  в этом доказательстве. Одна, это то, что вам немножко было тяжело интуитивно и, казалось,
[59:16.220 --> 59:22.540]  не совсем формально жонглировать случайными величинами, которые имеют биномиальное распределение,
[59:22.540 --> 59:28.260]  зависящее от предшественниц. И вот с этим, наверное, надо вам тоже подразобраться, подумать,
[59:28.260 --> 59:33.940]  насколько это действительно осознается, корректно и так далее. Вот, это первый момент. А второй
[59:33.940 --> 59:41.620]  момент, ну не вот этот второй момент, а второй момент доказательств. Я сюда просто показываю и
[59:41.620 --> 59:46.380]  вижу, что это второй момент. А второй момент, это то, что я немножко вас замучил, наверное, вот этими
[59:46.380 --> 59:50.540]  выглотками. Но если вы на них внимательно посмотрите, вы поймете, что это я вас мучил,
[59:50.540 --> 59:57.540]  потому что вам понятно было, вам непривычно, а так-то они же очень простые. Подогнал просто
[59:57.540 --> 01:00:04.780]  классическую теорию. В принципе, я мог сделать здесь рассуждение в духе вот того большого уклонения
[01:00:04.780 --> 01:00:11.340]  пьяницы от кабака тоже бы сработало, и тогда не нужно было бы ни асимптотик неравномерной исходимости,
[01:00:11.340 --> 01:00:17.420]  просто там тяп-ляп, чосинусы какие-то оценили. Ну, я почему-то решил продемонстрировать на таком
[01:00:17.420 --> 01:00:23.260]  варианте. Кажется, что это тоже неплохо. Два тонких момента, которые могли вас чуть смущать
[01:00:23.260 --> 01:00:30.300]  в доказательствах, но продумайте их до конца, и все будет хорошо. Так, у меня есть еще несколько минут.
[01:00:30.300 --> 01:00:37.900]  Я хочу, наверное, начать рассказывать про последнюю тему о случайном графе, которая есть в этом
[01:00:37.900 --> 01:00:44.540]  семестре. Но я, собственно, обещал, что я расскажу впервые за всю историю вот это и расскажу то,
[01:00:44.540 --> 01:00:51.700]  что уже рассказывал, это изоморфизм случайных графов. Тоже мне кажется, очень хорошая иллюстрация
[01:00:52.420 --> 01:01:01.020]  того, как в случае, когда поступающие вам на вход графы не имеют какой-то наперед заданной
[01:01:01.020 --> 01:01:07.100]  известной вам структуры, здорово пользоваться понятием случайности. Ну, то есть, помните,
[01:01:07.100 --> 01:01:12.940]  я говорил про жадный алгоритм, например, раскраски графов? Конечно, если вы знаете,
[01:01:12.940 --> 01:01:18.700]  что тот граф, который вам предстоит раскрашивать, имеет какую-то достаточно специфическую структуру,
[01:01:18.700 --> 01:01:25.260]  то пользоваться жадным алгоритмом преступно. Но если вы действительно заранее не знаете,
[01:01:25.260 --> 01:01:30.780]  какой граф из общей тучки графов на вас капнет, но почему бы не воспользоваться жадным алгоритмом?
[01:01:30.780 --> 01:01:38.220]  Он же ошибается почти всегда не больше, чем вдвое. Это здорово. Вот с изоморфизмом ровно такая же
[01:01:38.220 --> 01:01:46.380]  ситуация. Есть великая проблема изоморфизма графа, которая близко даже не решена.
[01:01:46.380 --> 01:02:09.020]  Пt больше единиц, но мы же его посчитали. Пt поделить на n. t и t фиксировано, но, естественно,
[01:02:09.020 --> 01:02:13.860]  мы считаем, что оно настолько велико, чтобы это было меньше единиц. Да, конечно, это правильный
[01:02:13.860 --> 01:02:18.980]  вопрос. Конечно, наверное, если бы я рассказывал прямо до запятых, как некоторые делают,
[01:02:18.980 --> 01:02:27.460]  да, это, наверное, стоило бы сказать. Спасибо, да, это правильно. t и t фиксировали. В итоге оказалось,
[01:02:27.460 --> 01:02:33.780]  что t зависит от n, но, слава богу, это логариф men, поэтому все равно все в порядке. Еще раз надо
[01:02:33.780 --> 01:02:40.060]  проверить, что в итоге t это логариф men, но все равно это корректно определенная величина,
[01:02:40.060 --> 01:02:47.540]  у всех достаточно больших. Так, ну давайте про изоморфизм несколько слов скажу начально.
[01:02:47.540 --> 01:02:57.040]  Кто такое граф изоморфный? Это, я надеюсь, все понимают или надо пояснять? Существует
[01:02:57.040 --> 01:03:03.180]  биекция между множеством вершин, которая ребра переводит в ребра, а не ребра в неребра. Казалось
[01:03:03.180 --> 01:03:09.020]  бы, ну, елки-палки, на входе вам даются два графа. Ну, естественно, с общим множеством вершин. Ну,
[01:03:09.020 --> 01:03:17.420]  или отождествляем, то есть биекция может быть. Мощность одинаковая. Неужели трудно проверить,
[01:03:17.420 --> 01:03:22.600]  они одинаковой формы или не одинаковые? Ну, вот трудно, да, никто не знает. Я не знаю,
[01:03:22.600 --> 01:03:28.540]  насколько вас учили этому в каком-то виде на алгоритмах, я как-то не слежу. Не учили,
[01:03:28.540 --> 01:03:33.900]  но это, да, это другая история, и я не собираюсь учить. Ну, можете, я не знаю, какие-то вещи,
[01:03:33.900 --> 01:03:39.060]  которые совсем просты, попробовать сделать самостоятельно. Например, легко придумать,
[01:03:39.060 --> 01:03:44.020]  относительно легко придумать алгоритм, который за линейное по числу ребер время проверяет и
[01:03:44.020 --> 01:03:56.140]  заморзнули два дерева. Ну, да, может быть, потому что я его знаю, мне кажется, что просто. Наверное,
[01:03:56.140 --> 01:04:01.460]  да, да, наверное, в этом дело, да, действительно, может быть, даже для деревьев не так просто. Даже
[01:04:01.460 --> 01:04:07.780]  для деревьев не так просто. Согласен. Вот, есть на данном, на данном этапе разные классы графов.
[01:04:07.780 --> 01:04:18.300]  Ну, вот я уже сказал, деревья, удается быстро проверить на изоморфизм. Удается. Там бывают,
[01:04:18.300 --> 01:04:24.500]  например, планарные графы, про которые, кажется, у вас какие-то семинары были, да. На семинарах
[01:04:24.500 --> 01:04:29.500]  что-то про планарность было, а на лекциях я не рассказывал. Но, тем не менее, если есть два
[01:04:29.500 --> 01:04:37.860]  планарных графа, это сложный алгоритм реально, вот, он умеет проверять их на изоморфизм. Так,
[01:04:37.860 --> 01:04:47.860]  ну, еще какой пример я помню. Ну, скажем, если, так вот, существует к, такое, что для любого в из
[01:04:47.860 --> 01:05:07.540]  в дек в не превосходит к, тогда за у от n в к можно проверить. Да, к сожалению, так. Но это
[01:05:07.540 --> 01:05:12.140]  грустно, да, я согласен. Я рассказываю, что, вот, видите, есть какое-то ограниченное количество случаев,
[01:05:12.140 --> 01:05:17.380]  и даже асимптотика ужасная, потому что, если вы хотите ограничить степень каждой вершины десяткой,
[01:05:17.380 --> 01:05:26.500]  ну, извините, n в десятой, это, конечно, все равно, что 2 в степени n. Ну, почти что. Или там n в сотой,
[01:05:26.500 --> 01:05:32.940]  но это уж точно 2 в степени n. Насколько я помню, да, то есть, в общем случае, вроде бы, да,
[01:05:32.940 --> 01:05:38.860]  может быть, были какие-то прорывы, которых я не отследил, но вроде бы так. Еще, например, я помню
[01:05:38.860 --> 01:05:44.540]  такой класс, я не претендую на исчерпывающее знание всех классов, этим занимаются люди,
[01:05:44.540 --> 01:05:50.220]  я просто не настолько уж прям специалист. Есть так называемые интервальные графы, которые с вами
[01:05:50.220 --> 01:05:54.860]  можно проверять на интервальность достаточно быстро, там за линейное время все делается,
[01:05:54.860 --> 01:06:01.420]  вот. Ну, и между собой сравнивать. Интервальные – это графы, у которых вершины отрезки прямой,
[01:06:01.420 --> 01:06:08.500]  а ребра, если отрезки пересекаются. Такие графы называются интервальными. Тут за линейное время
[01:06:08.500 --> 01:06:14.580]  делают. Нет, ну есть масса других классов, я просто могу не знать. Конечно, какие возникают идеи,
[01:06:14.580 --> 01:06:24.860]  надо придумать какой-нибудь вариант. Число вершин, число ребер. Ну, к сожалению, не помогает. Там
[01:06:24.860 --> 01:06:31.620]  собственные числа графов какие-нибудь можно брать. Ну ладно, просто степень вершин те же самые. К
[01:06:31.620 --> 01:06:36.700]  сожалению, два графа с одинаковым набором степеней вершин могут быть неизоморфны, это легко построить
[01:06:36.700 --> 01:06:43.420]  примеры такие. Я понятно говорю, да? То есть проблема в итоге оказывается дико трудной и,
[01:06:43.420 --> 01:06:49.980]  в общем, никто не знает, как ее решить, вообще имеется ли там полинамиальный алгоритм,
[01:06:49.980 --> 01:06:54.740]  не полинамиальный, она даже классу ни к какому сейчас не отнесена. Ну, вернее, ни к какому из
[01:06:54.740 --> 01:07:03.100]  таких основных, это NP трудная задача или нет. Вот этого мы не знаем. Конечно, там классов тоже
[01:07:03.100 --> 01:07:08.540]  великое множество. Вам когда будут сложность вычислений рассказывать, там Мусатов и компания,
[01:07:08.540 --> 01:07:13.460]  вы узнаете такие классы, о которых и не подозревают. В этом смысле изоморфизм,
[01:07:13.460 --> 01:07:17.780]  конечно, к некоторым из них относится. Но вот NP труден он или нет, это неизвестно.
[01:07:17.780 --> 01:07:27.980]  Был сравнительно недавно результат товарища Ласло Бабайи венгерского происхождения,
[01:07:27.980 --> 01:07:32.140]  живущего в Америке, классика абсолютного, который очень много в этой области придумал,
[01:07:32.140 --> 01:07:39.460]  и часть этих результатов принадлежит тоже ему и сделана еще в 80-е, если не 70-е годы. Ну вот он
[01:07:39.460 --> 01:07:46.060]  занимался, занимался, занимался и да занимался до того, что придумал вот такой сложности алгоритм
[01:07:46.060 --> 01:07:55.660]  логарифом N, а здесь в степени гамма, ну где гамма, к сожалению, больше единицы. То есть он придумал
[01:07:55.660 --> 01:08:03.820]  алгоритм, который умеет проверять изоморфизм произвольных графов на N вершинах за вот столько
[01:08:03.820 --> 01:08:13.340]  операций. Ну это весьма круто, потому что это не E в степени N, а это почти E в степени логариф МН,
[01:08:13.340 --> 01:08:20.220]  но правда логариф МН все-таки в степени больше единицы. То есть полиномиальный алгоритм,
[01:08:20.220 --> 01:08:26.700]  это алгоритм, который вот так работает, это алгоритм МН, а тут вот эта C или гамма там в
[01:08:26.700 --> 01:08:33.660]  показателе степени. Но все равно это безумно круто, там доказательства на 100 страниц. У нас когда-то
[01:08:33.660 --> 01:08:39.620]  тут был целый семинар, Мусатов и Миш Тихомиров вместе пытались со студентами разбирать это
[01:08:39.620 --> 01:08:49.220]  доказательство. Год разбирали, но что-то разобрали. Вот такая вот потрясающая совершенно история.
[01:08:49.220 --> 01:08:58.940]  Вот, а я хочу в следующий раз рассказать такую теорему, которую в начале 80-х же
[01:08:58.940 --> 01:09:06.700]  годов двадцатого века доказали три человека. Первый из которых по алфавиту это Бабаи,
[01:09:06.700 --> 01:09:15.780]  но тот же самый. Второй это Эрдаш, но я уже начал писать латиницей. Давайте так же и буду писать,
[01:09:15.780 --> 01:09:24.180]  но Эрдаша мы знаем хорошо. Вот, и товарищ которого я не знаю и, к сожалению, не знаю как читать
[01:09:24.180 --> 01:09:31.140]  правил. Ну то есть я не знаю какого он происхождения, поэтому я боюсь наврать в прочтении его фамилии.
[01:09:31.140 --> 01:09:40.940]  Ну вроде не Венгер по крайней мере, поэтому вообще если Венгер, то вот это эс должно читаться как
[01:09:40.940 --> 01:09:49.980]  Шей, тогда у него очень неплохая фамилия Шелков. Ну вот, ну не знаю, в общем, это может быть немец,
[01:09:49.980 --> 01:09:56.780]  кто угодно. Я, к сожалению, просто не знаю. Вот, они доказали такую замечательную совершенно теорему,
[01:09:56.780 --> 01:10:12.360]  что если GH это случайные графы, ну я вот так напишу, взяли два случайных графа, то вот их
[01:10:12.360 --> 01:10:21.040]  эзоморфизом можно проверить за линейное по числу ребер время, то есть за O от N в квадрате.
[01:10:21.040 --> 01:10:31.560]  Ну это, конечно, не очень четко. Давайте я скажу так. Существует такое множество графов K, N,
[01:10:31.560 --> 01:10:50.880]  это под множество множества тех графов на N вершинах, графов на N вершинах, то мощность K,
[01:10:50.880 --> 01:10:58.080]  N поделить на 2 в степени C из N по 2 стремиться к единице при N, стремящемся к бесконечности,
[01:10:58.080 --> 01:11:27.920]  то проверить принадлежит ли G, K, N можно за O от N в квадрате. И если G и H принадлежат
[01:11:27.920 --> 01:11:36.840]  к N, то проверить их эзоморфизом тоже можно за O от N в квадрате. То проверить G изоморф на
[01:11:36.840 --> 01:11:49.720]  ли H можно за O от N в квадрате. То есть нетремиальность теоремы состоит в том, чтобы придумать,
[01:11:49.720 --> 01:11:54.320]  как описать это множество, чтобы проверка принадлежности к нему графа не занимала
[01:11:54.320 --> 01:11:59.760]  большого времени, а дальше еще так его описать, чтобы вероятность попадания в него, то есть вот
[01:11:59.760 --> 01:12:05.600]  эта дробь стремилась к единице. Но это вероятность просто того же случайный граф принадлежит K, N,
[01:12:05.600 --> 01:12:14.200]  я зачем-то так записал. Понятен пафос, да? Если опять, если вы считаете, что графы на вас
[01:12:14.200 --> 01:12:23.120]  капают равномерно, не с какой-то наперед-задной структурой, одинаково, вероятно, все, то легко
[01:12:23.280 --> 01:12:29.880]  проверить их на изоморфизм, если вы готовы ошибиться в O маленьком от общего числа случаев.
[01:12:29.880 --> 01:12:35.880]  Всего два в степени C из N по два графов, а ошибаетесь вы в такой доле графов,
[01:12:35.880 --> 01:12:46.280]  которая стремится к нулю. Ну типа того, да, можно и так выразиться, конечно, да. Ну естественно,
[01:12:46.280 --> 01:12:52.560]  да, конечно, конечно. То есть вот на случайных графах удается эту пыль под ковер заметать,
[01:12:52.560 --> 01:12:58.600]  а на всех графах 100 страниц, классификация конечных групп и так далее, там жесть полная.
[01:12:58.600 --> 01:13:02.400]  Ну слушайте, как точно-то я отговорил. Все, в следующий раз докажем теорию.

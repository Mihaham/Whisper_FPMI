[00:00.000 --> 00:26.000]  Ну нехорошо, а что я могу сделать? Я что, не лечусь что-то? Народными, ненародными способами? Не лечу?
[00:27.000 --> 00:31.000]  К сожалению, не лечится
[00:47.000 --> 00:49.000]  Добрый день
[00:49.000 --> 00:59.000]  Так, товарищи, ну чего? С голосом-то у меня, конечно, все так же
[00:59.000 --> 01:06.000]  Беда, вот просто беда, понимаете, вот не знаю, что с этим делать, читаю лекции, он не восстанавливается
[01:06.000 --> 01:13.000]  Ну а как-то бросить их читать, мне жалко
[01:14.000 --> 01:22.000]  Ну как вот я их брошу на неделю? Кто ж вам прочитает про матрицу Адамара, как не я?
[01:25.000 --> 01:32.000]  Помните, чем мы закончили прошлую лекцию? А, вы не думали ни над чем?
[01:37.000 --> 01:40.000]  Да, понятно, а какая очевидная?
[01:43.000 --> 01:53.000]  Ну, это кому как очевидно. Вот интересно, поднимите, пожалуйста, руки те, кто вообще думал над вопросом про матрицу Адамара
[01:53.000 --> 01:58.000]  Ну не густо, не густо, дальше можно не спрашивать
[02:00.000 --> 02:05.000]  Так, ну давайте я все-таки поясню, потому что мне кажется, что это важно пояснить
[02:05.000 --> 02:15.000]  Это важно просто с той точки зрения, что вы дополнительно замотивируетесь заниматься такими вещами, это тоже очень полезно
[02:17.000 --> 02:19.000]  Так, значит
[02:25.000 --> 02:31.000]  Предлагалось рассматривать вот такой вот дистанционный граф, такой граф Джонсона
[02:31.000 --> 02:39.000]  Я напоминаю, что это граф Джонсона
[02:42.000 --> 02:44.000]  Называется граф Джонсона
[02:45.000 --> 02:49.000]  Так, слушайте, а кто-нибудь помнит вообще, как он определялся?
[02:51.000 --> 03:00.000]  Нет, товарищи, если вы даже этого не помните, то это не очень хорошо, потому что я стараюсь читать же не на предмет того, чтобы вы записали, мы это как раз в прошлый раз обсуждали
[03:00.000 --> 03:06.000]  А на предмет того, чтобы вы еще и поняли, чего происходит, ну и могли это еще успевали записать
[03:06.000 --> 03:12.000]  Поэтому как-то хочется, чтобы вы старались следить за происходящим на протяжении всего семестра
[03:13.000 --> 03:16.000]  Так, сейчас я разговорюсь как-то, я уже это чувствую
[03:16.000 --> 03:32.000]  Значит, смотрите, я напоминаю, что конкретно в этом случае вершинами графа служат все возможные под множество множества, которые я обозначил там R с индексом N
[03:32.000 --> 03:36.000]  Это было множество просто состоящее из каких-то N элементов
[03:36.000 --> 03:46.000]  Ну, R с индексом N это 1, 2 и так далее N
[03:53.000 --> 03:56.000]  И мощность каждого множества равняется N пополам
[03:57.000 --> 03:59.000]  Вот такими являются вершины
[03:59.000 --> 04:06.000]  Это все возможные N пополам элементные под множество некоторого N элементного множества
[04:06.000 --> 04:27.000]  Так, а ребрами мы соединяем две вершины, два под множество, если мощность их пересечения в данном конкретном случае равняется величине N поделить на 4
[04:27.000 --> 04:29.000]  Ну, сейчас должны вспомнить
[04:32.000 --> 04:36.000]  Так, причем же здесь матрица Адамара?
[04:36.000 --> 04:38.000]  Матрица Адамара
[04:38.000 --> 04:40.000]  Смотрите
[04:41.000 --> 04:52.000]  Матрица Адамара, как я напоминал в прошлый раз, это матрица, которая целиком и полностью состоит из плюс и минус единиц
[04:52.000 --> 05:01.000]  Причем, каждые две ее строчки перпендикулярны друг другу, то есть их скалярное произведение равняется нулю
[05:02.000 --> 05:08.000]  Так, товарищи, тоже помним? Тоже помним, молодцы
[05:10.000 --> 05:11.000]  Так
[05:12.000 --> 05:21.000]  Ну, смотрите, я напомню, что это в частности означает, что матрицу можно привести к каноническому виду
[05:22.000 --> 05:28.000]  А именно, можно считать, что матрица Адамара выглядит следующим образом
[05:28.000 --> 05:41.000]  У нее верхняя строчка состоит из сплошных единиц, а дальше каждая последующая строчка содержит половину единиц и половину минус единиц
[05:43.000 --> 05:46.000]  Так, друзья, вот это помните, нет?
[05:46.000 --> 05:49.000]  Я с большой надеждой на вас гляжу
[05:50.000 --> 05:51.000]  Шикарно
[05:52.000 --> 06:02.000]  Так, ну, можно, например, типично вот так вот нарисовать половину единиц, половину минус единиц, вот они накладываются, получается ноль
[06:02.000 --> 06:05.000]  Скалярное произведение получается равным нулю
[06:07.000 --> 06:10.000]  Ну, дальше там тоже как-то происходит
[06:11.000 --> 06:16.000]  Тут минус один, например, тут один, тут минус один, ну и так далее
[06:16.000 --> 06:24.000]  Это матрица размера N умножить на N
[06:25.000 --> 06:32.000]  Ну и когда мы обсуждали матрицу Адамара в прошлом году, мы в частности поняли, что вот из всего этого знаете, чего следует?
[06:33.000 --> 06:34.000]  Что N делится на 4
[06:35.000 --> 06:41.000]  Было, конечно, такое, N обязательно делится на 4, это простое упражнение для тех, кто этого не знает
[06:41.000 --> 06:46.000]  Ну, можете послушать соответствующую лекцию в записи, это было тоже сделано
[06:46.000 --> 06:52.000]  В общем, N, давайте так напишу, делится на 4, как следствие из определения
[06:52.000 --> 07:01.000]  Ну и великая гипотеза состояла в том, что если четверка является делителем числа N, то такая матрица всегда существует
[07:02.000 --> 07:12.000]  Причем гипотеза не доказана, а доказана это только в частных случаях, и я приводил там какие-то примеры конструкции, когда это действительно получается
[07:13.000 --> 07:16.000]  Вот, ну хорошо, причем же здесь вот этот граф?
[07:17.000 --> 07:24.000]  Смотрите, давайте я от этой матрицы перейду к матрице, которая по сути ей равносильна
[07:25.000 --> 07:32.000]  Мы даже когда строили примеры таких матриц или доказывали какие-то оценки про них, что-то подобное делали
[07:33.000 --> 07:35.000]  Ну, может быть еще проделаем чуть позже
[07:36.000 --> 07:44.000]  Значит, мы в этой матрице единицы сохраним, а минус единицы заменим нулями
[07:44.000 --> 07:55.000]  Так, очень просто, возьмем просто единицы сохраним, а минус единицы заменим нулями
[07:56.000 --> 07:59.000]  Половина единиц теперь будет половина нулей
[08:00.000 --> 08:09.000]  Тут вот четверть единиц, четверть нулей, четверть единиц, четверть нулей, ну и так далее
[08:11.000 --> 08:12.000]  Так, все пока понятно
[08:14.000 --> 08:19.000]  Так, ну посмотрите сюда, вот на эту строчку
[08:20.000 --> 08:28.000]  В ней половина, то есть n пополам единиц и n пополам нулей
[08:32.000 --> 08:36.000]  Ну, да-да-да, я говорю, посмотрите вот на эту строчку, я ее показал, да
[08:37.000 --> 08:41.000]  Первую строчку забываем, вот так отделяем и смотрим все, что ниже первой строчки
[08:44.000 --> 08:46.000]  Так, друзья, успеваете все фиксировать?
[08:47.000 --> 08:55.000]  Значит, вот все, что ниже первой строчки, там каждый раз половина, то есть n пополам единиц и n пополам нулей
[08:58.000 --> 08:59.000]  Да?
[09:00.000 --> 09:06.000]  Ну, слушайте, как этому теперь поставить в соответствие вершины графа Джонсона?
[09:07.000 --> 09:20.000]  Ну, давайте скажем, что вот здесь, например, вершина графа Джонсона, которая отвечает строчке, это просто множество, состоящее из первых n пополам чисел вот отсюда
[09:22.000 --> 09:24.000]  1, 2 и так далее n пополам
[09:25.000 --> 09:33.000]  Вот для этой строчки, давайте еще вот так напишем, 1, 2 и так далее n пополам
[09:33.000 --> 09:44.000]  Вот для этой строчки соответственно у нас получится 1, 2 и так далее n поделить на 4, потом будет n пополам плюс 1 и так далее 3n на 4
[09:47.000 --> 09:51.000]  Ну, я настолько подробно рассказываю, чтобы у вас точно было понимание, чего происходит
[09:52.000 --> 10:00.000]  То есть, я просто сопоставляю строчки из нулей единиц номера позиции, на которых стоят единицы
[10:00.000 --> 10:04.000]  Образуя тем самым вершины этого графа
[10:05.000 --> 10:07.000]  Ну и последний штрих
[10:08.000 --> 10:11.000]  Смотрите, изначально вот эти векторы были артагональны
[10:13.000 --> 10:19.000]  Значит, как пересекались множество их единиц между собой?
[10:20.000 --> 10:21.000]  Poin на 4
[10:23.000 --> 10:25.000]  Друзья, вот это понятно?
[10:25.000 --> 10:32.000]  Множество единиц одного вектора и множество единиц любого другого пересекаются по общим n на 4 элементам
[10:33.000 --> 10:38.000]  То есть, что образуют вот эти вершины в графе Джонсона?
[10:40.000 --> 10:42.000]  О, многие знают даже слово клика
[10:43.000 --> 10:45.000]  Ну, я бы сказал, полный подграф
[10:46.000 --> 10:48.000]  Полный подграф, он не будет в графе Джонсона
[10:48.000 --> 10:50.000]  О, многие знают даже слово клика
[10:51.000 --> 10:53.000]  Ну, я бы сказал, полный подграф
[10:54.000 --> 10:57.000]  Полный подграф, он по-другому еще называется словом клика
[10:58.000 --> 11:00.000]  Поднимите, пожалуйста, руки те, кто знает слово клика
[11:01.000 --> 11:02.000]  А, ну все знают, отлично
[11:04.000 --> 11:10.000]  Значит, понимаете, что в терминах вот этого графа, который имеет прямое отношение к теории кодирования
[11:11.000 --> 11:13.000]  Максимальная клика в этом графе
[11:14.000 --> 11:17.000]  Максимальная клика, то есть максимальный полный подграф
[11:18.000 --> 11:22.000]  Это в точности вопрос о существовании матрицы Адамара
[11:23.000 --> 11:30.000]  Вопрос о том, чтобы найти максимальную клику в этом графе, равносилен тому, чтобы построить матрицу Адамара
[11:35.000 --> 11:37.000]  Нет, она не может быть больше, чем на n вершинах
[11:38.000 --> 11:39.000]  Он именно равносилен
[11:40.000 --> 11:44.000]  То есть, если у вас есть клика, вы же можете обратно по ней построить вот такую матрицу
[11:45.000 --> 11:46.000]  Тут все взаимно заменимо
[11:46.000 --> 11:50.000]  И тогда у вас получится что-то большее, чем матрица Адамара, но такого не бывает
[11:57.000 --> 12:01.000]  Да, да, да, но матрица Адамара это максимум, что можно построить, конечно
[12:02.000 --> 12:04.000]  И она-то неизвестно существует ли
[12:05.000 --> 12:13.000]  Но если она существует, то несложно из линейно-алгебраических соображений понять, что к ней уже ничего ниже не добавишь
[12:14.000 --> 12:20.000]  Просто, ну как, если они попарно-ортагональны в пространстве, то их не больше, чем размерность
[12:22.000 --> 12:29.000]  А при этом они лежат еще в гиперплоскости, которая имеет размерность n-1, потому что сумма координат равняется нулю
[12:30.000 --> 12:32.000]  Поэтому их точно не больше, чем n-1
[12:33.000 --> 12:36.000]  Я не знаю, друзья, вот эта последняя дискуссия была понятна?
[12:37.000 --> 12:46.000]  Это важный момент, что если матрица Адамара существует, это уже круто, но добавить к ней какой-то вектор ниже точно не получится
[12:50.000 --> 12:51.000]  Так, нормально все?
[12:53.000 --> 12:56.000]  Ну кроме того, что я поскрипываю голосом в остальном, все нормально?
[12:57.000 --> 12:58.000]  Это хорошо
[13:01.000 --> 13:03.000]  Так, на этот вопрос ответили, теперь смотрите
[13:04.000 --> 13:05.000]  Второй вопрос
[13:06.000 --> 13:07.000]  Какой у нас был второй вопрос?
[13:09.000 --> 13:10.000]  Число треугольников
[13:11.000 --> 13:12.000]  Вот сложная задача
[13:13.000 --> 13:19.000]  Кстати, да, число треугольников – это число способов составить три строчки в матрице Адамара, правильно?
[13:21.000 --> 13:27.000]  Число треугольников – это и есть три строчки в матрице Адамара, вот сколькими способами их можно построить
[13:28.000 --> 13:29.000]  Так, ну, смотрите
[13:33.000 --> 13:35.000]  Я написал количество вершин
[13:37.000 --> 13:38.000]  Согласны?
[13:39.000 --> 13:40.000]  Давайте я напишу это число вершин
[13:45.000 --> 13:47.000]  Давайте теперь посчитаем количество ребер
[13:48.000 --> 13:50.000]  Вот мы зафиксировали какую-то вершину, да?
[13:54.000 --> 13:55.000]  Правильно!
[13:56.000 --> 13:58.000]  Да, давайте зафиксируем одну вершину
[13:59.000 --> 14:04.000]  Ну, например, один, два, и так далее, n пополам
[14:05.000 --> 14:07.000]  И посчитаем, какова ее степень
[14:08.000 --> 14:10.000]  То есть, сколько ребер из нее выходит
[14:11.000 --> 14:12.000]  Как это посчитать?
[14:14.000 --> 14:18.000]  Надо отсюда выбрать n поделить на 4 элемента, правильно?
[14:19.000 --> 14:22.000]  И из оставшихся тоже выбрать n поделить на 4
[14:23.000 --> 14:26.000]  Так, друзья, никого не кокает такая простая комбинаторика
[14:27.000 --> 14:29.000]  Все успевают понять, что происходит
[14:30.000 --> 14:32.000]  Ну, нам нужна вот эта вершина А
[14:33.000 --> 14:36.000]  Есть вершина B, которая вместе с А образует ребро
[14:37.000 --> 14:41.000]  Ну, значит, она должна пересекаться с этим множеством по n поделить на 4 элемента
[14:42.000 --> 14:52.000]  Мы можем t из n на 2 по n поделить на 4 способами выбрать вот эти общие n поделить на 4 элементов
[14:54.000 --> 14:55.000]  Правильно!
[14:56.000 --> 14:58.000]  И это надо возвести в квадрат
[14:59.000 --> 15:04.000]  И из оставшихся n пополам элементов нам тоже надо выбрать n поделить на 4
[15:06.000 --> 15:09.000]  Так, это степень каждой вершины
[15:11.000 --> 15:13.000]  Степень каждой вершины
[15:14.000 --> 15:16.000]  Но это называется граф регулярно
[15:17.000 --> 15:22.000]  Ну, мы знаем, что сумма степеней всех вершин – это удвоенное количество ребер
[15:22.000 --> 15:30.000]  Поэтому число ребер вот в таком графе Джонсона, который мы сейчас рассматриваем, это 1 вторая
[15:31.000 --> 15:33.000]  c из n по n на 2
[15:34.000 --> 15:36.000]  на c из n на 2
[15:37.000 --> 15:39.000]  по n на 4 в квадрате
[15:40.000 --> 15:44.000]  Друзья, все не зря вы увидите, сейчас будет некий микрокатарсис, когда мы посчитаем
[15:47.000 --> 15:49.000]  Так, но нас-то интересует число треугольников, правильно?
[15:49.000 --> 15:52.000]  Вот мы посчитали, сколько есть ребер
[15:53.000 --> 15:55.000]  Как теперь посчитать, сколько есть треугольников?
[15:56.000 --> 16:07.000]  Надо найти число способов построить вершину, которая и вот с этой, и вот с этой образует ребро
[16:08.000 --> 16:11.000]  Смотрите, давайте я картинку такую нарисую
[16:12.000 --> 16:17.000]  Вот это a, в ней n пополам элементов
[16:17.000 --> 16:29.000]  Дальше есть b, в ней тоже n пополам элементов, а вот эта общая часть, она размера n поделить на 4
[16:30.000 --> 16:32.000]  Потому что они образуют ребро
[16:33.000 --> 16:38.000]  А, вот это b, и картинка в терминах множества, она получается вот такая
[16:39.000 --> 16:41.000]  Нормально?
[16:42.000 --> 16:49.000]  Так, теперь нас интересует c, такая, что она образует ребро из a из b
[16:50.000 --> 16:58.000]  То есть, смотрите, она должна вот эту a пересекать по n на 4 элементов
[16:59.000 --> 17:03.000]  И b тоже должна пересекать по n поделить на 4
[17:04.000 --> 17:05.000]  Все успевают?
[17:06.000 --> 17:10.000]  Ну, давайте буквой, сейчас нарисую красиво
[17:12.000 --> 17:15.000]  Знаете, по мне проснулся художественный талант
[17:16.000 --> 17:19.000]  Я всегда считал, что я очень плохо рисую, а смотрите, как красиво
[17:21.000 --> 17:23.000]  Чертежник прям
[17:24.000 --> 17:30.000]  Так, вот здесь давайте будет, так, здесь чертежник
[17:31.000 --> 17:33.000]  Так, здесь у нас n на 4
[17:34.000 --> 17:38.000]  А давайте b
[17:39.000 --> 17:44.000]  Сейчас я как-то так вот нарисую, о, не, плохо, плохой рисунок
[17:45.000 --> 17:48.000]  О, вот так, а?
[17:49.000 --> 17:51.000]  Докторская что, диссертация?
[17:52.000 --> 17:56.000]  А, колбаса, сарделька
[17:57.000 --> 18:02.000]  Ну, докторская колбаса, да, видите, вот, у кого какая ассоциация, да?
[18:03.000 --> 18:07.000]  Вроде бы я главный специалист по сарделькам, но мне говорят, докторская, говорю, диссертация
[18:08.000 --> 18:13.000]  О чем мне докторская диссертация? У меня уже 20 лет как докторская диссертация
[18:16.000 --> 18:17.000]  Так
[18:19.000 --> 18:22.000]  Ну, похоже, я себя перехвалил
[18:23.000 --> 18:33.000]  Давайте буковка k, это количество элементов той части множества c, которая попадает вот сюда
[18:35.000 --> 18:37.000]  Просто так обозначим
[18:38.000 --> 18:46.000]  То есть, ну, если хотите, k это что такое? Это a-b пересеченное с c, просто по мощности
[18:47.000 --> 18:50.000]  Просто по определению k это мощность вот этого пересечения
[18:53.000 --> 18:59.000]  Так, сколько стало быть в этой докторской вот здесь элементов?
[19:00.000 --> 19:01.000]  А?
[19:02.000 --> 19:04.000]  n на 4 минус k
[19:05.000 --> 19:12.000]  Смотрите, c должно пересекаться с a, чтобы образовать ребро по n поделительно 4 элементам
[19:13.000 --> 19:14.000]  Должно?
[19:16.000 --> 19:18.000]  Так, кто-то мне решил позвонить, секунду
[19:19.000 --> 19:20.000]  А я не знаю кто это
[19:22.000 --> 19:23.000]  Интересно, кстати
[19:24.000 --> 19:31.000]  Так, ну, значит, вот здесь у нас n поделительно 4 минус k
[19:34.000 --> 19:35.000]  Видно что-нибудь?
[19:36.000 --> 19:40.000]  Вот эта вот маленькая сардельечка, она размера n поделительно 4 минус k
[19:43.000 --> 19:50.000]  Так, дальше есть кусочек множества c, который находится вот в этой части, каков его размер?
[19:51.000 --> 19:57.000]  k правильно, потому что тумба вот этих двух чисел должна быть n поделительно 4
[19:58.000 --> 20:00.000]  Чтобы образовалось ребро уже с b
[20:01.000 --> 20:02.000]  Успеваете?
[20:04.000 --> 20:09.000]  Так, и у нас еще вот тут вот сколько элементов?
[20:10.000 --> 20:12.000]  Еще n на 4, правильно
[20:13.000 --> 20:16.000]  Потому что тут n на 4, тут n на 4, тут n на 4
[20:17.000 --> 20:21.000]  А всего же n значит есть еще свободное n поделительно 4 элементов
[20:23.000 --> 20:25.000]  И здесь есть кусочек нашей сардельки
[20:27.000 --> 20:32.000]  Ну, ясно, что его размер это n поделительно 4 минус k
[20:34.000 --> 20:39.000]  То есть, смотрите, вершину c можно выбрать
[20:40.000 --> 20:48.000]  t из n на 4 по k умножить на c из n на 4 по n на 4 минус k
[20:49.000 --> 20:52.000]  умножить на c из n на 4 по k
[20:53.000 --> 20:59.000]  и умножить на c из n на 4 по n на 4 минус k способами, если k зафиксировано
[20:59.000 --> 21:03.000]  Друзья, мне кажется, я предельно понятен, но может быть я...
[21:04.000 --> 21:05.000]  Что?
[21:06.000 --> 21:10.000]  Но вот мы их зафиксировали одним из вот стольких способов
[21:11.000 --> 21:12.000]  Да?
[21:13.000 --> 21:16.000]  И при каждой фиксации a и b вот в таком количестве
[21:17.000 --> 21:20.000]  У нас столько способов выбрать c при данном k
[21:21.000 --> 21:23.000]  Ну, то есть, дальше надо просуммировать по k
[21:24.000 --> 21:26.000]  от 0 до n на 4, правильно?
[21:26.000 --> 21:28.000]  Надо просуммировать по k от 0 до n на 4
[21:29.000 --> 21:31.000]  И умножить вот на это
[21:32.000 --> 21:35.000]  Ай, виноват, и еще поделить на 3
[21:36.000 --> 21:38.000]  Так, все понимают, почему поделить на 3?
[21:39.000 --> 21:41.000]  Ну, конечно!
[21:42.000 --> 21:43.000]  Что?
[21:44.000 --> 21:48.000]  Тут уже пополам поделили, поэтому когда третью добавляем только на 3
[21:50.000 --> 21:54.000]  То есть, ответ про число треугольников, который мы делаем
[21:54.000 --> 21:57.000]  Ответ про число треугольников, куда бы его написать?
[21:58.000 --> 21:59.000]  Это 1 шестая
[22:00.000 --> 22:03.000]  c из n по n на 2
[22:04.000 --> 22:06.000]  c из n на 2
[22:07.000 --> 22:09.000]  по n на 4 в квадрате
[22:10.000 --> 22:12.000]  Умножить на сумму
[22:13.000 --> 22:14.000]  по k от 0
[22:15.000 --> 22:17.000]  до n поделить на 4
[22:18.000 --> 22:20.000]  c из n на 4 по k
[22:21.000 --> 22:23.000]  В четвертой степени
[22:24.000 --> 22:26.000]  Поняли, да, почему я эту задачу поставил?
[22:27.000 --> 22:29.000]  Помните про четвертую степень?
[22:30.000 --> 22:31.000]  Говорили
[22:32.000 --> 22:35.000]  Сумма четвертых степеней, ее асимптотика
[22:36.000 --> 22:40.000]  Отвечает за асимптотику числа треугольников в дистанционном графе Джонсона
[22:41.000 --> 22:46.000]  Ну, по модулю вот этой фигни, которую мы в прошлый раз вообще сосчитали с помощью Стирлинга
[22:47.000 --> 22:49.000]  Это совсем легко
[22:50.000 --> 22:52.000]  Вот так вот
[22:53.000 --> 22:56.000]  По-моему это очень иллюстративно, все-таки надо было это проговорить
[23:00.000 --> 23:01.000]  Так
[23:05.000 --> 23:07.000]  Есть какие-то вопросы?
[23:08.000 --> 23:09.000]  Комментарии?
[23:10.000 --> 23:11.000]  Впечатления?
[23:22.000 --> 23:27.000]  Вообще голос, конечно, скрипучий, но чуть-чуть как будто бы позвончий
[23:28.000 --> 23:31.000]  Так, в целом скрипучий, но вроде бы что-то такое
[23:33.000 --> 23:36.000]  Говорят, вот есть народные средства, интересно, конечно
[23:37.000 --> 23:38.000]  Надо че-то делать
[23:39.000 --> 23:40.000]  Вот так вот
[23:41.000 --> 23:42.000]  Вот так вот
[23:43.000 --> 23:44.000]  Вот так вот
[23:45.000 --> 23:46.000]  Вот так вот
[23:48.000 --> 23:49.000]  Вот так вот
[23:50.000 --> 23:51.000]  Вот так вот
[23:52.000 --> 23:53.000]  Че-то такое шарахнуть
[23:54.000 --> 23:56.000]  Чтобы уже никогда не заболевал
[24:05.000 --> 24:06.000]  Че такого, да?
[24:08.000 --> 24:09.000]  Я вот сам думаю, чего
[24:13.000 --> 24:15.000]  Так, друзья, давайте двигаться дальше
[24:16.000 --> 24:21.000]  На самом деле, конечно, все вот эти асимптотические истории я как мог замотивировал
[24:22.000 --> 24:25.000]  Но они будут вылезать у нас постоянно в разных задачах
[24:26.000 --> 24:29.000]  И вот сейчас я хочу перейти к очень важному разделу
[24:30.000 --> 24:33.000]  Который касается того, что называется случайные графы
[24:34.000 --> 24:37.000]  Но смысл этого раздела, он очень естественный
[24:38.000 --> 24:39.000]  Он очень простой
[24:40.000 --> 24:44.000]  Вот что можно сказать про типичный граф?
[24:46.000 --> 24:49.000]  Можно найти какие-то типичные его свойства
[24:49.000 --> 24:54.000]  Может быть, отладить какие-то алгоритмы можно не на всех графах, но только на типичных
[24:55.000 --> 24:56.000]  Вот что значит типичный граф?
[25:01.000 --> 25:06.000]  Ну, это, конечно, верно, особенно если просто проведено либо нет
[25:07.000 --> 25:08.000]  Но это правильно, да
[25:09.000 --> 25:12.000]  То есть, на самом деле, я хотел сказать философски более широко
[25:13.000 --> 25:16.000]  Типичный граф, это зависит от ситуации
[25:16.000 --> 25:23.000]  Действительно, если мы изучаем какие-нибудь процессы, связанные с формированием сложных сетей типа интернета
[25:24.000 --> 25:28.000]  То кажется, что типичный граф устроен не совсем так, как Миша предлагает
[25:29.000 --> 25:36.000]  Потому что типичный граф, который интерпретируется как вершины сайта, а ребра это гиперссылки между ними
[25:37.000 --> 25:41.000]  Он совсем не такой случайный, что в нем либо проведено ребро, либо нет
[25:41.000 --> 25:45.000]  На самом деле, у него есть какие-то свойства, которые делают его сильно отличающимся
[25:46.000 --> 25:50.000]  Поэтому в зависимости от того, какую жизненную ситуацию вы изучаете
[25:51.000 --> 25:53.000]  Друзья, вы это для себя как-то вот отразите
[25:54.000 --> 25:58.000]  В зависимости от того, с какой именно конкретной физической ситуацией или там
[25:59.000 --> 26:02.000]  Ситуацией в теории алгоритмов вы встречаетесь
[26:03.000 --> 26:06.000]  Случайность нужно понимать по-разному, типичность нужно понимать по-разному
[26:06.000 --> 26:08.000]  Но вот мы начнем с очень важной модели
[26:09.000 --> 26:11.000]  Я ее тоже сейчас замотивирую
[26:12.000 --> 26:15.000]  Которая была предложена давно, давайте так скажем
[26:16.000 --> 26:24.000]  Но всерьез начала с математической точки зрения исследоваться на рубеже 50-х и 60-х годов 20-го века
[26:25.000 --> 26:29.000]  Давайте я что-нибудь вот так сделаю
[26:29.000 --> 26:35.000]  Это годы, когда появились самые такие вот, как бы это сказать, пионерские работы на эту тему
[26:36.000 --> 26:41.000]  В которых действительно доказывались очень существенные свойства относительно этой модели
[26:42.000 --> 26:47.000]  Принято ее приписывать в честь этого двум товарищам
[26:48.000 --> 26:51.000]  Одного и второго, а другого и третьего
[26:52.000 --> 26:56.000]  Принято ее приписывать в честь этого двум товарищам
[26:57.000 --> 27:01.000]  Одного из которых звали Эрдеш, другого Ренни
[27:02.000 --> 27:05.000]  Ну я просто ударение поставил, чтобы было понятно
[27:06.000 --> 27:08.000]  Вот
[27:09.000 --> 27:16.000]  Но физики, вы тут у нас не физики, но физики обижаются
[27:16.000 --> 27:20.000]  Говорят, да мы знали эту модель задолго, да, Эрдеш и Ренни
[27:21.000 --> 27:23.000]  Нахрена вы этих математиков вообще сюда приплели?
[27:24.000 --> 27:32.000]  Ну, приплели, потому что, повторяю, они стали математически, систематически исследовать эту модель, о которой я сейчас скажу
[27:33.000 --> 27:40.000]  Но, чтобы не обижать физиков, говорят, классическая модель
[27:41.000 --> 27:50.000]  А еще, совсем правильно, отражая суть модели, называют ее биномиальной
[27:51.000 --> 27:53.000]  Она же биномиальная
[27:59.000 --> 28:02.000]  Классическая, она же биномиальная
[28:04.000 --> 28:07.000]  Ну а смысл ровно такой, как было сказано
[28:07.000 --> 28:14.000]  У нас есть, давайте, просто числа от единицы до n
[28:15.000 --> 28:18.000]  И мы их интерпретируем как вершины нашего графа
[28:19.000 --> 28:25.000]  Ну, то есть, есть n объектов, неважно какой природе, можно считать, что это просто первые n натуральных чисел
[28:26.000 --> 28:28.000]  Вот они образуют множество вершин графа
[28:29.000 --> 28:31.000]  Это множество никоим образом не случайно
[28:32.000 --> 28:40.000]  Дальше мы берем ребра полного графа на этом множестве вершин
[28:41.000 --> 28:48.000]  Графы считаем пока что обыкновенными, то есть, без петель, без кратных ребер, без ориентации
[28:49.000 --> 28:52.000]  Вот в полном графе c и z подва ребер
[28:53.000 --> 28:59.000]  Дальше берем некоторое число p из отрезка 0,1
[29:00.000 --> 29:05.000]  И интерпретируем его как вероятность возникновения каждого из этих ребер
[29:05.000 --> 29:17.000]  То есть, e и t проводим с вероятностью p
[29:18.000 --> 29:28.000]  И не проводим с вероятностью...
[29:29.000 --> 29:30.000]  Какой?
[29:31.000 --> 29:34.000]  1 минус p, да, но я думал вы скажете q
[29:37.000 --> 29:43.000]  У вас в курсе теории вероятностей было что-то про такую схему? Или не успели еще?
[29:45.000 --> 29:46.000]  Ну, хорошо, да
[29:47.000 --> 29:51.000]  Ничего страшного, я все напомню, все докажу независимо от теории вероятностей
[29:52.000 --> 29:53.000]  Это будет только на пользу
[29:54.000 --> 29:58.000]  Да, я продублирую какой-то материал, но я его со своей точки зрения проинтерпретирую
[29:58.000 --> 30:00.000]  И мне кажется, так будет только лучше
[30:01.000 --> 30:04.000]  Называется это схема испытаний Бернули
[30:05.000 --> 30:10.000]  Если вы такого не слышали в курсе теории вероятностей, не переживайте, скоро услышите
[30:11.000 --> 30:13.000]  А поднимите руки, кто слышал?
[30:14.000 --> 30:16.000]  А, ну вроде практически все
[30:17.000 --> 30:21.000]  Схема испытаний Бернули, то есть я не сказал важную вещь
[30:22.000 --> 30:26.000]  Все вот эти вот операции проведения и непроведения, они, конечно, взаимно независимы
[30:26.000 --> 30:31.000]  Но потому что вообще это важно сказать
[30:32.000 --> 30:34.000]  Вдруг есть какие-то зависимости?
[30:35.000 --> 30:37.000]  Но мы предполагаем, что никаких зависимости нет
[30:39.000 --> 30:41.000]  Так, дурацкий вопрос
[30:42.000 --> 30:47.000]  Вот представим себе, что n равняется 4
[30:48.000 --> 30:50.000]  И вот нам дан вот такой конкретный граф
[30:51.000 --> 30:56.000]  1, 2, 3, 4 на наших четырех вершинах
[30:57.000 --> 31:01.000]  Какова вероятность его возникновения, если схема описана вот так?
[31:06.000 --> 31:10.000]  Правильно совершенно, да, p в четвертый умножить на q в квадрате
[31:11.000 --> 31:15.000]  Мой богатейший опыт подсказывает, что не все это сходу понимают
[31:16.000 --> 31:21.000]  То есть вот чему ж вас точно, я думаю, учили в курсе теории вероятностей
[31:22.000 --> 31:24.000]  Подтвердите или опровергните, пожалуйста
[31:25.000 --> 31:31.000]  Это что все зиждется на элементарных исходах
[31:32.000 --> 31:34.000]  Они же элементарные события
[31:35.000 --> 31:36.000]  Было такое?
[31:37.000 --> 31:42.000]  Вот здесь кто является элементарным событием?
[31:43.000 --> 31:44.000]  Нет
[31:45.000 --> 31:46.000]  Граф
[31:48.000 --> 31:55.000]  Значит, если давать формальное теоретико-вероятностное определение, то есть множество элементарных событий
[31:56.000 --> 32:01.000]  И это множество состоит из всевозможных графов на n вершинах
[32:02.000 --> 32:09.000]  То есть мощность множества элементарных событий это 2 в степени c из n под 2
[32:09.000 --> 32:14.000]  Значит, мы не знаем, с какой вероятностью граф связан?
[32:40.000 --> 32:41.000]  Так
[32:42.000 --> 32:45.000]  Вот такое вот обозначение было
[32:46.000 --> 32:47.000]  Или f писали?
[32:49.000 --> 32:51.000]  А, Борелевская, черт
[32:52.000 --> 32:54.000]  А вот так писали?
[32:55.000 --> 32:58.000]  Ну не попал сходу, да
[32:59.000 --> 33:01.000]  У вас было слово страшное сигма-алгебра
[33:04.000 --> 33:05.000]  А нам не надо
[33:05.000 --> 33:07.000]  Ну у нас все конечное, товарищи
[33:08.000 --> 33:09.000]  Ну пусть большое, конечно
[33:10.000 --> 33:11.000]  Но конечное же, понимаете?
[33:12.000 --> 33:13.000]  То есть что такое fn?
[33:14.000 --> 33:16.000]  Это просто 2 в степени ωn
[33:18.000 --> 33:23.000]  Ну множество всех под множество в множестве всех графов
[33:26.000 --> 33:27.000]  Сейчас, друзья, понятно?
[33:28.000 --> 33:30.000]  То есть событие, событие
[33:31.000 --> 33:32.000]  Это просто, как я уже говорил
[33:32.000 --> 33:33.000]  Множество графов
[33:35.000 --> 33:37.000]  По-другому, это свойство графа
[33:38.000 --> 33:40.000]  Вот мы говорим граф связан
[33:41.000 --> 33:45.000]  Это значит, что он принадлежит множеству, состоящему из всех связанных графов
[33:47.000 --> 33:49.000]  Друзья, я понятно говорю?
[33:51.000 --> 33:57.000]  Вероятность того, что граф связан, это просто сумма вероятностей связанных графов
[33:57.000 --> 34:02.000]  Но вы на меня смотрите грустными, наверное, глазами, потому что непонятно, как такую сумму считать
[34:03.000 --> 34:04.000]  Почему я, типа, клоню
[34:05.000 --> 34:10.000]  А вот я клоню к тому, что методы теории вероятностей в сочетании с методами дискретной математики
[34:11.000 --> 34:14.000]  И в сочетании со всякими вот этими асимптатическими ужасами
[34:15.000 --> 34:16.000]  Они позволяют это решать
[34:22.000 --> 34:24.000]  Так, друзья, вот это все
[34:25.000 --> 34:29.000]  Так, друзья, почему я вообще на связанность так напираю?
[34:30.000 --> 34:34.000]  Потому что для свойства связанности вот эта модель
[34:35.000 --> 34:38.000]  Наиболее естественна с точки зрения именно практической интерпретации
[34:39.000 --> 34:42.000]  Ну, потому что, как я обычно говорю людям
[34:45.000 --> 34:48.000]  Сейчас я сотру и скажу
[34:48.000 --> 34:49.000]  Как я обычно говорю людям?
[34:50.000 --> 34:51.000]  Я им говорю так
[34:52.000 --> 34:53.000]  Вот послушайте
[34:54.000 --> 34:56.000]  Есть компьютеры
[34:57.000 --> 34:58.000]  Видите какие компьютеры?
[35:01.000 --> 35:02.000]  Я сегодня в ударе
[35:04.000 --> 35:06.000]  А? Какое художество
[35:07.000 --> 35:08.000]  Я в ударе
[35:09.000 --> 35:10.000]  Я в ударе
[35:11.000 --> 35:12.000]  Я в ударе
[35:13.000 --> 35:14.000]  Я в ударе
[35:15.000 --> 35:16.000]  Я в ударе
[35:16.000 --> 35:17.000]  Какое художество
[35:19.000 --> 35:22.000]  Один компьютер стоит в Москве, другой в Санкт-Петербурге
[35:23.000 --> 35:25.000]  Третий там в Мурманске
[35:26.000 --> 35:27.000]  Твертый на Байкале
[35:28.000 --> 35:30.000]  Пятый где-нибудь в Сочи
[35:31.000 --> 35:33.000]  Видите, как я знаю географию
[35:37.000 --> 35:38.000]  Но важно другое
[35:39.000 --> 35:41.000]  Важно, что мы давайте предположим
[35:42.000 --> 35:43.000]  Изначально
[35:45.000 --> 35:47.000]  Каждые два компьютера на этой картинке
[35:48.000 --> 35:50.000]  Но я умру это все рисовать
[35:51.000 --> 35:54.000]  Попарно соединены какими-то линиями связи
[35:55.000 --> 35:57.000]  Ну, то есть, есть два телефона, там два модема
[35:58.000 --> 36:00.000]  Можно между ними передавать какую-то информацию туда-обратно
[36:01.000 --> 36:02.000]  И тут тоже есть два телефона
[36:03.000 --> 36:05.000]  Пожалуйста, можно между ними передавать информацию
[36:08.000 --> 36:09.000]  Все-все-все
[36:09.000 --> 36:10.000]  Я не все нарисовал
[36:11.000 --> 36:12.000]  Но и это есть
[36:16.000 --> 36:17.000]  И вот такое есть
[36:18.000 --> 36:19.000]  Я погибну все это рисовать
[36:20.000 --> 36:21.000]  Друзья, понятно, да?
[36:22.000 --> 36:24.000]  Раз, два, три, четыре, пять, шесть
[36:25.000 --> 36:28.000]  То есть, с из шести по два это двадцать, что ли? Пятнадцать
[36:29.000 --> 36:30.000]  Пятнадцать
[36:31.000 --> 36:32.000]  Ну, тут не пятнадцать получилось
[36:35.000 --> 36:37.000]  Так, а дальше происходит очень простая вещь
[36:37.000 --> 36:38.000]  Естественная
[36:39.000 --> 36:40.000]  Возникают помехи
[36:43.000 --> 36:44.000]  Ну, помехи могут возникать как?
[36:45.000 --> 36:46.000]  Они могут просто случайно возникать
[36:47.000 --> 36:49.000]  Просто из-за того, что канал связи зашумлен
[36:50.000 --> 36:51.000]  Могут целенаправленно возникать
[36:52.000 --> 36:54.000]  Есть какой-то противник, который пытается нам повредить
[36:55.000 --> 36:57.000]  Который не позволяет, значит, передавать информацию
[36:58.000 --> 36:59.000]  По каждому отдельно взятому кану
[37:01.000 --> 37:03.000]  Вот, пусть, например, буква П
[37:03.000 --> 37:05.000]  Пусть, например, буква П
[37:06.000 --> 37:10.000]  Это вероятность, с которой каждая отдельная связь сохраняется
[37:13.000 --> 37:17.000]  Вы понимаете, что такое вопрос о связанности случайного графа?
[37:19.000 --> 37:23.000]  Это в точности вопрос о том, что несмотря на помехи
[37:24.000 --> 37:26.000]  На злонамеренные действия противника
[37:27.000 --> 37:30.000]  У нас сохраняется возможность передачи информации
[37:30.000 --> 37:32.000]  Между любыми двумя компами
[37:34.000 --> 37:35.000]  Согласны?
[37:36.000 --> 37:38.000]  Ну, не напрямую, там, по какой-то цепочке
[37:39.000 --> 37:42.000]  Ну, свет идет со скоростью света, это ж хорошо
[37:45.000 --> 37:47.000]  У нас что-то сегодня сплошная физика, прям
[37:50.000 --> 37:51.000]  Вот, так что...
[37:52.000 --> 37:54.000]  Такой вот очень естественный вопрос
[37:54.000 --> 37:56.000]  При этом, смотрите, вот здесь же непонятно, что такое типичный граф
[37:57.000 --> 37:59.000]  То есть, мы же не знаем, какое П
[38:00.000 --> 38:02.000]  Может быть, например, знаете, какой вопрос поставлен?
[38:03.000 --> 38:06.000]  Насколько маленьким можно выбрать вот это П?
[38:07.000 --> 38:11.000]  Насколько маленьким можно выбрать вот это П?
[38:12.000 --> 38:17.000]  Если мы хотим, чтобы связанность сохранилась с вероятностью, например, больше, чем одна вторая
[38:18.000 --> 38:21.000]  Так, друзья, понимаете, чем меньше вероятность, тем
[38:21.000 --> 38:24.000]  меньше вероятность сохранить ребро
[38:24.000 --> 38:26.000]  То есть, тем больше вероятность его потерять
[38:26.000 --> 38:28.000]  Вот я хочу найти, например,
[38:29.000 --> 38:30.000]  самое маленькое значение П
[38:31.000 --> 38:33.000]  При котором, вероятность сохранения связанности
[38:34.000 --> 38:36.000]  все-таки больше, чем одна вторая
[38:36.000 --> 38:40.000]  То есть, чаще связанность сохраняется, нежели не сохраняется
[38:41.000 --> 38:45.000]  notified, Save
[38:45.000 --> 38:52.980]  нежели не сохраняются. Понимаете, да? Для чего раз на ИП рассматривать? А то вы мне можете
[38:52.980 --> 38:59.120]  сказать, а зачем мне тут П? Я могу одну-вторую написать. Тогда все графы равны вероятности.
[38:59.120 --> 39:08.200]  Заметьте, давайте я вот здесь напишу вероятность конкретного графа по аналогии вот с этой ситуации,
[39:08.200 --> 39:16.040]  которую мы как пример рассмотрели. Это P в степени мощность E на Q в степени C из N по 2 минус
[39:16.040 --> 39:21.800]  мощность E. Это понятно. И вероятность любого события, как я уже говорил, это просто сумма
[39:21.800 --> 39:30.120]  вероятностей конкретных графов, которые этому событию благоприятствуют. А вы мне скажете,
[39:30.120 --> 39:37.040]  P равно Q равно 1 и 2, тогда это просто 1 поделить на 2 в степени C из N по 2. Классическое такое
[39:37.040 --> 39:44.440]  вероятностное пространство. Но нет, потому что, видите, вот в этой задаче не обязательно P равно
[39:44.440 --> 39:50.480]  1 и 2. Нам, может быть, хочется действительно выбрать очень маленьким это P, то есть разрешить
[39:50.480 --> 39:56.440]  гаду противнику уничтожать ребра с вероятностью близко к единице, и вдруг, тем не менее, получится,
[39:56.440 --> 40:04.080]  что граф останется связанным с вероятностью больше, чем 1 и 2. Понимаете, да? Но я вам точную
[40:04.080 --> 40:10.760]  формулировку скоро дам. Перерыв 5 минут. Давайте рассаживайтесь, пожалуйста, и затихайте по
[40:10.760 --> 40:18.880]  возможности. Я, конечно, голос, как вы видите, не особо берегу, потому что мне жалко как-то,
[40:18.880 --> 40:27.760]  но я ж люблю читать эмоционально, но как вот я буду шептать, что ли? Можно завести микрофон и
[40:27.760 --> 40:38.200]  шептать в микрофон. Вы смеетесь, а я однажды это делал. Ладно, я вам расскажу этот исторический
[40:38.200 --> 40:44.280]  анекдот. У меня на самом деле вот эта проблема с голосом, она вечная, она уже много лет существует,
[40:44.280 --> 40:50.200]  я регулярно теряю его и теряю вот так где-то на месяц, на два, никак не могу восстановить,
[40:50.200 --> 40:56.560]  потому что читаю очень много лекций. В 2011 году, как некоторые, наверное, из вас знают,
[40:56.560 --> 41:02.520]  мне случилось получить премию президента. Ну, такая большая премия, меня в Кремль там возили,
[41:02.520 --> 41:11.360]  награждали и так далее, но я не знаю, вы в курсе, не в курсе, это было в феврале 2012 года. За 2011 год
[41:11.360 --> 41:18.000]  была премия. В общем, тогда Медведев был президентом, в общем, с Медведевым я еще
[41:18.000 --> 41:24.680]  повстречался, там я еще был в голосе, на следующий день я потерял голос. Хрен его знает почему,
[41:25.040 --> 41:30.200]  может как простуда была, неважно, на следующий день я его потерял, в чем потерял вот не как сейчас,
[41:30.200 --> 41:42.400]  а я просто открываю щуку рот, не слышно что пойдет, вообще ничего не слышно. А на фистехе, конечно,
[41:42.400 --> 41:49.200]  очень обрадовались, что я получил эту премию и в субботу, а это был голос, я потерял в четверг,
[41:49.200 --> 41:55.320]  в субботу было общее собрание профессорско преподавательского состава, на котором я
[41:55.320 --> 42:00.920]  должен был выступить с докладом о том, за что я получил эту премию, ну чтобы как-то так вот
[42:00.920 --> 42:08.240]  попиарить меня, как человек, который вот на фистехе получил такую премию. Но все, голоса нет,
[42:08.240 --> 42:13.480]  большой концертный зал, знаете наш концертный зал, но он чуть-чуть другой был, ну неважно,
[42:13.480 --> 42:20.520]  то же самое помещение, это его отремонтировали сейчас, но размер примерно такой же. Все,
[42:20.520 --> 42:24.960]  я мог только шептать, мне дали микрофон, я вот так его прям поднес совсем к губам,
[42:24.960 --> 42:42.760]  ну и там что-то спасибо и так далее. Бывает, так что это не впервой, вы не думаете. Так,
[42:42.760 --> 42:49.800]  давайте чтобы вам как-то было, ну поинтереснее, я сейчас сформулирую теорему,
[42:49.800 --> 43:00.000]  которую частично докажу в конечном счете, частично не докажу, вот, но, наверное,
[43:00.000 --> 43:14.640]  начну доказывать не сегодня. Сейчас вы поймете почему. Собственно, вот это вот один из основополагающих
[43:14.640 --> 43:22.360]  результатов, который был получен в упомянутые годы, 59-й, 61-й, это один из многих важных результатов,
[43:22.360 --> 43:34.160]  которые они получили. Значит, пусть вероятность ребра случайного графа, кстати знаете,
[43:34.160 --> 43:43.120]  что я забыл, я забыл написать, что эта модель еще вот так вот обозначается. Это модель классическая,
[43:43.120 --> 43:48.120]  она же биномиальная. Так, ну все поняли, почему она биномиальная, да, схема Бернули, это бином
[43:48.120 --> 43:58.480]  Ньютона, конечно. Вот, значит, она еще обозначается GATNP. Значит, пусть P сейчас чуть-чуть кокнет,
[43:58.480 --> 44:06.080]  и я буду стараться это потом исправить, то есть пояснять на примерах, почему так важно, нужно и
[44:06.080 --> 44:14.120]  так далее. А потом докажу, и вы поймете, откуда это берется. Вот я подчеркну, что это не константа,
[44:14.120 --> 44:22.200]  а это пункция, которая изменяется с изменением числа вершин. А именно так изменяется. Вот кокнет,
[44:22.200 --> 44:35.720]  когда я напишу, как она изменяется. C умножить на логариф M и поделить на M. Ну, может быть,
[44:35.720 --> 44:40.840]  кто-то когда-то слышал, я это и школьникам люблю рассказывать, но не знаю, слышал кто-то из
[44:40.840 --> 44:46.640]  присутствующих от меня это раньше или нет, неважно, вот пусть P ведет себя таким загадочным образом,
[44:46.640 --> 44:52.600]  почему-то. Да, точку я зря поставил, но давайте я подчеркну, что C это просто константа,
[44:52.600 --> 44:59.440]  которая больше нуля. Вот какая-то такая странная функция, но единственное, что про нее понятно,
[44:59.440 --> 45:10.560]  это что она при любом C стремится к нулю. Ну, это очевидно. Тогда, сейчас будет опять физика,
[45:10.560 --> 45:27.640]  шучу. Если C больше единицы, то вероятность, с которой случайный граф, вот я так напишу,
[45:27.640 --> 45:39.480]  воспользуюсь обозначением, вероятность, с которой случайный граф связан, стремится к единице при
[45:39.480 --> 45:53.040]  N, стремящемся к бесконечности. Второе, если T меньше единицы, то та же самая вероятность
[45:53.040 --> 46:12.560]  стремится к нулю при N, стремящемся к бесконечности. Что? А, если равна, это я позже скажу. Вот эту теорему
[46:12.560 --> 46:18.320]  мы целиком и полностью докажем в конечном счете. Если равна, это правильный вопрос. Да,
[46:18.320 --> 46:25.320]  что если C равно единице? Давайте я чуть-чуть отложу ответ на этот вопрос, хорошо? Но я на него
[46:25.320 --> 46:32.600]  отвечу сегодня. На этот вопрос я сегодня отвечу. Что будет, если C равно единице? Давайте сначала
[46:32.600 --> 46:43.320]  поймем пафос этого результата. Ну, во-первых, имеет место такой качественный скачок. Смотрите,
[46:43.320 --> 46:53.160]  здесь вероятность стремится к единице, а здесь она уже стремится к нулю. Но согласитесь, что если
[46:53.160 --> 47:00.280]  говорить на банальном таком общечеловеческом языке, то в каком-то смысле тут, в этой ситуации,
[47:00.280 --> 47:11.200]  почти любой граф связан, а тут почти любой не связан. Но, то есть, вот эта ситуация, которая даже
[47:11.200 --> 47:18.320]  лучше, чем то, о чем я говорил здесь, помните? С вероятностью больше, чем одна вторая сохраняется
[47:18.320 --> 47:25.400]  связанность. А здесь она не просто больше, она стремится к единице. Ну, правда, надо еще понимать,
[47:25.400 --> 47:32.440]  с какой скорость. Это отдельный как бы практический вопрос. Про него я тоже скажу. Но чисто качественно,
[47:32.440 --> 47:41.960]  понятно? Почему я сказал, что это физика? Потому что это в каком-то смысле похоже на переход,
[47:41.960 --> 47:52.000]  ну, например, от жидкого состояния к какому-нибудь там твердому, да? Вот ноль градусов, чуть-чуть влево
[47:52.000 --> 48:00.800]  свалились, в минус, и получился лед. Чуть-чуть право свалились, и получилась вода, да? Вот здесь
[48:00.800 --> 48:07.800]  такая же ситуация. Смотрите, вы чуть-чуть вправо свалились от единицы, чуть-чуть умножили,
[48:07.800 --> 48:17.240]  там на одну целую, я не знаю, одну квадриллиону, и уже почти все графы связаны. Чуть-чуть свалились
[48:17.240 --> 48:25.320]  влево, умножили на ноль, там 6 девяток после запятой, и все, почти все графы не связаны. В физике это
[48:25.320 --> 48:32.240]  называется фазовый переход. Вот в этой теории случайных графов, часть физиков, которые, как
[48:32.240 --> 48:37.720]  известно, на самом-то деле ее и придумали, такие ситуации тоже называются фазовыми переходами.
[48:37.720 --> 48:46.760]  Так, друзья, ну это не то, чтобы там обязательно записывать, но вы успеваете при желании, да? Я не
[48:46.760 --> 48:59.840]  очень быстро говорю. Так, ну чувствуете, что вообще это удивительно, да? Подождите. Сейчас я вам
[48:59.840 --> 49:07.280]  совсем практический смысл дам. Вот такой пример, один со звездочкой, не то что он сложный, а просто,
[49:07.280 --> 49:13.120]  ну я не буду это доказывать от какие-то скучные расчеты, это просто тебе запишите, это можно
[49:13.120 --> 49:25.240]  посчитать из доказательства исходя. Значит, если n больше 100, а c равняется 3, то мы находимся,
[49:25.240 --> 49:35.920]  конечно, вот в этом режиме, но больше того, вероятность того, что g от np связан,
[49:35.920 --> 49:46.880]  больше либо равна 1-1n. То есть мы можем контролировать скорость, с которой вот эта
[49:46.880 --> 49:56.360]  вероятность стремится к единице. Вот, друзья, чтобы просто вас не кокало, чтобы вы понимали,
[49:56.360 --> 50:02.160]  как устроена вот эта вот граница, вот этот момент фазового перехода, вот здесь написано
[50:02.160 --> 50:11.640]  сейчас 3 умножить на логарифм поделить на n, да? Вот представьте себе, что n это 2000, ну в России
[50:11.640 --> 50:28.240]  2000 городов, например, в каждом компьютере. Один. Так, подставляем 3 логарифм 2000 поделить на 2000.
[50:28.240 --> 50:40.200]  Ну, я вроде помню, это примерно 0011. Можете на калькуляторе проверить, это в общем невелика
[50:40.200 --> 50:48.440]  задача, вдруг ошибся. Мне почему-то кажется, что так. Это p, товарищи, это вероятность,
[50:48.440 --> 50:57.480]  с которой сохраняется отдельное ребро. А вероятность, с которой сохраняется общая связность,
[50:57.480 --> 51:11.960]  какая? 0, 9, 9, 9, 5. Ну, 1 минус 1, 2000. Уж это я могу посчитать. Друзья, понимаете, вот пафосы
[51:11.960 --> 51:18.160]  именно в этом, поэтому важно, чтобы p была функцией от n в том числе. Чем больше изначально этих
[51:18.160 --> 51:25.560]  компьютеров, тем меньшую можно взять вероятность сохранения жизни каждой отдельной связи с тем,
[51:25.560 --> 51:32.200]  чтобы вероятность того, что общая связность сохранится, была вот настолько близко к единице,
[51:32.200 --> 51:43.880]  или еще ближе. Понимаете, что это впечатляющий результат? Убедил? Так, ну теперь я отвечу на ваш
[51:43.880 --> 51:56.080]  вопрос, наконец. Отвечу я издевательски. Да ладно, ладно. Не издевательски, а с перехлёстом.
[51:56.080 --> 52:11.280]  Значит, пункт 3 с двумя звездочками в смысле, что он сложнее, я его доказывать точно не
[52:11.280 --> 52:17.240]  собираюсь, а это просто скучная выкладка, поэтому я этого тоже делать не буду, и с вас не потребую.
[52:17.240 --> 52:30.000]  Значит, 3 с двумя звездочками утверждение такое, пусть p от n равняется логарифму n плюс гамма
[52:30.000 --> 52:39.400]  поделить на n, где гамма это константа. Ну, если гамма равняется нулю, то это вот ровно то,
[52:39.440 --> 52:47.160]  что вы меня спросили. Это c равно единице вот здесь, правильно? Если гамма равняется нулю. А если
[52:47.160 --> 52:51.880]  гамма не равняется нулю, друзья аналитики, вы же понимаете, что эта ситуация не укладывается
[52:51.880 --> 53:01.880]  вообще в эту теорию? Ну, здесь c это константа, а не функция от n. То есть, асимптатически
[53:01.880 --> 53:08.000]  равно логарифм n поделить на n, но не равно в точности, если гамма не равно нулю. То есть,
[53:08.000 --> 53:16.280]  это как бы еще один случай, который в целом не укладывается сюда. Нет, но гамма может быть и
[53:16.280 --> 53:23.840]  больше и меньше нуля. Совершенно верно. Вот если гамма будет стремиться к плюс бесконечности,
[53:23.840 --> 53:29.240]  то уже будет стремление к единице, а гамма стремится к минус бесконечности, будет стремление к нулю.
[53:29.240 --> 53:36.280]  Значит, смотрите, в этом случае вероятность того, что g от np связан,
[53:36.280 --> 53:48.320]  но это просто красиво. Стремится к е в степени минус е в степени минус гамма.
[53:48.320 --> 54:02.720]  Я поэтому так и приберегал, но не сразу же вам такое показать. Нет, ну вы может быть угадали,
[54:02.720 --> 54:07.400]  в случае гамма равно нулю, там 1 поделить на е ответ. Если гамма равняется нулю,
[54:07.400 --> 54:15.760]  тут будет единица, е в минус первое, это 1 поделить на е. Ну и правильно, если гамма стремится к
[54:15.760 --> 54:22.520]  плюс бесконечности, то е в степени минус гамма это ноль в пределе, е в минус нуля будет единица.
[54:22.520 --> 54:29.280]  Если гамма стремится к минус бесконечности, тогда тут получается положительная бесконечность,
[54:29.280 --> 54:37.120]  е в степени минус положительная бесконечность это ноль. То есть фазовый переход, он даже не от
[54:37.120 --> 54:43.440]  с больше единицы к с меньше единицы, а от вот этого гамма очень большого, гамма очень маленькая.
[54:43.440 --> 54:53.040]  Настолько, знаете, говорят, очень узкое окно фазового перехода. Вот эти ноль градусов, когда замерзает,
[54:53.040 --> 55:09.600]  а если гамма равно нулю, это е в минус первое, 1 поделить на е. Ну да, в каком-то смысле. Похоже
[55:09.600 --> 55:16.200]  на вероятность беспорядка. Считали вероятность беспорядка, да, ведь? А, мы со мной считали,
[55:16.200 --> 55:26.120]  ну отлично, да. Похоже, да. Да, и последнее, что надо сказать здесь, это что когда вероятность
[55:26.120 --> 55:32.040]  какой-то последовательности событий стремится к единице, говорят, что эта последовательность
[55:32.040 --> 55:42.960]  событий выполнена, внимание, асимпатически почти, наверное. Пишут вот так.
[55:42.960 --> 55:57.720]  Асимпатически почти, наверное. По-английски тоже вот так пишут. Асимптотик Леолмас Чурли.
[55:57.720 --> 56:05.000]  Ну иногда еще пишут по-английски вот так. With high probability.
[56:05.000 --> 56:17.320]  VHP, да, with high probability. Я же вроде расшифровал. Асимптотик Леолмас Чурли.
[56:17.320 --> 56:29.600]  Так, для того, чтобы доказать теорему, а сделаем мы это уже в следующий раз,
[56:29.600 --> 56:40.760]  докажем теорему. Нам понадобятся такие инструменты, как неравенство Маркова и Чебышова. Знаете ли вы их? Вряд ли.
[56:40.760 --> 56:50.800]  Наверно, потому что у нас какое сегодня число? 21, да. Просто не могли успеть наши вероятности
[56:50.800 --> 56:59.000]  это рассказать. Хорошо. А знаете ли вы, что такое мат ожидания случайной величины? Напомнить, да?
[56:59.000 --> 57:07.560]  Ну давайте так. Вам же дают вероятность в относительно общем ключе, но дискретном пока что,
[57:07.560 --> 57:15.720]  но тем не менее там бывают, наверное, и бесконечные пространства, правда же? Бывают, да? А у меня их
[57:15.720 --> 57:23.200]  не бывает. Значит, друзья, как я уже говорил, я хочу немножко задублировать материал курса теории
[57:23.200 --> 57:29.280]  вероятностей. Это делаю сознательно. То есть они вам дадут очень мощную системную подготовку по этому
[57:29.280 --> 57:36.960]  вопросу, а я вам дам некоторую интуицию, которая и там будет полезна и будет совершенно строгой в
[57:36.960 --> 57:45.840]  случае конечных вероятностных пространств. У нас же все конечное. Помните, да? Графов же конечное число.
[57:45.840 --> 57:56.600]  Так, хорошо. Ну давайте я еще лучше спрошу, а что такое случайная величина-то вы знаете? Или тоже не
[57:56.600 --> 58:12.400]  проходили еще? Понятно. Но смотрите, вот есть какое-то пространство. Вот это вы проходили. Друзья,
[58:12.400 --> 58:22.200]  все проходили? Знаете, что есть элементарные исходы, есть события и есть вероятность. Ну,
[58:22.240 --> 58:28.600]  друзья, но воспринимайте, что вот это множество графов, например, ну или какое это конечное множество,
[58:28.600 --> 58:35.520]  f для нас всегда это просто 2 в степени омега. То есть нас никакие сигма-алгебры не беспокоят.
[58:35.520 --> 58:43.160]  f это просто множество всех под множество. Его можно даже не писать в том контексте, в котором
[58:43.160 --> 58:50.040]  мы с вами будем работать. Ну а вероятность, это просто писалки отдельные. То есть вероятность
[58:50.040 --> 58:58.440]  устроена как? У нас омега состоит из каких-то элементарных исходов и вероятность просто вот
[58:58.440 --> 59:06.000]  так определяется. p от омегаитова равняется какому-то питому и, естественно, сумма этих питов равна
[59:06.000 --> 59:17.880]  единице. Все, вот вся вероятность. Так, друзья, я не очень быстро говорю, понятно? Точно? Вероятность
[59:17.880 --> 59:24.920]  задана просто значениями вероятности на элементарных исходах. Так что если у нас есть какое-то
[59:24.920 --> 59:34.280]  событие, то вероятность этого события, это сумма по всем омега, которые ему благоприятствуют,
[59:34.280 --> 59:49.320]  вероятности вот этих омег. Все. Понятно? Что такое случайная величина? Тяжело? Дала.
[59:49.320 --> 01:00:08.320]  Место кси напишу х. А я вот не очень, кси красиво, да я не очень. Ну кси вот какая-то, кси. А как интересно.
[01:00:08.320 --> 01:00:23.760]  Все, товарищи, вот это вот случайная величина. Любая функция. Знаете, когда я школьникам объясняю,
[01:00:23.760 --> 01:00:29.840]  но вы почти школьники еще, да, я обычно говорю, представьте себе, что вот это множество, это вы
[01:00:29.840 --> 01:00:38.600]  сидящие в аудитории, а я кого-то одного вызываю к доске случайным образом. Вот такой вот вероятности.
[01:00:38.600 --> 01:00:44.240]  У меня есть любимчики, есть те, кого я не очень хочу вызывать, поэтому у каждого вероятность своя.
[01:00:44.240 --> 01:01:02.000]  А дальше вызвал к доске и измеряю вес. У меня весы тут стоят. Ну согласитесь, что до того,
[01:01:02.000 --> 01:01:08.000]  как я человека вызвал к доске, это случайная величина, потому что человек вызывается
[01:01:08.000 --> 01:01:13.040]  случайным образом. Но когда он уже сюда вышел, все, ничего случайного тут не будет,
[01:01:13.040 --> 01:01:19.520]  он встанет на весы и у нас получится конкретное число. Вот случайная величина, это такой термин,
[01:01:19.520 --> 01:01:26.920]  это любая функция на множестве элементарных случайных событий. В том числе константа,
[01:01:26.920 --> 01:01:37.320]  конечно, да, вес, рост, количество треугольников в графе, чего хотите. Кого? Количество волос,
[01:01:37.320 --> 01:01:48.400]  да, прекрасно. Так, ну, друзья, я думаю, что понятно. У случайных величин есть такие
[01:01:48.400 --> 01:01:59.800]  характеристики, называются мат ожидания. Математическое ожидание. Математическое
[01:01:59.800 --> 01:02:06.000]  ожидание. Значит, у математического ожидания есть два обозначения традиционных. Одно из них
[01:02:06.000 --> 01:02:12.680]  вот здесь, и я буду использовать его, потому что наши лекторы используют его. Но есть альтернатива,
[01:02:12.680 --> 01:02:22.120]  можно писать вот так. Про это у меня тоже есть анекдот жизненный. Е, конечно, происходит от
[01:02:22.120 --> 01:02:35.600]  слова expectation, ожидание. Я, когда начинал преподавать вероятность, я думал, что М от слова мат ожидания.
[01:02:35.600 --> 01:02:42.920]  И хотя слово математика не русское, но мне казалось, что мат ожидания более, как сказать,
[01:02:42.920 --> 01:02:51.400]  патриотично, что ли, нежели expectation. Я очень привык вот это писать и долгие годы так преподавал.
[01:02:51.400 --> 01:03:00.640]  Потом мне объяснили, что вот это м, оно не от мат ожидания, а оно от, ну, вообще даже не от
[01:03:00.640 --> 01:03:08.440]  английского, а тогда еще от немецкого mittelwerte, но по-английски это mean value, среднее значение.
[01:03:08.440 --> 01:03:18.400]  Но как называется, шах и мат, да? Вот, я продолжал использовать это, потому что привык, а потом
[01:03:18.400 --> 01:03:23.960]  стал использовать это, потому что почему-то на фистехе все это используют. Ну и что же я буду вас
[01:03:23.960 --> 01:03:46.080]  переучивать, что ли? Я сам переучился. Так, определение. Базовое определение совсем простое,
[01:03:46.080 --> 01:04:00.520]  это сумма по всем омега х от омега умножить на п от омега. Ну, то есть, мы вызываем людей к доске,
[01:04:00.520 --> 01:04:08.720]  взвешиваем, а потом складываем полученные величины, виноват тоже с весами, с весами,
[01:04:08.720 --> 01:04:18.160]  которые равны вероятностям быть вызванными к доске. Так, друзья, понятно, такое взвешенное среднее.
[01:04:18.160 --> 01:04:28.560]  Ну, как бы, сколько мы ожидаем получить веса в среднем? Наверное, если у нас в любимчиках
[01:04:28.560 --> 01:04:38.440]  ходят толстяки, то в среднем мы получим больше, чем просто среднеаррифметическое, правда? Хотя,
[01:04:38.800 --> 01:04:45.320]  если вдруг вес Константин по аудитории, то что не делай, а все время получится среднеаррифметическое.
[01:04:45.320 --> 01:04:52.040]  Но так не бывает. Я думаю, что все мы весим немножко по-разному. Даже если выражать в килограммах,
[01:04:52.040 --> 01:04:58.120]  ну а уж если до граммов доходить, то, конечно. Так, друзья, ну смысл этого определения, по-моему,
[01:04:58.120 --> 01:05:07.400]  предельно ясен. Нет вопросов? Можно переписать по-другому. Смотрите, чего произошло вот тут на
[01:05:07.400 --> 01:05:14.800]  верхней строчке. Вместо всей прямой я написал просто перечень разных значений, которые принимает
[01:05:14.800 --> 01:05:22.320]  случайная величина. Их k, при том, что здесь n. Ну, k может равняться единице, правильно? Может быть
[01:05:22.320 --> 01:05:29.880]  константа. k может равняться n, если все значения разные. Ну и любому другому числу в пределах от
[01:05:29.880 --> 01:05:37.920]  единицы даем. Я надеюсь, это тоже понятно в таком темпе, да? Ну, давайте сгруппируем просто
[01:05:37.920 --> 01:05:44.480]  слагаемые по величинам вот этих икс от омега. Может быть y1, может быть y2, там и так далее.
[01:05:44.480 --> 01:05:57.480]  Чего у нас получится? У нас получится y1 умножить на сумму по всем омега, на которых х принимает
[01:05:57.480 --> 01:06:09.400]  значение y1, p от омега, плюс и так далее, плюс ykt на сумму по всем омега, на которых x от омега
[01:06:09.400 --> 01:06:20.280]  принимает значение ykt, p от омега. Понятно, да, чего я написал? Я сгруппировал, вынес за скобку.
[01:06:20.280 --> 01:06:28.160]  Начало вынес за скобку y1, но оно умножается на сумму всех вероятностей, которые соответствуют
[01:06:28.160 --> 01:06:45.560]  элементарным исходам, дающим y1. Ну и так далее. Так, друзья, я не очень спешу. Спеваем? Нет, mx это
[01:06:45.560 --> 01:06:54.920]  все. Это просто альтернативное обозначение, которое я похерил, потому что все используют сейчас вот это.
[01:06:54.920 --> 01:07:03.520]  Про mx не надо ничего. Равенство пошло вот сюда. Это вот это равенство. Я просто в этой сумме
[01:07:03.520 --> 01:07:13.160]  сгруппировал слагаемые по величине х от омега. Может быть она равна y1. Ну сколько раз она равна
[01:07:13.160 --> 01:07:23.080]  y1. Столько раз, сколько есть х от омега равных y1. Вот я и написал это. Просто складываю вероятности
[01:07:23.080 --> 01:07:30.080]  тех омег, на которых х принимает значение y1. А за скобками стоит как раз y1. Ну и так далее.
[01:07:30.080 --> 01:07:44.160]  Ну а это по-другому можно написать вот так. Это сумма по i от 1 до k yt на вероятность того,
[01:07:44.160 --> 01:07:54.560]  что x равняется yt. Так, друзья, опять темп нормальный. Понятно, что такое вероятность того,
[01:07:54.560 --> 01:08:04.400]  что x равняется yt. Это вот ровно вот эта сумма. Это просто мера множества тех омег, на которых х от
[01:08:04.400 --> 01:08:11.180]  омега равняется yt. Ну не принято писать вот эти фигурные скобки омега, двоеточия, пишут просто
[01:08:11.180 --> 01:08:23.840]  вот так, подразумевая это. Понятно? Но тоже очень естественное определение. Можно вот так взвесить
[01:08:23.840 --> 01:08:33.440]  значения, а можно перечислить разные значения и каждые умножить на свой тоже вес. Это называется
[01:08:33.440 --> 01:08:47.920]  мат ожидания случайной величины. Есть роскошное свойство математического ожидания, которое
[01:08:47.920 --> 01:08:55.640]  мгновенно следует из определения. Причем вот это. Свойство такое, оно называется линейность.
[01:08:55.640 --> 01:09:12.720]  Мат ожидания любой линейной комбинации двух случайных величин, это линейная комбинация мат
[01:09:12.720 --> 01:09:30.200]  ожидания. Так, доказательство оно мгновенно следует из определения. Можно я его писать не буду?
[01:09:30.200 --> 01:09:42.160]  Или писать? Писать. Кокнул что ли? Друзья, ну не должен был, ну не может быть, чтобы я в таком
[01:09:42.160 --> 01:09:50.800]  темпе был невоспринимаем. Понятно, что происходит, нет? Можно я не буду раскрывать скобки тут?
[01:09:50.800 --> 01:09:59.200]  Это просто тривиальное следствие того определения, прям мгновенно. Почему это мега свойство? Вот смотрите,
[01:09:59.200 --> 01:10:15.160]  какое замечательное наблюдение. Пусть х это число треугольников. Помните, мы сегодня считали
[01:10:15.160 --> 01:10:23.880]  треугольник. Х это число треугольников, но в графе g. То есть мы находимся в пространстве g от NP,
[01:10:23.880 --> 01:10:31.560]  мы работаем со случайными графами, и нас интересует количество треугольников. Так, дорогие друзья,
[01:10:31.560 --> 01:10:43.240]  если граф на n вершинах, сколько может быть в графе треугольников? Может быть 0, да? Самый
[01:10:43.240 --> 01:10:52.120]  максимум это c из NP. Все понимают, да? То есть вот это x от g, оно принимает вот такие значения 0,
[01:10:52.120 --> 01:11:04.440]  1, 2, c из NP. Это вот y1 и так далее и yk. Для такой случайной величины k равняется c из NP
[01:11:04.440 --> 01:11:15.160]  плюс a1. Такой вот простой факт. Вопрос, как посчитать математическое ожидание такой случайной величины x?
[01:11:15.160 --> 01:11:31.640]  Не, ну можно, конечно, взять вот это определение, да? Сейчас я его возьму. Я уж поиздеваюсь. Нет,
[01:11:31.640 --> 01:11:47.400]  не хотите. А я хочу. Это полезно. Сейчас. Во-первых, понимаете, да, что именно так получится? Вот здесь
[01:11:47.400 --> 01:11:56.440]  написано какие-то y и t, но у меня y и t это просто y или k. y и t это просто последовательные натуральные
[01:11:56.440 --> 01:12:08.560]  числа. Поэтому я их могу обозначить k. И тут будет тоже k. Все успевают? Почему это полезно? Друзья,
[01:12:08.560 --> 01:12:16.360]  попробуйте дома. Настоятельно рекомендую, прям вот очень рекомендую посчитать вероятность того,
[01:12:16.360 --> 01:12:26.920]  что x равняется нулю. Ну или там вы мне скажете, ладно, k равно нулю, такой вероятности тут нет.
[01:12:26.920 --> 01:12:40.540]  Ну посчитайте вероятность того, что x равно и t. Как? Не, это не унициклический граф. Это граф без
[01:12:40.540 --> 01:12:49.180]  треугольников. Вот это граф без треугольников. Это с ровно одним треугольником. Ну ровно один
[01:12:49.180 --> 01:12:54.780]  треугольник, это во-первых, он не обязан быть связан, он унициклический связан. А главное,
[01:12:54.780 --> 01:13:02.140]  что треугольник же, а не непроизвольный цикл, да? В общем, это другое. И, ладно бы, количество,
[01:13:02.140 --> 01:13:08.780]  если вы, а, ну вы не понимаете, я же, конечно, беру вот такое пространство и просто количество.
[01:13:08.780 --> 01:13:14.900]  У каждого графа с одним треугольником вообще-то своя вероятность в зависимости от того,
[01:13:14.900 --> 01:13:21.940]  сколько в нем рёдер. Друзья, я понятно говорю, нет? Может быть, граф, у которого просто один
[01:13:21.940 --> 01:13:27.780]  треугольник, а дальше изолированные вершины. А может быть, граф, у которого один треугольник,
[01:13:27.780 --> 01:13:39.860]  из него растут тэпи какие-то. Тэпи растут из треугольника. Три штуки. В общем, очень прошу,
[01:13:39.860 --> 01:13:48.660]  попробуйте это посчитать. Офигеете? Ну, серьёзно, попробуйте, хотя бы там для n равного пяти или шести.
[01:13:48.660 --> 01:14:01.620]  Прогать придётся, но вы попробуйте. Так, всё, то есть этот путь покнул нас. Ну, потому что мы
[01:14:01.620 --> 01:14:09.300]  даже отдельные вероятности не можем посчитать, как ещё их складывать потом. Давайте линейность применим.
[01:14:18.660 --> 01:14:28.900]  Да, и ещё вот глядя сюда, вы только поймите, что здесь стоит на самом деле формула включения
[01:14:28.900 --> 01:14:37.580]  исключений где-то в глубине. Поэтому считать реально тяжело. Ну, попробуйте. Так, значит,
[01:14:37.580 --> 01:14:47.780]  чего надо сделать? Надо наступить на горло своей алгоритмической песни. Вот вы здесь люди,
[01:14:47.780 --> 01:14:57.500]  которые любят программировать. Ну, большинство из вас, да? Не, ну, по крайней мере вы плохо,
[01:14:57.500 --> 01:15:04.540]  наверное, отнесётесь к человеку, который вам предложит считать треугольники в графе следующим
[01:15:04.540 --> 01:15:12.420]  образом. Вот вам поступает на вход граф, наверное, на вершинах, а человек вам предлагает делать
[01:15:12.420 --> 01:15:24.140]  следующее. Запускаем тройной цикл такой по и по ж и по к. И для каждой троечки и ж к смотрим,
[01:15:24.140 --> 01:15:31.140]  вот в поступившем графе, это троечка вершин, образует треугольник или нет. Согласитесь, да,
[01:15:31.140 --> 01:15:37.660]  классный алгоритм. Ну, я об этом, да, то есть я же сказал, надо наступить на горло своей
[01:15:37.660 --> 01:15:45.660]  алгоритмической песни. То есть люди, которые хотят оптимизировать время, от такого должны
[01:15:45.660 --> 01:15:58.460]  охренеть. Ну, вот посортировать мы не будем. Да, да, да, да, все тройки ребер, нет, я все тройки
[01:15:58.460 --> 01:16:04.300]  вершин. Короче, друзья, я же не просто так издеваюсь, чтобы вы там сейчас попридумывали
[01:16:04.300 --> 01:16:15.060]  какие-то более быстрые алгоритмы. Конечно, с алгоритмической точки зрения задача подсчета
[01:16:15.060 --> 01:16:21.100]  числа треугольников это интересная задача. Она до конца не решена, там есть масса вопросов. Но
[01:16:21.100 --> 01:16:35.540]  я все-таки подойду к задаче именно так. Значит, что такое х и т? х и т от g это единица. Если
[01:16:35.540 --> 01:16:56.300]  и т тройка вершин так образует треугольник в g и 0 иначе. Так, друзья, понимаете, что х и
[01:16:56.300 --> 01:17:02.820]  т это тоже случайная величина. Но она смешная, конечно, она принимает всего два значения, 1 и 0,
[01:17:02.820 --> 01:17:11.260]  но это случайная величина. На каждом графе она равна либо единице, либо нулю. Мы как-то
[01:17:11.260 --> 01:17:19.260]  нумируем все тройки вершин от единицы до c и z, как угодно. И дальше каждую очередную тройку,
[01:17:19.260 --> 01:17:25.100]  как я обещал в цикле в тройном, тестируем на то, что она образовала треугольник, поступившим
[01:17:25.100 --> 01:17:35.900]  на вход графе. Так, друзья, это ровно тот самый тупой алгоритм. Я понятно выразился? Теперь
[01:17:35.900 --> 01:17:47.140]  смотрите. Мат ожиданий х всегда равно сумме мат ожиданий. Вы мне скажете, а эти величины зависимы?
[01:17:47.140 --> 01:17:56.500]  А вы мне скажете, это знаете, что такое зависимо-независимо? Ну, понимаете, если вот такой треугольник
[01:17:56.500 --> 01:18:05.300]  есть в графе, то, наверное, вот такой треугольник есть уже с большей вероятностью, правда? То есть
[01:18:05.300 --> 01:18:12.420]  вот эти величины, вот эти индикаторы, их называют индикаторами, они зависимы. Но линейность верна всегда,
[01:18:12.420 --> 01:18:19.260]  мы тут ничего не говорили про независимость. Всегда верна. Плевать, что она независима. Да,
[01:18:19.260 --> 01:18:38.740]  независимая, но плевать. Это равно ех1, плюс ех3. Следили? Осталось чуть, сейчас будет звонок.
[01:18:38.740 --> 01:18:46.980]  Ему равняется мат ожиданий индикатора. Вот тут полезен второй вариант определения. Мат ожидания.
[01:18:46.980 --> 01:19:00.820]  У индикатора всего два значения. 1 и 0. То есть вот это ех1. Успеваете? А вот это ех2. То есть мат
[01:19:00.820 --> 01:19:09.500]  ожиданий ех2 это единица умножить на вероятность того, что ех2 равно единице. Но с какой вероятностью
[01:19:09.500 --> 01:19:17.180]  ех2 равно единице? С какой вероятностью конкретная тройка точек образует треугольник в случайном
[01:19:17.180 --> 01:19:27.100]  графе? П в кубе, правильно. То есть мы получаем просто C из N по 3 на P в кубе и вся недолга. Вот,
[01:19:27.100 --> 01:19:36.700]  посчитайте дома вот эту вероятность. Поймите, что это катастрофа, а тут мы в одну строчку
[01:19:36.700 --> 01:19:43.020]  и нашли мат ожидания. Вот потренируйтесь, следующий раз продолжим. Все.

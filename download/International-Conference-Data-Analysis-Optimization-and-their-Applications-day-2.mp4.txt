[00:00.000 --> 00:11.760]  Я считаю, что мы должны были работать с нормализованными данными.
[00:11.760 --> 00:16.400]  В действительности, это не обязательно, потому что мы можем работать также с
[00:17.680 --> 00:22.880]  начальством данных, в результате нормализации, но с нормализацией иногда это более
[00:23.600 --> 00:31.520]  комфортно, и это один из способов, чтобы участвовать в нормализации.
[00:32.160 --> 00:36.640]  И модель, которую я использовал, которую я представил, это был
[00:39.600 --> 00:46.320]  модель, в котором вместо использования этих первоначальных данных, даже в нормализованной форме,
[00:46.880 --> 00:56.320]  мы смещаем их к векторам деменциала,
[00:58.800 --> 01:02.400]  меньше чем один, в сравнении с деменциалом
[01:02.400 --> 01:08.560]  первоначальных данных, мы смещаем их к
[01:11.280 --> 01:18.960]  векторам деменциала, к углу каждой части
[01:21.200 --> 01:22.480]  деменциала.
[01:23.440 --> 01:29.600]  И потом, эта работа была использована в многих сетях,
[01:31.360 --> 01:34.080]  мы имеем несколько публикаций об этом,
[01:35.040 --> 01:43.440]  включая одну публикацию с Борисом в журнале в России, но потом мы
[01:44.160 --> 01:49.120]  конфронтировались с следующим проблемом, который был действительно очень серьезным
[01:50.240 --> 01:55.680]  ситуацией, который был первый раз, когда я работал с большими данными,
[01:56.400 --> 02:03.040]  когда нужно было делать это с миллионами сетей этого рода,
[02:03.040 --> 02:10.400]  которые были, которые презентировали,
[02:11.200 --> 02:20.000]  этот курс презентировал консумерное поведение очень большой ретейл
[02:20.800 --> 02:28.640]  сетей. Я не могу сказать об этом открыто, потому что я подписал контракт с НДА, но
[02:29.600 --> 02:34.880]  это было в Европе, и у нас были миллионы этих курсов,
[02:37.920 --> 02:38.420]  и
[02:41.920 --> 02:47.440]  здесь написано индекс 1, индекс 2, но это, например, продукты.
[05:58.640 --> 05:59.280]  Ну,
[06:28.640 --> 06:33.640]  Редактор субтитров Е.Воинова Корректор А.Кулакова
[06:58.640 --> 07:01.640]  Корректор А.Кулакова
[07:28.640 --> 07:31.640]  Корректор А.Кулакова
[07:58.640 --> 08:01.640]  Корректор А.Кулакова
[08:28.640 --> 08:31.640]  Корректор А.Кулакова
[08:58.640 --> 09:01.640]  Корректор А.Кулакова
[09:29.640 --> 09:32.640]  Важная часть этой истории
[09:32.640 --> 09:35.640]  Важная часть этой истории
[09:35.640 --> 09:38.640]  Важная часть этой истории
[09:38.640 --> 09:41.640]  Важная часть этой истории
[09:41.640 --> 09:44.640]  Важная часть этой истории
[09:44.640 --> 09:47.640]  Важная часть этой истории
[09:47.640 --> 09:50.640]  Важная часть этой истории
[09:50.640 --> 09:53.640]  Важная часть этой истории
[09:53.640 --> 09:56.640]  Важная часть этой истории
[09:56.640 --> 09:59.640]  Важная часть этой истории
[09:59.640 --> 10:02.640]  Важная часть этой истории
[10:02.640 --> 10:05.640]  Важная часть этой истории
[10:05.640 --> 10:08.640]  Важная часть этой истории
[10:08.640 --> 10:11.640]  Важная часть этой истории
[10:11.640 --> 10:14.640]  Важная часть этой истории
[10:14.640 --> 10:17.640]  Важная часть этой истории
[10:17.640 --> 10:20.640]  Важная часть этой истории
[10:20.640 --> 10:23.640]  Важная часть этой истории
[10:23.640 --> 10:26.640]  Важная часть этой истории
[10:26.640 --> 10:29.640]  Важная часть этой истории
[10:29.640 --> 10:32.640]  Важная часть этой истории
[10:32.640 --> 10:35.640]  Важная часть этой истории
[10:35.640 --> 10:38.640]  Важная часть этой истории
[10:38.640 --> 10:41.640]  Важная часть этой истории
[10:41.640 --> 10:44.640]  Важная часть этой истории
[10:44.640 --> 10:47.640]  Важная часть этой истории
[10:47.640 --> 10:50.640]  Важная часть этой истории
[10:50.640 --> 10:53.640]  Важная часть этой истории
[10:53.640 --> 10:56.640]  Важная часть этой истории
[10:56.640 --> 10:59.640]  Важная часть этой истории
[10:59.640 --> 11:02.640]  Важная часть этой истории
[11:02.640 --> 11:05.640]  Важная часть этой истории
[11:05.640 --> 11:08.640]  Важная часть этой истории
[11:08.640 --> 11:11.640]  Важная часть этой истории
[11:11.640 --> 11:14.640]  Важная часть этой истории
[11:14.640 --> 11:17.640]  Важная часть этой истории
[11:17.640 --> 11:20.640]  Важная часть этой истории
[11:20.640 --> 11:23.640]  Важная часть этой истории
[11:23.640 --> 11:26.640]  Важная часть этой истории
[11:26.640 --> 11:29.640]  Важная часть этой истории
[11:29.640 --> 11:32.640]  Важная часть этой истории
[11:32.640 --> 11:35.640]  Важная часть этой истории
[11:35.640 --> 11:38.640]  Важная часть этой истории
[11:38.640 --> 11:41.640]  Важная часть этой истории
[11:41.640 --> 11:44.640]  Важная часть этой истории
[11:44.640 --> 11:47.640]  Важная часть этой истории
[11:47.640 --> 11:50.640]  Важная часть этой истории
[11:50.640 --> 11:53.640]  Важная часть этой истории
[11:53.640 --> 11:56.640]  Важная часть этой истории
[11:56.640 --> 11:59.640]  Важная часть этой истории
[11:59.640 --> 12:02.640]  Важная часть этой истории
[12:02.640 --> 12:05.640]  Важная часть этой истории
[12:05.640 --> 12:08.640]  Важная часть этой истории
[12:08.640 --> 12:11.640]  Важная часть этой истории
[12:11.640 --> 12:14.640]  Важная часть этой истории
[12:14.640 --> 12:17.640]  Важная часть этой истории
[12:17.640 --> 12:20.640]  Важная часть этой истории
[12:20.640 --> 12:23.640]  Важная часть этой истории
[12:23.640 --> 12:26.640]  Важная часть этой истории
[12:26.640 --> 12:29.640]  Важная часть этой истории
[12:29.640 --> 12:32.640]  Важная часть этой истории
[12:32.640 --> 12:35.640]  Важная часть этой истории
[12:35.640 --> 12:38.640]  Важная часть этой истории
[12:38.640 --> 12:41.640]  Важная часть этой истории
[12:41.640 --> 12:44.640]  Важная часть этой истории
[12:44.640 --> 12:47.640]  Важная часть этой истории
[12:47.640 --> 12:50.640]  Важная часть этой истории
[12:50.640 --> 12:53.640]  Важная часть этой истории
[12:53.640 --> 12:56.640]  Важная часть этой истории
[12:56.640 --> 12:59.640]  Важная часть этой истории
[12:59.640 --> 13:02.640]  Важная часть этой истории
[13:02.640 --> 13:05.640]  Важная часть этой истории
[13:05.640 --> 13:08.640]  Важная часть этой истории
[13:08.640 --> 13:11.640]  Важная часть этой истории
[13:11.640 --> 13:14.640]  Важная часть этой истории
[13:14.640 --> 13:17.640]  Важная часть этой истории
[13:17.640 --> 13:20.640]  Важная часть этой истории
[13:20.640 --> 13:23.640]  Важная часть этой истории
[13:23.640 --> 13:26.640]  Важная часть этой истории
[13:26.640 --> 13:29.640]  Важная часть этой истории
[13:29.640 --> 13:32.640]  Важная часть этой истории
[13:32.640 --> 13:35.640]  Важная часть этой истории
[13:35.640 --> 13:38.640]  Важная часть этой истории
[13:38.640 --> 13:41.640]  Важная часть этой истории
[13:41.640 --> 13:44.640]  Важная часть этой истории
[13:44.640 --> 13:47.640]  Важная часть этой истории
[13:47.640 --> 13:50.640]  Важная часть этой истории
[13:50.640 --> 13:53.640]  Важная часть этой истории
[13:53.640 --> 13:56.640]  Важная часть этой истории
[13:56.640 --> 13:59.640]  Важная часть этой истории
[13:59.640 --> 14:02.640]  Важная часть этой истории
[14:02.640 --> 14:05.640]  Важная часть этой истории
[14:05.640 --> 14:08.640]  Важная часть этой истории
[14:08.640 --> 14:11.640]  Важная часть этой истории
[14:11.640 --> 14:14.640]  Важная часть этой истории
[14:14.640 --> 14:17.640]  Важная часть этой истории
[14:17.640 --> 14:20.640]  Важная часть этой истории
[14:20.640 --> 14:23.640]  Важная часть этой истории
[14:23.640 --> 14:26.640]  Важная часть этой истории
[14:26.640 --> 14:29.640]  Важная часть этой истории
[14:29.640 --> 14:32.640]  Важная часть этой истории
[14:32.640 --> 14:35.640]  Важная часть этой истории
[14:35.640 --> 14:38.640]  Важная часть этой истории
[14:38.640 --> 14:41.640]  Важная часть этой истории
[14:41.640 --> 14:44.640]  Важная часть этой истории
[14:44.640 --> 14:47.640]  Важная часть этой истории
[14:47.640 --> 14:50.640]  Важная часть этой истории
[14:50.640 --> 14:53.640]  Важная часть этой истории
[14:53.640 --> 14:56.640]  Важная часть этой истории
[14:56.640 --> 14:59.640]  Важная часть этой истории
[14:59.640 --> 15:02.640]  Важная часть этой истории
[15:02.640 --> 15:05.640]  Важная часть этой истории
[15:05.640 --> 15:08.640]  Важная часть этой истории
[15:08.640 --> 15:11.640]  Важная часть этой истории
[15:11.640 --> 15:14.640]  Важная часть этой истории
[15:14.640 --> 15:17.640]  Важная часть этой истории
[15:17.640 --> 15:20.640]  Важная часть этой истории
[15:20.640 --> 15:23.640]  Важная часть этой истории
[15:23.640 --> 15:26.640]  Важная часть этой истории
[15:26.640 --> 15:29.640]  Важная часть этой истории
[15:29.640 --> 15:32.640]  Важная часть этой истории
[15:32.640 --> 15:35.640]  Важная часть этой истории
[15:35.640 --> 15:38.640]  Важная часть этой истории
[15:38.640 --> 15:41.640]  Важная часть этой истории
[15:41.640 --> 15:44.640]  Важная часть этой истории
[15:44.640 --> 15:47.640]  Важная часть этой истории
[15:47.640 --> 15:50.640]  Важная часть этой истории
[15:50.640 --> 15:53.640]  Важная часть этой истории
[15:53.640 --> 15:56.640]  Важная часть этой истории
[15:56.640 --> 15:59.640]  Важная часть этой истории
[15:59.640 --> 16:02.640]  Важная часть этой истории
[16:02.640 --> 16:05.640]  Важная часть этой истории
[16:05.640 --> 16:08.640]  Важная часть этой истории
[16:08.640 --> 16:11.640]  Важная часть этой истории
[16:11.640 --> 16:14.640]  Важная часть этой истории
[16:14.640 --> 16:17.640]  Важная часть этой истории
[16:17.640 --> 16:20.640]  Важная часть этой истории
[16:20.640 --> 16:23.640]  Важная часть этой истории
[16:23.640 --> 16:26.640]  Важная часть этой истории
[16:26.640 --> 16:29.640]  Важная часть этой истории
[16:29.640 --> 16:32.640]  Важная часть этой истории
[16:32.640 --> 16:35.640]  Важная часть этой истории
[16:35.640 --> 16:38.640]  Важная часть этой истории
[16:38.640 --> 16:41.640]  Важная часть этой истории
[16:41.640 --> 16:44.640]  Важная часть этой истории
[16:44.640 --> 16:47.640]  Важная часть этой истории
[16:47.640 --> 16:50.640]  Важная часть этой истории
[16:50.640 --> 16:53.640]  Важная часть этой истории
[16:53.640 --> 16:56.640]  Важная часть этой истории
[16:56.640 --> 16:59.640]  Важная часть этой истории
[16:59.640 --> 17:02.640]  Важная часть этой истории
[17:02.640 --> 17:05.640]  Важная часть этой истории
[17:05.640 --> 17:08.640]  Важная часть этой истории
[17:08.640 --> 17:11.640]  Важная часть этой истории
[17:11.640 --> 17:14.640]  Важная часть этой истории
[17:14.640 --> 17:17.640]  Важная часть этой истории
[17:17.640 --> 17:20.640]  Важная часть этой истории
[17:20.640 --> 17:23.640]  Важная часть этой истории
[17:23.640 --> 17:26.640]  Важная часть этой истории
[17:26.640 --> 17:29.640]  Важная часть этой истории
[17:29.640 --> 17:32.640]  Важная часть этой истории
[17:32.640 --> 17:35.640]  Важная часть этой истории
[17:35.640 --> 17:38.640]  Важная часть этой истории
[17:38.640 --> 17:41.640]  Важная часть этой истории
[17:41.640 --> 17:44.640]  Важная часть этой истории
[17:44.640 --> 17:47.640]  Важная часть этой истории
[17:47.640 --> 17:50.640]  Важная часть этой истории
[17:50.640 --> 17:53.640]  Важная часть этой истории
[17:53.640 --> 17:56.640]  Важная часть этой истории
[17:56.640 --> 17:59.640]  Важная часть этой истории
[17:59.640 --> 18:02.640]  Важная часть этой истории
[18:02.640 --> 18:05.640]  Важная часть этой истории
[18:05.640 --> 18:08.640]  Важная часть этой истории
[18:08.640 --> 18:11.640]  Важная часть этой истории
[18:11.640 --> 18:14.640]  Важная часть этой истории
[18:14.640 --> 18:17.640]  Важная часть этой истории
[18:17.640 --> 18:20.640]  Важная часть этой истории
[18:20.640 --> 18:23.640]  Важная часть этой истории
[18:23.640 --> 18:26.640]  Важная часть этой истории
[18:26.640 --> 18:29.640]  Важная часть этой истории
[18:29.640 --> 18:32.640]  Важная часть этой истории
[18:32.640 --> 18:35.640]  Важная часть этой истории
[18:35.640 --> 18:38.640]  Важная часть этой истории
[18:38.640 --> 18:41.640]  Важная часть этой истории
[18:41.640 --> 18:44.640]  Важная часть этой истории
[18:44.640 --> 18:47.640]  Важная часть этой истории
[18:47.640 --> 18:50.640]  Важная часть этой истории
[18:50.640 --> 18:53.640]  Важная часть этой истории
[18:53.640 --> 18:56.640]  Важная часть этой истории
[18:56.640 --> 18:59.640]  Важная часть этой истории
[18:59.640 --> 19:02.640]  Важная часть этой истории
[19:02.640 --> 19:05.640]  Важная часть этой истории
[19:05.640 --> 19:08.640]  Важная часть этой истории
[19:08.640 --> 19:11.640]  Важная часть этой истории
[19:11.640 --> 19:14.640]  Важная часть этой истории
[19:14.640 --> 19:17.640]  Важная часть этой истории
[19:17.640 --> 19:20.640]  Важная часть этой истории
[19:20.640 --> 19:23.640]  Важная часть этой истории
[19:24.640 --> 19:27.640]  Благодарю про Brisbane прощения
[19:27.640 --> 19:29.640]  Д般 scientifically
[19:29.640 --> 19:32.640]  Д般 Physically
[19:32.640 --> 19:35.640]  dismissed
[19:35.640 --> 19:38.640]  Вы27
[19:38.640 --> 19:41.640]  И you
[19:46.640 --> 19:49.640]  У рожденияSe sec
[19:49.640 --> 19:52.240]  Дского родного
[19:52.240 --> 19:58.240]  Борис Григорьевич, как известно, имеет разнообразные научные интересы,
[19:58.240 --> 20:02.240]  но, так сказать, на мой взгляд, может я ошибаюсь, он меня поправит,
[20:02.240 --> 20:10.240]  что, конечно, основные интересы сосредоточены вокруг кластеризации,
[20:10.240 --> 20:15.240]  поэтому я буду говорить сейчас исключительно о кластеризации.
[20:15.240 --> 20:22.240]  И здесь, конечно, наверное, трудно чем-то удивить новым Борис Григорьевича,
[20:22.240 --> 20:30.240]  поскольку я обычно своим студентам говорю, что когда возникает какой-то вопрос
[20:30.240 --> 20:34.240]  ложный, связанный с кластеризацией, я говорю, у вас есть уникальная возможность
[20:34.240 --> 20:39.240]  получить консультацию с первого хуста человека, который знает про кластеры все,
[20:39.240 --> 20:45.240]  и направляю их к вам. Я не знаю, доходит ли они до вас, но я так делаю.
[20:45.240 --> 20:48.240]  Нет, не доходит.
[20:48.240 --> 20:55.240]  Да, ну вот так. Поэтому удивить как бы нечем тут в этом плане, я так думаю,
[20:55.240 --> 21:01.240]  но можно удивить именно объектами кластеризации, что кластеризовать.
[21:01.240 --> 21:08.240]  В этом смысле я хочу сейчас как раз рассказать о кластеризации таких объектов,
[21:08.240 --> 21:14.240]  которые называются телами свидетельств. В рамках теории Демслера-Шефера
[21:14.240 --> 21:18.240]  рассматриваются такие объекты, которые называются телами свидетельств.
[21:18.240 --> 21:26.240]  Они имеют такую сложную структуру. Во-первых, это такие пары,
[21:26.240 --> 21:33.240]  состоящие из наборов подможеств некоторого множества и связанных с ними
[21:33.240 --> 21:41.240]  функциями массы. Сами подможества характеризуют степень уверенности,
[21:41.240 --> 21:47.240]  то множество, которому может принадлежать истинная альтернатива,
[21:47.240 --> 21:53.240]  а массы, частоты характеризуют степень уверенности, что истинная альтернатива
[21:53.240 --> 21:59.240]  принадлежит тому или иному множеству. Вот эти тела свидетельств могут иметь
[21:59.240 --> 22:03.240]  довольно сложную структуру. Да, сами вот эти подможества называют
[22:03.240 --> 22:07.240]  фокальными элементами. Они могут иметь довольно сложную структуру, могут пересекаться,
[22:07.240 --> 22:11.240]  могут вкладываться, могут не пересекаться и так далее и так далее.
[22:11.240 --> 22:18.240]  И таких фокальных элементов может быть довольно много в реальных телах свидетельств.
[22:18.240 --> 22:26.240]  Поэтому требуется их кластеризовать для того, чтобы решать, во-первых, задачу
[22:26.240 --> 22:31.240]  аппроксимации, когда мы заменяем тело свидетельств из множества.
[22:31.240 --> 22:38.240]  Заменим более простым тему свидетельств, состоящих из нескольких фокальных элементов,
[22:38.240 --> 22:47.240]  но тем не менее, чтобы какая-то мера близости между этими объектами выполнялась.
[22:47.240 --> 22:52.240]  Во-вторых, вычислительно-осложно.
[22:53.240 --> 22:59.240]  Основные вычисления там связанные. Они носят экспоненциальный характер.
[22:59.240 --> 23:05.240]  Поэтому чем меньше тел свидетельств, тем вычислительная сложность этих алгоритмов
[23:05.240 --> 23:11.240]  связанных с обработкой таких объектов будет выше.
[23:11.240 --> 23:17.240]  Это что касается мотивации исследований.
[23:18.240 --> 23:23.240]  Пару слов скажу о самой этой теории Демпстера-Шефера, в рамках которой
[23:23.240 --> 23:27.240]  рассматриваются эти объекты тела свидетельств.
[23:27.240 --> 23:32.240]  Это довольно старая теория. Берет свое начало от двух таких работ,
[23:32.240 --> 23:38.240]  пионерских Демпстера 1967 года и Шафера 1976 года.
[23:38.240 --> 23:46.240]  Демпстер – это чистый статистик из Гарварда, сейчас очень приклонного возраста.
[23:46.240 --> 23:52.240]  Недавно я даже видел его работу совсем свежую.
[23:52.240 --> 23:58.240]  Шафер помоложе, значительно моложе и имеет такие разнообразные интересы.
[23:58.240 --> 24:04.240]  В это время он был совсем молодым человеком, когда написал эту монографию.
[24:04.240 --> 24:10.240]  Он как раз занимался развитием этой теории.
[24:10.240 --> 24:15.240]  Эта монография, кстати, не потеряла актуальность даже в настоящее время.
[24:15.240 --> 24:20.240]  Основные тут моменты такие, что рассматривается некоторое множество,
[24:20.240 --> 24:25.240]  универсальное множество, как обычно Х, и на нем рассматривается какое-то
[24:25.240 --> 24:33.240]  конечное подможество этого множества. Называют эти подможества фокальными элементами.
[24:33.240 --> 24:37.240]  Так будем обозначать.
[24:37.240 --> 24:40.240]  С этими подможествами связана функция масс.
[24:40.240 --> 24:44.240]  Функция масс – это не отрицательная функция, удовлетворяющая условия нормировки,
[24:44.240 --> 24:47.240]  что сумма всех масс равна единице.
[24:47.240 --> 24:54.240]  Вот такая как раз пара из множества фокальных элементов и функций масс называется телом свидетельств.
[24:54.240 --> 25:00.240]  Считается, что для каждого фокального элемента масса не нулевая.
[25:00.240 --> 25:05.240]  Простейшим телом свидетельств является категоричная телосвидетельств,
[25:05.240 --> 25:10.240]  состоящая всего из одного фокального элемента с единичной массой.
[25:10.240 --> 25:17.240]  Любое телосвидетельство можно представить в виде выпуклой комбинации категоричных тел свидетельств
[25:17.240 --> 25:22.240]  с коэффициентами равными массам.
[25:22.240 --> 25:25.240]  Чуть посложнее, чем категоричная телосвидетельств.
[25:25.240 --> 25:29.240]  Это такое свидетельство, называется простое.
[25:29.240 --> 25:35.240]  Когда с каким-то коэффициентом мы берем категоричное телосвидетельство,
[25:35.240 --> 25:38.240]  построенное на каком-то фокальном элементе,
[25:38.240 --> 25:43.240]  а всю остальную массу относим к всему универсальному множеству Х.
[25:43.240 --> 25:48.240]  Вот эта часть, второе слагаемое, она характеризует как бы степень нашего незнания
[25:48.240 --> 25:54.240]  о принадлежности истинной альтернативы множеству А.
[25:54.240 --> 25:57.240]  Степень нашего незнания.
[25:57.240 --> 26:03.240]  С этим телом свидетельств связывают несколько таких функций.
[26:04.240 --> 26:09.240]  Функции множеств, но самые популярные из них это функции доверия,
[26:09.240 --> 26:12.240]  функции правдоподобия, которые вот так строятся,
[26:12.240 --> 26:17.240]  и которые являются нижними и верхними оценками вероятности события того,
[26:17.240 --> 26:23.240]  что истинная альтернатива принадлежит множеству А.
[26:23.240 --> 26:31.240]  Значит, с такой с графовой точки зрения телосвидетельство можно считать
[26:31.240 --> 26:37.240]  вот гиперграфом, вчера как раз этот термин уже вспоминался на конференции.
[26:37.240 --> 26:42.240]  Можно считать, что это есть такой гиперграф, когда у нас элементы множества Х
[26:42.240 --> 26:48.240]  это вершины этого гиперграфа, а гипердуги это как раз вот эти фокальные элементы.
[26:48.240 --> 26:55.240]  Вот пример приведен, вот так можно в строчку это записать, как выпуклую комбинацию
[26:55.240 --> 27:01.240]  категоричных телосвидетельств, построенных на фокальных элементах.
[27:01.240 --> 27:07.240]  Вот здесь внизу фокальные элементы как раз перечислены нашего телосвидетельства.
[27:07.240 --> 27:13.240]  Это масса их, а это вот такое гиперграфовое представление этого телосвидетельства.
[27:13.240 --> 27:16.240]  Так вот, такие объекты надо кластеризовать. Что кластеризовать?
[27:16.240 --> 27:22.240]  Кластеризовать, конечно, в первую очередь нужно вот эти фокальные элементы,
[27:22.240 --> 27:28.240]  но с учетом их масс, конечно. Если там какой-то фокальный элемент имеет маленькую массу,
[27:28.240 --> 27:35.240]  то он может быть для нас не такой важный. Если там большая масса,
[27:35.240 --> 27:39.240]  то это более важные такие фокальные элементы, с учетом масс, конечно.
[27:39.240 --> 27:47.240]  Причем вот здесь при кластеризации часто метрические характеристики не важны.
[27:47.240 --> 27:54.240]  Не столь важны метрические характеристики. Сколько? Важны характеристики,
[27:54.240 --> 28:00.240]  связанные с понятием конфликта между фокальными элементами,
[28:00.240 --> 28:05.240]  между телами свидетельств, конфликта либо противоречия.
[28:05.240 --> 28:10.240]  Что можно считать конфликтом и не конфликтами фокальные элементы?
[28:10.240 --> 28:17.240]  Если сильно пересекаются, грубо говоря, сильно пересекаются два фокальных элемента,
[28:17.240 --> 28:23.240]  то такую пару можно назвать парой не конфликтных фокальных элементов,
[28:23.240 --> 28:29.240]  поскольку истинная альтернатива, один источник, например, принадлежит множеству A,
[28:29.240 --> 28:33.240]  второй множеству B. Эти множества довольно близкие, и в этом случае
[28:33.240 --> 28:38.240]  эти свидетельства от этих источников, они действительно воспринимаются не конфликтно.
[28:38.240 --> 28:43.240]  Если они слабенько пересекаются, то вот здесь уже есть какой-то слабый конфликт.
[28:43.240 --> 28:47.240]  Если они вообще не пересекаются, то здесь уже есть такой сильный конфликт
[28:47.240 --> 28:53.240]  между этими фокальными элементами и соответствующими свидетельствами.
[28:53.240 --> 28:59.240]  Если у нас есть тела свидетельств из множества таких пар построенных,
[28:59.240 --> 29:03.240]  множество фокальных элементов, то можно по-разному мерить конфликт,
[29:04.240 --> 29:09.240]  но вот такая белинейная форма, самая популярная для измерения конфликта
[29:09.240 --> 29:17.240]  сумма произведений масс фокальных элементов, которая берется по парам фокальных элементов,
[29:17.240 --> 29:23.240]  которые между собой не пересекаются. То есть тут учитывается только третья ситуация,
[29:23.240 --> 29:28.240]  конфликтность. Средняя не учитывается, хотя можно ее учитывать,
[29:28.240 --> 29:32.240]  вот здесь нужно просто вставить какой-то коэффициент, меру пересечения,
[29:32.240 --> 29:39.240]  типа индекса Джакара или еще что-нибудь, и тогда можно и учитывать слабую ситуацию,
[29:39.240 --> 29:47.240]  но не суть, не в этом суть. Что касается кластеризации,
[29:47.240 --> 29:58.240]  то тут есть несколько подходов, которые я сейчас коротко охарактеризую.
[29:58.240 --> 30:02.240]  Первый подход – это такая иерархическая кластеризация.
[30:02.240 --> 30:08.240]  Она была предложена лет 20 назад Дено, Тири Дено,
[30:08.240 --> 30:15.240]  и в некоторых еще работах она встречалась, даже более ранние работы есть,
[30:15.240 --> 30:23.240]  и позже развивалась. Это такая вот, как бы мы сказали,
[30:23.240 --> 30:30.240]  агломеративная иерархическая кластеризация. В этом случае строятся два тела свидетельств –
[30:30.240 --> 30:36.240]  тело внутренней кластеризации и внешней кластеризации.
[30:36.240 --> 30:44.240]  Строится так, это берутся, да, вот обозначено они F-, F+, тело внутренней кластеризации,
[30:44.240 --> 30:51.240]  внешней кластеризации. Значит, тело свидетельств внутренней кластеризации
[30:51.240 --> 31:01.240]  строится на пересечении некоторых множеств из исходного множества фокальных элементов,
[31:01.240 --> 31:05.240]  а тело свидетельств внешней кластеризации на их объединение.
[31:05.240 --> 31:08.240]  При этом и в том, и в другом случае массы суммируются.
[31:08.240 --> 31:15.240]  Массы просто суммируются. Значит, какие пары выбирать для пересечений и для объединений,
[31:15.240 --> 31:20.240]  но решается по-разному. Ну, например, можно построить,
[31:20.240 --> 31:27.240]  использовать вот такой, например, функционал – функционал неточности тела свидетельств.
[31:27.240 --> 31:33.240]  Значит, чем больше у нас тел свидетельств больших по мощности с большими массами,
[31:33.240 --> 31:40.240]  тем более неточное у нас это свидетельство.
[31:40.240 --> 31:47.240]  Так вот, можно выбирать такие пары, которые при объединении либо при пересечении
[31:47.240 --> 31:53.240]  мало меняют вот этот функционал неточности. То есть в этом случае можно ожидать,
[31:53.240 --> 31:59.240]  что какая-то мера близости у нас будет, выполняться мера близости,
[31:59.240 --> 32:03.240]  между тем, что мы получим в результате этой иерархической кластеризации
[32:03.240 --> 32:05.240]  и исходным телом свидетельств.
[32:05.240 --> 32:12.240]  Но вот если расписать для пар этот функционал, то он при объединении пар примет такой вид,
[32:12.240 --> 32:16.240]  а при пересечении такой вид. Ну и тогда, соответственно, выбираются те пары
[32:16.240 --> 32:20.240]  для внутренней кластеризации, которая минимизирует этот функционал,
[32:20.240 --> 32:24.240]  для внешней, которая минимизирует этот функционал.
[32:24.240 --> 32:31.240]  На том же примере, который там был, у нас в середине тела свидетельств,
[32:31.240 --> 32:37.240]  в результате применения этой внешней внутренней кластеризации мы получим такие результаты.
[32:37.240 --> 32:43.240]  Вот будет такая внутренняя кластеризация и вот такая внешняя кластеризация.
[32:43.240 --> 32:47.240]  То есть в этом случае число кластеров задается заранее,
[32:47.240 --> 32:52.240]  на каком этапе мы хотим остановиться, сколько кластеров получить.
[32:52.240 --> 32:56.240]  Ну вот здесь, например, два кластера дошли до двух кластеров.
[32:56.240 --> 33:01.240]  Получается вот такая иерархистская кластеризация.
[33:01.240 --> 33:07.240]  Другой класс кластеризации, он связан с оптимизацией конфликта,
[33:07.240 --> 33:13.240]  вот о чем я как раз говорил. В этом случае по-разному поступает.
[33:13.240 --> 33:19.240]  Например, можно выделить некоторое маленькое под множество фокальных элементов,
[33:19.240 --> 33:27.240]  вот альфа штрих, которые из таких значимых фокальных элементов.
[33:27.240 --> 33:31.240]  Что такое значимый, сейчас я поговорю об этом.
[33:31.240 --> 33:35.240]  Ну и потом, после этого можно перераспределить остальные,
[33:35.240 --> 33:39.240]  например, фокальные элементы, отнести вот к этим значимым,
[33:39.240 --> 33:42.240]  мы получим тогда такие какие-то кластеры.
[33:42.240 --> 33:49.240]  Либо можно использовать вот какой-то аналог алгоритма комбинц.
[33:49.240 --> 33:57.240]  Ну вот что касается первого подхода, когда мы выделяем значимые элементы,
[33:57.240 --> 34:04.240]  то вот в прошлом году как раз моим соавтором Андреем Бронеевичем вышла такая статья,
[34:04.240 --> 34:08.240]  где в том числе, она не только этому посвящена,
[34:08.240 --> 34:14.240]  в том числе был предложен такой алгоритм, основанный на так называемой функции плотности конфликта.
[34:14.240 --> 34:22.240]  Плотность конфликта это такая функция, которая принимает большие значения в данном множестве,
[34:22.240 --> 34:28.240]  если рядом с ним, грубо говоря, нет пересекающихся множеств,
[34:28.240 --> 34:31.240]  либо нет сильно пересекающихся множеств.
[34:31.240 --> 34:37.240]  В идеале функция равна единице, принимает максимальное значение, если таких вообще нет.
[34:37.240 --> 34:45.240]  И наоборот, нулевое значение, если это множество пересекается со всеми другими множествами Иисра.
[34:45.240 --> 34:47.240]  Множество фокальных элементов.
[34:47.240 --> 34:51.240]  Ну третье условие – это условие линейности для простоты.
[34:51.240 --> 34:55.240]  Так вот, есть ли такую функцию конфликта?
[34:55.240 --> 35:00.240]  Да, можно несложно показать, что такая плотность, она будет как раз равна единице,
[35:00.240 --> 35:04.240]  минус функция правдоподобия, о котором я говорил,
[35:04.240 --> 35:08.240]  которая является верхней оценкой вероятности события.
[35:08.240 --> 35:10.240]  Она тут будет так вычисляться.
[35:10.240 --> 35:15.240]  Ну и кроме того, можно еще учесть массу своего фокального элемента.
[35:15.240 --> 35:22.240]  То есть нас интересует только фокальный элемент, который,
[35:22.240 --> 35:29.240]  когда мы говорим, тут будет такой довольно простой,
[35:29.240 --> 35:37.240]  мы упорядчиваем все множество фокальных элементов по убыванию вот этой функции плотности конфликта.
[35:37.240 --> 35:47.240]  И выбираем значимые конфликты элементы, начиная в соответствии с этим порядком.
[35:47.240 --> 35:54.240]  Ну и кроме того, используем еще функцию расстояния так, чтобы рядом два фокальных элемента
[35:54.240 --> 36:02.240]  из этой последовательности, находящейся рядом, чтобы они не попали в это множество h'.
[36:02.240 --> 36:06.240]  То есть используем какую-то метрику.
[36:06.240 --> 36:12.240]  Что касается метрики, можно использовать, конечно, метрику между фокальными элементами
[36:12.240 --> 36:16.240]  как мощность симметрического разности множества,
[36:16.240 --> 36:21.240]  либо, если это измеримое пространство, то это какая-то мера измеримого разности множества.
[36:22.240 --> 36:27.240]  Если это метрическое пространство, можно использовать метрику Хауссдорфа.
[36:27.240 --> 36:33.240]  Но вот чаще используют вот такую метрику, очень популярна вот в этой теории,
[36:33.240 --> 36:42.240]  вот такая метрика, которая с индексами Джакарда, тут такая белинейная форма.
[36:42.240 --> 36:48.240]  Это действительно метрика, можно доказать, что это настоящая метрика, вот она чаще всего используется.
[36:49.240 --> 36:55.240]  Этот алгоритм, конечно, имеет свои аналоги, такие точечные.
[36:55.240 --> 37:02.240]  Например, один из таких алгоритмов, я привел ссылку, но таких алгоритмов довольно точечных много.
[37:02.240 --> 37:10.240]  Если тот же пример использовать, то в результате мы получим два вот таких кластера.
[37:10.240 --> 37:20.240]  После этого можно, как говорят, решая задачу минимизации расхождения между исходным телом свидетель
[37:20.240 --> 37:26.240]  и нашим телом свидетель с какими-то коэффициентами неизвестными, найти эти коэффициенты.
[37:26.240 --> 37:32.240]  Мы в результате получим очень близкий результат к тому, что было у нас в эроргической кластеризации.
[37:32.240 --> 37:37.240]  Очень близкий, там было 0.7, 0.3, но здесь масса чуть-чуть другая.
[37:37.240 --> 37:53.240]  Можно в общем случае решать задачу перераспределения фокальных элементов по выбранным кластерам.
[37:53.240 --> 37:56.240]  Решать ее следующим образом.
[37:56.240 --> 38:03.240]  С каждым выбранным кластером можно связать свое тело свидетельство, которое строится вот таким образом.
[38:03.240 --> 38:11.240]  Масса того элемента, который у нас был в исходном теле, совпадает с исходной массой.
[38:11.240 --> 38:21.240]  Но тут тогда сумма всех этих масс внутри кластера не будет равна единице в общем случае.
[38:21.240 --> 38:26.240]  Поэтому оставшуюся часть массы мы отнесем на массу всего множества.
[38:26.240 --> 38:30.240]  Это то, что мы говорим, не знание массы всего множества.
[38:30.240 --> 38:38.240]  Вот так можно, как говорят, натянуть на этот кластер тело свидетельство.
[38:38.240 --> 38:52.240]  Тогда алгоритм перераспределения оставшихся фокальных элементов по выбранным кластерам h' будет следующим.
[38:52.240 --> 39:03.240]  Следующий мы берем очередной фокальный элемент, присоединяем его к какому-то кластеру,
[39:03.240 --> 39:09.240]  потом натягиваем на вот этот новый кластер соответствующий тело свидетельств
[39:09.240 --> 39:15.240]  и считаем конфликт между новым вот этим телом свидетельств и всеми другими телами свидетельств.
[39:15.240 --> 39:21.240]  Это будет внешний конфликт между кластерами.
[39:21.240 --> 39:27.240]  И в том случае, когда мы получаем максимальный внешний конфликт между кластерами,
[39:27.240 --> 39:32.240]  вот тому кластеру будем относить этот фокальный элемент.
[39:32.240 --> 39:36.240]  Тут есть полная аналогия с принципом компактности.
[39:36.240 --> 39:47.240]  Мы должны в один кластер поместить те элементы, которые близки друг к другу, в разные кластеры, которые далеки друг к другу.
[39:47.240 --> 39:55.240]  Вот здесь вместо метрической характеристики близости используется конфликтность.
[39:55.240 --> 40:03.240]  То есть в один кластер относим те фокальные элементы, которые мало конфликтны друг с другу, а в разные, которые сильно конфликтны друг с другом.
[40:03.240 --> 40:13.240]  В результате такой кластеризации мы получим такую кластеризацию, которая цветом обозначена здесь как раз.
[40:13.240 --> 40:20.240]  Это будет примерно соответствует тому, что было в иерархической кластеризации внешней кластеризации.
[40:20.240 --> 40:26.240]  И наконец, комминс. Совсем коротко, я понимаю, времени у меня почти не осталось.
[40:26.240 --> 40:40.240]  В этом случае рассматривается функционал минимизации суммарного внутреннего конфликта между каким-то центром кластера.
[40:40.240 --> 40:45.240]  Сейчас я расскажу вам, что это такое. Центр кластера – это тоже телосвидетельство.
[40:45.240 --> 40:54.240]  Что интересно, это не фокальный элемент какой-то, а это именно телосвидетельство, натянутое на какие-то фокальные элементы.
[40:54.240 --> 41:06.240]  И телом свидетельств, натянутым на элемент этого кластера, вот такой функционал минимизируется.
[41:06.240 --> 41:13.240]  Что касается того самого интересного, ситуация с центрами кластера.
[41:13.240 --> 41:23.240]  Центр кластера, если искать его в таком виде, как линейная комбинация категоричных тел свидетельств из этого кластера, что естественно.
[41:23.240 --> 41:38.240]  То тут не трудно доказать такую теорему, что вот этот функционал при фиксированном разбиении, но мы меняем центры кластеров.
[41:38.240 --> 41:44.240]  При фиксированном разбиении будет достигать минимума, когда центры кластеров строятся следующим образом.
[41:44.240 --> 41:59.240]  Строится по этой формуле, естественно, один, но в качестве множества тут выбираются такие множества, которые максимизируют вот такую функцию правдоподобия, суженную на этот кластер.
[41:59.240 --> 42:04.240]  Сужение на кластер, функцию правдоподобия максимизируют.
[42:04.240 --> 42:14.240]  В этом случае мы используем классический алгоритм Каминс с такой оговоркой.
[42:14.240 --> 42:33.240]  Единственное, что тут сложное заключается в том, что вот эта теорема, что в отличие от метрической ситуации, как правило, возможно множество решений, множество коэффициентов будут удовлетворять условиям этой теоремы.
[42:33.240 --> 42:38.240]  И множество центров кластеров мы получаем, не один центр кластера в этом случае.
[42:38.240 --> 42:43.240]  Можно, в принципе, работать и с таким множеством, ничего страшного.
[42:43.240 --> 42:52.240]  Но если мы хотим все-таки сузить, надеяться, чтобы там у нас был один центр кластера, то нужно использовать какие-то дополнительные процедуры, дополнительные условия.
[42:52.240 --> 42:56.240]  Это могут быть разные условия, например, условия минимизации покрытия.
[42:56.240 --> 43:01.240]  В общем случае мы получаем не разбиение, а покрытие по крайних элементах.
[43:01.240 --> 43:07.240]  Можно использовать условия минимизации покрытия, то есть чтобы покрытие было близко к разбиению.
[43:07.240 --> 43:14.240]  Например, вот такое условие нужно минимизировать, суммарную мощность кластеров.
[43:14.240 --> 43:20.240]  Либо минимизировать какую-то меру неточности, например такую.
[43:20.240 --> 43:36.240]  Либо минимизировать, ну в общем разные есть варианты дополнительных процедур, которые позволяют выделить из вот этого множества центров кластера, выделить в каждом кластере по одному центру.
[43:36.240 --> 43:45.240]  Так, ну вот, например, на том же примере, если посмотреть, то получается, ну тот же самый, в общем-то, получается результат.
[43:45.240 --> 43:51.240]  Так, ну вот, собственно говоря, и все, что я хотел рассказать.
[43:51.240 --> 44:05.240]  Да, вот как дополнительный бонус можно использовать вот эту процедуру кластеризации для оценивания меры внутреннего конфликта кластеров, меры внутреннего конфликта тела свидетельств.
[44:05.240 --> 44:25.240]  Вот когда мы кластеризацию получим, то посчитав внешний конфликт вот для тел свидетельств, натянутых на эти кластеры, мы получим меру внутреннего конфликта, которая вот в общем случае у нас считается как бы сложнее, чем мера внешнего конфликта между кластерами неоднозначной.
[44:25.240 --> 44:29.240]  Поэтому вот это такой довольно важный момент.
[44:29.240 --> 44:34.240]  Так, ну вот, пожалуй, и все, что я хотел сказать.
[44:34.240 --> 44:42.240]  Happy birthday, Борис Григорьевич. Все. Если есть вопросы, я готов ответить.
[44:50.240 --> 44:53.240]  Подходите к микрофону, заодно проверим микрофон.
[44:53.240 --> 44:55.240]  Раз, два.
[44:55.240 --> 44:58.240]  Да, Борис Григорьевич, конечно.
[44:58.240 --> 45:04.240]  Ну да, мы с вами на двоих тогда будем говорить.
[45:04.240 --> 45:14.240]  Значит, я не очень понимаю, где взять эти данные? Вот исходные данные для этого кластера. Как это получается?
[45:14.240 --> 45:17.240]  Это могут быть, например, экспертные оценки.
[45:17.240 --> 45:20.240]  Первое, что это экспертные оценки?
[45:20.240 --> 45:32.240]  То есть мы, например, выбираем кого-то, какого-то руководителя выбираем, и опросили коллектив.
[45:32.240 --> 45:40.240]  Столько-то человек сказали, что они могут, например, ответить так, что мы не можем выбрать кого-то одного.
[45:40.240 --> 45:47.240]  Из пяти кандидатов я склоняюсь больше к первому, к второму.
[45:47.240 --> 45:54.240]  Другой скажет, что я из пяти кандидатов склоняюсь ко второму, к третьему и так далее.
[45:54.240 --> 45:57.240]  Мы получаем вот эти множества. Такая простая ситуация.
[45:57.240 --> 46:05.240]  Либо это, например, если речь идет о прогнозировании каких-то...
[46:05.240 --> 46:15.240]  Ну, например, прогнозирование стоимости акции, какая будет, например, через месяц.
[46:15.240 --> 46:23.240]  То есть эксперт говорит о том, что стоимость акции будет в каком-то интервале лежать, например, от 40 до 50 единиц.
[46:23.240 --> 46:25.240]  То есть вот это уже будет фокальный элемент.
[46:25.240 --> 46:30.240]  Он не говорит, что стоимость акции будет 45 единиц ровно.
[46:30.240 --> 46:35.240]  Точечное значение не дает. Он говорит множественное значение в виде интервалов.
[46:35.240 --> 46:38.240]  Это уже будет какой-то фокальный элемент.
[46:38.240 --> 46:44.240]  Либо он говорит, например, от 40 до 50 степень уверенности большая, где-то примерно 0,7.
[46:44.240 --> 46:50.240]  Ну, может быть там от, допустим, от 48 до 55 со степенью уверенности 0,3.
[46:50.240 --> 46:52.240]  Например, вот такого типа.
[46:53.240 --> 46:56.240]  Спасибо. Я понял. Спасибо.
[46:57.240 --> 47:04.240]  Сообщение посвящено управляемости треугольными системами со сменами фазового пространства.
[47:04.240 --> 47:11.240]  В двух фазовых пространствах X и Y разной размерности.
[47:11.240 --> 47:15.240]  Пространства могут иметь как одну размерность, так и разную.
[47:15.240 --> 47:21.240]  Возможен переход как из пространства большей размерности в пространство меньшей размерности, так и наоборот.
[47:21.240 --> 47:26.240]  В фазовых пространствах X и Y движения описываются следующими системами.
[47:26.240 --> 47:32.240]  В пространстве X движения описываются с системой I на отрезке времени 0T.
[47:32.240 --> 47:38.240]  В пространстве Y движения описываются с системой II на отрезке времени TaoT.
[47:38.240 --> 47:43.240]  Интервалы времени Tao и Tт заданы.
[47:43.240 --> 47:50.240]  Системы 1 и 2 были рассмотрены коробовым впервые.
[47:50.240 --> 48:03.800]  и названы треугольными. В дальнейшем они тоже и рассматривались и также изучались, например,
[48:03.800 --> 48:14.520]  задачах стабилизации каскадов систем. К подобного вида системам сводится ряд физических управляемых
[48:14.520 --> 48:24.480]  процессов. Допустимыми управлениями являются непрерывные функции U и V со значениями в R1.
[48:24.480 --> 48:35.560]  Также нам задано в пространстве X начальное множество гиперплоскость перехода, не пересекающий
[48:35.560 --> 48:43.360]  с начальным множеством 0. Задано отображение Q, с помощью которого осуществляется переход
[48:43.360 --> 48:49.680]  из одного пространства в другое. И в пространстве Y задано конечное множество m1,
[48:49.680 --> 49:03.520]  не пересекающиеся с множеством Q от гамма. Движение объекта осуществляется по следующей
[49:03.520 --> 49:10.640]  схеме. Пусть на отрезке времени 0 Tau объект движется из начального множества m0 в пространство
[49:10.640 --> 49:18.880]  X по решениям системы X1. В момент времени Tau объект попадает на гамма и под действием отображения Q
[49:18.880 --> 49:28.000]  происходит переход в пространство Y. Мы получаем точку Y от Tau, которая является начальной для
[49:28.000 --> 49:33.440]  движения объектов в пространстве Y. Дальнейшее движение осуществляется на отрезке времени Tau
[49:33.440 --> 49:52.040]  и большой по решениям системы 2 на множество m1. Причем Y от Tau не принадлежит m1, иначе задача
[49:52.040 --> 49:59.160]  решена. Задача заключается в том, чтобы найти условия, при которых объект описываемый системой 1 и 2
[49:59.160 --> 50:08.000]  будет управляемым на отрезке 0 Tau из начального множества m0 в пространство X в конечное множество
[50:08.000 --> 50:19.240]  m1 в пространство Y. Объект описываемый системой 1 и 2 называется управляемым из m0 в m1, а если
[50:19.240 --> 50:24.760]  существуют такие допустимые управления U и V, то соответствующим решениям систем
[50:24.760 --> 50:38.680]  удовлетворяют ограниченным условиям. Следующая теорема дает достаточно условия управляемости данной
[50:38.680 --> 50:47.040]  системы, и в ходе доказательства удается получить непосредственно траекторию, которая связывает
[50:47.040 --> 50:58.960]  начальную точку в m0 и точку в m1. Если мы в пространстве X рассматриваем систему,
[50:58.960 --> 51:08.760]  вводим замены переменных следующим образом, и через Zn плюс 1 обозначаем новое управление.
[51:09.480 --> 51:18.640]  После такой замены система 1 приводится к линейной системе, которая к силу рангового
[51:18.640 --> 51:25.320]  критерия Калмана является полностью управляемой за время Tau, а по определению полной управляемости
[51:25.320 --> 51:31.760]  у нас существует управление, которое переводит объект изначального множества m0 в произвольную
[51:31.760 --> 51:41.920]  точку гамма. Тогда мы новое управление можем выбрать в виде функции от времени T, таким образом,
[51:41.920 --> 51:51.040]  чтобы за время Tau попасть из точки z0 в точку zeta Tau. Например, его можно выбрать вот в таком виде.
[51:51.040 --> 52:05.680]  Далее, подставив в левую часть соотношения 1 и 5 и вместо переменных полученной функции и управления,
[52:05.680 --> 52:12.080]  мы последовательно находим функции x1, xn и получаем траекторию, которая удовлетворяет
[52:12.080 --> 52:20.720]  ограниченным условиям. Далее мы переходим в пространство y под действием отображения q и в y
[52:20.720 --> 52:34.560]  рассматриваем аналогичное движение. Проводим аналогичные процедуры и получаем второй кусок
[52:34.560 --> 52:40.320]  траектории, который соединяет точки уже в пространстве y и таким образом мы получаем
[52:40.320 --> 52:46.880]  управляемость объекта из m0 в m1 и траекторию, которая соединяет начальное и коничное множество.
[52:46.880 --> 53:02.320]  Я. Ряд ссылок на литературу и спасибо за внимание.
[53:10.320 --> 53:18.480]  Спасибо. Борис Григорьевич, я бы хотела поздравить вас и желаю вам хорошего здоровья и здоровья.
[53:22.320 --> 53:34.000]  Это немного жаль, что я говорю, я говорю никто, вместо вас, но я надеюсь, что вы слышите меня и
[53:34.000 --> 53:46.960]  и Борис Миркин был моим учителем, моим учителем и учителями мы всегда любили
[53:48.080 --> 53:54.880]  его учителя, наслаждались их, они были очень интересными, мы appreciate его
[53:54.880 --> 53:55.520]  учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, учителя, у
[54:24.880 --> 54:26.880]  учителя, учителя, учителя.
[54:54.880 --> 55:11.880]  У нас есть группа индивидуалов, и они должны сделать коллективное решение, чтобы выбрать кандидата из группы возможных кандидатов.
[55:11.880 --> 55:27.880]  И в этом фреймворке есть проблема, что кандидаты могут предпочитать свои предпочтения, чтобы получать более предпочитательный результат.
[55:27.880 --> 55:31.880]  Это называется «манипуляционным проблемом в социальном выборе».
[55:32.880 --> 55:53.880]  Так что, в некотором смысле, это естественно и логично, потому что, если кандидаты могут предпочитать результат выбора, то они имеют возможность, они имеют возможность предпочитать результат выбора,
[55:53.880 --> 55:59.880]  и, в этом смысле, возможность манипуляции является последователем этого.
[56:00.880 --> 56:16.880]  Так что, есть известный теорет «Гиберт-Сеттет-Вейт», который говорит, что вся природа, вся fair voting rule,
[56:16.880 --> 56:35.880]  которая считает минимум три альтернатива как возможный результат, они можно манипулировать, и нандиктатуральные вибрации, они можно манипулировать.
[56:35.880 --> 56:54.880]  Но, если все нандиктатуральные вибрации в социальном выборе можно манипулировать, то это интересно узнать, до какого уровня они можно манипулировать, они способны к манипуляции.
[56:54.880 --> 57:08.880]  И очень широкая использовательная процедура для решения этого вопроса, это калкуляция probabilностей манипуляции.
[57:08.880 --> 57:19.880]  Так что, калкуляция пропорции профилей, возможности ситуаций, где манипуляция возможна.
[57:19.880 --> 57:40.880]  Так что, например, для «М» альтернатив и «Н» вибраций, есть «М» факториал, «Н» возможных профилей, возможных ситуаций,
[57:41.880 --> 57:55.880]  и мы калкулируем пропорцию манипуляции.
[57:55.880 --> 58:05.880]  Мы также считаем, что проблема манипуляции под инкомплитной информацией.
[58:05.880 --> 58:12.880]  Так что мы добавляем эту ассумпцию, чтобы сделать модуль более реалистичным в некотором смысле.
[58:12.880 --> 58:24.880]  Так что, вибрацы не знают все преференции других, они просто знают информацию о профиле преференции.
[58:24.880 --> 58:44.880]  Если вибратор имеет инсцентив манипуляции в профиле «П», но это не означает, что манипуляция он успешна в этом профиле,
[58:44.880 --> 58:52.880]  потому что он или она не знает преференции, они просто знают информацию.
[58:52.880 --> 59:10.880]  И он или она, альтернатив, имеет инсцентив манипуляции, если в сете всех возможных ситуаций у него есть возможность улучшить его преференцию, в том числе и в некотором из них.
[59:10.880 --> 59:19.880]  Так что, в простых словах, мы увидим это в деталях дальше.
[59:19.880 --> 59:23.880]  Так что, я сейчас объясню фреймворк.
[59:23.880 --> 59:32.880]  Позвольте, что N будет сетом манипуляций, и количество манипуляций означает маленький N.
[59:32.880 --> 59:35.880]  X является сетом альтернатив.
[59:36.880 --> 59:43.880]  Так что, кардинальность этого сета – M.
[59:43.880 --> 59:54.880]  Pi – это нотация для преференции вибратора I, который является линией.
[59:54.880 --> 59:59.880]  И P, вибратор P – это профиль преференции.
[59:59.880 --> 01:00:06.880]  Мы рассматриваем правила агрегации в том числе.
[01:00:06.880 --> 01:00:18.880]  Корреспонденция вибратора I – это правила, которая берет профиль преференции и, в результате, дает сету альтернатив.
[01:00:18.880 --> 01:00:24.880]  Так что, если у нас есть какие-то сеты, альтернативы вибратора I, которые получаются по поводу этого процедуры,
[01:00:24.880 --> 01:00:36.880]  мы используем правила агрегации вибратора I, чтобы выбрать уникальный победитель.
[01:00:37.880 --> 01:00:45.880]  Эта правила считается алфабетикой, так что есть определенные правила.
[01:00:45.880 --> 01:01:03.880]  И мы выбираем из сета а, один альтернатив, который не доминируется по этому правилу, который указан по правилам P.T.
[01:01:03.880 --> 01:01:15.880]  И вибратор V – это правила, которая берет профиль преференции и, в результате, дает сету альтернативы.
[01:01:15.880 --> 01:01:22.880]  И мы рассматриваем правила агрегации вибратора V, которые получаются по поводу этого процедуры.
[01:01:22.880 --> 01:01:30.880]  Первое, профиль – это полная информация, когда мы знаем все о профиле преференции.
[01:01:30.880 --> 01:01:41.880]  Так что, функция полной информации P – это функция для публичной информации.
[01:01:41.880 --> 01:01:45.880]  Что мы знаем о профиле преференции P?
[01:01:45.880 --> 01:01:50.880]  Ранк PIF – полная информация функция.
[01:01:50.880 --> 01:02:00.880]  Это функция, которая вернует рейтинг альтернативы по этому правилу.
[01:02:00.880 --> 01:02:03.880]  Правила также известна.
[01:02:03.880 --> 01:02:07.880]  Вибратор PIF вернует…
[01:02:07.880 --> 01:02:13.880]  Извините, что здесь должен быть C of P.
[01:02:13.880 --> 01:02:18.880]  Вернует сету альтернатив, которые вернуют.
[01:02:18.880 --> 01:02:27.880]  И вибратор Unique Winner PIF вернует Unique Winner после тайбреки.
[01:02:28.880 --> 01:02:33.880]  Так что, есть четыре типа функций полной информации.
[01:02:33.880 --> 01:02:38.880]  И что это означает для вибратора, чтобы иметь альтернативы к манипуляции?
[01:02:38.880 --> 01:02:42.880]  У него есть информация-сет.
[01:02:42.880 --> 01:02:49.880]  Так что, это сета всех возможных ситуаций или профилей преференции,
[01:02:49.880 --> 01:02:54.880]  которые консистентны с его информацией.
[01:02:54.880 --> 01:03:00.880]  С информацией, что он имеет из общественных средств.
[01:03:00.880 --> 01:03:07.880]  Так что, это его мир, в некотором смысле.
[01:03:08.880 --> 01:03:11.880]  Дефинирование манипуляции.
[01:03:11.880 --> 01:03:17.880]  Если это коалиционная манипуляция…
[01:03:17.880 --> 01:03:22.880]  Нет, это индивидуальная манипуляция.
[01:03:22.880 --> 01:03:25.880]  Когда индивидуальный манипулирует.
[01:03:25.880 --> 01:03:33.880]  Если в его информации-сете есть такой профиль преференции,
[01:03:33.880 --> 01:03:41.880]  так что, поменяв свои преференции к P tilde,
[01:03:41.880 --> 01:03:46.880]  он может получать что-то лучше,
[01:03:46.880 --> 01:03:51.880]  и он не получит что-то хуже, как результат.
[01:03:51.880 --> 01:03:58.880]  Так что, есть возможность улучшить результат вибраторного процедуры.
[01:03:58.880 --> 01:04:03.880]  И нет шанса получать что-то хуже.
[01:04:03.880 --> 01:04:10.880]  Проблемность индивидуальной манипуляции
[01:04:10.880 --> 01:04:16.880]  демонстрирует индекс I
[01:04:16.880 --> 01:04:21.880]  с небольшим индексом,
[01:04:22.880 --> 01:04:29.880]  и в диалоге с профилем преференции,
[01:04:29.880 --> 01:04:32.880]  есть хоть один votа,
[01:04:32.880 --> 01:04:39.880]  который может манипулировать в этом роли.
[01:04:39.880 --> 01:04:46.880]  Что значит манипуляция в коалиционной манипуляции?
[01:04:46.880 --> 01:04:50.880]  Когда мы считаем манипуляцию в коалиционной манипуляции.
[01:04:50.880 --> 01:05:01.680]  Each voter has a set of other voters, who have the same preferences, so his coalition,
[01:05:01.680 --> 01:05:04.680]  co-minded people.
[01:05:04.680 --> 01:05:18.600]  And if all his members in this coalition change their preferences the same way, and they can
[01:05:18.600 --> 01:05:25.560]  achieve a better voting result, then they all have an incentive to manipulate.
[01:05:25.560 --> 01:05:34.360]  And as in the previous definition, they don't have a chance to get something worse from their
[01:05:34.360 --> 01:05:35.860]  manipulation.
[01:05:35.860 --> 01:05:47.400]  So that's a way a voter can think about other voters' actions in the framework of incomplete
[01:05:47.400 --> 01:05:52.360]  information.
[01:05:52.360 --> 01:06:03.920]  So we consider several social choice correspondences, social choice rules.
[01:06:03.920 --> 01:06:13.640]  Among them there is the most important class of scoring rules, which are defined by a scoring
[01:06:13.640 --> 01:06:15.480]  vector.
[01:06:15.480 --> 01:06:26.320]  And Sj in this vector denotes the score assigned to any alternative for its j-th position in
[01:06:26.320 --> 01:06:28.840]  individual preferences.
[01:06:28.840 --> 01:06:38.240]  So S1 denotes how many scores it gets for the top position, for the first position in preferences.
[01:06:38.240 --> 01:06:44.960]  Sm denotes how many scores it gets from the bottom position.
[01:06:44.960 --> 01:06:57.120]  And we sum these scores over all voters to get the total score of each alternative.
[01:06:57.120 --> 01:07:01.520]  So the most popular rule is plurality, for example.
[01:07:01.520 --> 01:07:07.000]  We assign exactly one score to each alternative for the top position.
[01:07:07.000 --> 01:07:12.040]  And we assign zero, we give zero scores for any other positions.
[01:07:12.040 --> 01:07:18.960]  So we count just the number of top positions for each alternative.
[01:07:18.960 --> 01:07:29.680]  In Vitor rule, we minimize the number of bottom positions for alternatives.
[01:07:29.680 --> 01:07:39.800]  And in this case, we need to give one score for each position, which is not bottom position,
[01:07:39.800 --> 01:07:41.760]  to any alternative.
[01:07:41.760 --> 01:07:48.760]  So we just give zero for the bottom and ones for every other position.
[01:07:48.760 --> 01:07:51.760]  And Vitor rule is the following.
[01:07:51.760 --> 01:08:04.520]  We give M-1 alternative score for the first position, and then we give M-2, M-3, etc.,
[01:08:04.520 --> 01:08:10.000]  until zero for the bottom position.
[01:08:10.000 --> 01:08:14.480]  So why I explain in such details these rules?
[01:08:14.480 --> 01:08:21.080]  Because the main results of this research are about scoring rules.
[01:08:21.080 --> 01:08:35.360]  For example, this one is the asymptotic behavior of manipulability index for individual manipulation,
[01:08:35.360 --> 01:08:43.520]  so the probability of individual manipulation under the information, public information
[01:08:43.520 --> 01:08:51.000]  about the unique winner for plurality rule, the most popular one.
[01:08:51.000 --> 01:08:57.560]  When we use alphabetic tie-breaking, then when the number of voters approaches infinity,
[01:08:57.560 --> 01:09:03.740]  goes to infinity, then this index, this probability tends to one.
[01:09:03.740 --> 01:09:11.940]  So it means that in almost in every possible situation, you will have an individual, at
[01:09:11.940 --> 01:09:18.760]  least one individual, who has an incentive to misrepresent his preferences.
[01:09:18.760 --> 01:09:29.660]  So it couldn't be called protected from manipulation this rule, for example, under this very kind
[01:09:29.660 --> 01:09:32.380]  of information.
[01:09:33.220 --> 01:09:41.380]  However, when you change the type of information to winner, so it means that we know a set
[01:09:41.380 --> 01:09:53.460]  of winners, then this value depends on the number of alternatives, this value, which this
[01:09:53.460 --> 01:09:58.820]  index approaches to when the number of voters goes to infinity.
[01:09:58.820 --> 01:10:02.620]  It is 1 minus 1 divided by m.
[01:10:02.620 --> 01:10:09.220]  So we change the type of information and we immediately change the number of possible situations,
[01:10:09.220 --> 01:10:17.340]  which are susceptible to manipulation.
[01:10:17.340 --> 01:10:20.860]  This is coalitional manipulation.
[01:10:20.900 --> 01:10:33.220]  When we switch from individuals to coalitions, then this index again is equal to 1 in asymptotic.
[01:10:33.220 --> 01:10:40.380]  With alphabetic tie-breaking, almost in every situation, again, we'll have at least one
[01:10:40.380 --> 01:10:50.580]  voter who has an incentive to manipulate, to vote strategically, insincerely.
[01:10:50.660 --> 01:10:54.740]  And he thinks about his coalition members.
[01:10:54.740 --> 01:11:03.460]  So when he feels some support from commended people.
[01:11:03.460 --> 01:11:06.100]  That's the asymptotic behavior of border rule.
[01:11:06.100 --> 01:11:13.020]  It's again 1 for coalitions.
[01:11:13.020 --> 01:11:23.940]  And we prove that for any number of voters and any number of alternatives, the probability
[01:11:23.940 --> 01:11:31.020]  of coalitional manipulation equals the probability of individual manipulation when we have information
[01:11:31.020 --> 01:11:42.340]  about a single, a unique winner of the election for all scoring rules, for every scoring rule.
[01:11:42.340 --> 01:11:48.260]  And we conducted some computational experiments just to view, to observe how these indices
[01:11:48.260 --> 01:11:55.300]  behave and you may clearly observe it.
[01:11:55.300 --> 01:12:08.460]  And compare the left-hand side graph illustrates individual manipulations, the probability
[01:12:08.500 --> 01:12:16.900]  of individual manipulation, popularity rule and different kinds of public information.
[01:12:16.900 --> 01:12:25.620]  And on the right-hand side, you can see coalitional manipulability index.
[01:12:25.620 --> 01:12:36.220]  And you see that for individual manipulation, we see only one graph approaching one.
[01:12:36.220 --> 01:12:48.980]  It's the graph, it's a line corresponding to the information which is the least perfect.
[01:12:48.980 --> 01:12:51.940]  So it's information about the unique winner.
[01:12:51.940 --> 01:13:04.060]  And this graph illustrates the first theorem in our slides.
[01:13:04.140 --> 01:13:14.620]  And for coalitional manipulation, we see this tendency, I think, maybe even for all of these graphs,
[01:13:14.620 --> 01:13:21.140]  this one approaching tendency.
[01:13:21.140 --> 01:13:24.700]  And what else we could see?
[01:13:24.700 --> 01:13:36.620]  The less information we have, the greater is the probability of manipulation.
[01:13:36.620 --> 01:13:47.460]  So in more situations, at least one voter will have an incentive to manipulate it.
[01:13:47.460 --> 01:14:08.220]  And it's the causes that information is not perfect, but it's useful that we have this here.
[01:14:08.260 --> 01:14:19.620]  So just the same situation for border rule, I will not consider it.
[01:14:19.620 --> 01:14:28.660]  And Vita rule is, we considered it, defined it.
[01:14:28.700 --> 01:14:44.540]  And in some sense, it is an exceptional rule, because for when we restrict information,
[01:14:44.540 --> 01:14:49.900]  when we give less information to voters, and when we switch to coalitional manipulation,
[01:14:49.900 --> 01:15:01.780]  this index for this very rule is getting smaller, is getting less.
[01:15:01.780 --> 01:15:13.300]  And for most kinds of information, it is almost zero or equals to zero in most cases.
[01:15:13.340 --> 01:15:23.900]  So Vita rule is, in some sense, it is protected from manipulations of individuals and coalitions,
[01:15:23.900 --> 01:15:34.460]  and from manipulations under incomplete information in such a way that in most situations,
[01:15:34.460 --> 01:15:40.460]  the manipulation is impossible in this rule.
[01:15:40.660 --> 01:15:47.020]  So these are the main points of my research.
[01:15:47.020 --> 01:15:50.300]  Thank you very much for your attention.
[01:15:50.300 --> 01:15:55.260]  I would be happy to answer your questions.
[01:16:10.460 --> 01:16:33.420]  In fact, the sample space was the set of all preference profiles here.
[01:16:34.140 --> 01:16:42.060]  And I just generated, created the set of all of them.
[01:16:42.060 --> 01:16:50.780]  So that's why I conducted the experiment just for the number of voters up to 15.
[01:16:50.780 --> 01:16:57.260]  And that was my limit for this.
[01:16:57.740 --> 01:17:05.580]  Another question is that when you are adding the whole source to the final voting process,
[01:17:05.580 --> 01:17:14.380]  as you mentioned, but previously you told that one voter may be averse to the other voter
[01:17:14.380 --> 01:17:17.580]  option, and you can think about it.
[01:17:17.580 --> 01:17:26.700]  So when you bring that in this case, then the voter might be influenced by the source
[01:17:26.780 --> 01:17:30.380]  What is the difference between a voter who is independent and a voter who is not?
[01:17:30.380 --> 01:17:44.380]  What is the difference between a voter who is independent and a voter who is not?
[01:17:44.380 --> 01:17:55.420]  He just thinks that voters with the same preferences as he has, they are identical
[01:17:55.420 --> 01:17:57.580]  to himself.
[01:17:58.220 --> 01:18:07.740]  And in some sense, their incentives are identical too.
[01:18:07.740 --> 01:18:19.740]  So all these voters of the same type have the same incentives, and they want the same candidate
[01:18:19.740 --> 01:18:20.220]  to win.
[01:18:21.100 --> 01:18:28.620]  So that's why they may misrepresent their preferences in the same way.
[01:18:28.620 --> 01:18:34.300]  And this is what our manipulating voter takes into account.
[01:18:34.300 --> 01:18:44.780]  Same thing that what if all my coalition members will also change preferences in this way,
[01:18:44.780 --> 01:18:53.180]  and then we'll collectively achieve something better in this sense.
[01:19:15.180 --> 01:19:23.180]  Yes, yes, yes, yes, of course.
[01:19:23.180 --> 01:19:23.980]  Thank you.
[01:19:23.980 --> 01:19:25.980]  Any other question?
[01:19:25.980 --> 01:19:27.980]  Please.
[01:19:27.980 --> 01:19:31.980]  Could you come here, I invite you personally.
[01:19:31.980 --> 01:19:33.980]  The microphone.
[01:19:33.980 --> 01:19:37.980]  You are the next speaker.
[01:19:37.980 --> 01:19:39.980]  By the way.
[01:19:40.780 --> 01:19:45.180]  So for online participants to hear the question.
[01:19:45.180 --> 01:19:47.180]  Yes, please.
[01:19:47.180 --> 01:19:51.180]  I want to ask about the population size.
[01:19:51.180 --> 01:19:53.180]  Looking at these scales.
[01:19:53.180 --> 01:19:55.180]  Your population size is limited.
[01:19:55.180 --> 01:19:57.180]  Right, right.
[01:19:57.180 --> 01:20:03.180]  And when you look at the scales, they are pretty converging.
[01:20:03.180 --> 01:20:05.180]  Yes.
[01:20:05.180 --> 01:20:09.180]  How representative is this finding?
[01:20:09.180 --> 01:20:15.180]  Let's say, increase the value to 100, or 1,000.
[01:20:15.180 --> 01:20:22.780]  So these experiments are conducted for the case with three alternatives, you see, and the
[01:20:22.780 --> 01:20:29.980]  number of voters from 3 to 15 everywhere.
[01:20:29.980 --> 01:20:35.180]  And for this case, we observe some tendency.
[01:20:35.180 --> 01:20:47.180]  So it's just for illustration results, which we obtained for general M and N.
[01:20:47.180 --> 01:20:53.180]  So you see, these results are for any number of alternatives.
[01:20:53.180 --> 01:21:03.180]  When we have infinitely, we have a number of voters tending to infinity,
[01:21:03.180 --> 01:21:07.180]  then this holds.
[01:21:07.180 --> 01:21:17.180]  But I couldn't say something for sure for other kinds of public information, you see.
[01:21:17.180 --> 01:21:23.180]  I haven't proved, for example, for rank function or score function.
[01:21:23.180 --> 01:21:25.180]  So I couldn't say anything.
[01:21:25.180 --> 01:21:27.180]  I couldn't say anything.
[01:21:27.180 --> 01:21:35.180]  But the graphs show that it may also tend to 1 or 2 something.
[01:21:35.180 --> 01:21:39.180]  So have I answered your question?
[01:21:39.180 --> 01:21:41.180]  Yes.
[01:21:41.180 --> 01:21:43.180]  Thank you.
[01:21:43.180 --> 01:21:47.180]  Any more questions?
[01:21:47.180 --> 01:21:49.180]  Okay.
[01:21:49.180 --> 01:21:51.180]  Thank you very much.
[01:21:51.180 --> 01:21:53.180]  Thank you.
[01:21:53.180 --> 01:21:57.180]  The next speaker is the guide to Turkish.
[01:22:15.180 --> 01:22:17.180]  Good afternoon, colleagues.
[01:22:17.180 --> 01:22:19.180]  I shall apologize for my voice.
[01:22:19.180 --> 01:22:21.180]  I think I could record.
[01:22:21.180 --> 01:22:23.180]  My name is Tendai.
[01:22:23.180 --> 01:22:25.180]  And I collaborate with Professor Boris Karangorin.
[01:22:25.180 --> 01:22:33.180]  And I want to present pseudobullying polynomials for dimensionality reduction and image processing.
[01:22:33.180 --> 01:22:37.180]  It's a celebration of Professor McKinsey.
[01:22:37.180 --> 01:22:43.180]  So I shall start.
[01:22:43.180 --> 01:22:49.180]  Now, the purpose of this talk is to showcase the usage of pseudobullying polynomials
[01:22:49.180 --> 01:22:53.180]  for those shortlisted tasks.
[01:22:53.180 --> 01:22:59.180]  It is a new formulation or a new technical approach to actually solving these tasks.
[01:22:59.180 --> 01:23:07.180]  So the main one is invariant dimensionality reduction, which we then use for purposes of cluster analysis,
[01:23:07.180 --> 01:23:11.180]  outlier detection, and feature selection.
[01:23:11.180 --> 01:23:19.180]  Then the other task is dedicated to detecting edges and blobs in image data,
[01:23:19.180 --> 01:23:29.180]  which we can use as well for image segmentation, as well as optical character recognition.
[01:23:29.180 --> 01:23:37.180]  So I think I'm not going to explain how you formulate pseudobullying polynomials,
[01:23:37.180 --> 01:23:43.180]  because I think you already realized yesterday from Professor Boris Karangorin.
[01:23:43.180 --> 01:23:51.180]  I'll just go to the properties that we desire to use for these purposes.
[01:23:51.180 --> 01:23:59.180]  So when we reduce a sample or an input matrix in pseudobullying formulation,
[01:23:59.180 --> 01:24:05.180]  there are basically three properties that arise, which I think you realized yesterday.
[01:24:05.180 --> 01:24:13.180]  There was P equivalent, there was P truncation, as well as compacting the initial data.
[01:24:13.180 --> 01:24:21.180]  So now the compacting property is the one property that we want to use for invariant or lossless data compression.
[01:24:21.180 --> 01:24:27.180]  And then P equivalent, we desire to use that for similarity comparison,
[01:24:27.180 --> 01:24:37.180]  in the sense that a pseudobullying polynomial would be a low-dimensional embedding of the initial data.
[01:24:37.180 --> 01:24:41.180]  And also there is gradient shift detection.
[01:24:41.180 --> 01:24:48.180]  Like in an image, we understand that when there is a change of color in the image matrix, we want to detect that.
[01:24:48.180 --> 01:24:54.180]  And by formulating the pseudobullying polynomials, we select some pitches of a given size,
[01:24:54.180 --> 01:25:02.180]  and then we check the power or the order or the degree of the resulting pseudobullying polynomial.
[01:25:02.180 --> 01:25:08.180]  If it is higher, it shows that the pitch is overlapping an edge.
[01:25:08.180 --> 01:25:13.180]  If it is lower or equal to zero, that means that it's basically the same information, same color.
[01:25:13.180 --> 01:25:16.180]  I will show in the following slides.
[01:25:16.180 --> 01:25:24.180]  Why do we desire to use pseudobullying formulation for dimensionality reduction,
[01:25:24.180 --> 01:25:31.180]  which we then proceed to use for clustering or unsupervised classification?
[01:25:31.180 --> 01:25:37.180]  We noticed that the other alternative tools that are available, for example,
[01:25:37.180 --> 01:25:41.180]  the t-distribute stochastic neighborhood embedding, t-SNE.
[01:25:41.180 --> 01:25:47.180]  Yes, they work fine for a number of selected cases.
[01:25:47.180 --> 01:25:55.180]  However, they have this nonconformity, this cost function involved.
[01:25:55.180 --> 01:26:09.180]  So the results might differ by differing the input values that we initialize during the formulation of the dimensionality reduction task.
[01:26:09.180 --> 01:26:14.180]  And this results in variance.
[01:26:14.180 --> 01:26:22.180]  There's also the other limitation of dependence on the whole population of samples.
[01:26:22.180 --> 01:26:29.180]  For example, using principal component analysis, we have to calculate some densities across each and every sample,
[01:26:29.180 --> 01:26:37.180]  and then make a distance function to compare, and then determine that, okay, this is the lower dimensionality of this value.
[01:26:37.180 --> 01:26:42.180]  And penalize those which have the smallest distance.
[01:26:42.180 --> 01:26:51.180]  However, with pseudobullying polynomials then, we operate every sample independently, one at a time.
[01:26:51.180 --> 01:26:57.180]  It does not involve any other information of any other sample in the population.
[01:26:57.180 --> 01:27:06.180]  And there is no variance, because the only input data in the formulation is the data that is described in that sample.
[01:27:06.180 --> 01:27:11.180]  There is nothing, no epsilon or whatsoever.
[01:27:11.180 --> 01:27:20.180]  So now, when we reduce, let's say, a data structure, which is, let's say, four dimensions, five dimensions,
[01:27:20.180 --> 01:27:33.180]  our desire is to reduce it to, let's say, three or two, because we can easily plot these dimensions on paper or on our computer screens.
[01:27:33.180 --> 01:27:45.180]  And then, once we have this lower dimension, we can actually find some lines that separate samples just by plotting,
[01:27:45.180 --> 01:27:51.180]  and then find the best separating line or the best separating plane in a Cartesian space.
[01:27:52.180 --> 01:28:02.180]  I think usually, let's say, for example, this very popular data set, Iris flower data set,
[01:28:02.180 --> 01:28:08.180]  is often used during the introduction to machine learning or data science, et cetera.
[01:28:08.180 --> 01:28:11.180]  And we all know that it is four-dimensional data.
[01:28:11.180 --> 01:28:13.180]  Why do we say four-dimensional data?
[01:28:13.180 --> 01:28:21.180]  Because every sample is described by the shape or length and its width, as well as the petal length and its petal width.
[01:28:21.180 --> 01:28:30.180]  Now, if we are to understand if there is any linear relationship between these particular values to the label that this flower depends on,
[01:28:30.180 --> 01:28:36.180]  physical or citrus or virginal, we realize that it is difficult.
[01:28:36.180 --> 01:28:48.180]  But now we decided that why don't we create an embedding that allows us to find an incidence of these particular dimensions,
[01:28:48.180 --> 01:29:01.180]  whereas the physical feature, like the sample and the petal, they represent it as rows with incidence with their physical quantities of length and width.
[01:29:01.180 --> 01:29:10.180]  And then looking at this, we realized that we already have a typical input to a formulation of suitable polynomials.
[01:29:10.180 --> 01:29:16.180]  And I would like to outline in this scenario that, of course, our method is not universal,
[01:29:16.180 --> 01:29:21.180]  because we require, we desire that our information is in matrix form.
[01:29:21.180 --> 01:29:29.180]  And there is also, and the matrix form makes sense, like the sense that the incidence values, they are related.
[01:29:33.180 --> 01:29:43.180]  Right. In this figure, I would like to highlight to you that each sample is reduced.
[01:29:43.180 --> 01:29:48.180]  Let me go back to the previous slide. Each sample is reduced, if you can see.
[01:29:51.180 --> 01:29:54.180]  Here, oops, it's not writing.
[01:30:02.180 --> 01:30:12.180]  Right. This sample ID 1, when we process it through a pseudobooling polynomial, it reduces to this value, 6.0, 2.4Y2.
[01:30:12.180 --> 01:30:18.180]  And all the other values, they also reduce into a characteristic like this.
[01:30:18.180 --> 01:30:27.180]  And then this aggregation, when we take it and express it as Cartesian coordinates,
[01:30:27.180 --> 01:30:32.180]  we can actually find some lines which separate these samples.
[01:30:32.180 --> 01:30:43.180]  And we denote that there is a single outlier, which we looked at it and realized that it is a particular outlier.
[01:30:43.180 --> 01:30:48.180]  And then there is also the other values, I think, here.
[01:30:50.180 --> 01:30:53.180]  They are very close to each other.
[01:30:53.180 --> 01:31:00.180]  We had to find some line that separated this.
[01:31:00.180 --> 01:31:05.180]  But looking at this, we realized that just by finding these separator lines,
[01:31:05.180 --> 01:31:12.180]  we can actually achieve a clustering after reducing this data into lower dimensions.
[01:31:12.180 --> 01:31:20.180]  And then we would also like to show in a more practical scenario
[01:31:24.180 --> 01:31:28.180]  the Wisconsin breast cancer diagnosis dataset.
[01:31:28.180 --> 01:31:36.180]  In this dataset, we have 30 features that describe a certain sample.
[01:31:36.180 --> 01:31:41.180]  And that is a very huge dimensionality.
[01:31:41.180 --> 01:31:51.180]  So we find how best we can represent this as a matrix that can input to the pseudobooling polynomial.
[01:31:51.180 --> 01:31:58.180]  In this scenario, I would also like to show that this is an example where we show the feature selection
[01:31:58.180 --> 01:32:06.180]  or feature dropping technique that results from this formulation.
[01:32:06.180 --> 01:32:13.180]  Now, this typical sample, for example, that you see here, it is reduced to this polynomial.
[01:32:13.180 --> 01:32:20.180]  And this is the characteristic polynomial for each and every sample in the dataset of 159 samples.
[01:32:20.180 --> 01:32:29.180]  And this automatically represents an XYZ vector, which we plot in the Cartesian space.
[01:32:29.180 --> 01:32:38.180]  And looking at this, I think this aggregation, we managed to drop, I think, two or three features
[01:32:38.180 --> 01:32:42.180]  so that we would find a line that best separates these samples.
[01:32:42.180 --> 01:32:50.180]  And it's an accuracy of close to 95.4% linear clustering.
[01:32:50.180 --> 01:32:59.180]  If we had taken this, we've also experimented by taking these reduced samples through an approximation technique
[01:32:59.180 --> 01:33:01.180]  like support vector machine or k-means.
[01:33:01.180 --> 01:33:04.180]  We actually can get even higher scores.
[01:33:04.180 --> 01:33:10.180]  But another goal of our research is to remove these approximations
[01:33:10.180 --> 01:33:16.180]  so that we can have invariant processing of our data.
[01:33:16.180 --> 01:33:21.180]  Now we go to the image processing part.
[01:33:21.180 --> 01:33:32.180]  Here we are particularly interested in our gradient shift that results by formulating pseudoboom polynomials.
[01:33:32.180 --> 01:33:40.180]  The property that we want to exploit here is the degree property of the resulting polynomial.
[01:33:40.180 --> 01:33:52.180]  And it allows us to detect whether a certain image that we have processed in the data is over an edge or it is over a blob region in the image data.
[01:33:52.180 --> 01:33:56.180]  It is also helpful to us.
[01:33:56.180 --> 01:34:03.180]  We want to also extend p-equivalents so that we can go into object detection actually.
[01:34:03.180 --> 01:34:08.180]  It is an ongoing research.
[01:34:08.180 --> 01:34:21.180]  Now the algorithm is that we take the image metrics and then we extract some windows, sliding windows, or rather a page of some literal size.
[01:34:21.180 --> 01:34:26.180]  Let's say 3x3 is shown here, or we take 8x8.
[01:34:26.180 --> 01:34:32.180]  It's just conditional. It's conditional to how much sensitive we want our edge detection to be.
[01:34:33.180 --> 01:34:38.180]  Now let's say this region, the first region that I colored here, it has only five values.
[01:34:38.180 --> 01:34:40.180]  These are constant values.
[01:34:40.180 --> 01:34:52.180]  And since the pseudoboom formulation that we use here is plain-autopaste, all the other values will aggregate to zero because we are subtracting down the row.
[01:34:52.180 --> 01:34:56.180]  And then summing the first row, we get 15.
[01:34:56.180 --> 01:34:59.180]  And this means we have a zero degree.
[01:34:59.180 --> 01:35:02.180]  There is no variable here.
[01:35:02.180 --> 01:35:10.180]  But looking at the other metrics here, it is different values that we process.
[01:35:10.180 --> 01:35:18.180]  And it results in a polynomial with the degree of 2.
[01:35:18.180 --> 01:35:21.180]  This is the maximum degree.
[01:35:21.180 --> 01:35:34.180]  Because it is happening due to the fact that the other rows are overlapping the data that is differing in color on the image metrics.
[01:35:34.180 --> 01:35:39.180]  So we already indicate them as overlaying an edge.
[01:35:40.180 --> 01:35:43.180]  Now, so practical examples that we did.
[01:35:43.180 --> 01:35:52.180]  For example, here I show a comparison of this edge detection processing comparing with the Kearney method here.
[01:35:52.180 --> 01:36:01.180]  You would see that the Kearney method allows us to just find the outward edges of this paper fruit in the input image here.
[01:36:01.180 --> 01:36:06.180]  But using pseudoboom polynomials here, we can actually see the depth.
[01:36:06.180 --> 01:36:20.180]  Because if you look at this 3D representation of this degrees, it shows that this region is power 2 or power 3 or power 0.
[01:36:20.180 --> 01:36:29.180]  And you can have a projection actually in 3D while at the same time achieving the edges that you desire.
[01:36:29.180 --> 01:36:37.180]  We also experimented in image segmentation task with the Dubai Satellite data set.
[01:36:37.180 --> 01:36:46.180]  And I would like to highlight the preprocessing that we are doing here, a Gaussian filter and a pixel length threshold.
[01:36:46.180 --> 01:36:54.180]  So that our input values are limited at least to a range of 0 to 10 or instead of 0 to 255.
[01:36:55.180 --> 01:36:59.180]  And this is the result that we have.
[01:36:59.180 --> 01:37:10.180]  Of course, we still need to go to the part where we have to detect that this region, it is a water body and this is a road.
[01:37:10.180 --> 01:37:12.180]  We are still working on that.
[01:37:12.180 --> 01:37:20.180]  But at the edge part of detecting it in segmentation, they are impressive results, I would say.
[01:37:20.180 --> 01:37:27.180]  So in summary, I would like to say that our proposed methods, they are explainable and combinatorial.
[01:37:27.180 --> 01:37:33.180]  I think in the medical sense, for example, in the Wisconsin data set for breast cancer,
[01:37:33.180 --> 01:37:39.180]  somebody would desire that they have a diagnosis which was approximated by this method because it is explainable.
[01:37:39.180 --> 01:37:47.180]  Instead of, let's say, a k-means vector machine algorithm.
[01:37:47.180 --> 01:37:55.180]  And then also we have the invariance initialization of parameters and independence of sample population and dimensionality reduction.
[01:37:55.180 --> 01:38:05.180]  And also very much controllable or flexible inputs to image segmentation or edge detection.
[01:38:05.180 --> 01:38:14.180]  In the future, we propose to exploit the p-equivalence property for actual object detection.
[01:38:14.180 --> 01:38:23.180]  We hope to use probably the resulting pseudobulding polynomials as lower dimensionality embeddings for the neural network, for example.
[01:38:23.180 --> 01:38:34.180]  And also exploit this agent blob detection to optical character recognition and interpolation of 3D data from 2D image data.
[01:38:34.180 --> 01:38:40.180]  If you desire to understand much about the formulation of pseudobulding polynomials,
[01:38:40.180 --> 01:38:50.180]  I recommend this book, Data Aggregation for P-Medium Problems, by Obatoi and Professor Boris Konenguari.
[01:38:50.180 --> 01:38:53.180]  There is also a series of information in industrial engineering.
[01:38:53.180 --> 01:39:00.180]  It is a very good book to understand the whole process of pseudobulding polynomial formulation.
[01:39:00.180 --> 01:39:05.180]  After this, I thank you for your attention. I can listen to your questions.
[01:39:05.180 --> 01:39:09.180]  Thank you very much.
[01:39:09.180 --> 01:39:18.180]  I would like to say thank you for your contributions to the episode book of the conference.
[01:39:18.180 --> 01:39:24.180]  So many changes to your spaces and to really independently.
[01:39:24.180 --> 01:39:37.180]  I really appreciate your contributions to this very, very frequently changed scale.
[01:39:37.180 --> 01:39:42.180]  I apologize and thank you very much.
[01:39:42.180 --> 01:39:47.180]  Maybe you have some questions. We have plenty of time.
[01:39:47.180 --> 01:39:50.180]  You're welcome.
[01:39:50.180 --> 01:40:05.180]  Yes.
[01:40:05.180 --> 01:40:18.180]  The thing is, can you help me? I need to open the presentation of Professor Konenguari.
[01:40:18.180 --> 01:40:27.180]  If you noticed from yesterday, the formulation of pseudobulding polynomial, we are subtracting the data.
[01:40:27.180 --> 01:40:36.180]  We are trying to minimize the cost of initial matrix.
[01:40:36.180 --> 01:40:45.180]  We are penalizing the least expensive feature or the previous one.
[01:40:45.180 --> 01:40:57.180]  Can you open the presentation of Professor Konenguari?
[01:40:57.180 --> 01:41:15.180]  That penalizing feature, when we apply it into image processing, we are now seeing that the previous image or the previous role in this page was less expensive or less expressive.
[01:41:15.180 --> 01:41:23.180]  For example, if it is a color 10, it is close to black, but if it is 250, then it is close to white.
[01:41:23.180 --> 01:41:31.180]  The 250 is more expressive, and it shows that there was a gradient shift from this value to that value.
[01:41:31.180 --> 01:41:38.180]  Whenever there are values which are transitioning from a smaller to a higher value or from a higher value to a smaller value,
[01:41:39.180 --> 01:41:58.180]  there is definitely going to be a variable attached to that monomial here.
[01:41:58.180 --> 01:42:16.180]  If you look in this processing, here we have created our delta C matrix, and then we are utilizing the permutation matrix, and we are utilizing it to represent our variables.
[01:42:16.180 --> 01:42:26.180]  When there is a difference between the previous value and the previous value, we are definitely going to have another y1 or y2 or y3.
[01:42:26.180 --> 01:42:34.180]  That is indicative of a shift in values. That is why we decided that it should represent an edge.
[01:42:56.180 --> 01:43:08.180]  This is the combinatorial combination.
[01:43:08.180 --> 01:43:18.180]  This is the combinatorial combination, just after the numerical example.
[01:43:18.180 --> 01:43:28.180]  This is the combinatorial combination.
[01:43:28.180 --> 01:43:38.180]  This is the combinatorial combination.
[01:43:38.180 --> 01:43:48.180]  This is the combinatorial combination.
[01:43:48.180 --> 01:43:58.180]  This is the combinatorial combination.
[01:43:58.180 --> 01:44:08.180]  This is the combinatorial combination.
[01:44:08.180 --> 01:44:18.180]  This is the combinatorial combination.
[01:44:18.180 --> 01:44:28.180]  This is the combinatorial combination.
[01:44:28.180 --> 01:44:38.180]  This is the combinatorial combination.
[01:44:38.180 --> 01:44:48.180]  This is the combinatorial combination.
[01:44:48.180 --> 01:44:58.180]  This is the combinatorial combination.
[01:44:58.180 --> 01:45:08.180]  This is the combinatorial combination.
[01:45:08.180 --> 01:45:18.180]  This is the combinatorial combination.
[01:45:18.180 --> 01:45:28.180]  This is the combinatorial combination.
[01:45:28.180 --> 01:45:38.180]  This is the combinatorial combination.
[01:45:38.180 --> 01:45:48.180]  This is the combinatorial combination.
[01:45:48.180 --> 01:45:58.180]  This is the combinatorial combination.
[01:45:58.180 --> 01:46:08.180]  This is the combinatorial combination.
[01:46:08.180 --> 01:46:18.180]  This is the combinatorial combination.
[01:46:18.180 --> 01:46:28.180]  This is the combinatorial combination.
[01:46:48.180 --> 01:46:58.180]  This is the combinatorial combination.
[01:46:58.180 --> 01:47:08.180]  This is the combinatorial combination.
[01:47:08.180 --> 01:47:18.180]  This is the combinatorial combination.
[01:47:18.180 --> 01:47:28.180]  This is the combinatorial combination.
[01:47:28.180 --> 01:47:38.180]  This is the combinatorial combination.
[01:47:38.180 --> 01:47:48.180]  This is the combinatorial combination.
[01:47:48.180 --> 01:47:58.180]  This is the combinatorial combination.
[01:47:58.180 --> 01:48:08.180]  This is the combinatorial combination.
[01:48:08.180 --> 01:48:18.180]  This is the combinatorial combination
[01:48:18.180 --> 01:48:28.180]  This is the combinatorial combination.
[01:48:28.180 --> 01:48:38.180]  This is the combinatorial combination.
[01:48:38.180 --> 01:48:48.180]  This is the combinatorial combination.
[01:48:48.180 --> 01:48:58.180]  This is the combinatorial combination.
[01:48:58.180 --> 01:49:08.180]  This is the combinatorial combination.
[01:49:08.180 --> 01:49:18.180]  This is the combinatorial combination.
[01:49:18.180 --> 01:49:26.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:49:26.180 --> 01:49:34.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:49:34.180 --> 01:49:44.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:49:44.180 --> 01:49:54.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:49:54.180 --> 01:50:04.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:50:04.180 --> 01:50:14.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:50:14.180 --> 01:50:24.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:50:24.180 --> 01:50:34.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:50:34.180 --> 01:50:44.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:50:44.180 --> 01:50:54.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:50:54.180 --> 01:51:04.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:51:04.180 --> 01:51:14.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:51:14.180 --> 01:51:24.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:51:24.180 --> 01:51:34.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:51:34.180 --> 01:51:44.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:51:44.180 --> 01:51:54.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:51:54.180 --> 01:52:04.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:52:04.180 --> 01:52:14.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:52:14.180 --> 01:52:24.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:52:24.180 --> 01:52:34.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:52:34.180 --> 01:52:42.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:52:42.180 --> 01:52:48.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:52:48.180 --> 01:52:56.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:52:56.180 --> 01:53:06.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:53:06.180 --> 01:53:12.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:53:12.180 --> 01:53:20.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:53:20.180 --> 01:53:30.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:53:30.180 --> 01:53:36.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:53:36.180 --> 01:53:44.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:53:44.180 --> 01:53:52.180]  If you compare the performance of your algorithm, for example, with real nouns, I probably missed it.
[01:53:53.180 --> 01:53:57.180]  We want to specify that our approach is indicative.
[01:53:57.180 --> 01:54:04.180]  It's not a replacement of the neural network.
[01:54:15.180 --> 01:54:19.180]  The improvement is the indicative nature of this approach.
[01:54:22.180 --> 01:54:24.180]  We want to specify that our approach is indicative of this approach.
[01:54:24.180 --> 01:54:26.180]  We want to specify that our approach is indicative of this approach.
[01:54:26.180 --> 01:54:28.180]  We want to specify that our approach is indicative of this approach.
[01:54:28.180 --> 01:54:30.180]  We want to specify that our approach is indicative of this approach.
[01:54:30.180 --> 01:54:32.180]  We want to specify that our approach is indicative of this approach.
[01:54:32.180 --> 01:54:34.180]  We want to specify that our approach is indicative of this approach.
[01:54:34.180 --> 01:54:36.180]  We want to specify that our approach is indicative of this approach.
[01:54:36.180 --> 01:54:38.180]  We want to specify that our approach is indicative of this approach.
[01:54:38.180 --> 01:54:40.180]  We want to specify that our approach is indicative of this approach.
[01:54:40.180 --> 01:54:42.180]  We want to specify that our approach is indicative of this approach.
[01:54:42.180 --> 01:54:44.180]  Yes, I don't know whether you Id Hello?
[01:54:49.180 --> 01:54:53.180]  We don't know what that is.
[01:54:53.180 --> 01:54:57.180]  Сборная линия просто...
[01:54:57.180 --> 01:55:04.180]  Да, мы просто найдем, какая у нас лучшая сборная линия.
[01:55:04.180 --> 01:55:10.180]  И вы поднимаете эти два линии объективным функциям?
[01:55:10.180 --> 01:55:14.180]  Да, это как в СВС.
[01:55:14.180 --> 01:55:23.180]  Как вы видите, в этой объективной линии, если вы посмотрите,
[01:55:23.180 --> 01:55:31.180]  в начале у меня было 10 разных колонн, но в этой объективной линии
[01:55:31.180 --> 01:55:36.180]  я выбрал 7 колонн вместо всех остальных,
[01:55:36.180 --> 01:55:39.180]  потому что в остальных колонн мы просто добавляем звуку.
[01:55:39.180 --> 01:55:46.180]  Так что мы хотели показать, что наш процессор позволяет нам анализировать
[01:55:46.180 --> 01:55:52.180]  эти multidimensional data и выделять эти функции, которые влияют на наш получение.
[01:55:52.180 --> 01:55:57.180]  И тогда мы можем использовать эти колонны и, может быть,
[01:55:57.180 --> 01:56:00.180]  тренироваться в нейронетворе и получать, вероятно, лучшую аккуратность.
[01:56:00.180 --> 01:56:03.180]  У меня нет никаких предложений о вашем участии.
[01:56:03.180 --> 01:56:07.180]  У вас есть комментарии о вашем участии?
[01:56:07.180 --> 01:56:08.180]  Да, у нас есть комментарии о вашем участии.
[01:56:08.180 --> 01:56:09.180]  Да, у нас есть комментарии о вашем участии.
[01:56:09.180 --> 01:56:10.180]  Да, у нас есть комментарии о вашем участии.
[01:56:10.180 --> 01:56:11.180]  Да, у нас есть комментарии о вашем участии.
[01:56:11.180 --> 01:56:12.180]  Да, у нас есть комментарии о вашем участии.
[01:56:12.180 --> 01:56:13.180]  Да, у нас есть комментарии о вашем участии.
[01:56:13.180 --> 01:56:14.180]  Да, у нас есть комментарии о вашем участии.
[01:56:14.180 --> 01:56:15.180]  Да, у нас есть комментарии о вашем участии.
[01:56:15.180 --> 01:56:16.180]  Да, у нас есть комментарии о вашем участии.
[01:56:16.180 --> 01:56:17.180]  Да, у нас есть комментарии о вашем участии.
[01:56:17.180 --> 01:56:18.180]  Да, у нас есть комментарии о вашем участии.
[01:56:18.180 --> 01:56:19.180]  Да, у нас есть комментарии о вашем участии.
[01:56:19.180 --> 01:56:20.180]  Да, у нас есть комментарии о вашем участии.
[01:56:33.180 --> 01:56:34.180]  Да, у нас есть комментарии о вашем участии.
[01:56:34.180 --> 01:56:35.180]  Да, у нас есть комментарии о вашем участии.
[01:56:35.180 --> 01:56:36.180]  Да, у нас есть комментарии о вашем участии.
[01:56:36.180 --> 01:56:37.180]  Да, у нас есть комментарии о вашем участии.
[01:56:37.180 --> 01:56:38.180]  Да, у нас есть комментарии о вашем участии.
[01:56:38.180 --> 01:56:39.180]  Да, у нас есть комментарии о вашем участии.
[01:56:39.180 --> 01:56:40.180]  Да, у нас есть комментарии о вашем участии.
[01:56:40.180 --> 01:56:41.180]  Да, у нас есть комментарии о вашем участии.
[01:56:41.180 --> 01:56:42.180]  Да, у нас есть комментарии о вашем участии.
[01:56:42.180 --> 01:56:43.180]  Да, у нас есть комментарии о вашем участии.
[01:56:43.180 --> 01:56:44.180]  Да, у нас есть комментарии о вашем участии.
[01:56:44.180 --> 01:56:45.180]  Да, у нас есть комментарии о вашем участии.
[01:56:45.180 --> 01:56:46.180]  Да, у нас есть комментарии о вашем участии.
[01:56:46.180 --> 01:57:00.180]  Да, у нас есть комментарии о вашем участии.
[01:57:00.180 --> 01:57:13.180]  Да, у нас есть комментарии о вашем участии.
[01:57:13.180 --> 01:57:38.180]  Да, у нас есть комментарии о вашем участии.
[01:57:38.180 --> 01:58:05.180]  Да, у нас есть комментарии о вашем участии.
[01:58:05.180 --> 01:58:32.180]  Да, у нас есть комментарии о вашем участии.
[01:58:32.180 --> 01:58:59.180]  Да, у нас есть комментарии о вашем участии.
[01:58:59.180 --> 01:59:26.180]  Да, у нас есть комментарии о вашем участии.
[01:59:26.180 --> 01:59:53.180]  Да, у нас есть комментарии о вашем участии.
[01:59:53.180 --> 02:00:20.180]  Да, у нас есть комментарии о вашем участии.
[02:00:20.180 --> 02:00:22.180]  Да, у нас есть комментарии о вашем участии.
[02:00:22.180 --> 02:00:23.180]  Да, у нас есть комментарии о вашем участии.
[02:00:23.180 --> 02:00:24.180]  Да, у нас есть комментарии о вашем участии.
[02:00:24.180 --> 02:00:25.180]  Да, у нас есть комментарии о вашем участии.
[02:00:25.180 --> 02:00:26.180]  Да, у нас есть комментарии о вашем участии.
[02:00:26.180 --> 02:00:27.180]  Да, у нас есть комментарии о вашем участии.
[02:00:27.180 --> 02:00:28.180]  Да, у нас есть комментарии о вашем участии.
[02:00:28.180 --> 02:00:29.180]  Да, у нас есть комментарии о вашем участии.
[02:00:29.180 --> 02:00:30.180]  Да, у нас есть комментарии о вашем участии.
[02:00:30.180 --> 02:00:31.180]  Да, у нас есть комментарии о вашем участии.
[02:00:31.180 --> 02:00:32.180]  Да, у нас есть комментарии о вашем участии.
[02:00:32.180 --> 02:00:33.180]  Да, у нас есть комментарии о вашем участии.
[02:00:33.180 --> 02:00:34.180]  Да, у нас есть комментарии о вашем участии.
[02:00:34.180 --> 02:00:35.180]  Да, у нас есть комментарии о вашем участии.
[02:00:35.180 --> 02:00:36.180]  Да, у нас есть комментарии о вашем участии.
[02:00:36.180 --> 02:00:37.180]  Да, у нас есть комментарии о вашем участии.
[02:00:37.180 --> 02:00:38.180]  Да, у нас есть комментарии о вашем участии.
[02:00:38.180 --> 02:00:39.180]  Да, у нас есть комментарии о вашем участии.
[02:00:39.180 --> 02:00:40.180]  Да, у нас есть комментарии о вашем участии.
[02:00:40.180 --> 02:00:41.180]  Да, у нас есть комментарии о вашем участии.
[02:00:41.180 --> 02:00:42.180]  Да, у нас есть комментарии о вашем участии.
[02:00:42.180 --> 02:00:43.180]  Да, у нас есть комментарии о вашем участии.
[02:00:43.180 --> 02:00:44.180]  Да, у нас есть комментарии о вашем участии.
[02:00:44.180 --> 02:00:45.180]  Да, у нас есть комментарии о вашем участии.
[02:00:45.180 --> 02:00:46.180]  Да, у нас есть комментарии о вашем участии.
[02:00:46.180 --> 02:00:47.180]  Да, у нас есть комментарии о вашем участии.
[02:00:48.180 --> 02:00:49.180]  Да, у нас есть комментарии о вашем участии.
[02:00:49.180 --> 02:00:50.180]  Да, у нас есть комментарии о вашем участии.
[02:00:50.180 --> 02:00:51.180]  Да, у нас есть комментарии о вашем участии.
[02:00:51.180 --> 02:00:52.180]  Да, у нас есть комментарии о вашем участии.
[02:00:52.180 --> 02:00:53.180]  Да, у нас есть комментарии о вашем участии.
[02:00:53.180 --> 02:00:54.180]  Да, у нас есть комментарии о вашем участии.
[02:00:54.180 --> 02:00:55.180]  Да, у нас есть комментарии о вашем участии.
[02:00:55.180 --> 02:00:56.180]  Да, у нас есть комментарии о вашем участии.
[02:00:56.180 --> 02:00:57.180]  Да, у нас есть комментарии о вашем участии.

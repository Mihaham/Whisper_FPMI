[00:00.000 --> 00:11.040]  Всем доброго вечера! Мы с вами продолжаем изучение CUDA. Сегодня у нас вторая лекция из нашего курса,
[00:11.040 --> 00:15.520]  и сегодня мы будем говорить, наконец-таки, уже про более глубокие вещи, а именно про
[00:15.520 --> 00:22.560]  лимитирующие факторы, которые есть на видеокарте. Сегодня мы рассмотрим, собственно, немножко более
[00:22.560 --> 00:28.600]  глубокое устройство видеокарты, поймем, что такое варпы, и поймем, какие кошлинии существуют на
[00:28.600 --> 00:36.480]  видеокартах. Значит, наверное, мы с вами в прошлый раз, значит, когда мы говорили уже на семинарах,
[00:36.480 --> 00:45.760]  я спросил, что вы использовали код на семинарах, да, и каким образом вы замеряли время, которое
[00:45.760 --> 00:58.480]  выполняет программа? Замеряли как-нибудь? Наверное, вы использовали утилиту под названием nvprof. Была
[00:58.480 --> 01:20.040]  такая? Сейчас я напишу. Было такое? Что? Можете включить? Прошу маним.
[01:20.040 --> 01:37.040]  Нет. Все, хорошо. Знаете, в чем проблема эта утилита? Она не работает на новых видеокартах.
[01:37.040 --> 01:53.000]  Да, это поколение 383090. Да, то есть она работает на поколениях не более 20xx. На самом деле выполняется
[01:53.000 --> 02:02.000]  ассинхронно. То есть, если мы говорим про исполнение кода, вот это момент времени t, у нас с вами есть два
[02:02.000 --> 02:13.560]  потока. Один поток исполнения на CPU, а второй поток исполнения на GPU. И здесь нам понадобится
[02:13.560 --> 02:19.840]  небольшое знание диаграмм UML, чтобы вы поняли, как это работает. Совсем небольшое. Значит,
[02:19.840 --> 02:27.880]  когда вы вызываете функцию на потоке GPU, то вы по факту выполняете ее с получением какого-то
[02:27.880 --> 02:39.480]  результата. То есть, у вас функция на самом деле является синхронной. Синхронной. Когда вы выполняете
[02:39.480 --> 02:45.800]  какую-то еще операцию на CPU, если вы не создаете отдельный поток, то вы работаете в синхронном
[02:45.800 --> 02:57.280]  режиме. Важно, что когда вы вызываете функцию kernel с угловыми скобками, и эти угловые скобки
[02:57.600 --> 03:08.560]  что означает? Вот это означает количество блоков, он же еще grid-dim, и есть у нас количество
[03:08.560 --> 03:17.760]  трэдов. Это называется блок-dim. То у вас вот этот кернов будет исполняться ассинхронно.
[03:22.440 --> 03:24.000]  Что такое ассинхронная функция?
[03:27.280 --> 03:36.720]  Да, это функция, которую мы отправляем, но результат которой мы не дожидаемся. То есть,
[03:36.720 --> 03:43.360]  мы отправляем код на видеокарту. Значит, каким образом достичь синхронизации вот этого всего
[03:43.360 --> 03:51.320]  процесса? А вот это у нас, кстати, таймлайн для исполнения гидра. Каким образом дождаться
[03:51.320 --> 03:58.040]  результату выполнения функции? Здесь нужно использовать один из двух механизмов. Механизм
[03:58.040 --> 04:11.520]  номер раз, плохой. Вызвать функцию. Не пугайте, это завтра будет. Первая функция, это барьерная
[04:11.520 --> 04:23.760]  функция, которая называется kuda-device-synchronize. Но это ставит глобальный барьер на выполнение кода
[04:23.760 --> 04:29.720]  на цпу и на гпу. Kuda-device-synchronize функция, которая выполняет ядро. То есть, она блокирует все
[04:29.720 --> 04:36.520]  потоки исполнения до тех пор, пока и не на цпу, и на гпу не выполнится наш код. Это вариант плохой,
[04:36.520 --> 04:47.080]  потому что это глобальная блокировка. Поэтому давайте сделаем чуть хитрее. Как этого избежать?
[04:47.080 --> 04:52.760]  Значит, для того, чтобы замерить время работы одного ядра, ну, это глупо делать, заставить
[04:52.760 --> 05:00.440]  общую блокировку. Поэтому давайте сделаем хитрую вещь. Значит, мы с вами воспользуемся такой функцией,
[05:00.440 --> 05:10.680]  которая позволит нам замерить. Вот, смотрите внимательно. Поставить точку, отсечку, отсечку на гпу таймлайне.
[05:10.680 --> 05:21.520]  Дальше, когда мы делаем здесь, выполняем наш код, который у нас идет, синхронный или асинхронный,
[05:21.520 --> 05:29.360]  после вызова асинхронного кода, мы поставим отсечку в коде, который стоит после запуска ядра. То
[05:29.360 --> 05:36.640]  есть мы ставим отсечку, просто говорим, что окей, в ветке исполнения у нас появляется
[05:36.640 --> 05:47.120]  здесь наш старт, а здесь у нас появляется наш конец. При этом, вот когда мы ставим отсечку,
[05:47.120 --> 05:56.760]  это событие реально еще может быть не выполнено. Это Netify на будущее. Что нужно сделать для того,
[05:56.760 --> 06:03.960]  чтобы мы получили результат времени? Вот тогда мы уже ставим блокировку на то, что мы хотим получить
[06:03.960 --> 06:13.600]  именно вот этот результат. То есть ставим какой-нибудь синг по временной линии,
[06:13.600 --> 06:19.600]  какой-то момент времени. И тогда, после того, как мы поставим синг на то, что у нас событие n закончено,
[06:19.600 --> 06:28.920]  мы получим результат вычисления нашей функции. Теперь нам нужно понять, когда нам надо ставить
[06:28.920 --> 06:36.600]  точку синхронизации. Желательно ее ставить до синхронной операции. То есть смотрите, у нас
[06:36.600 --> 06:44.880]  выполняется ядро, после этого мы выполняем какую-то синхронную операцию после, а после того, как мы
[06:44.880 --> 06:51.880]  выполнили эту синхронную операцию, вызывать дожидание, вызывать ожидание вот это, наступления вот
[06:51.880 --> 06:57.600]  этого события. Это событие уже выполнено, поэтому функция синхронизации сработает за вот единиц.
[06:57.600 --> 07:05.760]  То есть ставим это все за момент времени выполнения ядра. Какие синхронные функции бывают?
[07:05.760 --> 07:18.960]  Наверное, первая функция, но она глобальная. Какая еще функция? Какие функции вообще посмотрели два семинар?
[07:18.960 --> 07:29.600]  Не, CUDA Event Create, а то вот как раз засечка точки. Вам даже про CUDA Event Create уже рассказали. Я
[07:29.600 --> 07:51.800]  как раз про них рассказываю. Какие еще функции были? Ну, как обычно называется.
[07:59.600 --> 08:14.400]  Так, ладно. Да, бинго. CUDA Memspy. Значит, смотрите, у функции CUDA Memspy, что она делает? Она копирует
[08:14.400 --> 08:20.000]  память с одного источника на другой, либо с девайса на хост, либо с девайса на девайс,
[08:20.000 --> 08:26.480]  либо с хоста на девайс. Так вот, если функция CUDA Memspy вызывается с параметрами h2d и d2h,
[08:26.480 --> 08:37.280]  и функциями d2h, то эта функция является синхронной, потому что нам нужно дождаться события того,
[08:37.280 --> 08:44.160]  что и там, и там появляются источники данные. То есть обычно происходит так, что вы засекаете
[08:44.160 --> 08:50.520]  точку времени, ставите end, после этого ставите какой-нибудь CUDA Memspy и после этого получаете
[08:50.520 --> 08:58.680]  информацию о том, а действительно за какое время у вас получено событие. Просто если вы так не
[08:58.680 --> 09:08.360]  сделаете, то у вас в итоге ноль получится, потому что у вас вот эта delta t будет равной нулю. То есть
[09:08.360 --> 09:15.120]  она будет обновлять событие только после того, как вот событие по факту выполнется. Понятно вот
[09:15.120 --> 09:24.360]  этот концепция. Вывод из всех этих рассуждений состоит в том, что нам помимо точки замера
[09:24.360 --> 09:32.800]  времени, нужно еще точка синхронизации с часами, которые идут на видеокарте. И для этого как раз есть
[09:32.800 --> 09:40.680]  некоторые функции. CUDA Event T это событие времени, которое мы можем с вами использовать. CUDA Event Create
[09:40.680 --> 09:48.080]  это функция, которая создает событие. CUDA Event Record это записывать это событие. CUDA Event Create
[09:48.080 --> 09:55.280]  это конструктор класса, точнее инициализатор структуры. CUDA Event Synchronize это ожидание
[09:55.280 --> 10:00.400]  исполнения события. То есть вот как раз вот этот розовый sync, который я написал, это как раз вызов
[10:01.000 --> 10:18.280]  CUDA Event Synchronize. Да? Не, не нужно. Да, то есть вы вызываете CUDA Memcpi, CUDA Event Synchronize
[10:18.280 --> 10:24.160]  после Memcpi и у вас эта синхронизация пройдет вообще мгновенно, потому что это синхронное
[10:24.160 --> 10:33.520]  событие уже наступило. Вот. Зачем ждать события? Собственно, вот экран у нас. Да, и тут говорится,
[10:33.520 --> 10:38.760]  что ядро, еще раз давайте почистим, ядро выполняется ассинхронно. Более того, там ассинхронность
[10:38.760 --> 10:44.880]  намного более круче. Мы это рассмотрим на четвертой нашей лекции. Да, если не вызвать CUDA Event
[10:44.880 --> 10:51.840]  Synchronize, то вы получите 0 по времени. Вот. И это можно сравнить с трекером прохождения дистанции.
[10:51.840 --> 11:00.600]  То есть вы когда, не знаю, жите, вам важно именно только ваше время, а не время того,
[11:00.600 --> 11:10.200]  которое произошло от начала старта. Вот. Собственно, вот такой код. Давайте его разберем. Возможно,
[11:10.200 --> 11:17.520]  вы его видели. Уже. То есть у нас происходит замера точки CUDA Event Create от старта. Потом у нас
[11:17.520 --> 11:25.520]  происходит замер события Stop. Потом мы вызываем с вами CUDA Event Record от старта. Дальше функцию Add
[11:25.520 --> 11:31.920]  мы вызываем от количества блоков и размера блока. Ставим события Stop. После этого делаем CUDA
[11:31.920 --> 11:40.760]  Memcpi и только после этого ставим CUDA Event Synchronize. Вот Stop. Вот. После этого мы можем
[11:40.760 --> 11:46.320]  дернуть события CUDA Event Tail Up Time, которая замерит нам количество миллисекунд по чужну,
[11:46.320 --> 12:11.080]  которая исполняется наше ядро. Давайте, а? Вопрос. Да. Зачем здесь стоит Synchronize? Ну,
[12:11.080 --> 12:14.760]  специально стоит, чтобы, типа, убедиться, что это событие действительно произошло.
[12:14.760 --> 12:21.000]  Можно попробовать, кстати, запустить код с синхронизом и без синхрониза. Посмотреть,
[12:21.000 --> 12:37.560]  что произойдет. Что? Да, да, да. Они гарантируют, что те трекеры, которые ставятся на CPU,
[12:37.560 --> 12:48.680]  они идут в порядке. То есть они линиаризованы. Ну, получаем вот такой вот результат. А теперь,
[12:48.680 --> 12:58.960]  так, давайте вопрос по примеру. Это откуда, куда мы копируем память? То есть, смотрите,
[12:58.960 --> 13:05.000]  мы вычислили код на ядре. Мы создали dx, dy, а дальше нам нужно скопировать результат на
[13:05.000 --> 13:11.840]  наше устройство. То есть у нас же есть две памяти. У нас есть память хоста, есть память девайса.
[13:11.840 --> 13:18.920]  Вот, мы берем наш память девайса, в котором мы записали сумму двух массивов, и нам надо
[13:18.920 --> 13:24.520]  его скопировать на операционную систему. Вот. И это указывает направление, откуда, куда мы копируем.
[13:24.520 --> 13:33.400]  Так, а теперь, смотрите, тонкий момент, который необходимо посчитать. Нам нужно посчитать
[13:33.400 --> 13:40.440]  количество операций, которые мы с вами выполняем. Значит, для этого я скажу, какая функция там считалась.
[13:40.440 --> 13:52.920]  Функция, которая считалась там, она была такая. х плюс равно y-векторно. То есть там for int и равно нулю,
[13:52.920 --> 13:59.160]  и меньше n, х и то равно х и то плюс y и то. Давайте посчитаем количество операций,
[13:59.160 --> 14:04.440]  которые мы с вами можем делать. Значит, все замеры производились на видеокартах,
[14:04.440 --> 14:13.240]  которые у нас есть на кластере. У нас на кластере 4352 ядра. 4352 ядра. Многовато.
[14:13.240 --> 14:30.280]  Будет. Значит, частота ядра при этом самого полтора гигагерца. Гигагерца. Мощный такой процессор,
[14:30.280 --> 14:37.480]  но два раза слабее, чем обычный. Более того, мы, смотрите, еще можем прогонять две операции за цикл.
[14:37.480 --> 14:44.600]  Это связано как раз с информацией о рендеринге, которая у нас была. То есть по факту видеокарта
[14:44.600 --> 14:51.320]  устроена так, что она умеет гонять две операции за цикл. То есть заменять тот источник, который у
[14:51.320 --> 15:03.480]  нас имеется. В итоге мы получаем с вами вот такую прекрасную вещь. 4352 умножить на 1545 умножить на 2.
[15:03.480 --> 15:14.520]  Это количество операций в секунду, которое мы можем делать в мегагерцах. То есть 13,5 миллионов
[15:14.520 --> 15:30.120]  мегагерц. Это 13,5 тысяч гигагерц. Получаем 13,5 терра флопс. Терра это 10 в какой-то большой степени.
[15:31.000 --> 15:39.240]  А это расшифровывается как floating operation per seconds.
[15:51.080 --> 15:51.960]  Многовато будет.
[15:51.960 --> 16:05.400]  То есть можно посчитать много чего. Вопрос. Кажется, мощность большая, все замечательно.
[16:05.400 --> 16:10.920]  Но чем приходится платить? Это раз.
[16:10.920 --> 16:24.960]  Ну есть контекст такое. Давайте вспомним пример, который вам показывали на семинарах уже. А что с
[16:24.960 --> 16:30.920]  памятью, собственно? Тут проблема с памятью. Смотрите в чем. Просто. Насколько я помню,
[16:30.920 --> 16:42.520]  в примере складывается 2.28 элементов. Проверим сейчас. Я могу, в принципе,
[16:42.520 --> 16:46.280]  по SSH на сервер зайти и сделать сейчас прям эксперимент.
[16:46.280 --> 17:11.720]  Так, пожалуйста, тут нужно это. Так, значит, по real computer. Так, clear делаем. Так, значит,
[17:11.720 --> 17:25.640]  куда. Смотрите, у нас тут 2.28 элементов. Выделяется память, замеряется время.
[17:25.640 --> 17:30.800]  Да, тут, к сожалению, Vim. Вот. И у нас получается количество миллисекунд.
[17:30.800 --> 17:51.280]  nvcc mainq-o3 main. Смотрим NVIDIA SME. Какая видеокарта свободная. Ну, пятая.
[17:51.280 --> 18:00.680]  Девайс с пять. Вам рассказали вот про такую штуку? Про переменное окружение. Ну, хорошо. Она
[18:00.680 --> 18:14.560]  позволяет установить определенную видеокарту. Так. Так, сейчас. А, ну, смотрите, пример. Видите,
[18:14.560 --> 18:19.480]  CUDA Event Record Stop. Что происходит, если мы поставим CUDA Event Lab, ставим до CUDA WMCPI.
[18:20.480 --> 18:28.760]  Логично, что мы получим ноль. Возвращаем сюда. Заодно проверим, нужно ли нам событие.
[18:28.760 --> 18:40.840]  Сейчас. А потому что у нас CUDA Event Record, CUDA замер вот этого события произошел до того,
[18:40.840 --> 18:47.280]  как это событие реально произошло по времени. То есть пока у нас ядро выполнялось на видеокарте,
[18:47.440 --> 18:58.960]  мы сразу взяли дельту. Хотя там еще должно 5 миллисекунд протечь по времени. Так,
[18:58.960 --> 19:12.360]  еще один пример. Ну, кстати, видно, что CUDA Event Synchronize можно не использовать.
[19:12.360 --> 19:19.160]  Вот спросили пример, нужно ли его использовать или нет. Так, ну, смотрите,
[19:19.160 --> 19:28.840]  мы получаем на 2,28 элементов сложение за 6 миллисекунд. Так, ну, давайте посчитаем,
[19:28.840 --> 19:42.920]  сколько это у нас. 2,28 делить на 0, 0 сколько? 6 делить на миллиард.
[19:58.840 --> 20:16.840]  44. Да. Да, и как говорится, и как тут поется в одной известной песне Сакнаэль,
[20:16.840 --> 20:23.080]  не будем ее цитировать, благо у нас тут учебное заведение. Что-то здесь не так.
[20:23.080 --> 20:31.640]  Да, собственно, что-то у нас происходит не то. Давайте разбираться, где подвох.
[20:31.640 --> 20:42.960]  Процессе взаимодействия в памяти. Именно так. А давайте посчитаем пропускную способность
[20:42.960 --> 20:48.800]  канала. То есть по факту, что у нас происходит? У нас происходит 2,28 элементов. У нас с вами
[20:48.800 --> 20:55.520]  происходит операция чтений из памяти, видового элемента массива и операция записи видового
[20:55.520 --> 21:00.320]  элемента массива. Поэтому посчитаем пропускную способность, которая у нас есть. Значит, сейчас,
[21:00.320 --> 21:06.280]  я не знаю, на курсе по операционным системам, вам рассказали, как работает RM внутри?
[21:06.280 --> 21:18.760]  Random Access Memory. Как вот эта вот платка работает? Ну, в целом, да, но просто
[21:18.760 --> 21:24.280]  как считается пропускная способность канала? Просто сейчас понадобится. В общем,
[21:24.280 --> 21:29.680]  пропускная способность вычисляется из следующих трех коэффициентов. Во-первых,
[21:29.680 --> 21:35.000]  это частота, на которой работает оперативная память. Сколько тактов мы можем прогонять. Дальше
[21:35.000 --> 21:41.560]  пропускная способность самого канала, шина. И еще есть такой коэффициент под названием Efficiency,
[21:41.560 --> 21:52.120]  эффективность канала пропускных данных. Наверное, вы слышали, что есть DDR2, DDR3, DDR4. А? Уже 5
[21:52.120 --> 22:00.720]  появился. Это специальное, так сказать, поколение памяти DDR. И каждое это поколение самостоятельно
[22:00.720 --> 22:07.200]  собственно улучшается. И вот этот коэффициент эффективности как раз зависит от архитектуры DDR.
[22:07.200 --> 22:15.840]  Сколько по факту, там сказать, сколько раз мы можем прогнать наш сигнал за счет того,
[22:15.840 --> 22:21.420]  что у нас современная оперативная память. Итак, частота оперативной памяти для той
[22:21.420 --> 22:30.440]  видеокарты, которая у нас была 1750 МГц. 1,7 ГГц. Пропускная способность шины
[22:30.440 --> 22:36.760]  352 бита, подчеркну внимание, а не байты. А эффективность равняется восьмерке.
[22:36.760 --> 22:50.920]  Спрашивается, как это посчитать? Ну, я не ожидаю от вас ответа. Знаешь, это, точнее, давайте,
[22:50.920 --> 22:56.320]  все-таки одну часть ответа я хочу дождаться. Я хочу дождаться, почему здесь есть двойка точно.
[22:56.320 --> 23:07.240]  Ну, двойку из восьми, по крайней мере, мы сможем с вами объяснить. Потому что D,
[23:07.240 --> 23:25.720]  как расшифровывается? Double data rate. Вот одна двойка идет отсюда. Осталось четыре.
[23:26.320 --> 23:38.000]  Почему? Ну да, осталось две двойки. Не-не-не. Вообще архитектура,
[23:38.000 --> 23:47.080]  которая стоит в 28 ста, это GDDR6. Graphical double data rate 6. Вот. И если почитать спецификации
[23:47.080 --> 23:52.760]  именно этой архитектуры, то окажется, там четыре передачи за цикл. Потом, по-моему,
[23:52.760 --> 23:59.400]  следующий появился тоже GDDR6X, и у нее уже 8 передач за цикл было. То есть,
[23:59.400 --> 24:05.600]  вам цикл передает двойной кусок данных, и у вас еще есть четыре передач за цикл. В итоге 8.
[24:05.600 --> 24:25.560]  Ну как это? С чем бы это сравнить? А, ну смотрите, пример, канонический пример.
[24:25.560 --> 24:31.480]  Вот вы хотите пройти через турникет в электричке. Да, ну, на станции
[24:31.480 --> 24:36.240]  Долгопрудная или Станция Новодачная. Хотя на Ставодачной там этот, как это называется?
[24:36.240 --> 24:42.480]  Валидатор, да, на Долгопрудной. Допустим, валидаторов нет. Ну и как можно пройти? Пока охранник не видит,
[24:42.480 --> 24:49.600]  можно пройти не одному человеку за один билет, а нескольким человеком за один билет. Ну вот,
[24:49.600 --> 24:56.440]  приблизительно так же здесь и происходит. Да. Но только это здесь легально происходит за все
[24:56.440 --> 25:15.360]  специфики архитектуры. Ну да. Не, ну я просто про суть, типа как это может быть, рассказал. Вот.
[25:15.360 --> 25:23.960]  В итоге получается, смотрите, 616 гигабайт в секунду, если перемножить все эти четыре коэффициента.
[25:23.960 --> 25:37.120]  То есть 1750 на 352. Почему я не перемножил на 8? Да, потому что мы уме поделили на 8.
[25:37.120 --> 25:52.560]  Что? А потому что в битах измерялись, а не в байтах. 616 гигабайт в секунду. А теперь давайте посчитаем,
[25:52.560 --> 26:02.760]  сколько у нас с вами в нашей задаче операция чтения и памяти. У нас два в 28 раз. Мы делаем
[26:02.760 --> 26:11.120]  следующее. Выполняем задачу у равно плюс равно х. Сколько операции чтения на один элемент массива?
[26:11.120 --> 26:31.000]  И записи. Да. Read x, read y, write y. В итоге получается, сколько у нас элементов задействовано.
[26:31.000 --> 26:44.800]  Сколько памяти нам надо перегнать? Два в 28 на три на четыре байта. Два в 28 на 12 байт.
[26:44.800 --> 26:52.880]  Вот столько памяти нам надо перегнать. Давайте посчитаем, сколько два в 28 на 12 байт.
[26:52.880 --> 27:21.840]  Сколько? 3, 2, 21. Это у нас гигабайта. А теперь поделим вот это число на 616.
[27:21.840 --> 27:24.760]  Ой.
[27:40.960 --> 27:49.760]  Мы 5,2 мс из наших шести потратили просто на взаимодействие с памятью. То есть память
[27:49.760 --> 27:59.480]  является лимитирующим фактором. Ну если теперь честно, честно вот эти вот которые у нас были
[27:59.480 --> 28:15.320]  два в 28 делить на 0, 0, 0 сколько там? 8? Ну уже лучше по крайней мере. А это знаете,
[28:15.320 --> 28:18.960]  что я посчитал? Это сколько терафопсов на самом деле выдает видеокарта на сами
[28:18.960 --> 28:27.760]  вычисления? Ну там зависит от. Тут можно еще пятерку накинуть. Ну в общем уже под терафопс можно
[28:27.760 --> 28:33.000]  посчитать. То есть операции на самом деле столько. Но мы еще не учитываем, что у нас есть операция
[28:33.000 --> 28:39.120]  загрузки, загрузки выгрузки регистров и всякое такое. Еще есть цикл 4, по которому нам надо ходить.
[28:39.120 --> 28:44.720]  Чтобы террироваться, плюс еще переключать контекст. То есть у нас получается, смотрите,
[28:44.720 --> 28:50.280]  большую часть времени мы просто занимаемся копированием памяти, чтением записью памяти.
[28:50.280 --> 29:04.920]  87 процентов времени. Надо как-то это решать. Логично? Погнали решать. Да, мы вот это как раз посчитали.
[29:04.920 --> 29:10.280]  Типа количество операций, которые необходимы для чтения записи. То есть мы получаем три
[29:10.280 --> 29:16.640]  операции. Всего 12 n-байт. Ну вот. И мы таким образом можем посчитать эффективную пропускную
[29:16.640 --> 29:22.680]  способность. Если мы посчитаем количество операций, поделим на время исполнения программы. То есть это
[29:22.680 --> 29:32.360]  каждая операция взаимодействует с четырьмя байтами, три операции на каждый элемент. Это
[29:32.360 --> 29:42.920]  тоже самый пример. Вот. КПД у меня получалось 90,5 процентов времени у нас было, но тут даже еще
[29:42.920 --> 29:51.280]  меньше получилось. Как можно ускорить? Давайте подумаем. Вот вы, разработчики видеокарт,
[29:51.280 --> 30:05.040]  не поможет. Кэши надо добавить. Нам нужно добавить кэш-линия. Вот. Ну, можно еще сделать
[30:05.040 --> 30:10.440]  еще другим способом. Значит, если вы хотите быстрее копировать участки памяти, это немножко
[30:10.440 --> 30:17.640]  другая тема. Если вы замеряли код, вы видели, что КУДУМ-25 девайс-ту-хост очень долго времени
[30:17.640 --> 30:23.120]  использует. Вот. Поэтому, если вам нужно просто эффективно перекидывать память с хоста на девайс,
[30:23.120 --> 30:30.360]  то вы можете алоцировать память, которая не является записываемой памятью. То есть вы в нее не
[30:30.360 --> 30:40.080]  можете записывать память, но можете передавать между разными ядрами. Это называется Pint Memory.
[30:40.080 --> 30:45.560]  Функция, которая делает это, называется КУДА-хост-лог. То есть вы алоцируете память,
[30:45.560 --> 30:50.680]  именно прибиваете ее к земле для того, чтобы в дальнейшем можно было копировать. Но это
[30:50.680 --> 30:56.200]  продвинутые совсем вещи. Преимущественно обычно нет. По-моему, у меня есть тоже пример,
[30:56.200 --> 31:04.160]  в который можно увидеть этот КУДА-хост-лог. А вот он, в принципе. То есть, смотрите, тоже еще одна
[31:04.160 --> 31:09.640]  особенность нискоренных систем. Значит, у нас память на хосте, на самом деле, когда
[31:09.640 --> 31:18.240]  отправляется на видеокарту, она происходит следующее. Ее прикрепляют, Pint Memory, и потом мы копируем
[31:18.240 --> 31:24.640]  это все в оперативную память устройства. Вот. При этом, если у нас, то есть нам нужен дополнительный
[31:24.640 --> 31:31.200]  участок копирования памяти осуществить. Pageable памяти, в которой вы сможете прочитать. Вот это
[31:31.200 --> 31:39.760]  память вы не сможете прочитать, у вас сиквел включится. Вот. Поэтому, если вам не нужно записывать
[31:39.760 --> 31:44.600]  память на хосте, то лучше делать так. Как сделано справа. То есть, один раз пригвоздили, а дальше
[31:44.600 --> 31:52.800]  копируете, пересылаете и так далее. Вот. И мы наконец-таки достигаем вот этой пирамиды,
[31:52.800 --> 32:00.520]  которая является пирамидой памяти. Ой, это быстро. Наверное, вы даже делали вот такой вот эксперимент,
[32:00.520 --> 32:08.160]  что ходили и смотрели, сколько занимает память, сколько денег стоит тот или иной участок памяти.
[32:08.160 --> 32:15.600]  Значит, здесь что касается иерархии памяти. Значит, на самом низком уровне здесь находятся самый
[32:15.600 --> 32:22.240]  медленный источник памяти. Чем мы идем выше, тем у нас быстрее источник памяти, но тем дороже нам
[32:22.240 --> 32:31.680]  приходится платить. Ага. Кто-нибудь знаком с самым низким уровнем хранения данных в этой пирамиде,
[32:31.680 --> 32:49.800]  которая у нас здесь есть? Да, магнитная лента. А? Не знаю. Ну, до сих пор, если есть какие-то данные в
[32:49.800 --> 32:57.840]  архивах, они записываются на вот эти вот магнитные ленты с бобиной, из которых можно достать. Кстати,
[32:57.840 --> 33:05.360]  для специально хранения данных на ленте, чтобы данные быстро восстанавливались и не особо бились,
[33:05.360 --> 33:13.080]  придумали специальный формат хранения данных. Знаете, какой?
[33:35.360 --> 33:47.400]  Тар. Тейп-архив расшифровывается. Так что теперь вы знаете, что такое. Почему исторически в
[33:47.400 --> 33:54.160]  линуксе используется TAR Gazette архивы? Потому что они еще и в UNIX были для записи на ленты.
[33:54.160 --> 34:02.560]  Ну, наверное, с лентами вы вряд ли столкнетесь. Следующий уровень – это диск. Это hard drive. Давайте
[34:02.560 --> 34:10.080]  посчитаем скорость доступа к hard drive. Какая скорость доступа к hard drive обычно у нас? Сейчас.
[34:14.680 --> 34:23.680]  Ну ладно, вы что, фильмы не копируете или что? Сейчас в интернете смотрим. Ну, давайте посчитаем
[34:23.680 --> 34:30.400]  где-то 100 мегабайт в секунду. Скорость копирования данных на hard drive, которую реально можно достичь.
[34:30.520 --> 34:46.680]  Так, сколько стоит hard? Так, рубрика эксперимента. Так, извините. Так, давайте.
[34:46.680 --> 35:14.400]  Так, идем покупать HDD. Так, ну 4 терабайта возьмем.
[35:14.400 --> 35:28.400]  Что получается? Две сто. Ой, извините, рублей рисовать не умею. Две сто рублей на терабайт.
[35:28.400 --> 35:38.560]  Ну вот. Так, поднимаемся по иерархии выше. Что у нас в иерархии выше?
[35:38.560 --> 36:05.920]  Нет, подождите, какой рам? SSD у нас дальше. Так, покупаем SSD. Вот такой возьмем, да?
[36:05.920 --> 36:20.360]  Так, что у нас получается? Скорость доступа у нас 500 мегабит. Такая же будет?
[36:20.360 --> 36:32.920]  Где? А есть тут категория?
[36:35.920 --> 36:54.920]  SSD M2, да? А, вот они. Это NVMe. Вот он. 12 тысяч, да? 12 тысяч 7800, да?
[36:54.920 --> 37:16.920]  Так, сколько скорость у него доступа? Три с половиной, да? Три с половиной терабайта. Три с половиной гигабайт в секунду.
[37:16.920 --> 37:30.920]  Смотрите, что произошло. Здесь, в принципе, не сильный фазовый переход случился. Дальше мы поднимаемся в рам.
[37:30.920 --> 37:44.920]  Здесь нужно понять, сколько мы... Какая скорость ощущения в оперативной памяти?
[37:44.920 --> 37:58.920]  Купаем оперативку. Нет, DDR4. Хорошо. Значит, скорость доступа 2,6 гигагерца.
[37:58.920 --> 38:16.920]  2,6 гигагерца стоит 4000 на 16 гигабайт. 4000 рублей уже на 16 гигабайт.
[38:16.920 --> 38:44.920]  Получается, на терабайт это будет много. Надо 4000 поделить на 16 умножить на... На сколько? На тысяч. 250 тысяч рублей на терабайт.
[38:44.920 --> 38:56.920]  Хорошо. Сколько мы за эту скорость платим? Мы получаем 2,6 гигагерца в секунду.
[38:56.920 --> 39:04.920]  2,6 гигагерца. Тут шину памяти нужно смотреть. В порядке тут тоже возрастут.
[39:04.920 --> 39:20.920]  Гигагерц. Надо поспать машину. Давайте посмотрим. Рубрика сегодня. Покупаем видеокарты.
[39:20.920 --> 39:44.920]  Давайте DDR4 Bandwidth... А, вот они. 90 гигабайт в секунду.
[39:44.920 --> 39:58.920]  Смотрите, что произошло. У нас пропуская способность выросла в 20 раз. 25 раз. Но при этом что произошло у нас с вами?
[39:58.920 --> 40:14.920]  А цена не в 25 раз выросла. Под 40 где-то. Дальше вычислять все сложнее, потому что в нашей иерархии возникает кэш.
[40:14.920 --> 40:28.920]  Да, ну теперь... А все остальное находится в процессорах на самом деле. Что, какие последние видеокарты? О, процессор Ryzen там какой-нибудь.
[40:28.920 --> 40:54.920]  А откуда тут DDR4, кстати? Тут в комплекте что ли продается? Ну вот. А, это максимальная поддержка. Все.
[40:54.920 --> 41:08.920]  Ну, смотрите. Давайте смотреть. У нас объем кэша L2 4 мегабайта, 32 мегабайта. То есть у нас объем 4 мегабайта, 32 мегабайта.
[41:08.920 --> 41:26.920]  Скорость доступа у нас здесь будет, по-моему, если не ошибаюсь, где-то в 20 раз быстрее, чем в оперативной по кэшам. То есть это приблизительно 1800 гигабайт в секунду.
[41:26.920 --> 41:44.920]  Ну, вы можете прогнать информацию. Там будет следующий слайд. А стоимость этого чудо-дела будет уже... Ну, сколько вы отводите времени на то, что у вас есть кэш в процессоре?
[41:44.920 --> 42:09.920]  Давайте тысячу рублей оставим. Ну, не тысячу, ну ладно, 500 рублей даже. На сколько? На 32 мегабайта. Ну, получаете 2-3 миллиона рублей на терабайт.
[42:09.920 --> 42:29.920]  А? Ну да. Напишем просто много. Можно посчитать. То есть видно, как у нас хлопывается пирамида. И, кстати, что на самом верхнем уровне этой пирамиды? Регистры.
[42:29.920 --> 42:45.920]  Регистры получаем за один такт. То есть работаем по частоте оперативной памяти, ой, по частоте нашего процессора. Регистров всего сколько? Ну, 32.
[42:45.920 --> 43:12.920]  Вы можете посчитать, сколько стоит один... Да, тут будет бесконечность. Где? Вот тут? Ой, блин, надо считать. Не, скорость увеличится... Смотрите, стоимость уменьшилась в 8 раз по прикидкам, а скорость уменьшилась в 500 раз.
[43:12.920 --> 43:40.920]  То есть 500 делить на 8. Ну, смотрите, тут у нас деление на 8, а здесь у нас идет умножение на 500. Ну, то есть 120... А, на 5 тысяч. Не, на 500, на половинку от этой суммы. То есть вот это поделить на вот это это 500.
[43:42.920 --> 44:10.920]  16 тысяч на 32. 500. То есть здесь у нас получается 500 делить на 8. 60, да. Сколько это будет? 15 миллионов рублей, кажется.
[44:12.920 --> 44:22.920]  Ну да, 15 миллионов рублей на трабайт мы получаем где-то. Это по прикидкам, что мы приняли, что кэш у нас стоит 500 рублей всего лишь от всего процессора.
[44:22.920 --> 44:42.920]  Ну да, все прекрасно. Теперь вы понимаете, собственно, почему эта иерархия выглядит именно таким образом. Ты прох, так сказать. Вот это понятно, да?
[44:42.920 --> 44:57.920]  Если мы поднимаемся на уровень кэша регистров и оперативной памяти, здесь происходит следующая вещь. Что у нас есть L1 и L2 кэш. L1 и L2 кэш отдельно для каждого ядра. И дальше мы обращаемся к L3 кэшу, если у нас получается cache miss.
[44:57.920 --> 45:12.920]  В итоге у нас получается до L1 и L2 кэша мы доходим за 5-12 циклов процессора. Потом мы до L3 кэша заходим за 35 циклов процессора. А если мы поднимаемся дальше в оперативную память, то это 200 плюс циклов процессора.
[45:12.920 --> 45:28.920]  Ну, кстати, по цифрам мы с вами совпали. То есть вы понимаете, насколько у нас отличаются результаты. В иерархии GPU на самом деле все очень похоже, но есть некоторые детали. Вы помните, что такое стриминг-мултипроцессор?
[45:28.920 --> 45:43.920]  Это по факту аналог ядра в нашем тетральном процессоре. Здесь есть две уровни кэша. Это L1 кэш, который доступен на каждом стриминг-мултипроцессоре, и L2 кэш, который является общим между всеми стриминг-мултипроцессорами.
[45:43.920 --> 45:55.920]  Обычно L1 кэш, кстати, здесь имеет некоторые особенности. И смотрите внезапно. Ширина кэшлини 128 байт.
[45:55.920 --> 46:14.920]  Здесь должен произойти инсайт. Сколько элементов у нас поместится в одну кэшлинию?
[46:14.920 --> 46:32.920]  А базовый тип, сколько весит в байтах? 4,4.
[46:32.920 --> 46:51.920]  Что у нас имеет параметр 32? Количество куда ядер в одном ворпе. То есть сделано все максимально так, чтобы у вас кэшлиня подгружалась на ворп.
[46:51.920 --> 47:06.920]  То есть вот он у вас ворп. И к нему прямо одновременно прилетает участок памяти, который идет последовательно. То есть если вы к участку памяти обращаетесь не последовательно, у вас получается кэшвиз.
[47:06.920 --> 47:16.920]  То есть в нулевом левете массив надо обращаться желательно к нулевому, к пятому, к пятому, к девятому, к девятому. А они делают это сикосинакоси.
[47:16.920 --> 47:28.920]  Вот уже пошли приколы, связанные с видеокартой. Хорошо. Так, вот это понятно про кэшлиню.
[47:28.920 --> 47:41.920]  Теперь отличия от ЦПУ и ГПУ. В ЦПУ у нас есть L3 кэш, здесь у нас есть L2 кэш. При этом смотрите, насколько сильно замедляется скорость доступа к L1 кэшу.
[47:41.920 --> 47:51.920]  За 80 тактов приблизительно это делается. А если мы говорим про L2 кэш, то скорость доступа к нему приблизительно такая же, как к оперативной памяти видеокарт.
[47:51.920 --> 48:01.920]  Ну чуть быстрее, но в целом сойдет. А теперь, так, вот это тоже понятно.
[48:01.920 --> 48:10.920]  Особенностью, значит, как выглядит это теперь в логическом устройстве. Значит, здесь есть огромное количество памяти, которое мы с вами еще не смотрели.
[48:10.920 --> 48:20.920]  Значит, у нас с вами, напоминаю, что grid, вот это вот набор блоков, он делится у нас с вами на блоке, а в блоке есть потоки.
[48:20.920 --> 48:28.920]  Вот, и оказывается, что есть глобальная память, есть константная память, константная память лимитирована, куда вы можете сложить все свои константы.
[48:28.920 --> 48:38.920]  Есть текстурная память, откуда вы можете подгрузить текстуры. Дальше, у каждого потока есть, у каждого блока есть shared memory, которая доступна на блок.
[48:42.920 --> 48:54.920]  Текстура? Смотрите, представьте себе, вы меня, вы решили отрисовать вот эту вот стенку, которая здесь есть в игре.
[48:54.920 --> 49:05.920]  И решили вместо вот этого вот прозрачного фона наложить какой-нибудь орнамент. Вы берете png-шку и накладываете эту png-шку как орнамент. Вот эта вот png-шка называется текстурой, которую вы накладываете.
[49:11.920 --> 49:20.920]  Ну как раз то, что мы хотим отрисовать, то есть то, что мы хотим положить на какой-то поверхность для того, чтобы отрисовать ее на экране.
[49:25.920 --> 49:39.920]  Напоминаю, раньше это тоже было текстурной памятью, но мы, когда перешли в концепцию куды, мы поняли, что нам нужно использовать видеокарту не только как задачу отрисовки графики, но и как задачу общего назначения.
[49:39.920 --> 49:42.920]  Поэтому отделили эти два вида памяти.
[49:43.920 --> 49:57.920]  Смотрите, у нас важно, shared memory находится на блоке. Регистры при этом привязаны к каждому потоку, и у каждого потока существует локальная память по аналогии с хипом.
[49:57.920 --> 50:07.920]  Ой, по аналогии не с хипом, извините, со стэком. То есть внутри ядра тоже можно использовать свой стэк, но тут есть некоторые ограничения.
[50:07.920 --> 50:13.920]  И замечательная особенность, что разделяемая память доступна на блоке.
[50:13.920 --> 50:20.920]  А теперь сюрприз-сюрприз. Можете ли вы управлять кэшом в процессорах общего назначения?
[50:20.920 --> 50:25.920]  Вы когда-нибудь видели инструкцию, в которой вы управляете кэшом?
[50:25.920 --> 50:37.920]  Ну да. А здесь это является особенностью в видеокартах.
[50:37.920 --> 50:47.920]  На самом деле, когда вы используете подчеркивание подчеркивания shared или используете разделяемую память, вы как раз пользуетесь L1 кэшом.
[50:47.920 --> 50:55.920]  Уникальная особенность видеокарты стоит в том, что мы можем на уровне кода использовать L1 кэш.
[50:55.920 --> 51:05.920]  При этом ключевое слово, которое объявляется, это shared. То есть вы объявляете, допустим, массив внутри блока.
[51:05.920 --> 51:13.920]  Это будет выглядеть так. shared int, допустим, dx.
[51:13.920 --> 51:17.920]  Ну, 256 элементов.
[51:17.920 --> 51:23.920]  И каждый поток будет видеть этот участок памяти внутри одного блока.
[51:23.920 --> 51:32.920]  То есть внутри одного блока у вас будет один участок памяти, в другом блоке выделится такой же участок памяти.
[51:36.920 --> 51:42.920]  Вот. И при этом скорость доступа будет на уровне L1 кэша.
[51:42.920 --> 51:46.920]  Данные распространяются между всеми потоками в одном блоке.
[51:46.920 --> 51:52.920]  И, собственно, мы работаем с shared памяти уже на скорости несколько терабайт в секунду.
[51:52.920 --> 51:55.920]  Мне кажется, супер. Да, правда, есть ограничения.
[51:55.920 --> 52:03.920]  Значит, размер этой памяти внутри блока ограничен сверху обычно каким-то параметром, вида 48 килобайт.
[52:03.920 --> 52:06.920]  То есть много слишком памяти вы не выделите.
[52:09.920 --> 52:20.920]  Вам рассказывали на семинарах, какое количество потоков может вместиться в одном блоке сверху?
[52:20.920 --> 52:28.920]  Ну да, причем степень двойки.
[52:28.920 --> 52:34.920]  Не больше, чем 1024 потока вы можете выделить в современных архитектурах видеокарт на один блок.
[52:34.920 --> 52:39.920]  Ну вот. Ну, в общем, вот так вот.
[52:39.920 --> 52:42.920]  Вот этот участок возникает такой замечательный.
[52:42.920 --> 52:45.920]  И теперь вы можете делать всякие локальные операции.
[52:45.920 --> 52:47.920]  Допустим, считать производные.
[52:48.920 --> 52:51.920]  Ну что, что такое производные?
[52:51.920 --> 52:55.920]  Ну, предел. Не, если мы говорим в терминах вычислительной математики.
[53:07.920 --> 53:11.920]  f от x плюс дельта х минус f от x поделить на дельта х.
[53:11.920 --> 53:15.920]  Где дельта х, это по факту следующий элемент массива.
[53:15.920 --> 53:20.920]  То есть вам нужно эффективно брать соседей из определенного элемента.
[53:20.920 --> 53:26.920]  Вот. Так же будет работать блок на перемножение матриц, который вам придется реализовать.
[53:26.920 --> 53:29.920]  То есть вы берете блок, принимаете как шерсть памяти.
[53:29.920 --> 53:32.920]  И в нем делаете перемножение.
[53:32.920 --> 53:34.920]  Строки на ставца.
[53:34.920 --> 53:37.920]  Вот это будет работать намного больше.
[53:39.920 --> 53:41.920]  Взяли текущий элемент массива.
[53:41.920 --> 53:44.920]  Это момент времени x плюс x.
[53:44.920 --> 53:48.920]  Взяли следующий элемент массива, который x плюс дельта х.
[53:48.920 --> 53:50.920]  Посчитали между ними разность.
[53:54.920 --> 53:56.920]  Да, тоже с точки.
[53:56.920 --> 53:58.920]  Значение нашей функции.
[54:02.920 --> 54:04.920]  Кто? Значение функции?
[54:04.920 --> 54:06.920]  Ну, мы их подаем на вход.
[54:06.920 --> 54:08.920]  Они у нас посчитаны.
[54:09.920 --> 54:11.920]  Ну, то есть это вычислительная задача.
[54:11.920 --> 54:13.920]  Нам нужно просчитать производную функцию.
[54:15.920 --> 54:17.920]  Ну и берем, считаем.
[54:17.920 --> 54:19.920]  Так. Как определить размер шерсть памяти?
[54:19.920 --> 54:22.920]  Первый способ. Он выглядит вот так.
[54:24.920 --> 54:26.920]  То есть вы явно объявляете размер массива.
[54:27.920 --> 54:30.920]  Второй способ. Это вот такой вот.
[54:30.920 --> 54:34.920]  Вы указываете следующую конструкцию.
[54:34.920 --> 54:36.920]  Extern.
[54:36.920 --> 54:38.920]  Shared.
[54:40.920 --> 54:42.920]  Int.
[54:42.920 --> 54:44.920]  x. И оставляете квадратные скулки.
[54:47.920 --> 54:49.920]  Не указываете размер.
[54:50.920 --> 54:52.920]  Как указать размер?
[54:57.920 --> 54:59.920]  Extern. Что означает ключевое слово?
[55:01.920 --> 55:03.920]  Где-то там будет. А где это там?
[55:04.920 --> 55:14.920]  Ну, где-то там, на самом деле, это в параметре функции вызова ядра.
[55:14.920 --> 55:18.920]  Третьим параметром в функции вызова ядра,
[55:18.920 --> 55:20.920]  в операторе вызова ядра,
[55:20.920 --> 55:22.920]  это количество оперативной памяти,
[55:22.920 --> 55:24.920]  которое вы делаете на блок.
[55:26.920 --> 55:28.920]  В байтах сразу предупреждаю.
[55:28.920 --> 55:30.920]  А не в количестве элементов массива.
[55:30.920 --> 55:32.920]  Собственно, здесь уже начинается прикол.
[55:32.920 --> 55:34.920]  Почему?
[55:34.920 --> 55:40.920]  Потому что до текущего момента мы вычисляли все элементы со соседним.
[55:40.920 --> 55:44.920]  То есть мы вычитали векторные операции, которые могли считаться параллельно.
[55:44.920 --> 55:46.920]  Здесь же нам нужна синхронизация.
[55:46.920 --> 55:52.920]  Потому что мы берем, допустим, значение элементов в текущем массиве,
[55:52.920 --> 55:54.920]  на текущем элементе в следующем элементе массива
[55:54.920 --> 55:56.920]  и записываем значение в и текущий элемент массива.
[55:58.920 --> 56:00.920]  В чем проблема вот этого кода?
[56:08.920 --> 56:10.920]  Ну, у нас дата рейс.
[56:12.920 --> 56:16.920]  Более того, у нас потоки могут выполняться следующим образом.
[56:16.920 --> 56:18.920]  Что у нас есть один поток, у нас есть второй поток,
[56:18.920 --> 56:20.920]  а потом у нас вычисляется код.
[56:20.920 --> 56:24.920]  Если мы не поставим барьер, то мы можем сильно поплатиться.
[56:24.920 --> 56:28.920]  Из-за того, что код выполняется параллельно.
[56:28.920 --> 56:32.920]  Допустим, здесь элемент массива еще не посчитался.
[56:32.920 --> 56:36.920]  То есть нам нужно приметить синхронизации,
[56:36.920 --> 56:40.920]  когда мы работаем со соседним элементом массива.
[56:40.920 --> 56:44.920]  И на уровне блоков мы можем делать все эти операции.
[56:44.920 --> 56:46.920]  Когда мы работаем со соседним элементом массива.
[56:48.920 --> 56:52.920]  И на уровне блока есть такой оператор,
[56:52.920 --> 56:54.920]  ну или одного каша.
[56:54.920 --> 56:56.920]  Это некоторый пример кода,
[56:56.920 --> 57:00.920]  который считает там какой-то дельта х.
[57:00.920 --> 57:04.920]  И какое здесь есть слово, которое вы еще ни разу не видели.
[57:06.920 --> 57:08.920]  Сингтрец. Что делает сингтрец?
[57:08.920 --> 57:12.920]  Сингтрец это барьер внутри одного блока.
[57:14.920 --> 57:18.920]  То есть у вас два блока между собой могут не синхронизироваться.
[57:18.920 --> 57:20.920]  Но внутри они должны синхронизироваться.
[57:20.920 --> 57:24.920]  То есть после каждой записи в оперативную память,
[57:24.920 --> 57:26.920]  нужно писать сингтрец.
[57:28.920 --> 57:30.920]  Но это барьер.
[57:30.920 --> 57:32.920]  То есть все потоки должны пройти друг в друга.
[57:32.920 --> 57:34.920]  Должны пройти через этот барьер.
[57:36.920 --> 57:38.920]  Так.
[57:38.920 --> 57:40.920]  Сейчас помню.
[57:40.920 --> 57:42.920]  Вопрос. Как дедлог получить?
[57:42.920 --> 57:44.920]  В цель неаккуратным мы использовали сингтрец.
[57:52.920 --> 57:54.920]  Да.
[58:02.920 --> 58:04.920]  Ура, мы получили сингтрец.
[58:04.920 --> 58:06.920]  Ну что?
[58:06.920 --> 58:08.920]  Ура, мы получили дедлог.
[58:08.920 --> 58:10.920]  Ну не писать такой фот.
[58:12.920 --> 58:14.920]  А?
[58:16.920 --> 58:18.920]  Все?
[58:18.920 --> 58:20.920]  Ну все, программа завечна.
[58:22.920 --> 58:24.920]  Ну понятно?
[58:26.920 --> 58:28.920]  Да.
[58:28.920 --> 58:30.920]  Ну все.
[58:30.920 --> 58:32.920]  Ну все.
[58:32.920 --> 58:34.920]  Ну все.
[58:34.920 --> 58:36.920]  Еще drive, да?
[58:36.920 --> 58:38.920]  Да, CTRL-C.
[58:40.920 --> 58:42.920]  Или kill-9, в зависимости от того, что вам больше нравится.
[58:44.920 --> 58:46.920]  Так, про доступ данно.
[58:46.920 --> 58:47.920]  Тоже поговорим.
[58:47.920 --> 58:49.920]  Тоже еще один тезис про pipeline cache
[58:49.920 --> 58:51.920]  Размер блока 256
[58:51.920 --> 58:53.920]  Хотим одним потоком обрабатывать два элемента.
[58:53.920 --> 58:55.920]  И здесь два примера.
[58:55.920 --> 58:57.920]  Способ один.
[58:57.920 --> 58:59.920]  Это обрабатывать элементы вот таким способом.
[58:59.920 --> 59:01.920]  То есть Первый поток обрабатывает элементы 01,
[59:01.920 --> 59:08.240]  обрабатывать элементы 2, 3 и так далее, либо второй способ это обрабатывать потоки под
[59:08.240 --> 59:16.240]  номером 0, 256, 1, 257 и так далее. Что, как вы думаете, более натуральный для видеокарты?
[59:16.240 --> 59:26.840]  А? Конечно, второй способ, потому что первый способ просто ломает все кашлинии. Вот такой
[59:26.840 --> 59:32.680]  вот код есть. Вы даже его на семинарах посмотрите. Одна функция называется add,
[59:32.680 --> 59:38.320]  другая называется stupid add. Вторая функция высчитает элементы последовательно. Видите,
[59:38.320 --> 59:43.720]  тут в цикле кефор мы берем последний элемент. Либо мы пропускаем его через разбер блока.
[59:43.720 --> 59:52.120]  Результаты. Хотите узнать? Первый способ, только не в секундах, в миллисекундах. Первый способ
[59:52.120 --> 01:00:01.120]  18,6 миллисекунды, которая stupid add, а который add тот самый, который мы с вами видели. 5,8, 5,9 миллисекунды.
[01:00:01.120 --> 01:00:13.960]  А? В 4 раза. И чем больше элементов у вас вычисляется на поток, тем больше у вас эта разница будет.
[01:00:13.960 --> 01:00:28.400]  Ага, да. Еще у нас кашмисы везде мы получаем. То есть вы представляете, вы еще в два раза
[01:00:28.400 --> 01:00:36.040]  увеличиваете количество вычислений из-за того, что у вас кашмис. Пропуск такой быстрее и этот
[01:00:36.040 --> 01:00:47.440]  пропуск равный размеру блока. Все, хорошо. Так, я посмотрел, сколько времени у нас есть. Собственно,
[01:00:47.440 --> 01:00:56.240]  как можно сломать кашлению? Рубрика слева у нас call-esque доступ, справа это uncall-esque доступ.
[01:00:56.240 --> 01:01:05.040]  Значит, я специально сегодня почитал Stack Overflow, ну и всякие другие источники, и решил изучить
[01:01:05.040 --> 01:01:11.240]  информацию, типа, насколько больнее будет uncall-esque доступ. На самом деле сейчас в современных
[01:01:11.240 --> 01:01:16.760]  архитектурах уже разница не сильно высокая будет, но учитывайте в то, что если вы попадаете в одну
[01:01:16.760 --> 01:01:21.960]  кашлению, то если вы попадаете в одну кашлению, в принципе порядок, в котором вы считываете
[01:01:21.960 --> 01:01:28.880]  элементы внутри одной кашлении, не сильно проблемный, как сказать. На старых версиях видеокарты это
[01:01:28.880 --> 01:01:34.040]  доставляло бы огромную проблему. То есть в первых версиях CUDA код, который у нас запускается вот
[01:01:34.040 --> 01:01:39.760]  с красным, с, так сказать, бардаком, намного был бы медленнее, чем код слева. В современных
[01:01:39.760 --> 01:01:48.560]  архитектурах не совсем так. Ну, вот исправили это все дело. Ну и, соответственно, чем больше
[01:01:48.560 --> 01:01:54.480]  непосредственность доступов, тем больше время работы, потому что мы сильнее ломаем кашлению нашу.
[01:01:54.480 --> 01:02:01.400]  Так, ну тоже можете попробовать функцию рандома какого-нибудь взять и посчитывать всякие элементы
[01:02:01.400 --> 01:02:11.480]  массива. То сделать random permutation внутри 32 элементов. Так, хорошо. Если здесь тоже хорошо,
[01:02:11.480 --> 01:02:15.960]  давайте поговорим про поток управления. В видеокарте работает концепция предсказательного
[01:02:15.960 --> 01:02:22.720]  вычисления. Я не знаю, слышали ли вы про такое в стандартных процессорах. Собственно,
[01:02:22.720 --> 01:02:28.240]  за счет чего современные процессоры ускоряются. У вас есть IF, какое-нибудь, вы заходите
[01:02:28.240 --> 01:02:34.360]  одновременно, прежде чем вычислить условия внутри IF, из-за конвейерности вы идете в ветку,
[01:02:34.360 --> 01:02:41.640]  которая внутри ZEN и внутри LSA и считаете ее параллельно. После того, как у вас эти ветки
[01:02:41.640 --> 01:02:46.280]  вычислись, возможно вычисляется вперед, это все дело, вы проверяете условия и делаете
[01:02:46.280 --> 01:02:51.960]  джамп по определенной маске. То есть в какую ветку вычисления вам нужно забирать результат.
[01:02:51.960 --> 01:02:59.000]  То есть это называется предсказательное вычисление. То есть вы по факту заходите в тело условия до того,
[01:02:59.000 --> 01:03:06.480]  как вы заходите в обычный IF. Вы проверяете флаг, который там стоит.
[01:03:11.640 --> 01:03:16.080]  А потому что это операция jump. Операция jump тяжелая с точки зрения ассемблера.
[01:03:16.080 --> 01:03:27.800]  Conditional jump. Ну да, jump легкий, conditional jump тяжелый. А тем более на видеокарте.
[01:03:27.800 --> 01:03:33.760]  С учетом того, что у вас половина потоков в одном варпе, то есть у вас есть в ворп,
[01:03:33.760 --> 01:03:40.400]  у вас половина потоков в варпе прыгает в ZEN, а половина потоку прыгает в LSA.
[01:03:40.400 --> 01:03:49.080]  Вот проще не дождаться этих двух коопераций, а сразу пресчитать вперед, а потом уже.
[01:03:49.080 --> 01:03:58.200]  Поэтому вы, если вы увидите там библиотеку Torch, PyTorch знакомый с такой, то увидите то,
[01:03:58.200 --> 01:04:05.760]  что там нету функции типа IF, ZEN и так далее. Там есть функция when. То есть вы говорите,
[01:04:05.760 --> 01:04:10.720]  если у вас условие на какой-то тензор выполняется, вы идете в одну ветку. Если условие на элементы
[01:04:10.720 --> 01:04:15.920]  тензора не выполняется, вы идете в другую ветку. Представляете, другой элемент массива.
[01:04:15.920 --> 01:04:21.800]  Тем самым вы по факту максимально оптимально используете концепцию предсказательных учислений.
[01:04:21.800 --> 01:04:34.360]  Вот это понятно, да? И, значит, вычисляется ветка Syphilis cells,
[01:04:34.360 --> 01:04:39.840]  потом вычисляется значение в условии на варпе и дальше делается уже, собственно, переход.
[01:04:39.840 --> 01:04:43.280]  Так, скажите, в чем проблема в этом козе?
[01:04:43.400 --> 01:04:54.360]  Происходит следующее. Человек решил в третьем потоке посчитать сумму элементов.
[01:04:54.360 --> 01:05:04.880]  Ничего. То есть, смотрите, у нас получается, если номер потока третий,
[01:05:04.880 --> 01:05:11.160]  то мы тысячу раз элементы складываем иначе, мы записываем значение элементов. В итоге у нас
[01:05:11.160 --> 01:05:16.120]  разветвляется ветка и в итоге вместо того, чтобы посчитать этот код на других ветках,
[01:05:16.120 --> 01:05:25.840]  мы по факту замедляем нашу программу 31 раз. Поэтому не надо внутри EFA делать тяжелую операцию.
[01:05:25.840 --> 01:05:32.040]  Делайте так, чтобы у вас количество элементов внутри EFA было максимальным. Внутри одного варпа.
[01:05:32.040 --> 01:05:35.400]  Кстати, это нам поможет в следующем занятии.
[01:05:35.400 --> 01:05:41.520]  Когда мы будем оптимизировать с вами сложение чисел массива.
[01:05:41.520 --> 01:05:49.800]  Да, но они будут строить быстрее.
[01:05:49.800 --> 01:06:02.640]  Так, понятно, что не нужно выполнять операцию. Внутри 3dx равно равно какой-то константе.
[01:06:02.640 --> 01:06:09.600]  Хорошо. Следующая история. Особенности национальной синхронизации, как я говорю.
[01:06:09.600 --> 01:06:17.160]  Значит, счетчики. Нам нужно посчитать какую-то информацию внутри счетчика,
[01:06:17.160 --> 01:06:23.080]  который должен быть расшарен. Ну, статистику какого-то посчитать.
[01:06:23.080 --> 01:06:27.840]  Простенькую. А что в данном случае можно сделать?
[01:06:27.840 --> 01:06:36.840]  Понятно, что плюс равно использовать не вариант. На один int какой-нибудь. Потому что у вас из кучи
[01:06:36.840 --> 01:06:45.120]  потоков сыпется операция плюс равно и в итоге у вас получается боль и страдание, связанная с тем,
[01:06:45.120 --> 01:06:51.560]  что у вас очень много датарейсов и практически половину значений при сложении вы пропустите.
[01:06:51.560 --> 01:06:57.800]  Вам показывали пример того, как выглядит операция плюс равно в ассемблере?
[01:06:57.800 --> 01:07:08.680]  Я не помню. Ассемблер.
[01:07:08.680 --> 01:07:21.160]  Начал? Так. Или не вот это нужно?
[01:07:21.640 --> 01:07:22.880]  А!
[01:07:22.880 --> 01:07:36.840]  Похоже, у меня болт.
[01:07:36.840 --> 01:07:41.040]  Не, lanes are down.
[01:07:41.040 --> 01:08:01.240]  Так, а может какое-нибудь другое компиля?
[01:08:01.240 --> 01:08:02.240]  Ладно.
[01:08:02.240 --> 01:08:18.080]  То есть, видите, плюс равно это две операции.
[01:08:18.080 --> 01:08:29.640]  Ну да, но все равно, вот видите, две операции, в итоге у вас синхронизация может произойти
[01:08:29.640 --> 01:08:30.840]  в разные моменты времени.
[01:08:30.840 --> 01:08:34.960]  То есть, у вас один поток начнет выполнять одну операцию, а второй поток влезет
[01:08:34.960 --> 01:08:39.400]  и сделает перемещение этого элемента.
[01:08:39.400 --> 01:08:40.400]  А?
[01:08:40.400 --> 01:08:45.200]  Да, в итоге половину элементов вы просто пропустите.
[01:08:45.200 --> 01:09:00.400]  Да, в общем, да, да, да, поэтому так не нужно делать, и для этого есть атомарная
[01:09:00.400 --> 01:09:01.400]  операция.
[01:09:01.400 --> 01:09:05.720]  Атомарная операция, которая выполняется за не так время.
[01:09:05.720 --> 01:09:15.960]  Atomic add и есть compare and set, atomic exchange, то есть это синхронизация
[01:09:15.960 --> 01:09:16.960]  локальна.
[01:09:16.960 --> 01:09:22.840]  Теперь, как осуществлять синхронизацию между блоками?
[01:09:23.840 --> 01:09:32.760]  Ну да, либо, смотрите, можем сделать, берем вот такую блокировку, пишем.
[01:09:32.760 --> 01:09:33.760]  А?
[01:09:33.760 --> 01:09:34.760]  А?
[01:09:34.760 --> 01:09:41.720]  Да, собственно, лок, всем известные локи.
[01:09:41.720 --> 01:09:46.680]  Либо, мы можем с вами делать вот такую функцию threadfence.
[01:09:46.680 --> 01:09:51.320]  Значит, что делает runfence, он блокирует независимые операции
[01:09:51.320 --> 01:09:52.320]  между собой.
[01:09:53.320 --> 01:09:57.800]  То есть, ошеляет барьер на независимых операциях.
[01:09:57.800 --> 01:10:04.360]  Вот, при этом shared используется внутри блока, для threadfence, значит, блокировка
[01:10:04.360 --> 01:10:05.360]  как идет?
[01:10:05.360 --> 01:10:08.000]  Для shared памяти, если вы обращаетесь к ней, то она работает внутри
[01:10:08.000 --> 01:10:09.000]  блока.
[01:10:09.000 --> 01:10:13.320]  Если мы, для девайса, для элементов массива, которые на девайсе, это ошеляет
[01:10:13.320 --> 01:10:14.320]  глобальную блокировку.
[01:10:14.320 --> 01:10:18.000]  Вот это ключевое слово threadfence.
[01:10:18.000 --> 01:10:22.280]  То есть, есть подчеркивание, подчеркивание sync threads,
[01:10:22.280 --> 01:10:27.040]  есть подчеркивание, подчеркивание threadfence, а есть подчеркивание,
[01:10:27.040 --> 01:10:33.720]  подчеркивание threadfence system, которое делает полную блокировку
[01:10:33.720 --> 01:10:34.720]  нашего ядра.
[01:10:34.720 --> 01:10:38.840]  Так, ага, тут понятно, да?
[01:10:38.840 --> 01:10:42.200]  Ну, еще посмотрим, сейчас, если время остается, threadfence
[01:10:42.200 --> 01:10:43.200]  system.
[01:10:43.200 --> 01:10:45.680]  Теперь важная информация про регистры.
[01:10:45.680 --> 01:10:49.440]  Регистров, смотрите, есть ограничения некоторые.
[01:10:49.440 --> 01:10:56.600]  Количество регистров на блок равняется 2 в 16.
[01:10:56.600 --> 01:10:57.600]  Немало.
[01:10:57.600 --> 01:11:00.280]  Ну, теперь, смотрите, все зависит от того, какой размер
[01:11:00.280 --> 01:11:01.280]  блока вы поставите.
[01:11:01.280 --> 01:11:05.280]  Если вы поставите размер блока 1024, у вас регистров
[01:11:05.280 --> 01:11:06.280]  будет меньше.
[01:11:06.280 --> 01:11:10.040]  Если вы поставите размер блока 256, то у вас регистров
[01:11:10.040 --> 01:11:11.040]  будет больше.
[01:11:11.040 --> 01:11:18.320]  Да, но есть еще одно ограничение, до 255 регистров на поток.
[01:11:18.320 --> 01:11:27.320]  Максимальное количество поток на shared stream процессоре
[01:11:27.320 --> 01:11:28.320]  1024.
[01:11:28.320 --> 01:11:33.520]  Размер блока 1024, если, то смотрите, если мы используем
[01:11:33.520 --> 01:11:39.280]  1024 регистров на поток, то у нас будет 64 регистров
[01:11:39.280 --> 01:11:42.600]  на поток, если размер блока 1024.
[01:11:42.600 --> 01:11:44.880]  Если размер блока 32, то количество регистров будет
[01:11:44.880 --> 01:11:45.880]  255.
[01:11:45.880 --> 01:11:49.000]  Куда вы от этого не прыгнете.
[01:11:49.000 --> 01:11:53.440]  Поэтому нам всегда необходим компромисс в вычтении этих
[01:11:53.440 --> 01:11:54.440]  ресурсов.
[01:11:54.440 --> 01:11:55.440]  А?
[01:11:55.440 --> 01:11:58.880]  Ну, то есть, выбирайте размер блока оптимальным
[01:11:58.880 --> 01:11:59.880]  образом.
[01:11:59.880 --> 01:12:03.240]  Так, чтобы у нас не произошло memory spilling, то, что у нас
[01:12:03.240 --> 01:12:06.360]  информация, которая была в переменной, вдруг свалилась
[01:12:06.360 --> 01:12:11.480]  в память на stack, а находилась в регистре, чем больше
[01:12:11.480 --> 01:12:13.160]  memory spilling, тем хуже.
[01:12:13.160 --> 01:12:17.800]  Потому что вы идете уже в память, несмотря на то,
[01:12:17.800 --> 01:12:21.640]  что эта память может быть в L1 Cache находиться.
[01:12:21.640 --> 01:12:24.760]  Но здесь уже использование L1 Cache будет не явным.
[01:12:24.760 --> 01:12:31.800]  Так, ну, наверное, давайте подытожим, что у нас сегодня
[01:12:31.800 --> 01:12:32.800]  было.
[01:12:32.800 --> 01:12:34.760]  У нас сегодня была такая лекция, связанная именно
[01:12:34.760 --> 01:12:37.520]  с синхронизацией всего и вся.
[01:12:37.520 --> 01:12:40.320]  Мы с вами поняли, что главным лимитирующим фактором в
[01:12:41.320 --> 01:12:44.200]  видеокартах является оперативная память.
[01:12:44.200 --> 01:12:47.240]  Разобрали иерархию оперативной памяти, поискали информацию
[01:12:47.240 --> 01:12:51.840]  в магазинах, поняли, что у нас с вами все не так просто.
[01:12:51.840 --> 01:12:52.840]  Где это?
[01:12:52.840 --> 01:12:53.840]  Так, не это.
[01:12:53.840 --> 01:12:54.840]  Во!
[01:12:54.840 --> 01:12:57.400]  Вот она наша первая вида памяти с вычислениями.
[01:12:57.400 --> 01:13:00.080]  После этого мы разобрали виды памяти и поняли, что
[01:13:00.080 --> 01:13:03.120]  у нас есть L1 Cache, которым мы можем пользоваться через
[01:13:03.120 --> 01:13:07.160]  ключевое слово shared, и вы с ним будете смотреть
[01:13:07.160 --> 01:13:08.160]  на семинарах.
[01:13:09.000 --> 01:13:12.600]  Дальше мы поняли, что синхронизация начинается тогда, когда
[01:13:12.600 --> 01:13:15.120]  нам нужно взаимодействовать с соседними элементами.
[01:13:15.120 --> 01:13:17.320]  И у нас есть локальная синхронизация при помощи
[01:13:17.320 --> 01:13:20.840]  sync threads, с которой очень легко выстрелить себе в
[01:13:20.840 --> 01:13:21.840]  ноги.
[01:13:21.840 --> 01:13:25.600]  Есть глобальная синхронизация, которую лучше не использовать,
[01:13:25.600 --> 01:13:27.920]  если вы не хотите замедлить свою программу.
[01:13:27.920 --> 01:13:28.920]  Вот.
[01:13:28.920 --> 01:13:31.240]  И в конце немножко поговорили с информацией по регистрам.
[01:13:31.240 --> 01:13:35.040]  Значит, в следующий раз мы с вами будем рассматривать
[01:13:35.040 --> 01:13:36.040]  интересную вещь.
[01:13:36.040 --> 01:13:38.640]  Мы решим с вами две больших задачи.
[01:13:38.640 --> 01:13:39.640]  Задача 1.
[01:13:39.640 --> 01:13:41.560]  Мы научимся считать сумму всех чисел массива.
[01:13:41.560 --> 01:13:46.960]  Это на самом деле не сложная операция на видеокарте,
[01:13:46.960 --> 01:13:50.680]  но в целом надо поработать с shared памятью очень сильно.
[01:13:50.680 --> 01:13:55.040]  Хотя можно и посчитать сумму в Atomic Ad, это я так по секрету
[01:13:55.040 --> 01:13:56.040]  скажу.
[01:13:56.040 --> 01:13:59.360]  Это будет операция не очень медленная.
[01:13:59.360 --> 01:14:03.120]  То есть вы берете просто, пишете, вы заходите в код
[01:14:03.120 --> 01:14:06.040]  кедра и пишете просто Atomic Ad вот в это число.
[01:14:06.040 --> 01:14:09.440]  И не поверьте, на видеокарте это работает быстро.
[01:14:09.440 --> 01:14:15.080]  То есть атомарная блокировка работает очень быстро.
[01:14:15.080 --> 01:14:19.600]  Тут надо учитывать, что внутри варпа можно выполнять
[01:14:19.600 --> 01:14:20.600]  разные операции.
[01:14:20.600 --> 01:14:23.720]  То есть посмотрим некоторые махинации, которые есть.
[01:14:23.720 --> 01:14:26.440]  И вторая операция – это подсчет суммы чисел на префиксе.
[01:14:26.440 --> 01:14:29.440]  Вот.
[01:14:29.440 --> 01:14:32.280]  Префиксную сумму научимся считать, причем научимся
[01:14:32.520 --> 01:14:33.520]  считать параллельно.
[01:14:33.520 --> 01:14:35.720]  Вам, наверное, еще никто не рассказал, как сумму
[01:14:35.720 --> 01:14:38.720]  на префиксе можно параллельно считать.
[01:14:38.720 --> 01:14:39.720]  Вот.
[01:14:39.720 --> 01:14:43.000]  Так, на этом все.
[01:14:43.000 --> 01:14:54.320]  Если есть вопросы по материалу, то задавайте в каком плане
[01:14:54.320 --> 01:14:55.320]  экогерентности кэши.
[01:14:55.320 --> 01:15:13.320]  Есть такой механизм, да, спасибо за вопрос.
[01:15:13.320 --> 01:15:15.360]  Там это на уровне варпа происходит даже.
[01:15:15.360 --> 01:15:19.000]  То есть там получается, что нельзя делать так, чтобы
[01:15:19.000 --> 01:15:21.600]  два потока внутри в одного варпа писали в один и тот
[01:15:21.600 --> 01:15:22.600]  же элемент.
[01:15:23.600 --> 01:15:25.800]  В один и тот же индекс варпа.
[01:15:25.800 --> 01:15:27.680]  Это называется банк конфликтом.
[01:15:27.680 --> 01:15:32.680]  Да, это спойлер к следующей лекции.
[01:15:32.680 --> 01:15:35.040]  Мы как раз будем решать банк конфликта в следующий
[01:15:35.040 --> 01:15:36.040]  раз.
[01:15:36.040 --> 01:15:37.040]  Вот.
[01:15:37.040 --> 01:15:40.040]  Ну, что-то похожее есть.
[01:15:40.040 --> 01:16:04.480]  Вы делаете 0,1,2,3, значит 256,257, ну и там дальше.
[01:16:04.480 --> 01:16:05.480]  Смотрите.
[01:16:05.480 --> 01:16:09.240]  Значит, когда вы складываете ноль у нулевого потока,
[01:16:09.900 --> 01:16:16.120]  взял 0-ой элемент, и 256,п deaf element, п是我ude element это
[01:16:16.120 --> 01:16:17.120]  257.
[01:16:17.160 --> 01:16:22.780]  И получается, что вот кэш линия подгружается 연ordu
[01:16:22.780 --> 01:16:29.480]  и Ocean nom seating, то есть значит simultaneously V 0,1,1.
[01:16:29.480 --> 01:16:35.840]  Что происходит, когда мы spill over . summarize thismumbling
[01:16:35.840 --> 01:16:40.120]  а первый элемент... а первый поток получает второй элемент массива.
[01:16:41.640 --> 01:16:43.640]  Хотя мы ждали,
[01:16:44.120 --> 01:16:47.320]  что этому потоку получится первый элемент массива дать.
[01:16:49.160 --> 01:16:53.200]  То есть мы ожидаем, что этому потоку отгрузится первый элемент массива, а он уйдет и берет второй.
[01:16:53.200 --> 01:16:54.240]  Получается кошмис.
[01:16:55.800 --> 01:16:56.300]  А?
[01:16:59.680 --> 01:17:01.080]  Да, а последствия чтения?
[01:17:03.520 --> 01:17:04.720]  Все, приплыли.
[01:17:06.680 --> 01:17:08.680]  Угу.
[01:17:13.040 --> 01:17:18.240]  Который последовательно вот этот вот, то есть нулевой, что нулевой поток будет заниматься сложением этих элементов массива,
[01:17:18.240 --> 01:17:20.240]  первый будет заниматься сложением этих и так далее.
[01:17:26.800 --> 01:17:33.120]  Не-не-не, они по факту должны считать параллельно, типа, что нулевой параллельно подгрузит нулевой,
[01:17:33.360 --> 01:17:35.760]  а первый подгрузит второй. Но из-за того, что они,
[01:17:36.840 --> 01:17:38.720]  так сказать,
[01:17:38.720 --> 01:17:40.320]  ломают кэш-линию,
[01:17:40.320 --> 01:17:46.840]  да, то в видеокарте существует все-таки механизм, чтобы это работало, а не ломалось, а они переключаются на последний доступ в память.
[01:17:46.840 --> 01:17:48.840]  Мы будем их вставить.
[01:18:01.160 --> 01:18:09.680]  Ну, потому что организация идет параллельно. У нас вот эти потоки стоят в одном варпе, то есть это у нас одна ассемлерная инструкция по факту.
[01:18:09.680 --> 01:18:26.680]  И поскольку это одна особенная инструкция, архитектура видеокарты сделана таким, это на уровне архитектуры видеокарты сделана таким образом, чтобы в одной особенной инструкции подгружался просто блок памяти.
[01:18:26.680 --> 01:18:33.680]  То есть получается, на одну инструкцию мы выдаем один блок.
[01:18:33.680 --> 01:18:47.680]  Да, подними. Да, да, да. А тут приходит первый поток и говорит, мне что-то другое дайте.
[01:18:47.680 --> 01:19:02.680]  Ну да, ну видели, что когда мы сделали 8 потоков, то есть каждый поток начал перезаписывать 8 раз, но мы замедлились в 3 раза.
[01:19:02.680 --> 01:19:12.680]  Ну да, то есть у нас вот она одна кашление сразу складывается, тут вторая кашление сразу подгружается.

[00:00.000 --> 00:13.100]  Кто может сказать, какой сейчас самый большой HDD диск? Вот если один диск взять, какой у него
[00:13.100 --> 00:23.000]  объем будет? Есть уже больше, есть уже 24, может быть и есть 28, но там я уже могу ошибаться. Вот
[00:23.000 --> 00:28.040]  такие вот объемы. Если этих дисков несколько сделать, из них какой-нибудь raid-massive, у нас
[00:28.040 --> 00:35.560]  будет под 100 терабайт и это будет один сервер. Точно так же с оперативной памятью. Сейчас есть сервера,
[00:35.560 --> 00:44.160]  у которых оперативная память 2-5 терабайт, поэтому то, что было большими данными лет 7-8 назад и для чего
[00:44.160 --> 00:50.160]  нужен был какой-нибудь Hadoop, сейчас для этого Hadoop на самом деле не нужен. Но когда мы выходим даже
[00:50.160 --> 00:58.000]  за эти рамки, нам все равно нужна какая-то big data. Это что касается volume, что касается остальных
[00:58.000 --> 01:04.520]  свойств. Следующее свойство variety — это то, что данные приходят из разных источников, с разной
[01:04.520 --> 01:17.840]  периодичностью, с разного объема. И тут можно взять для примера задачу оценки кредитных рисков. Вот как
[01:17.840 --> 01:23.600]  вы думаете, какую информацию он должен получить банк, чтобы понять, стоит вам выдавать кредит или
[01:23.600 --> 01:35.840]  не стоит. Ну задача решается машинным обучением, но к каким источникам подключиться нужно? Там
[01:35.840 --> 01:42.200]  есть что-то внутри банка, какие были у человека транзакции, какие у него есть вклады, депозиты,
[01:42.200 --> 01:49.320]  вот это все внутри банка достается. Что еще можно извне банка достать? Какую информацию можно достать?
[01:49.320 --> 02:07.520]  Делать что? А логи какие, что в них там хранится? А как это связано с тем,
[02:07.520 --> 02:18.920]  чтобы выдавать кредит или нет? Да, но нам нужно принять какое-то решение. Вот конкретный человек,
[02:18.920 --> 02:24.560]  что мы про него должны знать, чтобы понять, ему выдавать кредит или он нам его не отдаст и как бы
[02:24.560 --> 02:35.080]  это будет убыток. Какие были у него до этого кредитные истории, проверка на то, какие там вообще в МПЦ. Какие,
[02:35.080 --> 02:44.120]  например, данные? Кредитная история, судимость, потом еще выясняются всякие данные, которые есть
[02:44.120 --> 02:50.480]  в открытых источниках. Например, семейный статус, место работы, как часто он меняет место работы,
[02:50.480 --> 02:56.000]  куда он едет в отпуск. Это все при желании можно достать и поисследовать. И вот как раз вероятно,
[02:56.000 --> 03:03.360]  мы видим, что данные приходят с разных каких-то источников, выглядят они по-разному. И третье
[03:03.360 --> 03:14.040]  свойство velocity это то, как часто мы обрабатываем данные. То есть одно дело, если мы что-то собираем
[03:14.040 --> 03:20.800]  в течение нескольких месяцев и потом запускаем какой-нибудь код, который нам генерит раз в
[03:20.800 --> 03:27.120]  квартал отчет. Это как бы одна задача. И вторая задача это какая-нибудь рекламная система. Вы
[03:27.120 --> 03:32.960]  наверное сталкивались с таким, что вы ищете на сайте каком-нибудь интернет-магазине, например,
[03:32.960 --> 03:40.880]  я хочу купить ноутбук и через совсем маленькое время вам эти ноутбуки начинают рекомендовать
[03:40.880 --> 03:49.920]  везде. Лезет реклама на всех сайтах, это происходит максимально быстро. И здесь как раз нужна не просто
[03:49.920 --> 04:00.640]  обработка больших данных, а нужна real-time обработка. Ну и дальше, что нам нужно вообще для того,
[04:00.640 --> 04:07.000]  чтобы такую инфраструктуру построить. Вот мы уже поняли, что у нас есть жирный сервер, на нем куча
[04:07.000 --> 04:13.400]  современных дисков, куча памяти, но все равно не хватает. Что можно сделать еще? Можно масштабироваться
[04:13.400 --> 04:18.480]  вертикально или горизонтально. Вертикальное масштабирование это когда мы еще больше растим
[04:18.480 --> 04:28.680]  наш сервер. Если вы посмотрите на ценники, за какую цену можно арендовать сервера разных
[04:28.680 --> 04:34.200]  мощностей, то увидите, что там в какой-то момент стоимость начинает по экспоненте расти. То есть
[04:34.200 --> 04:39.480]  есть как бы стандартные линейки серверу, в которых цена по сути линейная, если мы конечно не берем
[04:39.480 --> 04:49.080]  GPU-шки. И есть в какой-то момент мы переходим в разряд HPC, High Performance Computer, и там цена
[04:49.080 --> 04:54.280]  просто становится заоблачной. Поэтому вертикальное масштабирование до какого-то момента оно хорошо,
[04:54.280 --> 05:03.720]  дальше не очень, и еще мы как бы теряем в надежности такой системы, потому что сервер один,
[05:03.720 --> 05:12.720]  что-то с ним случилось и все. Поэтому второй подход горизонтальное масштабирование. Мы берем
[05:12.720 --> 05:20.640]  несколько серверов обычных с обычными дисками, с обычной мощностью и создаем из них какую-то
[05:20.640 --> 05:25.920]  единую систему. И тут главное сделать так, чтобы мы когда работаем с такой системой, чтобы она была
[05:25.920 --> 05:32.400]  нам так же удобна, как вот то, что вы сейчас делаете за ноутбуком. То есть вы можете там одной кнопкой
[05:32.400 --> 05:38.360]  взять перетащить куда-то папочку, одной командой что-то скопировать, и нам нужно, чтобы система
[05:38.360 --> 05:44.640]  работала так же, а у нее под капотом было куча компьютеров между собой связанных. Это кстати
[05:44.640 --> 05:53.600]  отличие от MPI, с которым мы уже успели поработать. Как вы думаете, какие будут проблемы, если мы
[05:53.600 --> 06:11.040]  будем заниматься горизонтальным масштабированием? Взаимодействие. Какие там риски могут быть? Да,
[06:11.040 --> 06:15.920]  балансировка, синхронизация, чтобы у нас все не валилось на один сервер, 10 не простаивал рядом,
[06:15.920 --> 06:33.040]  еще. Нам нужно распределить роли между машинками правильно, и еще нам нужно думать про сеть,
[06:33.040 --> 06:38.440]  потому что когда у нас был один компьютер, он мог тупо сломаться, с ним что-то произойдет,
[06:38.440 --> 06:44.160]  мы будем его чинить. А теперь у нас появляется сеть, и чем больше у нас эта сеть, тем чаще она
[06:44.160 --> 06:48.840]  ломается. Среди вас, наверное, есть те, кто работает в больших компаниях, типа Яндекс,
[06:48.840 --> 06:56.400]  МТС, Сбер, еще чего-нибудь. У вас там есть большие дата-центры по несколько, наверное,
[06:56.400 --> 07:03.480]  сотен тысяч серверов. Как вы думаете, или может как вы знаете, как часто там происходят сбои в одной
[07:03.480 --> 07:10.880]  отдельно взятой машинке? Если взять какой-нибудь супер ЦОД типа Сбера, как там часто машинка одна
[07:10.880 --> 07:25.560]  будет ломаться? Какая-нибудь. Еще какие варианты? То есть во всем ЦОДе у тебя один раз сломалась одна
[07:25.560 --> 07:41.520]  машинка за время твоей работы. А сколько ты времени там работал? На самом деле это, видимо,
[07:41.520 --> 07:46.960]  говорит о том, что инфраструктура Сбера настолько надежная, что все, что там внутри ломается,
[07:46.960 --> 08:01.440]  вы просто этого не видите. Ломается оно, очевидно, чаще. На самом деле берите выше несколько минут,
[08:01.440 --> 08:07.840]  раз в несколько минут. Что-то происходит не так. Вот если вы возьмете любой дата-центр какой-нибудь
[08:07.840 --> 08:15.200]  и посмотрите на логи того, что происходит, и там постоянно будет перегрев чего-нибудь, битые
[08:15.200 --> 08:20.800]  сектора, мониторинг сигнализирует о том, что диски отвалились, и это происходит раз в несколько минут,
[08:20.800 --> 08:31.000]  постоянно сыпется. Да, нам нужны хелс-чеки, и нам нужно сделать что-то такое, чтобы мы как пользователи
[08:31.000 --> 08:39.040]  не видели и не реагировали постоянно на то, что происходит сбои. Сбои будут происходить всегда.
[08:46.000 --> 08:51.960]  В общем, нам нужна такая система, которая вот эту всю постоянно ломающуюся штуку от нас скроет,
[08:51.960 --> 08:58.240]  и мы будем спокойно жить. Такая система одна из нескольких есть. Первая такая система появилась
[08:58.240 --> 09:05.080]  в 2003 году, называется Google File System. Но она, как и все такие продукты Google, она была закрытая,
[09:05.080 --> 09:11.800]  можно было просто посмотреть, можно было ее там запустить, попользоваться, но посмотреть код,
[09:11.800 --> 09:17.880]  посмотреть, как она реализована, было нельзя. И появилась на основе каких-то статей, которые
[09:17.880 --> 09:26.600]  сотрудники Google написали, появилась система Hadoop, которую мы будем говорить. Кто знает,
[09:26.600 --> 09:37.280]  почему Hadoop так называется вообще? Это не аббревиатура, это просто название. Почему он
[09:37.280 --> 09:44.320]  так называется и почему у него символ желтый слоник? Потому что у одного из создателей этой системы
[09:44.320 --> 09:50.600]  был маленький сынишка, у него была любимая игрушка, желтый слоник. И вот это слово Hadoop,
[09:50.600 --> 09:56.520]  это первое слово, которое он произнес. Неизвестно, что оно обозначает, но вот это то, что он сказал
[09:56.520 --> 10:09.480]  впервые в жизни. Ну и собственно HDFS, файловая система Hadoop Distributed File System. Давайте посмотрим,
[10:09.480 --> 10:17.400]  как она устроена. Вот я уже слышал, сегодня кто-то говорил про мастер-ноду, вот она здесь у нас
[10:17.400 --> 10:24.760]  наконец-то появилась. Что у нас есть? У нас есть несколько ролей, вы сейчас видите три, на самом
[10:24.760 --> 10:30.600]  деле их чуть больше, но вот основные три. Первая роль это клиентская машина, вот она там маленькая
[10:30.600 --> 10:38.560]  изображена слева. Это та машина, на которую вы заходите, и мы с вами будем работать в основном на ней,
[10:38.560 --> 10:45.600]  то есть все остальное взаимодействие с кластером, с Hadoop будет происходить через нее. Дальше у нас
[10:45.600 --> 10:53.280]  есть две роли, это узел имен и узел данных, NameNode и Datanode. NameNode это вот этот вот сервер,
[10:53.280 --> 10:59.440]  который стоит в середине, и он собственно хранит имена, он не хранит данные, он хранит только такие
[10:59.440 --> 11:08.800]  ссылки на то, где лежат данные, то есть это такая библиотека. На Datanode хранятся данные, но при этом
[11:08.800 --> 11:16.480]  Datanode не знает о связках, то есть как эти данные между собой связаны. Данные хранятся в виде блоков,
[11:16.480 --> 11:23.200]  блоки чем-то похожи на блоки в обычной файловой системе. Кстати, кто знает,
[11:23.200 --> 11:28.160]  какой стандартный размер блока вот в файловых системах типа NTFS, FAT,
[11:28.160 --> 11:42.320]  GIGABYTE. Бывает и так, на самом деле можно настроить и так, но если взять по дефолту,
[11:42.320 --> 11:49.440]  кто-нибудь может видел. 4 килобайта, да, легко проверить. Берете какую-нибудь там флешку или диск
[11:49.440 --> 11:57.080]  обычный, загоняете на нее минимальный файл, например текстурик с одним символом, и видите,
[11:57.160 --> 12:03.160]  что такой файл реально занимает сколько-то байт, а на диске он будет занимать 4 kb сразу, сразу блок.
[12:03.160 --> 12:12.640]  В Hadoop'е блоки тоже есть, но они сильно большего размера. Тут написано 64, у нас на кластере,
[12:12.640 --> 12:24.320]  по-моему, тоже 64, в реальной жизни бывает до 512 где-то так, гигов. То есть вот такая у нас
[12:24.320 --> 12:30.800]  есть архитектура. Мы с вами заходим на клиентскую машину, на ней будут храниться наши коды,
[12:30.800 --> 12:38.800]  с нее мы будем запускать коды, которые будут лезть куда-то сюда. И вот на мастере у нас хранится
[12:38.800 --> 12:44.360]  так называемый слепок файловой системы, то есть мастер знает о том, на каких датанодах какие файлики
[12:44.360 --> 12:52.160]  лежат. Если посмотреть на эту схему, что вам здесь не нравится, какие проблемы вы здесь видите?
[12:52.160 --> 13:09.400]  Да, у нас не только все клиенты, у нас и все датаноды через одну машинку работают. То есть это
[13:09.400 --> 13:19.120]  то, что называется SPOV, Single Point of Failure. И получается, что у нас нейм-нода одна и она самая важная. Если
[13:19.120 --> 13:24.200]  что-то с ней случится, несмотря на сохранность всех данных, на кластере мы работать не сможем вообще.
[13:24.200 --> 13:34.680]  Надо что-то с этим сделать. Что можно с этим сделать? Давайте тоже разберем несколько способов. Первый
[13:34.680 --> 13:40.040]  способ это просто сделать бэкап. У нас есть одна супер дорогая машинка, давайте купим рядом еще
[13:40.040 --> 13:45.280]  одну такую же супер большую дорогую машинку вместе, их свяжем, поставим. Да, еще сеть нужно тоже
[13:45.280 --> 13:53.200]  для постоянной синхронизации, тоже какую-нибудь там гигабитную или выше даже. То есть как бы рабочий
[13:53.200 --> 14:06.000]  вариант, но дорогой. Второй способ это secondary-нейм-нода и здесь я сразу скажу, что это, наверное, самое
[14:06.000 --> 14:12.880]  неудачное название роли сервера вообще в Hadoop, потому что кажется, что это вторичная нейм-нода,
[14:12.880 --> 14:17.080]  что это какой-то бэкап, что он может заменить нейм-ноду, на самом деле ничего подобного.
[14:17.080 --> 14:23.920]  Secondary-нейм-нода это просто машинка, которая делает ровно то, что написано на слайде, то есть она
[14:23.920 --> 14:33.680]  занимается склеиванием слепка файловой системы с какими-то изменениями. То есть процесс работает так,
[14:33.680 --> 14:42.080]  у нас есть слепок файловой системы, мы взаимодействуем с Hadoop, что-то туда пишем, что-то туда удаляем и
[14:42.080 --> 14:48.800]  все это пишется в специальный файл, который называется edit-log. Потом в какой-то момент вот эта
[14:48.800 --> 14:54.680]  secondary-нейм-нода, она запрашивает этот edit-log, плюс у нее хранится слепок файловой системы и она
[14:54.680 --> 15:03.000]  делает операцию merge. И потом новый обновленный уже edit-log возвращает на нейм-ноду обратно. На нашем
[15:03.000 --> 15:08.240]  кластере, на котором мы будем заниматься, это делается раз в час. Во всяких промышленных кластерах
[15:08.240 --> 15:17.120]  это делается чаще. В общем, если с нейм-нодой что-то случится, то secondary-нейм-нода она заменить
[15:17.120 --> 15:24.520]  ее не сможет. Все, чем она сможет помочь, это то, что на ней будет лежать вот этот слепок, причем он
[15:24.520 --> 15:32.320]  будет устаревший на какое-то время, в нашем случае примерно на час, потому что edit-log последние туда
[15:32.320 --> 15:37.360]  могут не успеть докатиться, нейм-нода упадет, а вот это устаревший слепок он у нас останется.
[15:37.360 --> 15:47.600]  Вот, собственно, можете увидеть на схемке, как это все происходит. Вот secondary-нейм-нода, вот она
[15:47.600 --> 15:54.960]  запрашивает edit-log, обновляет, возвращает. Ну и вообще файловая система, вот этот слепок,
[15:54.960 --> 16:09.520]  он состоит из двух частей, собственно, сам слепок и edit-log. Это может быть физический
[16:09.520 --> 16:14.000]  сервер, это может быть виртуалка просто обычная, это может быть docker-контейнер, все что угодно.
[16:14.000 --> 16:25.680]  По сути, вот эти все ноды, они со собой представляют набор всяких там джарников и набор процессов.
[16:25.680 --> 16:33.720]  Например, у нас на кластере secondary-нейм-нода и обычная нейм-нода находятся на одном и том же сервере.
[16:33.720 --> 16:41.720]  Не делайте так в реальной жизни, но у нас это так. Вот и вот этот сервер, на котором находится
[16:41.720 --> 16:49.760]  secondary и обычная нейм-нода. Вместе с датонодами, которых у нас девять штук, кажется, и плюс
[16:49.760 --> 16:55.240]  клиент, все вот эти штуки, они собой представляют виртуалки, которые хостятся на одном железном
[16:55.240 --> 17:00.600]  сервере, который стоит в подвале КПМ. Тоже так не делайте, потому что это по сути один железный
[17:00.600 --> 17:07.160]  сервер. И реально, если что-то произойдет с этим сервером, то у нас грохнется сразу все.
[17:07.160 --> 17:20.000]  Ну, когда-нибудь мы, наверное, сделаем, когда будет возможность расширить еще больше наш кластер.
[17:20.000 --> 17:36.000]  Кстати, в какой-то момент у нас действительно будет новый кластер,
[17:36.000 --> 17:46.600]  он будет сделан лучше, он будет больше. Не знаю, успеем ли мы на вашем потоке,
[17:46.600 --> 17:54.600]  вот конкретно на ваших параллелках это опробовать. Не факт, что успеем, но к следующему к лету он точно
[17:54.600 --> 18:07.960]  будет. В общем, да, у нас там будет убунта наконец-то более новая, не четырнадцатая. В общем,
[18:07.960 --> 18:13.400]  сделали мы secondary name-node, но это разгрузило основную name-node, но ничем особо не помогло,
[18:13.400 --> 18:23.440]  поэтому думаем, что можно сделать еще. А еще у нас можно сделать вот такие два способа,
[18:23.440 --> 18:29.400]  которые в реальной жизни делается и то, и то, они одновременно применяются. HDFS Federation,
[18:29.400 --> 18:36.040]  когда у нас не одна name-node, которая контролирует все, а у нас несколько name-node, каждый из которых
[18:36.040 --> 18:42.880]  контролирует какой-то кусочек файловой системы, например, там папка users, папка с данными или еще
[18:42.880 --> 18:49.920]  что-то. И high-availability name-node, это когда у нас вместо одной name-node, у нас еще создается
[18:49.920 --> 18:57.360]  параллельно несколько штук. Чем это отличается от самого первого способа с backup-ами? Тем,
[18:57.360 --> 19:07.480]  что эти ноды, они более слабые. И еще я одну важную вещь забыл сказать, что вот этот слепок,
[19:07.480 --> 19:13.600]  он хранится в оперативной памяти. Для чего? Для того, чтобы мы могли быстрее с ним взаимодействовать.
[19:13.600 --> 19:20.600]  Так вот, у этих stand-by-node, у них он хранится на диске. То есть, stand-by-node, они более слабые,
[19:20.600 --> 19:27.080]  чем обычные. Если с обычной что-то случается, то среди stand-by-node происходит выбор лидера,
[19:27.080 --> 19:34.920]  и среди них временно выбирается основная name-node. Она будет тормозить, мы будем постоянно ходить на
[19:34.920 --> 19:38.680]  диск, чтобы к этому слепку обратиться, но по крайней мере кластер будет работать.
[19:38.680 --> 19:50.880]  Ну и теперь, что касается data-node. Файлы на data-node хранятся в виде блоков
[19:50.880 --> 19:56.600]  фиксированного размера. И для того, чтобы у нас все это надежно работало, есть механизм
[19:56.600 --> 20:02.720]  репликации. То есть, когда мы пишем что-то в HDFS, мы сразу на лету это все копируем на несколько
[20:02.720 --> 20:09.640]  серверов. И причем Hadoop устроен так, что эти копии он старается разместить как можно
[20:09.640 --> 20:14.920]  дальше друг от друга. Вот есть у этого подхода и плюсы, и минусы. Кто может сказать какие?
[20:14.920 --> 20:40.560]  Да, нужно будет очень часто и очень далеко ходить, поэтому может быть это все происходить довольно медленно.
[20:40.560 --> 20:54.600]  Вот как вы можете видеть, на этой картинке у нас некоторые блоки размещены далеко друг от друга.
[20:54.600 --> 21:05.400]  Например, вот C5 здесь и здесь, но C1 рядом. Я только что сказал, что мы их хотим разместить как
[21:05.400 --> 21:13.320]  можно дальше друг от друга. Почему они оказываются рядом? Потому что параллельно с процессами записи и
[21:13.320 --> 21:22.320]  репликации у нас еще работает такая штука как балансер. Это такой отдельный специальный процесс,
[21:22.320 --> 21:27.760]  который следит за тем, чтобы на нотах была примерно одинаковая нагрузка. И если мы видим,
[21:27.760 --> 21:34.360]  что какая-то нода перегружена, балансер начинает эти реплики переносить с одной ноды на другую. И
[21:34.360 --> 21:42.200]  там уже мы не следим за тем, чтобы реплики были размещены как можно дальше. То есть может
[21:42.200 --> 21:49.560]  быть такое, что они потом опять рядом окажутся. Ну и вообще в терминах ходупа, что такое дальше,
[21:49.560 --> 21:58.240]  что такое рядом? То есть там нигде внутри ходупа не хранится какие-то метрики расстояния между серверами.
[21:58.240 --> 22:06.960]  А что хранится? Хранится специальный конфиг, в котором мы указываем для каждого сервера стойку и
[22:06.960 --> 22:14.640]  дата-центр. И у нас есть вместо того, чтобы мерить расстояние, мы меряем уровни локальности данных,
[22:14.640 --> 22:24.040]  data-locality. Когда-нибудь слышали такое? Уровни data-locality, они бывают несколько уровней. И это не
[22:24.040 --> 22:29.200]  только касается ходупа, а вообще касается любой распределенной обработки данных. Самый
[22:29.200 --> 22:36.440]  первый, самый близкий уровень data-locality это процесс local. Вот вы пишете обычную программу на питоне,
[22:36.440 --> 22:43.120]  у вас там какие-то переменные, все они находятся в одном процессе. И это самый такой базовый уровень
[22:43.120 --> 22:50.760]  data-locality. А следующий уровень это node-local. То есть процессы уже разные, но сервер один. Дальше
[22:50.760 --> 22:59.000]  rec.local. Это одна стойка. Data-center.local такого уровня data-locality нет, сразу идет уровень any. То
[22:59.000 --> 23:12.360]  есть уже неважно нам, где находятся сервера. То есть вот так в совсем упрощенном виде у нас
[23:12.360 --> 23:17.680]  выглядят ноды. Есть нода, на ней лежат блоки, и у каждого блока есть реплика, которая лежит на
[23:17.680 --> 23:29.240]  разных нодах. Вот. И я вам только что сказал, что блоки одинакового размера, но на самом деле они
[23:29.240 --> 23:35.360]  не всегда одинакового размера. И сейчас я зайду на ходуп-кластер, и мы прямо в реальном времени
[23:35.360 --> 23:40.880]  сможем посмотреть, почему блоки бывают разные и в каком случае они бывают разные.
[23:40.880 --> 23:57.960]  Нет. Для того, чтобы отключить интернет-долгопрудном, вам нужны root-права на каком-нибудь из серверов.
[23:57.960 --> 24:04.480]  Ну и Тон, это было давно, и наверное сейчас уже так легко не получится это сделать.
[24:04.480 --> 24:14.720]  В каком году это было? В каком году это было? Ну, наверное, 16-й год. То есть реально достаточно давно.
[24:14.720 --> 24:37.080]  Скорее всего, если сейчас, если на уровне нашего центра обработки данных сработает монитор,
[24:37.080 --> 24:42.640]  что вы что-то большое качаете, ваш аккаунт просто отключит. Кстати, кто сдавал MPI,
[24:42.640 --> 24:48.760]  вы, наверное, с такой штукой сталкивались. С чем? С тем, что аккаунты на какое-то время банятся.
[24:48.760 --> 25:08.520]  Вот. Как нам попасть на нейм-ноду? Заходим на кластер, пробрасываем порт 50 070 с мастера,
[25:08.520 --> 25:15.520]  ну вот вы видите там команду вверху, и после этого заходим в браузер на вот этот порт.
[25:15.520 --> 25:40.120]  Так лучше? Хорошо. Вот. Что мы тут видим? Давайте делаем такой небольшой обзор вот этого веб-нтерфейса.
[25:40.120 --> 25:48.800]  Что мы здесь видим? Мы видим, сколько вообще хранить данных мы можем на этом кластере. Вот.
[25:48.800 --> 25:58.440]  Самая первая строчка. Чуть меньше пяти терабайт, на самом деле, не много. Потому что маленькие диски
[25:58.440 --> 26:07.200]  на машинках. Ну, на самом деле, для ваших заданий этого просто более чем достаточно, даже если вас
[26:07.200 --> 26:15.280]  будет не пять человек сейчас, а тысяча. Вот. И дальше мы видим, сколько у нас места занято.
[26:15.280 --> 26:24.120]  Сколько места занято на нон-дфс нужды, кстати, скажите, а что это вообще такое может быть? То есть
[26:24.120 --> 26:31.080]  у нас есть кластер, на нем стоит ходуб. Это набор каких-то там джава-программ, жарников и сами
[26:31.080 --> 26:38.800]  данные, собственно. Куда может уйти вот такое огромное количество ресурсов, если это не связано
[26:38.800 --> 26:52.280]  с ходубом что-то? Логи? Ну, вот логи, которые весят полтерабайта, это как-то вообще круто для
[26:52.280 --> 26:57.040]  такого маленького кластера. Да, сама система и причем не на одном сервере. То есть у нас много
[26:57.040 --> 27:03.440]  нод, на каждом из них стоит операционка, стоят какие-нибудь там сервисы, которые еще нам нужны,
[27:03.440 --> 27:11.400]  джедека, питон, разные библиотеки питона. Вот у нас будет домашка по спарку, там надо будет
[27:11.400 --> 27:16.760]  обрабатывать данные на питоне и, соответственно, нужен будет какой-нибудь пандос. И этот пандос должен
[27:16.760 --> 27:24.000]  стоять на всех узлах кластера. А пандос достаточно тяжелая штука, и там он не один стоит. Поэтому
[27:24.000 --> 27:33.920]  суммарно получается вот такой большой объем. Что еще тут интересного мы видим? Вот мы видим...
[27:33.920 --> 27:47.240]  Чего? Еще крупнее. Окей. Ну, больше, наверное, увеличивать смысла нет. Вы просто можете зайти
[27:47.240 --> 27:55.920]  на тот же интерфейс, что и я. Вот. Здесь мы видим, собственно, наши датаноды и насколько они сейчас
[27:55.920 --> 28:02.040]  все забиты данными. Видим, что они не сильно поровну, но чего-то критичного нет.
[28:02.040 --> 28:15.680]  Ну, там много кто сидит. Там сидят дипломники, шестикурсники, перездача прошлых потоков.
[28:15.680 --> 28:22.200]  Параллельно с вами еще на пятом курсе человек четыреста занимается, поэтому людей много.
[28:22.200 --> 28:31.600]  Вот. Собственно, про блоки мы будем с вами смотреть вот здесь. Вот наша файловая система.
[28:31.600 --> 28:39.560]  Находим какой-нибудь большой файл. Вот, например, файл. Ну, для нашего кластера можно сказать,
[28:39.560 --> 28:46.160]  что он большой. Он почти 12 Гигов. И давайте посмотрим, как он хранится на файловой системе.
[28:46.160 --> 28:56.440]  Вот у него фактор репликации 3. Размер блока у него 128. И вот мы тут видим свойства по каждому блоку.
[28:56.440 --> 29:04.640]  Вот блок 0. Что мы видим? Мы видим ID-шник этого блока. Мы видим блок pool ID. В нашем случае,
[29:04.640 --> 29:10.160]  на нашем кластере блок pool ID это можно сказать константа. Потому что у нас как таковой data locality
[29:10.160 --> 29:17.640]  нет. Железный сервер у нас один. Ноды все равноценные. Они не помечены никакими лейблами. Поэтому
[29:17.640 --> 29:23.120]  можно сказать, что у нас вот то, что вы видите блок pool, это константа, которая будет всегда одинаковая.
[29:23.120 --> 29:32.840]  Дальше generation stamp это, по сути, версия. То есть вот этот процесс репликации, когда идет запись
[29:32.840 --> 29:40.440]  данных файловую систему, ходу нужно каким-то образом понимать. Если он встретит версию этого файла,
[29:40.440 --> 29:46.120]  какая из этих версий новее, какая старше. Вот с помощью вот этого generation stamp это и происходит.
[29:46.120 --> 29:56.960]  Это не совсем прям ID-шник версии, но нечто похожее. Дальше размер и ноды, на которых блок хранится.
[29:56.960 --> 30:02.120]  И давайте потыкаемся в разные блоки и посмотрим, что меняется. Вот тут поменялся ID-шник,
[30:02.120 --> 30:09.080]  поменялись ноды. Опять поменялся ID-шник, поменялись ноды. Заметьте, size у нас не меняется.
[30:09.080 --> 30:19.560]  Теперь давайте идем в самый конец. И здесь у нас поменялся size. Почему? Потому что просто
[30:19.560 --> 30:26.720]  размер файла такой, что он на размер блока не делится. И еще есть второй случай. Вот если
[30:26.720 --> 30:36.400]  мы вернемся к презе. Первый случай уже понятно почему, потому что последний блок может быть меньше.
[30:36.400 --> 30:44.040]  А второй случай, если последний блок у нас совсем маленький, если он не так как сейчас, вот он
[30:44.040 --> 30:49.240]  занимает где-то процентов 70 от размера блока, а если он занимает процента 2 от размера блока,
[30:49.240 --> 30:55.080]  тогда мы его, мы не выделяем отдельный блок на него, а мы этот маленький кусочек прикрепляем
[30:55.080 --> 31:02.280]  к предыдущему блоку. Зачем так делать? Потому что про каждый блок у нас хранится так называемая
[31:02.280 --> 31:09.240]  метаинформация. Кто создатель, к какому файлу он относится, этот блок, всякие там права доступа,
[31:09.240 --> 31:15.960]  где он хранится. Вот эта вся метаинформация, она конечно небольшая, она константная для
[31:15.960 --> 31:23.080]  любого блока, но она хранится на неймноде и она хранится в оперативной памяти. Поэтому если мы
[31:23.080 --> 31:28.960]  будем создавать много вот этих маленьких блоков, то у нас будет расходоваться оперативка неймноды,
[31:28.960 --> 31:36.480]  она гораздо дороже стоит, чем диск. Поэтому если у нас последний блок очень маленький,
[31:36.480 --> 31:42.800]  мы его прикрепляем к предпоследнему, а если у вас просто маленький файл и вы хотите его
[31:42.800 --> 31:49.520]  сохранить в ходупе, то старайтесь не хранить маленькие файлы в ходупе. Потому что получается,
[31:49.520 --> 31:58.000]  что вы не расходуете диск, у вас на диске, например, занято вместо 128 мегабайт, занято 1 мегабайт,
[31:58.000 --> 32:05.280]  и диск простаивает. Но оперативка неймноды затрачивается так же, как если бы вы хранили 100 мегабайт.
[32:05.280 --> 32:16.560]  Это очень такая распространенная проблема в ходупе. Кому более интересно, загуглите small
[32:16.560 --> 32:28.160]  file problem in Hadoop, там даже комиксы про это есть. Да, если нужны маленькие данные,
[32:28.160 --> 32:33.360]  то их можно хранить прямо на клиенте. Или есть еще такая штука, про которую подробнее расскажу
[32:33.360 --> 32:39.000]  в следующий раз. Это распределенный кэш, distributed кэш. Это как раз такое хранилище,
[32:39.360 --> 32:55.560]  да, оно в ходупе, но оно не в HDFS. Кому-то еще давно пришли, кому-то сегодня я запускал рассылку,
[32:55.560 --> 33:00.120]  вам пришли вот такого типа письма. Может быть с другими портами, с другими пользователями,
[33:00.120 --> 33:04.680]  но вот через такие реквизиты вы будете ходить на кластер.
[33:04.680 --> 33:16.560]  Вот, мы переходим постепенно к чтению записи, поэтому какие-то вопросы сейчас есть по именно
[33:16.560 --> 33:22.600]  устройству файловой системы. Дальше у нас будет про чтение записи и еще посмотрим всякие примеры.
[33:22.600 --> 33:34.560]  Окей, вопросов нет, тогда давайте посмотрим на чтение. Как работает в ходу печатение? Оно работает
[33:34.560 --> 33:40.680]  в два этапа. То есть когда вам нужно прочитать какой-то файл, где у нас хранятся данные о том,
[33:40.680 --> 33:46.840]  как найти файл. Они хранятся на нейм-ноде. Поэтому мы с клиента идем сначала на нейм-ноду и нейм-нода
[33:46.840 --> 33:53.600]  возвращает нам блок location, то есть где находятся блоки, которые нам нужно читать. И следующий
[33:53.600 --> 33:59.800]  этап, это мы уже идем непосредственно в ту датаноду, куда нас отправили и возвращаемся обратно,
[33:59.800 --> 34:16.840]  уже минуя нейм-ноду. Что ловим? Нет, блокировки только на этапе записи. Вот с записью тут вообще
[34:16.840 --> 34:23.080]  все сложнее, потому что действительно есть блокировки и она еще и долго работает, потому что
[34:23.080 --> 34:30.760]  запись в ходу пей синхронная. Вот посмотрите на эту схему, как происходит запись. Мы тоже сначала
[34:30.760 --> 34:38.200]  идем в нейм-ноду, говорим, я хочу записать такой-то файл и нейм-нода, оценив размер этого файла,
[34:38.200 --> 34:45.240]  выдает нам блок range, куда нам писать. И мы начинаем писать вот таким каскадным методом. Сначала на
[34:45.240 --> 34:50.800]  первую датаноду, потом первая датанода передает данные на вторую, вторая и на третью, и пока у нас
[34:50.800 --> 34:57.640]  не закончится коэффициент репликации, то есть пока мы не сделаем столько реплик, сколько хотим,
[34:57.640 --> 35:05.440]  мы запись не закончим. И главное то, что пока мы вот эту длинную, длинный процесс записи не закончим,
[35:05.440 --> 35:12.040]  мы не сможем работать с этими данными. То есть они у нас будут доступны только тогда, когда полностью
[35:12.040 --> 35:16.960]  вся вот эта запись пройдет, а тут могут быть еще какие-нибудь падения. То есть каждая нода,
[35:16.960 --> 35:23.120]  когда мы на нее пишем файл, вот видите стрелочка вверх, в нейм-ноду. То есть датанода каждый раз
[35:23.120 --> 35:28.760]  отчитывается, что запись успешна. Если она не отчиталась, нейм-нода ждет. Если она не дожидается,
[35:28.760 --> 35:33.760]  мы начинаем писать на следующую датаноду. То есть это может занять какое-то время. Но вы
[35:33.760 --> 35:42.080]  собственно сами можете взять, выполнить какую-нибудь команду типа HDFS, dfs-put, положить файл в HDFS и посмотреть,
[35:42.080 --> 35:47.680]  что даже небольшой файл он будет записываться, ну может быть, секунды три.
[35:57.440 --> 36:03.360]  Давайте посмотрим, как вообще взаимодействовать с нашей экосистемой Hadoop, с файловой системой.
[36:03.360 --> 36:10.400]  Подробнее вам расскажут семинаристы, мы просто сделаем такой обзор. Я вам уже показал, что у HDFS
[36:10.400 --> 36:21.800]  есть WebUI, а помимо WebUI у него еще есть REST API, и мы можем с помощью HTTPS запросов файлы читать,
[36:21.800 --> 36:29.280]  писать, удалять, в общем, делать с файловой системой все. Поэтому вот этот порт 50070,
[36:29.280 --> 36:34.880]  он у нас наружу не открыт, потому что иначе можно было без авторизации делать что угодно
[36:34.880 --> 36:45.320]  в файловой системе. Помимо REST API у нас еще есть просто обычные разные API, например, есть HDFS Shell,
[36:45.320 --> 36:57.000]  вот тут есть список команд, но если вы посмотрите на этот список, то вы увидите, что большая часть
[36:57.000 --> 37:04.280]  команды, она похожа на обычные команды в линуксе, там CP, MOV, RAM, CHMOD, CHOWN, все эти команды здесь
[37:04.280 --> 37:21.560]  есть. Есть API на джава, но, наверное, мы сейчас на него особо останавливаться не будем. Как
[37:21.560 --> 37:31.720]  показывает практика, не очень многие любят джаву. Согласен, но все равно идем дальше,
[37:31.840 --> 37:40.480]  и давайте попробуем выполнить команду вот эту на кластере. То есть вот представьте,
[37:40.480 --> 37:48.040]  что нам нужно прочитать первые 10 символов с какого-то файла, и вот для этого мы курлом
[37:48.040 --> 37:56.720]  выполняем эту команду. Из чего она состоит? Давайте посмотрим. Мы подключаемся к интерфейсу
[37:56.720 --> 38:05.640]  на имноды вот по этому порту. Дальше Web HDFS version 1, ну это чисто такая практика, когда в какой-нибудь
[38:05.640 --> 38:11.640]  системе разрабатывается REST API, то сразу делают закладку под версию, потому что, может быть,
[38:11.640 --> 38:18.200]  потом будет version 2, version 3, и чтобы была обратная совместимость, чтобы не убивать прошлой версии,
[38:18.200 --> 38:26.240]  оставляют вот здесь номер версии. В ходу-то никакой version 2 не случилось, работаем под первой
[38:26.240 --> 38:35.640]  версией. Дальше у нас название файла, и потом идут через параметры get запроса разные аргументы,
[38:35.640 --> 38:45.360]  что мы с этим файлом делаем. То есть дальше есть операция открытия и чтение 10 знаков. Давайте
[38:45.360 --> 39:13.600]  проверим, как это работает. Вот, команда сработала, но мы видим не 10 символов, а вместо них мы видим
[39:13.600 --> 39:17.840]  какие-то логи. Кто может сказать, что тут произошло, если вы посмотрите на экран?
[39:21.840 --> 39:30.920]  Ну только не данные, а просто нас куда-то перенаправили. Так, а что еще там интересного мы видим?
[39:30.920 --> 39:39.240]  Да, нас перенаправили на ноду, давайте посмотрим, что выдаст вот эта ссылка, куда нас перенаправили.
[39:39.240 --> 39:48.720]  То есть что мы видим по ссылке? Нода 4, вот ее интерфейс, путь к данным, операция open,
[39:48.720 --> 39:57.480]  name-node-rpc-адрес вот этот, то есть мы видим откуда нас перенаправили, и дальше аргументы 10 символов,
[39:57.480 --> 40:10.520]  offset 0. Вот, а теперь мы уже видим, что ответ у нас 200, и мы видим, что символы мы прочитали.
[40:10.520 --> 40:22.840]  Да, на самом деле, конечно, вот так вот двухэтапный метод вручную делать не стоит, потому что у
[40:22.840 --> 40:37.040]  укурла есть еще аргумент минус L. Так, сейчас я его тут добавлю. Вот, минус L большое, авторедиректы, мы тут видим ответ сразу.
[40:37.040 --> 40:52.560]  Джавапи, давайте я покажу вам, у меня есть код.
[40:52.560 --> 41:15.520]  Ну вот, в качестве примера код, который читает первые пять символов, да вроде пять, где тут,
[41:15.520 --> 41:23.440]  вот, топ-5, пять символов с какого-нибудь файла. То есть здесь, конечно, вот этот двухэтапный
[41:23.440 --> 41:28.960]  способ, он скрыт под капот, мы просто создаем объект файл-систем, подключаемся к файловой
[41:28.960 --> 41:35.280]  системе и делаем что-то похожее, как мы взаимодействуем с обычной файловой системой. Если
[41:35.280 --> 41:42.080]  тут есть джависты, наверняка вы работали с файлами в джаве, там все, особенно если брать более старые
[41:42.080 --> 41:47.920]  версии джавы, то там все достаточно печально выглядит, чтобы подключиться к файлу, нужно
[41:47.920 --> 41:54.920]  посоздавать объектов, вложенных очень много. Ну а экосистема Hadoop, она сейчас все еще живет
[41:54.920 --> 42:01.800]  в основном на уровне джавы где-то восьмой-девятой. Есть уже, по-моему, 21 версия, но Hadoop все еще пока
[42:01.800 --> 42:10.040]  там. 22. Ну они сейчас выходят все-таки не потому, что там какие-то новые фичи появляются, какие-то
[42:10.040 --> 42:28.560]  глобальные, а по-моему, просто есть периодичность определенная. То есть теперь не нужно дергать
[42:28.560 --> 42:45.120]  джавак и проходить циклом все файлы. Отлично. Ну, кстати, я знаю очень многих джавистов, которые
[42:45.120 --> 42:50.400]  говорят, что начиная с версии седьмой, все умерло и работать с таким невозможно, поэтому мнения
[42:50.400 --> 43:12.480]  бывают разные, но вот даже Hadoop уже довольно давно отошел от седьмой версии. Вот, ну теперь если
[43:12.480 --> 43:20.880]  посмотреть на то, как выглядит HDFS Shell, то по сути это просто линуксовые команды, которые вы перед ними
[43:20.880 --> 43:37.560]  пишете префикс и немножко дольше ждете, чем в обычном линуксе. Какие еще есть обертки? Вот есть
[43:37.560 --> 43:45.800]  обертка на C++. Когда-то HDFS и сам Hadoop был сильно популярен в компании IBM, и там сделали набор
[43:45.800 --> 43:57.480]  оберток под язык C++. И есть очень много питоновых библиотек. Согласен. Чего? Какой-то вопрос? Нет,
[43:57.480 --> 44:04.680]  просто непонятно, как можно в такой системе с питоном жить. В такой системе с питоном... В такой
[44:04.680 --> 44:11.560]  системе зачем питон? Понятно, за тем, чтобы люди, которые просто хотели писать пук-пук три команды...
[44:11.560 --> 44:20.760]  Питон на самом деле тут везде, потому что вот ты, наверное, хочешь сказать, что Hadoop сложный и на
[44:20.760 --> 44:28.560]  нем тяжело писать код. Но при этом уже давно есть понимание, что большие данные, где больше всего
[44:28.560 --> 44:34.800]  применяются большие данные? Это... Чего? Ну и искусственный интеллект, если широко брать. То есть
[44:34.800 --> 44:43.960]  там и data science, машинка, всякая биоинформатика, направление типа умный дом, рекомендательные
[44:43.960 --> 44:51.960]  системы, вот это все большие данные. То есть там везде нужен анализ данных. С чем до Hadoop,
[44:51.960 --> 44:59.320]  с чем работали data scientists, data analytics? Это, по сути, питон. Pandas, Matplotlib, NumPy, вот эти вот основные
[44:59.320 --> 45:06.040]  библиотеки. И есть понимание, что для того, чтобы вот эта вся сложная система стала популярна в
[45:06.040 --> 45:12.720]  большом бизнесе, надо, чтобы это все умело работать с питоном. Поэтому Hadoop и Spark, и Hive, все,
[45:12.720 --> 45:19.360]  что мы будем изучать, оно умеет работать с питоном. И, собственно, здесь есть тоже вот разные
[45:19.360 --> 45:26.280]  библиотеки. Я привел четыре, хотя их на самом деле намного больше. И вот библиотека HDFS CLI,
[45:26.280 --> 45:33.280]  она самая простая в плане работы именно с HDFS. То есть если вы ее настроите, на семинарах вам покажут,
[45:33.280 --> 45:38.840]  как ее настроить. Там определенный конфиг нужно сделать, небольшой. И вы сможете с Hadoop
[45:38.840 --> 45:43.480]  взаимодействовать так же просто, как просто с обычным файлом. То есть we is open и поехали.
[45:43.480 --> 45:52.480]  Вот, например, есть EmerJob. Это такая мощная библиотека, которая умеет MapReduce задачи запускать,
[45:52.480 --> 45:56.960]  умеет с HDFS работать. То есть вам вообще не нужна будет никакая Java, никакой Bash,
[45:56.960 --> 46:04.240]  если вы будете работать с EmerJob. Но у нее довольно сложные порог хождения. То есть когда я начинал
[46:04.280 --> 46:08.680]  с ней работать, там довольно тяжело было ее настраивать и вообще понять, почему она
[46:08.680 --> 46:17.040]  все время падает. Поэтому на нашем курсе EmerJob мы с ней работать не будем, а вам семинаристы покажут
[46:17.040 --> 46:32.040]  другие инструменты. Вот, ну и если у вас сейчас с собой ноутбуки, вы можете в принципе проделать
[46:32.040 --> 46:37.200]  сами вот эти команды и убедиться, что мы реально работаем с разными файловыми системами. То есть вы
[46:37.200 --> 46:44.880]  зашли на сервер, сделали ssh-mipt-клиент, и у вас при этом есть одна файловая система,
[46:44.880 --> 46:52.120]  которая просто обычный Linux внутри сервера, и есть HDFS, которая совсем другая файловая система и в
[46:52.120 --> 47:01.720]  которой лежат другие данные. И вот можно даже увидеть, какой коэффициент репликации у каждого
[47:01.720 --> 47:10.400]  файла, если вы выполните вот этот du-h. Кто знает вообще команду du, что она делает? Она и в обычном Linux есть.
[47:10.400 --> 47:19.200]  Ну да, если есть su, то больше ничего не надо.
[47:19.200 --> 47:29.200]  Это ты имеешь в виду завершение циклов, Баша?
[47:29.200 --> 47:35.800]  Диск usage, du.
[47:46.040 --> 47:51.280]  Так, может сейчас какие-то вопросы есть, мы как-то довольно быстро идем.
[47:59.200 --> 48:10.520]  Ну вот давайте тогда немного поисследуем файловую систему с помощью FSTK. Опять же вопрос,
[48:10.520 --> 48:15.040]  FSTK это вообще что? Кто знает, что это такое? Безотносительно Hadoop.
[48:15.040 --> 48:29.040]  Да, а ЦК? А у кого-нибудь было такое, что вот на ноутбуке какая-то проблема с диском,
[48:29.040 --> 48:34.400]  вы запускаете какой-нибудь чекер, чтобы проверить битые сектора или там какие-то ошибки исправить?
[48:34.400 --> 48:41.680]  На винде есть сервис проверки дисков тоже.
[48:41.680 --> 48:53.600]  Но она не то что чинит это утилиты, она просто мониторит какие ошибки есть,
[48:53.600 --> 49:01.080]  то есть собственно File System Check. И в Hadoop тоже есть File System Check. Он не столько для того,
[49:01.080 --> 49:06.280]  чтобы увидеть какие ошибки, потому что в Hadoop, если мы берем реальный Hadoop систему,
[49:06.280 --> 49:10.840]  там всегда что-нибудь не так, всегда каких-то не хватает блоков, потому что какой-нибудь сервер
[49:10.840 --> 49:16.520]  лежит и заменить его еще не успели. Скорее FSTK нужно для того, чтобы поисследовать,
[49:16.520 --> 49:21.560]  где какие блоки хранятся, как это все вообще сказывается на файловой системе.
[49:21.560 --> 49:23.760]  Давайте такую команду проверим.
[49:23.760 --> 49:42.360]  Вот что у нас лежит в этой папке, мы сейчас для нее выполним вот эту вот команду.
[49:53.760 --> 50:09.280]  Так, вот у нас получилась такая штука, это Summary. Давайте листать наверх,
[50:09.280 --> 50:14.440]  листаем, листаем, листаем, листаем, долго листаем и давайте посмотрим, что у нас вообще тут есть.
[50:14.440 --> 50:22.160]  Нет, это у нас ничего не сломалось, это просто файл большой и блоков много.
[50:22.160 --> 50:29.880]  Ну это просто данность. Мы сейчас будем смотреть, это хорошо или не очень хорошо.
[50:29.880 --> 50:36.800]  Что мы тут видим? Сначала мы видим, что у нас тут вот такая папка, вот такой файл и мы его
[50:36.800 --> 50:45.560]  начинаем исследовать. Нет, это просто папка в HDFS, которая вики, в которой лежит кусочек википедии.
[50:45.560 --> 50:55.360]  Вот такой размер у нас файла, 92 блока и дальше мы начинаем про каждый блок чего-то узнавать.
[50:55.360 --> 51:01.880]  Что мы тут видим? Блок Pull, как я уже сказал, это константа, потом у нас идет ID-шник блока,
[51:01.880 --> 51:08.120]  потом идет версия блока, потом идет его размер, количество реплик 3, вот LiveReplex,
[51:08.120 --> 51:17.840]  DataNodeWithStorage, вот он тоже есть и здесь мы видим про каждую реплику информацию. Что мы видим?
[51:17.840 --> 51:24.080]  Мы видим IP-шник, это IP-шник датоноды, на которой лежит блок, потом мы видим ID-шник,
[51:24.080 --> 51:30.760]  это собственно ID-шник тоже датоноды, почему два идентификатора? Потому что IP может меняться,
[51:30.760 --> 51:36.360]  вообще может меняться все в Hadoop Cluster, может поменяться IP, может поменяться hostname,
[51:36.360 --> 51:43.000]  а Hadoop Cluster при этом должен работать, поэтому нам нужен ID-шник. Дальше у нас идет, насколько
[51:43.000 --> 51:48.920]  видишь, вот идентификатор диск, но это собственно обозначает, что данные хранятся на диске,
[51:48.920 --> 51:55.120]  потому что они могут храниться не только на диске, а еще допустим в докере, еще в каком-нибудь
[51:55.120 --> 52:02.240]  S3-хранилище, в оперативке, но вот у нас диск и вот такую штуку мы видим по каждому блоку.
[52:06.360 --> 52:10.640]  Листаем, листаем. Размер у нас все время одинаковый, кроме вот…
[52:10.640 --> 52:16.160]  Получается вопрос по тому, что мы собрали блок в профессии, но только теперь записано вот таким
[52:16.160 --> 52:23.120]  обзором. Здесь больше информации. Да, вот видите размер поменялся. Здесь больше информации,
[52:23.120 --> 52:32.920]  потому что вот мы видим вот эту вот статистику. Over-replicated, under-replicated, то есть если у нас не все
[52:32.920 --> 52:41.560]  в порядке с репликами. Вот, кстати, кто говорил, что это хорошо, что у нас много выводов, хорошо
[52:41.560 --> 52:47.720]  это вот здесь, когда тут стоят нули и нет никаких проблемных блоков, потому что вот under-replicated,
[52:47.720 --> 52:57.720]  ну понятно, упала нода, вместе с ней пропала реплика. А вот это я у вас хочу для начала спросить.
[52:57.720 --> 53:16.680]  Реплика, можем поменять. Нет, они удаляются. Они сразу удаляются, запускается процесс
[53:16.680 --> 53:24.440]  удаления. И over-replicated, это не то что у нас как бы больше фактор репликации стал, а это именно
[53:24.440 --> 53:30.880]  отличие от того, что мы хотим. Вот мы хотим, чтобы было три, а у нас реально хранится четвертая реплика.
[53:30.880 --> 53:43.200]  Вот, какие варианты? Откуда она может взяться? Гарбич коллектор? Да, только он с данными напрямую
[53:43.200 --> 53:53.480]  не работает. Он работает с... ГЦ, он удаляет всякие объекты в оперативной памяти, которые
[53:53.480 --> 53:59.680]  хранятся в java-процессах, а это данные, они лежат на диске. Я ничего-то не видел, чтобы гарбич коллектор
[53:59.680 --> 54:09.960]  стирал что-то с диска. А не может быть так, что там сборы по сети? Так. И из-за этого получилось так,
[54:09.960 --> 54:18.040]  что мы дважды создали реплику. То есть мы начали создавать реплику, сеть обрушилась, нам надо
[54:18.040 --> 54:24.680]  создать новую. Мы пошли на новую, теперь у нас четыре реплика. Именно, да. То есть это может быть сбои сети, это
[54:24.680 --> 54:32.360]  может быть какие-то временные выпадения машинок. Когда машинка выпала, hard-бит Наимноду переслать не
[54:32.360 --> 54:40.360]  успела, Наимнода решила, что машинка упала и создала реплику где-то еще. Потом машинка проснулась и говорит,
[54:40.360 --> 54:46.200]  у меня тоже реплика есть. И получается, у нас лишняя реплика. В таком случае у нас все равно
[54:46.200 --> 54:50.880]  через какое-то время запускается балансер. И балансер, помимо того, что он двигает реплики между
[54:50.880 --> 54:57.120]  собой, чтобы сбалансировать, он еще и удаляет лишнее и создает, если что-то не хватает. То есть он
[54:57.120 --> 55:03.480]  исправляет проблемы, если его правильно настроить, он исправляет проблемы с under-replicated и over-replicated.
[55:03.480 --> 55:19.160]  Бывает просто такое, что у нас реплика есть, но мы к ней не можем подключиться, допустим.
[55:19.160 --> 55:34.460]  Ну и можем точно также выполнить команду для какого-нибудь одного блока и посмотреть
[55:34.460 --> 55:51.900]  информацию про него. Вот это мы здесь видим примерно то, что мы видели в веб-интерфейсе,
[55:51.900 --> 55:56.580]  то есть где хранится блок, к какому файлу он относится, на каких нодах и какой у него статус.
[56:04.460 --> 56:15.020]  Теперь давайте мы с вами попробуем решить задачку, которую часто решают ходуб админы. И вообще те,
[56:15.020 --> 56:23.420]  кто хочет в компании создать ходуб, его допустим еще не было, но стоит задача, чтобы в компании
[56:23.420 --> 56:29.260]  ходуб появился и понятно, сколько данных мы там хотим хранить. Вот нам нужно спланировать
[56:29.260 --> 56:35.620]  нейм-ноду. Давайте посмотрим на условия. То есть у нас объем всех дисков кластера вот такой,
[56:35.620 --> 56:45.180]  размер блока, метаинформацию и фактор репликации мы тоже знаем. И давайте мы оценим минимальный
[56:45.180 --> 56:52.060]  объем оперативки. Это, кстати, тоже вопрос, почему мы не можем сказать точно, каким он должен быть,
[56:52.060 --> 57:12.820]  почему мы только оценить можем. Давайте считать, что мы рассчитываем оперативную память только
[57:12.820 --> 57:19.300]  полезную для хранения вот этого слепка файловой системы. То, что у нас сверху там еще есть операционка,
[57:19.300 --> 57:28.540]  Java, GC обязательно. Вот это мы как бы сейчас учитывать не будем. Вот именно для хранения слепка.
[57:28.540 --> 57:39.100]  Как нам это посчитать? Это 2 петабайта, да? Да, 2 петабайта. А также сколько? Это после? Тера, потом пета, да.
[57:39.100 --> 57:56.500]  Получается на блок у нас 600 байт, оценим 1 килобайт. Это, короче, можно так, погрешность,
[57:56.500 --> 58:07.460]  можно сказать 64. Минимальный? Ну ладно, 65 мегабайт на 1 блок. У нас на 1 блок делаются 3 реплики,
[58:07.460 --> 58:18.740]  значит у нас 64 нужно умножить на 4. Почему на 4? Ну типа файл, еще к нему 3 реплики. А, реплики именно
[58:18.740 --> 58:33.900]  создаются учитывая этот файл, то есть всего 3. Ну оценим как на 4, чтобы умножать на 2. Ну это 2 в 2, это 2 в 6, 2 в 8, 128 мегабайт.
[58:33.900 --> 58:39.900]  Ты просто расскажи последовательность действий, как ты считаешь, то есть считать в уме петабайта не нужно.
[58:39.900 --> 58:51.300]  У нас есть блок. Мы считаем, насколько у нас на каждый блок тратится память. Это размер блока плюс метод информации.
[58:51.300 --> 59:06.100]  На каждый блок у нас реплики, поэтому нам нужно умножить на количество реплик. А вот это все, это что еще раз?
[59:06.660 --> 59:13.220]  Сейчас я скажу. Нет, мы нашли сначала блок. Вот размер блока. У нас есть размер 7 диск. Нам нужно
[59:13.220 --> 59:19.300]  посчитать количество таких блоков, которые было найдено. Главное учесть, что на вот этих 2 петабайтах
[59:19.300 --> 59:25.100]  мы должны хранить не только сам блок, а его реплики тоже. Нам же их больше негде хранить. Так.
[59:25.100 --> 59:48.820]  Умножить на 3, размер диска разделить на вот это число, так? Да, число блоков получим.
[59:48.820 --> 01:00:06.580]  4 килобайта это блок в обычной файловой системе. А у нас метод информация вот написана.
[01:00:06.580 --> 01:00:23.100]  Вот такая у нас получается штука. А теперь скажите, в каком случае будет вот такой вот идеальный
[01:00:23.100 --> 01:00:29.140]  вариант? То есть у нас есть 2 битабайта. Нам принесли кучу данных, разложенных как попало,
[01:00:29.140 --> 01:00:34.380]  как нам их правильно упаковать, чтобы минимизировать затраты на вот нейм-ноду.
[01:00:34.380 --> 01:00:53.580]  А как мы можем неравномерно распределить? Если мы напрямую, вот когда мы что-то пишем в ходу,
[01:00:53.580 --> 01:00:59.780]  мы напрямую не влияем на то, как оно по блокам будет распределяться. Наше дело просто там HDFS,
[01:01:00.740 --> 01:01:08.500]  запулили в ходу. И нам нужно понять, как эти файлы правильно упаковать, какого они должны быть размера.
[01:01:08.500 --> 01:01:17.780]  Каждый файл, получается, нужно делать кратный размер блока, чтобы его можно было нормально,
[01:01:17.780 --> 01:01:24.420]  оптимально положить. Не было так, чтобы один блок заходил там 1 мегабайт. Да, то есть это должно
[01:01:24.420 --> 01:01:32.940]  быть или равен блоку, или кратен размер блока, или такой отдельный вариант, что у нас весь файл
[01:01:32.940 --> 01:01:38.540]  размера 2 питабайта. Если у нас огромный файл какой-то, в ходу сам разберется, как правильно его разложить.
[01:01:38.540 --> 01:01:48.620]  И еще такой вопрос со звездочкой, а можем ли мы сделать так, что память на нейм-ноде будет
[01:01:48.620 --> 01:01:56.500]  еще меньше, чем тут написано. Тоже не надо говорить про джаву, про оптимизацию к ГЦ и вообще
[01:01:56.500 --> 01:02:02.980]  работу с операционкой. Именно с данными мы можем вот как-то их еще лучше упаковать, так чтобы затраты
[01:02:02.980 --> 01:02:13.620]  были меньше. А какая разница, они все равно туда лягут в ту же систему.
[01:02:18.620 --> 01:02:31.340]  И куда мы будем кэш складывать?
[01:02:31.340 --> 01:02:45.620]  Принципе ок, но вот опять же если мы считаем, что данные мы можем только перепаковывать, как-то резать,
[01:02:45.620 --> 01:02:56.060]  то есть сжать мы их и сделать, чтобы глобальная сумма была меньше, мы не можем. А вот я минут 20
[01:02:56.060 --> 01:03:03.020]  назад говорил, что есть случай, когда вот размер блока, а после него идет маленький кусочек блока,
[01:03:03.020 --> 01:03:11.420]  который прикрепляется к предпоследнему. Если помните, такое было, что вот есть файл,
[01:03:11.420 --> 01:03:18.180]  мы его разбили на блоки и последний блок обычно меньше. Если он сильно меньше, то чтобы не расходовать
[01:03:18.180 --> 01:03:24.940]  оперативку на им-ноды, мы вот этот маленький кусочек прикрепляем к предпоследнему блоку и
[01:03:24.940 --> 01:03:32.220]  предпоследний блок становится чуть больше. Так можно здесь на этом сыграть и разбить файлы так,
[01:03:32.220 --> 01:03:39.020]  что у них будет размер не 64 мегабайта, а допустим 64 и 5, ну то есть еще какой-то маленький кусочек.
[01:03:39.020 --> 01:03:45.260]  И тогда ходу эти маленькие кусочки прикрепит к блоку и у нас будет размер блока чуть больше,
[01:03:45.260 --> 01:03:50.980]  будет не 64, а больше, и соответственно оперативки тут будет немного меньше. Это
[01:03:50.980 --> 01:03:55.260]  будет совсем чуть-чуть, скорее всего даже не ощутимо, но вот такой способ есть.
[01:03:55.260 --> 01:04:02.300]  Есть какие-нибудь вопросы сейчас?
[01:04:02.300 --> 01:04:15.700]  Это действительно редко используют, просто желательно, чтобы вы как бы знали,
[01:04:15.700 --> 01:04:22.380]  что такое есть, а по сути на разных ходу кластерах вот этот порог может быть маленький,
[01:04:22.380 --> 01:04:27.860]  плюс его довольно тяжело задавать, он задается не в конфигах настройки, а он задается в самом коде
[01:04:27.860 --> 01:04:33.700]  ходу. Возможно в последних версиях это поправили, надо посмотреть, но вообще если
[01:04:33.700 --> 01:04:38.740]  брать вот ходу версии 2.6, с которой мы сейчас работаем, то там этот порог он внутри кода,
[01:04:38.740 --> 01:04:45.380]  и если мы хотим вот этот порог поменять, нам надо пересобирать все. Где-то это пересобирают,
[01:04:45.380 --> 01:04:50.540]  где-то нет, где-то эту функцию вообще отключили, поэтому это просто вот такая штука,
[01:04:50.540 --> 01:04:54.180]  ее интересно знать, если на собесе спросят, тоже можно ответить, будет круто.
[01:04:54.180 --> 01:04:59.500]  Еще какие-нибудь вопросы, может быть есть?
[01:04:59.500 --> 01:05:15.300]  Окей, тогда на сегодня осталось только два момента. Первое, я вам хочу показать,
[01:05:15.300 --> 01:05:23.180]  как эти файлы хранятся на самой датоноде и на неймноде. Более подробно вам покажут на семинарах,
[01:05:23.180 --> 01:05:34.300]  но я вам сейчас просто структуру этих файлов покажу. Если мы выполняем какие-то команды вот такого
[01:05:34.300 --> 01:05:40.860]  типа, то мы видим обычная файловая система, вот есть файлы, у них есть какие-то метаданные,
[01:05:40.860 --> 01:05:53.620]  где они хранятся. Давайте пойдем для этого на ноду. Вот мы оказались на ноде. Это тоже обычный
[01:05:53.620 --> 01:05:59.460]  сервер под линуксом, неважно, как он устроен технически, это может быть docker-container,
[01:05:59.460 --> 01:06:05.420]  это может быть виртуалка, кстати, у нас это виртуалка, но все равно это линукс. Где тут найти ходуб?
[01:06:05.420 --> 01:06:16.220]  Есть в корне папочка DFS, здесь у нас есть датонода, и меня сюда не пустят, поэтому я получу
[01:06:16.220 --> 01:06:30.340]  рута. Вот датонода, дальше вот наш блок pool, он у нас один, вот наши блоки, и вот тут в папке
[01:06:30.340 --> 01:06:37.740]  finalized можно найти блоки, которые на этой ноде хранятся. Как они хранятся? Давайте посмотрим. Это просто
[01:06:37.740 --> 01:06:46.580]  вот набор папок, сабдир, сабдир, сабдир. Как они мапятся с реальными папками файловой системы,
[01:06:46.580 --> 01:06:52.900]  вот эта датонода об этом ничего не знает. Если мы зайдем в какой-нибудь сабдир,
[01:06:52.900 --> 01:07:02.380]  какой-то у нас сабдир 74, допустим. Что мы тут видим? Мы видим тут опять сабдиры, то есть вот эти
[01:07:02.380 --> 01:07:08.700]  вот сабдиры безымянные друг в друга вложенные, и если мы по этим сабдирам погуляем, внутри будут
[01:07:08.700 --> 01:07:16.020]  храниться сами блоки, но блоки будут собой представлять просто бинарные файлы,
[01:07:16.020 --> 01:07:31.100]  которые как бы название у них будет равно айдишнику. Давайте посмотрим. Скорее всего будет ошибка просто.
[01:07:31.100 --> 01:07:43.660]  Вот сабдир 102, тут что-то есть.
[01:07:51.660 --> 01:07:58.980]  Там, так как это не какой-то исполняемый файл, то понятно, что ничего тут не будет.
[01:07:58.980 --> 01:08:05.420]  Давайте посмотрим.
[01:08:05.420 --> 01:08:27.340]  Тут, смотрите, тут еще какая-то псевдографика, то есть мы тут все даже прочитать не сможем.
[01:08:27.340 --> 01:08:36.140]  Это, кстати, может быть лог, потому что часто сам Hadoop, он свои же логи пишет и сохраняет в HDFS.
[01:08:36.140 --> 01:08:40.020]  Я же не знаю, что это за файл мы открыли, мы просто какой-то рандомный блок открыли.
[01:08:40.020 --> 01:08:48.380]  Да, интересный вопрос, почему так?
[01:08:48.420 --> 01:09:04.020]  Скорее всего, это связано с тем, что мы можем положить в HDFS в том числе исполняемые файлы,
[01:09:04.020 --> 01:09:09.540]  и вот если вы зайдете в HDFS, посмотрите просто разные команды, то вы увидите,
[01:09:09.540 --> 01:09:16.460]  что большинство файлов, у них там права как бы RX сразу. И для папок, и для файлов. Нет различий.
[01:09:16.460 --> 01:09:33.900]  Сам Hadoop его выполнить не сможет через файловую систему, а вы можете зайти на ноду и что-то выполнить.
[01:09:33.900 --> 01:09:46.020]  На самом деле, в Hadoop подобных системах у них с безопасностью очень много всяких
[01:09:46.020 --> 01:09:51.500]  дырок, потому что когда такие системы разрабатывали, думали про отказоустойчивость,
[01:09:51.500 --> 01:09:57.300]  надежность в плане вычислений, но не в плане безопасности. Поэтому в компаниях,
[01:09:57.300 --> 01:10:03.180]  если есть Hadoop, то поверх него накручивают очень много продуктов, связанных с безопасностью,
[01:10:03.180 --> 01:10:10.540]  как минимум. Kerberos всегда есть. Ну и давайте еще посмотрим на нейм-ноду.
[01:10:10.540 --> 01:10:39.060]  Вот смотрите, что мы тут видим. Мы тут видим папку SNN и папку NN, потому что конкретно на этом
[01:10:39.060 --> 01:10:43.500]  кластере, как я уже сказал, у нас и нейм-нода, и секонд и нейм-нода находятся на одной машинке.
[01:10:43.500 --> 01:10:58.820]  Ну соответственно перестанет работать файловая система.
[01:11:09.060 --> 01:11:26.660]  Вот давайте посмотрим, что у нас есть. У нас есть FS Image и есть файлики edit, edit, edit. Вот посмотрите,
[01:11:26.660 --> 01:11:32.100]  каждый час у нас появляются новые эдиты и потом они переходят на секонд и нейм-ноду,
[01:11:32.100 --> 01:11:37.340]  там мержатся и появляется новый FS Image. Вот мы тоже видим эти файлы.
[01:11:37.340 --> 01:11:49.420]  Нет, они тут хранятся. Они удаляются из оперативной памяти, то есть вот эта вся штука,
[01:11:49.420 --> 01:11:56.540]  точнее не вся, а вот FS Image актуальный, он хранится в оперативке, а на диск он скидывается уже в виде
[01:11:56.540 --> 01:12:17.620]  бэкапа. Это где? Ну это просто файл с версией текущего слепка файловой системы. Это в общем
[01:12:17.620 --> 01:12:22.980]  что-то типа того, как вы, возможно, видели какая у вас есть какая-то программа, и там есть файл,
[01:12:22.980 --> 01:12:29.540]  который называется PID, и там записан ID шник текущего процесса, в котором она работает.
[01:12:29.540 --> 01:12:31.420]  Вот здесь записана точно также версия.
[01:12:31.420 --> 01:12:58.220]  Обычно ставят, если у нас какая-то директория, то на нее ставят права RX,
[01:12:58.220 --> 01:13:06.060]  потому что один просто X, ну в реальной жизни смысла нету ставить права на
[01:13:06.060 --> 01:13:08.300]  исполнение, потому что нам надо как минимум прочитать.
[01:13:08.300 --> 01:13:37.140]  На сегодня последний слайд, вот несколько дополнительных статей,
[01:13:37.140 --> 01:13:42.340]  что еще можно прочитать про HDFS. Вот первая статья, что в действительности делает secondary
[01:13:42.340 --> 01:13:49.380]  name-node. Вот картинки, которые я вам про нее показывал, как раз брал отсюда, и тут можно подробнее
[01:13:49.380 --> 01:13:57.860]  прочитать. Потом вот статья про архитектура HDFS, это то, что я показывал вам последние 5 минут,
[01:13:57.860 --> 01:14:06.140]  и то, что на семинарах вам покажут подробнее, только еще в более развернутом виде. Вот такой сайт,
[01:14:06.140 --> 01:14:13.020]  он довольно страшно выглядит, потому что компании Hortonworks вот этой, которая ввела блог, нету. Есть
[01:14:13.020 --> 01:14:24.700]  только вот такая вот китайская копия этого сайта. Сейчас она откроется. Ну очень медленный интернет,
[01:14:24.700 --> 01:14:31.340]  можете отдельно кликнуть по ссылке, посмотреть. В общем, там будет описание вот этой структуры,
[01:14:31.340 --> 01:14:43.580]  которая внутри папок DFS DN, DFS NN, вы там все это сможете увидеть. Дальше еще одна статья уже более
[01:14:43.580 --> 01:14:51.180]  такая научная теоретическая от Константина Швачко, это один из коммитеров сам ходуб, и там просто
[01:14:51.180 --> 01:14:59.140]  описано, почему HDFS работает именно так, с пояснениями, доказательствами, с формулами. И книжка
[01:14:59.140 --> 01:15:11.540]  The Definition Guide, вот глава номер 3 про HDFS. Книжка достаточно старая, ей уже наверное лет 9,
[01:15:11.540 --> 01:15:18.420]  но она интересна тем, что там про каждую из самых популярных ходуб-систем хотя бы по чуть-чуть
[01:15:18.420 --> 01:15:25.260]  написано. То есть HDFS, ходуб, Spark, Hive, HBase, вот эти все системы написано, как они устроены,
[01:15:25.260 --> 01:15:30.020]  для чего нужны, где применяются. Тут такое в качестве обзора можно почитать.
[01:15:30.020 --> 01:15:34.980]  Вот, на этом на сегодня все. Какие-то вопросы есть?

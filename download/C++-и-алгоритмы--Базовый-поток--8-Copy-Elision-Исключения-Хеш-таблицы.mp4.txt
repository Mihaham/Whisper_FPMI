[00:00.000 --> 00:14.160]  Так, давайте тогда начнем. Закончим то, на чем мы остановились в прошлый раз. Мы хотели поговорить
[00:14.160 --> 00:22.560]  про Copy and Leash. Напомню, что Copy and Leash — это некоторая оптимизация компилятора, которая делает
[00:22.560 --> 00:27.640]  вызов move-конструктора, так скажем, несколько затруднительным. А именно Copy and Leash полностью
[00:27.640 --> 00:32.320]  убирает ненужные копирования и перемещения в случае, если он может или считает это целесообразным
[00:32.320 --> 00:37.960]  сделать. Ну вот, например, давайте еще раз вернемся. Вот пример один, первый блок. Значит, здесь,
[00:37.960 --> 00:44.080]  понятное дело, что у компилятора нет никакого выбора. Он обязан сделать перемещающий, он обязан
[00:44.080 --> 00:49.320]  здесь вызвать перемещающий конструктор в строке 47. Почему? Потому что вы создали объект A, и дальше
[00:49.320 --> 00:53.800]  говорите, что из объекта A вам нужно что-то переместить в B. То есть вы, грубо говоря, утверждаете,
[00:53.800 --> 00:59.360]  что вам нужно одновременно и объект A, и объект B. Ну просто содержимое объекта A вы перенесли в
[00:59.360 --> 01:05.120]  объект B. Ну и действительно, если посмотрим на вывод 1, то действительно мы увидим, что вызван
[01:05.120 --> 01:10.200]  default-конструктор, и дальше вызван move-конструктор. Все немного интереснее, если, допустим, посмотрим на
[01:10.200 --> 01:15.520]  блок номер 3. Здесь мы вроде как ожидаем то же самое. То есть я напомню, что функция F просто-напросто
[01:15.520 --> 01:21.400]  берет и возвращает A. То есть просто создает в return statement объект типа A и возвращает его.
[01:21.400 --> 01:27.640]  Вот. И здесь мы чего ожидаем? Мы ожидаем, что внутри функции F нам создастся некоторый объект,
[01:27.640 --> 01:33.560]  точнее, функция F создаст некоторый временный объект, и который дальше будет перемещен в объект
[01:33.560 --> 01:37.920]  A. То есть, собственно, то, чего мы ожидаем. Но на самом деле здесь происходит иначе. Здесь вызывается
[01:37.920 --> 01:42.560]  всего лишь один default-конструктор. То есть объект, который создается функцией F, он создается
[01:42.560 --> 01:47.080]  непосредственно в том месте, где он в итоге пригодится. То есть компилятор убирает все вот эти
[01:47.080 --> 01:52.280]  ненужные объекты и создает его там, где мы, собственно, попросили. Ну и то же самое происходит с функцией
[01:52.280 --> 01:58.400]  G. В функции G мы создаем локальную переменную A и потом ее возвращаем. Вот. Он возвращается в виде
[01:58.400 --> 02:03.800]  временного значения, и дальше это временное значение должно, по сути, как мы ожидаем, переместиться в
[02:03.800 --> 02:08.560]  объект A в строке 65. Но тем не менее это не происходит. Происходит всего лишь одно default-конструирование.
[02:08.560 --> 02:11.920]  И вот удивительным образом оказывается, что этот default-конструирование — это то конструирование,
[02:11.920 --> 02:18.200]  которое происходит внутри функции G. То есть локальный объект внутри функции G, он, по сути,
[02:18.200 --> 02:23.920]  является тем же самым, что и объект в строке 65. То есть локальный объект внутри функции G
[02:23.920 --> 02:28.320]  создает именно тот объект, который нужно. Вот. Соответственно, если тут поэкспериментировать,
[02:28.320 --> 02:33.920]  например, давайте что мы тут можем сделать. Давайте выведем на экран адрес переменной A.
[02:33.920 --> 02:47.120]  И внутри функции G. Тоже возьмем локальную переменную и выведем ее адрес.
[02:47.120 --> 03:05.280]  И в итоге пример номер 4. Посмотрим, например, номер 4. Вот. И что мы здесь увидим? Смотрите,
[03:05.280 --> 03:08.840]  мы здесь вызываем функцию G. Вот. Посмотрим на default-конструктор. То есть сначала у нас
[03:08.840 --> 03:13.440]  внутри функции G вызывается default-конструктор. Это первая строка. Дальше вводится адрес локальной
[03:13.440 --> 03:17.760]  переменной, а потом мы вводим адрес переменной, который находится внутри функции main. И оказывается,
[03:17.760 --> 03:21.120]  что локальная переменная внутри функции G и локальная переменная внутри функции main
[03:21.120 --> 03:24.920]  обладают одним и тем же адресом. То есть, грубо говоря, это один и тот же объект. То есть, казалось бы,
[03:24.920 --> 03:29.280]  мы привыкли к тому, что у нас есть как бы стэк вызовов. То есть есть стэк функции main. Дальше идет
[03:29.280 --> 03:32.880]  стэк функции G, в котором создаются локальные переменные. И после того, как мы выходим из
[03:32.880 --> 03:37.120]  функции G, у нас стэк слопывается, и все локальные переменные функции G уничтожаются. В случае копии
[03:37.120 --> 03:44.040]  legion, такого не происходит. То есть мы работаем с одним и тем же объектом. Ну и, собственно,
[03:44.040 --> 03:50.160]  копии legion — это такая штука, которая на самом деле работает даже тогда, когда вы явно запретили как-то
[03:50.160 --> 03:55.120]  копировать или перемещать ваш объект. Вот представьте себе, что у вас есть структура A, в которой
[03:55.120 --> 04:00.760]  вы явно совсем запретили копировать ваши элементы. И кроме того, что запретили копировать ваши элементы,
[04:00.760 --> 04:06.520]  вы еще запретили их перемещать. То есть вы в принципе запретили компилятор, запретили вашу программу и
[04:06.520 --> 04:11.120]  как-то копировать элементы или как-то создавать одни элементы на основе других. Тем не менее,
[04:11.120 --> 04:16.040]  копии legion работает в этом случае. Почему? Потому что копии legion в принципе не вызывает ни конструкторов
[04:16.040 --> 04:20.640]  копирования, ни конструкторов перемещения, он просто делает то, что как бы семантически
[04:20.640 --> 04:25.480]  ожидается. Ну вот, в частности, в строке 21 мы вроде как ожидаем, что мы создаем локальный объект,
[04:25.480 --> 04:32.480]  а дальше этот локальный объект перемещаем в аргумент функции G. Тем не менее, такого не
[04:32.480 --> 04:37.680]  происходит, даже несмотря на то, что вы запретили 아무 перемещений и копирований,
[04:37.680 --> 04:44.220]  просто объект A сразу создается внутри аргумента функции G. И то же самое если мы вызываем G от F.
[04:44.220 --> 04:48.440]  То есть, F казалось бы, казалось бы функция Ф создает временный объект, и дальше этот временный объект
[04:48.440 --> 04:52.420]  должен как-то попасть в аргумент функции G. То есть с помощью копирования или перемещения,
[04:52.420 --> 04:56.320]  тем не менее и этого не происходит. То есть копии legion делает так, что у вас объект,
[04:56.320 --> 05:01.180]  который создается функцией Ф, он сразу же сразу же создается в нужном месте, то есть внутри тартинаCHS.
[05:01.180 --> 05:07.180]  Так, да, ну в общем, CopyEleasion — это оптимизация, которая позволяет избежать лишних копирований или перемещений объектов,
[05:07.180 --> 05:10.180]  когда вы работаете со временными объектами.
[05:10.180 --> 05:17.180]  Ну и, соответственно, да, то есть если компилятор выполняет такую оптимизацию, CopyEleasion, то все побочные эффекты,
[05:17.180 --> 05:21.180]  даже если таковые присутствуют на реконструкторе копирования или конструкторе перемещения, он игнорирует.
[05:21.180 --> 05:29.180]  То есть более того, мы видим, я показывал пример, что если даже вы удаляете конструктора копирования и конструктора перемещения,
[05:29.180 --> 05:35.180]  и более того, делаете их приватными, то есть вот полностью запрещаете, они и приватные, и удаленные, и тому подобное,
[05:35.180 --> 05:41.180]  все равно остается возможность так называемой копии инициализации из временных объектов.
[05:41.180 --> 05:49.180]  Да, ну и даже если вы как-то закладывались на то, что у вас конструктор копирования и конструктор перемещения имеет какие-то побочные эффекты,
[05:49.180 --> 05:54.180]  или вы по какой-то причине написали конструктор копирования или конструктор перемещения, который на самом деле не перемещает,
[05:54.180 --> 06:01.180]  а просто выводит там hello world, то этого ничего не будет, потому что копия иллюзион.
[06:01.180 --> 06:08.180]  Существует три основных контекста применения, три основных контекста, когда выполняется копия иллюзион,
[06:08.180 --> 06:13.180]  ну когда компилятор все это оптимизирует, значит первый момент, это когда вы инициализируете объект с помощью pair value,
[06:13.180 --> 06:18.180]  то есть с помощью чистого временного значения, ну в частности, когда вы пишете a равно a с круглыми скобками,
[06:18.180 --> 06:21.180]  то есть тут не происходит создание временного объекта, а потом его перемещение.
[06:21.180 --> 06:27.180]  Нет, здесь объект создается сразу же в нужном месте, то есть по сути a создается с помощью дефолтного конструктора.
[06:27.180 --> 06:32.180]  Ну и то же самое, далее a, b, дальше a в круглых скобках, то есть тут тоже.
[06:32.180 --> 06:37.180]  То есть казалось бы, это синтактиз вызова конструктора копирования или конструктора перемещения, на самом деле нет.
[06:37.180 --> 06:41.180]  То есть компилятор понимает, что вы создаете временный объект, а потом с помощью него создаете объект b.
[06:41.180 --> 06:47.180]  Ну и понятное дело, что компилятор понимает, что зачем вам выполнить лишние действия, он сразу как бы создаст объект b с помощью конструктора по умолчанию.
[06:48.180 --> 06:52.180]  То есть в первом примере и то и то, это просто вызов конструктора по умолчанию.
[06:52.180 --> 06:55.180]  Ровно то, что мы обсуждали на лекциях по конструкторам.
[06:55.180 --> 06:59.180]  Дальше, второй контекст, это return value optimization.
[06:59.180 --> 07:04.180]  Значит, это про то, что если вы внутри return statement, то есть вы после return пишете pair value,
[07:04.180 --> 07:07.180]  ну вот как здесь вот, то есть вы сразу внутри return создаете объект,
[07:07.180 --> 07:11.180]  то этот объект на самом деле не создается на стеке функции f и потом возвращается в нужное место,
[07:11.180 --> 07:13.180]  потом куда-то перемещается, нет.
[07:13.180 --> 07:17.180]  То есть этот объект, он может быть сразу создан в том месте, где вы попросили.
[07:17.180 --> 07:23.180]  То есть, грубо говоря, если вы инициализируете, аа равно f, то вот этот вот объект,
[07:23.180 --> 07:27.180]  он будет создан сразу же непосредственно вот в месте памяти, который зарядили под a.
[07:27.180 --> 07:30.180]  То есть по сути будет вызван тоже один конструктор.
[07:30.180 --> 07:35.180]  Ну и named return value optimization, это тоже про оптимизацию возвращаемого значения,
[07:35.180 --> 07:38.180]  но как говорится в названии named, то есть по имени.
[07:38.180 --> 07:41.180]  То есть здесь вы создаете какой-то локальный объект f,
[07:41.180 --> 07:44.180]  вы можете с ним как-то работать внутри функции, внутри функции f,
[07:44.180 --> 07:47.180]  То есть можете как-то его заполнять, можете как-то его изменять и так далее,
[07:47.180 --> 07:49.180]  но если в итоге вся цепочка действия привела к тому,
[07:49.180 --> 07:52.180]  что вы этот объект просто-напросто возвращаете из функции,
[07:52.180 --> 07:56.180]  то на самом деле оказывается, ну я показывал тоже пример,
[07:56.180 --> 08:01.180]  то в итоге оказывается что вот этот объект, который вы создаете возвращаемым значением функции g
[08:01.180 --> 08:04.180]  и вот этот объект, который находится, f, в примере.
[08:04.180 --> 08:09.180]  И тот объект, который вы создаете внутри функции f, это один и тот же объект.
[08:09.180 --> 08:12.780]  компилятор оптимизирует это так, что вот этот объект сразу же делается
[08:12.780 --> 08:16.100]  эквивалентным вот этому объекту, то есть создается в том месте в памяти, и дальше
[08:16.100 --> 08:20.540]  уже все манипуляции выполняются как бы с ним. И возвращение значения на
[08:20.540 --> 08:27.100]  самом деле не происходит. Понятно? Есть вопросы?
[08:27.340 --> 08:33.620]  Ну и там немного истории. До C++11, до 11 стандарта, вообще говоря,
[08:33.620 --> 08:37.380]  в Copy and Leash был просто некоторые оптимизации, которые реализовали
[08:37.380 --> 08:41.060]  компиляторы, и в стандарте языка такого понятия как Copy and Leash вообще в принципе не было.
[08:41.060 --> 08:46.620]  То есть компиляторы это все реализовали не на свой страх или просто было понимание,
[08:46.620 --> 08:52.180]  что это так эффективнее, поэтому, собственно, это было реализовано. При этом понятное дело,
[08:52.180 --> 08:57.580]  что так как в стандарте этого нет, то вообще говоря, это могло приводить к некоторому
[08:57.580 --> 09:00.700]  странному коду. То есть вы ожидаете, что, скажем, ваш конструктор копирования что-то
[09:00.700 --> 09:04.740]  вводит на экран, а он на самом деле ничего не вводит. Поэтому возникало такое противоречие.
[09:04.740 --> 09:10.140]  Ну и как выключать эту оптимизацию в стандарте C++11 и C++14? Я показывал, да,
[09:10.140 --> 09:16.740]  то есть опция fno-elite-constructors. В C++11 Copy and Leash был легализован в некотором смысле,
[09:16.740 --> 09:21.620]  то есть в C++11 просто добавили некоторые утверждения о том, что Copy and Leash возможен.
[09:21.620 --> 09:25.900]  То есть некоторые компиляторы могут выполнять и такое, то есть они не обязаны это делать,
[09:25.900 --> 09:31.820]  но вот в принципе такое может происходить. То есть это называется non-mandatory Copy and Leash.
[09:31.820 --> 09:41.540]  Ну а в C++17 Copy and Leash стал обязательным, то есть в предпоследнем 17 стандарте вот это
[09:41.540 --> 09:48.220]  вот поведение Copy and Leash в случае первого пункта, вот этого пункта, в случае return value
[09:48.220 --> 09:52.860]  optimization, они стали обязательными. То есть теперь, собственно, вам гарантируется,
[09:52.860 --> 09:56.700]  что именно это и будет происходить. Если вы как-то завязываетесь на то поведение,
[09:56.700 --> 10:00.540]  что при вызове конструктора копирования или конструктор перемещения у вас что-то будет
[10:00.540 --> 10:06.460]  происходить, то вот в случае 1 и в случае 2 вы не имеете на это права. В общем, компилятор
[10:06.460 --> 10:11.500]  обязан выполнять вот эти оптимизации. С третьим пунктом все сложнее, вот третий пункт по-прежнему
[10:11.500 --> 10:15.580]  остается необязательным, то есть в принципе компиляторы не гарантируют, что в случае
[10:15.580 --> 10:19.620]  named return value optimization, что-то будет происходить. Но как правило, именно это и происходит.
[10:19.620 --> 10:27.860]  Ну и в общем-то это амблюзибет того, что мы говорили на прошлой лекции. Мы говорили про
[10:27.900 --> 10:36.020]  cdmove, cdforward и вот говорили про copy elision. В принципе, на этом с темами второго модуля все. Такие есть вопросы.
[10:36.020 --> 10:47.620]  Ну отлично, тогда перейдем к темам третьего модуля или к темам третьего задания. Ну небольшой
[10:47.620 --> 10:54.780]  анонс, о чем мы будем говорить. Мы будем говорить про исключение C++, про правила написания безопасного
[10:54.780 --> 11:02.140]  кода, потом немного поговорим про стандартную библиотеку языка C++, как ей пользоваться, что там
[11:02.140 --> 11:06.660]  есть, чего там нет и так далее. Ну и в рамках курса алгоритма и структуры данных рассмотрим
[11:06.660 --> 11:24.500]  хэштаблицы. Вот такой план. Ну давайте двигаться к новой теме. Начнем мы с исключений. Сегодня
[11:24.500 --> 11:30.060]  начнем так потихоньку втягиваться в то, что такое исключение, что такой безопасный код на языке
[11:30.060 --> 11:36.980]  C++ и так далее. Значит, сразу же хочется сказать некоторый дисклеймер по поводу того, что мы сейчас
[11:36.980 --> 11:44.020]  будем обсуждать, скажем так, ошибки при написании программ на C++. И когда я говорю о ошибке, я вообще
[11:44.020 --> 11:48.900]  говоря говорю не про те ошибки, которые вы получаете в контесте, то есть не про wrong cancer и так далее.
[11:48.900 --> 11:53.780]  Вот эти ошибки это полностью на вашей совести. Я говорю про те ошибки, которые на самом деле
[11:53.780 --> 11:58.500]  являются некоторыми исключительными ситуациями. Ну смотрите, когда мы обсуждаем алгоритмы,
[11:58.500 --> 12:02.340]  ну не знаю, алгоритмы сортировки, ну да, давайте возьмем какой-нибудь алгоритм
[12:02.340 --> 12:07.220]  сортировки с влиянием. Вот алгоритм сортировки с влиянием, он что делает? Он развивает
[12:07.220 --> 12:13.180]  элементы по одному, и дальше все эти элементы попарно сливает в некоторый временный буфер,
[12:13.180 --> 12:17.860]  и потом из этого временного буфера копирует, собственно, исходный массив. И в принципе,
[12:17.860 --> 12:22.340]  на словах все кажется понятно, то есть как запрогать тоже понятно, несложно и так далее.
[12:22.340 --> 12:26.060]  И когда вы пишете программу, вы в принципе, до сегодняшнего момента, когда вы писали
[12:26.060 --> 12:29.780]  программу, вы в принципе, наверное, не задумывались о том, что у вас что-то может пойти не так.
[12:29.780 --> 12:35.660]  Опять же, я не говорю про ошибки, что мы там где-то набагали и так далее. Вообще говоря,
[12:35.660 --> 12:42.220]  программы выполняются не в сферическом вакууме и так далее. Программы выполняются в реальных
[12:42.220 --> 12:46.660]  средах. А в реальных средах может многое пойти не так. Ну, например, банальное, если мы говорим
[12:46.660 --> 12:51.380]  про merge sort, merge sort может не хватить памяти. То есть вы внутри merge сорта вы выделяете
[12:51.380 --> 12:56.060]  дополнительную память под хранение временных результатов. Ну а что произойдет, если операционная
[12:56.060 --> 13:01.380]  система вам скажет, что нет у меня столько памяти, ну что делать? Описывается ли это там в рамках
[13:01.380 --> 13:04.860]  нашей алгоритмической модели? Ну нет, алгоритм вообще в принципе не говорит о том, что делать,
[13:04.860 --> 13:09.980]  если у вас в обычном классическом merge сорте не хватает памяти. Это первый момент. Второй момент,
[13:09.980 --> 13:13.540]  ваши программы очень часто взаимодействуют, опять же, программы работают неизолированно,
[13:13.540 --> 13:17.700]  они работают в некоторой среде. Они взаимодействуют с другими программами, с другими процессами,
[13:17.780 --> 13:21.460]  которые они взаимодействуют с кем-то возможным по сети, с пользовательским вводом и так
[13:21.460 --> 13:26.820]  далее. Ну и понятное дело, что, как бы предполагать, что ваша программа работает хорошо,
[13:26.820 --> 13:30.820]  а все остальные компоненты с которыми она работает будут работать корректно, ну как-то грубо говоря
[13:30.820 --> 13:35.180]  наивно. Да, то есть наивно полагать, что скажем пользователь будет водить там обязательно все
[13:35.180 --> 13:39.760]  корректно, что сервер всегда будет вам отвечать, что соединение по сети всегда доступно, что процессы
[13:39.760 --> 13:44.500]  там всегда работают тоже исправно и так далее. То есть всякие неожиданные ситуации, которые грубо
[13:44.500 --> 13:48.740]  не описываются алгоритмически, но которые там чисто технические, технические
[13:48.740 --> 13:51.980]  проблемы, которые могут возникать, они могут приводить к некоторым
[13:51.980 --> 13:55.580]  проблемам. И, наверное, хотелось бы такие проблемы тоже решать оперативно,
[13:55.580 --> 14:00.060]  то есть вряд ли хочется, чтобы на каждый неправильный пользовательский ввод
[14:00.060 --> 14:03.140]  ваша программа просто-напросто брала и падала. Вряд ли, если вы заходите на
[14:03.140 --> 14:05.780]  какой-то сайт, вводите свой логин-пароль, и в случае ввода неправильного
[14:05.780 --> 14:10.100]  пароля у вас падает весь сайт. Такого явно не происходит.
[14:10.100 --> 14:13.540]  В принципе, сложно каким-то запросам положить сервер. Большим количеством
[14:13.540 --> 14:16.660]  запросов можно, но одним каким-то нет. И вот как обеспечить такую
[14:16.660 --> 14:20.060]  бесперебойную работу или как вашему коду сообщать о том, что что-то пошло не так,
[14:20.060 --> 14:24.780]  об этом мы и будем говорить все это время. Сегодняшнюю и следующую лекцию.
[14:24.780 --> 14:30.940]  Мотивация понятна? Хорошо. Давайте посмотрим такой игрушечный пример.
[14:30.940 --> 14:35.180]  Есть функция Divide, которая просто берет два произвольных. Давайте скажем, что
[14:35.180 --> 14:39.420]  тип T это всегда число для простоты. Функция Divide берет два числа
[14:39.420 --> 14:44.980]  произвольных и делит одно на другое. Ну и тут сразу возникает вопрос, а что
[14:44.980 --> 14:51.140]  делать, если у вас в качестве y был передан ноль? В принципе, деление на ноль для
[14:51.140 --> 14:55.020]  целых чисел приводит к undefinedBehaviour, то есть неопределенное поведение.
[14:55.020 --> 14:58.380]  Функция Divide не может работать нормально в случае целых чисел и в случае
[14:58.380 --> 15:02.500]  деления на ноль. Что делать? Как сообщить пользователю об ошибке, то есть как
[15:02.500 --> 15:06.300]  сообщить тому, кто вызывает вашу функцию Divide, что что-то пошло не так?
[15:07.020 --> 15:10.380]  Есть несколько вариантов. Первый вариант это не как. То есть просто-напросто
[15:10.380 --> 15:14.780]  ничего не делать. И в принципе, там большинство из нас до сих пор этим
[15:14.780 --> 15:19.020]  пользуется. Ну и на самом деле это не то чтобы прям очень плохой очень плохой
[15:19.020 --> 15:23.860]  вариант обработки ошибок. Ну действительно, смотрите, то есть если вы, если ваш
[15:23.860 --> 15:27.780]  контракт функции заключается в том, что вы говорите, что если вы делите одно
[15:27.780 --> 15:31.540]  не нулевое, то есть если делите одно число на другое не нулевое, то у вас все
[15:31.540 --> 15:34.620]  работает корректно. Если вы делите что-то на нулевой число, то произвести
[15:34.620 --> 15:38.400]  может все что угодно. Если контракт вашей функции устроен так, то в принципе все
[15:38.400 --> 15:42.620]  нормально. То есть вы можете эту ошибку не обрабатывать. Более того, все плюс-плюс,
[15:42.620 --> 15:46.420]  очень большое количество функций и большое количество операций вот таких
[15:46.420 --> 15:49.980]  проверок не выполняют. Классический пример — это выход за границу массива.
[15:49.980 --> 15:54.180]  То есть мы все понимаем, что если выходим за границу массива, то ничего хорошего
[15:54.180 --> 15:57.660]  от этого не получится. Но при этом компилятор не осуществляет никаких
[15:57.660 --> 16:02.180]  проверок выхода за границу массива. Почему? Ну для эффективности.
[16:02.860 --> 16:06.340]  Если мы постоянно на каждое обращение к массиву будем проверять
[16:06.340 --> 16:11.620]  выход за границы, то это лишняя инструкция процессора, то есть лишнее
[16:11.620 --> 16:15.740]  время работы. Ну а все плюс-плюс в принципе устроен так, что у него
[16:15.740 --> 16:18.500]  философия такая, что мы не платим за то, что не используем. То есть если мы
[16:18.500 --> 16:21.820]  корректно пользуемся массивом, то никогда не входим за границы, то есть пишем
[16:21.820 --> 16:25.180]  логичные программы, то все работает эффективно без дополнительных
[16:25.180 --> 16:32.420]  проверок. И вообще мы красной нитью так принесем через вот эту вот сегодняшнюю
[16:32.420 --> 16:36.500]  и следующую лекцию вопрос о том, что на самом деле безопасность и эффективность
[16:36.500 --> 16:40.140]  они не всегда совместимы, к сожалению. То есть к сожалению, безопасные программы
[16:40.140 --> 16:43.540]  они редко когда эффективны, а эффективные программы они редко когда безопасны.
[16:43.540 --> 16:48.180]  Ну вот собственно классический пример. То есть если вы не осудяете проверок, то
[16:48.180 --> 16:50.940]  есть ничего не делаете в случае, если у вас некорректный пользовательский вод,
[16:50.940 --> 16:53.740]  то в принципе у вас все эффективно. То есть вы выполняете всего лишь один
[16:53.740 --> 16:57.020]  ретерн и все. Если вы выполняете дополнительную проверку, то это лишние
[16:57.020 --> 17:01.740]  инструкции, соответственно происходит некоторая неэффективность. У вас вопрос?
[17:01.740 --> 17:06.980]  Отличный вопрос. Контракт функций. Ну давайте скажем так, что контракт функций
[17:06.980 --> 17:11.140]  говорит о следующем. Что эта функция принимает, что эта функция возвращает и
[17:11.140 --> 17:14.540]  что происходит. То есть грубо говоря, эта функция описывает, что она принимает на
[17:14.540 --> 17:19.100]  вход и что происходит в случае, если вы там подали ей то или иное. Ну например,
[17:19.780 --> 17:25.900]  скажем вы решаете контест какую-то задачу. Там понятный контракт. То есть вам
[17:25.900 --> 17:30.780]  говорят, что будет подано то-то и мы от вас ожидаем этого и так далее.
[17:30.780 --> 17:35.180]  В принципе контракт функции может включать в себя какое-то описание, что делать,
[17:35.180 --> 17:39.500]  если там не это. Скажем, если у вас задача стоит в том, чтобы разделить одно число на
[17:39.500 --> 17:43.500]  другое, и при этом вам говорится, что если вы делите число на ноль, то
[17:43.500 --> 17:46.780]  нужно там вывести ошибку. А если вам говорится, что вы делите одно число на
[17:46.780 --> 17:49.020]  другое, и при этом не говорится ничего о том, что должно происходить
[17:49.020 --> 17:52.380]  пределение на ноль, то в принципе вы можете делать что угодно. Собственно, это и
[17:52.380 --> 17:54.380]  называется undefinedBehaviour.
[17:58.380 --> 18:02.980]  Следующий способ как-то сообщить об ошибке, ну помимо того, что ничего не
[18:02.980 --> 18:07.180]  делать, это вернуть некоторое специальное значение. Но, соответственно, у нас есть
[18:07.180 --> 18:11.260]  х, у нас есть у, мы делим х на у. И если у нас у вдруг оказался равным нулю, то
[18:11.260 --> 18:15.660]  этот способ предполагает, что мы вернем некоторое значение, которое будет
[18:15.660 --> 18:19.860]  говорить о том, что вот что-то пошло не так. Да, то есть мы после вызова функции
[18:19.860 --> 18:23.180]  divide просто проверяем верно ли, что то, что нам вернуло функции divide, это
[18:23.180 --> 18:26.740]  некое специальное значение. Если да, то это значит, что у нас произошла ошибка, если
[18:26.740 --> 18:29.780]  нет, то мы продолжаем работу. Ну и тут есть очевидная проблема, которая
[18:29.780 --> 18:33.580]  заключается в том, что а как вообще говоря в рамках системы типов языка
[18:33.580 --> 18:38.940]  C++ определить корректное значение от некорректного значения. Например, я
[18:38.940 --> 18:43.380]  выполняю divide двух интов. То есть я делю один инт на другой инт. Что мне нужно
[18:43.380 --> 18:47.900]  вернуть в качестве специального значения? 0, минус 1, 1, бесконечность и так
[18:47.900 --> 18:52.900]  далее. Ну непонятно, потому что любой из этих значений там 0, 1, минус 1,
[18:52.900 --> 18:57.340]  бесконечность, это в принципе корректный результат деления. Да, то есть если вам
[18:57.340 --> 19:00.420]  функция возвращает ноль, вы в принципе не можете определить. То есть вам
[19:00.420 --> 19:03.340]  действительно вернулся ноль как результат операции, ну как результат
[19:03.340 --> 19:06.380]  вызова функции divide, или вам вернулся ноль в качестве некоторого
[19:06.380 --> 19:12.260]  детектора ошибки. То есть не всегда возможно там отделить специальное
[19:12.260 --> 19:18.140]  значение от результата вызова функции. Следующий пример, который очень
[19:18.140 --> 19:22.220]  популярен в языке C, это возврат некоторого специального кода ошибки. То есть в языке C
[19:22.220 --> 19:25.660]  большинство функций они возвращают не результатом выполнения функции или
[19:25.660 --> 19:29.180]  результатом выполнения операции, а они возвращают собственно код ошибки. А
[19:29.180 --> 19:32.900]  результат можно записывать, например, в специальную выходную
[19:32.900 --> 19:35.180]  переменную.
[19:36.180 --> 19:43.020]  Да, специальную выходную переменную. Что мы делаем? Если y равен нулю, то мы
[19:43.020 --> 19:48.340]  возвращаем код ошибки единица. Если y не равен нулю, то мы просто выходную
[19:48.340 --> 19:52.780]  переменную, которую принимаем, например, по адресу, вот по адресу, по этому
[19:52.780 --> 19:57.780]  адресу записываем результат x деленный на y. И возвращаем ноль как
[19:57.780 --> 20:00.140]  индикатор того, что ошибки не произошло.
[20:00.140 --> 20:05.300]  Окей? То есть мы в качестве возвращаемого значения возвращаем, собственно, код
[20:05.300 --> 20:08.220]  ошибки, а результат записываем в какую-то третью переменную, которую нам
[20:08.220 --> 20:13.460]  передали. Ну и четвертый способ, который тоже присутствует в языке C, это
[20:13.460 --> 20:18.860]  использовать некоторую глобальную переменную. То есть понятная проблема с
[20:18.860 --> 20:21.420]  тем, что мы возвращаем код ошибки. То есть если мы возвращаем код ошибки, то в
[20:21.420 --> 20:24.420]  принципе оказывается так, что у нас функция возвращает не результат, а
[20:24.420 --> 20:29.580]  возвращает там нечто иное, нечто другое. Мы этого не хотим. Мы хотим, чтобы
[20:29.580 --> 20:32.060]  функция возвращала результат, а ошибка была записана где-то в другом месте.
[20:32.060 --> 20:34.980]  Но это может добиться, например, с помощью специальной глобальной перемены
[20:34.980 --> 20:39.340]  rno. Вот если у вас функция отработала некорректно, то, соответственно, rno
[20:39.340 --> 20:42.780]  устанавливается в какой-то код ошибки. Если функция отработала корректно, то
[20:42.780 --> 20:45.820]  rno устанавливается в ноль. Ну а результат возвращается, как раньше,
[20:45.820 --> 20:51.100]  в виде возвращаемого значения функции divide. Ну и проблемы этого похода тоже
[20:51.100 --> 20:55.020]  понятны. У нас есть глобальная переменная. Почему глобальные переменные плохи, думаю,
[20:55.020 --> 21:02.900]  объяснять лишний раз не нужно. Ну и да, собственно, как я сказал, в языке C принято,
[21:02.900 --> 21:07.700]  принято стилем до языка C++, когда был язык C, было принято обработка
[21:07.700 --> 21:10.420]  ошибок в виде некоторого возвращаемого значения. И код выглядел примерно вот
[21:10.420 --> 21:14.140]  таким образом. Если функция f состоит из вызова функции g и h, то
[21:14.140 --> 21:18.020]  происходит следующая вещь. Вы вызываете функцию g, смотрите на ее код ошибки. Если
[21:18.020 --> 21:21.500]  произошла ошибка, то вы что-то делаете. Если нет, то идете дальше. Снова вызываете
[21:21.500 --> 21:24.860]  функцию h, сохраняете результат ошибки в некоторую переменную. Ну и так далее.
[21:24.860 --> 21:30.060]  Ну и недостатки этого подхода, они ясны. То есть код очень сильно раздувается, то
[21:30.060 --> 21:33.820]  есть вы на каждый вызов функции делаете свой обработчик и
[21:33.820 --> 21:38.700]  после каждого вызова функции делаете свой if, в зависимости от того,
[21:38.700 --> 21:43.580]  что вам вернулось в качестве ошибки. Второй недостаток тоже понятен.
[21:43.580 --> 21:47.540]  Чтобы получить результат, вам нужно не сохранить результат вызова функции, а вам
[21:47.540 --> 21:50.460]  нужно передать некоторую дополнительную переменную, либо по ссылке, либо по
[21:50.460 --> 21:54.740]  адресу. Ну и возвращаемое значение, на самом деле, легко проигнорировать.
[21:54.740 --> 21:59.500]  То есть скажем, если функция, то есть в принципе, вас никто не обязывает
[21:59.500 --> 22:02.740]  возвращаемое значение в функции g как-то проверять. То есть вы можете вызвать
[22:02.740 --> 22:07.220]  функцию g, проигнорировать ошибку, и программа дальше продолжит работу.
[22:07.220 --> 22:12.460]  Но при этом понятно, что там будет что-то сломано. Еще один дисклеймер, который
[22:12.460 --> 22:15.980]  хочу сказать. Цель лекции, на самом деле, не заключается в том, чтобы
[22:15.980 --> 22:20.620]  покритиковать обработку ошибок стиле c. Вот такой способ обработки ошибок, на
[22:20.620 --> 22:23.820]  самом деле, популярен во многих языках программирования, в частности, там есть
[22:23.820 --> 22:28.580]  популярный язык голанг, в котором тоже принято, что используется
[22:28.580 --> 22:32.540]  идиома с возразом кода ошибок. Вот такая схема тоже норма, и она обладает
[22:32.540 --> 22:36.620]  большим количеством преимуществ. В частности, мы тут явно видим, что
[22:36.620 --> 22:40.460]  функции могут возвращать ошибки, мы явно видим их обработку. То есть плюс это
[22:40.460 --> 22:43.940]  очень сильно дисциплинирует, чтобы вы понимаете, что как бы функции могут
[22:43.940 --> 22:47.140]  работать некорректно, и поэтому их нужно обрабатывать, и так далее. Я сейчас лишь
[22:47.140 --> 22:50.660]  хочу раскрыть некоторые недостатки, которые они обладают, и показать
[22:51.660 --> 22:55.540]  схему, которая используется в языке C++, которая эти недостатки как-то
[22:55.540 --> 23:03.540]  преодолевает. Ну и теперь, собственно, перейдем к тому, как все устроено в языке
[23:03.540 --> 23:08.460]  C++, какие механизмы в языке C++ были добавлены, чтобы как-то работать с
[23:08.460 --> 23:12.260]  невалидным кодом. В языке C++ используется схема с
[23:12.260 --> 23:17.340]  исключениями. Что такое исключение? Исключение — это некоторый объект, который
[23:17.340 --> 23:21.140]  генерируется во время, когда у вас происходит некоторая исключительная
[23:21.140 --> 23:25.780]  ситуация или некоторая ошибка. Ну и, как правило, предполагается, что в этом
[23:25.780 --> 23:28.500]  объекте у вас сохранена некоторая информация о том, почему эта ошибка
[23:28.500 --> 23:33.220]  произошла, в каком месте она произошла, и так далее, и так далее. Чтобы сгенерировать
[23:33.220 --> 23:38.420]  ошибку, нужно писать ключевое слово throw, и дальше после этого вы указываете
[23:38.420 --> 23:42.340]  объект, который вы назначаете исключением. Но в данном случае у нас функция divide
[23:42.340 --> 23:47.220]  возвращает исключение типа int. То есть вы говорите, что в случае, если у меня
[23:47.220 --> 23:53.900]  что-то пошло не так, то я выбрасываю исключение типа int.
[23:53.900 --> 23:58.220]  Что означает выбросить исключение? Выбрасывание исключения — это
[23:58.220 --> 24:01.660]  артагональный механизм возврата из функции. То есть если у вас функция что-то
[24:01.660 --> 24:04.560]  возвращает, то считается, что функция отработала нормально.
[24:04.560 --> 24:09.060]  Если функция выбрасывает какое-то исключение, то функция завершилась
[24:09.060 --> 24:12.740]  ненормально. У нас есть два возможных механизма условно возврата из
[24:12.740 --> 24:17.580]  функции. Классический — это return, и вот с помощью исключения — это throw. Далее
[24:17.580 --> 24:22.940]  поподробнее. Что происходит, когда вы выбрасываете throw? Когда вы выбрасываете throw,
[24:22.940 --> 24:27.100]  у вас происходит так называемая размотка стека. То есть если у вас внутри
[24:27.100 --> 24:33.100]  функции было сгенерировано исключение с помощью throw, то работа этой
[24:33.100 --> 24:36.220]  функции полностью прекращается. То есть вот на этом месте функция останавливается,
[24:36.220 --> 24:39.300]  и дальше, понятное дело, уничтожаются все локальные перемены, которые были созданы
[24:39.300 --> 24:44.420]  до этого. Дальше это исключение попадает в функцию, которая вызывала эту функцию.
[24:44.420 --> 24:48.060]  Понятное дело, что у вас программа всегда состоит из последовательности вызовов
[24:48.060 --> 24:53.940]  функции f, g, h, main и так далее. Значит, если в функции h произошло какое-то
[24:53.940 --> 24:57.540]  исключение, то это исключение попадает в функцию g, потом если g не смогло
[24:57.540 --> 25:01.340]  обработать эту ошибку, она попадает в функцию f, ну и так далее. Вот так вот
[25:01.340 --> 25:05.020]  каскадным образом все функции завершают работу. Ну вот пример. Допустим у меня есть
[25:05.020 --> 25:08.700]  функция main, которая вызывает там функцию курения. Курение вызывает рак, рак бросает
[25:08.820 --> 25:15.300]  исключение. Когда мы бросили здесь исключение, что у нас произошло? Функция
[25:15.300 --> 25:19.980]  cancer перестала работать, то есть вот то, что идет после этого cdcout, это не будет
[25:19.980 --> 25:23.820]  выведено, и дальше управление передается вот сюда, вот в эту строку.
[25:23.820 --> 25:27.900]  Здесь функция smoking видит, что cancer бросила исключение, поэтому она тоже
[25:27.900 --> 25:31.460]  прекращает работу. Дальше это все попадает в main, main видит, что и smoking было
[25:31.460 --> 25:35.020]  выброшено исключение, и main тоже завершает работу. В случае, если исключение
[25:35.020 --> 25:38.260]  выбрасывается из main, то есть если исключение покидает main, то программа
[25:38.260 --> 25:43.020]  аварийно завершает работу. Ну вызывает нам функцию специально, cdeterminate.
[25:43.460 --> 25:53.620]  Все понятно? Хорошо. Да, ну и немного подробнее про то, что происходит, когда вы
[25:53.620 --> 25:58.300]  делаете throw некоторого объекта. Значит, когда вы делаете throw некоторого объекта,
[25:58.300 --> 26:01.540]  то вот этот объект исключения, он сохраняется в некотором специальном
[26:01.540 --> 26:04.980]  области памяти. Понятное дело, что объект исключения не может
[26:04.980 --> 26:09.300]  храниться на стеке. Ну по понятным причинам, да? Потому что если в объект
[26:09.300 --> 26:12.660]  исключения хранится на стеке, то когда у вас завершается функция, у вас
[26:12.660 --> 26:16.380]  стек схлопывается и исключение погибает. Вот поэтому исключение хранится
[26:16.380 --> 26:21.140]  в специальной области памяти, в которую все обращаются, когда хотят
[26:21.140 --> 26:26.100]  понять, есть исключение или нет. Ну и понятное дело, что когда вы создаете throw
[26:26.100 --> 26:30.260]  с помощью, когда вы передаете, скажем, какую-то переменную, например throw a,
[26:30.260 --> 26:34.660]  то вот эта переменная a просто соберет и копируется в то место, где
[26:34.660 --> 26:38.740]  хранится исключение. Если вы делаете throw от std-mufata, то понятное дело, что эта
[26:38.740 --> 26:41.500]  переменная, она будет перемещаться в эту специальную область памяти. Ну то есть
[26:41.500 --> 26:45.500]  в принципе все работает так, как там обычное копирование или перемещение
[26:45.500 --> 26:54.180]  объектов, окей? Вот. Теперь, ну хорошо, мы понимаем, как сообщать об ошибке, то есть
[26:54.180 --> 26:57.620]  мы понимаем, что если у нас там произошло что-то не так, то мы делаем throw, то есть
[26:57.620 --> 27:00.820]  мы бросаем исключение. Ну возникает вопрос, хорошо, ну вот мы понимаем, что мы
[27:00.820 --> 27:03.900]  бросаем исключение, и при этом это исключение приводит к тому, что у меня
[27:03.900 --> 27:07.260]  завершается там первая функция, дальше следующая функция, и в общем-то
[27:07.260 --> 27:10.540]  каскадным образом все функции завершают работу. А что если я не хочу, что у меня
[27:10.540 --> 27:15.020]  программа завершала работу? Что если я хочу эту ошибку обработать? То есть я хочу
[27:15.020 --> 27:19.740]  как-то попытаться решить проблему. И это можно сделать с помощью так называемого
[27:19.740 --> 27:24.980]  catch-блока, точнее try-catch-блока. Что мы делаем? Ну вот, допустим, у нас есть функция
[27:24.980 --> 27:31.020]  divide, про которую мы знаем, что она может бросить исключение. Что мы делаем? То есть мы
[27:31.020 --> 27:35.180]  понимаем, что функция divide может бросить исключение типа int. То есть вызов
[27:35.180 --> 27:39.060]  функции divide потенциально опасен. Мы делаем следующую вещь. Если мы вызываем
[27:39.060 --> 27:44.100]  функцию внутри main или любой другой функции, мы оборачиваем эту функцию в try-блок,
[27:44.100 --> 27:47.740]  то есть пишем try, ну и даже в фигурных скобках мы оборачиваем потенциально
[27:47.740 --> 27:52.420]  опасный код. То есть говорим, попробуй выполнить вот этот код. И если вызов вот
[27:52.420 --> 27:56.140]  этого кода, который находится внутри блока try, приводит к тому, что из какой-то
[27:56.140 --> 28:00.280]  функции выбрасывается исключение, значит мы пытаемся его поймать вот это
[28:00.280 --> 28:04.080]  исключение внутри блока catch. Значит, блок catch устроен следующим образом. Мы пишем catch, а
[28:04.080 --> 28:11.160]  дальше в круглых скобках мы пишем, собственно, исключение, которое мы хотим поймать. В данном
[28:11.160 --> 28:16.720]  случае мы пишем, что блок catch хочет поймать исключение типа int и назвать его r. То есть,
[28:16.720 --> 28:20.640]  грубо говоря, создаем локальную переменную внутри блока catch. Ну и соответственно мы говорим,
[28:20.640 --> 28:24.240]  что когда мы поймали исключение, то есть, собственно, мы его поймали, мы его захлопнули и
[28:24.240 --> 28:29.960]  дальше там выводим какую-то, например, отладочную информацию. Важно понимать, что после того,
[28:29.960 --> 28:34.400]  как отработает блок catch, программа продолжит нормальное функционирование. Вот блок catch как
[28:34.400 --> 28:39.640]  раз отвечает за то, что вы обрабатываете ошибку и не даете ей полететь дальше. Вот если бы мы тут не
[28:39.640 --> 28:46.000]  написали try catch block, то исключение, которое бы вылетело из функции divide, оно бы вылетело в
[28:46.000 --> 28:49.760]  функцию main, дальше оно бы покинуло в функцию main и программа бы аварийно завершилась. Здесь такого не
[28:49.760 --> 28:54.600]  происходит, здесь программа завершается нормально. Внутри блока try было выброшено исключение, в
[28:54.600 --> 28:59.960]  кейче мы его поймали, то есть считается, что мы его обработали и пошли дальше. Ну и при этом мы
[28:59.960 --> 29:08.240]  пошли дальше не от момента того, как мы выбросили исключение, а после catch блока. Понятно? То есть,
[29:08.240 --> 29:13.920]  если у нас try block не получился, то дальше мы его выполнять не продолжаем. Мы продолжаем вот после
[29:13.920 --> 29:26.160]  всего try catch блока. Окей? Все ясно? Да, ну соответственно, по завершению блока catch исключение
[29:26.160 --> 29:29.080]  считается успешно выработанным и выполнение программы продолжается нормально в режиме.
[29:29.080 --> 29:37.440]  Вот, соответственно, если здесь в divide выбрасывается исключение, то работает catch, выводится это
[29:37.440 --> 29:43.120]  сообщение об ошибке. Вот это сообщение не выводится, потому что до него, собственно, наш... Что?
[29:43.120 --> 29:51.040]  Дискуссионный вопрос, в общем. Короче говоря, это сообщение не будет выведено на экран,
[29:51.040 --> 29:55.680]  это важно, это главное. То есть мы продолжим работу непосредственно сразу со следующей инструкцией,
[29:55.680 --> 30:05.320]  то есть return 0. Да, значит, одному блоку try может соответствовать несколько блоков catch. Ну вообще
[30:05.320 --> 30:08.560]  говоря, это стандартная ситуация, когда у вас из одной функции может вылетать исключение разных
[30:08.560 --> 30:15.320]  типов. Ну, например, в случае деления на ноль вы бросаете исключение типа int, в случае, не знаю,
[30:15.320 --> 30:20.440]  какого-то переполнения вы бросаете исключение типа в данном случае double. То есть это нормальная
[30:20.440 --> 30:24.600]  ситуация, что у вас из одной функции могут вылетать исключение разных типов. Ну, в отличие от механизма
[30:24.600 --> 30:29.720]  return, если у нас функция что-то возвращает, то она должна возвращать объект определенного типа.
[30:29.720 --> 30:33.520]  Вот в случае, когда вы выбрасываете исключение, в принципе, вы можете бросить исключение абсолютно
[30:33.520 --> 30:40.360]  любого типа. То есть можете бросить как int, можете бросить double. Ну и в этом случае у вас выбирается именно
[30:40.360 --> 30:45.320]  тот блок, который соответствует типу исключения. То есть если divide, функция divide бросает исключение
[30:45.320 --> 30:55.160]  типа int, то оно будет поймано в catch-блоке соответствующему типу int. Да, ну и соответственно,
[30:55.160 --> 31:00.080]  если нужный catch-блок найден не будет, то исключение считается необработанным. В данном случае
[31:00.080 --> 31:04.560]  у вас divide бросает int, но при этом вы написали два catch-блока, который принимает double и который
[31:04.560 --> 31:08.600]  принимает констанция развёточка. int не является double и int не является констанция развёточки,
[31:08.600 --> 31:12.640]  поэтому считается, что исключение необработанное, оно полетит дальше. Так как если бы вы не написали
[31:12.640 --> 31:22.320]  трое catch-блок. Здесь может возникнуть вопрос. Ну смотрите, допустим, у меня есть функция, и функция
[31:22.320 --> 31:30.760]  принимает double. Я же в эту функцию, в принципе, могу передать int. То есть я в функцию, которая принимает
[31:30.760 --> 31:36.360]  не целые числа, могу передать целое число, и при этом у меня будет выполняться приведение int к
[31:36.360 --> 31:43.120]  double. Ну казалось бы, а почему здесь не так? Ну здесь не так, потому что не так. Catch-блок работает не так.
[31:43.120 --> 31:47.720]  То есть catch-блок не выполняет никаких преобразований. То есть catch-блок ищет соответствия именно по
[31:47.720 --> 31:54.320]  точному совпадению типа. То есть он проверяет, равен ли int даблу, int не является даблом, поэтому этот
[31:54.320 --> 31:59.320]  catch-блок игнорируется. Равен ли int констанция развёточки? Нет, не равен. Этот catch-блок игнорируется.
[31:59.320 --> 32:05.800]  Catch-блоки закончились, всё, исключение летит дальше. То есть нужный catch-блок и ищется именно по
[32:05.800 --> 32:11.640]  нужному, по точному соответствию. Никаких приведений типа не происходит за некоторым исключением.
[32:11.640 --> 32:22.480]  Да, это тоже про то же самое. Да, вот про исключение. Значит, есть пара исключений, которые касаются
[32:22.480 --> 32:35.040]  трай-catch-блоков. Первое исключение это void-звёздочка или void-указатель. С помощью void-указателя
[32:35.520 --> 32:41.600]  вы можете поймать любой указатель. Вот такое правило. То есть если вы внутри трай-блока бросаете
[32:41.600 --> 32:46.800]  исключение типа указатель на int или указатель на любой другой объект, то этот указатель вы можете
[32:46.800 --> 32:53.120]  поймать с помощью указателя на void. То есть void принимает абсолютно любой указатель. Здесь
[32:53.120 --> 32:59.040]  приведение типа происходит. Типизированный указатель приводится к нетипизированному указателю.
[32:59.040 --> 33:04.200]  Это первый момент. Ну и второй момент. Происходит приведение в рамках иерархии
[33:04.200 --> 33:14.160]  наследования. То есть если у вас стип B унаследован от A. При этом важно, что у вас стип B унаследован
[33:14.160 --> 33:21.920]  публичным образом от A. То есть, грубо говоря, B является A. То есть мы говорили, что публичное
[33:21.920 --> 33:26.240]  наследование реализует отношение являться. То есть если B унаследовано от A, то это значит,
[33:26.240 --> 33:34.600]  что B тоже является A. Поэтому если вы бросаете в какой-то функции исключение типа B, то вы в
[33:34.600 --> 33:42.440]  принципе его можете поймать в кетч-блоке, который ловит A. Это вполне корректно. То есть приведение
[33:42.440 --> 33:53.880]  типов тут происходит. Все ясно. Еще одно правило выбора блока catch. Если так получается, что подходит
[33:53.880 --> 33:58.320]  несколько блоков catch. Ну а такое может происходить только в случае наследования и в случае
[33:58.320 --> 34:04.960]  указателей. Потому что все остальные кетч-блоки ищутся ровно по точному соответствию. Так вот,
[34:04.960 --> 34:11.960]  если у вас есть несколько подходящих блоков catch, то срабатывает всегда первый подходящий. Вот пример.
[34:11.960 --> 34:17.080]  Значит, у вас есть там некоторая функция, некоторый код, который генерирует исключение типа int
[34:17.080 --> 34:23.360]  указатель. И у вас есть два кетч-блока, один из которых ловит void указатель,
[34:23.360 --> 34:30.720]  а второй ловит int указатель. Так вот, отработает именно первый кетч, то есть int указатель
[34:30.720 --> 34:36.520]  поймается первым кетчом. Потому что поиск кетч-блока происходит последовательным образом. То есть
[34:36.520 --> 34:40.520]  сначала проверяется первый кетч-блок, подходит он или нет. Если нет, то идем дальше. Если подходит,
[34:40.520 --> 34:45.840]  то все. Мы тут останавливаемся и обрабатываем ошибку здесь. Поэтому второй блок, в принципе,
[34:45.840 --> 34:52.520]  никогда не будет отработан. Второй блок никогда не сработает. Любой указатель будет
[34:52.520 --> 34:59.280]  пойман void указателем, поэтому все будет плохо. Ну и то же самое касается иерархии наследования.
[34:59.280 --> 35:04.200]  Если у вас B унаследован от A, и вы бросаете объект типа B. Вот у вас есть два кетч-блока, один из
[35:04.200 --> 35:09.240]  которых ловит объекты A, а второй из которых ловит объекты B. То в этом случае у вас первый кетч-блок
[35:09.240 --> 35:13.960]  отработает, а второй выполнен никогда не будет. Почему? Потому что когда вы бросаете исключение
[35:13.960 --> 35:19.600]  типа B, B является A, и первый кетч-блок подходит. Второй кетч-блок даже рассматриваться не будет.
[35:19.600 --> 35:39.480]  Ну и так, что еще хотел сказать. Вообще говоря, нормальные компиляторы в такой
[35:39.480 --> 35:44.960]  ситуации, они выдают вам предупреждение и скажут вам о том, что вот те кетч-блоки,
[35:44.960 --> 35:48.680]  которые написаны снизу, они будут проигнорированы. Так что тут не все так плохо. То есть если вы написали,
[35:48.680 --> 35:54.400]  ну допустим, вы перепутали кетч-блоки местами, скажем, ну написали void звездочка, потом int звездочка,
[35:54.400 --> 36:00.360]  то компилятор вам любезно подскажет, что кетч-блок, который принимает int звездочка, он выполнен
[36:00.360 --> 36:04.120]  никогда не будет. Почему? Потому что все будет поймано void звездочка, и он предложит вам поменять
[36:04.120 --> 36:12.040]  их местами. Да, ну и вообще такое правило, что если у вас, что нужно идти от более специфичного типа к менее
[36:12.040 --> 36:15.520]  специфичному. То есть если мы тут действительно, вот представим себе, что вот мы в этом примере
[36:15.520 --> 36:21.320]  действительно хотим поймать и int звездочку, и void звездочку. То есть если мы хотим, чтобы когда
[36:21.320 --> 36:25.480]  вы бросаете int звездочку, у вас ловилось, у вас срабатывал кетч-блок, который соответствует int
[36:25.480 --> 36:29.760]  указателю. Если вы бросаете какой-нибудь другой указатель, вы хотите, чтобы работал именно кетч-блок,
[36:29.760 --> 36:34.080]  который соответствует void звездочки. Ну чтобы решить эту проблему, достаточно просто поменять
[36:34.080 --> 36:40.640]  местами эти кетч-блоки. Теперь у вас в первом будет рассматриваться кетч-блок, которому int указатель,
[36:40.640 --> 36:44.240]  а дальше если int указатель не подходит, то будет рассматриваться void звездочка, который поймает
[36:44.240 --> 36:54.680]  вообще в принципе любой указатель. Ну и то же самое касается кетч-блока, в котором два класса
[36:54.680 --> 37:00.520]  связаны иерархией наследования. Вот в этом случае, если вы хотите ловить объекты типа B в кетч-блоке B,
[37:00.520 --> 37:05.040]  а все остальные объекты в кетч-блоке A, то вы меняете их местами и тогда у вас сначала проверяется
[37:05.040 --> 37:10.840]  верно ли что объект типа B. Если объект не типа B, то дальше проверяете объект типа A и все нормально работает.
[37:10.840 --> 37:24.960]  Так, продолжим говорить про кетч-блок. В следующую очередь у нас ловили исключение по ссылке.
[37:24.960 --> 37:31.320]  Помимо того, что исключение можно ловить просто по значению, исключение можно ловить и по ссылке
[37:31.320 --> 37:38.240]  на объект. Так как я сказал, что исключение хранится в некоторой специальной отдельной области памяти,
[37:38.240 --> 37:43.520]  ну опять же исключение, за ним стоит отдельная область памяти, соответственно это означает,
[37:43.520 --> 37:48.720]  что само исключение, исключение которое летит, оно является lvalue. Если это lvalue, то на него
[37:48.720 --> 37:54.320]  можно создать ссылку. Собственно, ровно это и происходит. Допустим, мы бросаем исключение типа
[37:54.320 --> 37:59.600]  вектор из миллиона элементов. Сразу скажу, бросать вектор очень плохая идея, не бросайте вектор,
[37:59.600 --> 38:05.040]  вообще в принципе не бросайте ни инты, ни даблы о том, что нужно бросать, мы поговорим позже.
[38:05.040 --> 38:12.040]  Допустим, у вас судьба вынудила бросить какой-нибудь тяжелый объект, например, вектор. Чтобы его не
[38:12.040 --> 38:17.120]  копировать, то есть понятное дело, что если вы напишете кетч и дальше напишете std-вектор и как-то
[38:17.120 --> 38:21.400]  обзовете эту переменную, то у вас этот объект исключения скопируется внутрь блока кетч, то есть все
[38:21.400 --> 38:27.200]  эти миллионы объектов будут скопированы и будет не очень эффективно. Чтобы не копировать лишний
[38:27.200 --> 38:31.760]  раз исключения, их можно принимать по ссылке, либо по обычной ссылке, либо по константной ссылке,
[38:31.760 --> 38:36.160]  как здесь. В этом случае объекты исключения копироваться не будут, они просто будут ссылаться
[38:36.160 --> 38:43.760]  на ту область памяти, в которой хранится текущее исключение. Кроме того, еще одно преимущество
[38:43.760 --> 38:49.760]  ловли исключений по ссылке заключается в том, что если объект класса, которого вы бросаете,
[38:49.760 --> 38:54.880]  является наследником чего-то там или в принципе обладает полиморфным поведением, то есть у него есть
[38:54.880 --> 38:59.040]  какие-то виртуальные функции и так далее, то мы помним, что с помощью ссылок у нас становится
[38:59.040 --> 39:04.560]  доступно полиморфное поведение. То есть если у вас есть объект типа B, он наследован от A и,
[39:04.560 --> 39:10.640]  скажем, внутри A есть некоторая виртуальная функция, виртуальная функция F, и вы бросаете,
[39:10.640 --> 39:21.400]  скажем, какой-нибудь объект типа B, а потом ловите его по ссылке на A, то если вы через A вызываете
[39:21.400 --> 39:26.160]  F, то у вас будет вызываться именно та функция F, которая находится внутри объекта B, а не внутри
[39:26.160 --> 39:30.800]  объекта A. То есть это обычное классическое полиморфное поведение. Это бывает полезно, и более того,
[39:30.800 --> 39:34.320]  это нам пригодится, когда мы будем говорить про стандартную иерархию исключения в языке C++.
[39:34.320 --> 39:42.200]  Мораль ловить исключения по ссылке хорошо, не по ссылке плохо. Ну, опять же, если речь не идет
[39:42.200 --> 39:50.280]  о какие-то примитивные базовые типы. Теперь, допустим, мы не знаем, что мы хотим поймать. Ну,
[39:50.280 --> 39:57.760]  давайте так. Мы не знаем, какие типы исключений бросают в наши функции. Ну или в принципе нам
[39:57.760 --> 40:03.240]  не важно, что они бросают, нам важно поймать какую-то ошибку и что-то сделать дополнительное,
[40:03.240 --> 40:07.920]  дай как-то ее обработать. В этом случае есть специальный синтактик с кетч с многоточием. Вот если
[40:07.920 --> 40:12.960]  вы пишете кетч многоточие, то это означает, что данный блок кетч ловит вообще любое исключение.
[40:12.960 --> 40:21.000]  То есть, опять же, если вы дошли до блока кетча, то есть, понятное дело, у вас происходит проверка по
[40:21.000 --> 40:24.840]  порядку, то есть сначала там один кетч блок, второй кетч блок. Если вы что-то не поймали,
[40:24.840 --> 40:29.160]  попали в кетч блок с многоточием, то вот это любое исключение, какое бы оно ни летело, оно будет
[40:29.160 --> 40:34.520]  именно внутри кетч многоточия. Ну и дальше внутри него вы можете как-то там обработать ошибку и
[40:34.520 --> 40:39.560]  сделать там что-то более-менее полезное. Но у этого способа есть недостаток, который заключается в том,
[40:39.560 --> 40:44.200]  что если вы писали кетч многоточие, то у вас в принципе нет никакой возможности получить значение
[40:44.200 --> 40:48.280]  собственно ошибки. То есть, когда вы пишете кетч многоточие, вы говорите что, то есть вы как бы
[40:48.280 --> 40:52.000]  дополнительно говорите о том, что вы не знаете, какого типа у вас исключения. И соответственно,
[40:52.000 --> 40:59.040]  ну какого-то варианта узнать про это нет. То есть, если у вас есть кетч с обычной типа
[40:59.040 --> 41:03.480]  или комплизированный кетч, то вы можете создать там переменную этого типа и как-то с ней работать.
[41:03.480 --> 41:06.960]  То есть вы можете посмотреть, что там лежит, вы можете оттуда, например, вынуть какой-то там информацию
[41:06.960 --> 41:13.440]  об ошибке, выбросать вектор ошибок. То есть с кучей вообще все плохо пошло, у вас там миллион ошибок.
[41:13.440 --> 41:16.680]  Вот вы можете каждый из них посмотреть. В случае кетч многоточий у вас нет возможности заглянуть
[41:16.680 --> 41:21.600]  в объект исключения и понять, что происходит. То есть вы можете делать какие-то некоторые общие действия.
[41:21.600 --> 41:29.480]  Ну и, собственно, ключевой вопрос. Хорошо, что делать в кетчблоке? То есть, смотрите, казалось бы,
[41:29.480 --> 41:36.680]  вот все то, что мы до этого обсуждали, там, не знаю, закончилась память, сервер не отвечает и так далее.
[41:36.680 --> 41:41.720]  То есть кажется, что мы это починить не можем. То есть кажется, что есть причины, которые от нас не зависит,
[41:41.720 --> 41:48.520]  и, наверное, единственный способ нам что-то сделать это просто завершить программу. Ну, как мы говорили,
[41:48.760 --> 41:52.160]  что, как мы говорили, это не всегда вариант, так как, если вы там пишете какое-то серверное
[41:52.160 --> 41:55.660]  приложение или там просто нарисовать какой-то сервер, то, если у вас там сервер из-за любого ошибки
[41:55.660 --> 41:59.720]  падает, то это не очень хороший сервер. То есть, мы хотим, чтобы там ошибка была поймана,
[41:59.720 --> 42:05.640]  была как-то обработана и дальше нам мы что-то делали. Вот. Соответственно, что обычно делают в кетчблоке,
[42:05.640 --> 42:10.840]  для чего нужен кетчблок? Ну, первое очевидное, очевидное, что можно делать в кетчблоке,
[42:10.840 --> 42:15.240]  это попробовать починить проблему. Да, то есть если вы понимаете, что у вас там произошло деление на ноль,
[42:15.240 --> 42:23.240]  то вы можете внутри качблока написать на экран, что деление на ноль запрещено и дальше продолжить принимать пользовательский ввод, и так далее.
[42:23.240 --> 42:29.240]  Еще одна причина, по которой нужно использовать качблок, это чтобы избежать усечек памяти.
[42:29.240 --> 42:37.240]  Пример. У нас есть функция f, и она выделяет некоторую динамическую память.
[42:37.240 --> 42:40.240]  То есть выделяет в динамической области памяти некоторый int.
[42:40.240 --> 42:45.240]  Дальше вы понимаете, что у вас есть функция g, которая потенциально опасна, которая бросает исключения.
[42:45.240 --> 42:52.240]  Допустим, я тут не написал tri-catch-блок. Что произойдет, если у меня внутри функции g выскочит исключение?
[42:52.240 --> 43:00.240]  Очевидная утечка памяти. Потому что что происходит, когда у меня из функции выбрасывает исключение?
[43:00.240 --> 43:07.240]  Если функция g бросила исключение, и я это исключение не обрабатываю, у меня функция завершает работу, все локальные перемены уничтожаются.
[43:07.240 --> 43:13.240]  А если я выделил память с помощью new, то delete для нее, естественно, автоматически не вызывается.
[43:13.240 --> 43:17.240]  То есть это абсолютно на вашей совести.
[43:17.240 --> 43:26.240]  Поэтому, если вы хотите, чтобы при вылете исключений из какой-либо потенциально опасной функции у вас не возникало проблем с утечкой памяти,
[43:26.240 --> 43:29.240]  с утечкой каких-то других ресурсов, вы можете сделать следующую вещь.
[43:29.240 --> 43:35.240]  Вы можете сказать tri, обернуть потенциально опасную функцию или функцию i в этот tri-bloc,
[43:35.240 --> 43:43.240]  дальше поймать исключение, и соответственно внутри этого catch-bloc вы делаете delete-ptr, то есть очищаете ресурсы.
[43:43.240 --> 43:46.240]  Здесь пример функции, которая делает следующую вещь.
[43:46.240 --> 43:50.240]  Вы пытаетесь выполнить какую-то функцию, если у нее все получилось, то вы идете дальше.
[43:50.240 --> 43:57.240]  Если у нее что-то не получилось, вы удаляете память, присваиваете этому указателю null-ptr и идете дальше.
[43:58.240 --> 44:08.240]  Понятно? Чтобы избежать утечек памяти, мы используем tri-catch-bloc, и внутри catch-bloc выполняем delete.
[44:08.240 --> 44:15.240]  Этот пример немного странный, потому что он говорит следующее.
[44:15.240 --> 44:20.240]  Если в функции g произошла какая-то ошибка, то я должен удалить память, присвоить ptr и null-ptr,
[44:20.240 --> 44:24.240]  и в этом примере у меня функция f продолжает нормально функционировать.
[44:25.240 --> 44:30.240]  Наверное, хотелось бы сделать следующую вещь. Для чего я поймал исключение в этом примере?
[44:30.240 --> 44:36.240]  В этом примере я поймал исключение для того, чтобы его поймать и очистить память.
[44:36.240 --> 44:42.240]  То есть я хотел избежать утечки памяти. Но при этом как обработать ошибку, я не понимаю.
[44:42.240 --> 44:45.240]  Если я не понимаю, как обработать ошибку, то что мы делаем?
[44:45.240 --> 44:52.240]  Переносим ее на следующий уровень, говорим, что мы не знаем, что делать, и пусть следующая функция с этим разбирается.
[44:52.240 --> 44:56.240]  Вот как добиться такого поведения? Можно сделать примерно следующее.
[44:56.240 --> 45:02.240]  Можно сказать, можно писать так. Я вызываю функцию g, если она зафейлилась, я удаляю указатель,
[45:02.240 --> 45:08.240]  то есть удаляю память, которая была выделена в динамической области, и дальше бросаю исключение снова.
[45:08.240 --> 45:10.240]  Ну вот так.
[45:12.240 --> 45:16.240]  То есть вы поймали исключение, там что-то сделали, и потом выбросили его заново.
[45:16.240 --> 45:18.240]  Но тут есть несколько проблем.
[45:18.240 --> 45:22.240]  Первое. Если вы пишете вот так, то что происходит? То есть мы обсуждали.
[45:22.240 --> 45:26.240]  Если вы пишете throw и дальше пишете там некоторый объект существующий,
[45:26.240 --> 45:30.240]  то в специальной области, где хранится исключение, создается его копия.
[45:30.240 --> 45:36.240]  То есть вы как бы заново создаете то же самое исключение, копируя его в ту же самую область памяти.
[45:36.240 --> 45:40.240]  Или не в ту же самую, но просто берете то же самое исключение и копируете его.
[45:40.240 --> 45:47.240]  Понятно? Ну а хотелось бы, наверное, просто из клетки выпустить то самое исключение, которое было до этого.
[45:47.240 --> 45:53.240]  То есть не создавать новое исключение, а сказать, что вот то исключение, которое было до этого, оно должно продолжить лететь.
[45:53.240 --> 45:59.240]  И вторая проблема заключается в том, что если вы тут написали кетч с многоточием?
[45:59.240 --> 46:03.240]  То есть вы не знаете какого типа исключения, вот бросает функция g.
[46:03.240 --> 46:06.240]  Ну и в принципе вам не интересно, какие исключения она бросает.
[46:06.240 --> 46:11.240]  Вам просто хочется поймать исключение произвольное, очистить память и бросить его дальше.
[46:11.240 --> 46:15.240]  Вот что нужно написать здесь, чтобы бросить исключение дальше?
[46:15.240 --> 46:18.240]  Нам же хочется, чтобы летело именно то же самое исключение.
[46:18.240 --> 46:20.240]  Мы не хотим создавать другого.
[46:20.240 --> 46:25.240]  Потому что в исходном исключении хранилась некоторая информация о том, что пошло не так в той функции.
[46:25.240 --> 46:27.240]  Проблема ясна?
[46:27.240 --> 46:33.240]  То есть мы хотим бросить не копию исключения исходного, а именно то же самое исключение, которое летело до этого.
[46:33.240 --> 46:38.240]  И это можно сделать с помощью специального синтаксиса throw без аргументов.
[46:39.240 --> 46:45.240]  Вот если вы пишете throw без аргументов, то это дословно означает перебросить старое исключение.
[46:45.240 --> 46:48.240]  То есть то же самое исключение должно лететь дальше.
[46:48.240 --> 46:53.240]  Ну условно у вас было брошено исключение, вы его поймали в кетче, заковали в клетку.
[46:53.240 --> 46:56.240]  А когда вы пишете throw без чего угодно, то есть throw и точка за пятой,
[46:56.240 --> 47:00.240]  вы из клетки выпускаете зверя, которое вы поймали до этого.
[47:00.240 --> 47:03.240]  То есть никакого дополнительного исключения не создается.
[47:03.240 --> 47:07.240]  Вы бросаете именно то же самое исключение, которое летело до этого.
[47:07.240 --> 47:14.240]  И поэтому нормальный пример, который фиксит вот проблему цитички памяти, выглядит так.
[47:14.240 --> 47:17.240]  Вы потенциально опасный код оборачиваете в трайблок.
[47:17.240 --> 47:21.240]  Дальше, если там что-то пошло не так, вы пишете catch, ловите что угодно,
[47:21.240 --> 47:25.240]  нам вообще говоря, не важно что, мы очищаем память и дальше перебрасываем исключение дальше.
[47:25.240 --> 47:29.240]  То есть та функция, которая нас вызывала, пусть разбирается дальше с этой проблемой.
[47:31.240 --> 47:35.240]  Вот такой пример.
[47:37.240 --> 47:42.240]  Ну и наконец, собственно, вернемся к разговору про рай.
[47:42.240 --> 47:46.240]  Напомню, когда-то давно на лекции по деструкторам мы говорили,
[47:46.240 --> 47:50.240]  что есть такая идиома языка C++, resource acquisition из initialization.
[47:50.240 --> 47:52.240]  То есть захват ресурса есть инициализация.
[47:52.240 --> 47:57.240]  И мы тогда говорили такие вещи, что если мы реализуем эту идиому,
[47:57.240 --> 48:00.240]  то есть если мы захватываем ресурс в конструкторе, освобождаем его в деструкторе,
[48:00.240 --> 48:02.240]  то это все классно, здорово.
[48:02.240 --> 48:05.240]  Нам не нужно задуматься о том, чтобы автоматически вызывать самостоятельно delete.
[48:05.240 --> 48:09.240]  То есть вы в первом задании написали свой класс динамические строки,
[48:09.240 --> 48:13.240]  мы написали умные указатели и так далее.
[48:13.240 --> 48:16.240]  В общем, теперь мы память вообще не управляем.
[48:16.240 --> 48:18.240]  И вообще удобство использования, конечно, здорово.
[48:18.240 --> 48:21.240]  То есть здорово, что мы вручную не пишем new, delete.
[48:21.240 --> 48:26.240]  Но дополнительно преимущество стоит в том, что это рай.
[48:26.240 --> 48:29.240]  Оно играет новыми красками, когда мы говорим про исключение.
[48:29.240 --> 48:31.240]  Вот смотрите.
[48:31.240 --> 48:37.240]  Чтобы избежать утечки памяти в случае, когда я выделяю сырую память,
[48:37.240 --> 48:39.240]  то есть работаю явно с new и delete,
[48:39.240 --> 48:42.240]  мне нужно написать вот такое количество кода.
[48:42.240 --> 48:46.240]  А что произойдет, если я заменю, скажем так, обычный сырой указатель,
[48:46.240 --> 48:49.240]  то есть обычное new, на умный указатель?
[48:49.240 --> 48:54.240]  Только оказывается, что мне будет достаточно написать всего лишь две строки.
[48:54.240 --> 48:57.240]  То есть смотрите, если я создаю умный указатель,
[48:57.240 --> 48:59.240]  то я в принципе могу не задумываться об исключениях.
[48:59.240 --> 49:03.240]  Точнее о том, что у меня в результате выброса какого-то исключения
[49:03.240 --> 49:05.240]  будет утечка памяти.
[49:05.240 --> 49:06.240]  Почему?
[49:06.240 --> 49:10.240]  Потому что если вдруг G у меня бросает исключение,
[49:10.240 --> 49:11.240]  то что у меня происходит?
[49:11.240 --> 49:13.240]  У меня происходит раскрутка стека.
[49:13.240 --> 49:15.240]  Или схлопывание стека.
[49:15.240 --> 49:18.240]  А при схлопывании стека у меня уничтожаются все локальные переменные.
[49:18.240 --> 49:21.240]  Хорошо, у меня уничтожается локальная переменная ptr.
[49:21.240 --> 49:24.240]  Но если раньше ptr у меня уничтожалось как адрес,
[49:24.240 --> 49:26.240]  при уничтожении адреса у вас delete не вызывается,
[49:26.240 --> 49:30.240]  то здесь при уничтожении ptr у вас вызывается уник ptr,
[49:30.240 --> 49:32.240]  то есть деструктор умного указателя.
[49:32.240 --> 49:36.240]  А в деструкторе умного указателя у вас происходит ощущение памяти.
[49:36.240 --> 49:38.240]  Понятно?
[49:38.240 --> 49:44.240]  То есть мораль состоит в том, что когда вы используете всякие обертки вида умных указателей,
[49:44.240 --> 49:47.240]  вектор, стринг и так далее, у вас код становится более безопасным.
[49:47.240 --> 49:50.240]  Более безопасным в том смысле, что если вы пропустили какое-то исключение,
[49:50.240 --> 49:53.240]  то у вас не происходит никаких утечек памяти, никаких утечек ресурсов,
[49:53.240 --> 49:56.240]  потому что объекты сами контролируют свое время жизни,
[49:56.240 --> 50:02.240]  и даже в случае исключения при раскрутке стека эти объекты будут корректно освобождены и очищены.
[50:02.240 --> 50:06.240]  В отличие от сырых указателей или сырых ресурсов.
[50:06.240 --> 50:09.240]  Мораль ясна?
[50:09.240 --> 50:13.240]  На самом деле, RAI это как раз-таки скорее про безопасность,
[50:13.240 --> 50:17.240]  в таком смысле, а не про безопасность, что ой, мы забыли написать delete.
[50:17.240 --> 50:22.240]  То, что вы забыли написать delete, это, конечно, приятный бонус,
[50:22.240 --> 50:25.240]  что диструкторы автоматически все делают из-за вас.
[50:25.240 --> 50:30.240]  Но в случае написания безопасного кода RAI очень сильно помогает
[50:30.240 --> 50:33.240]  и позволяет избегать таких больших длинных конструкций,
[50:33.240 --> 50:36.240]  вида try-catch и так далее.
[50:36.240 --> 50:39.240]  Хорошо.
[50:39.240 --> 50:45.240]  На этом основная часть про try-catch и обработку исключения на сегодня закончим.
[50:45.240 --> 50:48.240]  Продолжим в следующий раз.
[50:48.240 --> 50:51.240]  Сейчас перейдем тоже к разговору про исключение,
[50:51.240 --> 50:55.240]  но про некоторую другую пастась, именно про статическую спецификацию исключений.
[50:55.240 --> 50:59.240]  Но, в частности, поговорим про слово noexcept
[50:59.240 --> 51:02.240]  и наконец поймем, что оно означает.
[51:02.240 --> 51:08.240]  Когда мы писали свой конструктор перемещений и пресваивающие перемещения,
[51:08.240 --> 51:11.240]  я говорил, что нужно их перемещать в noexcept.
[51:11.240 --> 51:15.240]  На самом деле не станет понятно почему, но станет понятно, что оно означает.
[51:15.240 --> 51:19.240]  Для чего мы пишем noexcept, станет понятно в следующий раз.
[51:19.240 --> 51:22.240]  Я умею держать интригу.
[51:22.240 --> 51:25.240]  Что означает слово noexcept?
[51:25.240 --> 51:28.240]  Noexcept – это спецификатор.
[51:28.240 --> 51:33.240]  Один из смыслов слова noexcept – это спецификатор.
[51:33.240 --> 51:37.240]  С помощью этого спецификатора вы можете поместить функцию как такую,
[51:37.240 --> 51:42.240]  что она не бросает исключений, или она не генерирует исключений.
[51:42.240 --> 51:46.240]  Если вы в конце функции написали, что написали noexcept,
[51:46.240 --> 51:49.240]  то вы говорите или обещаете компилятору и вообще всем остальным,
[51:49.240 --> 51:52.240]  что эта функция точно генерирует исключений не будет.
[51:52.240 --> 51:55.240]  В частности, когда мы писали, что конструктор перемещения noexcept
[51:55.240 --> 51:58.240]  или операция или перемещающий пресваивание noexcept,
[51:58.240 --> 52:01.240]  мы говорили, что эти функции не бросают исключения.
[52:01.240 --> 52:04.240]  Noexcept – это просто некоторое обещание.
[52:04.240 --> 52:07.240]  Что будет, если это обещание нарушить?
[52:07.240 --> 52:10.240]  Вообще говоря, я никому не рекомендую обманывать компилятора.
[52:10.240 --> 52:14.240]  Это не касается не только noexcept, а вообще любой другой штуки.
[52:14.240 --> 52:17.240]  Если вы бросаете исключения внутри функции,
[52:17.240 --> 52:21.240]  которая помещена noexcept, то вашу программу просто убивают.
[52:21.240 --> 52:24.240]  Вот здесь и сейчас.
[52:24.240 --> 52:27.240]  Без возможности восстановления, без возможности обработать ошибку и так далее.
[52:27.240 --> 52:30.240]  Если вы бросаете исключения из функции, которая помещена noexcept,
[52:30.240 --> 52:33.240]  то это автоматическое прекращение работы программы,
[52:33.240 --> 52:36.240]  и ничего с этим не поделать.
[52:36.240 --> 52:39.240]  Поэтому если вы написали noexcept, то будьте добры гарантировать,
[52:39.240 --> 52:42.240]  что это действительно noexcept.
[52:42.240 --> 52:45.240]  Ну или не пишите noexcept, если вы не уверены, что она ничего не бросает.
[52:45.240 --> 52:48.240]  Вообще с этим понятно.
[52:48.240 --> 52:52.240]  Вообще с правилом использования слова noexcept есть более сильное правило,
[52:52.240 --> 52:57.240]  которое называется правило, по-моему, Лакоса по имени,
[52:57.240 --> 53:00.240]  по имени инженера.
[53:00.240 --> 53:03.240]  Правило заключается в следующем.
[53:03.240 --> 53:06.240]  Вы должны помечать функцию noexcept не только в случае, если она ничего не бросает,
[53:06.240 --> 53:09.240]  но и в случае, если в ней внутри ничего не может произойти плохого.
[53:09.240 --> 53:12.240]  То есть у нее внутри нет undefinedBehaviour,
[53:12.240 --> 53:15.240]  у нее нет внутри чего-то другого плохого и так далее.
[53:15.240 --> 53:18.240]  То есть если функция всегда работает в штатном режиме,
[53:18.240 --> 53:21.240]  функция всегда завершается нормально,
[53:21.240 --> 53:24.240]  функция всегда делает то, что она заявляет, то это noexcept.
[53:24.240 --> 53:27.240]  Если в функции потенциально что-то может пойти не так,
[53:27.240 --> 53:30.240]  она может бросить исключения или в ней может произойти undefinedBehaviour,
[53:30.240 --> 53:33.240]  например, деление на ноль и так далее,
[53:33.240 --> 53:36.240]  то эту функцию помечать noexcept не стоит.
[53:36.240 --> 53:39.240]  Это не правило компилятора, не правило дискаута,
[53:39.240 --> 53:42.240]  просто некоторое правило приличия,
[53:42.240 --> 53:45.240]  что noexcept мы помечаем только в функции, в которых не происходит ничего плохого,
[53:45.240 --> 53:48.240]  которые не только не бросают исключения,
[53:48.240 --> 53:51.240]  а в принципе там все всегда хорошо.
[53:54.240 --> 53:57.240]  Ну и соответственно здесь пример такой, что если вы внутри функции,
[53:57.240 --> 54:00.240]  которая говорит, что она noexcept,
[54:00.240 --> 54:03.240]  то если внутри нее все-таки приходится вызывать небезопасные функции,
[54:03.240 --> 54:06.240]  то вы должны эти исключения как бы обработать сразу же внутри этой функции.
[54:06.240 --> 54:09.240]  То есть noexcept говорит лишь о том,
[54:09.240 --> 54:12.240]  что исключения не покидают эту функцию.
[54:12.240 --> 54:15.240]  То есть внутри этой функции в принципе исключения создаваться могут.
[54:15.240 --> 54:18.240]  В частности, вы можете вызвать функцию g,
[54:18.240 --> 54:21.240]  которая потенциально может бросить исключения.
[54:21.240 --> 54:24.240]  Вот если функция g бросит исключения, то ничего плохого не будет,
[54:24.240 --> 54:27.240]  если вы это исключение обработаете.
[54:27.240 --> 54:31.240]  вы это исключение обрабатываете, то есть корректно все работает и продолжаете дальше.
[54:31.240 --> 54:33.240]  То есть это нормально.
[54:33.240 --> 54:36.240]  То есть noexcept говорит о том, что исключения из функции f не вылетают.
[54:36.240 --> 54:39.240]  То, что происходит там внутри, в принципе, у вас не касается.
[54:43.240 --> 54:48.240]  Дальше. Следующее назначение слова noexcept это условный спецификатор.
[54:48.240 --> 54:51.240]  Значит, помимо того, что вы можете написать просто noexcept,
[54:51.240 --> 54:54.240]  вы можете написать noexcept в скобочках true.
[54:54.240 --> 54:57.240]  И это будет абсолютно то же самое, как если вы написали noexcept.
[54:57.240 --> 55:04.240]  Дальше. Если вы не пишете noexcept, то это то же самое, как если вы написали noexcept false.
[55:04.240 --> 55:10.240]  То есть существует некий дополнительный условный спецификатор noexcept,
[55:10.240 --> 55:16.240]  который принимает внутри себя некоторое выражение, которое может быть вычислено на этапе компиляции,
[55:16.240 --> 55:20.240]  и которое вычисляется либо как true, либо как false.
[55:20.240 --> 55:23.240]  И вот если оно вычисляется как true, то у вас функция noexcept.
[55:23.240 --> 55:28.240]  Если выражение вычисляется как false, то у вас функция noexcept.
[55:28.240 --> 55:31.240]  Для чего это может быть нужно?
[55:31.240 --> 55:34.240]  Игрушеч, например, вот такой. Дальше будет более смысленно.
[55:34.240 --> 55:35.240]  Ну, в общем, примерно так.
[55:35.240 --> 55:39.240]  Ну, представьте себе, что у вас есть функция h, которая является noexcept в случае,
[55:39.240 --> 55:44.240]  если у вас размер t достаточно большой, ну, например, размер типа t больше единицы,
[55:44.240 --> 55:46.240]  то есть в памяти занимает больше 0 байта.
[55:46.240 --> 55:50.240]  И является noexcept, если тип t занимает в памяти 1 байт.
[55:50.240 --> 55:52.240]  Тогда вы это условие можете записать так.
[55:52.240 --> 55:54.240]  noexcept и sizeof от t больше единицы.
[55:54.240 --> 55:57.240]  Тогда, если sizeof t действительно больше единицы, то у вас функция noexcept.
[55:57.240 --> 56:00.240]  Если sizeof t не больше единицы, то есть равен единице,
[56:00.240 --> 56:03.240]  тогда эта функция у вас не noexcept.
[56:05.240 --> 56:09.240]  Ну и третий смысл слова noexcept, то есть спецификатор noexcept,
[56:09.240 --> 56:13.240]  условный спецификатор noexcept и есть операция noexcept.
[56:13.240 --> 56:17.240]  Операция noexcept это некоторая операция,
[56:17.240 --> 56:21.240]  которая говорит о том, является ли выражение,
[56:21.240 --> 56:25.240]  которое стоит в круглых скобках noexcept или нет.
[56:25.240 --> 56:28.240]  Ну, условно. У вас есть функция f, которая noexcept,
[56:28.240 --> 56:30.240]  у вас есть функция g, которая не является noexcept.
[56:30.240 --> 56:33.240]  Как понять, какая из функций noexcept, какая не noexcept?
[56:33.240 --> 56:36.240]  Вы просто-напросто пишете noexcept,
[56:36.240 --> 56:40.240]  и в круглых скобках пишете условно вызов функции f,
[56:40.240 --> 56:42.240]  ну или как-то ее испортите.
[56:42.240 --> 56:44.240]  И вот эта вот операция noexcept вам возвращает,
[56:44.240 --> 56:47.240]  является ли вызов функции f безопасным или нет.
[56:47.240 --> 56:52.240]  При этом важно понимать, что noexcept на самом деле ничего не вычисляет.
[56:52.240 --> 56:55.240]  То есть если вы вызвали noexcept и в круглых скобках f,
[56:55.240 --> 56:57.240]  то f на самом деле вызвано не будет.
[56:57.240 --> 57:00.240]  То есть на accept он просто на этапе компиляции проверяет вот
[57:00.240 --> 57:03.240]  потенциально вызов функции f может привести к чему-то плохому или нет.
[57:03.240 --> 57:05.240]  Ну, равно как sizeof.
[57:05.240 --> 57:08.240]  Да, sizeof он тоже не вычисляет, что стоит у него в круглых скобках.
[57:08.240 --> 57:10.240]  Sizeof просто там смотрит на выражение,
[57:10.240 --> 57:12.240]  смотрит на то, что он возвращает,
[57:12.240 --> 57:15.240]  и вот то, что он возвращает, он как бы говорит, какой у него размер байтов.
[57:15.240 --> 57:17.240]  Вот с noexcept то же самое.
[57:17.240 --> 57:20.240]  Noexcept просто анализирует выражение, которое стоит в круглых скобках,
[57:20.240 --> 57:23.240]  и говорит, вот оно потенциально опасное или безопасное.
[57:23.240 --> 57:26.240]  Потенциально опасное будет в случае функций?
[57:26.240 --> 57:31.240]  Нет, в случае функций noexcept просто проверяет,
[57:31.240 --> 57:34.240]  все ли функции, которые написаны внутри круглых скобок,
[57:34.240 --> 57:36.240]  помещены как noexcept или нет.
[57:36.240 --> 57:41.240]  То есть в случае noexcept от f будет true, в случае noexcept от g будет false.
[57:41.240 --> 57:43.240]  То есть noexcept просто смотрит выражение
[57:43.240 --> 57:45.240]  и проверяет, есть ли там какие-то конструкции,
[57:45.240 --> 57:47.240]  которые могут бросить исключение.
[57:47.240 --> 57:50.240]  То есть есть ли там конструкции, которые не помещены как noexcept.
[57:50.240 --> 57:52.240]  И все.
[57:52.240 --> 57:54.240]  Ну вот в частности, если вы спросите,
[57:54.240 --> 57:56.240]  является ли noexcept 1 делить на 0,
[57:56.240 --> 57:58.240]  просто noexcept 1 делить на 0,
[57:58.240 --> 58:00.240]  то вам вернется true.
[58:00.240 --> 58:03.240]  Это выражение является noexcept. Почему?
[58:05.240 --> 58:07.240]  Потому что деление целых чисел
[58:07.240 --> 58:09.240]  никогда не приводит к выбросу исключения.
[58:09.240 --> 58:11.240]  Понятно?
[58:11.240 --> 58:15.240]  То есть операция noexcept проверяет верно лишь,
[58:15.240 --> 58:17.240]  что все, что написано из круглых скобок,
[58:17.240 --> 58:19.240]  не может бросить исключений.
[58:19.240 --> 58:21.240]  Деление на 0 не бросает исключений хотя бы потому,
[58:21.240 --> 58:24.240]  что язык C++ унаследован от языка C.
[58:24.240 --> 58:26.240]  А в языке C никаких исключений нет.
[58:26.240 --> 58:28.240]  Понятно?
[58:28.240 --> 58:32.240]  Поэтому формально 1 делить на 0 является noexcept.
[58:32.240 --> 58:35.240]  Более того, компилятор 1 делить на 0 даже вычислять не будет.
[58:35.240 --> 58:37.240]  Он просто посмотрит, что слева стоит int,
[58:37.240 --> 58:39.240]  справа стоит int.
[58:39.240 --> 58:41.240]  Если я int разделю на int, то исключений
[58:41.240 --> 58:43.240]  никакого не генерируется никогда.
[58:43.240 --> 58:45.240]  Поэтому это noexcept.
[58:45.240 --> 58:47.240]  А если я напишу v.pushback,
[58:47.240 --> 58:49.240]  ну и дальше что-то,
[58:49.240 --> 58:51.240]  то это уже false.
[58:51.240 --> 58:53.240]  Почему?
[58:53.240 --> 58:55.240]  Почему добавление в динамическую строку
[58:55.240 --> 58:57.240]  или в динамический массив,
[58:57.240 --> 58:59.240]  почему добавление в конец динамического массива
[58:59.240 --> 59:01.240]  не является noexcept?
[59:03.240 --> 59:05.240]  Да, не хватит памяти.
[59:05.240 --> 59:07.240]  Если мы добавляем что-то в динамический массив,
[59:07.240 --> 59:09.240]  в динамический массив он может расшириться.
[59:09.240 --> 59:11.240]  А когда у вас динамический массив расширяется,
[59:11.240 --> 59:13.240]  он запрашивает память. Памяти может не хватить.
[59:13.240 --> 59:15.240]  Если у вас не хватает памяти,
[59:15.240 --> 59:17.240]  то вам бросается исключение badalog.
[59:17.240 --> 59:19.240]  Поэтому вот это вот выражение
[59:19.240 --> 59:21.240]  не является noexcept не из-за того,
[59:21.240 --> 59:23.240]  что вы 1 делите на 0.
[59:23.240 --> 59:25.240]  То есть компилятор вообще плевать,
[59:25.240 --> 59:27.240]  что вы разделили, он это не вычисляет.
[59:27.240 --> 59:29.240]  Это не является noexcept,
[59:29.240 --> 59:31.240]  ровно потому, что pushback потенциально
[59:31.240 --> 59:33.240]  может бросить исключения.
[59:33.240 --> 59:35.240]  Это как noexcept.
[59:51.240 --> 59:53.240]  Если я напишу вот так,
[59:53.240 --> 59:55.240]  вот это выражение тоже всегда true.
[59:57.240 --> 59:59.240]  Почему? Потому что выход за границу массива
[59:59.240 --> 01:00:01.240]  тоже никогда не приводит к исключениям.
[01:00:01.240 --> 01:00:03.240]  То есть если вы выходите за границу
[01:00:03.240 --> 01:00:05.240]  обычного C массива,
[01:00:05.240 --> 01:00:07.240]  то C и C++ не проверяют выход за границу.
[01:00:07.240 --> 01:00:09.240]  Поэтому это undefined behavior.
[01:00:09.240 --> 01:00:11.240]  Вектор или любой динамическая строка,
[01:00:11.240 --> 01:00:13.240]  динамический массив, они тоже не проверяют
[01:00:13.240 --> 01:00:15.240]  выход за границу массива.
[01:00:15.240 --> 01:00:17.240]  Поэтому вот это выражение,
[01:00:17.240 --> 01:00:19.240]  несмотря на то, что оно потенциально опасное,
[01:00:19.240 --> 01:00:21.240]  оно всегда noexcept, потому что оно никогда не так.
[01:00:21.240 --> 01:00:23.240]  Оно является noexcept,
[01:00:23.240 --> 01:00:25.240]  потому что функция...
[01:00:25.240 --> 01:00:27.240]  Сейчас. Я соврал.
[01:00:27.240 --> 01:00:29.240]  Я соврал.
[01:00:29.240 --> 01:00:31.240]  Тут правило такое.
[01:00:31.240 --> 01:00:33.240]  Если v это обычный массив,
[01:00:33.240 --> 01:00:35.240]  ну, C-шный массив,
[01:00:37.240 --> 01:00:39.240]  C-шный массив,
[01:00:41.240 --> 01:00:43.240]  то эта штука скорее всего является noexcept. Почему?
[01:00:43.240 --> 01:00:45.240]  Потому что
[01:00:45.240 --> 01:00:47.240]  язык C++
[01:00:49.240 --> 01:00:51.240]  совместим с языком C.
[01:00:51.240 --> 01:00:53.240]  В языке C никаких исключений нет.
[01:00:53.240 --> 01:00:55.240]  Поэтому это выражение с обычными C-шными массивами
[01:00:55.240 --> 01:00:57.240]  и динамическими массивами
[01:00:57.240 --> 01:00:59.240]  оно привести к выбросу исключений не может.
[01:00:59.240 --> 01:01:01.240]  Так это выражение привести к выбросу исключений не может,
[01:01:01.240 --> 01:01:03.240]  оно noexcept. То есть тут true.
[01:01:03.240 --> 01:01:05.240]  Вот первый факт понятен, вот этот.
[01:01:05.240 --> 01:01:07.240]  Если вы вызываете noexcept,
[01:01:07.240 --> 01:01:09.240]  и вот тут написан обычный C-шный массив,
[01:01:09.240 --> 01:01:11.240]  то это всегда noexcept,
[01:01:11.240 --> 01:01:13.240]  потому что C-шные массивы не образуют исключений.
[01:01:13.240 --> 01:01:15.240]  Теперь, если v это вектор,
[01:01:17.240 --> 01:01:19.240]  то здесь noexcept false.
[01:01:19.240 --> 01:01:21.240]  Почему это так?
[01:01:21.240 --> 01:01:23.240]  Потому что
[01:01:25.240 --> 01:01:27.240]  оператор
[01:01:27.240 --> 01:01:29.240]  квадратной скобки,
[01:01:29.240 --> 01:01:31.240]  который реализован
[01:01:31.240 --> 01:01:33.240]  внутри вектора,
[01:01:33.240 --> 01:01:35.240]  не помечен как noexcept.
[01:01:39.240 --> 01:01:41.240]  Потому что правило лакоса.
[01:01:41.240 --> 01:01:43.240]  Правило говорит о том, что
[01:01:43.240 --> 01:01:45.240]  несмотря на то, что оператор квадратной скобки
[01:01:45.240 --> 01:01:47.240]  не может выбросить исключения,
[01:01:47.240 --> 01:01:49.240]  потенциально внутри вектора что-то может пойти не так.
[01:01:49.240 --> 01:01:51.240]  Поэтому оно не помечается noexcept.
[01:01:53.240 --> 01:01:55.240]  То есть noexcept проверяет,
[01:01:55.240 --> 01:01:57.240]  верно ли что та функция, которую я вызываю,
[01:01:57.240 --> 01:01:59.240]  является noexcept.
[01:01:59.240 --> 01:02:01.240]  Внутри вектора оператор квадратной скобки
[01:02:01.240 --> 01:02:03.240]  noexcept не является.
[01:02:03.240 --> 01:02:05.240]  Поэтому здесь будет noexcept false.
[01:02:11.240 --> 01:02:13.240]  Ну и финальный пример.
[01:02:13.240 --> 01:02:15.240]  Соответственно,
[01:02:15.240 --> 01:02:17.240]  мы приводили для условного спискатора noexcept,
[01:02:17.240 --> 01:02:19.240]  мы приводили некоторые игрушечные примеры,
[01:02:19.240 --> 01:02:21.240]  что у меня функция является noexcept,
[01:02:21.240 --> 01:02:23.240]  если size of t больше единицы,
[01:02:23.240 --> 01:02:25.240]  и не noexcept, если size of t не больше единицы.
[01:02:25.240 --> 01:02:27.240]  Но это примеры игрушечные,
[01:02:27.240 --> 01:02:29.240]  более-менее согласованные с реальностью примеров.
[01:02:29.240 --> 01:02:31.240]  Вот я пишу функцию сам.
[01:02:31.240 --> 01:02:33.240]  Давайте пока сюда только посмотрим.
[01:02:33.240 --> 01:02:35.240]  Я пишу функцию суммы,
[01:02:35.240 --> 01:02:37.240]  которая просто принимает два произвольных значения,
[01:02:37.240 --> 01:02:39.240]  абсолютно произвольные значения,
[01:02:39.240 --> 01:02:41.240]  x и y,
[01:02:41.240 --> 01:02:43.240]  и складывает.
[01:02:43.240 --> 01:02:45.240]  И я задаю вам вопрос, верно ли что
[01:02:45.240 --> 01:02:47.240]  сумма x и y noexcept?
[01:02:53.240 --> 01:02:55.240]  Сложение двух нотов когда-нибудь может привести
[01:02:55.240 --> 01:02:57.240]  к исключению?
[01:02:57.240 --> 01:02:59.240]  А двух доблов?
[01:02:59.240 --> 01:03:01.240]  Я складываю два значения,
[01:03:01.240 --> 01:03:03.240]  никогда не бросаю исключения,
[01:03:03.240 --> 01:03:05.240]  поэтому надо пометь noexcept.
[01:03:05.240 --> 01:03:07.240]  Что может быть проблема?
[01:03:09.240 --> 01:03:11.240]  Я утверждаю, что сложение двух чисел,
[01:03:11.240 --> 01:03:13.240]  так как сложение двух чисел,
[01:03:13.240 --> 01:03:15.240]  они пришли к нам язык аси,
[01:03:15.240 --> 01:03:17.240]  язык си не бросает исключения,
[01:03:17.240 --> 01:03:19.240]  поэтому сложение двух чисел никогда не бросает исключений.
[01:03:19.240 --> 01:03:21.240]  Что?
[01:03:21.240 --> 01:03:23.240]  Указатель.
[01:03:23.240 --> 01:03:25.240]  Указатель я в принципе не могу складывать.
[01:03:29.240 --> 01:03:31.240]  Да, отлично.
[01:03:31.240 --> 01:03:33.240]  Пора избавляться,
[01:03:33.240 --> 01:03:35.240]  порадим мы первый семестр,
[01:03:35.240 --> 01:03:37.240]  когда мы работали с примитивными типами и так далее.
[01:03:37.240 --> 01:03:39.240]  У нас ООП.
[01:03:39.240 --> 01:03:41.240]  В ООП мы пишем собственные классы,
[01:03:41.240 --> 01:03:43.240]  у нас есть другие классы.
[01:03:43.240 --> 01:03:45.240]  В принципе,
[01:03:45.240 --> 01:03:47.240]  исходное предположение о том,
[01:03:47.240 --> 01:03:49.240]  что в функцию суммы могут прийти только числа,
[01:03:49.240 --> 01:03:51.240]  неверно.
[01:03:51.240 --> 01:03:53.240]  У нас есть собственные классы,
[01:03:53.240 --> 01:03:55.240]  у нас есть класс строки.
[01:03:55.240 --> 01:03:57.240]  Строки все плюс-плюс можно складывать с помощью плюсов,
[01:03:57.240 --> 01:03:59.240]  это просто-напросто конкатинация строк.
[01:03:59.240 --> 01:04:01.240]  Что если я функцию сам передам две строки
[01:04:01.240 --> 01:04:03.240]  и буду складывать две строки?
[01:04:03.240 --> 01:04:05.240]  Верно ли, что сложение двух строк или конкатинация их?
[01:04:05.240 --> 01:04:07.240]  Это noexcept.
[01:04:07.240 --> 01:04:09.240]  Нет, потому что почему?
[01:04:09.240 --> 01:04:11.240]  Ключение двух строк к чему приводит?
[01:04:11.240 --> 01:04:13.240]  Вы выделяете память для хранения суммы двух строк
[01:04:13.240 --> 01:04:15.240]  и копируете две строки туда.
[01:04:15.240 --> 01:04:17.240]  При выделении памяти
[01:04:17.240 --> 01:04:19.240]  эта память может закончиться
[01:04:19.240 --> 01:04:21.240]  и вам будет выброшено исключение.
[01:04:21.240 --> 01:04:23.240]  Поэтому x плюс y в случае строк
[01:04:23.240 --> 01:04:25.240]  может быть исключение.
[01:04:25.240 --> 01:04:27.240]  И вот тут возникает проблема.
[01:04:27.240 --> 01:04:29.240]  Для int'ов
[01:04:29.240 --> 01:04:31.240]  x плюс y это noexcept,
[01:04:31.240 --> 01:04:33.240]  для строк x плюс y это не noexcept.
[01:04:33.240 --> 01:04:35.240]  Как понять,
[01:04:35.240 --> 01:04:37.240]  помечает ли мне функцию шаблонную сумму noexcept или нет?
[01:04:37.240 --> 01:04:39.240]  Ну вот, собственно,
[01:04:39.240 --> 01:04:41.240]  использовать условный спецификатор noexcept
[01:04:41.240 --> 01:04:43.240]  и операцию noexcept.
[01:04:43.240 --> 01:04:45.240]  Как это работает?
[01:04:45.240 --> 01:04:47.240]  Я говорю, что у меня функция noexcept
[01:04:47.240 --> 01:04:49.240]  при условии,
[01:04:49.240 --> 01:04:51.240]  что noexcept x плюс y.
[01:04:53.240 --> 01:04:55.240]  Что делает noexcept x плюс y?
[01:04:55.240 --> 01:04:57.240]  Noexcept x плюс y проверяет.
[01:04:57.240 --> 01:04:59.240]  Верно ли, что x плюс y никогда не бросает исключений?
[01:04:59.240 --> 01:05:01.240]  В случае int'ов это правда.
[01:05:01.240 --> 01:05:03.240]  В случае int'ов
[01:05:03.240 --> 01:05:05.240]  вот этот внутренний noexcept
[01:05:05.240 --> 01:05:07.240]  возвращает true.
[01:05:07.240 --> 01:05:09.240]  Поэтому тут в итоге получается noexcept от true.
[01:05:09.240 --> 01:05:11.240]  А noexcept от true это то же самое, что noexcept.
[01:05:11.240 --> 01:05:13.240]  Для строк.
[01:05:13.240 --> 01:05:15.240]  Если я спрашиваю для строк,
[01:05:15.240 --> 01:05:17.240]  верно ли, что noexcept x плюс y?
[01:05:17.240 --> 01:05:19.240]  Noexcept x плюс y для строк возвращает false.
[01:05:19.240 --> 01:05:21.240]  Потому что x плюс y для строк
[01:05:21.240 --> 01:05:23.240]  потенциально может бросить исключение.
[01:05:23.240 --> 01:05:25.240]  Поэтому тут возникает noexcept в скобочках false.
[01:05:25.240 --> 01:05:27.240]  А noexcept в скобочках false
[01:05:27.240 --> 01:05:29.240]  то же самое, что мы не написали noexcept.
[01:05:31.240 --> 01:05:33.240]  Поэтому у меня функция является noexcept или не является
[01:05:33.240 --> 01:05:35.240]  noexcept, в зависимости от того,
[01:05:35.240 --> 01:05:37.240]  верно ли, что x плюс y бросает исключение или нет.
[01:05:37.240 --> 01:05:39.240]  Понятно?
[01:05:39.240 --> 01:05:41.240]  Ну вот.
[01:05:41.240 --> 01:05:43.240]  Такие дела.
[01:05:43.240 --> 01:05:45.240]  Ну и последний пункт.
[01:05:49.240 --> 01:05:51.240]  Он не связан с тем, о чем мы говорили.
[01:05:51.240 --> 01:05:53.240]  Это просто как бы некоторый факт,
[01:05:53.240 --> 01:05:55.240]  которым можно пользоваться.
[01:05:57.240 --> 01:05:59.240]  Давайте скажу следующую вещь.
[01:05:59.240 --> 01:06:01.240]  Вообще говоря,
[01:06:01.240 --> 01:06:03.240]  давайте общие слова про исключения.
[01:06:03.240 --> 01:06:05.240]  Что исключение?
[01:06:05.240 --> 01:06:07.240]  Лучше чем возврат кода ошибки в языке C.
[01:06:07.240 --> 01:06:09.240]  Во-первых, исключения они более явны.
[01:06:09.240 --> 01:06:11.240]  Скажем, вы не можете
[01:06:11.240 --> 01:06:13.240]  проигнорировать исключения.
[01:06:13.240 --> 01:06:15.240]  Проигнорировать код ошибки вы можете.
[01:06:15.240 --> 01:06:17.240]  То есть вы вызываете функцию,
[01:06:17.240 --> 01:06:19.240]  то, что она возвращается вы можете приближать и проигнорировать ошибку.
[01:06:19.240 --> 01:06:21.240]  Исключения в языке C dolly
[01:06:21.240 --> 01:06:23.240]  вы не можете проигнорировать.
[01:06:23.240 --> 01:06:25.240]  Если у вас возникает исключение,
[01:06:25.240 --> 01:06:27.240]  то у вас программа завершается.
[01:06:27.240 --> 01:06:29.240]  Это как бы более явно сказать,
[01:06:29.240 --> 01:06:33.520]  что-то идет не так, как завершить программу, короче, способов нет. Поэтому ошибки вы
[01:06:33.520 --> 01:06:39.000]  всегда обязаны обрабатывать. Вот это первый момент. Второй, еще один важный момент,
[01:06:39.000 --> 01:06:45.800]  заключается в том, что делать, если у вас исключение возникает в конструкторе?
[01:06:45.800 --> 01:06:50.000]  Вот, смотрите, допустим, у вас в конструкторе возникло исключение. Как вам сообщить о том,
[01:06:50.000 --> 01:06:53.800]  что в конструкторе возникло исключение? С помощью кода ошибки это сделать невозможно.
[01:06:53.800 --> 01:06:58.160]  Почему? Потому что конструктор ничего не возвращает. Конструктор лишь создает объект.
[01:06:58.160 --> 01:07:02.200]  А вот если вы бросаете исключение и вы бросаете исключение из конструктора, то в принципе это все
[01:07:02.200 --> 01:07:09.600]  можете обработать. То есть исключения обладают преимуществами, но и также обладают недостатками.
[01:07:09.600 --> 01:07:16.160]  Например, один из недостатков состоит в том, что они достаточно тяжелые. То есть если у вас
[01:07:16.160 --> 01:07:21.080]  возникает исключение, то в принципе программа начинает работать долго по сравнению с возрастом
[01:07:21.080 --> 01:07:26.480]  кода ошибки. Возврат кода ошибки это всегда быстро. Просто одна ретерн инструкция и так далее.
[01:07:26.480 --> 01:07:32.920]  И в принципе использовать исключение или нет, это полностью зависит от вас. На самом деле,
[01:07:32.920 --> 01:07:39.200]  в ближайший год, наверное, не от вас, а от преподавателей, от работодателей, как принято в том
[01:07:39.200 --> 01:07:44.400]  месте, где вы работаете и так далее. В частности, в Google Code Style явно прописано, что они не используют
[01:07:44.400 --> 01:07:51.200]  исключения. То есть Google решили для себя, что мы не используем исключения, мы используем кода
[01:07:51.200 --> 01:07:55.480]  ошибок и так далее. Некоторые считают наоборот, что мы используем исключение, кода ошибок это
[01:07:55.480 --> 01:08:00.440]  некоторая устаревшая вещь и так далее. И тут возникает там некоторая проблема. Ну смотри, допустим,
[01:08:00.440 --> 01:08:05.360]  вот вы приняли для себя решение, ну или кто-то принял решение, что с исключением мы не работаем.
[01:08:05.360 --> 01:08:10.360]  То есть по какой-то причине нам не нравится исключение, то есть мы считаем, что они работают долго
[01:08:10.360 --> 01:08:19.600]  и так далее. Возникает вопрос. Ну смотрите, new. Как я сказал, когда вы вызываете new, у вас
[01:08:19.600 --> 01:08:24.880]  потенциально может не хватить памяти. Если у вас не хватает памяти, то new может бросить исключение.
[01:08:24.880 --> 01:08:31.280]  И это как-то плохо соотносится с тем, что, скажем, мы не работаем с исключениями, и new бросает
[01:08:31.280 --> 01:08:35.280]  исключение. То есть все-таки в некоторых ситуациях вам, казалось бы, нужно использовать исключение.
[01:08:35.280 --> 01:08:41.040]  Но вот это на самом деле не так. Существует специальная форма оператора new, которая исключений
[01:08:41.040 --> 01:08:47.640]  не бросает. И это так называемая no throw new, то есть не бросающий new. Как это выглядит? Если вы хотите,
[01:08:47.640 --> 01:08:54.760]  чтобы new не бросал исключений, вы пишите new, дальше в круглоскобок std no throw, ну и дальше там
[01:08:55.040 --> 01:09:02.320]  обычный массив или объект, который вы хотите создать. Этот new в случае нехватки памяти бросать
[01:09:02.320 --> 01:09:08.360]  исключения не будет. А что он будет делать? Он будет просто возвращать вам nullptr или не nullptr,
[01:09:08.360 --> 01:09:13.880]  но, собственно, как сишный молок. То есть если new завершился с ошибкой, то есть ему не удалось
[01:09:13.880 --> 01:09:19.320]  выделить память или не удалось создать объекты, он вернет вам nullptr. Если ему удалось все сделать,
[01:09:19.320 --> 01:09:25.640]  то он вернет не nullptr. Поэтому в случае, когда вы используете не бросающий new, вы пишете
[01:09:25.640 --> 01:09:30.000]  ptr равно new что-то там, дальше проверяете верно ли что то, что вам вернулось, не является nullptr,
[01:09:30.000 --> 01:09:34.320]  и если это не nullptr, то вы выполняете некоторые действия. Если nullptr, значит что-то пошло не так.
[01:09:34.320 --> 01:09:42.960]  Ну, собственно, вот некоторый аналог возврата кода ошибки для new. Так, сегодня говорим про хэш таблицы.
[01:09:42.960 --> 01:09:53.960]  Общие слова. Что вы знаете вообще про структуру данных поиска? Вот, допустим, вам дан
[01:09:53.960 --> 01:09:59.520]  некоторый массив, произвольный вообще, в котором нужно найти определенный элемент. Вот дан массив
[01:09:59.520 --> 01:10:11.760]  A, нужно понять верно ли, что х лежит в массиве A. За какое время это можно сделать? Ну, за линию,
[01:10:11.760 --> 01:10:16.160]  да? Быстрее, чем за линию, скорее всего, не получится. Почему? Потому что если массив
[01:10:16.160 --> 01:10:20.320]  действительно произвольный, то в худшем случае нам придется пройтись по всем его элементам,
[01:10:20.320 --> 01:10:25.560]  и, соответственно, выяснение того, содержится ли там элемент x или нет, занимает линейное вот
[01:10:25.560 --> 01:10:31.160]  размер массива время. Хорошо. Существуют ли примеры массив, на которых, в принципе,
[01:10:31.160 --> 01:10:38.320]  можно это сделать быстрее, чем за линию? Ну, сортированный массив, да. То есть, если у вас
[01:10:38.320 --> 01:10:43.360]  массив A сортированный, то выяснение, содержится элемент в сортированном массиве или нет,
[01:10:43.360 --> 01:10:47.280]  занимает логарифмическое время. Почему? Потому что внутри массива есть некоторая определенная
[01:10:47.280 --> 01:10:51.480]  структура, мы ее понимаем, мы понимаем, что, скажем, если мы смотрим на какой-то элемент,
[01:10:51.480 --> 01:10:55.800]  то слева все меньше, справа все больше, ну и таким образом мы можем ускорить поиск.
[01:10:55.800 --> 01:11:02.280]  Окей. Ну хорошо, ну вот есть замечательный отсортированный массив, в котором поиск
[01:11:02.280 --> 01:11:06.320]  осуществляется за логарифмическое время. Ну все, отлично, логарифм на самом деле очень,
[01:11:06.800 --> 01:11:13.160]  очень хорошая функция, она на всех разумных значениях, ну скажем, принимает очень маленькое..
[01:11:13.160 --> 01:11:16.440]  Ну, короче, она принимает очень маленькое значение, поэтому, в принципе,
[01:11:16.440 --> 01:11:20.760]  логарифма нам почти всегда достаточно. Почему бы в качестве структуры данных поиска,
[01:11:20.760 --> 01:11:25.580]  скажем, мне хочется написать структуру данных следующего вида. То есть эта структура данных
[01:11:25.580 --> 01:11:28.960]  должна хранить элементы. Я в нее хочу периодически добавлять элементы,
[01:11:28.960 --> 01:11:32.780]  периодически удалятьank и-эт. Ну, и единственное там, наверное, запрос, который я хочу в ней
[01:11:32.780 --> 01:11:38.780]  подсылать, это выяснять, содержится ли в ней элемент x или нет. В чём проблема?
[01:11:40.540 --> 01:11:43.860]  Вот есть отсортированный массив, я в него периодически добавляю элементы,
[01:11:43.860 --> 01:11:49.340]  удаляю элементы, и за логическое время узнаю, что в нём хранится.
[01:11:49.340 --> 01:11:53.820]  Да, возникает естественный вопрос, как добавлять, как добавлять элементы в
[01:11:53.820 --> 01:11:56.860]  массив. Ну скажем, понятное дело, что если вы добавляете максимальный элемент, то
[01:11:56.860 --> 01:12:00.260]  вы его оставляете в конец и всё. Если вы добавляете минимальный элемент, то уже
[01:12:00.260 --> 01:12:04.580]  проблемы. Вам нужно весь массив сдвинуть целиком на одну единицу вправую и так
[01:12:04.580 --> 01:12:08.180]  далее. Если вы оставляете элементы в середину, то то же самое. Вам нужно
[01:12:08.180 --> 01:12:11.740]  потенциально расширить массив, и потом все элементы, которые там правее,
[01:12:11.740 --> 01:12:14.900]  их нужно каким-то образом сдвинуть. То есть вставка на самом деле занимает
[01:12:14.900 --> 01:12:21.140]  линейное время. Существуют ли вы структуры данных, которые
[01:12:21.140 --> 01:12:24.260]  позволяют осуществлять логарифмический поиск, но при этом вставку осуществляют
[01:12:24.260 --> 01:12:31.060]  быстрее, чем за линию? Список. А как вы в списке собираетесь искать за
[01:12:31.060 --> 01:12:39.860]  логарифмическое время? Дерево. Бинарное дерево поиска. В прошлом семестре вы
[01:12:39.860 --> 01:12:42.980]  рассматривали бинарные деревья поиска. Ну и действительно, бинарные деревья
[01:12:42.980 --> 01:12:46.500]  поиска, они тоже хранят элементы в некотором порядочном виде, а именно в виде
[01:12:46.500 --> 01:12:52.060]  бинарного дерева. В качестве левого по дереву вступаются элементы меньше,
[01:12:52.060 --> 01:12:55.660]  х в качестве правого по дереву все элементы больше х. Ну и в такой структуре
[01:12:55.660 --> 01:13:00.340]  понятное дело, то есть понятное дело, как искать поиск, по сути, выполняется
[01:13:00.340 --> 01:13:03.580]  точно так же, как в бинарном поиске, то есть в зависимости того,
[01:13:03.580 --> 01:13:08.260]  чему равен х выведется либо в левое по дереву, либо вправо по дереву. Ну и плюс
[01:13:08.260 --> 01:13:11.980]  из-за того, что это дерево, так скажем, оформлено в виде некоторого списка,
[01:13:11.980 --> 01:13:15.860]  в виде списка узлов, которые легко удалять, которые легко добавлять, у вас
[01:13:15.860 --> 01:13:19.060]  вставка удаления, в принципе, тоже может быть выполнена за логарифмическое время.
[01:13:19.060 --> 01:13:30.340]  То есть все операции работают за логарифмическое время. Хорошо, но не кажется ли вам,
[01:13:30.340 --> 01:13:35.140]  что тут есть, скажем так, некоторая неэффективность? И связана с тем, что,
[01:13:35.140 --> 01:13:40.860]  ну смотрите, если я храню какие-то элементы х, то единственное, что я хочу узнать,
[01:13:40.860 --> 01:13:45.300]  я напоминаю, единственное, что я хочу знать, это содержится элемент х в умножстве или нет?
[01:13:45.300 --> 01:13:49.860]  Но помимо того, что я храню информацию о том, хранится у меня элемент в умножстве или нет,
[01:13:49.860 --> 01:13:55.420]  я храню еще некоторые упорядоченные элементы. Ну то есть я знаю несколько больше, чем мне нужно,
[01:13:55.420 --> 01:14:02.900]  ну казалось бы. И вот как раз и на поддержание вот этого факта того, что у меня дерево,
[01:14:02.900 --> 01:14:06.940]  бинарное дерево поиска хранит, собственно, упорядоченность элементов, ну кажется,
[01:14:06.940 --> 01:14:14.020]  что я за счет этого расплачиваюсь логарифмическостью поиска. Короче говоря,
[01:14:14.020 --> 01:14:27.220]  цель хотелось бы вот эти операции выполнять за единицу, ну ровно как и вставку, удаление и поиск.
[01:14:27.220 --> 01:14:34.500]  Короче, моя мечта? Построить такую структуру данных, которая бы все это делала за единицу.
[01:14:34.500 --> 01:14:40.100]  И, ну кажется, что это можно сделать. Почему? Ну опять же, мотивацию я объяснил.
[01:14:40.100 --> 01:14:44.940]  Почему здесь логарифм? Ну в логарифме здесь возникает потому, что я очень много сил,
[01:14:44.940 --> 01:14:49.300]  трачу очень много сил и времени, трачу на то, чтобы хранить вот упорядоченность элементов между
[01:14:49.300 --> 01:14:54.500]  собой. Более того, в бинарном дереве поиска мне что необходимо? Мне необходимо, чтобы элементы
[01:14:54.500 --> 01:14:58.980]  были сравнимы друг с другом, то есть я мог сравнивать элементы на больше, меньше и так далее. Но вообще
[01:14:58.980 --> 01:15:04.100]  говоря, для всех множество это справедливо. Ну скажем, не знаю, если я хочу хранить комплексные числа,
[01:15:04.100 --> 01:15:09.380]  множество комплексных чисел, храню комплексные числа, добавляю, удаляю и спрашиваю верно лишь,
[01:15:09.380 --> 01:15:13.500]  что комплексное число х лежит в этом множестве или нет. Как его оформить в бинарном дереве поиска?
[01:15:13.500 --> 01:15:17.180]  Ну никак, там нет естественной операции меньше и больше. То есть бинарный дерево поиска здесь
[01:15:17.180 --> 01:15:27.420]  не подходит. Ну вот, соответственно, выходом из этой ситуации является хэштаблиция. Идея такая.
[01:15:27.420 --> 01:15:51.860]  Давайте для начала допустим, хотим хранить числа от нуля до m-1, где m мало. Ну что значит
[01:15:51.860 --> 01:15:56.740]  мало? Это некоторая эфемерная понятия. В общем, считаем, что m достаточно маленькая, чтобы не
[01:15:56.740 --> 01:16:02.740]  заботиться о том, что мы там много памяти потратим на хранение всех этих чисел, окей? Предложите
[01:16:02.740 --> 01:16:10.740]  какой-нибудь алгоритм, который мог хранить числа из множества 0, 1 и так далее, m-1. Массив. Какой
[01:16:10.740 --> 01:16:22.820]  массив? Какого размера? Что в нем хранится? Да, давайте заведем массив какого-нибудь b, размера m. Так,
[01:16:22.820 --> 01:16:37.220]  окей. И что будем делать? Ну да, значит, как мы будем выполнять ставку? То есть как мы будем вставлять
[01:16:37.220 --> 01:16:46.460]  элемент в наши множества? Ну очень просто. Будем говорить, что b от x равно true. Такой план. Да,
[01:16:46.460 --> 01:16:50.020]  то есть если нам приходит число x, то мы просто говорим, что если нам пришла, допустим, двойка, то мы
[01:16:50.020 --> 01:16:54.820]  говорим, что вот в этом месте мы ставим plus условно. Этот элемент в нашем множестве нет. В этом множестве
[01:16:54.820 --> 01:17:06.780]  есть. Если мы хотим удалить элемент из множества x, то что мы делаем? b от x равно false. То есть если мы в
[01:17:06.780 --> 01:17:14.740]  какой-то момент захотели удалить двойку, то мы просто здесь ударяем plus, ставим, допустим, минус. Да? Все
[01:17:14.740 --> 01:17:19.900]  работает за единицу. Ну, поиск тоже понятно как. То есть просто смотрим, что хранится в ячейке b,
[01:17:20.260 --> 01:17:24.580]  то есть plus или minus, и зависимо от этого говорим хранится наш элемент в множестве или нет. Согласны?
[01:17:24.580 --> 01:17:46.820]  Окей. Ну в общем базовая идея понятна. Идем дальше. Хорошо, а что если теперь пусть k произвольное
[01:17:46.820 --> 01:17:59.540]  множество ключей? То есть не обязательно числа, могут быть строки, могут быть комплексные числа,
[01:17:59.540 --> 01:18:03.260]  вот все что угодно. В общем, k большое это просто некоторое произвольное множество ключей,
[01:18:03.260 --> 01:18:08.660]  которое я хочу, для которого я хочу выполнять те же самые операции. Вставки в мое множество,
[01:18:08.660 --> 01:18:14.900]  удаление из множества и узнавать хранится элемент или нет. Вот. Как мне в рамках той же идеологии
[01:18:14.900 --> 01:18:23.380]  сделать, реализовать ту же самую идею? Предлагаю сделать следующую вещь. Для любого x,
[01:18:23.380 --> 01:18:35.100]  принадлежащего k, я определю h от x. h от x, который принадлежит множеству 0 и так далее,
[01:18:35.100 --> 01:18:46.500]  м-1. Так называемое hash значение. Ну и вообще говоря, функция, которая будет отображать множество
[01:18:46.500 --> 01:19:01.340]  ключей во множество 0, 1 и так далее, м-1, будет называться hash функцией. Ну а m будем называть
[01:19:01.340 --> 01:19:17.500]  размер hash таблицы. Вот. Ну то есть план такой. Допустим, у меня есть некоторая функция h,
[01:19:17.500 --> 01:19:23.380]  которая умеет отображать множество ключей в конечное множество от 0 и так далее, м-1. Ну что я
[01:19:23.380 --> 01:19:27.820]  тогда делаю? Я тогда по сути могу выполнить те же самые операции следующим образом. Вот insert,
[01:19:27.820 --> 01:19:48.860]  от x у меня выглядит так, b от h от x равно плюс, и raise от x, b от h от x равно минус. Вот такая
[01:19:48.860 --> 01:20:17.620]  идея. В чем проблема? Еще раз. Как по x получать что? А, ну в смысле, как построить функцию h?
[01:20:17.620 --> 01:20:30.700]  Как построить обратную кашу? А зачем вам обратная кашу? Вот. Да, это, ну не знаю, вы хотели вытаскать то
[01:20:30.700 --> 01:20:36.380]  или нет, но действительно. Смотрите, проблема в чем. Вообще говоря, я рассматриваю у k произвольное множество
[01:20:36.380 --> 01:20:42.540]  ключей. И вообще говоря, множество k может быть достаточно большим. Например, ну короче говоря,
[01:20:42.540 --> 01:20:47.260]  может возникнуть проблема с тем, что мощность k, она больше или даже много больше, чем m.
[01:20:47.260 --> 01:20:53.660]  Ну скажем, m у меня 100, а k это множество всех строк. Вот открою тайну, но строк всего в мире больше
[01:20:53.660 --> 01:21:01.380]  чем 100. Вот. Беда. То есть невозможно построить инъективное отображение из большего множества в
[01:21:01.380 --> 01:21:10.860]  меньшее множество. Да? Вот. То есть возникает проблема или назовем беда. Беда заключается в следующем.
[01:21:10.860 --> 01:21:27.260]  Существует x и y принадлежащие k, но при этом x не равны k. Такие что h от x равно h от y. То есть
[01:21:27.260 --> 01:21:32.980]  проблема звучит следующим. А что если у меня нашлось два ключа, ну там два комплексных числа или две
[01:21:32.980 --> 01:21:40.500]  строки. Такие что они попали в одну и ту же корзину. Да, я не сказал, но давайте вот эти вот.
[01:21:40.500 --> 01:21:56.060]  Вот эти ячейки будем называть корзинами. Корзины или, от английского слова bucket. Вот. Проблема ясна?
[01:21:56.060 --> 01:22:01.340]  Есть у меня два элемента. Попали в одну корзину, то что происходит? Приходит x и в соответствующие
[01:22:01.340 --> 01:22:09.540]  ячейки ставят плюс. Приходит y. y не равен x, но их h значения совпадают. Я смотрю сюда и вижу плюс
[01:22:09.540 --> 01:22:14.460]  и говорю, что y у меня в множестве. А это не так. Вот такая ситуация называется коллизией.
[01:22:14.460 --> 01:22:27.780]  Коллизия. Вот. И собственно коллизия является, ну короче, коллизия является единственной
[01:22:27.780 --> 01:22:32.900]  проблемой на пути к решению вот нашей задачи за от единицы. Понятно? То есть если мы разберемся,
[01:22:32.900 --> 01:22:37.620]  что делать вот с такими ситуациями, то в принципе в рамках вот этого пайплайна у нас получится
[01:22:37.620 --> 01:22:41.900]  построить структуру данных, которая позволяет за единицу вставлять элементы, удалять элементы и
[01:22:41.900 --> 01:22:51.140]  делать поиск. Окей? Ну хорошо, мы осознали проблему. Давайте попробуем что-нибудь придумать,
[01:22:51.140 --> 01:23:00.540]  чтобы ее решить. Наиболее популярным способом решения проблемы с коллизиями является метод
[01:23:00.540 --> 01:23:10.340]  цепочек. Ну вот собственно его мы будем рассматривать. Следующий пункт. Метод цепочек.
[01:23:10.340 --> 01:23:22.340]  Ну прежде чем я расскажу его, может кто-нибудь предложит решение. Может быть основываюсь на
[01:23:22.340 --> 01:23:33.940]  названии или на своих догадках. Ну вот смотрите, повторюсь, в чем проблема. Возможно такая
[01:23:33.940 --> 01:23:39.860]  ситуация, при которой x и y отображаются в одну и ту же ячейку. Мне хочется с этой проблемой как-то
[01:23:39.860 --> 01:23:45.620]  бороться. Ну вот придумать какую-нибудь идею, чтобы я мог однозначно определять, что у меня
[01:23:45.620 --> 01:23:50.980]  является x, что у меня является y. Ну возможно неэффективно. Давайте пока так. Что вы предложили,
[01:23:50.980 --> 01:24:08.780]  чтобы разрешить эту проблему? Что? Многомерный массив и что в нем хранить? Да, да, отлично. Ну
[01:24:08.780 --> 01:24:12.500]  собственно идея метод цепочек заключается в этом. Только мы храним не двумерный массив, а мы храним
[01:24:12.500 --> 01:24:26.300]  все-таки массив списков. Что мы делаем? Если h от x равен h от y, ну понятное дело при x не равном y,
[01:24:26.300 --> 01:24:47.260]  то в h от x, точнее в ячейке b от h от x храним список из x и y. Ну то есть в каждой ячейке мы
[01:24:47.260 --> 01:24:52.860]  теперь храним не просто булевское значение true или false, есть там элемент или нет, а мы храним
[01:24:52.860 --> 01:25:00.420]  список всех элементов, которые там содержатся. Понятен план? Если у меня вдвоем попали какие-то
[01:25:00.420 --> 01:25:12.420]  элементы, то я тут храню x и потом храню, давайте отдельно. Здесь у меня находится список,
[01:25:12.420 --> 01:25:19.060]  здесь хранится x, здесь хранится y, здесь может быть хранится какой-то z, ну и здесь может быть
[01:25:19.060 --> 01:25:38.820]  тоже какой-то длинный список a, b, c. Ну некоторые списки пустые просто. Как теперь выглядит алгоритм
[01:25:38.820 --> 01:25:46.380]  ставки? Что я делаю при поиске элементов? Сначала разберемся. Вот мне нужно найти элемент x, как я
[01:25:46.380 --> 01:25:54.620]  действую. Я определяю номер ячейки, то есть я вычисляю хэш функцию и смотрю в какой ячейке
[01:25:54.620 --> 01:26:02.300]  у меня лежит этот элемент, если лежит, конечно. И что я делаю дальше? Да, и дальше я просто-напросто
[01:26:02.300 --> 01:26:09.460]  прохожусь по списку и смотрю, есть ли там x или нет. Если там x нет, то возвращаю false, если там x есть,
[01:26:09.460 --> 01:26:14.260]  то true. Как выглядит ставка? Вставка выглядит точно так же, как поиск. Но только если я ничего не
[01:26:14.260 --> 01:26:19.340]  нашел, то я дополнительно вот в этот список вставляю элемент. Как выглядит удаление? Опять же,
[01:26:19.340 --> 01:26:23.300]  удаление выглядит так же, как и поиск. Осуществляя поиск, если я нашел элемент, то я его из списка
[01:26:23.300 --> 01:26:33.740]  удаляю, если элемент не нашел, ну и ладно, удалять нечего. Все просто, да, пока? В чем проблема?
[01:26:33.740 --> 01:26:50.380]  А зачем его ускорять? Он разве медленный? Он работает за линию. Ну, вообще говоря, да.
[01:26:50.380 --> 01:27:06.820]  Давайте так. Будем анализировать время работы find от x. Ну и давайте замещение напишем, что
[01:27:06.820 --> 01:27:20.700]  insert от x и erase от x. Время работы insert и время работы erase по большому счету совпадает с временем
[01:27:20.700 --> 01:27:28.540]  работы find. Ну понятно, почему это так, да? Потому что чтобы вставить элемент, мне по сути надо
[01:27:28.540 --> 01:27:33.580]  понять, был ли там ли этот элемент до этого, если его не было, то вставить. То есть по сути мне нужно
[01:27:33.700 --> 01:27:38.180]  выполнить поиск. Для удаления элемента мне тоже, мне нужно его как минимум найти. Если я этот элемент
[01:27:38.180 --> 01:27:44.780]  нашел, то я его за единицу из списка удаляю. Поэтому достаточно лишь оценить время работы find. И вот с
[01:27:44.780 --> 01:27:58.380]  временем работы find от x все не очень хорошо. Время работы find от x занимает единицы плюс l от x,
[01:27:58.380 --> 01:28:26.500]  где l от x это длина цепочки, с которой лежит x. Согласны? А чему у меня максимально может
[01:28:26.500 --> 01:28:32.900]  быть равна длина цепочки? N. Да, то есть у меня может возникнуть довольно печальная ситуация.
[01:28:32.900 --> 01:28:40.500]  Меня может так не повести, что я беру x, беру y, a, b, c, z. То есть вставляю их все в хэштаблицу и они
[01:28:40.500 --> 01:28:49.180]  зараз такие попали все в одну ячейку. И просто выстроились в единый список. Ну то есть есть хэштаблица,
[01:28:49.180 --> 01:28:56.460]  есть ячейка и в ней вот все элементы хранятся. И тогда сложность поиска в хэштаблице,
[01:28:56.460 --> 01:29:00.540]  у меня ничем не лучше, чем поиск просто в обычном линейном списке, ну или просто в неотсортированном
[01:29:00.540 --> 01:29:09.500]  оси. Согласны? То есть это не больше, чем N. То есть в худшем случае у меня действительно время
[01:29:09.500 --> 01:29:16.340]  работы операции поиска N. Печально. Ну и действительно такую ситуацию очень легко устроить, но если,
[01:29:16.340 --> 01:29:23.540]  например, взять какую-нибудь плохую хэш-функцию h от x тождественно равную нулю. Как вам такая хэш-
[01:29:23.540 --> 01:29:30.260]  функция? h от x тождественно равную нулю, тождественно равной единице. h от x равная, не знаю, остатку
[01:29:30.260 --> 01:29:37.860]  отделения на два, которая просто осуществляет распределение по нолику единичке. Поэтому в общем-то
[01:29:37.860 --> 01:29:44.940]  основная проблема заключается в подбору хороших хэш-функций. Согласны? Теперь давайте определимся
[01:29:44.940 --> 01:29:55.540]  тем, что такое хорошая хэш-функция. Ну вот как вы ее считаете? Что такое хорошая хэш-функция?
[01:29:55.540 --> 01:30:06.900]  Из каких соображений ее лучше всего выбирать? Ну биекции у нас не получится.
[01:30:15.220 --> 01:30:23.580]  Вот, да, я услышал слово ровномерно распределяет. То есть нам бы хотелось, чтобы наша хэш-функция,
[01:30:23.580 --> 01:30:28.200]  ну, она не особо стремилась распределять объекты в одну корзину. То есть нам бы хотела чтобы
[01:30:28.200 --> 01:30:32.280]  хэш функция, она как-то все элементы равномерно размазывала по всем корзинам. Да, если у нас
[01:30:32.280 --> 01:30:36.140]  все элементы будут равномерно размазаны по всем корзинам, то каждая отдельная корзиночка,
[01:30:36.140 --> 01:30:41.740]  она будет достаточно маленькая, да. И тогда в ней поиск будет осуществляться быстро. То есть хочется,
[01:30:41.740 --> 01:30:57.460]  не понятно пока, что это значит чисто формально, да, в математичке, но хочется равномерно распределяющей
[01:30:57.460 --> 01:31:15.580]  х-функции. Окей. Предложение. Можешь пока не записывать, просто предложение. Ну, смотрите,
[01:31:15.580 --> 01:31:23.540]  вот, допустим, я знаю, что x это int, что я храню целые числа. Ну, давайте для простоты я возьму
[01:31:23.540 --> 01:31:27.700]  беззнаковые целые числа. В общем, все числа принимают значение от нуля до там четырех миллиардов.
[01:31:27.700 --> 01:31:39.380]  И я беру в качестве h от x вот такую х-функцию x процент m. То есть просто беру x, беру остаток
[01:31:39.380 --> 01:31:45.580]  отделения на m и использую эту х-функцию. Вопрос. Удовлетворяет ли эта х-функция свойство
[01:31:45.580 --> 01:32:07.900]  хорошести? Навек понятен. Казалось бы, эта х-функция хороша. Ну, то есть если я предполагаю,
[01:32:07.900 --> 01:32:13.180]  что у меня x приходится случайно, например, то в принципе эта х-функция нормально размазывает
[01:32:13.180 --> 01:32:17.860]  все числа равномерно по всем корзинам. То есть если x случайно, то и остаток отделения на m тоже
[01:32:17.860 --> 01:32:25.060]  в принципе случайно некоторым образом. Но проблема в том, что пользователь непредсказуем. И более
[01:32:25.060 --> 01:32:29.340]  того, никто вам не гарантировал, что числа будут расплены равномерно. А в нашем контесте уж точно,
[01:32:29.340 --> 01:32:35.100]  никто вам это не гарантировать не будет. Поэтому, в принципе, существует для любой х-функции,
[01:32:35.100 --> 01:32:39.900]  для любой х-функции существует некий плохой вход. Ну, в принципе, ну, там тест может быть
[01:32:39.900 --> 01:32:43.140]  устроен таким образом, что вот он специально для этой х-функции будет подбирать плохие
[01:32:43.140 --> 01:32:50.300]  значения, так чтобы они будут распределяться в одну корзину. Снова беда, пока не видно просвета.
[01:32:50.300 --> 01:32:59.260]  Давайте откатимся немного назад в первый семестр. Вот там вы обсуждали быструю сортировку. Вот я
[01:32:59.260 --> 01:33:06.580]  утверждаю, что у быстрой сортировки была такая же проблема. Какая там была проблема? Пивот, да,
[01:33:06.640 --> 01:33:11.660]  смотрите, у нас был массив. Быстра сортировка устроена таким образом, что мы на каждом
[01:33:11.660 --> 01:33:14.900]  этапе выбираем, ну, некоторый опорный элемент, который сбивает элементы там на правую часть и на
[01:33:14.900 --> 01:33:20.840]  левую часть. И, в принципе, быстра сортировка не такая уж и быстрая на самом деле. Если мы
[01:33:20.840 --> 01:33:26.180]  в качестве пивота всегда выбираем, допустим, первый элемент, то мы можем подстроить такой тест,
[01:33:26.180 --> 01:33:30.620]  который бы все элементы назначал, ну, подавал на вход элементы от сортированной в обратном
[01:33:30.620 --> 01:33:34.060]  порядке, да и тогда в качестве пивота у нас всегда бы выбирался, допустим, максимальный элемент.
[01:33:34.060 --> 01:33:39.060]  Максимальный элемент всегда бы у нас делил массив в пропорции 1 ко всем остальным.
[01:33:39.060 --> 01:33:42.060]  Но это соответственно квадратичная сложность.
[01:33:42.060 --> 01:33:47.060]  Поэтому, вообще говоря, быстросортировка с детерминированным выборотом пивота
[01:33:47.060 --> 01:33:50.060]  работает за квадратичное время, то есть долго.
[01:33:50.060 --> 01:33:52.060]  Здесь то же самое, смотрите.
[01:33:52.060 --> 01:33:55.060]  Если я фиксирую какую-то hash-функцию, то, в принципе,
[01:33:55.060 --> 01:33:58.060]  очень просто подобрать вход, на который у меня все будет плохо.
[01:33:58.060 --> 01:34:00.060]  Ну вот, например, такой.
[01:34:00.060 --> 01:34:04.060]  Как мы решали эту проблему в случае быстросортировки?
[01:34:05.060 --> 01:34:08.060]  Например, выбирая пивот случайно.
[01:34:09.060 --> 01:34:12.060]  А что нам дает случайный выбор пивота?
[01:34:14.060 --> 01:34:17.060]  Во-первых, случайный выбор пивота дает нам тот факт,
[01:34:17.060 --> 01:34:20.060]  что подобрать тест довольно сложно.
[01:34:20.060 --> 01:34:23.060]  Сложно подстроиться под случайность, если она действительно случайна.
[01:34:23.060 --> 01:34:25.060]  Это первый момент.
[01:34:25.060 --> 01:34:27.060]  А второе, что нам дает случайность?
[01:34:27.060 --> 01:34:33.060]  Случайность нам дает возможность вероятностно оценить наш алгоритм.
[01:34:35.060 --> 01:34:39.060]  Ну, помните, что быстросортировка работает за n log n в среднем.
[01:34:40.060 --> 01:34:43.060]  Если мы пивот выбираем не случайно, можем ли ему утверждать,
[01:34:43.060 --> 01:34:46.060]  что быстросортировка работает от n log n в среднем?
[01:34:46.060 --> 01:34:49.060]  Нет, потому что у меня случайности в алгоритме нет.
[01:34:49.060 --> 01:34:53.060]  Если я привношу в алгоритм случайность, то я уже могу анализировать средний случай.
[01:34:53.060 --> 01:34:55.060]  Вот здесь то же самое.
[01:34:55.060 --> 01:34:57.060]  Давайте ашу выбирать случайно.
[01:34:58.060 --> 01:35:00.060]  Ну так, чтобы пользователь об этом не знал,
[01:35:00.060 --> 01:35:03.060]  ну и так, чтобы у нас от запуска к запуску хэш-функция там как-то менялась.
[01:35:03.060 --> 01:35:06.060]  Ну, чтобы пользователь не мог подстроиться под плохие данные.
[01:35:06.060 --> 01:35:09.060]  Ну и, в принципе, чтобы плохой случай редко реализовывался.
[01:35:10.060 --> 01:35:14.060]  А можно от m случайно выбирать?
[01:35:15.060 --> 01:35:17.060]  С m...
[01:35:17.060 --> 01:35:20.060]  У нас же m будет конечным числом.
[01:35:21.060 --> 01:35:24.060]  Ну, m – это размер хэш-таблицы, вообще говоря.
[01:35:24.060 --> 01:35:26.060]  Мы поговорим...
[01:35:26.060 --> 01:35:29.060]  Давайте m пока не трогай, давайте считать, что m фиксировано.
[01:35:29.060 --> 01:35:32.060]  Про то, как выбирать m, мы поговорим, это отдельная история.
[01:35:32.060 --> 01:35:34.060]  Понятное дело, что m должно меняться.
[01:35:34.060 --> 01:35:37.060]  Почему? Потому что если у вас m равен 1000,
[01:35:37.060 --> 01:35:39.060]  а вы вставили миллион элементов,
[01:35:39.060 --> 01:35:41.060]  то тут уж какую хэш-функцию не выбирай,
[01:35:41.060 --> 01:35:45.060]  у вас, короче, маленькой цепочке не получится.
[01:35:46.060 --> 01:35:48.060]  Поэтому m должен быть разумным.
[01:35:48.060 --> 01:35:50.060]  Значит, как его настраивать?
[01:35:50.060 --> 01:35:52.060]  Мы поговорим отдельно, отдельным пунктом.
[01:35:52.060 --> 01:35:54.060]  Пока считаем, что m у нас фиксировано,
[01:35:54.060 --> 01:35:56.060]  пока с такой проблемой разберемся.
[01:35:58.060 --> 01:36:00.060]  Делаем хэш-функцию случайной.
[01:36:00.060 --> 01:36:03.060]  Следующий пункт – простое равномерное хэширование.
[01:36:07.060 --> 01:36:09.060]  Хэширование.
[01:36:13.060 --> 01:36:15.060]  Хэширование.
[01:36:16.060 --> 01:36:18.060]  Ну или буду сокращать до.
[01:36:24.060 --> 01:36:27.060]  Что нам говорит простое равномерное хэширование?
[01:36:27.060 --> 01:36:31.060]  Простое равномерное хэширование – это такой способ выбрать хэш-функцию...
[01:36:31.060 --> 01:36:34.060]  Ну, короче, это просто случайный способ выбрать хэш-функцию.
[01:36:34.060 --> 01:36:36.060]  Каким образом?
[01:36:36.060 --> 01:36:38.060]  Ну, есть два определения, на самом деле, эквивалентных.
[01:36:38.060 --> 01:36:40.060]  Давайте выпишем.
[01:36:40.060 --> 01:36:48.060]  Первое – это h выбирается случайно
[01:36:50.060 --> 01:36:54.060]  из множества всех возможных хэш-функций.
[01:36:58.060 --> 01:37:00.060]  Такая запись понятна, да?
[01:37:02.060 --> 01:37:04.060]  Ну, там MatLog и так далее.
[01:37:06.060 --> 01:37:11.060]  B в степени a – это просто множество всех функций из a в b.
[01:37:13.060 --> 01:37:17.060]  То есть здесь я убираю хэш-функцию из множества всех функций
[01:37:17.060 --> 01:37:20.060]  из ключей в множество от 0 до f-1.
[01:37:23.060 --> 01:37:26.060]  То есть я беру абсолютно произвольную функцию
[01:37:26.060 --> 01:37:28.060]  и говорю, что вот эта хэш-функция и так далее.
[01:37:30.060 --> 01:37:32.060]  Ну, если непонятно...
[01:37:32.060 --> 01:37:35.060]  То есть тут, вообще говоря, это определение строгое,
[01:37:35.060 --> 01:37:38.060]  но непонятно, как им пользоваться на практике.
[01:37:38.060 --> 01:37:40.060]  То есть непонятно, как сформировать множество всевозможных функций,
[01:37:40.060 --> 01:37:42.060]  как из него выбрать и так далее.
[01:37:42.060 --> 01:37:44.060]  Поэтому существует эквалентное определение, которое,
[01:37:44.060 --> 01:37:47.060]  так скажем, делает более понятным процесс выбора хэш-функций.
[01:37:47.060 --> 01:37:49.060]  Давайте делать так.
[01:37:49.060 --> 01:37:55.060]  Значит, если x – новый ключ,
[01:37:59.060 --> 01:38:03.060]  то говорим, что h от x – это просто некоторое случайное число.
[01:38:05.060 --> 01:38:07.060]  Ну, от 0 до m.
[01:38:09.060 --> 01:38:11.060]  Вот нам приходит ключ.
[01:38:11.060 --> 01:38:13.060]  Мы понимаем, что этот ключ мы никогда ранее не встречали.
[01:38:13.060 --> 01:38:17.060]  То есть он никогда не был аргументом в файне, в инсерте, в эрейзе.
[01:38:17.060 --> 01:38:19.060]  Ну и тогда мы просто геерим для него свое хэш-значение.
[01:38:19.060 --> 01:38:21.060]  Вот приходит новый элемент, говорю, вот тебе хэш-значение.
[01:38:21.060 --> 01:38:23.060]  Все, он берет номерок и уходит дальше.
[01:38:25.060 --> 01:38:29.060]  А иначе, если x уже ранее используется,
[01:38:29.060 --> 01:38:31.060]  то есть нам приходит новый ключ x,
[01:38:31.060 --> 01:38:34.060]  и мы понимаем, что он уже ранее встречался, то что мы делаем?
[01:38:34.060 --> 01:38:36.060]  Ну, просто используем старое значение.
[01:38:36.060 --> 01:38:48.060]  Иначе используем старый h от x.
[01:38:56.060 --> 01:39:02.060]  И уже вот эта функция получена с помощью простого равномерного хэширования.
[01:39:02.060 --> 01:39:06.060]  Ну вот я утверждаю, что она в вполне себе в полном смысле
[01:39:06.060 --> 01:39:09.060]  удовлетворяет вот этому требованию хорошести.
[01:39:09.060 --> 01:39:11.060]  Понятно, почему так?
[01:39:11.060 --> 01:39:13.060]  Потому что какой бы x у меня ни пришел,
[01:39:13.060 --> 01:39:16.060]  я для него генерирую случайное хэш-значение.
[01:39:16.060 --> 01:39:24.060]  А случайное хэш-значение, оно у меня равновероятно распределит в любую из ячеек.
[01:39:26.060 --> 01:39:29.060]  Ну поэтому на таком бытовом смысле, наверное, понятно,
[01:39:29.060 --> 01:39:33.060]  что если у меня хэш-функция, она случайно каждый элемент распределяет там по своей ячейке,
[01:39:33.060 --> 01:39:37.060]  то в целом в среднем каждая ячейка у меня будет иметь небольшую глубину.
[01:39:37.060 --> 01:39:41.060]  Пока все понятно интуитивно?
[01:39:44.060 --> 01:39:47.060]  Ну давайте как-то вот это свойство попробуем формализовать и доказать.
[01:39:47.060 --> 01:39:51.060]  Давайте попробуем доказать, что вот эта хэш-функция действительно в некотором смысле хорошая.
[01:39:54.060 --> 01:39:57.060]  Прежде чем ее анализировать, мне нужно вести несколько понятий,
[01:39:57.060 --> 01:39:59.060]  ну или просто напомнить несколько понятий из теории вероятности.
[01:39:59.060 --> 01:40:03.060]  В общем, ничего сложного, а просто напомню какие-то школьные факты.
[01:40:04.060 --> 01:40:06.060]  Никто не против?
[01:40:09.060 --> 01:40:11.060]  Давайте так.
[01:40:12.060 --> 01:40:14.060]  Ликбез по терверу.
[01:40:19.060 --> 01:40:21.060]  Пункт номер ноль.
[01:40:22.060 --> 01:40:28.060]  Пусть ω большое это пространство,
[01:40:30.060 --> 01:40:33.060]  ну не пространство, а множество, давайте скажем множество.
[01:40:33.060 --> 01:40:48.060]  Пространство слишком множество равновероятных исходов эксперимента.
[01:40:51.060 --> 01:40:53.060]  Ну то есть у меня есть некоторый эксперимент,
[01:40:53.060 --> 01:40:56.060]  и я знаю, что он может как-то завершиться каким-то результатом.
[01:40:56.060 --> 01:40:58.060]  И вот допустим все эти результаты равновероятны.
[01:40:58.060 --> 01:41:00.060]  Ну например, подбрасывание кубика.
[01:41:00.060 --> 01:41:03.060]  В случае подбрасывания кубика, чему у меня равна ω?
[01:41:04.060 --> 01:41:06.060]  Ноль, один и так далее.
[01:41:07.060 --> 01:41:09.060]  Один, два и так далее, шесть.
[01:41:11.060 --> 01:41:13.060]  То есть результат подбрасывания кубика, ну в принципе можно считать,
[01:41:13.060 --> 01:41:17.060]  что это выпадение в какой-то грани, причем равновероятно.
[01:41:17.060 --> 01:41:20.060]  Подбрасывание монетки тоже равновероятное выпадение орла или решки.
[01:41:21.060 --> 01:41:22.060]  Окей?
[01:41:23.060 --> 01:41:29.060]  Значит первое, да, нет, еще в рамках нулевого пункта остаемся.
[01:41:29.060 --> 01:41:42.060]  Ну и событие называется множество а под множество ω.
[01:41:43.060 --> 01:41:48.060]  Ну например, событие, что на кубике у меня выпало четное число.
[01:41:51.060 --> 01:41:53.060]  Как описывается это событие?
[01:41:53.060 --> 01:41:56.060]  Ну это событие просто описывается значениями два, четыре, шесть.
[01:41:58.060 --> 01:42:02.060]  То есть каждое событие я описываю исходами, которые благоприятствуют этому исходу.
[01:42:02.060 --> 01:42:05.060]  Ну, все понятно, да? Надеюсь.
[01:42:07.060 --> 01:42:09.060]  Так, первый осмысленный пункт.
[01:42:15.060 --> 01:42:17.060]  Никого не удивляет.
[01:42:17.060 --> 01:42:22.060]  Вероятность события, это просто мощность а деленная на мощность вообще всех возможных исходов.
[01:42:22.060 --> 01:42:27.060]  То есть количество благоприятных исходов деленное на общее количество исходов.
[01:42:31.060 --> 01:42:33.060]  Второй.
[01:42:35.060 --> 01:42:37.060]  Во втором пункте поговорим про среднее.
[01:42:37.060 --> 01:42:55.060]  Значит, пусть х результат некоторого измерения.
[01:42:55.060 --> 01:42:58.060]  Ну например, я бросил кубик и посмотрел какое там число.
[01:42:58.060 --> 01:43:00.060]  Обозначил это число за х.
[01:43:00.060 --> 01:43:20.060]  Среднее значение х я буду называть сумму по всевозможным х, х умноженной на вероятность того, что х равен х малым.
[01:43:20.060 --> 01:43:23.060]  Ну, грубо говоря, я определил среднее значение.
[01:43:23.060 --> 01:43:25.060]  Среднее значение некоторой величины.
[01:43:25.060 --> 01:43:27.060]  Но чему равно среднее значение величины?
[01:43:27.060 --> 01:43:33.060]  Я просто должен взять значение, которое оно принимает, с какой частотой оно его принимает, ну и дальше все это просуммировать.
[01:43:33.060 --> 01:43:35.060]  Согласны?
[01:43:35.060 --> 01:43:38.060]  Ну скажем, если у меня монетка, на монетке написано 0 или 1,
[01:43:38.060 --> 01:43:43.060]  и она там единицу принимает в 75% случаев, а 0 в 25% случаев,
[01:43:43.060 --> 01:43:47.060]  то тогда я говорю, что в среднем у меня монетка выдает 0,75.
[01:43:47.060 --> 01:43:52.060]  Потому что в 75 случаях из 100 она выдает единицу, а в остальных 0.
[01:43:52.060 --> 01:43:55.060]  75 деленное на 100, 0,75.
[01:43:55.060 --> 01:43:57.060]  Тоже норм, да?
[01:44:00.060 --> 01:44:02.060]  Так, давайте здесь.
[01:44:04.060 --> 01:44:06.060]  Что еще мне понадобится?
[01:44:09.060 --> 01:44:14.060]  Ну, такое свойство, я думаю, тоже никого не шокирует.
[01:44:21.060 --> 01:44:23.060]  Средняя сумма то же самое, что сумма средних.
[01:44:26.060 --> 01:44:33.060]  То есть неважно, я усредняю сумму, или сначала усредняю каждые слагами, а потом беру сумму.
[01:44:33.060 --> 01:44:36.060]  То есть это называется линейность среднего, линейность в от ожидания.
[01:44:36.060 --> 01:44:38.060]  Ну, почему это так?
[01:44:38.060 --> 01:44:44.060]  Ну, тоже обосновать несложно.
[01:44:50.060 --> 01:44:54.060]  Ну, вот это я посчитал средней суммой, согласны?
[01:44:56.060 --> 01:45:07.060]  Я утверждаю, что это то же самое, что вот эта штука.
[01:45:07.060 --> 01:45:09.060]  А это просто сумма средних.
[01:45:16.060 --> 01:45:18.060]  Ну и все, наверное.
[01:45:19.060 --> 01:45:21.060]  Я думаю, этого нам хватит.
[01:45:22.060 --> 01:45:27.060]  Так, вопросов не появилось? Все нормально?
[01:45:28.060 --> 01:45:36.060]  Ну окей, теперь давайте, собственно, покажем, что использование простого равномерного хэширования
[01:45:38.060 --> 01:45:44.060]  позволяет нам добиться хорошей хэштаблицы.
[01:45:47.060 --> 01:45:49.060]  Теорема.
[01:45:52.060 --> 01:45:58.060]  А, сейчас, подождите, вернемся сюда.
[01:45:58.060 --> 01:46:01.060]  Четвертый пункт понадобится тоже.
[01:46:01.060 --> 01:46:28.060]  Пусть х и у независимы и принимают значения равновероятно.
[01:46:29.060 --> 01:46:32.060]  То есть у нас все вероятности будут одинаковые.
[01:46:35.060 --> 01:46:42.060]  И равновероятно принимают значения во множестве от 0 до m-1.
[01:46:50.060 --> 01:46:56.060]  Тогда кто скажет, чему равна вероятность того, что эти величины совпадут?
[01:46:56.060 --> 01:47:00.060]  То есть я провожу два независимых эксперимента, абсолютно друг от друга никак не зависит.
[01:47:00.060 --> 01:47:03.060]  Ну, условно, подбрасываю один раз кубик, подбрасываю второй кубик.
[01:47:03.060 --> 01:47:06.060]  С какой вероятностью у них значения совпадут?
[01:47:08.060 --> 01:47:10.060]  Ну, 1m тогда.
[01:47:12.060 --> 01:47:17.060]  Ну, тут, наверное, надо пояснить. Давайте коротко доказательства распишем.
[01:47:17.060 --> 01:47:22.060]  Ну, просто по пункту 1, по простому определению вероятности.
[01:47:22.060 --> 01:47:32.060]  Значит, х равно у. Что мне составляет множество всевозможных исходов?
[01:47:32.060 --> 01:47:38.060]  Ну, согласны, что множество всевозможных исходов мне составляют всевозможные пары х и у ?
[01:47:39.060 --> 01:47:43.060]  Да? Согласны? То есть я независимо, получаю значения з mich, независимо, поч seals значения у
[01:47:43.060 --> 01:47:48.060]  Поэтому у меня тут получается все возможные значения и j, ну, понятно дело,
[01:47:48.060 --> 01:47:52.060]  Жир лежит в промежутке от 0 до m-1, жир лежит в промежутке от 0 до m-1.
[01:47:52.060 --> 01:47:55.060]  А что мне составляет множество благоприятных сходов?
[01:47:55.060 --> 01:47:59.060]  Какие сходы из этих меня благоприятствуют?
[01:47:59.060 --> 01:48:06.060]  Ну пары i, i.
[01:48:06.060 --> 01:48:08.060]  Чему равно количество пар i, i?
[01:48:08.060 --> 01:48:09.060]  m.
[01:48:09.060 --> 01:48:12.060]  То есть это 0, 0, 1, 1, 2, 2 и так далее.
[01:48:12.060 --> 01:48:15.060]  Чему равно количество пар i, j?
[01:48:15.060 --> 01:48:16.060]  1 на m квадрат.
[01:48:16.060 --> 01:48:18.060]  То есть я могу выбрать первое число m способами,
[01:48:18.060 --> 01:48:22.060]  второе число m способами, m квадрат.
[01:48:22.060 --> 01:48:25.060]  1 на m.
[01:48:25.060 --> 01:48:28.060]  Все.
[01:48:28.060 --> 01:48:30.060]  Окей.
[01:48:30.060 --> 01:48:32.060]  Так, ну теперь возвращаемся к формулировке.
[01:48:32.060 --> 01:48:44.060]  Значит при простом, просто при PRH.
[01:48:44.060 --> 01:48:59.060]  А средняя длина цепочки...
[01:48:59.060 --> 01:49:05.060]  Нет, давайте сразу Profite напишем.
[01:49:05.060 --> 01:49:21.060]  Среднее время поиска find at x
[01:49:21.060 --> 01:49:26.060]  составляет O от 1 плюс а.
[01:49:26.060 --> 01:49:30.060]  Уже α это n на m.
[01:49:30.060 --> 01:49:37.060]  n это количество элементов в хэш-таблице.
[01:49:37.060 --> 01:49:44.060]  А m это, собственно как и раньше, размер хэш-таблицы.
[01:49:44.060 --> 01:49:47.060]  Ну давайте с формулировкой разберемся, а потом на PRH пойдем.
[01:49:47.060 --> 01:49:48.060]  Что мне говорит теорема?
[01:49:48.060 --> 01:49:52.060]  Теорема говорит следующее, что если я использую просто равномерное хэширование,
[01:49:52.060 --> 01:49:55.060]  то есть вот такой хэш, то есть случайно распределяю элементы,
[01:49:55.060 --> 01:49:57.060]  случайно распределяю элементы по корзинам,
[01:49:57.060 --> 01:49:59.060]  то в среднем у меня каждая корзина по сути,
[01:49:59.060 --> 01:50:01.060]  то есть find зависит от длины корзины,
[01:50:01.060 --> 01:50:05.060]  то по сути я говорю, что средняя длина корзины у меня равна n делить на m.
[01:50:05.060 --> 01:50:09.060]  А n делить на m это как раз это самое равномерное размазывание всех элементов
[01:50:09.060 --> 01:50:12.060]  по всем корзинам.
[01:50:12.060 --> 01:50:14.060]  Понятно?
[01:50:14.060 --> 01:50:17.060]  Ну хорошо, после PRH докажем.
[01:50:17.060 --> 01:50:23.060]  Так, дополню, что вот это значение α еще называют load factor,
[01:50:23.060 --> 01:50:26.060]  по всей степени загруженность хэштаблицы.
[01:50:26.060 --> 01:50:31.060]  Это просто некоторая характеристика, насколько велики мои корзины,
[01:50:31.060 --> 01:50:34.060]  насколько велики списки.
[01:50:34.060 --> 01:50:40.060]  Ладно, перейдем к доказательству.
[01:50:40.060 --> 01:50:42.060]  Как мы будем доказывать?
[01:50:42.060 --> 01:50:48.060]  Понятное дело, что время поиска, оно тесно связано с длиной цепочки,
[01:50:48.060 --> 01:50:53.060]  в которую попадает элемент х, в который потенциально может находиться х.
[01:50:53.060 --> 01:50:59.060]  Давайте для начала определимся с тем, чему равна длина цепочки х.
[01:50:59.060 --> 01:51:05.060]  Я утверждаю, что длина цепочки х может быть выражена следующим образом.
[01:51:05.060 --> 01:51:10.060]  Давайте для начала здесь пропишем.
[01:51:10.060 --> 01:51:21.060]  Пусть х, х1 и так далее, хн, элементы хэштаблицы.
[01:51:21.060 --> 01:51:28.060]  То есть у меня есть хэштаблицы, в ней уже лежат элементы х1 и так далее, хн.
[01:51:28.060 --> 01:51:33.060]  Заметьте, что х это не то же самое, что k.
[01:51:33.060 --> 01:51:37.060]  За k мы обозначали множество всевозможных ключей.
[01:51:37.060 --> 01:51:40.060]  То есть в принципе те значения, которые мы теоретически можем засунуть в хэштаблицу.
[01:51:40.060 --> 01:51:45.060]  А х это те, которые фактически там уже лежат.
[01:51:45.060 --> 01:51:48.060]  История такая вот.
[01:51:48.060 --> 01:51:53.060]  Пусть х это элементы в хэштаблице, тогда как мы можем выразить длину цепочки,
[01:51:53.060 --> 01:51:55.060]  в которую попадает элемент х?
[01:51:55.060 --> 01:52:00.060]  На самом деле очень просто.
[01:52:00.060 --> 01:52:04.060]  Длина цепочки выражается следующим образом.
[01:52:04.060 --> 01:52:13.060]  Это сумма индикаторов того, что h от х...
[01:52:13.060 --> 01:52:18.060]  Ну индикатор это просто функция, которая принимает значение 1 или 0.
[01:52:18.060 --> 01:52:21.060]  В зависимости от того, верны ли там условия или нет.
[01:52:21.060 --> 01:52:23.060]  Не знаю пропишем.
[01:52:23.060 --> 01:52:32.060]  И от true равно единице, и от false равно 0.
[01:52:32.060 --> 01:52:38.060]  Понятно ли, что длина цепочки действительно может быть выражена вот таким образом?
[01:52:38.060 --> 01:52:40.060]  Что такое длина цепочки?
[01:52:40.060 --> 01:52:43.060]  Что составляет вообще говоря цепочку?
[01:52:43.060 --> 01:52:47.060]  Цепочка это те элементы, у которых одинаковый х.
[01:52:47.060 --> 01:52:53.060]  Я беру h от х, то есть беру номер цепочки, в котором лежит х,
[01:52:53.060 --> 01:53:00.060]  и проверяю, сколько элементов из х большого совпадает с моим хэшом.
[01:53:00.060 --> 01:53:04.060]  И вот ровно эти элементы образуют нужную мне цепочку.
[01:53:04.060 --> 01:53:08.060]  И количество совпадений это как раз есть длина цепочки, которая мне нужна.
[01:53:08.060 --> 01:53:11.060]  Понятно?
[01:53:11.060 --> 01:53:14.060]  Ладно.
[01:53:14.060 --> 01:53:20.060]  Осталось всего лишь, так как мы хотим среднее время поиска find от х,
[01:53:20.060 --> 01:53:24.060]  мы понимаем, что find от х напрямую зависит от l от x.
[01:53:24.060 --> 01:53:32.060]  Мы писали, что время файнда есть θ от 1 плюс как раз таки l от x.
[01:53:32.060 --> 01:53:38.060]  Чтобы посчитать среднее время работы файнда, мне достаточно посчитать среднюю длину цепочки.
[01:53:38.060 --> 01:53:40.060]  Давайте это сделаем.
[01:53:40.060 --> 01:53:43.060]  Средняя длина цепочки l от x.
[01:53:43.060 --> 01:53:48.060]  Это есть средняя, вот такой вот сумма, i от равна 1 до n.
[01:53:48.060 --> 01:53:51.060]  Ну и вот это переписываю.
[01:53:51.060 --> 01:53:56.060]  h от x равно h от xi.
[01:53:56.060 --> 01:53:59.060]  Так, как я могу переписать вот эту штуку?
[01:53:59.060 --> 01:54:02.060]  Я могу воспользоваться свойством 3.
[01:54:02.060 --> 01:54:05.060]  Ну, среднюю сумму я заменю на сумму средних.
[01:54:06.060 --> 01:54:13.060]  Давайте я тут пользуюсь 8,3.
[01:54:13.060 --> 01:54:18.060]  Сумма по i от 1 до n.
[01:54:18.060 --> 01:54:23.060]  h от x равно...
[01:54:23.060 --> 01:54:25.060]  Так.
[01:54:25.060 --> 01:54:34.060]  А чему равно средние величины, которые принимают значение 0 или 1?
[01:54:34.060 --> 01:54:37.060]  Ну, не 0,5.
[01:54:37.060 --> 01:54:42.060]  Я не говорил, что вероятность этого события у меня 0,5.
[01:54:42.060 --> 01:54:44.060]  Да.
[01:54:44.060 --> 01:54:48.060]  Согласны ли вы, что вероятность события, которое принимает значение 0 либо 1?
[01:54:48.060 --> 01:54:50.060]  То есть вероятность единицы...
[01:54:50.060 --> 01:54:52.060]  Ой, че я хочу? Я хочу среднее.
[01:54:52.060 --> 01:54:57.060]  Согласны ли вы, что среднее значение величины, которое принимает 0 или 1,
[01:54:57.060 --> 01:55:01.060]  тупо совпадает с вероятностью события 1?
[01:55:01.060 --> 01:55:03.060]  Ну, давайте я это пропишу.
[01:55:03.060 --> 01:55:05.060]  Сумма i от единицы до n.
[01:55:05.060 --> 01:55:12.060]  Вероятность того, что h от x совпадает с h от x i.
[01:55:12.060 --> 01:55:14.060]  Вот.
[01:55:14.060 --> 01:55:16.060]  А теперь вопрос.
[01:55:16.060 --> 01:55:20.060]  Чему равна вероятность того, что h от x совпадает с h от x i?
[01:55:25.060 --> 01:55:27.060]  При условии, что h от x у меня выбирается случайно,
[01:55:27.060 --> 01:55:29.060]  и h от x i тоже выбирается случайно.
[01:55:29.060 --> 01:55:31.060]  На отрезке от 0 до m-1.
[01:55:36.060 --> 01:55:38.060]  Одна mt.
[01:55:38.060 --> 01:55:44.060]  Ну, то есть утверждается, что вот так.
[01:55:48.060 --> 01:55:50.060]  Ну, логика здесь понятна.
[01:55:50.060 --> 01:55:52.060]  У меня h от x выбирается случайно.
[01:55:52.060 --> 01:55:54.060]  Ну, вот, смотрим сюда.
[01:55:54.060 --> 01:55:58.060]  x у меня выбирается случайно на отрезке от 0 до m-1.
[01:55:58.060 --> 01:56:00.060]  h от x это y.
[01:56:00.060 --> 01:56:02.060]  Тоже выбираются случайно на отрезке от 0 до m-1.
[01:56:02.060 --> 01:56:05.060]  Поэтому вероятность х совпадения это 1 на m.
[01:56:07.060 --> 01:56:13.060]  Тут есть небольшая несостыковка.
[01:56:13.060 --> 01:56:17.060]  Ну, тонкая правда, но это почти правда.
[01:56:17.060 --> 01:56:20.060]  Что здесь не так может быть?
[01:56:20.060 --> 01:56:30.060]  Ну, смотрите, верно ли, что h от x и h от x i независимы?
[01:56:32.060 --> 01:56:34.060]  Я бы даже так спросил.
[01:56:34.060 --> 01:56:37.060]  В каком случае h от x и h от x i независимы?
[01:56:37.060 --> 01:56:39.060]  Независят друг от друга.
[01:56:43.060 --> 01:56:47.060]  Почти всегда, кроме одного случая,
[01:56:47.060 --> 01:56:55.060]  когда я утверждаю, что вероятность того,
[01:56:55.060 --> 01:57:01.060]  что h от 5 равно h от 5, есть 1 на m.
[01:57:01.060 --> 01:57:03.060]  Как вы на это смотрите?
[01:57:05.060 --> 01:57:07.060]  Ну, это бред.
[01:57:07.060 --> 01:57:11.060]  То есть у меня h от x действительно независит от h от x i,
[01:57:11.060 --> 01:57:15.060]  так как я каждое значение х-функции выбираю случайно.
[01:57:15.060 --> 01:57:18.060]  Но это не выполняется в одном единственном случае,
[01:57:18.060 --> 01:57:21.060]  когда у меня х и есть x i.
[01:57:21.060 --> 01:57:25.060]  Поэтому тут мы немного исправим и напишем так.
[01:57:25.060 --> 01:57:28.060]  Это сумма по i от 1 до n.
[01:57:28.060 --> 01:57:31.060]  И напишем, ну как это пишется, вот так.
[01:57:31.060 --> 01:57:35.060]  Что это единица, если х совпадает с x i.
[01:57:35.060 --> 01:57:38.060]  Действительно, если х совпадает с x i,
[01:57:38.060 --> 01:57:42.060]  то у меня вероятность того, что и х-ж значения совпадут, равна 1.
[01:57:42.060 --> 01:57:45.060]  Ну, h-функция от одних и тех же элементов
[01:57:45.060 --> 01:57:48.060]  должна возвращать одно и то же значение.
[01:57:48.060 --> 01:57:52.060]  Но во всех остальных случаях она действительно равна 1 на m.
[01:57:52.060 --> 01:57:55.060]  При х неравном x i.
[01:57:55.060 --> 01:57:58.060]  Но это уже по свойству 4.
[01:57:58.060 --> 01:58:00.060]  По этому 4.
[01:58:00.060 --> 01:58:02.060]  Так.
[01:58:02.060 --> 01:58:06.060]  Ну и что у меня получится, когда я все это просуммирую?
[01:58:09.060 --> 01:58:10.060]  Ну, давайте так.
[01:58:10.060 --> 01:58:12.060]  Сколько раз у меня может быть в этой сумме,
[01:58:12.060 --> 01:58:15.060]  сколько раз у меня может быть выполнена эта первая строка?
[01:58:15.060 --> 01:58:17.060]  Ну, только один раз.
[01:58:17.060 --> 01:58:19.060]  Либо один, либо ноль раз.
[01:58:19.060 --> 01:58:22.060]  В зависимости от того, лежит у меня х во множестве х большое или нет.
[01:58:22.060 --> 01:58:23.060]  Согласны?
[01:58:23.060 --> 01:58:25.060]  Поэтому я напишу так.
[01:58:25.060 --> 01:58:27.060]  Что в случае, если х лежит во множестве х большое,
[01:58:27.060 --> 01:58:30.060]  то это 1 плюс n-1 деленное на m.
[01:58:30.060 --> 01:58:33.060]  В случае, если х малое, принадлежит х большому.
[01:58:33.060 --> 01:58:37.060]  А во всех остальных случаях, к чему эта сумма равна?
[01:58:37.060 --> 01:58:39.060]  Да, просто n на m.
[01:58:39.060 --> 01:58:41.060]  1 деленное на m.
[01:58:41.060 --> 01:58:43.060]  Ну, если х не принадлежит х.
[01:58:45.060 --> 01:58:47.060]  Ну, так или иначе, то есть реализуется первый вариант
[01:58:47.060 --> 01:58:49.060]  или второй вариант, неважно.
[01:58:49.060 --> 01:58:53.060]  Это все меньше либо равно, чем 1 плюс n деленное на m.
[01:58:53.060 --> 01:58:55.060]  Согласны?
[01:58:55.060 --> 01:58:59.060]  Ну, кажется, что доказательственно это можно завершить.
[01:59:04.060 --> 01:59:06.060]  Задайте какой-нибудь вопрос.
[01:59:09.060 --> 01:59:12.060]  Ну, какой-нибудь не обязательно по доказательству.
[01:59:12.060 --> 01:59:14.060]  Ладно, дела у меня хорошо, если что.
[01:59:14.060 --> 01:59:16.060]  В общем, все ясно, да?
[01:59:16.060 --> 01:59:18.060]  Я так полагаю.
[01:59:18.060 --> 01:59:20.060]  Окей.
[01:59:20.060 --> 01:59:22.060]  Ну и, собственно, отсюда следует,
[01:59:22.060 --> 01:59:24.060]  ну, давайте сразу напишем.
[01:59:24.060 --> 01:59:26.060]  Следствие среднее,
[01:59:26.060 --> 01:59:28.060]  среднее,
[01:59:28.060 --> 01:59:30.060]  среднее,
[01:59:30.060 --> 01:59:32.060]  среднее,
[01:59:32.060 --> 01:59:34.060]  среднее,
[01:59:34.060 --> 01:59:36.060]  среднее,
[01:59:36.060 --> 01:59:38.060]  среднее,
[01:59:39.060 --> 01:59:41.060]  время работы
[01:59:45.060 --> 01:59:47.060]  всех
[01:59:47.060 --> 01:59:49.060]  операций
[01:59:51.060 --> 01:59:53.060]  o большое от 1
[01:59:53.060 --> 01:59:55.060]  плюс n деленное на m.
[01:59:59.060 --> 02:00:01.060]  Все, то есть мы вроде как избавились
[02:00:01.060 --> 02:00:03.060]  от линейного поиска
[02:00:03.060 --> 02:00:05.060]  и теперь свели задачу
[02:00:05.060 --> 02:00:07.060]  к поиску n деленное на m шагов.
[02:00:07.060 --> 02:00:09.060]  Н деленное на m шагов в среднем.
[02:00:11.060 --> 02:00:13.060]  Так.
[02:00:13.060 --> 02:00:15.060]  Ну и теперь давайте вернемся
[02:00:15.060 --> 02:00:17.060]  к вопросу
[02:00:17.060 --> 02:00:19.060]  эффективности, а именно подбора m.
[02:00:21.060 --> 02:00:23.060]  Ну, смотрите, снова есть проблема.
[02:00:23.060 --> 02:00:25.060]  То есть, конечно, n делить на m это хорошо.
[02:00:25.060 --> 02:00:27.060]  То есть то, что у нас хэш-функция размазывает все элементы равномерно,
[02:00:27.060 --> 02:00:29.060]  это прям во, это прям замечательно.
[02:00:29.060 --> 02:00:31.060]  Но проблема заключается, опять же,
[02:00:31.060 --> 02:00:33.060]  вернемся к тому, что
[02:00:33.060 --> 02:00:35.060]  если у меня число элементов n большое,
[02:00:35.060 --> 02:00:37.060]  а хэш-таблица маленькая,
[02:00:37.060 --> 02:00:39.060]  тысяча элементов,
[02:00:39.060 --> 02:00:41.060]  тогда на поиск каждого элемента у меня уходит
[02:00:41.060 --> 02:00:43.060]  в среднем тысяча шагов.
[02:00:43.060 --> 02:00:45.060]  Что неприемлемо, да?
[02:00:45.060 --> 02:00:47.060]  Ну и, собственно, вопрос.
[02:00:55.060 --> 02:00:57.060]  Как подбирать m?
[02:00:57.060 --> 02:00:59.060]  Может, кого-то готов ответ?
[02:01:01.060 --> 02:01:03.060]  Ну так, вот с насколько?
[02:01:03.060 --> 02:01:05.060]  В зависимости от х, который подается.
[02:01:05.060 --> 02:01:07.060]  А как это?
[02:01:11.060 --> 02:01:13.060]  А что значит х достаточно большой?
[02:01:13.060 --> 02:01:15.060]  На самом деле, вот здесь,
[02:01:15.060 --> 02:01:17.060]  в чем крутость этой теоремы,
[02:01:17.060 --> 02:01:19.060]  ну и в чем крутость вот этого всего,
[02:01:19.060 --> 02:01:21.060]  это в том, что я от ха теперь вообще никак не завишу.
[02:01:21.060 --> 02:01:23.060]  То есть, неважно, х большой, маленький,
[02:01:23.060 --> 02:01:25.060]  и так далее, у меня h от х
[02:01:25.060 --> 02:01:27.060]  это случайная величина. То есть я
[02:01:27.060 --> 02:01:29.060]  от анализа х перешел к анализу h от х.
[02:01:29.060 --> 02:01:31.060]  Окей?
[02:01:31.060 --> 02:01:33.060]  У меня х от х тут явно вряд ли что-то зависит.
[02:01:35.060 --> 02:01:37.060]  Порядка n. Отлично.
[02:01:37.060 --> 02:01:39.060]  А как мне это обеспечить?
[02:01:39.060 --> 02:01:41.060]  Смотрите, я создаю х-таблицу.
[02:01:41.060 --> 02:01:43.060]  У вас есть задача там, скажем.
[02:01:43.060 --> 02:01:45.060]  Приходит произвольное количество элементов,
[02:01:45.060 --> 02:01:47.060]  вам нужно быстро отвечать на запросы вида,
[02:01:47.060 --> 02:01:49.060]  есть ли элементы, вы можете видеть их или нет.
[02:01:49.060 --> 02:01:51.060]  Как вы определите, вам придет тысяча элементов
[02:01:51.060 --> 02:01:53.060]  или миллион элементов?
[02:01:53.060 --> 02:01:55.060]  Но идея правильная.
[02:01:55.060 --> 02:01:57.060]  И снова вы с ней уже сталкивались
[02:01:57.060 --> 02:01:59.060]  в первом семестре, даже в этом семестре.
[02:01:59.060 --> 02:02:01.060]  Что нужно делать, если вы заранее не знаете
[02:02:01.060 --> 02:02:03.060]  количество элементов?
[02:02:03.060 --> 02:02:05.060]  Динамически расширять.
[02:02:05.060 --> 02:02:07.060]  Динамически расширять. Отлично.
[02:02:07.060 --> 02:02:09.060]  Идея такая.
[02:02:09.060 --> 02:02:11.060]  Первый пункт.
[02:02:11.060 --> 02:02:13.060]  Наблюдение, так скажем.
[02:02:13.060 --> 02:02:15.060]  Если n деленное на m, скажем,
[02:02:15.060 --> 02:02:17.060]  меньше либо равно единицы,
[02:02:23.060 --> 02:02:25.060]  то сложность
[02:02:25.060 --> 02:02:27.060]  в среднем какая?
[02:02:27.060 --> 02:02:29.060]  Если я контролирую,
[02:02:29.060 --> 02:02:31.060]  что n деленное на m
[02:02:31.060 --> 02:02:33.060]  всегда меньше или равно единицы,
[02:02:33.060 --> 02:02:35.060]  то средняя сложность поиска, вставки и так далее
[02:02:35.060 --> 02:02:37.060]  всегда от единицы, согласны?
[02:02:37.060 --> 02:02:39.060]  В среднем.
[02:02:39.060 --> 02:02:41.060]  Отлично.
[02:02:41.060 --> 02:02:43.060]  Как гарантировать...
[02:02:43.060 --> 02:02:45.060]  Теперь вопрос как
[02:02:49.060 --> 02:02:51.060]  гарантировать n деленное на m
[02:02:51.060 --> 02:02:53.060]  меньше или равно единицы?
[02:02:53.060 --> 02:02:55.060]  И ответ уже был
[02:02:55.060 --> 02:02:57.060]  динамически расширять m.
[02:02:57.060 --> 02:02:59.060]  Ровно как мы это делали в динамическом массиве.
[02:02:59.060 --> 02:03:01.060]  Окей?
[02:03:03.060 --> 02:03:05.060]  Ответ
[02:03:07.060 --> 02:03:09.060]  динамически
[02:03:11.060 --> 02:03:13.060]  увеличивать
[02:03:13.060 --> 02:03:15.060]  m.
[02:03:19.060 --> 02:03:21.060]  Ну, увеличивать...
[02:03:21.060 --> 02:03:23.060]  Давайте напишем пару b и m.
[02:03:25.060 --> 02:03:27.060]  Ну, m это характеристика b.
[02:03:27.060 --> 02:03:29.060]  Понятно, что я по факту
[02:03:29.060 --> 02:03:31.060]  увеличиваю количество корзин.
[02:03:33.060 --> 02:03:35.060]  Что я увеличиваю?
[02:03:35.060 --> 02:03:37.060]  Число корзин или сами корзины?
[02:03:39.060 --> 02:03:41.060]  Идея понятна, да?
[02:03:41.060 --> 02:03:43.060]  Грубо говоря,
[02:03:43.060 --> 02:03:45.060]  что вы делаете? Вы стартуете с нуля корзин.
[02:03:45.060 --> 02:03:47.060]  Вам приходит новый элемент,
[02:03:47.060 --> 02:03:49.060]  вы его вставляете, но вставлять некуда.
[02:03:49.060 --> 02:03:51.060]  И вы говорите, что у вас есть ровно одна корзина.
[02:03:51.060 --> 02:03:53.060]  Приходит еще один элемент,
[02:03:53.060 --> 02:03:55.060]  вы число корзин увеличиваете в два раза.
[02:03:55.060 --> 02:03:57.060]  Ну и так далее.
[02:03:57.060 --> 02:03:59.060]  Если в какой-то момент у вас число корзин
[02:03:59.060 --> 02:04:01.060]  совпало с числом хранимых элементов,
[02:04:01.060 --> 02:04:03.060]  то все, значит хэш-таблица уже заполнена.
[02:04:03.060 --> 02:04:05.060]  При следующей вставке вы не можете
[02:04:05.060 --> 02:04:07.060]  гарантировать вот такую штуку никак.
[02:04:07.060 --> 02:04:09.060]  Поэтому вы просто берете и что делать?
[02:04:09.060 --> 02:04:11.060]  Ну, увеличиваете число корзин в два раза, например.
[02:04:13.060 --> 02:04:15.060]  Понятное дело, что нужно это делать
[02:04:15.060 --> 02:04:17.060]  по мультипликативной схеме, так это
[02:04:17.060 --> 02:04:19.060]  амортизированную единицу при увеличении.
[02:04:23.060 --> 02:04:25.060]  Все, соответственно, мы готовы
[02:04:25.060 --> 02:04:27.060]  выписать финальный алгоритм.
[02:04:35.060 --> 02:04:37.060]  А в массиве у вас нет такой проблемы?
[02:04:37.060 --> 02:04:39.060]  Тут ровно такая же проблема.
[02:04:39.060 --> 02:04:41.060]  То есть у вас никогда
[02:04:41.060 --> 02:04:43.060]  не получается так, что
[02:04:43.060 --> 02:04:45.060]  число элементов, скажем так,
[02:04:45.060 --> 02:04:47.060]  если вы выделите в два раза, то у вас число корзин
[02:04:47.060 --> 02:04:49.060]  в два раза больше, чем число хранимых элементов.
[02:04:51.060 --> 02:04:53.060]  Что миллион, что два миллиона,
[02:04:53.060 --> 02:04:55.060]  не важно.
[02:04:55.060 --> 02:04:57.060]  То есть если вы выделяете память ее недостаточно,
[02:04:57.060 --> 02:04:59.060]  ну все, закончилась память.
[02:04:59.060 --> 02:05:01.060]  Ошибка.
[02:05:11.060 --> 02:05:13.060]  Это прям отличное замечание.
[02:05:13.060 --> 02:05:15.060]  Да, мы к нему вернемся.
[02:05:17.060 --> 02:05:19.060]  Грубо говоря, из вашего замечания следует,
[02:05:19.060 --> 02:05:21.060]  что вот это вот все не имеет смысла и, короче,
[02:05:21.060 --> 02:05:23.060]  вот так не получится. Но это спойлер.
[02:05:23.060 --> 02:05:25.060]  Пока алгоритм,
[02:05:25.060 --> 02:05:27.060]  который не работает.
[02:05:27.060 --> 02:05:29.060]  Алгоритм.
[02:05:35.060 --> 02:05:37.060]  Ну, давайте так.
[02:05:37.060 --> 02:05:39.060]  Find и
[02:05:39.060 --> 02:05:41.060]  Erase.
[02:05:41.060 --> 02:05:43.060]  Понятно, да?
[02:05:45.060 --> 02:05:47.060]  Ну, понятно, что происходит при Find и Erase.
[02:05:47.060 --> 02:05:49.060]  Ну, при Find просто
[02:05:49.060 --> 02:05:51.060]  ищем нужную корзину, ищем в этом списке.
[02:05:51.060 --> 02:05:53.060]  При Erase тоже. Смотрим в нужную корзину,
[02:05:53.060 --> 02:05:55.060]  ищем в этом списке нужный элемент
[02:05:55.060 --> 02:05:57.060]  и удаляем его.
[02:05:57.060 --> 02:05:59.060]  Давайте отдельно, давайте
[02:05:59.060 --> 02:06:01.060]  insert распишем, как более интересный.
[02:06:01.060 --> 02:06:03.060]  Insert.
[02:06:05.060 --> 02:06:07.060]  Если...
[02:06:07.060 --> 02:06:09.060]  Так.
[02:06:11.060 --> 02:06:13.060]  Если n плюс 1,
[02:06:13.060 --> 02:06:15.060]  деленное на m,
[02:06:15.060 --> 02:06:17.060]  меньше либо равно единице,
[02:06:19.060 --> 02:06:21.060]  то вставляем...
[02:06:23.060 --> 02:06:25.060]  Да, ну и так.
[02:06:27.060 --> 02:06:29.060]  Значит, давайте...
[02:06:29.060 --> 02:06:31.060]  Если x принадлежит
[02:06:31.060 --> 02:06:33.060]  hash таблице,
[02:06:33.060 --> 02:06:35.060]  то сразу делаем return.
[02:06:35.060 --> 02:06:37.060]  Ну, то есть в hash таблице мы храним
[02:06:37.060 --> 02:06:39.060]  только уникальные значения, окей?
[02:06:39.060 --> 02:06:41.060]  Все. Дальше я предполагаю, что
[02:06:41.060 --> 02:06:43.060]  x у меня не содержится в hash таблице.
[02:06:43.060 --> 02:06:45.060]  Значит, если при добавлении нового элемента
[02:06:45.060 --> 02:06:47.060]  у меня получается, что
[02:06:47.060 --> 02:06:49.060]  я остаюсь в рамках единицы,
[02:06:49.060 --> 02:06:51.060]  то есть у меня load factor для степени загруженности
[02:06:51.060 --> 02:06:53.060]  меньше либо равно единицы, то я просто вставляю элемент x,
[02:06:53.060 --> 02:06:55.060]  вставляю x в
[02:06:55.060 --> 02:06:57.060]  b от
[02:06:57.060 --> 02:06:59.060]  h от x.
[02:07:01.060 --> 02:07:03.060]  А
[02:07:03.060 --> 02:07:05.060]  иначе
[02:07:09.060 --> 02:07:11.060]  генерирую...
[02:07:11.060 --> 02:07:13.060]  Давайте так.
[02:07:15.060 --> 02:07:17.060]  Увеличиваю
[02:07:21.060 --> 02:07:23.060]  размер b
[02:07:23.060 --> 02:07:25.060]  до
[02:07:25.060 --> 02:07:27.060]  2m, например.
[02:07:27.060 --> 02:07:29.060]  Генерирую
[02:07:33.060 --> 02:07:35.060]  h' от x.
[02:07:35.060 --> 02:07:37.060]  Генерирую
[02:07:37.060 --> 02:07:39.060]  hash функцию h',
[02:07:39.060 --> 02:07:41.060]  которая
[02:07:41.060 --> 02:07:43.060]  принимает значение от 0 до
[02:07:43.060 --> 02:07:45.060]  2m-1.
[02:07:47.060 --> 02:07:49.060]  И вставляю
[02:07:51.060 --> 02:07:53.060]  x
[02:07:53.060 --> 02:07:55.060]  в новую
[02:07:55.060 --> 02:07:57.060]  hash таблицу.
[02:07:57.060 --> 02:07:59.060]  Понятно?
[02:08:01.060 --> 02:08:03.060]  Ну все. Если размер hash таблицы
[02:08:03.060 --> 02:08:05.060]  не позволяет вставить новый элемент, я его вставляю.
[02:08:05.060 --> 02:08:07.060]  Если размер hash таблицы не позволяет
[02:08:07.060 --> 02:08:09.060]  обеспечить вот такое условие,
[02:08:09.060 --> 02:08:11.060]  то я увеличиваю
[02:08:11.060 --> 02:08:13.060]  число координат два раза. Естественно,
[02:08:13.060 --> 02:08:15.060]  я должен
[02:08:15.060 --> 02:08:17.060]  заново назначить hash функцию,
[02:08:17.060 --> 02:08:19.060]  потому что старая hash функция мне генерирует значение
[02:08:19.060 --> 02:08:21.060]  от 0 до m.
[02:08:21.060 --> 02:08:23.060]  Новая должна генерирует значение от 0 до 2m.
[02:08:25.060 --> 02:08:27.060]  Понятное дело, что я должен
[02:08:27.060 --> 02:08:29.060]  перевставить элементы. Тут, наверное, не очевидно, но
[02:08:29.060 --> 02:08:31.060]  плюс надо
[02:08:31.060 --> 02:08:33.060]  перевставить
[02:08:37.060 --> 02:08:39.060]  элементы
[02:08:41.060 --> 02:08:43.060]  в новую hash таблицу.
[02:08:43.060 --> 02:08:45.060]  Ну и даже вставить новый элемент.
[02:08:45.060 --> 02:08:47.060]  Ну и работает это
[02:08:47.060 --> 02:08:49.060]  от единицы в среднем
[02:08:49.060 --> 02:08:51.060]  и
[02:08:51.060 --> 02:08:53.060]  в амортизационном смысле.
[02:08:53.060 --> 02:08:55.060]  Ну в среднем понятно, потому что
[02:08:55.060 --> 02:08:57.060]  это зависит от случайности hash функции,
[02:08:57.060 --> 02:08:59.060]  а амортизационный анализ
[02:08:59.060 --> 02:09:01.060]  мне дает то, что
[02:09:01.060 --> 02:09:03.060]  моя схема расширения
[02:09:03.060 --> 02:09:05.060]  работает в амортизационном смысле за единицу.
[02:09:05.060 --> 02:09:07.060]  Окей?
[02:09:15.060 --> 02:09:17.060]  Не-не-не, их тоже надо вставлять,
[02:09:17.060 --> 02:09:19.060]  потому что у вас hash функция могла измениться.
[02:09:19.060 --> 02:09:21.060]  То есть у вас старая hash функция, она генерирует
[02:09:21.060 --> 02:09:23.060]  значение от 0 до m, а новая будет
[02:09:23.060 --> 02:09:25.060]  генерирует значение от 0 до 2m.
[02:09:25.060 --> 02:09:27.060]  Их надо перевставить. Да, вот, собственно, вот это
[02:09:27.060 --> 02:09:29.060]  замечание про это.
[02:09:29.060 --> 02:09:31.060]  Что их надо заново перевставить.
[02:09:31.060 --> 02:09:33.060]  То есть их
[02:09:33.060 --> 02:09:35.060]  корзины могли поменяться.
[02:09:35.060 --> 02:09:37.060]  Ну хорошо.
[02:09:51.060 --> 02:09:53.060]  Так, вопросы по алгоритму есть?
[02:09:59.060 --> 02:10:01.060]  Так, ну,
[02:10:01.060 --> 02:10:03.060]  к сожалению, у этого алгоритма есть
[02:10:03.060 --> 02:10:05.060]  несколько проблем.
[02:10:05.060 --> 02:10:07.060]  Ну, начнем
[02:10:07.060 --> 02:10:09.060]  с малых,
[02:10:09.060 --> 02:10:11.060]  с малых из них,
[02:10:11.060 --> 02:10:13.060]  а именно
[02:10:13.060 --> 02:10:15.060]  ну сейчас будет пара технических деталей,
[02:10:15.060 --> 02:10:17.060]  которые
[02:10:17.060 --> 02:10:19.060]  могут
[02:10:19.060 --> 02:10:21.060]  вызывать проблемы.
[02:10:33.060 --> 02:10:35.060]  Ну, допустим,
[02:10:35.060 --> 02:10:37.060]  у меня какая-то такая получилась,
[02:10:37.060 --> 02:10:39.060]  ну, такая hash таблица.
[02:10:41.060 --> 02:10:43.060]  Ну, смотрите, естественным свойством
[02:10:43.060 --> 02:10:45.060]  любой структуры данных
[02:10:45.060 --> 02:10:47.060]  является, ну, возможность пройтись
[02:10:47.060 --> 02:10:49.060]  по всем ее элементам.
[02:10:49.060 --> 02:10:51.060]  Ну, согласны, да? Если у вас есть массив,
[02:10:51.060 --> 02:10:53.060]  то вы, наверное, хотите в какой-то момент
[02:10:53.060 --> 02:10:55.060]  вывести все его элементы.
[02:10:55.060 --> 02:10:57.060]  Скажем, если у вас есть бинарное дерево поиска,
[02:10:57.060 --> 02:10:59.060]  наверное, тоже естественное желание
[02:10:59.060 --> 02:11:01.060]  взять и вывести все его элементы.
[02:11:01.060 --> 02:11:03.060]  Если у вас есть hash таблица,
[02:11:03.060 --> 02:11:05.060]  то тоже, наверное, хочется взять и в какой-то момент
[02:11:05.060 --> 02:11:07.060]  там вывести все его элементы.
[02:11:07.060 --> 02:11:09.060]  Ну, почему нет, скажем.
[02:11:09.060 --> 02:11:11.060]  В чем здесь может быть проблема?
[02:11:11.060 --> 02:11:13.060]  Ну, давайте так,
[02:11:13.060 --> 02:11:15.060]  сначала вернемся к...
[02:11:15.060 --> 02:11:17.060]  Какая сложность вывода всех элементов из массива?
[02:11:17.060 --> 02:11:19.060]  Вот у вас есть массив, нужно вывести все элементы.
[02:11:19.060 --> 02:11:21.060]  Какая сложность этой операции?
[02:11:21.060 --> 02:11:23.060]  ОАТН. У вас есть,
[02:11:23.060 --> 02:11:25.060]  не знаю,
[02:11:25.060 --> 02:11:27.060]  пирамиды или бинарное дерево поиска.
[02:11:27.060 --> 02:11:29.060]  Какая сложность вывода всех элементов там?
[02:11:29.060 --> 02:11:31.060]  Ну, тоже ОАТН. То есть это тоже можно сделать за ОАТН.
[02:11:31.060 --> 02:11:33.060]  Ну, просто инордер обходом дерева
[02:11:33.060 --> 02:11:35.060]  или поствор.
[02:11:35.060 --> 02:11:37.060]  В общем, инордер обходом дерева, например.
[02:11:37.060 --> 02:11:39.060]  А вот здесь.
[02:11:39.060 --> 02:11:41.060]  Я хочу пройтись по всем элементам.
[02:11:41.060 --> 02:11:43.060]  По этим элементам.
[02:11:43.060 --> 02:11:45.060]  По порядку.
[02:11:45.060 --> 02:11:47.060]  Какая сложность здесь?
[02:11:49.060 --> 02:11:51.060]  Почему квадрат?
[02:11:55.060 --> 02:11:57.060]  ОАТН.
[02:11:59.060 --> 02:12:01.060]  О.
[02:12:01.060 --> 02:12:04.060]  А как вы это себе представляете?
[02:12:04.060 --> 02:12:08.060]  Ну, давайте попробуем это все проделать.
[02:12:08.060 --> 02:12:10.060]  Понятно, что я иду в первую картину,
[02:12:10.060 --> 02:12:12.060]  смотрю первый элемент, вывожу его,
[02:12:12.060 --> 02:12:14.060]  перехожу в следующий элемент, вывожу его,
[02:12:14.060 --> 02:12:16.060]  перехожу сюда, вывожу его.
[02:12:16.060 --> 02:12:18.060]  Список закончился, перехожу в следующий картине.
[02:12:18.060 --> 02:12:20.060]  Эта корзина то не пустая.
[02:12:20.060 --> 02:12:22.060]  Ну, хорошо, пойду дальше,
[02:12:22.060 --> 02:12:24.060]  Перехожу в следующий картине, она пустая.
[02:12:24.060 --> 02:12:26.060]  Перехожу в эту корзине, они пустые, все так далее.
[02:12:26.060 --> 02:12:31.820]  на что не наталкивает? n плюс m, да, то есть помимо того, что я прохожусь по всем
[02:12:31.820 --> 02:12:36.380]  элементам, я еще должен пройтись потенциально по всем пустым корзинам.
[02:12:36.380 --> 02:12:39.660]  Вот чтобы найти первую незаполненную корзину, мне тоже нужно потратить
[02:12:39.660 --> 02:12:45.260]  m времени. Вообще говоря, это не очень приятно, потому что m может быть больше,
[02:12:45.260 --> 02:12:50.100]  чем n, по крайней мере, мы так делаем, что с m хотя бы в два раза больше, а если мы
[02:12:50.100 --> 02:12:53.420]  еще делаем и raisin, то есть скажем, у нас хэштаблица увеличилась до тысячи
[02:12:53.420 --> 02:12:56.540]  элементов, потом в какой-то момент мы все элементы поудаляли, у нас осталось
[02:12:56.540 --> 02:13:00.140]  десять элементов, а хэштаблица осталась размером тысячи элементов, ну в общем
[02:13:00.140 --> 02:13:06.700]  неприятно. n плюс m. Хотелось бы это тоже улучшить до линейного
[02:13:06.700 --> 02:13:14.140]  прохода, и классически это делается так. На самом деле, на самом деле, в нормальной
[02:13:14.140 --> 02:13:18.140]  хэштаблице, то есть, хорошо, вот эта реализация хэштаблицы, она норм, вот, и сразу
[02:13:18.140 --> 02:13:21.340]  скажу, что у вас там в третьем задании будет задача реализовать хэштаблицу, и в
[02:13:21.340 --> 02:13:25.020]  принципе это нормально, то есть в качестве базового решения это зайдет, то есть балл вы
[02:13:25.020 --> 02:13:28.620]  свой получите. Вот у нас будут дополнительные пункты, где надо будет реализовать
[02:13:28.620 --> 02:13:36.060]  хэштаблицу по-человечески, а вот по-человечески это вот как. Вообще говоря,
[02:13:36.060 --> 02:13:38.940]  вы не храните несколько разных списков, вот, кстати, хранить несколько разных
[02:13:38.940 --> 02:13:42.620]  списков тоже в некотором смысле накладно, потому что вы должны, ну, несколько списков,
[02:13:42.620 --> 02:13:48.460]  несколько различных переменных и так далее. Идея заключается в следующем. Я буду хранить
[02:13:48.460 --> 02:14:12.060]  я буду хранить один общий список элементов, один общий список, ну, пусть так. Я храню один
[02:14:12.060 --> 02:14:15.420]  общий список элементов, но причем не просто так вот, не просто в произвольном порядке,
[02:14:15.660 --> 02:14:22.820]  а делаю следующую вещь. Элементы, которые лежат в одной корзине, обязательно должны
[02:14:22.820 --> 02:14:31.820]  идти подряд. Вот это свойство. Скажем, вот так, и вот так. Ну, и скажем, у меня заполнена вот эта
[02:14:31.820 --> 02:14:44.900]  корзина, это оранжевая, значит, вот эта зеленая корзина, красная корзина. Вот, то есть у меня есть
[02:14:44.900 --> 02:14:48.780]  корзина, у меня есть общий список элементов. Вопрос, что я тогда должен хранить в самих корзинах?
[02:14:48.780 --> 02:14:57.260]  Да, смотрите, мне достаточно хранить всего лишь указатель. Указатель на начало списка элементов.
[02:14:57.260 --> 02:15:03.700]  То есть я понимаю, что вот элементы из зеленой корзины начинаются здесь, поэтому я храню
[02:15:03.700 --> 02:15:09.900]  указатель сюда. Элементы из красной корзины начинаются здесь, храню указатель сюда. Оранжевая
[02:15:09.900 --> 02:15:19.420]  корзина здесь. Отлично. Вопрос следующий. Как мне понять, допустим, я хочу пройтись по всем
[02:15:19.420 --> 02:15:28.220]  элементам корзины. Как мне понять, что моя корзина закончилась? Что? А как я проверю,
[02:15:28.220 --> 02:15:37.620]  совпал ли указатель? Как я проверю, что вот на этот элемент указывает кто-то или нет? Быстро. Парень
[02:15:37.620 --> 02:15:49.020]  знает толпы в возвращениях, да, но нет. Что? Да, отлично. Чем характеризуются элементы одной корзины?
[02:15:49.020 --> 02:16:02.020]  У них одинаковый хэш. Тут h1, ну хэш значения. Тут h3, тоже 2. То есть чтобы понять, вышел я за пределы
[02:16:02.020 --> 02:16:08.340]  корзины или нет, мне достаточно посмотреть, какой хэш у этого чудака. Ага. Если у него хэш не совпался
[02:16:08.340 --> 02:16:13.860]  с хэшом моей корзины, то все, это значит, что я перешел в другую корзину и все. На этом я
[02:16:13.860 --> 02:16:19.980]  должен остановиться. Понятно? То есть достаточно сравнить хэш значения и все. Потому что каждая
[02:16:19.980 --> 02:16:33.180]  корзина характеризуется своим хэш значением. В общем, это уже больше похоже на правду. Ну,
[02:16:33.180 --> 02:16:41.500]  есть еще один момент. Еще один момент. Ну, он такой более тонкий, который заключается в следующем.
[02:16:41.500 --> 02:16:49.060]  Ну, смотрите, мне вопрос. Какой список я тут должен хранить? Двусвязный или односвязный? Двусвязный.
[02:16:49.060 --> 02:16:55.820]  Почему двусвязный? Да, ответ Cafe Jesus. Да, отлично. То есть кажется, что я должен хранить двусвязный список.
[02:16:55.820 --> 02:17:00.320]  Почему? Потому что я хочу удалить, скажем, вот мне сказали удалить вот этот элемент. Если
[02:17:00.320 --> 02:17:05.780]  я хочу удалить этот элемент, мне нужно, что делать? Я его удаляю и беру ссылку на предыдущий элемент и
[02:17:05.780 --> 02:17:10.800]  провязываю его со следующим. То есть мне нужно хранить как ссылку на предыдущий а так и на следующий.
[02:17:10.800 --> 02:17:18.480]  Вопрос. Можно ли как-то избавиться от этого? Ну потому что кажется, что на каждый элемент...
[02:17:18.480 --> 02:17:20.480]  то есть смотрите, что у вас хранится в каждой ячейке.
[02:17:20.480 --> 02:17:23.480]  В каждой ячейке у вас хранится значение value,
[02:17:23.480 --> 02:17:27.480]  в каждой ячейке у вас хранится указатель на next,
[02:17:27.480 --> 02:17:29.480]  хранится указатель на prev,
[02:17:29.480 --> 02:17:33.480]  и еще, возможно, хранится hash.
[02:17:33.480 --> 02:17:37.480]  То есть на самом деле hash можно хранить непосредственно в самом элементе,
[02:17:37.480 --> 02:17:38.480]  а можно его вычитать на ходу,
[02:17:38.480 --> 02:17:41.480]  но это в зависимости от того, работает ли у вас hash быстро или нет.
[02:17:41.480 --> 02:17:43.480]  В стандартной библиотеке так и реализовано,
[02:17:43.480 --> 02:17:46.480]  что если там, скажем, hash быстрый, то есть hash отчисел,
[02:17:46.480 --> 02:17:47.480]  то он не хранится.
[02:17:47.480 --> 02:17:50.480]  Если у вас hash от строк, а hash от строк работает долго,
[02:17:50.480 --> 02:17:51.480]  он работает за длину строки,
[02:17:51.480 --> 02:17:53.480]  вот тогда hash просто непосредственно хранится рядом с элементом.
[02:17:53.480 --> 02:17:55.480]  То есть у вас получается следующая вещь,
[02:17:55.480 --> 02:17:59.480]  у вас хранится value и плюс куча дополнительной информации.
[02:17:59.480 --> 02:18:02.480]  Ну а память хочется экономить.
[02:18:05.480 --> 02:18:08.480]  Можно ли как-то это все реализовать на односвязном списке?
[02:18:17.480 --> 02:18:20.480]  А как это нам поможет?
[02:18:27.480 --> 02:18:33.480]  Ну, допустим, я хочу удалить вот этот элемент.
[02:18:33.480 --> 02:18:36.480]  У меня есть указатель на этот элемент, я хочу его удалить.
[02:18:41.480 --> 02:18:44.480]  Да, нужно понять, какой у меня предыдущий элемент.
[02:18:46.480 --> 02:18:55.480]  Нет, нет, нет, это прям связный список.
[02:18:59.480 --> 02:19:02.480]  Ну, короче, идея стоит в том, что нужно хранить указатель
[02:19:02.480 --> 02:19:06.480]  не на самый элемент, а на элемент перед ним.
[02:19:06.480 --> 02:19:25.480]  То есть вот так, вот так, и вот так.
[02:19:25.480 --> 02:19:28.480]  Если я храню указатель на элемент перед ним, то у меня все хорошо.
[02:19:28.480 --> 02:19:31.480]  Ну скажем, я хочу удалить вот этот элемент.
[02:19:31.480 --> 02:19:34.480]  Если я хочу удалить этот элемент, то я должен хранить итератор,
[02:19:34.480 --> 02:19:37.480]  указатель не на него, а вот на этот элемент.
[02:19:38.480 --> 02:19:42.480]  Если я храню указатель на элемент перед ним, то все хорошо.
[02:19:43.480 --> 02:19:48.480]  Я удаляю этот элемент, и дальше просто провязываю ссылку вот сюда.
[02:19:57.480 --> 02:20:00.480]  Все понятно или вопрос какой-то есть?
[02:20:04.480 --> 02:20:06.480]  Ну, короче, чтобы у меня появилась возможность в односвязанном списке
[02:20:06.480 --> 02:20:10.480]  удалить эти элементы, мне нужно на самом деле при удалении
[02:20:10.480 --> 02:20:14.480]  хранить не сам указатель на элемент нужный, а на указатель перед ним.
[02:20:14.480 --> 02:20:17.480]  Если хранить указатель на переднем, то все нормально.
[02:20:17.480 --> 02:20:20.480]  Ну и собственно, по той же причине вы должны хранить указатель
[02:20:20.480 --> 02:20:24.480]  здесь не на самый элемент, а на указатель перед началом списка.
[02:20:24.480 --> 02:20:27.480]  Почему? Потому что если вы захотите удалить самый первый элемент корзины,
[02:20:27.480 --> 02:20:31.480]  ну вот здесь, представьте мне, что из этой корзины соuinely
[02:20:31.480 --> 02:20:34.480]  если вы хотите удалить самый первый элемент,
[02:20:34.480 --> 02:20:38.480]  тогда вам нужно хранить указатель, который указывает на элемент перед ним.
[02:20:38.480 --> 02:20:42.480]  В этом случае вы спокойно можете удалить первый элемент корзины и сделать вот так.
[02:20:42.480 --> 02:20:45.480]  Понятно?
[02:20:45.480 --> 02:20:50.480]  Но это уже на самом деле такие детали.
[02:20:50.480 --> 02:20:55.480]  Кстати, в стандартном классе forward-list об этом мы еще будем говорить.
[02:20:55.480 --> 02:21:00.480]  В связи с этой проблемой есть методы insertAfter.
[02:21:00.480 --> 02:21:04.480]  В обычном списке есть методы просто insert и erase,
[02:21:04.480 --> 02:21:07.480]  а в forward-list там другие методы.
[02:21:07.480 --> 02:21:10.480]  Там методы insertAfter и eraseAfter.
[02:21:10.480 --> 02:21:13.480]  Потому что мы можем вставлять только после элемента
[02:21:13.480 --> 02:21:16.480]  и удалять элемент, который стоит после него.
[02:21:21.480 --> 02:21:24.480]  Ну ладно, с этим закончим.
[02:21:24.480 --> 02:21:27.480]  И наконец, финальная проблема,
[02:21:27.480 --> 02:21:31.480]  которая уже была освещена,
[02:21:31.480 --> 02:21:36.480]  состоит в том, что это все невозможно.
[02:21:36.480 --> 02:21:39.480]  Это все не имеет смысла.
[02:21:39.480 --> 02:21:45.480]  Давайте вернемся к определению простого равномерного х-ширования.
[02:21:45.480 --> 02:21:48.480]  Что говорит простой равномерный х-ширование?
[02:21:48.480 --> 02:21:51.480]  Давайте одно из них, что у меня h генерируется случайно
[02:21:51.480 --> 02:21:56.480]  из множества от 0 до m-1 степени k.
[02:21:56.480 --> 02:21:58.480]  Ну либо я генерирую так.
[02:21:58.480 --> 02:22:05.480]  h от x равно random от m.
[02:22:05.480 --> 02:22:13.480]  Либо беру старое значение h от x, если x у меня уже раньше встречался.
[02:22:13.480 --> 02:22:17.480]  И я утверждаю, что простого равномерного х-ширования в жизни не существует.
[02:22:17.480 --> 02:22:20.480]  Вот его реализовать у вас не получится.
[02:22:20.480 --> 02:22:25.480]  Вот такой х-ш функции в реальности, в реальных приложениях быть не может.
[02:22:25.480 --> 02:22:27.480]  Тут есть два объяснения.
[02:22:27.480 --> 02:22:31.480]  Первая теоретика информационная, которая основана на первом определении.
[02:22:31.480 --> 02:22:34.480]  Ну, смотрите.
[02:22:34.480 --> 02:22:36.480]  Вот, допустим, у вас есть некоторое множество.
[02:22:36.480 --> 02:22:40.480]  Множество размера n.
[02:22:40.480 --> 02:22:44.480]  Сколько bit вам нужно, чтобы хранить в памяти компьютера
[02:22:44.480 --> 02:22:49.480]  один элемент этого множества?
[02:22:49.480 --> 02:22:52.480]  Ну, хорошо. У вас есть множество размера 10.
[02:22:52.480 --> 02:22:55.480]  Возможно ли это множество представить...
[02:22:55.480 --> 02:22:58.480]  Возможно ли каждый элемент этого множества представить одним bit-ом?
[02:22:58.480 --> 02:23:00.480]  Нет, потому что 1 bit – это 0 или 1.
[02:23:00.480 --> 02:23:05.480]  Возможно ли элементы 10-элементного множества представить двумя bit-ами?
[02:23:05.480 --> 02:23:08.480]  Нет, двумя bit-ами можно закодировать только 4 числа.
[02:23:08.480 --> 02:23:14.480]  То есть множество размера n можно закодировать только логарифмом доичным по n bit-ов.
[02:23:14.480 --> 02:23:15.480]  Да?
[02:23:15.480 --> 02:23:17.480]  То есть это...
[02:23:17.480 --> 02:23:19.480]  Давайте, словами скажу, что это количество bit,
[02:23:19.480 --> 02:23:22.480]  которое нужно потратить, чтобы в памяти компьютера
[02:23:22.480 --> 02:23:25.480]  представить элемент n-элементного множества.
[02:23:25.480 --> 02:23:32.480]  Какой размер этого множества?
[02:23:32.480 --> 02:23:39.480]  Огромный, правильный ответ.
[02:23:39.480 --> 02:23:41.480]  В степени k.
[02:23:41.480 --> 02:23:43.480]  И беру мощность этого множества.
[02:23:43.480 --> 02:23:46.480]  Ну, мощность этого множества вычисляется просто как мощность...
[02:23:46.480 --> 02:23:50.480]  Ну, мощность вот этого множества, то есть m в степени мощности вот этого множества.
[02:23:50.480 --> 02:23:53.480]  Ну, очень удобная аннотация, вот так.
[02:23:53.480 --> 02:23:55.480]  Если возьму логарифм от этой штуки,
[02:23:55.480 --> 02:23:59.480]  логарифм вот m в степени k,
[02:23:59.480 --> 02:24:04.480]  то это будет мощность k, умноженная на лог m.
[02:24:04.480 --> 02:24:06.480]  Ну, лог m, ладно, забили.
[02:24:06.480 --> 02:24:09.480]  Меня интересует больше вот эта вот штука.
[02:24:09.480 --> 02:24:11.480]  Bit.
[02:24:11.480 --> 02:24:14.480]  Чему равна мощность k в стандартном случае?
[02:24:14.480 --> 02:24:18.480]  Вот, допустим, мы хотим хэшировать n-ты.
[02:24:18.480 --> 02:24:20.480]  Сколько всего n-тов?
[02:24:20.480 --> 02:24:22.480]  4 миллиарда.
[02:24:22.480 --> 02:24:24.480]  То есть, чтобы сохранить хэш-функцию,
[02:24:24.480 --> 02:24:26.480]  которая хэширует n-ты,
[02:24:26.480 --> 02:24:29.480]  нам нужно 4 миллиарда бит, ну, как минимум 4 миллиарда бит.
[02:24:29.480 --> 02:24:33.480]  4 миллиарда бит это что-то типа полгигабайта.
[02:24:33.480 --> 02:24:36.480]  Готовы ли вы на каждую хэш-функцию тратить полгигабайта?
[02:24:36.480 --> 02:24:38.480]  Спорный вопрос.
[02:24:38.480 --> 02:24:40.480]  Хорошо, k это не числа.
[02:24:40.480 --> 02:24:42.480]  k это строки.
[02:24:42.480 --> 02:24:44.480]  Строк еще больше.
[02:24:44.480 --> 02:24:46.480]  Готовы ли вы тратить еще больше, чем полгигабайта?
[02:24:46.480 --> 02:24:48.480]  Вряд ли.
[02:24:48.480 --> 02:24:50.480]  Ну, в общем, хранить хэш-функции очень дорого.
[02:24:50.480 --> 02:24:53.480]  В общем, представить в памяти компьютера не представляется возможным.
[02:24:53.480 --> 02:24:57.480]  Это первый довод.
[02:24:57.480 --> 02:24:59.480]  Вы можете сказать так, ну, смотрите,
[02:24:59.480 --> 02:25:01.480]  а зачем нам хранить хэш-функции целиком?
[02:25:01.480 --> 02:25:05.480]  Ну, кой нам смысл хранить все возможные хэш-взначения для всех элементов?
[02:25:05.480 --> 02:25:08.480]  Мы же можем, как мы тут предполагали,
[02:25:08.480 --> 02:25:11.480]  можем для каждого элемента просто динамически генерировать хэш-взначения
[02:25:11.480 --> 02:25:13.480]  и хранить только их.
[02:25:13.480 --> 02:25:17.480]  Не видите ли вы тут какого-то противоречия, парадокса?
[02:25:23.480 --> 02:25:39.480]  Отлично. Как понять, встречался х или нет?
[02:25:39.480 --> 02:25:41.480]  Ну, смотрите, у меня тут выглядит как.
[02:25:41.480 --> 02:25:43.480]  Если х ранее у меня никогда не встречался,
[02:25:43.480 --> 02:25:45.480]  я генеру рандомное число.
[02:25:45.480 --> 02:25:47.480]  Если х ранее когда-то встречался,
[02:25:47.480 --> 02:25:49.480]  то я использую ранее сгенерированное значение.
[02:25:49.480 --> 02:25:51.480]  То есть мне нужно как-то хранить, как-то уметь хранить
[02:25:51.480 --> 02:25:53.480]  ранее встречавшееся значение
[02:25:53.480 --> 02:25:55.480]  и тот хэш, который я им сгенерировал.
[02:25:55.480 --> 02:25:57.480]  Но кажется, что эта задача состоит
[02:25:57.480 --> 02:25:59.480]  ровно в том, чтобы построить хэш-таблицу.
[02:25:59.480 --> 02:26:01.480]  А чтобы построить хэш-таблицу,
[02:26:01.480 --> 02:26:03.480]  мне нужна хэш-функция.
[02:26:03.480 --> 02:26:05.480]  И, ну, понятно.
[02:26:05.480 --> 02:26:07.480]  Проблема ясна?
[02:26:07.480 --> 02:26:09.480]  Вот, поэтому
[02:26:09.480 --> 02:26:11.480]  ни вот так, ни вот так
[02:26:11.480 --> 02:26:13.480]  ничего не получится.
[02:26:15.480 --> 02:26:17.480]  Нет.
[02:26:17.480 --> 02:26:19.480]  Все нормально.
[02:26:19.480 --> 02:26:21.480]  И победим эту проблему.
[02:26:21.480 --> 02:26:23.480]  Видимо, не сегодня, но
[02:26:23.480 --> 02:26:25.480]  я хотя бы
[02:26:25.480 --> 02:26:27.480]  дам некоторую надежду.
[02:26:29.480 --> 02:26:31.480]  Простого равномерного хэширования
[02:26:31.480 --> 02:26:33.480]  не существует.
[02:26:35.480 --> 02:26:37.480]  Но на простых равномерных
[02:26:37.480 --> 02:26:39.480]  хэшах свет клином не сошелся.
[02:26:39.480 --> 02:26:41.480]  Есть
[02:26:41.480 --> 02:26:43.480]  другой хороший класс
[02:26:43.480 --> 02:26:45.480]  хэш-функций.
[02:26:45.480 --> 02:26:47.480]  Ну, это уже тема второй лекции,
[02:26:47.480 --> 02:26:49.480]  ну давайте начнем ее, чтобы
[02:26:49.480 --> 02:26:51.480]  в следующий раз
[02:26:51.480 --> 02:26:53.480]  поменьше писать было.
[02:26:53.480 --> 02:26:55.480]  Значит, универсальное
[02:26:57.480 --> 02:26:59.480]  универсальное
[02:27:05.480 --> 02:27:07.480]  семейство
[02:27:09.480 --> 02:27:11.480]  хэш-функций.
[02:27:11.480 --> 02:27:13.480]  Мотивация, идея.
[02:27:13.480 --> 02:27:15.480]  Ну давайте
[02:27:15.480 --> 02:27:17.480]  сегодня только идею, наверное, обсудим.
[02:27:17.480 --> 02:27:19.480]  Смотрите.
[02:27:19.480 --> 02:27:21.480]  Вот в чем проблема простого равномерного хэширования?
[02:27:21.480 --> 02:27:23.480]  Проблема простого равномерного хэширования,
[02:27:23.480 --> 02:27:25.480]  что я выбираю хэш-функцию
[02:27:25.480 --> 02:27:27.480]  из вообще
[02:27:27.480 --> 02:27:29.480]  из класса
[02:27:29.480 --> 02:27:31.480]  всевозможных хэш-функций.
[02:27:31.480 --> 02:27:33.480]  Этот класс хэш-функций
[02:27:33.480 --> 02:27:35.480]  огромный.
[02:27:35.480 --> 02:27:37.480]  То есть сохранить его в памяти компьютера
[02:27:37.480 --> 02:27:39.480]  не представляется возможным.
[02:27:39.480 --> 02:27:41.480]  Как-то представить компактным образом
[02:27:41.480 --> 02:27:43.480]  тоже не получается.
[02:27:47.480 --> 02:27:49.480]  Но что можно сделать?
[02:27:49.480 --> 02:27:51.480]  Можно взять не все пространство
[02:27:51.480 --> 02:27:53.480]  хэш-функций,
[02:27:53.480 --> 02:27:55.480]  а лишь некоторые его подможества.
[02:27:55.480 --> 02:27:57.480]  Давайте обозначу h от альфы.
[02:27:57.480 --> 02:27:59.480]  Вот h от альфы это просто некоторое подможество
[02:27:59.480 --> 02:28:01.480]  хэш-функций.
[02:28:01.480 --> 02:28:03.480]  Я буду рассматривать не все
[02:28:03.480 --> 02:28:05.480]  хэш-функции,
[02:28:05.480 --> 02:28:07.480]  а только какой-то конечный поднабор.
[02:28:07.480 --> 02:28:09.480]  Словно, есть пространство
[02:28:09.480 --> 02:28:11.480]  всевозможных функций из r в r.
[02:28:15.480 --> 02:28:17.480]  Какая мощность
[02:28:17.480 --> 02:28:19.480]  этого множества?
[02:28:21.480 --> 02:28:23.480]  Что?
[02:28:23.480 --> 02:28:25.480]  Ладно, очень большая.
[02:28:25.480 --> 02:28:27.480]  Я понял.
[02:28:27.480 --> 02:28:29.480]  Короче, более чем консинвальная, да?
[02:28:29.480 --> 02:28:31.480]  Хорошо.
[02:28:31.480 --> 02:28:33.480]  А что если я вам скажу, что я рассматриваю
[02:28:33.480 --> 02:28:35.480]  не функции из r в r, а только функции
[02:28:35.480 --> 02:28:37.480]  f от x равно a от x.
[02:28:37.480 --> 02:28:39.480]  a умножить на x, точнее.
[02:28:39.480 --> 02:28:41.480]  Чему равна мощность
[02:28:41.480 --> 02:28:43.480]  вот этого множества?
[02:28:43.480 --> 02:28:45.480]  Мощность у всех функций, которые
[02:28:45.480 --> 02:28:47.480]  имеют вид a умножить на x.
[02:28:47.480 --> 02:28:49.480]  r.
[02:28:49.480 --> 02:28:51.480]  Да?
[02:28:51.480 --> 02:28:53.480]  То есть мощность этого множества совпадает
[02:28:53.480 --> 02:28:55.480]  с мощностью r. Уже полегче.
[02:28:55.480 --> 02:28:57.480]  А что если я скажу, что a принимает
[02:28:57.480 --> 02:28:59.480]  значения,
[02:28:59.480 --> 02:29:01.480]  ну, только натуральные значения?
[02:29:01.480 --> 02:29:03.480]  Тогда множество всех этих функций это
[02:29:03.480 --> 02:29:05.480]  просто счетное.
[02:29:05.480 --> 02:29:07.480]  Да?
[02:29:07.480 --> 02:29:09.480]  То есть мало того, что их счетное, так еще я могу
[02:29:09.480 --> 02:29:11.480]  вот этому множеству компактным образом представить.
[02:29:11.480 --> 02:29:13.480]  Чтобы сохранить одну такую функцию,
[02:29:13.480 --> 02:29:15.480]  мне достаточно хранить всего лишь одно число.
[02:29:15.480 --> 02:29:17.480]  Одно число a. То есть одно число a мне полностью
[02:29:17.480 --> 02:29:19.480]  описывает всю мою функцию.
[02:29:19.480 --> 02:29:21.480]  Ага.
[02:29:21.480 --> 02:29:23.480]  То есть чего я хочу?
[02:29:23.480 --> 02:29:25.480]  Я хочу построить h от альфа,
[02:29:27.480 --> 02:29:29.480]  которое описывается
[02:29:29.480 --> 02:29:31.480]  скажем так, небольшим
[02:29:37.480 --> 02:29:39.480]  небольшим числом
[02:29:39.480 --> 02:29:41.480]  параметров
[02:29:43.480 --> 02:29:45.480]  и удовлетворяет
[02:29:45.480 --> 02:29:47.480]  свойство.
[02:29:53.480 --> 02:29:55.480]  Давайте свойство выпишем и закончим.
[02:29:55.480 --> 02:29:57.480]  Я хочу, чтобы это семейство,
[02:29:57.480 --> 02:29:59.480]  вот это маленькое подношество
[02:29:59.480 --> 02:30:01.480]  всех хэш-функций,
[02:30:01.480 --> 02:30:03.480]  обладало таким же свойством,
[02:30:03.480 --> 02:30:05.480]  как и простое равномерное хширение.
[02:30:05.480 --> 02:30:07.480]  А простое равномерное хширение, ну, какое
[02:30:07.480 --> 02:30:09.480]  свойство мне от него нужно? На самом деле от простое
[02:30:09.480 --> 02:30:11.480]  равномерное хширение мне нужно было только
[02:30:11.480 --> 02:30:13.480]  вот это свойство.
[02:30:13.480 --> 02:30:15.480]  Что вероятность совпадения элементов равна 1 на m.
[02:30:15.480 --> 02:30:17.480]  И вот ровно это я и потребую
[02:30:17.480 --> 02:30:19.480]  от вот этого небольшого множества функций.
[02:30:19.480 --> 02:30:21.480]  Свойство
[02:30:21.480 --> 02:30:23.480]  для любых x и y,
[02:30:23.480 --> 02:30:25.480]  таких, что x не равен y,
[02:30:25.480 --> 02:30:27.480]  вероятность того, что
[02:30:27.480 --> 02:30:29.480]  h от x
[02:30:29.480 --> 02:30:31.480]  совпадает с h от y
[02:30:31.480 --> 02:30:33.480]  должно быть меньше вновь, чем 1 на m.
[02:30:35.480 --> 02:30:37.480]  Вот если мне удастся найти
[02:30:37.480 --> 02:30:39.480]  такое семейство хэш-функций,
[02:30:39.480 --> 02:30:41.480]  которое я могу легко описать, ну, то есть вот
[02:30:41.480 --> 02:30:43.480]  одним или двумя параметрами,
[02:30:43.480 --> 02:30:45.480]  и плюс мне будет удовлетворяться
[02:30:45.480 --> 02:30:47.480]  вот это свойство, что какие бы я два значения
[02:30:47.480 --> 02:30:49.480]  не взял, у них вероятность совпадения будет меньше
[02:30:49.480 --> 02:30:51.480]  вновь, чем 1 на m, то я победил.
[02:30:51.480 --> 02:30:53.480]  Тогда я вот эту хэш-функцию могу подставить
[02:30:53.480 --> 02:30:55.480]  вот сюда, и все будет то же самое.
[02:30:55.480 --> 02:30:57.480]  Согласны?
[02:30:57.480 --> 02:30:59.480]  Ну вот пример,
[02:30:59.480 --> 02:31:01.480]  как может выглядеть вот эта хэш-функция,
[02:31:01.480 --> 02:31:03.480]  мы приведем в следующий раз. Пока все.

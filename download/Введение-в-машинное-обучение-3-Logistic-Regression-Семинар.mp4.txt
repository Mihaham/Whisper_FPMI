[00:00.000 --> 00:10.680]  Давайте посмотрим вот сюда. Сначала с рок-кривой разберёмся, с ней на самом деле всё очень просто,
[00:10.680 --> 00:16.660]  если её ещё раз посмотреть. На всякий случай. Рок-кривая, да? Мы с вами варьируем по рок,
[00:16.660 --> 00:21.180]  который у нас есть. Вот я для примера пока тут нарисовал маленькую выборочку. Вот у нас там
[00:21.180 --> 00:27.160]  есть объектов. Сколько? Раз, два, три, четыре, пять, шесть, семь, восемь, девять объектов. Соответственно,
[00:27.160 --> 00:32.800]  вот вероятности, которые мы ему предсказываем нашим классикатором. Вот это наша P+. Хорошо? А
[00:32.800 --> 00:42.200]  это соответственно Y, то есть истинная метка класса. 1 либо 0. Согласились? Пока всё понятно. Для
[00:42.200 --> 00:46.280]  удобства, так как у нас все объекты независимы, можем их отсортировать таким образом, чтобы у
[00:46.280 --> 00:51.160]  нас убывала вероятность положительная. Окей? То есть как её можно построить на самом деле на
[00:51.160 --> 00:56.240]  практике? Ну вот, вот так мы построили. И соответственно, на самом деле площадь под кривой нам будет показывать
[00:56.240 --> 01:02.960]  вероятность, что из случайно заданной пары объектов, вот взяли пару объектов случайно, и они у нас
[01:02.960 --> 01:10.600]  будут упорядочены по вероятностям классикатора также, как и по своим меткам класса. Вот что он нам
[01:10.600 --> 01:15.200]  показывает. Что у нас правильно отсортировано будет. Можем вот так вот построить и просто-напросто
[01:15.200 --> 01:21.640]  двигать порог вверху. То есть сначала у нас порог вообще нулевой, точнее порог где-то единица, туда
[01:21.640 --> 01:29.040]  ни один объект не попал. True positive rate соответственно у нас равен нулю, правильно? У нас никто не попал
[01:29.040 --> 01:35.040]  в положительные объекты. False positive rate у нас тоже равен нулю, у нас никто в отрицательные объекты
[01:35.040 --> 01:42.080]  тоже не попал. Согласны? Вот мы с вами вот здесь сидим. Дальше соответственно мы начинаем порог
[01:42.080 --> 01:46.600]  понижать. Теперь у нас порог вот здесь проходит. Соответственно теперь у нас один объект попал в
[01:46.600 --> 01:52.080]  положительный класс. В отрицательный класс пока у нас никто не попал, правильно? Ну значит вот
[01:52.080 --> 02:00.560]  True positive rate у нас вырос, так на всякий случай тут у нас TPR, тут у нас FPR. Вот он у нас вырос,
[02:00.560 --> 02:05.400]  мы попали вот сюда. Теперь мы можем сдвинуть на один, но надо сразу сдвигать на два, потому что у нас
[02:05.400 --> 02:11.280]  два объекта с одинаковой вероятностью. Следующий порог. У нас получается один объект попал в
[02:11.280 --> 02:16.160]  отрицательный класс, один объект попал в положительный класс. Согласны? Значит True positive rate
[02:16.160 --> 02:21.600]  подрос на единичку, false positive rate тоже подрос на единичку. Ну соответственно вот мы попали сюда и сюда.
[02:21.600 --> 02:35.040]  Вот оно у нас. Примерно вот сюда, короче мы попали. Ой, простите, я наврал не туда.
[02:35.040 --> 02:50.400]  Вот сюда мы с вами попали, вот оно у нас. Едем дальше, теперь опять 0,6 порог идем вверх, 0,5 порог
[02:50.400 --> 03:02.000]  опять идем вверх, 0,4 порог идем вправо, 0,3 опять вправо, 0,2 вверх, 0,1 вправо. Теперь опять же порог
[03:02.000 --> 03:08.960]  у нас меньше 0,1, все объекты отнесены уже куда? В положительный класс, правильно? Соответственно True positive rate
[03:08.960 --> 03:13.600]  у нас равен единице, все объекты попали в положительный класс. False positive rate тоже единица, все
[03:13.600 --> 03:18.080]  объекты отрицательного класса тоже попали в положительный класс. Вот ваши все точки, вот мы с
[03:18.080 --> 03:24.920]  вами построили наш урок кривую. И соответственно вот тут задавали вопрос, а что, где гарантия,
[03:24.920 --> 03:31.920]  что у нас True positive rate растет быстрее, чем False positive rate? Ну на самом деле гарантия лишь в том,
[03:31.920 --> 03:38.240]  что если наш классикатор лучше, чем случайный, то есть он хотя бы как-то упорядочивает объекты
[03:38.240 --> 03:45.400]  одного класса относительно другого, то у нас будет кривая выше диагонали. Если он хуже, то понятное
[03:45.400 --> 03:49.160]  дело ничего хорошего не будет. И собственно почему мне этот график нравится? На самом деле это
[03:49.160 --> 03:57.720]  иллюстрация, ну авторство ее принадлежит Александру Дяканову, который много где преподавал,
[03:57.720 --> 04:02.560]  работал в МГУ, много чего рассказывает. У него есть очень классный блок анализ малых данных, я ссылки на
[04:02.560 --> 04:06.760]  самом деле сегодня после семинара скину. И отсюда, кстати, очень хорошо видно, что такое площадь под
[04:06.760 --> 04:12.440]  кривой краски, под урок кривой. Вот же мы ее нарисовали, это у нас единичный квадрат, тут у нас была долика
[04:12.440 --> 04:16.240]  краски положительного класса объектов, тут отрицательного. Заметьте, тут четыре краски,
[04:16.240 --> 04:21.920]  вот четыре объекта отрицательного класса, тут пять делений, пять объектов. И вот эта вот штуковина под
[04:22.000 --> 04:28.800]  кривой. Мы с вами краску покрываем какой-то множество квадратиков. Каждый квадратик это что? Это пара,
[04:28.800 --> 04:33.760]  какой-то объект отрицательного класса, какой-то объект положительного класса, верно? Это и показывает
[04:33.760 --> 04:38.160]  вам вот сколько у вас пара объектов правильно ориентирована относительно истинных метокласс.
[04:38.160 --> 04:43.560]  То есть насколько хорошо ваш классикатор умеет ранжировать ваши объекты так, чтобы классикатор,
[04:43.560 --> 04:47.640]  объект, который положительного класса, имел большую вероятность, чем объект, который отрицательного
[04:47.640 --> 05:07.280]  класса. Уловили? Да, такое бывает, то есть она может какой-то вот такой вид иметь. Это нормально,
[05:07.280 --> 05:12.240]  тут нас волнует именно площадь под кривой. Если у вас площадь меньше чем 0,5, значит у вас
[05:12.240 --> 05:17.560]  вероятность, что объекты ориентированы правильно, меньше чем 0,5. Поменяйте метки классов,
[05:17.560 --> 05:26.880]  вероятность опять будет больше чем 0,5. Только и всего. Ну значит, что у вас в среднем объекты
[05:26.880 --> 05:32.800]  случайным образом ориентированы, только и всего. То есть у вас при каких-то порогах, типа вот здесь
[05:32.800 --> 05:37.640]  она вроде растет лучше, вот здесь наоборот хуже, но в среднем у вас классикатор случайным образом
[05:37.640 --> 05:44.080]  относит порядок объектов, предсказывает порядок объектов, только и всего. Так, понять не стало срок
[05:44.080 --> 05:53.280]  кривой, что это такое? А, вот этот наклонный. На самом деле, потому что можно было нарисовать один вверх,
[05:53.280 --> 05:57.240]  один справа, но непонятно как, потому что у нас два объекта, которых вероятности одинаковые,
[05:57.240 --> 06:02.920]  а метки классов разные. Соответственно у нас и TPR вырос, и FPR вырос, поэтому мы пошли по
[06:02.920 --> 06:08.920]  диагонали. Ну опять же, это на самом деле нормально, почему? Предположим, вообще выраженный случай, у вас два
[06:08.920 --> 06:13.360]  объекта, один положительного класса, другой отрицательного, вероятность для них одинакового.
[06:13.360 --> 06:18.120]  Тогда у вас должна быть диагональная прямая, потому что у вас вероятность правильно их отранжировать
[06:18.120 --> 06:33.320]  50 процентов. Размер шажочка, вот это, по сути, да, это труп-пост-фрейд, это один объект относительно
[06:33.320 --> 06:37.960]  всей выборки положительных объектов. Тут, допустим, их пять, поэтому здесь пять карасушков.
[06:37.960 --> 06:49.040]  Уловили, нет? Раз, два, три, четыре, пять, шесть, семь, восемь, девять. Пять единиц, четыре нуля.
[06:49.040 --> 06:57.200]  Да, здесь по ноль двадцать пять, то есть каждый шаг, по сути, это доля выборки. Один объект,
[06:57.200 --> 07:02.000]  в данном случае, 25 процентов отрицательной выборки и 20 процентов положительной выборки.
[07:02.000 --> 07:15.800]  Все. Так, ну чего, стало понятней? Да, да, да. Так, смотрите, отранжировал, шел порогом сверху вниз,
[07:15.800 --> 07:21.440]  соответственно, для каждого порога была точка. Если непонятно, то я вот скину там,
[07:21.440 --> 07:27.160]  протестовала статья написать. Но, по сути, вот вы идете сверху вниз по отранжированным вещам,
[07:27.160 --> 07:34.080]  если единичка вверх, если нолик вправо. Но это такой дубовый вариант. Плюс, опять же, у меня сегодня
[07:34.080 --> 07:39.200]  много фраз вопроса собеседований, это нормально. Про линейную классификацию очень любят гонять на
[07:39.200 --> 07:44.840]  собеседованиях в условных крупных компаниях, поэтому лучше это, чтобы вас от дубов отскакивало,
[07:44.840 --> 07:50.440]  если вы пойдете туда собеседоваться. У меня в свое время когда-то завалили вопросом про урок
[07:50.440 --> 07:56.160]  кривую для многоклассовой классификации. Я вот пытался понять, как ее можно построить. Ответ?
[07:56.160 --> 08:03.360]  Никак, для One versus Rest можно для всех построить, для каждого класса отдельно. А что-то там
[08:03.360 --> 08:09.880]  многомерное придумать, пока такого нет. Вот, итак, вопрос по желанию есть?
[08:09.880 --> 08:22.400]  3, 2, 1. Хорошо. Так, у кого сегодня ноуты есть? Классно. Но я тогда предлагаю немножко покодить,
[08:22.400 --> 08:27.720]  потому что с ноутбуками на самом деле сегодня будет достаточно любопытно. Мы с вами,
[08:27.720 --> 08:33.960]  недолго думая, сейчас залезем в самый замечательный PyTorch и все наши линейные классикаторы
[08:33.960 --> 08:40.240]  напишем уже на нем. Так что открывайте ноуты, я пока закомечу все, что надо.
[08:40.240 --> 08:58.940]  Вот квадратик – это у вас все возможные пары. Лосик под кривой – это отношение правильно
[08:58.940 --> 09:10.260]  отсердированных пар ко всем. ТПР – это доля правильных ответов относительно всех
[09:10.260 --> 09:16.380]  положительных. А ФПР – это доля неправильных ответов про положительные относительно всех
[09:16.380 --> 09:36.220]  отрицательно. Итак, коллеги, удобно. Но с телефона разве что кодить неудобно. Смотрите,
[09:36.220 --> 09:41.900]  если с телефона предлагаю, тогда просто на самом деле посмотреть. Уже, я думаю, разницы,
[09:41.900 --> 09:56.340]  я думаю, не будет. Хорошо, коллеги, в репозитории появился нужный нам, соответственно, пример. Я,
[09:56.340 --> 10:01.220]  наверное, запущу вообще все локально, потому что локально оно будет работать чуть постабильней.
[10:01.220 --> 10:11.340]  Смотрите, тут в гет есть, там в мл курсе опечатка, там нижний прочерк вместо дефис. Я глазами
[10:11.340 --> 10:17.580]  менял ссылку, поэтому ошибся. Короче, смотрите, собственно, о чем на самом деле сегодня у нас
[10:17.580 --> 10:25.460]  пойдет речь. Речь пойдет в двух, скажем так, шагах про две различные истории. Во-первых,
[10:25.460 --> 10:30.380]  мы с вами поговорим про PyTorch в целом, потому что это крайне классный фреймворк. Если кто-то из
[10:30.380 --> 10:35.020]  вас приверженит CTEF, это абсолютно нормально. Они, в принципе, сопоставимы друг с другом. Просто
[10:35.020 --> 10:40.620]  для учебных целей, по-моему, предпочтительнее PyTorch, он привычнее, он понятнее. Во-вторых,
[10:40.620 --> 10:45.580]  мы с вами поговорим про линейную классикацию и посмотрим, как можно построить линейный классикатор
[10:45.580 --> 10:51.460]  на коленке. Более того, по факту мы с вами сегодня уже маленькую игрушечную нейронную сеть построим,
[10:51.460 --> 11:02.020]  потому что что такое нейронная сеть, как не просто линейная модель на спироидах. Сегодня у нас
[11:02.020 --> 11:06.540]  спироидов не будет, поэтому будет просто линейное отображение плюс сегмоиды в конце. Но тем не менее,
[11:06.660 --> 11:11.620]  все остальное работает. Более того, сегодня мы с вами вспомним чуть-чуть о том, какие бывают
[11:11.620 --> 11:16.540]  проблемы, когда мы работаем с градиентной оптимизацией и почему может быть полезным
[11:16.540 --> 11:21.100]  нормировать ваши данные. Ну что ж, давайте потихоньку начинать. Все, у кого есть ноутбуки,
[11:21.100 --> 11:26.140]  рекомендую прям вместе со мной кодить. Там кода понадобится три строчки. Все, у кого ноутбука
[11:26.140 --> 11:33.220]  нет, внимание на экран. Если что-то не видно, пожалуйста, говорите. Принято? Хорошо. Итак,
[11:33.220 --> 11:38.420]  ну всякие дифферам бы пои торчу, я с вашего позволения петь сейчас не буду. Если коротко,
[11:38.420 --> 11:43.700]  то с NumPy вы уже знакомы. По сути, PyTorch это эдакий NumPy на максималках, который умеет
[11:43.700 --> 11:49.780]  работать со всяким GPU, умеет автоматически читать градиенты, умеет из коробки строить кучу,
[11:49.780 --> 11:54.380]  ну не строить, а давать вам в качестве строительных блоков кучу различных операций,
[11:54.380 --> 11:59.220]  которые гораздо удобнее взять оттуда, чем руками реализовывать и так далее. Ну и в принципе,
[11:59.220 --> 12:05.300]  у него огромная комьюнити. И с недавних пор, кстати, PyTorch теперь находится под крылом не отдельной
[12:05.300 --> 12:11.660]  крупной корпорации, а под крылом Linux Foundation, по-моему, но точнее, аналогичной ему PyTorch Foundation.
[12:11.660 --> 12:17.340]  Так что, чистый open-source, наше все. На самом деле, в отделе исследования и образования я считаю,
[12:17.340 --> 12:22.500]  что это большое добро, когда ни один, грубо говоря, крупный игрок не имеет прямого влияния на тот
[12:22.500 --> 12:27.700]  или иной инструмент. Конечно, в свое время и PyTorch и TensorFlow сильно поднялись за счет того,
[12:27.700 --> 12:32.740]  что их гигант IT-индустрии поддерживали. PyTorch, соответственно, запрещенная в России
[12:32.740 --> 12:39.420]  организация. TensorFlow не запрещенный пока в России Google. Но сейчас, скажем так, они, я так понимаю,
[12:39.420 --> 12:45.220]  перевели его под крыловое сообщество, что в принципе хорошо. Ну а развиться им это помогло. Итак,
[12:45.220 --> 12:51.620]  ну что ж, пара, наверное, слов. С NumPyme, я думаю, все работать умеют. Вы можете вывести Shape,
[12:51.620 --> 12:56.660]  чего-нибудь, вы можете сложить все поэлементно, вы можете там использовать всякие дот-продукты,
[12:56.660 --> 13:01.940]  операции и так далее. На всякий случай, предупрежу вас, в NumPyme можно дот-продукт даже
[13:01.940 --> 13:06.620]  с матрицами использовать, несмотря на то, что вроде как с точки зрения теории дот-продукт
[13:06.620 --> 13:18.940]  определен только для векторов. Как правило, framework может включать себе пачку в библиотеку,
[13:20.460 --> 13:26.260]  но вообще в данном контексте я бы сказал, что ни в чем. Просто иногда обдывают по-разному.
[13:26.260 --> 13:32.580]  Просто PyTorch все-таки называют обычный deep learning framework, хотя библиотека для каких-нибудь там
[13:32.580 --> 13:37.900]  построений граф-учислений в принципе тоже подходит. Но хотя Torch, как правило, включает
[13:37.900 --> 13:44.980]  себя допустим там, как его называют, короче. Ну с точки зрения структурирования именно кода,
[13:44.980 --> 13:50.140]  по сути там несколько прям отдельных пакетов, но на самом деле как в NumPyme тоже. Тут надо в
[13:50.140 --> 13:55.660]  терминологии его право углубить. Короче, я надеюсь NumPyme все пользоваться умеют,
[13:55.660 --> 14:04.460]  тут комментарии излишни, правильно? Пожелания есть какие-то у вас? Три-два раз? Тишина. Ну ладно.
[14:04.460 --> 14:10.100]  Плюс PyTorch в том, что вам не придется переучиваться. С PyTorch вы можете делать абсолютно все то же
[14:10.100 --> 14:15.060]  самое. У вас даже синтаксис меняется практически никак. Вот я почему заговорил про дот-продукт на
[14:15.060 --> 14:19.340]  самом деле. В PyTorch себе таких вольнский не позволяли, поэтому здесь используется нормальный
[14:19.340 --> 14:26.420]  матмул, matrix multiplication, который в NumPy есть, он в принципе работает. Тут стоит помнить, что некоторые,
[14:26.420 --> 14:32.500]  но редкие, но тем не менее некоторые ключевые слова и функции в PyTorch в NumPy работают по-разному,
[14:32.500 --> 14:37.260]  поэтому, как всегда я говорю, читайте доки. То есть не надо думать, что вы сели за новый фреймворк и
[14:37.260 --> 14:42.780]  начали писать на нем свободно, хотя с PyTorch почти так и выйдет. Короче, они похожи друг на друга,
[14:42.780 --> 14:50.020]  исключительно в этом была суть. Но есть небольшие разницы. Например, Bshape переходит во Vue. На самом
[14:50.020 --> 14:57.020]  деле тут даже стоит сказать, что Vue именно пытается строить Vue. В чем отличие Vue от построения новой
[14:57.020 --> 15:04.580]  матрицы? Понимаете? Да, то есть на всякий случай, если вдруг у кого там BD не было или чего. Когда
[15:04.580 --> 15:09.020]  вы строите Vue, у вас данные все еще лежат в памяти тем же образом, вы просто определенным образом их
[15:09.020 --> 15:16.460]  считываете. У вас нет дюпликации данных. Если вы именно строите новый тензор, новую матрицу с
[15:16.460 --> 15:25.740]  другой формой, вы именно передаписывать данные. В NumPy? В NumPy, наверное. Просто в PyTorch тоже есть,
[15:25.740 --> 15:30.780]  и не всегда PyTorch может построить Vue. Он тогда сам об этом говорит с ошибкой и предлагает
[15:30.780 --> 15:36.060]  именно перестроить ее заново. Я про это предупреждаю. То есть Vue работает в PyTorch не всегда,
[15:36.060 --> 15:40.060]  иногда у вас размерности может быть какие-то неправильные. Он вам об этом скажет, попросит так
[15:40.060 --> 15:46.020]  не делать. Во-вторых, самая большая разница и что руки постоянно забывают, постоянно приходится
[15:46.020 --> 15:51.780]  себя поправлять, это Exus становится Dim Dimension. Ну и типа Danuhup и Torch соответственно тоже свои.
[15:51.780 --> 15:57.460]  В принципе, почитать Doki ничего страшного. Но давайте для начала что-нибудь тогда красивенькое
[15:57.460 --> 16:04.340]  находим, просто чтобы было красиво. Тут вы как понимаете, пока ничего не написано. Собственно,
[16:04.340 --> 16:12.020]  давайте вот эту формулу сверху нарисуем своими руками. Ну я думаю, здесь комментарии излишние.
[16:12.020 --> 16:24.100]  Просто чтобы начали писать на PyTorch. И коллеги, на всякий случай, я думаю, что у вас уже много было
[16:24.100 --> 16:30.140]  курсов по программированию и так далее. Но лучше, когда вот у вас такие функции используются,
[16:30.140 --> 16:38.580]  лучше импортировать их, как бы сказать, не from Torch import cos, а просто импортировать Torch и
[16:38.580 --> 16:42.980]  дергать Torch cos, чтобы вам явно было видно, а главное не вам, а всем остальным, откуда именно функция
[16:42.980 --> 16:48.460]  Cosinus используется. Потому что есть Cosinus в математическом пакете, есть NumPy, есть SyPy, есть PyTorch,
[16:48.460 --> 16:53.860]  есть многое еще. И, вторых, пожалуйста, никогда не используйте импорт звездочка, ладно? Вот за это
[16:53.860 --> 16:59.060]  прям можно по рукам получить где-нибудь в коллективе. Импорт звездочка не делайте.
[16:59.060 --> 17:09.900]  Ну скажем так, если вы искренне понимаете, что вы делаете, вы можете делать что угодно,
[17:09.900 --> 17:16.700]  но в принципе это правило хорошего тона, импорт звездочка не делать. Знаете, дзену... читали дзен
[17:16.700 --> 17:24.820]  Питона. Многие читали, правильно? Кто не читал Zen of Python? Ну вот вы сейчас в Питоне сидите,
[17:24.820 --> 17:36.860]  напишите import this. Ну, собственно, явно лучше, чем не явно. Вот это ровно о том, что вы говорите,
[17:36.860 --> 17:40.740]  как бы в своих файлах по идее можно, но лучше явно что-то импортируйте, потому что импорт звездочка,
[17:40.740 --> 17:44.380]  вы не знаете, что вы импортировали. Вам туда сосед написал какую-то гадость,
[17:44.380 --> 17:56.740]  оно вас тоже импортировалось. Не надо так. Вот так. Полтора на синус. Ничего осмысленного в этом нет,
[17:56.740 --> 18:03.980]  просто возможность нарисовать красивую кривулену, используя уже PyTorch. Все. И так, чтобы вы начали
[18:03.980 --> 18:10.780]  что-то с вами руками. Но собственно, в чем плюс PyTorch? В принципе, когда мы с вами пишем какую-нибудь
[18:10.780 --> 18:22.700]  формулу, ну, например, я там не знаю, х квадрат, логарифм, х в третьей степени на логарифм 15х,
[18:22.700 --> 18:28.020]  не знаю, просто вот так написал. Вот эту штуку продиференцировать все мы с вами можем. Но,
[18:28.020 --> 18:32.260]  благо, дифференцировать все умеют. Но при этом, скажите, вот вам охота руками это считать,
[18:32.260 --> 18:38.580]  особенно если у вас там в вашей модели такие функции 15 штук? Обычно нет. Ну, если кому-то хочется,
[18:38.580 --> 18:56.260]  пожалуйста, я не возражаю. А? Вы намекаете на то, что здесь фон белый или что? Не, не, коллеги,
[18:56.260 --> 19:00.660]  здесь разве что вас будут ругать, если вы будете осенью подписывать. Это как бы, ну, если это
[19:00.660 --> 19:04.820]  осмысленные оси, здесь нет никаких осей, это просто две координаты. Но если у вас условно там
[19:04.820 --> 19:10.700]  зависимость пресижена от реколла, то ось надо подписывать. А так, как бы, ваши графики должны
[19:10.700 --> 19:15.540]  быть читаемые. Причем, ну, как бы не только вам условно можете проверять на соседей. Если соседу
[19:15.540 --> 19:21.380]  читаемый, значит все нормально. Вот. Так вот, короче, дифференцировать это руками лень, банально.
[19:21.380 --> 19:25.820]  Благо, дифференцирование, операция простая, сильно проще интегрирования, поэтому можно научить
[19:25.820 --> 19:30.980]  вот эту всю технику делать это за нас. Поэтому PyTorch имеет в основатической дифференцировании не
[19:30.980 --> 19:37.420]  он один, умеет Jax, умеет TensorFlow, умеет Тиана, которая уже давно почила ее в себя, поглотил
[19:37.420 --> 19:43.340]  TensorFlow по сути, ну и так далее. Поэтому давайте на это посмотрим. На всякий случай я сейчас пару
[19:43.340 --> 19:49.580]  вещей еще расскажу про течение градиентов, потому что это важно. Смотрите, есть вот этот флажок,
[19:49.580 --> 19:56.700]  ну не флажок точнее, а атрибут requiresGrad, требует градиентов. Он ровно отвечает за то,
[19:56.700 --> 20:03.740]  что у тензора есть требование посчитать для него градиенты. На всякий случай в PyTorch и в TensorFlow
[20:03.740 --> 20:09.580]  на самом деле, вот TensorFlow, все вот эти многомерные матрики обзывают тензорами. Никакая тензорная
[20:09.580 --> 20:14.460]  алгебра на них в общем случае не задана, это просто многомерные матрики, но их обзывали тензорами.
[20:14.460 --> 20:19.580]  Хорошо? То есть просто опять же, так же как метрики классификации, к метрикам из линейного алгебра
[20:19.580 --> 20:27.460]  не имеют никакого отношения, но их так обозвали. Так вот, для каждого тензора мы это можем придумать.
[20:27.460 --> 20:36.100]  И собственно, что здесь можно сказать? Давайте я сразу тогда чуть-чуть нарисую, что-нибудь красивое.
[20:36.100 --> 20:40.940]  Вот у вас есть какой-нибудь граф вычислений, что такое граф вычислений? Вы каждую формулу можете
[20:40.940 --> 20:45.540]  на самом деле представить в виде графа. Ну, например, вот у вас есть x, вот у вас есть y,
[20:45.540 --> 20:52.180]  вот у вас есть z. У вас соответственно пусть x и y, операция над ними плюс, потом здесь
[20:52.180 --> 20:59.180]  соответственно операция над ними умножение. Вот здесь у вас соответственно результат. Как
[20:59.180 --> 21:07.860]  можно это записать в виде формулы? Ну, это у нас получается x плюс y умножить на z. Согласны? Мы
[21:07.860 --> 21:11.860]  любую формулу можем представить в виде графа вычислений. Собственно, что значит, что кто-то
[21:11.860 --> 21:16.740]  требует градиента? Значит, что мы попытаемся посчитать для него градиент. Например, вот у нас
[21:16.740 --> 21:23.860]  с вами есть r, и мы с вами уже откуда-нибудь знаем dr. Точнее, как мы с вами хотим посчитать dr по
[21:23.860 --> 21:34.100]  dx. Ну, например, вот мы хотим с вами посчитать. Нам для этого что понадобится посчитать? Нам
[21:34.100 --> 21:39.300]  понадобится вспомнить производную сложные функции и посчитать, собственно, промежуточно. dr по dr
[21:39.300 --> 21:48.020]  понятно. Потом вот это можно обозначить q, и dr по dx. Это что? Это dr по dq на dq по dx. Правильно?
[21:48.020 --> 21:55.060]  Собственно, градиенты у нас посчитаются вот здесь, вот здесь, вот здесь и вот здесь. Согласны?
[21:55.060 --> 22:07.140]  Коллеги, да, это сложение, это умножение. q это просто промежуточное значение, вот здесь я его так
[22:07.140 --> 22:18.660]  обозвал. q, а, хорошо, да, лишнее, простите, вот это давайте я к вам зову. Вот, да, результат сложения,
[22:18.660 --> 22:24.340]  спасибо. Вот, собственно, вот мы посчитали. И, по сути, у нас градиенты потекли отсюда сюда, отсюда
[22:24.340 --> 22:30.060]  сюда, отсюда сюда. Все понятно, правильно? То есть, если мы с вами бы сказали, что y тоже
[22:30.060 --> 22:34.540]  хочет градиенты, мы бы их и сюда посчитали. По z, например, мы не просили градиенты, если мы здесь
[22:34.540 --> 22:40.580]  проверим z град, то он нам скажет, что ничего там нет. То есть, каждый раз, когда у вас есть какая-то
[22:40.580 --> 22:46.020]  формула, у нее есть какие-то вершины этого графа вычислений. По сути, это всегда граф вычислений,
[22:46.020 --> 22:50.140]  по сути, даже это всегда дерево, потому что мы ждем, что у нас одна будет корневая вершина,
[22:50.540 --> 22:55.420]  мы за нее можем подвести. У нас результат, как правило, это значение чего? Функции потерь,
[22:55.420 --> 23:00.900]  мы функции потерь дифференцируем по параметрам, у нас текут градиенты. Вы можете явно указывать,
[23:00.900 --> 23:05.580]  кому нужны градиенты, кому нет. Вот, например, можем с вами это явно сделать, ну, например,
[23:05.580 --> 23:24.460]  там a это torch once 5 и он требует градиенты, а b, соответственно, это torch и он не требует
[23:24.460 --> 23:32.220]  градиенты. По умолчанию никто не требует градиентов. С это a dot b и, соответственно,
[23:32.220 --> 23:40.500]  смотрите, вот c у нас чему-то там равен. Ну ладно, 0 с плюс 2. Вот он чему-то равен. Мы можем
[23:40.500 --> 23:46.220]  теперь попросить backward у него. Backward это как раз-таки дергаем рубильник, посчитать производные
[23:46.220 --> 23:50.980]  для всего, что хочет посчитать для себя градиенты. И после этого, смотрите, вы можете проверить,
[23:50.980 --> 23:58.780]  что у a градиенты, например, появились, вот, это производная c по a. Логично, что производная
[23:58.780 --> 24:02.020]  скаляра по вектору будет иметь ту же размерность, что и вектор. Согласны?
[24:02.020 --> 24:21.060]  Нет, смотрите, проще. У вас есть, как бы, вот у вас есть множество переменных, все они типа
[24:21.060 --> 24:26.660]  тензор. У каждой переменной есть атрибут, хочет ли она градиенты. Соответственно, либо она их
[24:26.660 --> 24:30.620]  хочет, потому что вы это явно задали, либо это промежуточная переменная, и так как она является
[24:30.620 --> 24:36.140]  суммой, короче, эта переменная зависит от той, которая хочет градиенты, она автоматом тоже хочет
[24:36.140 --> 24:41.780]  градиенты. Все. Потом вы от любой, на самом деле, функции, которая возвращает вам скаляр, можете
[24:41.780 --> 24:51.700]  запросить backward, то есть продиференцировать ее по всему, что было до нее. Получит градиенты,
[24:51.700 --> 25:00.980]  и то есть граф будет вот так считаться. А что такое две функции подряд?
[25:00.980 --> 25:14.500]  Нет, погодите, вы имите, а потом вы a умножить на b, это у вас была функция 1, вы посчитали от нее backward,
[25:14.500 --> 25:19.780]  потом a умножить на c, это функция 2, вы от нее тоже дернули backward. Не совсем так, градиент тогда от нее
[25:19.780 --> 25:25.500]  накопится, ну вот давайте сделаем, вот собственно. Да, на самом деле могу объяснить, почему d это a,
[25:25.500 --> 25:37.260]  например dot b на 2. Вот. Да, это вы тоже сейчас, давайте дойдем, пока вот давайте проследим. Смотрите,
[25:37.260 --> 25:42.660]  вот у нас две функции, да? Вот, для начала можем проверить, градиента a нет, градиента b нет.
[25:42.660 --> 25:51.660]  Дернули первую функцию backward, градиенту a появился, градиенту b не появилось. Давайте теперь сделаем d тоже backward.
[25:51.660 --> 26:01.700]  Градиенту a уже стал 6, это как раз-таки кто? Это градиент от первого и от второго сложен. Зачем
[26:01.700 --> 26:07.700]  это надо? На самом деле надо это с точки зрения банального удобства при оптимизации. Предположим,
[26:07.700 --> 26:14.660]  что у нас с вами выборка, там не знаю, из 10 объектов, и мы не можем все 10 объектов одновременно
[26:14.660 --> 26:20.060]  прогнать через нашу машину и сложить от них градиенты, усреднить. Ну просто банальной памяти,
[26:20.060 --> 26:24.500]  например, хватает, у тебя не 10, а 10 миллионов. Тогда мы можем сделать что? Мы можем посчитать
[26:24.500 --> 26:28.460]  градиент от первой пятерки, от второй пятерки, от третьей и так далее. Получается, мы одну и ту
[26:28.460 --> 26:33.220]  же функцию на разных объектах запускаем. Градиент краски у вас тогда просто-напросто накапливается,
[26:33.220 --> 26:38.900]  суммируется, чтобы его потом могли при желании усредить. Или, что проще, вот у вас две функции,
[26:38.900 --> 26:45.180]  например, функция потерь и функция регуляризации. У вас градиенты должны идти от обеих, поэтому они
[26:45.180 --> 26:53.900]  накапливаются, они не перезаписываются. Уловили? Вот как обнулить? Есть путь человеческий, есть
[26:53.900 --> 27:10.540]  нечеловеческий. Сейчас покажу. А чего там про Египиан говорили? У кого? Здесь вы, по сути,
[27:10.540 --> 27:28.380]  тоже можете Египиан и Гессиан посчитать совершенно спокойно. Смотрите, а у вас в данном случае
[27:28.380 --> 27:33.060]  функция всегда это либо линейное преобразование, либо какие-то нелинейные преобразования. У вас и
[27:33.060 --> 27:38.100]  параметры и данные, на самом деле, это просто входные данные. Просто параметры у вас требуют
[27:38.100 --> 27:44.340]  градиенты, а данные не требуют градиентов. Так удобно. Ну и давайте от краски разберемся
[27:44.340 --> 27:57.260]  на практике. Ух ты! Они в очередной раз что-то там убрали. Я понял. Лу от Бостон скоро вырубится.
[27:57.260 --> 28:09.340]  Я понял. Хорошо. Ладно, пока работает. Ребят, мы изучаем этические проблемы в датсенсе сегодня.
[28:09.340 --> 28:15.420]  Короче, давайте построим пока что на пальцах на низком уровне нашу простую модельку,
[28:15.420 --> 28:20.100]  пока даже линейную регрессию. Собственно, вот наши параметры омега и б. Они требуют градиентов,
[28:20.100 --> 28:24.500]  это просто скаляры, но тем не менее скаляры это просто одномерный тензор. Нормально. x и y это,
[28:24.500 --> 28:28.940]  соответственно, наши данные. Видите, это тоже тензоры, абсолютно те же самые, но они градиентов не
[28:28.940 --> 28:36.140]  требуют. Данные это датасет, цена жилья в Бостоне в зависимости там от кучи каких-то параметров. Мы
[28:36.140 --> 28:39.980]  взяли просто последний столбец, по-моему это площадь, что ли, или удаленность от центра,
[28:39.980 --> 28:45.780]  что-то такое. Неважно. Вот у нас специальная картинка нарисована. Вот такую зависимость мы
[28:45.780 --> 28:51.340]  будем пытаться описать. По оси y, соответственно, целевая переменная, по оси x наш признак. Все,
[28:51.340 --> 28:59.500]  вот наши данные. Вот наша модель. Да, пока делаем регрессию, пока все просто. Вот наша модель
[28:59.500 --> 29:05.180]  омега x плюс b. Вот наша функция потерь. Все, видите, явно пишем просто формулу. y пред минус y в
[29:05.180 --> 29:11.700]  квадрате. И среднее потом считаем, средняя ошибка. Все, посчитали, посчитали градиенты. Вот у нас
[29:11.700 --> 29:18.060]  градиенты появились. Для омеги и для b. На всякий случай для x мы никакие градиенты с вами не
[29:18.060 --> 29:24.500]  видим, потому что нет градиентов, он их не просит. Вот. Ну и, соответственно, давайте теперь обучим
[29:24.500 --> 29:28.740]  нашу модельку, каким образом. Градиентным спуском. У нас каждый раз есть градиент, мы можем обновлять
[29:28.740 --> 29:34.340]  параметры по антиградиенту, получать результат. Тут весь код абсолютно тот же самый, просто заново
[29:34.340 --> 29:40.660]  написан. Омег и b реализовали. Нормально. Вот посчитали лоз. Вот наш градиентный шаг, learning
[29:40.660 --> 29:46.940]  rate 005. Минус равно, то есть вычитаем. И вот как мы это за нуляем. Но, опять же, сейчас мы, по сути,
[29:46.940 --> 29:53.020]  с вами на низком уровне, и мы залезли немного в потрохар всех тех функций, поэтому мы обращаемся
[29:53.020 --> 29:58.860]  к методу именно 0 с подчеркиванием нижним, что это на самом деле значит. Все методы с нижним подчеркиванием
[29:58.860 --> 30:03.860]  по и торче на конце, это inplace метода, то есть мы inplace что-то заменяем. То есть в данном случае
[30:03.860 --> 30:08.500]  мы говорим, так, градиенты, конкретно их данные, потому что сам град это сложная структура,
[30:08.500 --> 30:18.860]  за нули. И вот рисуем картинку. Что нечеловеческое? Есть человеческое, мы до него сейчас доберемся.
[30:18.860 --> 30:26.900]  Да. Ну, в смысле, он не то чтобы нечеловеческий, он, если вам надо конкретно написать какую-то сложную
[30:26.900 --> 30:33.300]  функцию и не использовать никакой сахара по и торча, никаких удобств. Вот кривуляна у нас постепенно,
[30:33.300 --> 30:37.860]  видите, подгоняется. Но кривуляна у нас на самом деле пока прямая, потому что мы с вами задли явно
[30:37.860 --> 30:44.260]  линейную зависимость. Вот она. Так что, собственно, вот вам еще маленькая задачка на 30 секунд. Ну,
[30:44.260 --> 30:50.060]  поменяйте каким-нибудь образом вот эту зависимость, чтобы она была не линейная от x и перезапустите.
[30:50.060 --> 30:55.420]  Короче, давайте сделаем модель лучше все-таки, подходящей к данным. Мы явно видим, что зависимость
[30:55.420 --> 31:06.060]  не линейная. Давайте что-нибудь сделаем. Вы делить на x? Ну, давайте поделим. Ну, вот,
[31:06.060 --> 31:19.260]  получилось неплохо, например. Ну, c тогда тоже будет параметром, или что? Ну, тогда нам придется,
[31:19.260 --> 31:29.380]  да, добавить, соответственно, c равняется опять же вот эта штука. Ну, давайте. Я не знаю,
[31:29.420 --> 31:51.260]  что получится, если что. Минус 0,05 на c град data. Ну, давайте попробуем. Ну, со степенями надо
[31:51.260 --> 32:16.140]  быть аккуратнее. Нет, он просто улетел куда-то в зону неопределенности. Вот так предлагаете?
[32:16.140 --> 32:22.660]  Давайте. Вот тоже замечательно подходит. Заметьте, да и неважно, на самом деле, лучше-хуже, да,
[32:22.660 --> 32:27.220]  логарифм здесь вообще идеально подходит. В чем, на самом деле, прикол? Заметьте, мы ничего в коде
[32:27.220 --> 32:32.420]  не меняли, мы просто саму формулу нашу поменяли, и все. Весь подсчет лоса, все градиенты обновления,
[32:32.420 --> 32:37.340]  если у нас новые параметры не появились, все работает так же. У нас просто меняется, по сути,
[32:37.340 --> 32:42.180]  целевая функция, целевое отображение, и все. И в этом, на самом деле, прелесть вот этого автодифа,
[32:42.180 --> 32:46.020]  что мы с вами просто задаем нужное отображение, а все эти функции потери, оптимизаторы и так
[32:46.020 --> 32:50.300]  далее, понятное дело, их уже давно написали, вы можете написать свои, и потом ими из коробки
[32:50.300 --> 33:07.500]  пользоваться. Вот. Понятно? Тут вопрос-комментарий есть? Хорошо. Чего? Смотрите. Нет, погодите,
[33:07.500 --> 33:12.500]  если не поняли. Вот ваша формула, вот ваша линейная модель, вот ваша функция потерь.
[33:12.500 --> 33:17.900]  Все. Вы можете поменять линейную модель на какую-то нелинейную, вам не придется при этом функцию
[33:17.900 --> 33:21.500]  потерь переписывать какие-то аргументы, менять вообще ничего не придется делать. То, что у вас
[33:21.500 --> 33:27.580]  функция потерь, она отдельна. Ваша модель, она отдельна. Вы можете ее поменять, граф вычислений
[33:27.580 --> 33:32.300]  у вас все равно подчиняется всем тем же правилам, работать по той же самой опишке. Вы можете написать
[33:32.300 --> 33:36.380]  любую подходящую модель. Подходящая в смысле, что у нее на входе та же размер, на выходе та же
[33:36.380 --> 33:41.420]  размер. Все будет работать. В этом вся прелесть, что вам не нужно об этом думать теперь. Вам не
[33:41.420 --> 33:50.340]  нужно руками это все писать. Это удобно. Точка дата, потому что когда вы просто говорите... Короче,
[33:50.340 --> 33:56.660]  возможно сейчас уже даже так будет работать. Давайте проверим. Две звездочки где? Это в степень.
[33:56.660 --> 34:05.100]  Две звездочки это в степень. Потому что вот эта штука, она не распознается. Короче, дата это
[34:05.100 --> 34:09.740]  обращение непосредственно к данным, которые в тензоре лежат. Вы с ним можете что-нибудь сделать.
[34:09.740 --> 34:16.500]  То есть конкретно к области памяти, где они записаны. Вот. Возможно сейчас уже даже без даты
[34:16.500 --> 34:25.740]  работает. Я честно сказать не знаю. Раньше вроде не работало. Ну, слушайте, я на пейторче вроде 16
[34:25.740 --> 34:32.780]  года пишу. Так что, если есть какие-то старые старообрядские штуки, прошу прощения, это просто
[34:32.780 --> 34:52.540]  сила привычки. Вы с торочного грата имеете в виду? Чтобы он в грав-почисления не попал?
[34:52.540 --> 34:58.300]  А, ну, кстати, да, возможно. Короче, когда вы лезете вот туда под капот, вы уже выходите за
[34:58.300 --> 35:03.260]  пределы вот этой штуки. Вы просто явно обращаетесь к каким-то данным. Торч про это,
[35:03.260 --> 35:08.940]  грубо говоря, не знает. Он говорит, я посмотрю в сторонку. Ладно, коллеги, у меня теперь к вам на
[35:08.940 --> 35:16.420]  самом деле есть маленький вопрос. Тут он уже ниже написан, но тем не менее. Вот тут стоит какое-то
[35:16.420 --> 35:24.100]  волшебное число, какое-то волшебное число делить на 10. Внимание, вопрос зачем? А почему на 10,
[35:24.100 --> 35:31.460]  почему не на 20? Бинго, пожалуйста, никогда так не делайте, потому что тут специально даже про это
[35:31.460 --> 35:36.940]  написано. Данные поделены на 10, какая-то волшебная константа. Что будет, если не делить на 10 весь тот
[35:36.940 --> 35:42.140]  же самый код, но, соответственно, без деления? Ну, вон она там немного попрыгала, как вы видели,
[35:42.140 --> 35:48.500]  и теперь у нас lost none. Градиентов нет, ничего нет. Можно, на самом деле, посмотреть на график
[35:48.500 --> 35:55.220]  градиентов. Ну, собственно, они у нас сейчас построены просто в естественном масштабе. Давайте
[35:55.220 --> 36:16.460]  посчитаем, NP log делаем. Вот, видите, в логарифмическом масштабе норма градиента растет линейно. То есть
[36:16.460 --> 36:22.060]  градиент у нас рост экспонентально. Что происходило? У нас с вами для линейной модели,
[36:22.060 --> 36:30.900]  какая получается производная ω и х? dL под ω, ну, dy под ω будет равна х, правильно? Если у вас х очень
[36:30.900 --> 36:36.380]  большой, вы получаете большую производную на ω, сильно меняете ω, у вас меняется loss, от этого
[36:36.380 --> 36:41.300]  вы получаете сильную производную от loss, плюс опять же большая производная будет на ω от х, опять
[36:41.300 --> 36:45.820]  сильно меняете и так далее. В итоге у вас просто-напросто расходится ваша оптимизация того,
[36:45.820 --> 36:52.300]  что слишком большая была увеличена градиента. Это лечится либо нормировкой ваших данных,
[36:52.300 --> 36:57.300]  либо нормировкой градиентного шага, либо что хорошо и тем и другим. Но так как градиентный
[36:57.300 --> 37:02.060]  шаг пока у нас какая-то константа, это еще один пример, почему полезно нормировать ваши данные.
[37:02.060 --> 37:07.060]  Не только потому, что это позволяет вам КНН правильно использовать и регуляризация вам в
[37:07.060 --> 37:11.100]  жизни портит, у вас еще и получается все нормально с градиентной оптимизацией.
[37:11.100 --> 37:21.740]  Вот, откуда брать нормировку? Давайте отнормируемся, собственно, давайте скажем,
[37:21.740 --> 37:27.900]  что у нас торч. Фу, х это что? Давайте просто-напросто сделаем классическую стандартизацию. Что это такое будет?
[37:27.900 --> 37:43.500]  x равно x минус x мин, xs равно 1, xs равно 0, точнее. А, Дим, да, вы правы, мы уже в пайторче.
[37:43.500 --> 37:57.260]  Делить на xstd, Дим равно 0. Это стандартное отклонение, корень из гисперсии. Вот, пожалуйста,
[37:57.260 --> 38:02.260]  теперь у вас ничего не упадает, потому что вы теперь все загнали примерно к нулю. Короче,
[38:02.260 --> 38:06.580]  у вас нулевое среднее стандартное отклонение равно днице гарантирует вам гораздо более
[38:06.580 --> 38:14.980]  устойчивую работу почти везде. Мы данные отнормировали, но у нас градиенты напрямую приходят из данных,
[38:14.980 --> 38:21.100]  потому что производная по z будет xмсу. Вот, поэтому частная производная у вас стала меньше по абсолютному
[38:21.100 --> 38:29.500]  значению, и вы можете красить. Нормировки данных, ну, есть функции для нормировки данных, но вызывать
[38:29.500 --> 38:37.780]  вам ее все равно придется. Нет, ну, есть условно, есть условно, это там, ну, в скалерне есть, вон,
[38:37.780 --> 38:47.020]  например, пром скалерн, припроцессинг, импорт, стандарт скейлер. Вот, пожалуйста, можете его
[38:47.020 --> 38:52.260]  дергать, и, соответственно, у вас будет краски средней дисперсии, на средней дисперсии нормироваться.
[38:52.260 --> 38:57.740]  Но главное не забывайте, пожалуйста, про нормировку. Очень часто из-за этого возникают проблемы,
[38:57.740 --> 39:03.140]  которые приходится потом краске, если про это не вспомнить, чинить подгоном learning rate, каким-то
[39:03.140 --> 39:07.860]  подбором стартовых весов и так далее. Это все боль, данные отнормировали, стало сильно лучше.
[39:07.860 --> 39:18.060]  Стандартизация – это минус средний поделить на дисперсию, но это стандартный способ нормировки.
[39:18.060 --> 39:22.900]  Есть еще мин-макс нормировка, например, опять же, вы вычитаете среднее, но делите на разницу между
[39:22.900 --> 39:36.700]  минимум и максимум. Есть еще много чего. Короче, вот. А давайте подумаем, почему с логарифмом беда?
[39:36.700 --> 39:45.940]  Не, потому что у вас логарифм от отрицательной величины – это что? У вас логарифм считается
[39:45.940 --> 39:51.980]  от чего-то с нулевым средним, поэтому как минимум половина х будет отрицательная, поэтому беда.
[39:51.980 --> 39:58.540]  Ну да, тогда у вас х должен быть строго положительным, чтобы вологрифмировать,
[39:58.540 --> 40:06.900]  иначе у вас краски выпадет надо. Короче, вот так. Окей, красивые штуки. Короче, красивый кадр,
[40:06.900 --> 40:14.380]  на самом деле, из фильма, по-моему. Опять забыл. «Семи самураев». Спасибо. Классный фильм, 51-го года.
[40:14.380 --> 40:20.500]  Рекомендую. Вот. На самом деле, здесь неспроста эта гибка, собственно. Будьте аккуратны со всеми
[40:20.500 --> 40:25.380]  этими тензорами, потому что даже на простых моделях можно себе замечательно выстрелить в ногу,
[40:25.380 --> 40:29.380]  например, не отнормировав данные. Тем паче, когда у вас все те же самые, по сути,
[40:29.380 --> 40:34.380]  преобразования используются с нейронной сетью на 50 слоев вот такой вот, там проблем будет кратно
[40:34.380 --> 40:39.580]  больше. Поэтому начинаем с простого, идем к сложному. Ну а теперь давайте всем те же самые
[40:39.580 --> 40:45.260]  позанимаемся, но используя что-то верхнеуровневое. На всякий случай, если у вас нотмнист не
[40:45.260 --> 40:51.260]  импортируется, значит у вас краски тот код в гетом на коллабе не сработал, потому что там
[40:51.260 --> 40:56.700]  ссылка неправильная. Хорошо? Да, на всякий случай, коллеги, когда вы видите где-то импорт чего-то,
[40:56.700 --> 41:02.540]  вы, пожалуйста, проверьте еще, что не должно быть в корне такого файла, потому что первая реакция
[41:02.540 --> 41:07.780]  у многих людей, когда видит, что импорт не проходит, PIP инсталт что-то там. В данном случае у нас
[41:07.780 --> 41:11.860]  локальный файл, который мы должны импортировать. Нам не нужен PIP инсталт нотмнист, я не знаю,
[41:11.860 --> 41:27.020]  что еще там в PIP по индексу нотмнист лежит. Вот. Не, вот смотрите, первая строчка, вот здесь была
[41:27.020 --> 41:32.320]  опечатка, тут MLCORS было написано через нижнее подчеркивание, а не через DEFICE. Поправьте,
[41:32.320 --> 41:37.300]  пожалуйста, тогда все и запустите, соответственно, ее раскомментируйте и запустите, тогда заработаю.
[41:37.300 --> 41:44.220]  Вот. Короче, что такое нотмнист? Есть такой классный датсет MNIST, я не помню,
[41:44.220 --> 41:50.260]  как это расшифровывается. Короче, это датсет из 9-10 цифр, от 0 до 9 рукописных. Это классический
[41:50.260 --> 41:56.140]  датсет, собранный, по-моему, Яном Лекуном, но мою хуйну врать. Кто-нибудь помнит? Ладно,
[41:56.140 --> 42:01.500]  предположим. Предположим, на один из просто основных датсетов, ранее используемых для
[42:01.500 --> 42:07.060]  классификации, 10 рукописных цифр, там всего 60 тысяч примеров, по-моему, по современным меркам,
[42:07.060 --> 42:13.780]  это все малюсенькое. Но, тем не менее, это хеллоуолд в мире работы с сетками, обработки
[42:13.780 --> 42:19.620]  изображений и так далее. Соответственно, нотмнист, это, как можно догадаться, не мнист, но, короче,
[42:19.620 --> 42:24.700]  это датсет, который содержит в себе уже 10 букв, причем в различных стилях написания,
[42:24.700 --> 42:29.500]  различных шрифтах на латинице от а до, соответственно, десятой букв алфавита,
[42:29.500 --> 42:34.620]  я не помню. И наш задачи точно так же их опознавать. Таких датсетов, на самом деле, достаточно много.
[42:34.620 --> 42:40.420]  Есть fashion minutes, где предметы одежды и пикселизованные, предлагаются там всякие туфли, футболки,
[42:40.420 --> 42:48.580]  кофты и так далее. Короче, наша задача научиться отличать вот а от б, а в прыжке 10 классов,
[42:48.580 --> 42:54.340]  все 10 букв друг от друга. Пока что мы с вами не умеем работать с изображениями по-нормальному,
[42:54.340 --> 42:59.700]  но вспомним первое занятие изображения, то есть матрица черно-белая, может быть,
[42:59.700 --> 43:05.980]  вытянуто в вектор, согласны? Я согласен. Поэтому пока что мы с вами делаем абсолютно по-дурацки,
[43:05.980 --> 43:10.660]  берем матрицу, вытягиваем вектор, работаем далее как с вектором. Понятное дело, что ничего хорошего
[43:10.660 --> 43:16.060]  в этом нет, сдвиг на 1 пиксель уже сильно меняет все, что происходит, но, словно, если мы вот эту
[43:16.060 --> 43:20.700]  букву а на 1 пиксель вправо подвинем, у нас сильно поменяется векторное представление, при этом
[43:20.700 --> 43:25.020]  букву а мы все еще будем видеть. Именно поэтому сверочные сети и трансформеры хорошо подходят
[43:25.020 --> 43:30.980]  к обработке изображений, но об этом будет дальше, пока мы до тут не дошли. Ну а теперь, собственно,
[43:30.980 --> 43:37.060]  отвечая на вопрос, как делают, скажем так, по привычке. Используют верхние уровни WiPyTorch. Там
[43:37.060 --> 43:42.860]  есть, во-первых, модуль, который называется NN-модуль, от которого наследуются примерно все
[43:42.860 --> 43:48.620]  преобразования, которые есть в PyTorch. NN-модуль обладает классными свойствами. Почему? Потому
[43:48.620 --> 43:54.220]  что NN-модуль написан, у него классный абстрактный класс, но он, по сути, NN-модуль это является
[43:54.220 --> 44:03.180]  абстрактный класс для всех модулей. На всякий случай. Все помните это такое, да? Да, вот. Что
[44:03.180 --> 44:08.620]  такое абстрактный класс, понятно? То есть все модули, которые отвечают за любые там линейные
[44:08.620 --> 44:13.900]  преобразования, свертки и так далее, они наследуются от NN-модуля и, по сути, многие методы уже определены
[44:13.900 --> 44:18.460]  в NN-модулях. В чем на самом деле плюс? Во-первых, всегда вот, читайте доклад, она даже специально здесь
[44:18.460 --> 44:22.860]  скрыта. Во-вторых, если вы хотите свой модуль написать, допустим, вот у вас какая-то новая
[44:22.860 --> 44:27.260]  Uber свертка или какой-нибудь там новый Fast Fourier Transform, неважно, что-нибудь вы придумали,
[44:27.260 --> 44:31.500]  что вам надо дифференцировать и так далее. Автоматически. И чтобы оно еще в PyTorch
[44:31.500 --> 44:38.100]  строилось быстро. Вы можете взять, отнаследоваться от NN-модуля и реализовать всего два метода внутри
[44:38.100 --> 44:42.380]  вашего класса. Первое это конструктор, он должен быть, чтобы вы задали, что там нужно внутри. И
[44:42.380 --> 44:49.420]  второе это метод forward, то есть ровно как он должен считаться. Прямой проход. Все остальное, если у вас
[44:49.420 --> 44:54.380]  внутри не используется недеференцируемая операция и не используется что-то не из PyTorch,
[44:54.380 --> 44:59.380]  например, вы не засудили numpy посреди кода из PyTorch, тогда все остальное PyTorch делает за вас,
[44:59.380 --> 45:03.580]  и вы точно также сможете вызывать backward от своего модуля и так далее. Вам ничего не
[45:03.580 --> 45:08.380]  надо писать руками с градиентами. Более того, вы можете его примотать изолентой к коду, который
[45:08.380 --> 45:16.060]  уже был в библиотеке, и все будет работать. Нет, погодите, модуль это название вот этого абстрактного
[45:16.060 --> 45:26.540]  класса. То есть любой модуль, смотрите, вот, например, если вы хотите какую-то функцию реализовать,
[45:26.540 --> 45:31.740]  в смысле, в смысле математическом, не в смысле программирования, то вы можете сделать класс,
[45:31.740 --> 45:37.140]  который односленного от модуля, который исполняет ровно то, что вам надо, и вы можете только написать
[45:37.140 --> 45:41.140]  там конструктор, и как она считается на прямом проходе, то есть не градиенты ее считать,
[45:41.140 --> 45:48.020]  а только как из входа получить ответ. Тогда градиенты выхода по входу она вам сама будет
[45:48.020 --> 45:57.260]  считать. Это удобно. backward это ровно вот вы от любого скаляра можете дернуть рубильник,
[45:57.260 --> 46:02.340]  и он тогда пойдет по графу вычислений по всем путям до листьев, которые требуют градиенты.
[46:02.340 --> 46:18.740]  По умолчанию backward работает только у скалярных, у функций, у которых скалярный output. То есть выход
[46:18.740 --> 46:22.860]  функции должен являться скаляром, тогда backward работает. Потому что иначе непонятно,
[46:22.860 --> 46:27.260]  что с производной делать. Делать какой размерности, усреднять или что. То есть вы
[46:27.260 --> 46:31.460]  можете на самом деле попросить его вежливо, что вам нужно производную вектору по матрице,
[46:31.620 --> 46:35.620]  чтобы получить трехмерный тензор. Но по умолчанию вам за это по рукам дадут,
[46:35.620 --> 46:44.380]  и не дадут это сделать. Пайторыч просто вам ошибку выдаст. У вас в код пятимерный, а выход
[46:44.380 --> 46:51.260]  скаляр. Вот смотрите, вот вам простой пример. У нас все еще сохранены вот наши a тензор,
[46:51.260 --> 47:02.020]  вот b тензор. Вот давайте a плюс b, c равно a плюс b, c backward. Вот, пожалуйста, он нам говорит,
[47:02.020 --> 47:08.420]  что можно неявно создавать граф вычислений вот этот вот только для скалярных output. Если вы
[47:08.420 --> 47:12.940]  хотите для векторного output это делать, вам нужно явно уже задавать, куда какие производные пойдут,
[47:12.940 --> 47:19.380]  как они будут насыряться и так далее. Ну короче, скажем так, вам это очень долго не понадобится в
[47:19.380 --> 47:23.900]  тот момент, когда вам все-таки понадобится все производные вектора по вектору в пайторче. Именно
[47:23.900 --> 47:27.900]  явно я имею в виду. Вы уже будете достаточно опытны и с тем, чтобы это написать без проблем.
[47:27.900 --> 47:38.180]  Не-не-не, погодите. Вы, смотрите, у вас модуль может совершенно спокойно выдавать
[47:38.180 --> 47:47.900]  любую величину. Двумерную, трехмерную, неважно. Вопрос лишь в том, что у вас в конечном итоге,
[47:47.900 --> 47:55.100]  вот, производная идет dr по dx, r скаляр. Вы не можете посчитать отдельно dq по dx,
[47:55.100 --> 48:00.140]  потому что тогда у вас производная будет краски многомерной для всех величин. Просто у вас
[48:00.140 --> 48:04.580]  получается всегда при дифференцировании у вас производная, допустим, этого поэтому на это
[48:04.580 --> 48:11.860]  поэтому, если два оператора в другую руку применить, будет того же размера, что и x. То есть у вас
[48:11.860 --> 48:16.500]  вызывается backward всегда скаляра. Промежуточная может быть что угодно, вам абсолютно нормально.
[48:16.500 --> 48:32.860]  Что? Так, ну вот, смотрите, вот у вас, например, вот, кстати, да, я же говорил, любое отображение,
[48:32.860 --> 48:37.340]  то есть вот целая модель, например, написано, смотрите, там используются свертки, но свертка,
[48:37.340 --> 48:43.260]  короче, предположим, какое-то линейное преобразование в каком-то смысле, плюс нелинейное. Вот у вас есть
[48:43.260 --> 48:47.460]  модель, вы сказали, какая она, сказали в каком порядке ее применять, потом она у вас предсказывает
[48:47.460 --> 48:53.860]  какое-то число или, допустим, метку класса, вероятность положительного класса, и вы потом от нее еще
[48:53.860 --> 48:59.060]  считаете, допустим, log loss. Потом вы у log loss дергаете рубильник, посчитай backward, он идет сюда и
[48:59.060 --> 49:04.500]  считает производную для всех вот этих вот матричек за линейное отображение отвечающие. Понятно, нет?
[49:04.500 --> 49:11.580]  Так, давай свертку забудьте, вообще она будет у нас на десятом занятиях. Она просто там в доках
[49:11.580 --> 49:16.700]  напитана, поэтому я про нее сказал. Не хватало еще сейчас вас допутать. Так, ладно, я думаю,
[49:16.700 --> 49:22.860]  пример на практике всегда лучше, мы сейчас это с вами сами напишем, будет понятно. Смотрите,
[49:22.860 --> 49:26.300]  собственно, вот наша задача построить ту самую логистическую регрессию, которую мы с вами
[49:26.300 --> 49:32.900]  сегодня разбирали. Вот она, вот наша сегмойда. Собственно, что мы можем сделать? Можем с вами
[49:32.900 --> 49:37.700]  написать через nn-модуль, можем написать по-простому. Давайте сначала напишем по-простому,
[49:37.700 --> 49:43.740]  потом через nn-модуль. Собственно, что такое логистическая регрессия? Это линейное отображение
[49:43.740 --> 49:51.260]  поверх него сегмойда. Согласны? Вот, тогда давайте, во-первых, добавим модуль nn-linear,
[49:51.260 --> 50:09.420]  вот на всякий случай, можно его посмотреть. Ладно, я забыл, как это делается. А, dir, спасибо.
[50:09.420 --> 50:28.980]  А, не, это его сигнатура, это все, что у него есть. Вот, все. У него есть метод, вот этот приватный
[50:28.980 --> 50:42.740]  док. Вот, классно, спасибо. Короче, вот, пожалуйста, есть линейные краски метод nn-linear,
[50:42.740 --> 50:48.540]  который по факту является чем? Он ровно отвечает за умножение на матричку. То есть,
[50:48.540 --> 50:52.940]  у вас на входе, допустим, размеры на стен, на выходе к, он внутри явно порождает матричку
[50:52.940 --> 51:01.420]  m на k и свободный член размера k. Все. Ну, давайте его краской построим. Вот наша
[51:01.420 --> 51:09.860]  своя модель, она отображает 784. Почему 784? Потому что у нас 784 пикселя, потому что это 28 на 28,
[51:09.860 --> 51:17.780]  это размерность картинки. Вот. В 1, 1 это что? Это одно число, это будет наша краски вероятность.
[51:17.780 --> 51:22.740]  Пока что это просто какая-то скалярная штука, ее обычно называют логитом. То есть,
[51:22.740 --> 51:26.660]  вероятность до того, как она попала под сигмоиду или, в общем случае, под софтмакс,
[51:26.660 --> 51:43.300]  это в многомерном случае, называют логитом. Хорошо? Логит. Лоджит, логит. Вот. И, соответственно,
[51:43.300 --> 51:47.620]  в конце мы с вами ставим сигмоиду. Вот. Та же самая сигмоида, она называется NN-сигмоид. Ну,
[51:47.620 --> 51:54.460]  понятно. Все. Можем посмотреть на параметры нашей модели. Вот в ней матричка есть 11984. Ну,
[51:54.460 --> 52:00.580]  логично. И один свободный член. Все. Ну, давайте проверим, что наша модель работает. Просто на
[52:00.580 --> 52:08.180]  практике. Возьмем первые три объекта х. Что? Не-не-не. Погодите, у сигмоида есть разные параметры?
[52:08.180 --> 52:14.460]  Ну, а линейное отображение как, в общем случае, сдается?
[52:14.460 --> 52:26.420]  Омега х плюс б. Если я здесь сделаю, например, двойку, то здесь будет два. Видите? Вот. Ну,
[52:26.420 --> 52:30.300]  соответственно, вот, запускаем нашу модель. Просто на игрушечном кейсе проверить,
[52:30.300 --> 52:35.340]  что все размены совпали. Запустили. Вот у нас три предсказания. Собственно, вероятность для
[52:35.340 --> 52:48.820]  первого объекта, второго и третьего. Согласны? Чего? Нет. Погодите, у вас три объекта, для каждого
[52:48.820 --> 52:55.420]  из них предсказанная вероятность. Они независимые никак. Вот. Ну и, собственно, вот у вас теперь есть
[52:55.420 --> 53:00.700]  модель. Осталось понять, как нам теперь из предсказаний получить функцию ошибки, потому что мы именно
[53:00.700 --> 53:06.660]  функцию ошибки будем минимизировать. Ну и скажем так, тут есть два путя. Я вам честно скажу,
[53:06.660 --> 53:12.660]  я вас честно прошу, вам охота сейчас вот эту функцию потерь расписывать? Пайторче. Мне тоже
[53:12.660 --> 53:38.380]  неохота, поэтому я предлагаю сделать так. NN binary. Нет. Нет ее там что ли? Вот binary cross entropy.
[53:38.380 --> 53:49.460]  Поздравляю. Ну вот, да, это binary cross entropy. Я говорю, в пайторче за нас уже вот это все написали.
[53:49.460 --> 54:01.860]  Ну мне сейчас тоже не очень охота, на самом деле, писать вот эту страшную формулу. Что? Ну да,
[54:01.860 --> 54:07.220]  можно. Смотрите, окей, можем написать, просто сейчас я поставлю обучаться, могу руками написать,
[54:07.220 --> 54:18.540]  увидеть, что то же самое. А, что там написано? Торч. Обычно вот здесь это все можно делать.
[54:26.900 --> 54:34.820]  Дока у пайторча очень приятная, поэтому вот. А, бце лос, она вот называется. Вот она, как
[54:34.820 --> 54:42.580]  считается? Пожалуйста, вот она же, самая написанная. Вот binary cross entropy. Соответственно,
[54:42.580 --> 54:56.820]  от кого у нас? Чего? Не, нам здесь просто нужно посчитать. Не, не, лос надо вызвать. Смотрите,
[54:56.820 --> 55:02.660]  на всякий случай вот с cross entropy очень опасная вещь, которая часто происходит, потому что люди
[55:02.660 --> 55:08.100]  привыкают работать со средней квадратичной ошибкой с МСЕ. МСЕ, как вы понимаете, симметричных,
[55:08.100 --> 55:12.260]  потому что это парабола. Вы можете из первого члена второго вычислить, из второго первого,
[55:12.260 --> 55:19.620]  разницы особо нет. Властно? Иначе не будет одинаковое. С cross entropy такого не работает,
[55:19.620 --> 55:23.860]  у вас там p log q, поэтому, пожалуйста, читайте доку, что подавать сначала, что подавать потом.
[55:23.860 --> 55:36.500]  Y predicted и Y. Вот и, по идее, лос у нас должен быть. Вот так cross entropy проверять не будем,
[55:36.500 --> 55:47.740]  а лос-то можно и попроверять. Вот, даже ассерты.
[55:47.740 --> 56:01.700]  Ладно. Ассерты были написаны, чтобы руками читать. Вот наш лос и, по сути, теперь вы можете его
[56:01.700 --> 56:06.700]  точно также дифференцировать и так далее. А теперь, говоря про то, как считают градиенты
[56:06.700 --> 56:12.780]  по-нормальному, а не руками. Смотрите, в торче есть пакет Optim, который, собственно, отвечает за
[56:12.780 --> 56:18.980]  оптимизацию. В чем плюс? Вы можете взять оттуда, ну, RMSProp мы с вами пока не знаем, давайте SGD возьмем,
[56:18.980 --> 56:25.180]  стокастический градиентный спуск. Вы можете совершенно спокойно взять оттуда SGD и прилепить
[56:25.180 --> 56:29.180]  его к модели, которая у вас есть. То есть вы что говорите явно? Вы говорите, так, уважаемый SGD,
[56:29.180 --> 56:35.860]  возьми параметры модели, которые получаются за модели вызовом метода parameters, и прицепись ко всем
[56:35.860 --> 56:39.860]  ним. Теперь ты отвечаешь за обновление всех этих параметров. Все параметры, которые хотят градиенты,
[56:39.860 --> 56:44.740]  будут им обновляться. Потом вы делаете loss backward, у вас появились все градиенты,
[56:44.740 --> 56:51.100]  а потом вы делаете optstep. Optstep вам разом обновляет все параметры, которые хотели градиенты,
[56:51.100 --> 56:55.460]  которые их получили. И после этого вы можете его так же явно попросить занудить градиенты,
[56:55.460 --> 57:07.620]  чтобы больше он их не использовал. Все, понятно? Так, ну и теперь это все можно написать. Ладно,
[57:07.620 --> 57:21.140]  я так уж и быть напишу. LR learning rate, но это тот самый гипер параметр. Вот помните,
[57:21.140 --> 57:35.940]  мы с вами писали, что там ω равно ω old минус 0.05 на dl dω. Вот это как раз learning rate. То есть
[57:35.940 --> 57:42.500]  это гипер параметр, насколько большой у вас должен быть шаг градиент. Так, давайте я за 30
[57:42.500 --> 57:48.820]  секунд напишу, чтобы оно у вас работало, и мы на этом закончим. Так, y predicted, это что у нас там?
[57:48.820 --> 57:51.940]  Model x batch.
[57:58.300 --> 58:10.660]  Так, проверили. Loss, это соответственно nn functional binary cross entropy, y predicted batch.
[58:10.660 --> 58:25.420]  На самом деле, вот эта тройка, loss backward, opt step, opt 0 grad, у вас будет где-то уже на подкорке,
[58:25.420 --> 58:32.980]  я думаю, сидеть, потому что она постоянно везде используется. Вот, ну и все. Пожалуйста,
[58:32.980 --> 58:36.580]  он пошел обучаться. Вот у нас loss падает, красивых график тут пока не строился. Ну вот,
[58:36.580 --> 58:44.420]  loss 0.75, 0.40, 0.23. Да, 0.1 соответственно дошло. Ну и можем посмотреть, что там наша модель
[58:44.420 --> 58:58.900]  на предсказывала. Model y test, 0. Так, у него тип данных что ли тут? Да. Окей, torch
[58:58.900 --> 59:13.260]  from numpy. Что-то зануда такой. Ой, xtest.
[59:15.420 --> 59:22.860]  Ну вот, бинарная классификация у нас точностью 97 процентов работает. То есть a от b наша моделька
[59:22.860 --> 59:27.860]  с вами отличать умеет даже просто на уровне пикселей. В принципе, все вот это то, что мы с
[59:27.860 --> 59:33.620]  вами написали, можно переписать для работы с многоклассной классификацией буквально тремя
[59:33.620 --> 59:37.340]  шагами. Я сейчас не буду это запускать, чтобы вас уже отпустить, я вижу, многие уже хотят пойти,
[59:37.340 --> 59:43.220]  бежать домой. Это нормально. Но просто для примера смотрите, собственно, что нам надо в модели поменять?
[59:43.220 --> 59:48.980]  Ужас. Во-первых, нам надо, чтобы допустим, это была трехклассовая классификация, у нас должно
[59:48.980 --> 59:56.500]  соответственно не один логит предсказываться, а три. Отображение в три. Все, модель у нас теперь
[59:56.500 --> 01:00:03.660]  такая и соответственно здесь нам надо будет предсказывать, точнее вот этот ассерт убираем.
[01:00:03.660 --> 01:00:09.980]  Соответственно, функция потери у нас теперь будет, где она не бинарная кросснотропия, а просто
[01:00:09.980 --> 01:00:29.740]  кросснотропия многомерная. На самом деле, кажется, все. Ладно, где-то видим. А, зачем вы имейте
[01:00:29.740 --> 01:00:40.860]  в виду там ноль? А, так я скину вам решенную ноутбук. Средством был 2.0, потому что у нас
[01:00:40.860 --> 01:00:48.340]  просто-напросто одна вероятность, мы отбрасывали эффективную размерность. Вот. Так, ну по идее это
[01:00:48.340 --> 01:00:55.860]  должно работать, я не очень понимаю, на что он тут конкретно ругается. Нигде. Игры предикта,
[01:00:55.860 --> 01:01:11.060]  кросснотропия. Target.tensor. Batch. А, погодите, у нас вся, все, я понял. У нас вся выборка пока что
[01:01:11.060 --> 01:01:27.980]  только для бинарной задачи. ABC. Вот. Дрын, дрын, дрын, дрын, дрын, дрын. Вот. Теперь, по идее,
[01:01:27.980 --> 01:01:49.780]  оно должно. Да ты зануда. Ой. А, Y. Batch. Torch. Long. Во, пожалуйста, все. У нас просто выборка была на
[01:01:49.780 --> 01:01:55.260]  два класса, поэтому он ругался, что ничего не работает. Вот, пожалуйста, поменял выборку
[01:01:55.260 --> 01:02:00.900]  теперь на трехклассовую. Мысли изначально. Вот. Поменялось три из турочки кода. Собственно,
[01:02:00.900 --> 01:02:07.820]  вот здесь он хочет индексы теперь классов в виде Torch.Long. Во-вторых, поменялась функция потерь,
[01:02:07.820 --> 01:02:11.980]  потому что для бинарной задачи у нас бинарная кросснотропия, для мультиклассовой мультиклассовая.
[01:02:11.980 --> 01:02:16.900]  И поменялась сама наша модель, теперь она соответственно отображает три класса. Все остальное
[01:02:16.900 --> 01:02:22.260]  сохранилось. Та самая линейная модель, вот которую мы с вами говорили, One versus Rest. Вот у нас,
[01:02:22.260 --> 01:02:27.700]  поэтому есть три выхода на каждый из трехклассов. У нас по одной, по сути, линейной гиперплоскости,
[01:02:27.700 --> 01:02:31.420]  которая строит для одного отделяющую гиперплоскость, для второго и для третьего,
[01:02:31.420 --> 01:02:40.500]  и строит для них ответ. А, ладно, хорошо. Вот еще что надо убрать. Вот. Теперь лучше. Ну,
[01:02:40.500 --> 01:02:46.940]  сегмой, да в бинарном случае применимо, в мультиклассовом неприменимо, это логично. Вот. Ну,
[01:02:46.940 --> 01:02:51.540]  что ж, коллеги, а на этом на самом деле все. Мы с вами будем гораздо глубже погружаться. Добро
[01:02:51.540 --> 01:02:55.620]  пожаловать в мир, на самом деле, глубокого обучения. Это уже непосредственно притеча всех
[01:02:55.620 --> 01:03:00.380]  этих сеточек. Добро пожаловать в линейную классикацию. Ну, и теперь у вас гораздо больше
[01:03:00.380 --> 01:03:05.100]  машинерия. А первую домашку уже можно сдавать, права на вторую тоже сейчас появятся. Все,

[00:00.000 --> 00:14.000]  Приветствую всех, кто нас слушает сегодня в Zoom. Давайте вспомним, как на второй лекции, в начале семестра,
[00:14.000 --> 00:24.000]  перед тем, как говорить про задачу репликации кивалиохранилища, мы нарисовали условную архитектуру распределенной базы данных.
[00:31.000 --> 00:37.000]  Мы сказали, что в самом основании этой архитектуры лежит локальное хранилище.
[00:42.000 --> 00:49.000]  И эту тему мы уже освоили достаточно подробно. Мы говорили про LSM, ну и сегодня будем говорить про crash consistency,
[00:49.000 --> 00:55.000]  как на этом уровне переживать отказы диска и какие-то нюансы работы файловых систем.
[00:55.000 --> 01:00.000]  Поверх локального хранилища мы расположили уровень репликации.
[01:02.000 --> 01:14.000]  Как, имея, скажем, levelDB или roxDB, в общем LSM, на отдельных машинах реплицировать его так, чтобы пользователи работали с группой машин,
[01:14.000 --> 01:17.000]  с группы реплик, как с одной отказоустойчивой машиной.
[01:18.000 --> 01:27.000]  На следующем уровне мы научились справляться с тем, что данные наши в одну машину не помещаются.
[01:27.000 --> 01:42.000]  Для этого мы их шардировали и разбирались, как именно можно при шардировании избежать узких мест, как их можно преодолеть.
[01:42.000 --> 01:50.000]  У нас была лекция про exabyte scale файловые системы и key value, и там мы масштабировали преимущественно хранилищами данных.
[01:51.000 --> 01:57.000]  Вот эти три уровня нами уже освоили, то есть мы можем построить бесконечно большое, горизонтально масштабируемое,
[01:57.000 --> 02:00.000]  линьоризуемое, отказоустойчивое key value хранилище.
[02:00.000 --> 02:07.000]  И настал момент подняться на уровень выше, поговорить про транзакции.
[02:07.000 --> 02:15.000]  Можно подумать, что транзакции это в первую очередь разговоры про базу данных, про таблицы,
[02:15.000 --> 02:19.000]  про операции, которые трогают много строчек, про гарантии ASET.
[02:19.000 --> 02:21.000]  Наверное, вы про них слышали.
[02:25.000 --> 02:28.000]  Наверное, мы интуитивно понимаем, о чем идет речь.
[02:29.000 --> 02:33.000]  Но сегодня наше понимание транзакций будет гораздо шире.
[02:33.000 --> 02:38.000]  И для того, чтобы мы это увидели, я хочу, чтобы мы вспомнили несколько примеров,
[02:38.000 --> 02:41.000]  когда необходимость в транзакциях, в наших задачах уже возникала.
[02:43.000 --> 02:47.000]  Давайте для начала вспомним систему, которая называлась GFS, Google File System.
[02:51.000 --> 02:57.000]  В Google File System не было никаких таблиц, никаких строчек, транзакций тоже, разумеется, не было.
[02:57.000 --> 03:03.000]  Были файлы, и была операция, которая позволяла вам фрагмент файла перезаписать.
[03:05.000 --> 03:08.000]  Вот у вас есть какой-то большой файл,
[03:10.000 --> 03:14.000]  и, допустим, вы хотите, давайте это разными цветами нарисуем,
[03:16.000 --> 03:18.000]  перезаписать какой-то его фрагмент.
[03:28.000 --> 03:33.000]  Этот фрагмент может быть очень небольшим, не знаю, какие-нибудь мегабайты.
[03:34.000 --> 03:36.000]  И у вас есть другая запись,
[03:40.000 --> 03:48.000]  которая пишет в этот же самый диапазон файла, тот же самый мегабайт перезаписывает его на байты B.
[03:48.000 --> 03:51.000]  Это условно, какие-то байты A, какие-то байты B.
[03:51.000 --> 03:54.000]  И вот эти две записи конкурируют между собой
[03:55.000 --> 03:59.000]  и пытаются перезаписать один и тот же диапазон файла.
[04:03.000 --> 04:06.000]  Давайте вспомним, как GFS обслуживало такие перезаписи.
[04:08.000 --> 04:11.000]  Перезаписи обслуживались на уровне чанков.
[04:13.000 --> 04:19.000]  Для каждого чанка GFS среди набора его реплик выбирала реплику primary.
[04:19.000 --> 04:25.000]  Эта реплика отвечала за то, чтобы принимать конфликтующие записи в чанк и упорядочивать их.
[04:26.000 --> 04:28.000]  Ну и, собственно, выполнять.
[04:30.000 --> 04:36.000]  С одной стороны, вроде бы, проблему конкуренции этих двух записей это primary решает.
[04:36.000 --> 04:43.000]  Но наша беда сегодня в том, что вот эта маленькая запись, этот маленький диапазон файла размером мегабайт,
[04:43.000 --> 04:45.000]  угодил на границу двух чанков.
[04:45.000 --> 04:51.000]  И вот этот чанк слева обслуживает...
[04:54.000 --> 04:56.000]  У нас какие-то проблемы со звуком.
[04:57.000 --> 04:59.000]  Пожалуйста, выключите микрофон, если он вас включит.
[05:01.000 --> 05:07.000]  И запись в левой чанк обслуживает какой-то узел primary 1,
[05:07.000 --> 05:15.000]  а чанк справа обслуживает другой узел primary 2.
[05:28.000 --> 05:30.000]  Нам сейчас не важно отказа устойчивости.
[05:30.000 --> 05:34.000]  Я напомню, что в GFS была фундаментальная проблема с тем,
[05:34.000 --> 05:41.000]  что если primary умирал и назначался новый primary, то согласованности при переходе не было.
[05:42.000 --> 05:47.000]  То есть в GFS был реализован такой коленочный, не совсем корректный протокол atomic broadcast.
[05:48.000 --> 05:52.000]  Но нам сейчас это не очень важно, нам важны не отказы, пусть даже никто не отказывает.
[05:53.000 --> 06:00.000]  Проблема в том, что каждая из записей, и запись A, и запись B, раз они попадают на границу двух чанков,
[06:00.000 --> 06:03.000]  эти две записи обслуживаются как две.
[06:03.000 --> 06:09.000]  Каждая из этих двух записей, пусть запись A, красная, обслуживается сразу на двух primary
[06:09.000 --> 06:12.000]  и упорядочивается независимо, сразу двумя primary.
[06:12.000 --> 06:15.000]  То же самое касается записи B.
[06:16.000 --> 06:21.000]  В итоге, каждый primary получает две записи, красную и синюю,
[06:21.000 --> 06:24.000]  и должен их упорядочить и выполнить в каком-то порядке.
[06:25.000 --> 06:29.000]  Вот допустим, primary 1 сначала выполнит запись B, потом запись A.
[06:31.000 --> 06:33.000]  Он имеет на это полное право.
[06:34.000 --> 06:39.000]  Primary у нас действует независимо друг от друга, просто для того, чтобы снять нагрузку с мастера системы.
[06:40.000 --> 06:49.000]  И вот это обворачивается тем, что два этих primary для двух чанков упорядочивают две записи разным образом.
[06:49.000 --> 06:56.000]  В итоге, когда две эти записи завершатся, то в файле вот этот фрагмент размером 1 мегабайт, очень небольшой,
[06:56.000 --> 07:03.000]  будет выглядеть так. Сначала в нем будут идти символы A, а потом символы B.
[07:05.000 --> 07:12.000]  Вот никто не умирал, никакие лидеры там primary не перевыбирались, никаких сбоев не было в исполнении.
[07:12.000 --> 07:22.000]  Но тем не менее, запись даже этого маленького мегабайтного фрагмента файла произошла неатомарно.
[07:23.000 --> 07:28.000]  Это первый пример. Второй пример связан тоже с файлом.
[07:28.000 --> 07:37.000]  И здесь нам бы как раз транзакции пригодились. Мы бы хотели атомарно выполнить запись и в первом чанке, в левом чанке и в правом чанке.
[07:37.000 --> 07:41.000]  Вот, пожалуйста, вполне себе пример транзакции.
[07:41.000 --> 07:50.000]  Другой пример тоже был связан с файловыми системами, но уже с масштабируемыми и отказоостойчивыми.
[07:50.000 --> 07:59.000]  Помните, мы сначала заменили GFS мастера на RSM, чтобы перерывать отказы, выделили слой chunk store,
[07:59.000 --> 08:05.000]  а дальше столкнулись с тем, что мета store, который хранил дерево файловой системы, и iNode не масштабировался.
[08:05.000 --> 08:11.000]  И для этого мы переложили мета информацию файловой системы в киволюхранилище.
[08:20.000 --> 08:39.000]  Как мы это сделали? Мы для каждой директории список файлов хранили в виде набора ключей.
[08:39.000 --> 08:48.000]  У нас была ID-директория и имя файла.
[08:48.000 --> 08:57.000]  И вот такой служебный ключ указывал на идентификатор файла, по которому можно было найти его список чанков.
[08:57.000 --> 09:19.000]  А список чанков тоже хранился в виде набора записей в киволюхранилище.
[09:19.000 --> 09:33.000]  Если бы мы хотели прочесть список чанков файла, то мы бы в киволюхранилище брали snapshot и читали по итератору все ключи,
[09:33.000 --> 09:36.000]  которые начинаются с данного идентификатора файла.
[09:36.000 --> 09:45.000]  Чтобы найти идентификатор файла, мы сначала бы выполняли дополнительный лукап, чтобы узнать его ID.
[09:45.000 --> 09:53.000]  Мы рассматривали такой дизайн на примере файловой системы Facebook, тектоник.
[09:53.000 --> 09:59.000]  Тектоник свой метод данной раскладывал в киволюхранилище разумным образом так, чтобы все записи,
[09:59.000 --> 10:05.000]  касающиеся одной директории и все записи, касающиеся одного файла находились в пределах одного таблета,
[10:05.000 --> 10:07.000]  одного шарда и обслуживались бы от Амарна.
[10:07.000 --> 10:14.000]  То есть мы можем работать со списком чанков одного файла от Амарна и мы можем работать с одной директорией от Амарна.
[10:14.000 --> 10:19.000]  Но у нас была проблема. А что если мы хотим выполнить операцию Rename?
[10:19.000 --> 10:24.000]  Перенести файл из одной директории в другую.
[10:24.000 --> 10:29.000]  Нам нужно одну запись стереть и другую запись добавить.
[10:29.000 --> 10:37.000]  Беда в том, что эти записи имеют разные директории ID, а это значит, что могут находиться в разных таблетах, в разных шардах.
[10:37.000 --> 10:41.000]  И от Амарности между этими таблетами уже нет.
[10:41.000 --> 10:46.000]  Другая операция, которую мы не можем сделать, это операция конкатинации.
[10:46.000 --> 10:54.000]  Нам нужно породить новый файл, у которого список чанков будет равен конкатинации списков чанков первого и второго файла.
[10:54.000 --> 11:03.000]  И снова у нас это не получается сделать, потому что списки чанков для двух исходных файлов лежат потенциально в разных таблетах Key Value.
[11:03.000 --> 11:10.000]  Нам снова не хватает транзакций, снова не хватает возможности от Амарны с ними работать, с разными таблетами, с разными шардами.
[11:10.000 --> 11:19.000]  Наконец, третий пример, которого мы еще на самом деле не знаем, но про который я расскажу в параллельном курсе, это очередь сообщений.
[11:26.000 --> 11:29.000]  Давайте я на примере системы Kafka это расскажу.
[11:29.000 --> 11:36.000]  Kafka это система, которая позволяет вам надежно хранить потоки данных, хранить потоки в виде очередей,
[11:36.000 --> 11:41.000]  где сообщения привязаны сквозной нумерацией.
[11:41.000 --> 11:50.000]  И я в прошлый раз, когда говорил про формальные методы, показывал вам DesignDoc, как в системе Kafka хотели сделать,
[11:50.000 --> 11:54.000]  ну и сделали, в конце концов, семантику ExactlyOnce.
[11:54.000 --> 11:55.000]  Что это означает?
[11:55.000 --> 12:00.000]  Что мы хотим иметь в Kafka ExactlyOnce Processing.
[12:10.000 --> 12:11.000]  Что это означает?
[12:11.000 --> 12:22.000]  Пусть у вас есть поток данных, который какой-то клиент вычитывает, каким-то образом обрабатывает,
[12:22.000 --> 12:27.000]  и после этого записывает измененный результат, какой-то выходной поток данных.
[12:27.000 --> 12:33.000]  То есть у него есть входная очередь, какой-то обработчик, который запускается на каком-то узле, и выходная очередь,
[12:33.000 --> 12:35.000]  куда он сохраняет результаты своей обработки.
[12:35.000 --> 12:45.000]  Беда в том, что у нас две очереди, и чтобы обработка была ExactlyOnce, мы должны атомарно уметь прочесть данные из входной очереди,
[12:45.000 --> 12:53.000]  передвинуть там курсор на чтение, обработать данные в памяти у себя и записать результат в выходную очередь.
[12:53.000 --> 12:58.000]  Вот передвинуть курсора во входной очереди и положить новое сообщение, новую запись в выходной очередь.
[12:58.000 --> 13:04.000]  Это ExactlyOnce Processing, но под капотом, чтобы его реализовать, нам тоже нужны транзакции.
[13:04.000 --> 13:08.000]  Мы хотим из очереди одно и прочитать, в другую очередь записать.
[13:08.000 --> 13:13.000]  Тоже пример транзакции, но при этом никаких таблиц у нас в явном виде здесь нет.
[13:13.000 --> 13:24.000]  Вот три примера, где транзакции естественным образом возникают, и в каждой из этих систем по умолчанию в капке не было транзакций.
[13:24.000 --> 13:30.000]  В KeyValue, над которым работает файловая система Facebook, нет транзакций.
[13:30.000 --> 13:36.000]  В GFS в этом месте тоже, конечно, не было транзакций, запись была неатомарная.
[13:36.000 --> 13:46.000]  Вот все эти примеры демонстрируют вам, что смысл транзакций гораздо шире, чем база данных и гарантия ACID.
[13:47.000 --> 13:55.000]  Смысл транзакций в следующем. Пусть у вас есть некоторая система, в которой есть какие-то компоненты,
[13:55.000 --> 14:00.000]  каждый из которых допускает конкурентные операции.
[14:00.000 --> 14:07.000]  Скажем, мы можем в Чанг писать конкурентно из разных потоков, разные байты, и праймари должны их упорядочивать.
[14:07.000 --> 14:13.000]  У нас есть KeyValue-хранилище, каждый шарт которого, разумеется, поддерживает конкурентные записи.
[14:13.000 --> 14:18.000]  У нас есть очереди сообщений в кавке, с которыми мы тоже можем работать конкурентно.
[14:18.000 --> 14:23.000]  Но при этом под капотом эти компоненты работают последовательно.
[14:23.000 --> 14:28.000]  То есть праймари упорядочивает внутри себя все записи в один поток.
[14:28.000 --> 14:35.000]  Каждый таблет в KeyValue-хранилище это отдельный RSM, и если вы пишете RSM в домашней работе,
[14:35.000 --> 14:42.000]  то вы знаете, что где-то там внутри есть поток, который последовательно вычитывает закоммитченные команды из лога и применяет их к автомату.
[14:42.000 --> 14:47.000]  В очереди сообщений тоже внутри все упорядочивается, потому что есть сквозная нумерация.
[14:47.000 --> 14:53.000]  Вот все эти примеры, они про то, что у вас есть объекты с конкурентным доступом, но они устроены последовательно.
[14:53.000 --> 14:56.000]  Они сами по себе атомарны или линеризуемы.
[14:56.000 --> 15:03.000]  И мы хотим поверх них уметь выполнять такие операции, которые трогают сразу много объектов,
[15:03.000 --> 15:08.000]  сразу много чанков, сразу много шардов, сразу много очередей сообщений.
[15:08.000 --> 15:19.000]  И такие операции, которые мы назовем транзакциями, относительно друга упорядочивались бы как атомарные целые.
[15:22.000 --> 15:25.000]  Вот транзакции, а не об этом.
[15:26.000 --> 15:28.000]  Есть ли вопросы?
[15:28.000 --> 15:41.000]  Наша цель с вами на вот эту тему, на сегодняшнее занятие и занятие через неделю,
[15:41.000 --> 15:45.000]  разобраться, как устроены транзакции в очень больших распределенных системах,
[15:45.000 --> 15:50.000]  а именно в Google Bigtable, это кивали ухранилище, который мы обсуждали позапрошлый раз,
[15:50.000 --> 15:54.000]  в Google Spanner, это вообще в принципе самая большая база данных в мире,
[15:54.000 --> 15:58.000]  и в ней реализован очень затейливый механизм транзакций.
[15:58.000 --> 16:03.000]  И мы бы хотели поговорить с вами про Яндекс.ДБ.
[16:03.000 --> 16:09.000]  Это большая распределенная система в Яндексе, которая служит хранилищем для Яндекс.Облака
[16:09.000 --> 16:12.000]  и которая реализует довольно радикальный подход к транзакциям.
[16:12.000 --> 16:16.000]  Очень интересный, мы хотим его тоже обсудить.
[16:16.000 --> 16:19.000]  Но это наша глобальная цель.
[16:19.000 --> 16:25.000]  Но вообще-то транзакции не привязаны именно к распределенным системам.
[16:25.000 --> 16:27.000]  Ну, разумеется, не привязаны к распределенным системам.
[16:27.000 --> 16:31.000]  И про транзакции можно думать не только на уровне какого-нибудь Spanner или Bigtable,
[16:31.000 --> 16:34.000]  или на уровне локальной даже базы данных.
[16:34.000 --> 16:38.000]  Транзакции можно представить себе даже в очень маленьком масштабе.
[16:38.000 --> 16:42.000]  Вот смотрите, мы говорим, что транзакция это операция, которая атомарно трогает,
[16:42.000 --> 16:48.000]  атомарно относительно других транзакций трогает сразу много последовательных объектов.
[16:48.000 --> 16:50.000]  Вот представим себе процессоры память.
[16:50.000 --> 16:52.000]  В процессоре много ядер.
[16:52.000 --> 16:55.000]  И они работают с общими ячейками памяти.
[16:55.000 --> 16:59.000]  И при этом каждая ячейка памяти, она, конечно, позволяет конкурентный доступ.
[16:59.000 --> 17:03.000]  Вы можете к ней обращаться из разных потоков, при условии, что вы какие-то атомики используете.
[17:03.000 --> 17:07.000]  Но при этом под капотом в процессоре есть протоколка гириантности,
[17:07.000 --> 17:10.000]  который говорит, что каждая ячейка памяти, она последовательна.
[17:10.000 --> 17:14.000]  Все чтения и записи в ней упорядочиваются.
[17:14.000 --> 17:21.000]  И вот мы бы, возможно, хотели иметь возможность в процессоре
[17:21.000 --> 17:24.000]  выполнять не просто атомарные операции над отдельными ячейками памяти,
[17:24.000 --> 17:27.000]  фич-ет, компер-эксченж, подобные операции.
[17:27.000 --> 17:32.000]  Мы бы хотели иметь атомарные операции, которые трогают сразу много ячеек.
[17:32.000 --> 17:37.000]  Вот если вы помните конец прошлого семестра, то в самом-самом конце,
[17:37.000 --> 17:41.000]  в бонусной лекции я вам рассказывал про тему каналы,
[17:41.000 --> 17:46.000]  про то, как делать примитивы коммуникации, которые позволяют
[17:46.000 --> 17:53.000]  передавать потоки данных из одного потока в другой или из одного файбера в другой.
[17:53.000 --> 17:56.000]  И говорил, что вместе с этими каналами хорошо бы иметь селект,
[17:56.000 --> 18:01.000]  который позволяет из нескольких каналов атомарно дождаться первого значения
[18:01.000 --> 18:03.000]  и вытащить его.
[18:03.000 --> 18:07.000]  И когда мы делали селект с гарантией лог-фри, что было особенно сложно,
[18:07.000 --> 18:10.000]  а так селект реализован в языке Котлин,
[18:10.000 --> 18:12.000]  то там мы столкнулись с проблемой.
[18:12.000 --> 18:14.000]  Нужно атомарно потрогать несколько ячеек.
[18:14.000 --> 18:17.000]  В общем случае нужно потрогать аж три ячеек.
[18:17.000 --> 18:19.000]  Ну или в простом случае две ячейки.
[18:19.000 --> 18:23.000]  Нужно передвинуть голову канала, из которой мы достаем значение,
[18:23.000 --> 18:30.000]  и нужно атомарно взвести какой-то бит в селекторе, что он значение уже забрал.
[18:30.000 --> 18:33.000]  В общем случае нужно потрогать три ячейки.
[18:33.000 --> 18:37.000]  И вот тут можно думать про какую-то операцию мультикасс,
[18:37.000 --> 18:40.000]  можно, имея касс над одним машинным словом, сделать касс,
[18:40.000 --> 18:42.000]  который работает сразу с несколькими ячейками,
[18:42.000 --> 18:47.000]  а вместо этого можно на уровне процессора реализовать механизм
[18:47.000 --> 18:51.000]  железных процессорных транзакций, которые позволят вам
[18:51.000 --> 18:54.000]  атомарно вот три ячейки памяти потрогать.
[18:57.000 --> 19:02.000]  Вот такой вот спектр применения транзакций.
[19:02.000 --> 19:06.000]  Но сегодня мы не будем говорить с вами про распределенные системы даже,
[19:06.000 --> 19:08.000]  намеренно не будем говорить.
[19:08.000 --> 19:12.000]  Сегодня наша цель поговорить про базовую теорию транзакций,
[19:12.000 --> 19:16.000]  про то, как про них вообще можно, про них правильно говорить,
[19:16.000 --> 19:19.000]  какие слова правильно использовать.
[19:21.000 --> 19:24.000]  Распределенность, отказоустойчивость, масштабируемость,
[19:24.000 --> 19:29.000]  все эти вещи нам будут важны, разумеется, но важны будут через неделю.
[19:29.000 --> 19:33.000]  А пока мы решаем задачу, которая связана именно с concurrent.
[19:33.000 --> 19:38.000]  Как, имея атомарные объекты и не думая про какие-то отказы,
[19:38.000 --> 19:42.000]  добиться атомарности на уровне операции сразу над многими объектами.
[19:43.000 --> 19:46.000]  Ну вот давайте перейдем к постановке задачи.
[19:47.000 --> 19:49.000]  Примеры мы сотрем.
[20:04.000 --> 20:09.000]  Хотя ACID я стер напрасно, давайте ACID мы вернем.
[20:11.000 --> 20:15.000]  Знакомы ли вам аббивиатура ACID, раз уж мы говорим про транзакции?
[20:15.000 --> 20:19.000]  Вот мы сейчас будем формально ставить задачу и рассуждать про транзакции,
[20:19.000 --> 20:21.000]  а пока неформальный разговор.
[20:21.000 --> 20:26.000]  Вот acronym.acid это требование к транзакциям в базах данных.
[20:26.000 --> 20:30.000]  Как эти буквы расшифровываются? Как это acronym расшифровывается?
[20:30.000 --> 20:38.000]  ACID это atomicity, C это consistency.
[20:38.000 --> 20:44.000]  Я сейчас не собираюсь пояснять, что именно каждый слов значит, мы это скоро увидим.
[20:44.000 --> 20:50.000]  Но сегодня на лекции и на семинаре мы поговорим, кажется, про все эти буквы.
[20:50.000 --> 20:54.000]  Итак, постановка задачи.
[20:55.000 --> 21:00.000]  Будем считать, что у нас не база данных, не очереди сообщений,
[21:00.000 --> 21:03.000]  а у нас абстрактное хранилище,
[21:10.000 --> 21:13.000]  в котором есть операции,
[21:13.000 --> 21:26.000]  в котором есть операции записи, значения по ключу и операция чтения по ключу.
[21:32.000 --> 21:35.000]  Мы считаем, что это хранилище линейализуемое.
[21:35.000 --> 21:40.000]  То есть оно допускает конкурентный доступ, но при этом любая конкурентная история,
[21:40.000 --> 21:45.000]  о ней можно думать, как будто бы все операции в ней произошли в некотором порядке
[21:45.000 --> 21:51.000]  с сохранением порядка операции в конкурентной истории от реального времени.
[21:51.000 --> 21:55.000]  Под datastore можно понимать самые разные объекты реальности.
[21:55.000 --> 21:58.000]  Ну, например, можно понимать память в компьютере,
[21:58.000 --> 22:00.000]  а можно понимать масштабируемое киварю хранилище,
[22:00.000 --> 22:03.000]  а можно понимать таблицу в базе данных.
[22:03.000 --> 22:05.000]  Все интерпретации подходят.
[22:05.000 --> 22:08.000]  Вот мы никакую конкретную не фиксируем сейчас.
[22:08.000 --> 22:12.000]  Над этим хранилищем мы хотим выполнять транзакции.
[22:12.000 --> 22:14.000]  Перейдем к ним.
[22:14.000 --> 22:19.000]  Под транзакциями мы будем понимать такие интерактивные программы,
[22:19.000 --> 22:24.000]  которые начинаются со служебной операции startTransaction,
[22:27.000 --> 22:34.000]  за которой следует какое-то количество сетов и гетов.
[22:35.000 --> 22:41.000]  И в конце концов транзакция завершается двумя способами.
[22:41.000 --> 22:44.000]  Либо она фиксирует свои результаты,
[22:44.000 --> 22:46.000]  commitTransaction,
[22:46.000 --> 22:50.000]  либо она отменяется.
[22:53.000 --> 22:55.000]  Почему транзакция решила отмениться?
[22:55.000 --> 23:00.000]  Ну, например, потому что вы в этой транзакции захотели перевести деньги с одного счета на другой,
[23:00.000 --> 23:03.000]  но запросили для перевода суммы, которая превышает, собственно, ваш баланс.
[23:03.000 --> 23:06.000]  Видимо, поэтому транзакция не может быть выполнена.
[23:06.000 --> 23:09.000]  Ну, а если все хорошо, если все варианты сошлись,
[23:09.000 --> 23:11.000]  то транзакция готова закомититься,
[23:11.000 --> 23:16.000]  и после комита изменения транзакции должны зафиксироваться надежно в этом хранилище.
[23:18.000 --> 23:21.000]  Транзакция, вообще говоря, интерактивная.
[23:21.000 --> 23:26.000]  То есть вы прям пишете программу, где, допустим, вы работаете с определенной системой.
[23:26.000 --> 23:28.000]  Вы создаете клиента и говорите
[23:28.000 --> 23:30.000]  client.startTransaction,
[23:30.000 --> 23:31.000]  client.set,
[23:31.000 --> 23:33.000]  client.get по какую-то серию
[23:33.000 --> 23:34.000]  таких операций,
[23:34.000 --> 23:37.000]  потом в конце говорите client.comitTransaction.
[23:37.000 --> 23:40.000]  Или, может быть, если вы работаете с традиционной базой данных,
[23:40.000 --> 23:43.000]  вы пишете какой-то декларативный скорей запрос,
[23:43.000 --> 23:45.000]  который традиционно что-то делает.
[23:45.000 --> 23:47.000]  Вот форма представления транзакции нам сейчас не важна.
[23:47.000 --> 23:51.000]  Мы будем считать, что она вот в общем виде такая программа.
[23:53.000 --> 23:55.000]  Кто обслуживает
[23:55.000 --> 23:57.000]  операции в этой программе?
[23:57.000 --> 23:59.000]  Этим занимается планировщик.
[24:07.000 --> 24:11.000]  Давайте его перенесем, а то я очень широко пишу.
[24:16.000 --> 24:19.000]  Обслуживанием транзакции занимается планировщик.
[24:24.000 --> 24:26.000]  Вот под ним находится хранилище.
[24:28.000 --> 24:31.000]  А над ним находятся клиенты.
[24:31.000 --> 24:35.000]  И клиенты в этот планировщик отправляют
[24:35.000 --> 24:37.000]  свои сеты и геты.
[24:37.000 --> 24:39.000]  Давайте я переименую сеты и геты.
[24:39.000 --> 24:40.000]  Я как-то неудачно их выбрал.
[24:40.000 --> 24:42.000]  Я буду говорить про чтение и записи.
[24:42.000 --> 24:44.000]  Дальше у нас будет просто чтение и записи.
[24:44.000 --> 24:46.000]  Например, какой-то клиент отправляет
[24:46.000 --> 24:48.000]  очередное чтение
[24:48.000 --> 24:50.000]  в этой транзакции
[24:50.000 --> 24:52.000]  для какого-то ключа х.
[24:52.000 --> 24:54.000]  Какой-то клиент отправляет
[24:54.000 --> 24:56.000]  для своей транзакции
[24:56.000 --> 25:00.000]  чтение ключа у.
[25:03.000 --> 25:05.000]  Планировщик принимает
[25:05.000 --> 25:07.000]  последовательно операции
[25:07.000 --> 25:09.000]  каждой из транзакций
[25:09.000 --> 25:11.000]  и должен перенаправлять их в хранилище.
[25:11.000 --> 25:13.000]  Что?
[25:13.000 --> 25:15.000]  Да, нотация.
[25:15.000 --> 25:17.000]  v это запись этой транзакции ключа х.
[25:17.000 --> 25:19.000]  Значение нам не важно.
[25:19.000 --> 25:21.000]  Просто запись какого-то значения.
[25:21.000 --> 25:23.000]  Это чтение житой транзакции по ключу у.
[25:24.000 --> 25:26.000]  Так вот.
[25:26.000 --> 25:28.000]  Планировщик получает
[25:28.000 --> 25:30.000]  для каждой транзакции
[25:30.000 --> 25:32.000]  последовательно вот такие вот операции,
[25:32.000 --> 25:34.000]  но при этом для разных транзакций конкурентно, разумеется.
[25:34.000 --> 25:36.000]  И должен эти
[25:36.000 --> 25:38.000]  чтение и записи
[25:38.000 --> 25:40.000]  перенаправлять в хранилище.
[25:40.000 --> 25:42.000]  Он может делать это прям последовательно.
[25:42.000 --> 25:44.000]  Может что-то делать параллельно, разумеется.
[25:44.000 --> 25:46.000]  Наверное, он хочет делать все это параллельно.
[25:46.000 --> 25:48.000]  И
[25:48.000 --> 25:50.000]  смотрите,
[25:50.000 --> 25:52.000]  вот планировщик каким-то образом
[25:52.000 --> 25:54.000]  планирует транзакции
[25:54.000 --> 25:56.000]  и на уровне хранилища
[25:56.000 --> 25:58.000]  что получается? Получается некоторая история.
[26:04.000 --> 26:06.000]  Запись первой транзакции
[26:06.000 --> 26:08.000]  по ключу х, запись
[26:08.000 --> 26:10.000]  второй транзакции по ключу х,
[26:10.000 --> 26:12.000]  запись третьей транзакции
[26:12.000 --> 26:14.000]  по ключу у.
[26:14.000 --> 26:16.000]  Вот планировщик на уровне
[26:16.000 --> 26:18.000]  хранилища данных
[26:18.000 --> 26:20.000]  порождает вот такую конкурентную историю
[26:20.000 --> 26:22.000]  с помощью своего алгоритма планирования.
[26:22.000 --> 26:24.000]  Мы сказали, что
[26:24.000 --> 26:26.000]  само хранилище линейризуемое.
[26:26.000 --> 26:28.000]  А это означает, что
[26:28.000 --> 26:30.000]  любая конкурентная история
[26:30.000 --> 26:32.000]  объясняется некоторой последовательной.
[26:32.000 --> 26:34.000]  То есть
[26:34.000 --> 26:36.000]  об этом конкурентном исполнении
[26:36.000 --> 26:38.000]  на уровне хранилища можно думать
[26:38.000 --> 26:40.000]  как о некотором
[26:40.000 --> 26:42.000]  последовательном, где, допустим, сначала
[26:42.000 --> 26:44.000]  выполнилась запись второй транзакции
[26:44.000 --> 26:46.000]  по ключу х, потом запись первой транзакции
[26:46.000 --> 26:48.000]  по ключу х, а потом запись
[26:48.000 --> 26:50.000]  третьей транзакции
[26:50.000 --> 26:52.000]  по ключу у.
[26:56.000 --> 26:58.000]  Так вот.
[26:58.000 --> 27:00.000]  Смотрите.
[27:00.000 --> 27:02.000]  У нас есть клиенты, которые запускают
[27:02.000 --> 27:04.000]  транзакции,
[27:04.000 --> 27:06.000]  отправляют их планировщику.
[27:06.000 --> 27:08.000]  Планировщик отправляет отдельные
[27:08.000 --> 27:10.000]  чтения записи на уровне хранилища.
[27:10.000 --> 27:12.000]  Все операции всех транзакций
[27:12.000 --> 27:14.000]  выстраиваются в некоторую цепочку
[27:14.000 --> 27:16.000]  в конце концов.
[27:16.000 --> 27:18.000]  Вот такую цепочку мы назовем
[27:18.000 --> 27:20.000]  словом расписание.
[27:30.000 --> 27:32.000]  Понятно ли, что
[27:32.000 --> 27:34.000]  планировщик таким образом генерирует
[27:34.000 --> 27:36.000]  расписание?
[27:36.000 --> 27:38.000]  Не то чтобы он прям выстраивает
[27:38.000 --> 27:40.000]  все операции всех транзакций
[27:40.000 --> 27:42.000]  в некоторую последовательность.
[27:42.000 --> 27:44.000]  Он
[27:44.000 --> 27:46.000]  может работать параллельно, то есть он может параллельно
[27:46.000 --> 27:48.000]  читать и описать в хранилище.
[27:48.000 --> 27:50.000]  Если, конечно, хранилище, но, разумеется, хранилище
[27:50.000 --> 27:52.000]  это, наверное, позволяет делать.
[27:52.000 --> 27:54.000]  Что память, что большое шардирование кивали у хранилища.
[27:54.000 --> 27:56.000]  Но при этом
[27:56.000 --> 27:58.000]  можно сказать, что планировщик порождает
[27:58.000 --> 28:00.000]  именно вот такие,
[28:00.000 --> 28:02.000]  некоторый класс вот таких вот
[28:02.000 --> 28:04.000]  последовательных расписаний.
[28:04.000 --> 28:06.000]  И
[28:06.000 --> 28:08.000]  наша цель на сегодня
[28:08.000 --> 28:10.000]  во-первых, подумать
[28:10.000 --> 28:12.000]  о какие требования
[28:12.000 --> 28:14.000]  мы предъявляем к планировщику,
[28:14.000 --> 28:16.000]  то есть какие
[28:16.000 --> 28:18.000]  расписания ему позволительно
[28:18.000 --> 28:20.000]  генерируют? Как описать такой класс
[28:20.000 --> 28:22.000]  хороших расписаний?
[28:22.000 --> 28:24.000]  Это будет понятие модели изоляции транзакций.
[28:24.000 --> 28:26.000]  И второй наш
[28:26.000 --> 28:28.000]  вопрос это то,
[28:28.000 --> 28:30.000]  как такой планировщик построить, который
[28:30.000 --> 28:32.000]  бы порождал только расписание
[28:32.000 --> 28:34.000]  в некотором смысле хорошей из
[28:34.000 --> 28:36.000]  класс.
[28:36.000 --> 28:38.000]  Вот давайте подумаем, какие
[28:38.000 --> 28:40.000]  расписания мы считаем хорошими.
[28:42.000 --> 28:44.000]  Но есть какие-то очевидные соображения.
[28:44.000 --> 28:46.000]  Наверное,
[28:46.000 --> 28:48.000]  расписание, где
[28:48.000 --> 28:50.000]  все транзакции,
[28:50.000 --> 28:52.000]  всех транзакций выложены подряд,
[28:52.000 --> 28:54.000]  в смысле сначала операции там первые транзакции, потом
[28:54.000 --> 28:56.000]  второй, потом третий,
[28:56.000 --> 28:58.000]  это расписание очевидно хорошее.
[28:58.000 --> 29:00.000]  Оно у нас устроило бы.
[29:00.000 --> 29:02.000]  Вот это то, что называется
[29:02.000 --> 29:04.000]  расписание.
[29:10.000 --> 29:12.000]  Это расписание, где
[29:12.000 --> 29:14.000]  сначала идут
[29:14.000 --> 29:16.000]  все операции, какой-то транзакции
[29:16.000 --> 29:18.000]  один, потом все операции, какой-то транзакции
[29:18.000 --> 29:20.000]  три, потом все операции, транзакции четыре
[29:20.000 --> 29:22.000]  и так далее.
[29:26.000 --> 29:28.000]  Определение понятно?
[29:32.000 --> 29:34.000]  Но, конечно, планировщик
[29:34.000 --> 29:36.000]  не сможет порождать только такие расписания.
[29:36.000 --> 29:38.000]  Это было бы очень неразумно,
[29:38.000 --> 29:40.000]  если бы он так делал,
[29:40.000 --> 29:42.000]  потому что тогда бы он выполнял по одной транзакции
[29:42.000 --> 29:44.000]  за раз.
[29:44.000 --> 29:46.000]  А наше масштабируемое кейвореохранилище
[29:46.000 --> 29:48.000]  потенциально может обслуживать очень много транзакций
[29:48.000 --> 29:50.000]  параллельно.
[29:50.000 --> 29:52.000]  Поэтому наш планировщик будет
[29:52.000 --> 29:54.000]  действовать сложнее и будет порождать разные
[29:54.000 --> 29:56.000]  расписания, не только такие.
[29:56.000 --> 29:58.000]  Вот как
[29:58.000 --> 30:00.000]  мы могли бы предъявить требования
[30:00.000 --> 30:02.000]  к такому планировщику?
[30:02.000 --> 30:04.000]  Чего бы мы могли
[30:04.000 --> 30:06.000]  потребовать от расписаний,
[30:06.000 --> 30:08.000]  которые он в конечном итоге
[30:08.000 --> 30:10.000]  порождает?
[30:10.000 --> 30:12.000]  Наверное, было бы здорово,
[30:12.000 --> 30:14.000]  если бы все расписания, которые
[30:14.000 --> 30:16.000]  планировщик порождал бы,
[30:16.000 --> 30:18.000]  не были бы отличимы для пользователя
[30:18.000 --> 30:20.000]  вот от таких серийных расписаний.
[30:20.000 --> 30:22.000]  Тогда пользователь
[30:22.000 --> 30:24.000]  мог бы думать о выполнении транзакций
[30:24.000 --> 30:26.000]  как о транзакциях,
[30:26.000 --> 30:28.000]  как о атомарных операциях. Сначала
[30:28.000 --> 30:30.000]  был бы у нас целиком одна, потом целиком другая и так далее.
[30:32.000 --> 30:34.000]  Для того, чтобы формализовать
[30:34.000 --> 30:36.000]  эти соображения, введем понятие
[30:36.000 --> 30:38.000]  view эквивалентности.
[30:48.000 --> 30:50.000]  Мы скажем, что два расписания
[30:50.000 --> 30:52.000]  они view эквивалентны,
[30:58.000 --> 31:00.000]  если все
[31:00.000 --> 31:02.000]  чтения в этих расписаниях
[31:02.000 --> 31:04.000]  возвращают одни и те же
[31:04.000 --> 31:06.000]  значения. И сами
[31:06.000 --> 31:08.000]  эти расписания со своими записями
[31:08.000 --> 31:10.000]  переводят хранилища в одно и то же
[31:10.000 --> 31:12.000]  итоговое состояние.
[31:12.000 --> 31:14.000]  То есть наблюдатель, который
[31:14.000 --> 31:16.000]  не видит
[31:16.000 --> 31:18.000]  то, что делает планировщик,
[31:18.000 --> 31:20.000]  а наблюдает просто результаты
[31:20.000 --> 31:22.000]  чтений, которыми планировщик
[31:22.000 --> 31:24.000]  отвечает,
[31:24.000 --> 31:26.000]  разницы между двумя расписаниями
[31:26.000 --> 31:28.000]  не видит. Для наблюдателя
[31:28.000 --> 31:30.000]  эти два расписания не отличаются.
[31:32.000 --> 31:34.000]  Хорошо, это определение
[31:34.000 --> 31:36.000]  эквивалентности, view эквивалентности.
[31:36.000 --> 31:38.000]  А дальше мы ведем определение
[31:38.000 --> 31:40.000]  view сериализуемости.
[31:40.000 --> 31:42.000]  Мы скажем, что расписание
[31:42.000 --> 31:44.000]  S
[31:44.000 --> 31:46.000]  view
[31:48.000 --> 31:50.000]  сериализуемо,
[31:52.000 --> 31:54.000]  если
[31:54.000 --> 31:56.000]  S view
[31:56.000 --> 31:58.000]  эквивалентно
[31:58.000 --> 32:00.000]  некоторому расписанию
[32:00.000 --> 32:02.000]  S со звездочкой серийным.
[32:04.000 --> 32:06.000]  То есть наблюдатель
[32:06.000 --> 32:08.000]  не может отличить
[32:08.000 --> 32:10.000]  под конкретное расписание
[32:10.000 --> 32:12.000]  от некоторого расписания,
[32:12.000 --> 32:14.000]  в котором все транзакции выполняются просто
[32:14.000 --> 32:16.000]  подряд.
[32:16.000 --> 32:18.000]  Видимо, такие расписания
[32:18.000 --> 32:20.000]  клиента устраивают.
[32:20.000 --> 32:22.000]  Видимо, такой планировщик клиента
[32:22.000 --> 32:24.000]  устраивает.
[32:24.000 --> 32:26.000]  Вот мы определили понятие
[32:26.000 --> 32:28.000]  view сериализуемости.
[32:40.000 --> 32:42.000]  Если нам дают
[32:42.000 --> 32:44.000]  view сериализуемый планировщик,
[32:44.000 --> 32:46.000]  то пользователь может
[32:46.000 --> 32:48.000]  не думать о том,
[32:48.000 --> 32:50.000]  как транзакции под капотом
[32:50.000 --> 32:52.000]  этого планировщика конкурируют.
[32:52.000 --> 32:54.000]  Он может считать, что
[32:54.000 --> 32:56.000]  транзакции как будто бы выполняются
[32:56.000 --> 32:58.000]  в некотором порядке, как будто бы они
[32:58.000 --> 33:00.000]  атомарны. Почему это требование
[33:00.000 --> 33:02.000]  очень удобно? Потому что если вы пишете какую-то
[33:02.000 --> 33:04.000]  сложную логику, какое-то приложение
[33:04.000 --> 33:06.000]  поверх базы данных,
[33:06.000 --> 33:08.000]  и данные ваши вы меняете
[33:08.000 --> 33:10.000]  с помощью транзакций,
[33:10.000 --> 33:12.000]  то вам нужно думать лишь о том, чтобы
[33:12.000 --> 33:14.000]  каждая транзакция сохраняла инварианты
[33:14.000 --> 33:16.000]  ваших данных
[33:16.000 --> 33:18.000]  в хранилище. Если каждая отдельная
[33:18.000 --> 33:20.000]  транзакция переводит базу данных из
[33:20.000 --> 33:22.000]  корректного состояния в корректное состояние,
[33:22.000 --> 33:24.000]  то этого
[33:24.000 --> 33:26.000]  достаточно, чтобы имея
[33:26.000 --> 33:28.000]  view сериализуемость даже при конкуренции
[33:28.000 --> 33:30.000]  транзакций всегда
[33:30.000 --> 33:32.000]  иметь согласованное состояние в базе.
[33:32.000 --> 33:34.000]  Вот такие простые соображения.
[33:34.000 --> 33:36.000]  Хорошо.
[33:36.000 --> 33:38.000]  Маленькое замечание.
[33:38.000 --> 33:40.000]  View сериализуемость
[33:40.000 --> 33:42.000]  вообще-то
[33:42.000 --> 33:44.000]  не требует,
[33:44.000 --> 33:46.000]  чтобы транзакция,
[33:46.000 --> 33:48.000]  которая...
[33:48.000 --> 33:50.000]  View сериализуемость
[33:50.000 --> 33:52.000]  не требует упорядочивать
[33:52.000 --> 33:54.000]  транзакции в соответствии
[33:54.000 --> 33:56.000]  с их порядком в реальном времени.
[33:56.000 --> 33:58.000]  То есть одна транзакция может завершиться
[33:58.000 --> 34:00.000]  до старта другой транзакции,
[34:00.000 --> 34:02.000]  а в порядке сериализации
[34:02.000 --> 34:04.000]  они выложатся в другом.
[34:06.000 --> 34:08.000]  Но это и требование
[34:08.000 --> 34:10.000]  естественно добавить к нашему.
[34:10.000 --> 34:12.000]  И тогда мы получим то, что называется
[34:12.000 --> 34:14.000]  strict view serializability.
[34:22.000 --> 34:24.000]  Вот это то, что называется
[34:24.000 --> 34:26.000]  моделью изоляции транзакций.
[34:26.000 --> 34:28.000]  В ACID это
[34:28.000 --> 34:30.000]  isolation.
[34:30.000 --> 34:32.000]  Isolation.
[34:34.000 --> 34:36.000]  Это семантика конкурирующих транзакций.
[34:38.000 --> 34:40.000]  Strict view serializability
[34:40.000 --> 34:42.000]  говорит нам, что
[34:42.000 --> 34:44.000]  если транзакции конкурируют,
[34:44.000 --> 34:46.000]  то можно думать, что они выполняются
[34:46.000 --> 34:48.000]  атомарно, при этом сохраняется
[34:48.000 --> 34:50.000]  порядок предшествования в реальном времени.
[34:52.000 --> 34:54.000]  И если мы имеем view сериализуемость
[34:54.000 --> 34:56.000]  для транзакций,
[34:56.000 --> 34:58.000]  то мы можем обеспечивать
[34:58.000 --> 35:00.000]  способность наших данных в хранилище.
[35:00.000 --> 35:02.000]  То есть поддерживать
[35:02.000 --> 35:04.000]  какие-то наши собственные инварианты.
[35:04.000 --> 35:06.000]  Вообще говоря, три буквы в этом
[35:06.000 --> 35:08.000]  акрониме, atomicity, durability
[35:08.000 --> 35:10.000]  и isolation
[35:10.000 --> 35:12.000]  это свойства реализации транзакций.
[35:12.000 --> 35:14.000]  А вот consistency
[35:14.000 --> 35:16.000]  это буквы, которые добавили только для того,
[35:16.000 --> 35:18.000]  чтобы получался акроним.
[35:18.000 --> 35:20.000]  Это не свойства базы данных,
[35:20.000 --> 35:22.000]  это свойства данных, которые
[35:22.000 --> 35:24.000]  вы храните в базе,
[35:24.000 --> 35:26.000]  ваши собственные инварианты,
[35:26.000 --> 35:28.000]  которые вы можете обеспечить с помощью
[35:28.000 --> 35:30.000]  остальных трех букв.
[35:32.000 --> 35:34.000]  Хорошо.
[35:34.000 --> 35:36.000]  Посмотрим на
[35:36.000 --> 35:38.000]  экран.
[35:42.000 --> 35:44.000]  Я вам показывал уже эту картинку,
[35:44.000 --> 35:46.000]  это диаграмма моделей
[35:46.000 --> 35:48.000]  согласованности и моделей
[35:48.000 --> 35:50.000]  изоляции транзакций.
[35:50.000 --> 35:52.000]  Strip serializer был это корень этой диаграммы
[35:52.000 --> 35:54.000]  и в левом под дереве
[35:54.000 --> 35:56.000]  были модели изоляции транзакций.
[35:56.000 --> 35:58.000]  То есть как мы думаем про конкуренцию транзакций?
[35:58.000 --> 36:00.000]  В правом под дереве
[36:00.000 --> 36:02.000]  у нас модели согласованности
[36:02.000 --> 36:04.000]  для конкурентных объектов.
[36:04.000 --> 36:06.000]  То есть у нас есть отдельный объект,
[36:06.000 --> 36:08.000]  например, ящейка памяти
[36:08.000 --> 36:10.000]  и с ней как-то конкурентно
[36:10.000 --> 36:12.000]  взаимодействуют.
[36:12.000 --> 36:14.000]  Вот правое под дерево про
[36:14.000 --> 36:16.000]  отдельные объекты, левое под дерево
[36:16.000 --> 36:18.000]  про изоляцию транзакций.
[36:20.000 --> 36:22.000]  На всякий случай у нас вопросы слышны
[36:22.000 --> 36:24.000]  потому что обычно вопросы есть,
[36:24.000 --> 36:26.000]  а тут все молчат.
[36:36.000 --> 36:38.000]  В этом определении мы не требовали никаких
[36:38.000 --> 36:40.000]  гарантий относительно расположения
[36:40.000 --> 36:42.000]  транзакций в реальном времени.
[36:42.000 --> 36:44.000]  Мы говорили просто, что
[36:46.000 --> 36:48.000]  планировщик,
[36:48.000 --> 36:50.000]  который обеспечивает
[36:50.000 --> 36:52.000]  view serializability, порождает такие
[36:52.000 --> 36:54.000]  расписания, которые неотличимы
[36:54.000 --> 36:56.000]  от некоторого серийного расписания.
[36:56.000 --> 36:58.000]  Но мы не накладывали
[36:58.000 --> 37:00.000]  никаких ограничений на то, в каком порядке
[37:00.000 --> 37:02.000]  могут идти вот эти транзакции.
[37:02.000 --> 37:04.000]  Слово strict означает,
[37:04.000 --> 37:06.000]  что если у вас была транзакция
[37:06.000 --> 37:08.000]  T1
[37:08.000 --> 37:10.000]  и она в реальном времени
[37:10.000 --> 37:12.000]  предшествовала транзакции
[37:12.000 --> 37:14.000]  T2,
[37:16.000 --> 37:18.000]  то вот в этом серийном расписании,
[37:18.000 --> 37:20.000]  в котором мы готовы объяснить
[37:20.000 --> 37:22.000]  конкурентное исполнение транзакций,
[37:22.000 --> 37:24.000]  транзакция T1 тоже должна
[37:24.000 --> 37:26.000]  предшествовать T2.
[37:26.000 --> 37:28.000]  Собственно, это то ограничение, которое у нас было в реализуемости.
[37:28.000 --> 37:30.000]  За это отвечает
[37:30.000 --> 37:32.000]  слово strict.
[37:34.000 --> 37:36.000]  Просто серийализуемость никаких гарантий
[37:36.000 --> 37:38.000]  про конкретный порядок не дает,
[37:38.000 --> 37:40.000]  но может быть любым.
[37:48.000 --> 37:50.000]  Вообще в этой диаграмме
[37:50.000 --> 37:52.000]  слово serializable означает
[37:52.000 --> 37:54.000]  view serializable, если ты об этом.
[37:54.000 --> 37:56.000]  Это такой
[37:56.000 --> 37:58.000]  дефолт. У нас будет сейчас другая
[37:58.000 --> 38:00.000]  сериализуемость, не view, а мы слово заменим.
[38:00.000 --> 38:02.000]  Вот оно здесь опущено и имеется в виду view,
[38:02.000 --> 38:04.000]  то есть неотличимы для наблюдателя.
[38:08.000 --> 38:10.000]  У меня замечание здесь было такое,
[38:10.000 --> 38:12.000]  почему вот это
[38:12.000 --> 38:14.000]  поддерево является поддеревом
[38:14.000 --> 38:16.000]  сериализуемости?
[38:16.000 --> 38:18.000]  Потому что мы говорим,
[38:18.000 --> 38:20.000]  почему линейализуемость здесь выделена
[38:20.000 --> 38:22.000]  как будто бы частный случай strict
[38:22.000 --> 38:24.000]  serializability.
[38:24.000 --> 38:26.000]  Смысл вот в чем.
[38:26.000 --> 38:28.000]  Если мы возьмем транзакции над базой данных,
[38:28.000 --> 38:30.000]  вот пусть у нас есть хранилище,
[38:30.000 --> 38:32.000]  которое умеет операции записи и чтения,
[38:32.000 --> 38:34.000]  и мы над ним делаем транзакции.
[38:34.000 --> 38:36.000]  Если мы делали транзакции,
[38:36.000 --> 38:38.000]  то мы должны объяснить пользователю,
[38:38.000 --> 38:40.000]  какова симантика конкуренции транзакций.
[38:40.000 --> 38:42.000]  Для этого мы определяем модель изоляции.
[38:42.000 --> 38:44.000]  Но в то же время мы
[38:44.000 --> 38:46.000]  с хранилищем можем работать
[38:46.000 --> 38:48.000]  как с хранилищем напрямую,
[38:48.000 --> 38:50.000]  то есть выполнять просто set и get.
[38:50.000 --> 38:52.000]  И получается, что у нас могут конкурировать транзакции
[38:52.000 --> 38:54.000]  и точечные операции.
[38:54.000 --> 38:56.000]  Это довольно неудобно.
[38:56.000 --> 38:58.000]  Поэтому, если у нас в системе есть
[38:58.000 --> 39:00.000]  транзакции, то удобнее сказать,
[39:00.000 --> 39:02.000]  что операции set и get это такие
[39:02.000 --> 39:04.000]  элементарные транзакции,
[39:04.000 --> 39:06.000]  состоящие из одной операции.
[39:06.000 --> 39:08.000]  Так вот, линейализуемость является
[39:08.000 --> 39:10.000]  частным случаем
[39:10.000 --> 39:12.000]  strict serializability в том смысле,
[39:12.000 --> 39:14.000]  что если мы
[39:14.000 --> 39:16.000]  возьмем все транзакции
[39:16.000 --> 39:18.000]  и сделаем их вырожденными,
[39:18.000 --> 39:20.000]  то есть оставим в них только одну
[39:20.000 --> 39:22.000]  операцию, то гарантия
[39:22.000 --> 39:24.000]  strict serializability превратится
[39:24.000 --> 39:26.000]  в линейализуемость.
[39:28.000 --> 39:30.000]  Вот такие
[39:30.000 --> 39:32.000]  рассуждения.
[39:34.000 --> 39:36.000]  Хорошо, возвращаемся на экран.
[39:36.000 --> 39:38.000]  На доску.
[39:38.000 --> 39:40.000]  Следующий наш шаг.
[39:40.000 --> 39:42.000]  Итак, мы хотим,
[39:42.000 --> 39:44.000]  видимо, построить планировщик,
[39:44.000 --> 39:46.000]  который обеспечит нам
[39:46.000 --> 39:48.000]  вот эту вот view-сериализуемость.
[39:48.000 --> 39:50.000]  Но я бы сказал,
[39:50.000 --> 39:52.000]  что мы обречены, потому что
[39:52.000 --> 39:54.000]  view-сериализуемость устроена очень
[39:54.000 --> 39:56.000]  непонятным образом.
[40:00.000 --> 40:02.000]  Вот давайте посмотрим
[40:02.000 --> 40:04.000]  на пример
[40:04.000 --> 40:06.000]  некоторого расписания
[40:06.000 --> 40:08.000]  и некоторой работы планировщика.
[40:08.000 --> 40:10.000]  Пусть у нас было два ключа,
[40:10.000 --> 40:12.000]  X и Y.
[40:12.000 --> 40:14.000]  И у нас были три транзакции,
[40:14.000 --> 40:16.000]  T1, T2, T3.
[40:22.000 --> 40:24.000]  Вот по вертикали время.
[40:26.000 --> 40:28.000]  Пусть транзакция T1
[40:28.000 --> 40:30.000]  записала что-то по ключу
[40:30.000 --> 40:32.000]  X.
[40:32.000 --> 40:34.000]  Потом транзакция 2
[40:34.000 --> 40:36.000]  записала
[40:36.000 --> 40:38.000]  что-то по ключу
[40:38.000 --> 40:40.000]  Y.
[40:40.000 --> 40:42.000]  Потом записала что-то
[40:42.000 --> 40:44.000]  по ключу X.
[40:44.000 --> 40:46.000]  А потом первая транзакция записала
[40:46.000 --> 40:48.000]  что-то по ключу Y.
[40:48.000 --> 40:50.000]  То есть две транзакции
[40:50.000 --> 40:52.000]  пишут один и тот же набор ключей,
[40:52.000 --> 40:54.000]  но в разном порядке.
[40:54.000 --> 40:56.000]  И мы все эти записи исполнили
[40:56.000 --> 40:58.000]  вот так вот. Сначала мы исполнили
[40:58.000 --> 41:00.000]  запись в X первой транзакции,
[41:00.000 --> 41:02.000]  потом две записи в второй транзакции,
[41:02.000 --> 41:04.000]  потом запись первой транзакции.
[41:04.000 --> 41:07.000]  Вот будет ли такое расписание серилизуемым?
[41:11.000 --> 41:14.000]  Вот расписание серилизуемое...
[41:16.000 --> 41:21.000]  Оно не будет, потому что у нас два ключа, и вроде бы мы должны...
[41:21.000 --> 41:24.000]  Если расписание серилизуется, то значит, что выполнилось сначала T1,
[41:24.000 --> 41:27.000]  сначала T1, потом T2, либо T2, потом T1.
[41:27.000 --> 41:31.000]  Но в обеих случаях, в обоих случаях мы должны видеть
[41:31.000 --> 41:35.000]  либо обе записи первой транзакции, либо обе записи второй транзакции.
[41:36.000 --> 41:39.000]  Вот так что планировщик, который действует таким образом,
[41:39.000 --> 41:41.000]  он действует немного странно.
[41:41.000 --> 41:44.000]  Он как будто бы уже серилизуемость нарушил.
[41:44.000 --> 41:47.000]  Но тут появляется еще одна транзакция,
[41:47.000 --> 41:50.000]  которая решает записать что-то
[41:52.000 --> 41:56.000]  по ключу, ну, допустим, X.
[41:57.000 --> 42:02.000]  А вот такая история будет серилизуемой?
[42:06.000 --> 42:10.000]  Вот кажется, что вот такое исполнение можно объяснить
[42:10.000 --> 42:12.000]  таким серийным расписанием,
[42:12.000 --> 42:15.000]  где сначала выполнилась транзакция T2,
[42:15.000 --> 42:20.000]  потом выполнилась транзакция T1, а потом выполнилась транзакция T3.
[42:21.000 --> 42:23.000]  Правда?
[42:24.000 --> 42:27.000]  Вот это view-серилизуемое расписание.
[42:27.000 --> 42:30.000]  Но ясно, что никакой планировщик не умеет
[42:30.000 --> 42:33.000]  предсказывать будущее, поэтому так он работать не может.
[42:33.000 --> 42:36.000]  Он не может полагаться на то, что появится транзакция T3
[42:36.000 --> 42:39.000]  и замаскирует вот наш косяк, который мы сделали здесь.
[42:39.000 --> 42:44.000]  И вообще задача, глядя на серилизуем,
[42:44.000 --> 42:47.000]  глядя на расписание, то есть на последовательность таких вот операций
[42:47.000 --> 42:50.000]  чтения и записи из разных транзакций,
[42:50.000 --> 42:53.000]  определить, серилизуется оно или нет,
[42:54.000 --> 42:56.000]  вот эта задача NP полная.
[42:56.000 --> 43:02.000]  То есть класс view-серилизуемых расписаний очень сложно устроен.
[43:08.000 --> 43:11.000]  Так что мы будем, мы себя упростим задачу.
[43:11.000 --> 43:17.000]  Мы скажем, что, ну да, вот есть все возможные view-серилизуемые расписания,
[43:21.000 --> 43:25.000]  но мы не будем пытаться строить планировщика,
[43:25.000 --> 43:28.000]  который умеет строить произвольные расписания из этого класса.
[43:28.000 --> 43:33.000]  Мы выделим в этом большом классе расписаний сложно устроенных,
[43:33.000 --> 43:39.000]  некоторые под класс, расписания, которые устроены просто.
[43:39.000 --> 43:44.000]  И дальше научим планировщик порождать расписания из этого класса.
[43:44.000 --> 43:47.000]  Ну а значит они будут по-прежнему хорошими,
[43:47.000 --> 43:51.000]  потому что они принадлежат классу view-серилизуемых расписаний.
[43:51.000 --> 43:53.000]  Ну вот давайте научимся это делать.
[43:53.000 --> 43:59.000]  Давайте подумаем, как можно упростить себе задачу.
[44:17.000 --> 44:23.000]  Это немного похоже, да.
[44:23.000 --> 44:26.000]  Сейчас параллелей будет еще больше.
[44:26.000 --> 44:28.000]  Вот действительно мы какими-то исполнениями готовы пожертвовать,
[44:28.000 --> 44:31.000]  потому что мы не понимаем, как они устроены.
[44:31.000 --> 44:39.000]  Вот давайте ведем вспомогательное определение конфликта в расписании.
[44:39.000 --> 44:43.000]  Вот смотрим на расписание, то есть на цепочку чтений и записей разных транзакций,
[44:43.000 --> 44:46.000]  как они линеризовались на уровне хранилища.
[44:46.000 --> 44:51.000]  И скажем, что две операции из двух разных транзакций конфликтуют,
[44:51.000 --> 44:58.000]  если эти две операции обращаются к одной и той же записи,
[44:58.000 --> 45:01.000]  к одному и тому же ключу, во-первых.
[45:01.000 --> 45:05.000]  А во-вторых, по крайней мере, одно из этих обращений – запись.
[45:05.000 --> 45:11.000]  Ну то есть, например, запись по ключу х из и этой транзакции
[45:11.000 --> 45:19.000]  конфликтует с чтением житой транзакции по этому же ключу.
[45:19.000 --> 45:21.000]  Это не конфликтует.
[45:21.000 --> 45:24.000]  С другой стороны, не конфликтует, например,
[45:24.000 --> 45:31.000]  две записи из разных транзакций по разным ключам.
[45:31.000 --> 45:40.000]  Или два чтения одного и того же ключа из разных транзакций.
[45:40.000 --> 45:42.000]  Вот они тоже не конфликтуют.
[45:42.000 --> 45:50.000]  Здесь нет, по крайней мере, одной записи, здесь просто ключи разные.
[45:50.000 --> 45:53.000]  Что нам дает это понятие конфликта?
[45:53.000 --> 45:59.000]  Вот заметим, что если у нас есть расписание,
[45:59.000 --> 46:03.000]  и в нем рядом есть две операции,
[46:03.000 --> 46:10.000]  вот, скажем, давайте вот такая и вот такая,
[46:10.000 --> 46:16.000]  которые не конфликтуют, то их можно было бы выполнить в другом порядке,
[46:16.000 --> 46:19.000]  их можно было бы свопнуть,
[46:19.000 --> 46:23.000]  и при этом для внешнего наблюдателя ничего бы не изменилось.
[46:23.000 --> 46:28.000]  То есть все чтения вернули бы те же самые результаты,
[46:28.000 --> 46:33.000]  и конечное состояние базового данного было бы таким же.
[46:33.000 --> 46:39.000]  Потому что две неконфликтующие операции коммутируют в этом расписании.
[46:39.000 --> 46:45.000]  С помощью вот такого наблюдения введем понятие конфликтной эквивалентности.
[46:45.000 --> 46:51.000]  Мы скажем, что расписание S и S' они...
[46:51.000 --> 46:57.000]  Можете сейчас, пожалуйста, представить конфликт-определение?
[46:57.000 --> 47:02.000]  Определение конфликта точно такое же, как в моделях памяти в прошлом семестре.
[47:02.000 --> 47:06.000]  У нас есть два обращения из разных транзакций,
[47:06.000 --> 47:14.000]  и мы говорим, что они конфликтуют, если эти два обращения обращаются к одному и тому же ключу,
[47:14.000 --> 47:21.000]  и, по крайней мере, одно из этих обращений – запись.
[47:21.000 --> 47:26.000]  Смысл конфликтующих операций в том, что неконфликтующие операции можно менять местами.
[47:26.000 --> 47:32.000]  Соседние неконфликтующие операции, например, два чтения, можно поменять местами.
[47:32.000 --> 47:35.000]  И при этом ничего не поменяется для пользователя.
[47:35.000 --> 47:43.000]  Или можно поменять две записи, которые пишут просто по разным ключам.
[47:43.000 --> 47:50.000]  Ну вот, а теперь можно ввести понятие конфликтной эквивалентности.
[47:50.000 --> 47:57.000]  Мы скажем, что S и S' эквивалентны, если, ну, очевидно, одно расписание можно получить из другого
[47:57.000 --> 48:05.000]  серии свопов соседних неконфликтующих операций.
[48:05.000 --> 48:23.000]  И мы скажем, что расписание S конфликтно-сериализуемо, если S конфликтно-эквивалентно
[48:23.000 --> 48:31.000]  некоторому серийному расписанию, где все транзакции выполняются подряд.
[48:31.000 --> 48:39.000]  То есть конфликтная сериализуемость означает, что серий свопов неконфликтующих операций,
[48:39.000 --> 48:45.000]  стоящих рядом, можно получить вот расписание такого вида.
[48:45.000 --> 48:55.000]  И вот тут мы получаем под класс расписаний, которые являются конфликтно-сериализуемыми.
[48:55.000 --> 48:59.000]  Тут довольно много определений возникает по пути, но все они довольно простые.
[48:59.000 --> 49:05.000]  Еще раз напомню всю конструкцию. У нас есть хранилище данных, над ним есть планировщик.
[49:05.000 --> 49:10.000]  Планировщик усыпится операцией транзакций, планировщик их направляет в хранилище.
[49:10.000 --> 49:16.000]  Хранилище линиализуемо, поэтому любая работа планировщика превращается в расписание.
[49:16.000 --> 49:22.000]  То есть мы все наши конкурирующие деты и сеты каким-то образом линиализуем на уровне хранилища,
[49:22.000 --> 49:26.000]  и как будто бы планировщик порождает цепочку чтений и записей.
[49:26.000 --> 49:31.000]  И мы дальше рассуждаем, а какие же вот такие цепочки, какие расписания являются хорошими.
[49:31.000 --> 49:37.000]  Хорошими являются те, которые неотличимы от таких вот расписаний, где транзакции выполняются подряд.
[49:37.000 --> 49:42.000]  Но беда в том, что такие хорошие расписания в U-сериализуемой устроены очень сложно,
[49:42.000 --> 49:49.000]  поэтому мы заменяем их на некоторые под класс, которые также...
[49:49.000 --> 49:59.000]  Это очевидно под класс, то есть любое конфликтно-сериализуемое расписание является в U-сериализуемым по понятным причинам.
[49:59.000 --> 50:04.000]  Но при этом я утверждаю, что класс вот таких расписаний устроен очень просто,
[50:04.000 --> 50:11.000]  что можно легко дать критерий конфликтно-сериализуемости для расписания.
[50:11.000 --> 50:21.000]  Вот посмотрим на расписание и посмотрим на две его какие-то операции O и O'.
[50:21.000 --> 50:26.000]  Вот операция O принадлежит транзакции T, операция O' принадлежит транзакции T'.
[50:26.000 --> 50:33.000]  И вот пусть две эти операции конфликтуют. Что это означает?
[50:33.000 --> 50:43.000]  Что как бы мы ни свопали не конфликтующие операции, мы не сможем две эти операции O и O' поменять местами.
[50:43.000 --> 50:51.000]  А это означает, что если мы хотим предъявить сериализацию для этого расписания S,
[50:51.000 --> 50:58.000]  то в этой потенциальной сериализации T будет обязательно предшествовать T'.
[50:58.000 --> 51:05.000]  То есть любая пара конфликтующих операций в расписании задает жесткое ограничение
[51:05.000 --> 51:11.000]  на возможный относительный порядок двух транзакций.
[51:11.000 --> 51:14.000]  Это вот две конкретные операции конфликтующие.
[51:14.000 --> 51:19.000]  А если мы возьмем всю совокупность таких пар, то есть всю совокупность таких ограничений,
[51:19.000 --> 51:24.000]  то мы получим то, что называется граф конфликтов.
[51:24.000 --> 51:34.000]  Вот мы можем по расписанию построить граф конфликтов,
[51:34.000 --> 51:39.000]  который устроен следующим образом. У нас кончается место на доске.
[51:39.000 --> 51:45.000]  Чем бы мы пожертвовали?
[51:45.000 --> 51:51.000]  А не будем ничем жертвовать. Мы скажем просто, что в этом графе конфликтов множество вершин,
[51:51.000 --> 51:58.000]  это множество транзакций расписания S,
[51:58.000 --> 52:04.000]  и между двумя вершинами, то есть двумя транзакциями есть направленная дуга,
[52:04.000 --> 52:15.000]  когда в расписании есть две операции O' таких, что они конфликтуют,
[52:15.000 --> 52:18.000]  и O предшествует O'.
[52:18.000 --> 52:23.000]  То есть мы просто в виде графа конфликтов фиксируем всю систему ограничений
[52:23.000 --> 52:29.000]  на относительный порядок транзакций, которые в расписании уже заложены.
[52:29.000 --> 52:35.000]  Определение понятно?
[52:36.000 --> 52:43.000]  Хорошо, тогда мы теперь с помощью вот этого понятия графа конфликтов
[52:43.000 --> 53:04.000]  можем легко сформулировать очень простой критерий конфликтной сериализуемости.
[53:04.000 --> 53:27.000]  Мы скажем, что расписание S конфликтно-сериализуемое тогда и только тогда, когда что?
[53:27.000 --> 53:32.000]  Но если у нас граф конфликтов задает все возможные ограничения на сериализацию,
[53:32.000 --> 53:41.000]  когда граф конфликтов для расписания S цикличный,
[53:41.000 --> 53:44.000]  ну вот давайте это докажем в две стороны.
[53:44.000 --> 53:47.000]  Сначала в прямую.
[53:47.000 --> 53:51.000]  То есть пусть у нас S конфликтно-сериализуемое расписание,
[53:51.000 --> 53:57.000]  то есть серий свопов соседних неконфликтующих операций можно привести S
[53:57.000 --> 54:08.000]  к расписанию серийному, где все транзакции выложены подряд.
[54:08.000 --> 54:20.000]  Ну давайте подумаем, как меняется граф при обмене двух соседних неконфликтующих операций?
[54:20.000 --> 54:23.000]  Что происходит с этим графом?
[54:23.000 --> 54:29.000]  Ну понятно, что в нем могут появиться и исчезнуть дуги, которые касаются только вот двух транзакций,
[54:29.000 --> 54:31.000]  которым принадлежат две эти операции.
[54:31.000 --> 54:37.000]  Но поскольку эти операции не конфликтуют, то значит обмен этих двух соседних неконфликтующих операций
[54:37.000 --> 54:40.000]  не добавляет некие дуги, не удаляет дуги.
[54:40.000 --> 54:48.000]  Иначе говоря, граф конфликтов инвариантен относительно свопа этих двух соседних неконфликтующих операций.
[54:48.000 --> 54:56.000]  Получается, что если мы расписание S можем привести к серийному расписанию S со звездочкой,
[54:56.000 --> 55:04.000]  то в силу этого свойства графа, графы конфликтов для S и для S со звездочкой одинаковые.
[55:04.000 --> 55:11.000]  Ну а очевидно, что граф конфликтов для расписания серийного S со звездочкой ацикличный.
[55:11.000 --> 55:16.000]  Просто вот по его виду, по построению этого графа.
[55:16.000 --> 55:22.000]  А и графы у этих расписаний одинаковые, ну значит ацикличный и граф для S.
[55:22.000 --> 55:26.000]  Теперь в обратную сторону.
[55:26.000 --> 55:35.000]  Пусть у нас граф конфликтов для расписания S ацикличный.
[55:35.000 --> 55:40.000]  Это означает, что в этом графе есть некоторые вершины,
[55:40.000 --> 55:45.000]  а вершины некоторые транзакции с нулевой входящей степени.
[55:45.000 --> 55:50.000]  Вот найдем такую вершину slash транзакцию.
[55:50.000 --> 55:57.000]  И посмотрим на расписание S. Я пишу под углом почему-то.
[56:01.000 --> 56:07.000]  Вот у нас где-то есть операция O из этой транзакции T со звездочкой.
[56:07.000 --> 56:13.000]  Что означает то утверждение, что в T нет входящих дуг.
[56:13.000 --> 56:18.000]  В T со звездочкой нет входящих дуг.
[56:18.000 --> 56:23.000]  Любая входящая дуга означает, что перед операцией транзакции есть конфликтующая операция с другой транзакции.
[56:23.000 --> 56:27.000]  Но вот входящих в T со звездочкой нет.
[56:27.000 --> 56:34.000]  Это означает, что в расписании S перед любой операции O, с транзакцией T со звездочкой все операции передней не конфликтующие,
[56:34.000 --> 56:41.480]  не конфликтующие. А это означает, что серии свопов можно передвинуть операцию O
[56:41.480 --> 56:46.160]  в начало расписания. Таким образом, мы можем построить по расписанию S
[56:46.160 --> 56:55.200]  расписание S, пусть S0, расписание S1, где в самом начале находится
[56:55.200 --> 57:01.840]  Т со звездочкой, а дальше что-то. Ну и дальше оторвем эту транзакцию от графа,
[57:01.920 --> 57:05.880]  то есть займемся стипологической сортировкой. В оставшемся графе снова есть
[57:05.880 --> 57:09.640]  вершина с нулевой входящей степени, и мы повторим рассуждение для нее.
[57:09.640 --> 57:17.080]  И вот таким образом мы рано или поздно свопами просто получим расписание S
[57:17.080 --> 57:31.480]  серийное. Ну вот такой вот простой критерий сериализуемости. Что это нам дает?
[57:31.480 --> 57:35.880]  Теперь мы можем строить необходимый нам планировщик следующим образом.
[57:39.880 --> 57:45.520]  Мы должны гарантировать, что если мы гарантируем, что планировщик будет
[57:45.520 --> 57:51.360]  порождать только такие расписания, что в графах конфликтов для этих
[57:51.360 --> 57:55.120]  расписаний не будет циклов, то это будет означать, что планировщик
[57:55.120 --> 57:59.880]  порождает только конфликт на сериализуемые расписания, а значит, только в U
[57:59.880 --> 58:05.080]  сериализуемые расписания, а значит, пользователь не отличает их от серийных,
[58:05.080 --> 58:10.640]  то есть пользователь может не думать про конкарнси. Вот наша задача с одной
[58:10.640 --> 58:15.440]  стороны получить планировщик, который порождает расписания, в которых в этих
[58:15.440 --> 58:20.600]  графах нет циклов, а с другой стороны этот планировщик по возможности работает
[58:20.600 --> 58:23.960]  параллельно, то есть он запускает какие-то чтения записи параллельно.
[58:24.040 --> 58:31.000]  Ну вот давайте рассмотрим дизайн такого планировщика.
[58:44.000 --> 58:48.960]  Планировщик получает транзакции, он не знает их заранее, то есть планировщик
[58:48.960 --> 58:53.640]  вот работает и ему прилетает команда start transaction какого-то клиента,
[58:53.640 --> 58:57.640]  потом прилетает команда первое чтение этого клиента, потом второе чтение,
[58:57.640 --> 59:02.640]  потом третья запись. Вот он транзакцию все заранее не видит, он работает с ними
[59:02.640 --> 59:11.640]  как интерактивно. Итак, мы хотим построить планировщик, который порождает только
[59:11.640 --> 59:15.640]  конфликт на сериализуемые расписания. Такой планировщик будет называться
[59:15.640 --> 59:26.640]  2PL scheduler, как вам больше нравится. Мы сейчас построим протокол, который
[59:26.640 --> 59:43.640]  называется двухфазные блокировки. Он очень тупой, очень простой. Итак,
[59:43.640 --> 59:50.640]  у нас есть хранилище. Давайте свяжем с каждым ключом некоторую блокировку.
[59:50.640 --> 59:54.640]  Не то, чтобы это мьютекс где-то лежит, но вот такую условную блокировку,
[59:54.640 --> 01:00:00.640]  она может быть захвачена, может быть свободна. Когда нам прилетает какая-то
[01:00:00.640 --> 01:00:08.640]  операция транзакции, то чтобы выполнить ее над хранилищем, мы сначала должны
[01:00:08.640 --> 01:00:18.640]  взять блокировку для заданного ключа, к которому мы обращаемся. Давайте я нарисую
[01:00:18.640 --> 01:00:25.640]  жизнь одной транзакции. Вот некоторая транзакция T. Мы получаем некоторую запись
[01:00:25.640 --> 01:00:32.640]  или некоторое чтение, и в первую очередь мы берем лог для ключа. После этого
[01:00:32.640 --> 01:00:40.640]  мы выполняем запись. Лог мы после этого не отпускаем, так и держим. Нам приходит
[01:00:40.640 --> 01:00:46.640]  новое чтение. Мы берем блокировку на ключ, к которому это чтение обращается,
[01:00:46.640 --> 01:00:51.640]  и после этого читаем. Вот у меня здесь горизонтальные стрелки это чтение
[01:00:51.640 --> 01:00:56.640]  записи, вертикальные это блокировки. Вот мы блокировки копим и копим и копим.
[01:00:56.640 --> 01:01:10.640]  В конце концов у нас транзакция завершается, нам прилетает команда commit transaction,
[01:01:10.640 --> 01:01:17.640]  и вот после того как транзакция закоммитчена, то есть мы надежно зафиксировали ее
[01:01:17.640 --> 01:01:21.640]  изменения в какой-то журнал, а про журнал мы сегодня поговорим на второй части занятия,
[01:01:21.640 --> 01:01:42.640]  то мы наконец все наши блокировки отпускаем. Здесь были локи, тут аналоги. Понятна ли идея,
[01:01:42.640 --> 01:01:53.640]  как протокол работает? Да, то есть если мы в транзакции решили что-то прочесть,
[01:01:53.640 --> 01:02:00.640]  то планировщик сначала возьмет блокировку. Пока он ее не получил, он читать не будет.
[01:02:00.640 --> 01:02:14.640]  Ну как, у нас для каждого ключа один лок, если мы им уже владеем, то хорошо,
[01:02:14.640 --> 01:02:18.640]  ничего не делаем, просто сразу читаем или пишем. Но если у нас лока не было еще для данного ключа,
[01:02:18.640 --> 01:02:27.640]  мы сначала должны его взять. Тут единственный аспект это то, что мы локи не отпускаем
[01:02:27.640 --> 01:02:35.640]  до тех пор, пока транзакция не завершится, не закоммитится. После этого мы все локи отпустим.
[01:02:35.640 --> 01:02:44.640]  Так подожди, у нас конфликты конечно могут происходить, то есть разные транзакции могут
[01:02:44.640 --> 01:02:48.640]  читать одни и те же ключи или писать одни и те же ключи. Собственно мы для этого блокировки
[01:02:48.640 --> 01:02:54.640]  и берем, чтобы эти транзакции развести, чтобы одна ждала другую. Про дедлоки поговорим,
[01:02:54.640 --> 01:02:59.640]  но конечно, если ты берешь много локов и не отпускаешь их, то и берешь их в обыкаком порядке,
[01:02:59.640 --> 01:03:04.640]  а ты не контролируешь, в каком порядке ты их берешь, конечно, то дедлок у тебя может возникнуть.
[01:03:04.640 --> 01:03:10.640]  Но давайте начало о хорошем, что если этот алгоритм не зависнет, то он пародит конфликтно-сириализуемое
[01:03:10.640 --> 01:03:17.640]  расписание. То есть он пародит расписание, в графе конфликтов которого не будет циклов.
[01:03:17.640 --> 01:03:25.640]  То есть смотрите, здесь у нас уже параллелизм есть, то есть транзакции, которые читают, которые работают
[01:03:25.640 --> 01:03:31.640]  с разными ключами, у них разные working set, вот они работают параллельно, потому что они берут разные блокировки.
[01:03:31.640 --> 01:03:37.640]  Но если транзакции пересекаются по ключам, то они где-то на какой-то блокировке все-таки затормозятся.
[01:03:37.640 --> 01:03:45.640]  Так что у нас какие-то конкурентные истории, но мы хотим тем не менее доказать, что любая такая история,
[01:03:45.640 --> 01:03:51.640]  если ее линеризовать на уровне хранилища, даст нам конфликтно-сириализуемое расписание.
[01:03:51.640 --> 01:03:55.640]  Для этого мы хотим показать, что в графе конфликтов не будет циклов.
[01:03:55.640 --> 01:04:01.640]  Ну и давайте сделаем этот противного. То есть предположим, что мы запустили такой планировщик,
[01:04:01.640 --> 01:04:08.640]  он пародил какое-то расписание, и в этом расписании в графе конфликтов появился цикл.
[01:04:15.640 --> 01:04:28.640]  Что означает цикл в графе конфликтов? То есть мы породили некоторое расписание,
[01:04:28.640 --> 01:04:36.640]  в графе конфликтов которого есть цикл. Вот давайте посмотрим на каждую дугу в этом цикле.
[01:04:36.640 --> 01:04:41.640]  Посмотрим на первую дугу. Что значит дуга в графе конфликтов?
[01:04:41.640 --> 01:04:55.640]  Что в расписании нашлась операция O1 и операция O2', O11-1 и O2' из T2,
[01:04:55.640 --> 01:05:02.640]  такие что эти операции конфликтуют, и O1 в расписании идет раньше, чем O2'.
[01:05:02.640 --> 01:05:11.640]  Ну почему O1 оказалось в расписании раньше, чем O2'?
[01:05:11.640 --> 01:05:16.640]  Посмотрите, раз они конфликтуют эти две операции, значит они обращаются просто по определению
[01:05:16.640 --> 01:05:21.640]  конфликта к одному и тому же ключу. Чтобы обратиться к одному и тому же ключу,
[01:05:21.640 --> 01:05:27.640]  нужно взять блокировку. Так что и это операция, и это брали одну и ту же блокировку.
[01:05:27.640 --> 01:05:36.640]  И то, что эти две операции упорядочились, означает, что их критические секции упорядочились.
[01:05:36.640 --> 01:05:49.640]  Что в свою очередь означает, что O1, то есть чтение или запись O1, предшествовала
[01:05:49.640 --> 01:06:08.640]  анлоку ключа O1, анлок предшествовал локу ключа, который читал или писал O2.
[01:06:08.640 --> 01:06:15.640]  Ну вот так вот. Такие вот соображения. Они справедливы для каждой дуги.
[01:06:15.640 --> 01:06:26.640]  То есть здесь точно также есть какие-то операции O2 и O3', которые конфликтуют.
[01:06:26.640 --> 01:06:34.640]  И здесь есть операции, ну давайте я напишу их так, O, какое-то KT, и O1',
[01:06:34.640 --> 01:06:38.640]  которые тоже конфликтуют и одно предшествует другому.
[01:06:38.640 --> 01:06:46.640]  И для каждой дуги я могу вот такое соотношение еще извлечь.
[01:06:46.640 --> 01:06:51.640]  Но этого мало, потому что я пока нигде не использовал никакие свойства алгоритма 2PL,
[01:06:51.640 --> 01:06:58.640]  что довольно странно. Вот из первого неравенства, из первой дуги я вывел то,
[01:06:58.640 --> 01:07:03.640]  что анлок для операции O1 предшествовал локу для операции O2'.
[01:07:03.640 --> 01:07:15.640]  А для второй дуги я могу аналогично вывести, что анлок операции O2' предшествует локу
[01:07:15.640 --> 01:07:24.640]  операции O3'. А что еще? Это пока свойство просто цикла.
[01:07:24.640 --> 01:07:31.640]  А где я использовал то, что это расписание построено алгоритмом 2PL?
[01:07:31.640 --> 01:07:37.640]  Алгоритм 2PL есть такое простое свойство, что все анлоки происходят после всех локов.
[01:07:37.640 --> 01:07:44.640]  Поэтому если у нас здесь есть лок в операции O2, у нас есть транзакция O2,
[01:07:44.640 --> 01:07:53.640]  и в одном месте я беру лок для ключа операции O2', а в другом месте делаю анлок для ключа операции O2',
[01:07:53.640 --> 01:08:02.640]  то просто по построению алгоритма 2PL вот этот лок должен предшествовать вот этому анлоку.
[01:08:02.640 --> 01:08:08.640]  Ну а дальше я могу получить такое длинное длинное неравенство, длинную такую цепочку предшествований
[01:08:08.640 --> 01:08:16.640]  и получить, что вот как у меня будет выглядеть последнее соотношение?
[01:08:16.640 --> 01:08:26.640]  Анлок ОК предшествует локу O1'. И вот я начинаю с анлока операции O1,
[01:08:26.640 --> 01:08:31.640]  и получается, что некоторая операция О1 из транзакции T1 предшествует локу
[01:08:31.640 --> 01:08:39.640]  некоторой другой операции из той же транзакции T1. Вот, а это нарушение протокола 2PL.
[01:08:39.640 --> 01:08:49.640]  В нем любой лок предшествует любому анлоку, а я получил наоборот.
[01:08:49.640 --> 01:09:00.640]  Это означает, что для любого расписания, который породил такой вот планировщик,
[01:09:00.640 --> 01:09:04.640]  в графе конфликтов этого расписания не может быть циклов.
[01:09:04.640 --> 01:09:08.640]  А это означает по критерию, что такое расписание будет конфликтно сериализуемым.
[01:09:08.640 --> 01:09:11.640]  Это в свою очередь означает, что такое расписание будет U-сериализуемым.
[01:09:11.640 --> 01:09:17.640]  А это по определению сериализуемости означает, что пользователь не отличает это расписание
[01:09:17.640 --> 01:09:21.640]  от расписания, где все происходило просто подряд.
[01:09:21.640 --> 01:09:29.640]  Таким образом, 2PL гарантирует нам сериализуемость. Вот такой вот тупой протокол.
[01:09:29.640 --> 01:09:35.640]  Правда-беда. Мы в этом протоколе берем много блокировок и не отпускаем их.
[01:09:35.640 --> 01:09:40.640]  И порядок блокировок выбираем не мы, а пользователи, которые выполняют свои операции.
[01:09:40.640 --> 01:09:45.640]  Можно получить это так, что один пользователь возьмет сначала лок на ключ X,
[01:09:45.640 --> 01:09:49.640]  а сначала запишет в X, потом в Y, захочет записать в X, потом в Y.
[01:09:49.640 --> 01:09:52.640]  А другой захочет записать сначала в Y, потом в X.
[01:09:52.640 --> 01:09:55.640]  И вот мы возьмем две блокировки в разном порядке.
[01:09:55.640 --> 01:09:58.640]  Точнее, попытаемся это сделать и получим дедлок.
[01:09:58.640 --> 01:10:04.640]  Нам нужен механизм, который нас от дедлока в этом планировщике защитит.
[01:10:06.640 --> 01:10:10.640]  Предлагается делать двумя способами.
[01:10:16.640 --> 01:10:20.640]  Когда каждая транзакция стартует, давайте она выберет себе временную метку.
[01:10:20.640 --> 01:10:24.640]  Совершенно произвольным образом. Посмотрит на локальные часы.
[01:10:24.640 --> 01:10:28.640]  Нам монотонности никакой не нужны, никакие сильные свойства не нужны.
[01:10:28.640 --> 01:10:31.640]  Просто вот примерное текущее время.
[01:10:31.640 --> 01:10:35.640]  Если у нас есть две транзакции и у одной временной метки меньше, чем у другой,
[01:10:35.640 --> 01:10:40.640]  то, видимо, первая старше, чем вторая. Нам вот таких соображений общих хватит.
[01:10:40.640 --> 01:10:44.640]  А теперь как мы берем локи?
[01:10:46.640 --> 01:10:51.640]  Мы в планировщик получаем операцию какой-то транзакции.
[01:10:51.640 --> 01:10:55.640]  Эта операция хочет взять лог. Мы пытаемся его взять.
[01:10:55.640 --> 01:10:59.640]  Два варианта. Если он свободен, то продолжаем. Захватываем и продолжаем.
[01:10:59.640 --> 01:11:05.640]  Если он оказался занят, то мы сравниваем таймстэмп нашей транзакции,
[01:11:05.640 --> 01:11:10.640]  ее временную метку, с временной меткой транзакции, которая владела локом.
[01:11:10.640 --> 01:11:19.640]  И тут есть две стратегии. Одна называется ваундвейт, другая называется вейтдай.
[01:11:21.640 --> 01:11:26.640]  Они симметричны. Давайте со второй начнем.
[01:11:26.640 --> 01:11:31.640]  Или давайте с этой. Не знаю, какая мне больше нравится. Давайте с этой.
[01:11:31.640 --> 01:11:39.640]  Если мы транзакция, пытаемся взять лог, который владеет другая транзакция,
[01:11:39.640 --> 01:11:43.640]  то мы сравниваем наши таймстэмпы. Если наш таймстэмп меньше, чем у транзакции,
[01:11:43.640 --> 01:11:47.640]  которая владеет локом, то мы считаем, что мы можем ее пооборотить.
[01:11:47.640 --> 01:11:53.640]  И мы эту транзакцию отменяем, мы ее приемтим. А если мы видим, что локом владеет транзакция,
[01:11:53.640 --> 01:12:01.640]  которая старше, чем наша, то мы ждем. Пока этот лог не отпустит.
[01:12:01.640 --> 01:12:10.640]  Если младше, то мы ее отменяем. А если она старше, то есть у нее таймстэмп меньше,
[01:12:10.640 --> 01:12:16.640]  чем у нас, то мы ее ждем. Здесь мы делаем симметрично.
[01:12:16.640 --> 01:12:23.640]  Если у нас таймстэмп меньше, чем у транзакции, которая владеет локом,
[01:12:23.640 --> 01:12:30.640]  то мы ждем. А если у нас таймстэмп больше, то мы сами оборотимся.
[01:12:30.640 --> 01:12:36.640]  Давайте зафиксируем первый способ, чтобы не путаться. И я объясню, почему он гарантирует нам
[01:12:36.640 --> 01:12:41.640]  что-то хорошее. То есть, если мы видим транзакцию с младшим таймстэмпом,
[01:12:41.640 --> 01:12:51.640]  которая уже владеет нужным локом, то мы ее приемтим. А если она старше нас, то мы ее ждем.
[01:12:51.640 --> 01:12:57.640]  Во-первых, почему с такой стратегией не будет циклов, почему не будет дедлоков?
[01:12:57.640 --> 01:13:02.640]  Вот если вы помните прошлый семестр, то мы вроде бы выяснили так с трудом,
[01:13:02.640 --> 01:13:11.640]  что дедлок — это цикл в графе ожидания, где мы рисуем дуги из потока, который ждет,
[01:13:11.640 --> 01:13:19.640]  в поток, которого этот поток ждет через Mutex. Так вот, в такой стратегии мы ждем,
[01:13:19.640 --> 01:13:24.640]  только если у нас таймстэмпы монотонные. То есть у нас более молодая транзакция
[01:13:24.640 --> 01:13:29.640]  ждет более старую транзакцию. Так что цикл в графе ожидания быть не может.
[01:13:32.640 --> 01:13:37.640]  С другой стороны, почему есть прогресс? Потому что рано или поздно,
[01:13:37.640 --> 01:13:41.640]  то есть отдельные транзакции могут приемтить, она будет перезапускаться,
[01:13:41.640 --> 01:13:47.640]  пробовать снова. Но когда транзакция перезапускается, то она свой таймстэмп сохраняет.
[01:13:47.640 --> 01:13:51.640]  И рано или поздно любая транзакция становится самой старой,
[01:13:51.640 --> 01:13:55.640]  потому что все новые транзакции получают в качестве метки текущее время.
[01:13:55.640 --> 01:13:59.640]  И вот самую старую транзакцию уже никто не способен приемтить.
[01:13:59.640 --> 01:14:02.640]  Поэтому рано или поздно она выполнится.
[01:14:12.640 --> 01:14:15.640]  Ну что, идея понятна?
[01:14:18.640 --> 01:14:22.640]  Тут есть еще много нюансов, на самом деле, которые нужно обсудить.
[01:14:22.640 --> 01:14:27.640]  Ну скажем, брать эксклюзивную блокировку и на чтении, и на запись неэффективно.
[01:14:27.640 --> 01:14:32.640]  То ли можно для записи брать эксклюзивную блокировку,
[01:14:32.640 --> 01:14:35.640]  а для чтения брать разделяемую блокировку, чтобы если две транзакции
[01:14:35.640 --> 01:14:38.640]  читают один и тот же ключ, то они могли бы делать это параллельно.
[01:14:38.640 --> 01:14:44.640]  Вот легко понять, что такие изменения в алгоритме не ломают вот эти рассуждения,
[01:14:44.640 --> 01:14:49.640]  потому что блокировки для конфликтующих операций тоже будут конфликтовать,
[01:14:49.640 --> 01:14:53.640]  поэтому все равно секции не будут пересекаться.
[01:14:53.640 --> 01:14:56.640]  Но при этом мы получим чуть больше параллелизма.
[01:14:57.640 --> 01:15:04.640]  Да, вот то, что мы описали, называется на самом деле не просто 2PL, а strict 2PL.
[01:15:04.640 --> 01:15:11.640]  Вот он как раз гарантирует, что расписание будет не просто view-серилизуемым,
[01:15:11.640 --> 01:15:17.640]  оно будет strict-серилизуемым. То есть оно уважает порядок предшествования транзакций в реальном времени.
[01:15:17.640 --> 01:15:24.640]  Вообще теория транзакции, она бесконечно большая, и нам ее конечно не покрыть.
[01:15:24.640 --> 01:15:29.640]  Вот есть отдельная книжка, где тысяча страниц примерно по транзакции,
[01:15:29.640 --> 01:15:36.640]  и серилизуемость это только одна половина, там еще вторая, называется recoverability.
[01:15:36.640 --> 01:15:42.640]  Но нам это сейчас не очень важно. Вообще почему мы изучаем 2PL?
[01:15:42.640 --> 01:15:52.640]  Это очень тупой протокол, но оказывается, что в Google Spanner применяется в том числе он.
[01:15:52.640 --> 01:15:58.640]  Но для того, чтобы разобраться, как именно он применяется, нужно потрудиться.
[01:15:58.640 --> 01:16:04.640]  На самом деле в Spanner не только 2PL, конечно, там есть еще другой протокол,
[01:16:04.640 --> 01:16:09.640]  и я потрачу еще, наверное, минут 15, чтобы о нем коротко рассказать.
[01:16:09.640 --> 01:16:17.640]  Но перед этим небольшое замечание, как вот этот протокол транзакций можно совершенно неожиданным
[01:16:17.640 --> 01:16:25.640]  и очень простым, очень элегантным способом перенести на память в процессоре.
[01:16:25.640 --> 01:16:34.640]  То есть как сделать транзакции на уровне процессора и ядер?
[01:16:34.640 --> 01:16:45.640]  Вот давайте подумаем. Мы процессор, и мы хотим автомарно работать с несколькими ячейками памяти.
[01:16:45.640 --> 01:16:49.640]  Вот этот протокол требует от нас брать блокировки.
[01:16:49.640 --> 01:16:55.640]  Эксклюзивные блокировки на запись ячейки памяти и разделяемые блокировки на чтение,
[01:16:55.640 --> 01:17:03.640]  и блокировки накапливать. Вот как же нам на уровне процессора,
[01:17:03.640 --> 01:17:07.640]  вот прямо внутри процессора такие блокировки реализовать?
[01:17:07.640 --> 01:17:15.640]  Эксклюзивный и разделяемый на ячейки памяти.
[01:17:15.640 --> 01:17:22.640]  Вот оказывается, что в процессоре уже есть, уже реализованы все эти механизмы блокировок на самом деле.
[01:17:22.640 --> 01:17:26.640]  Вот если вы помните, у нас была тема про протокол гениальности кашей.
[01:17:26.640 --> 01:17:30.640]  Давайте мы посмотрим на экран.
[01:17:30.640 --> 01:17:57.640]  Протокол гениальности мы можем прочесть какую-то ячейку памяти,
[01:17:57.640 --> 01:18:05.640]  и у нас эта ячейка оказывается в состоянии, ну ладно, сложно, тут эксклюзив, shared.
[01:18:05.640 --> 01:18:13.640]  Вспомним, у нас был такой протокол, который назывался MSI, протокол к гениальности.
[01:18:13.640 --> 01:18:19.640]  Мы говорили, что у каждого ядра процессора есть свой собственный кэш,
[01:18:19.640 --> 01:18:24.640]  и в этом кэше хранятся блоки памяти, мы называли их кэшлиниями.
[01:18:24.640 --> 01:18:30.640]  Разумеется, разные ядра могут в общей ячейке памяти писать и в общей ячейке читать.
[01:18:30.640 --> 01:18:41.640]  Так вот, чтобы записать что-то в ячейку памяти, вы должны сначала захватить эксклюзивное владение над соответствующей кэшлиней.
[01:18:41.640 --> 01:18:43.640]  Это состояние modified было.
[01:18:43.640 --> 01:18:50.640]  Оно означало, что кэшлиня с этой ячейкой находится только в вашем кэше, а в других кэшах ее инвалидировали, ее сбросили.
[01:18:50.640 --> 01:19:02.640]  Если же вы читаете ячейку памяти, то вы должны снова пройти через кэш и получить эту кэшлинию с этой ячейкой в свой кэш в состоянии shared.
[01:19:02.640 --> 01:19:09.640]  Состояние shared означает, что есть другие shared кэшлини в других кэшах, в других ядер, но при этом нет ни одного modified.
[01:19:09.640 --> 01:19:17.640]  Вот modified и shared это же и есть по сути эксклюзивное владение блоком памяти и разделяемое владение блоком памяти.
[01:19:17.640 --> 01:19:25.640]  А сам протокол когерентности обеспечивает обнаружение конфликтов.
[01:19:25.640 --> 01:19:35.640]  То есть, если вы хотите в ячейку памяти что-то записать, то вы должны сначала инвалидировать все другие modified и shared в других кэшах.
[01:19:35.640 --> 01:19:48.640]  Так вот, как теперь поверх протокола когерентности в кэшах, который уже по сути реализует нам блокировки и обнаружение конфликтов, сделать транзакции?
[01:19:48.640 --> 01:20:00.640]  Когда мы начинаем транзакцию в процессоре, то все наши чтения и записи будут оставаться в нашем кэше.
[01:20:00.640 --> 01:20:09.640]  И мы к этим трем состояниям, к трем альтернативам добавим еще некоторый флажок T – транзакционность.
[01:20:09.640 --> 01:20:23.640]  Когда мы что-то пишем под транзакцией, мы пишем в кэш и получаем кэшлинию в состоянии modified, то есть она у нас только в кэше есть, плюс она транзакционная.
[01:20:23.640 --> 01:20:32.640]  Когда мы что-то читаем в транзакции, то мы получаем кэшлинию в состоянии shared и ставим на ней тоже флажок транзакционная.
[01:20:32.640 --> 01:20:52.640]  И если вдруг какое-то другое ядро решило записать что-то в ту же ячейку, что читали и писали мы, то протокол когерентности должен пойти и, скажем, все modified или все shared для этой ячейки сбросить в других кэшах.
[01:20:52.640 --> 01:21:06.640]  И когда мы получаем от протокола когерентности уведомления, что нашу кэшлинию, например, вот эту какую-то, пишет другое ядро, а у нас это кэшлиния была под транзакцией, то что мы делаем?
[01:21:06.640 --> 01:21:18.640]  Мы просто все кэшлинии, которые у нас были помечены флажком T выбрасываем из своего кэша, не только вот конкретную кэшлинию, с которой у нас случился конфликт, а и все другие.
[01:21:18.640 --> 01:21:29.640]  Вот ощущаете, я не знаю, помните ли вы хорошо протокол когерентности, может быть не помните, но он слишком прост, чтобы его заново в умении придумать.
[01:21:29.640 --> 01:21:38.640]  И вот на основе этого очень простого протокола когерентности и этой очень простой надстройки мы можем внезапно получить транзакции на уровне процессора.
[01:21:38.640 --> 01:21:51.640]  Чтобы закоммитить транзакцию, мы просто стираем флажок T. Вот это операция коммита. Все, после этого протокол когерентности гарантирует, что все наши изменения падут в память.
[01:21:51.640 --> 01:22:01.640]  Это же удивительно простая идея, удивительно изящная идея. Как воспользоваться тем, что уже в процессоре есть, чтобы в него принести транзакции?
[01:22:01.640 --> 01:22:13.640]  И более того, вот такой механизм, он уже в современных x86 реализован, и он позволяет вам делать очень интересные вещи.
[01:22:13.640 --> 01:22:24.640]  Например, этот механизм используется в библиотеке Питредс. То есть, если вы под линуксом пишете и используете 100D Mutex, то вы в качестве реализации Mutex используете Питредный Mutex.
[01:22:24.640 --> 01:22:34.640]  А в этом Питредном Mutex используется, если ваш процессор поддерживает железные транзакции, поддерживает SLOG.REGION, стирание блокировок.
[01:22:34.640 --> 01:22:51.640]  Вот, предположим, вы пишете хэштаблицу, и у вас есть две записи в хэштаблице. И вы говорите, ну я не хочу думать на еженом таблице Mutex, чтобы эти записи могли работать, чтобы параллели на сети никакой не было,
[01:22:51.640 --> 01:23:06.640]  но при этом можно было бы работать с хэштаблицей из разных потоков. Как теперь устроен этот Mutex? Вот два потока приходят к нему. Обычно один из них блокировку захватывает, другой ждет.
[01:23:06.640 --> 01:23:22.640]  Вместо этого две вставки, которые пришли в одну и ту же хэштаблицу, начинают транзакцию. Они говорят, вызывают специальную инструкцию, которая называется XBGIN.
[01:23:22.640 --> 01:23:36.640]  Начинают транзакцию и проверяют, просто читают, что лог в свободном состоянии. Если лог в свободном состоянии, то они добавили его в свой рецепт.
[01:23:36.640 --> 01:23:59.640]  То есть у них в кышах сейчас ячейка с флажком лока находится в виде кыш-линии в состоянии shared с флажком транзакционное. И все, и дальше две критические секции ваших двух вставок выполняются параллельно.
[01:23:59.640 --> 01:24:19.640]  То есть у вас Mutex, в котором критические секции выполняются параллельно. И если две эти критические секции внезапно работают с разными бакетами хэштаблицы и не пересекаются, не ловят конфликты на уровне кыша, то две эти транзакции коммитятся параллельно.
[01:24:19.640 --> 01:24:34.640]  И получается, что у вас есть STD Mutex, но две критические секции в нем выполнены физически параллельно. Но просто потому что два пилета позволяет, и протокол к игретности конфликтов не обнаружил на блокировках, на виртуальных блокировках.
[01:24:34.640 --> 01:24:51.640]  Если же вдруг какие-то операции затрагивали одни и те же бакеты, ну или просто на уровне к игретности вы получили коллизию, то транзакция пооборотится. Но в этом случае вы откатитесь и попробуете уже честно взять Mutex.
[01:24:51.640 --> 01:25:00.640]  С этим механизмом есть проблемы, потому что он не гарантирует прогресса. Т.е. транзакция может переполнить кэш, если он очень больший, и отменить сама себя.
[01:25:00.640 --> 01:25:17.640]  Или просто получить конфликт между своими же записями на уровне кэша, что довольно глупо, и пооборотится. Но тем не менее, хоть железные транзакции не могут гарантировать прогресс, они могут служить полезной оптимизацией.
[01:25:17.640 --> 01:25:32.640]  И по сути эти транзакции реализованы на основе двух наблюдений, что у нас есть два ПЛ с блокировками, и вот эти блокировки по сути и конфликт на этих блокировках уже реализованы в протоколе к игретности процессора.
[01:25:32.640 --> 01:25:46.640]  Мы лишь немного его усложняем. Реальность, конечно, намного сложнее, чем я сейчас рассказываю, но все же вы вполне можете пользоваться вот такими параллельными Mutex, параллельными критическими секциями.
[01:25:46.640 --> 01:26:00.640]  Ладно, это первая часть. Теперь я коротко, насколько у меня силы позволит, расскажу вам про альтернативный механизм реализации планировщика.
[01:26:00.640 --> 01:26:18.640]  Почему мы вообще хотим чего-то альтернативного, когда мы построили уже два ПЛ, которые гарантируют нам сериализуемость?
[01:26:18.640 --> 01:26:34.640]  Дело в том, что планировщик два ПЛ очень пессимистичный. Он для каждого чтения и для каждой записи берет блокировку, чтобы две транзакции друг от друга изолировать.
[01:26:34.640 --> 01:26:51.640]  Можно делать чуть оптимистичнее. Почему это может быть целью? Вот представим себе, что у нас транзакции, что у нас система, в которой выполняются транзакции, это какое-то огромное распределенное хранилище.
[01:26:51.640 --> 01:27:10.640]  И в этом хранилище у нас есть какие-то точечные модификации, а есть какие-то огромные чтения, где мы читаем много-много данных. То есть пусть у нас распределенная база данных, в ней есть гигантские таблицы, и некоторые таблицы настолько большие, что мы читаем их с помощью reproduce операции.
[01:27:10.640 --> 01:27:27.640]  Вот настолько они большие. И при этом мы хотим, чтобы пока мы читаем таблицу, мы получали ее в согласованном состоянии. Так вот, если мы залочим всю таблицу на чтение, пока мы будем ее читать, то никто не сможет ничего делать больше с этой таблицей.
[01:27:27.640 --> 01:27:41.640]  Это неприятно. Мы бы хотели, чтобы в нашей базе, в нашем хранилище читающие транзакции могли бы работать физически параллельно вместе с пишущими, и они бы друг друга не блокировали.
[01:27:41.640 --> 01:27:57.640]  Вот в 2PL у вас для этого есть конфликтующие блокировки на чтение, на запись. Они друг друга исключают. Читающие транзакции исключают пишущие, и наоборот, на одних и тех же ключах. Мы бы хотели, чтобы читающие транзакции могли работать параллельно с пишущими.
[01:27:57.640 --> 01:28:03.640]  Для этого мы воспользуемся альтернативным подходом, который называется изоряция снайпшотов.
[01:28:12.640 --> 01:28:25.640]  Тут нужно вспомнить вашу домашнюю работу, где вы делали реплицированное хранилище отображений с ключей и значений. У вас там была задача.
[01:28:25.640 --> 01:28:37.640]  Каждая реплика хранила простое наблюдение. Как сделать так, чтобы читающие транзакции могли работать одновременно с пишущими?
[01:28:41.640 --> 01:28:50.640]  Если пишущие транзакции меняют состояние хранилища, а читающие должны читать старые, то, видимо, хранилище должно быть мультиверсионное.
[01:28:50.640 --> 01:28:58.640]  То есть нам мало просто операций записи по ключу чтения ключа. Нам нужно, чтобы в нашем хранилище могли лежать разные версии в один и тот же момент времени.
[01:28:59.640 --> 01:29:12.640]  И вот такую задачу вы уже решали в первой домашней работе, как это сделать. Пока я буду просто считать, что у меня есть хранилище, которое поддерживает версии.
[01:29:12.640 --> 01:29:30.640]  Я начинаю с некоторой версии S0, с пустого хранилища. И каждая транзакция, которая в нашем хранилище что-то меняет, будет порождать новую версию.
[01:29:30.640 --> 01:29:40.640]  Вот это просто отдельные версии. И каждая версия будет адресована некоторой временной меткой.
[01:29:40.640 --> 01:30:01.640]  Каждая версия является иммутабельной. То есть если мы коммитим какую-то транзакцию, вот здесь был коммит, то в нашем хранилище порождается новая иммутабельная версия.
[01:30:01.640 --> 01:30:16.640]  Ну разумеется, мы не пытаемся копировать все хранилище. Разумеется, новая версия, объем, который приносит новая версия, пропорциональна количеству ключей, которые мы там перезаписали.
[01:30:16.640 --> 01:30:34.640]  Но тем не менее, конструкция такая. Теперь, как выполняются транзакции? У нас есть мульти, мульти-восточная хранилища.
[01:30:34.640 --> 01:30:56.640]  Как выполняются транзакции? Каждая транзакция, когда она стартует, выбирает себе временную метку для чтения. То есть фиксирует ту версию хранилища, относительно которого будут выполняться все чтения.
[01:30:56.640 --> 01:31:08.640]  И дальше ее читает. Вот реализация этой конструкции должна быть такой, чтобы вот каждая версия уже больше не менялась, чтобы она фиксировалась. Новые транзакции пишущие могут продать новую версию.
[01:31:08.640 --> 01:31:19.640]  Но читающие транзакции будут читать, например, вот эту. Вот мы выбрали в качестве референса вот такую версию S1. Read timestamp указывает на нее.
[01:31:19.640 --> 01:31:32.640]  А дальше, когда мы выполняем все свои записи, то мы не то чтобы в хранилище прям их пишем. Мы их буферизуем, просто запоминаем из себя, что мы сделали запись по ключу.
[01:31:32.640 --> 01:31:50.640]  Какую-то запись В1, запись В2, запись В3. И в какой-то момент мы решаем это вот. Сказал, этот шаг start transaction, раньше у нас его не было, теперь он есть и он выбирает временную метку для чтения.
[01:31:50.640 --> 01:31:58.640]  То есть версию, которую мы будем читать. Потом мы выполняем какие-то записи, накапливаем их и хотим сделать commit.
[01:32:02.640 --> 01:32:21.640]  То есть мы хотим породить новую версию. Мы каким-то образом атомарно помещаем все наши записи в хранилище и порождаем версию 3.
[01:32:21.640 --> 01:32:34.640]  Вот версия 3 это commit timestamp.
[01:32:34.640 --> 01:32:44.640]  Когда мы комитим транзакцию, мы выбираем себе вот такую временную метку и под ней порождаем новую версию хранилища. То есть под ней пишем все свои записи.
[01:32:44.640 --> 01:32:59.640]  Но с нами могла конкурировать другая транзакция. Скажем, вот здесь началась какая-то другая транзакция, которая сделала запись W1, W2, W3. И она тоже хочет закомититься.
[01:32:59.640 --> 01:33:17.640]  Давайте такое пунктирное S4. Так вот, commit может быть успешен, а может быть неуспешен. Мы скажем, что чтобы закомитить все свои записи, чтобы породить новую версию,
[01:33:17.640 --> 01:33:32.640]  мы должны убедиться, что вот начиная с этой версии, с которой мы читали, не было изменений по тем же ключам, что мы пишем.
[01:33:32.640 --> 01:33:55.640]  Если окажется, что, например, запись W3' и W2 конфликтуют, то есть они обращаются к одному и тому же ключу, то это значит, что commit транзакции, это была транзакция, допустим, T1, это транзакция T, это T'.
[01:33:55.640 --> 01:34:10.640]  Вот чтобы штрихованные транзакции не комитятся, то есть ее commit проваливается. Вот это правило называется First Committer Wins.
[01:34:10.640 --> 01:34:26.640]  Идея супер простая. Вот вы ее все хорошо знаете, потому что вам знакома система контроля версий. Вот start транзакции, это вы буквально отщепляете ветку.
[01:34:26.640 --> 01:34:35.640]  Дальше вы в своей ветке, локально, никому не говоря об этом, делаете какие-то изменения. Вы их накопили и потом решили влить их обратно в мастер.
[01:34:36.640 --> 01:34:44.640]  Но вы вливаете в мастер успешно только тогда, когда ваши изменения не конфликтуют с теми изменениями, которые за время жизни вашей ветки накопились уже в мастере.
[01:34:44.640 --> 01:34:55.640]  Вот если конфликтов нет, то изменения вливаются. Если конфликты есть, то изменения откатываются.
[01:34:55.640 --> 01:35:03.640]  Вот такая простая схема. Тут очень много вопросов, как именно это реализовать, и сегодня мы не будем им задаваться.
[01:35:03.640 --> 01:35:11.640]  Ну, например, как именно сделать версионируемое хранилище? Как именно я порождаю новые версии? Как именно они там хранятся?
[01:35:11.640 --> 01:35:17.640]  Как именно я выбираю временные метки? Потому что они должны генерироваться монотонно, это очень важно.
[01:35:17.640 --> 01:35:23.640]  Монотонно в смысле, вот в реальном времени монотонно. То есть если одна транзакция закоммитировалась раньше, чем другая началась,
[01:35:23.640 --> 01:35:30.640]  то временная метка у новой транзакции на чтение должна быть строго больше, чем временная метка на запись у старой транзакции.
[01:35:30.640 --> 01:35:32.640]  В распределенном смысле?
[01:35:32.640 --> 01:35:36.640]  Ну, поначалу в локальном, вообще распределенном, конечно.
[01:35:36.640 --> 01:35:44.640]  Ну и наконец, как мы можем взять и атомарно записать в хранилище сразу много ключей? Это тоже не очень понятно.
[01:35:44.640 --> 01:35:48.640]  Потому что это как бы проект транзакции в каком-то смысле и были.
[01:35:48.640 --> 01:35:56.640]  Ну скажем, в levelDB мы можем атомарно записать много ключей разом, а вот в распределенной кивали у хранилищ в разной шарды мы не можем уже записать.
[01:35:56.640 --> 01:36:02.640]  Но вот эти все вопросы нужно решить, и мы про них поговорим в следующий раз.
[01:36:02.640 --> 01:36:09.640]  А пока вопрос, потеряли ли мы что-нибудь здесь относительно 2PL?
[01:36:09.640 --> 01:36:19.640]  Вот у нас было 2PL, и в нем была параллельность, но на одних и тех же ключах параллельности не было. Там были локи.
[01:36:19.640 --> 01:36:28.640]  Здесь мы от локов ушли, и чтение делаем без блокировок, потому что мы перешли к мультиверсионному хранилищу.
[01:36:28.640 --> 01:36:35.640]  Собственно, в домашке вы так и делали. Вы хотели избавиться от блокировок и перешли к мультиверсионному хранилищу.
[01:36:35.640 --> 01:36:40.640]  Вот здесь та же самая идея. В смысле транзакций, потеряли ли мы что-нибудь?
[01:36:40.640 --> 01:36:50.640]  Вот например, верно ли, что этот мастер, цепочка состояний, каждый из которых соответствует новому комиту транзакций, это есть сериализация?
[01:36:50.640 --> 01:36:55.640]  Верно ли, что порядок комитов здесь, это порядок сериализации транзакций?
[01:37:05.640 --> 01:37:19.640]  Давай нарисуем пример. Наверное, ты про это же говоришь. У примера есть собственное название.
[01:37:19.640 --> 01:37:37.640]  Он называется rights queue. Транзакция T1 и T2. Транзакция T1 делает следующее. Она читает ключ X, и если в ключе X видит единицу, то пишет в Y 0.
[01:37:37.640 --> 01:37:56.640]  Транзакция T2 читает Y, и если в Y видит единицу, то пишет в X 0. Вот две эти транзакции могут конфликтовать только по своим rights set, то есть по множеству ключей, которые они пишут.
[01:37:56.640 --> 01:38:17.640]  Потому что вот только так обнаруживаются конфликты по записям. И смотрите, что может произойти. У нас было некоторое начальное состояние S0, мы от него отщепили T1, T2.
[01:38:17.640 --> 01:38:35.640]  T1 сделал запись Y 0, T2 сделал запись X 0, и после этого эти две транзакции в каком-то порядке закомитились. Конфликтов между ними не было.
[01:38:35.640 --> 01:38:54.640]  Но в итоге у вас... Давайте я пример перерисую, потому что немного тупо. У меня начальное состояние, наверное, все нули. Вот так вот.
[01:38:54.640 --> 01:39:06.640]  И вот я перешел в состояние, где все нули, в состояние, где в обеих транзакциях единица. На состояние, где в обеих ключах единица. Такое состояние не отвечает никакому порядку сериализации.
[01:39:06.640 --> 01:39:17.640]  Если выполнялось бы сначала T1, потом T2, то T2 провалилось. Если бы выполнялось наоборот, то провалилось бы T1. Поэтому одна из транзакций не должна оставить следов, а они обе оставили.
[01:39:17.640 --> 01:39:33.640]  И вот на уровне изоляции никакого конфликта при этом нет. Что довольно неприятно. Вот то, что получилось, называется словом аномалия.
[01:39:33.640 --> 01:39:47.640]  И тут есть достаточно неловкая ситуация. Давайте посмотрим на экран. Вообще базы данных существуют гораздо раньше, чем была придумана изоляция снайпшотов. Вот этот подход.
[01:39:47.640 --> 01:39:57.640]  И так исторически сложилось, что комитет по стандартизации придумал уровни изоляции транзакций. Но вот на этой картинке они тоже есть.
[01:39:58.640 --> 01:40:10.640]  Если у вас был курс по базам данных, возможно, вы что-то такое проходили. И вот гарантии в этих уровнях сформулированы очень странным образом.
[01:40:10.640 --> 01:40:24.640]  Гарантии сформулированы относительно проблем, которых не бывает. И вышло так, что снайпшот изоляция исключает и вот такой вот проблем, и вот такой вот проблем, и такой вот проблем.
[01:40:24.640 --> 01:40:30.640]  То есть по принципу исключения она исключает все аномалии, которые были стандартизированы, поэтому является стерилизуемым.
[01:40:30.640 --> 01:40:43.640]  Стерилизуем по логике. По логике определения. Но при этом нет. Существует сценарий новый, которым этот подход дает нам нестерилизуемое расписание.
[01:40:43.640 --> 01:40:52.640]  Так что, когда вы читаете, скажем, в документации про базу данных слово serializable, то подумайте, что оно значит.
[01:40:52.640 --> 01:40:59.640]  Оно значит это безумное определение из древнего стандарта или оно значит serializability в математическом смысле.
[01:40:59.640 --> 01:41:05.640]  Вот кажется, что любая современная система использует serializable именно в математическом смысле.
[01:41:05.640 --> 01:41:11.640]  Что существуют серийные расписания, такое, что оно view эквивалентно нашему.
[01:41:11.640 --> 01:41:20.640]  Но вот тем не менее для изоляции снайпшотов возможны вот такие странные сценарии. С другой стороны, мы можем читать без блокировок.
[01:41:20.640 --> 01:41:28.640]  Давайте я все-таки кое-что успею сказать напоследок про реализацию. Как выбирать таймстемпы, обсудим в следующий раз.
[01:41:28.640 --> 01:41:35.640]  Как делать коммит атомарно и выполнять такую проверку сложную, обсудим тоже в следующий раз.
[01:41:35.640 --> 01:41:41.640]  А вот по поводу того, как сделать мультиверсионное хранилище.
[01:41:41.640 --> 01:41:53.640]  Вот пусть у нас уже есть хранилище, но которое не мультиверсионное. В нем есть операция put по ключевому значению и операция get.
[01:41:53.640 --> 01:42:00.640]  Ну и может быть еще есть снайпшоты, плюс итераторы.
[01:42:06.640 --> 01:42:16.640]  Вроде бы такое API предполагает, что когда вы мутируете состояние базы данных, то вы его перезаписываете, ставлю ключ на новый.
[01:42:16.640 --> 01:42:22.640]  Но если вы делаете снайпшоты, то вы можете получить снайпшоты на просто текущей версии.
[01:42:22.640 --> 01:42:29.640]  Собственно levelDB вам такое API предоставляет, мы это помним. Давайте на экране покажу еще раз.
[01:42:29.640 --> 01:42:40.640]  Вот API levelDB, вы можете взять снайпшот и по нему итерироваться, и другие потоки могут в это же время базу данных менять, но вы эти изменения не увидите.
[01:42:40.640 --> 01:42:43.640]  Но вы не хотите их видеть, вы хотите иметь фиксированный снайпшот.
[01:42:43.640 --> 01:42:56.640]  Так вот, если ваша база данных, ваше хранилище локальное умеет вот такие операции, то вы поверх него можете сделать мультиверсионное хранилище довольно несложным образом.
[01:42:56.640 --> 01:43:02.640]  Вы можете сказать, что put. Собственно, что значит мультиверсионное хранилище?
[01:43:13.640 --> 01:43:25.640]  Это означает, что у вас есть операция put, ключ k, значение v, временная метка ts.
[01:43:25.640 --> 01:43:35.640]  И у вас есть операция get, прочесть по ключу k максимальную версию не старше, чем временная метка ts bound.
[01:43:35.640 --> 01:43:47.640]  И когда я говорю, что в снайпшот-изоляции мы порождаем новую версию базы, разумеется, мы не то, что мы порождаем новую версию.
[01:43:47.640 --> 01:44:00.640]  Вот у нас было хранилище, где были ключи x и y, и под версией 1 в ключе x хранилось буква a, а в ключе y хранилось под версией 2 буква b.
[01:44:00.640 --> 01:44:14.640]  И мы делаем commit двух ключей x и y под timestamp 5. Пишем сюда какой-нибудь а штрих, b штрих.
[01:44:14.640 --> 01:44:25.640]  Вот мы просто добавляем по ключу k значение с новой временной меткой 5.
[01:44:25.640 --> 01:44:33.640]  А при этом, если другая транзакция читает что-то, то она читает из снайпшота с помощью такого чтения.
[01:44:33.640 --> 01:44:45.640]  Вот мы, например, делаем get по ключу x с ограничением на временную метку ts bound 3.
[01:44:45.640 --> 01:44:59.640]  Вот мы фактически читаем вот такой снайпшот. Мы читаем по ключу x значение a, по ключу y значение b.
[01:44:59.640 --> 01:45:11.640]  Вот имея хранилище с такой семантикой, вы можете сделать операции вот с такой семантикой.
[01:45:11.640 --> 01:45:19.640]  Тут, конечно, нужна аккуратность, как именно вы в это хранилище атомарно записываете сразу много ключей, но это отдельная история.
[01:45:19.640 --> 01:45:27.640]  Но, в принципе, вы в домашней работе уже вот такое преобразование делали.
[01:45:27.640 --> 01:45:33.640]  Вы, конечно, можете сделать его не только в локальном масштабе, а на уровне распределенного киварю хранилища.
[01:45:33.640 --> 01:45:45.640]  Так вот, нужно подвести итог, на чем-то остановиться. Мы, конечно, не разобрали, как устроен снайпшот isolation на уровне реализации.
[01:45:45.640 --> 01:45:51.640]  Но мы, в принципе, понимаем, как схема работает, с какими гарантиями.
[01:45:51.640 --> 01:46:01.640]  Мы понимаем, что гарантии у этого подхода слабее, чем у 2PL. Но у каждого из этих подходов есть свое преимущество.
[01:46:01.640 --> 01:46:07.640]  2PL гарантирует строгую сериализуемость, но это пессимистичный подход.
[01:46:07.640 --> 01:46:13.640]  В нем читающие и пишущие транзакции, обращающиеся к одним и тем же ключам, не могут работать параллельно.
[01:46:13.640 --> 01:46:17.640]  Здесь могут, но ценой потери сериализуемости.
[01:46:17.640 --> 01:46:25.640]  Вот в следующий раз мы посмотрим на три примера. Мы посмотрим на Google Bigtable.
[01:46:25.640 --> 01:46:35.640]  И как Google для своего индексатора интернета, поверх этой системы без транзакций, построил транзакции, реализованные на клиенте вообще снаружи системы,
[01:46:35.640 --> 01:46:41.640]  с помощью вот такого подхода. Им таких гарантий хватило, которые дают изоляция снайпшотов.
[01:46:41.640 --> 01:46:47.640]  А еще мы посмотрим на систему Google Spanner.
[01:46:47.640 --> 01:46:53.640]  И увидим, что Спандеру хочется уметь чтения и снайпшотов.
[01:46:53.640 --> 01:46:58.640]  Но при этом им хочется еще гарантировать и сериализуемость.
[01:46:58.640 --> 01:47:04.640]  Так вот, Spanner умудряется совместить два этих подхода, как мы в следующий раз увидим.
[01:47:04.640 --> 01:47:10.640]  То есть нам полезно знать и про этот подход, который более эффективный, но обладает слабыми гарантиями,
[01:47:10.640 --> 01:47:16.640]  и про этот подход, который более ограничительный для реализации, но при этом дает более сильные гарантии.
[01:47:16.640 --> 01:47:23.640]  И если правильно их скомбинировать, то мы получим что-то такое же эффективное, как Snapshot Isolation,
[01:47:23.640 --> 01:47:28.640]  и в то же время что-то настолько же корректное, как и 2PL.
[01:47:28.640 --> 01:47:34.640]  И вот наконец мы с вами способны будем прочесть статью про Google Spanner.
[01:47:34.640 --> 01:47:38.640]  Собственно, это моя просьба к следующему занятию.
[01:47:38.640 --> 01:47:44.640]  Мы с вами уже знаем про Google TrueTime и умеем им пользоваться для генерации монотонных временных меток.
[01:47:44.640 --> 01:47:50.640]  Это к вопросу о том, как генерировать временные метки для транзакций в этом подходе.
[01:47:50.640 --> 01:47:55.640]  Мы с вами знаем про репликацию через Multipax, если вы сейчас это пишете.
[01:47:55.640 --> 01:48:01.640]  Мы знаем про распределенную файловую систему Google, это Colossus.
[01:48:01.640 --> 01:48:08.640]  Мы знаем про два подхода к изоляции транзакций, 2PL и изоляция Snapshots.
[01:48:08.640 --> 01:48:15.640]  И вот это все преликвидиты, необходимые для того, чтобы статью про Spanner почитать теперь.
[01:48:15.640 --> 01:48:19.640]  Потому что в этой статье, написанной в 2013 году уже достаточно давно,
[01:48:19.640 --> 01:48:23.640]  от читателя понимание всех этих вещей требуется.
[01:48:23.640 --> 01:48:27.640]  И в предположении, что читатель это все понимает по-настоящему,
[01:48:27.640 --> 01:48:33.640]  авторы статьи могут ему рассказать, как именно в их системе реализованы транзакции,
[01:48:33.640 --> 01:48:36.640]  как именно они сочетают два этих подхода.
[01:48:36.640 --> 01:48:40.640]  В следующий раз мы про все это поговорим, а еще про альтернативный подход,
[01:48:40.640 --> 01:48:44.640]  который используется в Яндексе, транзакции типа Kelvin.
[01:48:44.640 --> 01:48:51.640]  И вот тогда уже через неделю мы обсудим все нюансы реализации именно распределенных транзакций.
[01:48:51.640 --> 01:48:56.640]  Ну что ж, а на сегодня тогда все. Спасибо большое, что задержались.

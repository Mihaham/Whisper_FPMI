[00:00.000 --> 00:11.400]  Так, ну что, окей, начинаем. Сегодня мы переходим тоже к одному из таком последнему разделу в
[00:11.400 --> 00:17.200]  нашем курсе. Это статистическая оптимизация. До этого мы с вами рассматривали задачи,
[00:17.200 --> 00:22.600]  где все вычисления, которые мы делали, были детерманистическими, честный градиент,
[00:22.600 --> 00:30.120]  честное значение функции, честный гисиан. Сейчас, соответственно, мы от этого оттолкнемся.
[00:30.120 --> 00:37.600]  Такую задачку, соответственно, рассматривали. Теперь давайте эту формулировку чуть-чуть
[00:37.600 --> 00:44.360]  модифицируем и запишем нашу целевую функцию вот в таком вот виде. Пусть у нас целевая функция,
[00:44.360 --> 00:51.320]  это некоторое математическое ожидание, какой-то случайный величины кси по распределению d.
[00:51.320 --> 00:58.280]  Нашу целевую функцию теперь зависит от двух аргументов, от x и от x. Но вообще в таком формате,
[00:58.280 --> 01:03.320]  конечно, непонятно, что тут записано, откуда такие проблемы вообще возникают. Поэтому давайте
[01:03.320 --> 01:13.000]  самый популярный пример – это машинное обучение, где у нас такого рода задачки возникают прям на
[01:13.000 --> 01:20.880]  каждом шагу. С машинным обучением, я так понимаю, судя по вчерашнему семинару с Сашей, вы знакомы,
[01:20.880 --> 01:27.720]  частично, хоть как-то. Соответственно, что у вас там в машинном обучении есть? Ну, или в домашнем
[01:27.720 --> 01:34.480]  задании вы тоже это чуть-чуть проделывали. Что вы там обучаете? Моделька. А что у этой модели есть?
[01:34.480 --> 01:41.000]  Что она может принимать на вход? Данные. В чем суть? У вас есть некоторая модель – это какой-то
[01:41.000 --> 01:45.240]  черный ящик. Может быть не черный, но может быть какая-нибудь совсем простая линейная модель,
[01:45.240 --> 01:49.400]  которая у вас там была в домашнем задании, то есть просто линейная зависимость. А может быть какая-то
[01:49.400 --> 01:55.440]  нейросеть, которая состоит из кучи блоков. Суть такая, что вы на вход этой нейросети можете
[01:55.440 --> 02:02.440]  подать некоторый объект X, ксет X. Это может быть картинка, текст или просто какой-то вектор признаков,
[02:02.440 --> 02:10.080]  в том числе описание грибов, с которым вы три задания подряд работали. В этой модели можно
[02:10.080 --> 02:18.880]  подкручивать веса, настраивать модель, чтобы она, получая объект на вход, выдавала какой-то
[02:18.880 --> 02:25.120]  предсказание. Настоящее предсказание у вас, соответственно, обозначается как xy. Ну и что мы
[02:25.120 --> 02:30.880]  делаем? Мы делаем и штрафуем нашу модель, ее предсказание, если они сильно отличаются,
[02:30.880 --> 02:37.960]  ну или просто отличаются от реального значения метки xy. Ну не угадал, например, модель ваш,
[02:37.960 --> 02:44.720]  что изображено на картинке, не угадал, не угадал модель ваш тональность текста, там есть отрицательный
[02:44.720 --> 02:49.800]  отзыв или положительный, вы, соответственно, как-то ее за это все безобразие штрафуйки. В домашнем
[02:49.800 --> 02:54.120]  задании, соответственно, у нас в первом задании были квадратичные штрафы, здесь у нас, соответственно,
[02:54.120 --> 03:01.320]  во втором задании, третьем, четвертом сигмойная функция, логистическая функция штрафа. Ну в общем-то,
[03:01.320 --> 03:06.000]  в любом случае, задачи формулируются вот таким вот образом. Откуда здесь берется математическое
[03:06.000 --> 03:12.360]  ожидание? Но у вас есть некоторая природа, откуда приходят данные. Эта природа обозначается в данном
[03:12.360 --> 03:19.480]  случае распределением d. Ну из этой природы вы сэмплите какие-то объекты, то есть это вот картинки
[03:19.480 --> 03:26.880]  ваши и что на них изображено, то есть пара xy и xx. Вы хотите что сделать? Вы хотите, чтобы ваша модель
[03:26.880 --> 03:33.120]  была в некотором смысле хорошим аппроксиматором вот той природы, которая наблюдается в этой
[03:33.120 --> 03:40.840]  задачке, которая генерируется из d. Поэтому вы хотите, чтобы ожидание, то есть некоторое взвешенное,
[03:40.840 --> 03:50.120]  среднее по всем сэмплам из этой природы, качество аппроксимации этой модели было минимальным.
[03:50.120 --> 03:58.000]  Вся суть. Машинное обучение подстроится под вот эту природу d и минимизировать вот это ожидание.
[03:58.000 --> 04:07.040]  Минимизировать вот это ожидание. Здесь понятно, да? Все супер. Хорошо, вот это мы только что обсудили,
[04:07.040 --> 04:13.240]  аппроксимировать зависимость между xy и xxa. Какие проблемы вы видите вот в такой формулировке,
[04:13.240 --> 04:21.200]  которую я вам рассказал? Тяжело считать, а часто вообще невозможно, потому что суть машинного
[04:21.200 --> 04:26.600]  обучения это подстроится под распределение d, которое вы скорее всего просто не знаете. Вот,
[04:26.600 --> 04:30.960]  либо оно действительно просто, ну как-то сложно считается. Ну какое распределение на картинках? Ну оно
[04:30.960 --> 04:35.360]  формально есть какое-то. Какое распределение на текстах? Оно тоже какое-то есть, но нужно его в
[04:35.360 --> 04:41.840]  некотором смысле просто аппроксимировать. Даже люди не особо задумывают, чтобы его искать. Его нужно
[04:41.840 --> 04:48.080]  просто в некотором смысле хорошо повторить. Повторить с помощью каких-то простых блоков,
[04:48.080 --> 04:54.560]  ну и модели, собранные из этих простых блоков. Распределение f вы не знаете, поэтому в реальности
[04:54.560 --> 05:00.800]  вот этот интеграл, который у вас в математическом ожидании зашит, его невозможно посчитать часто.
[05:00.800 --> 05:07.640]  То есть и значение функции, и значение интеграла, это вещи в данном случае не берущиеся. Поэтому
[05:07.640 --> 05:16.480]  хочется как-то отстолкнуться от той постановки, что у нас есть, и разрешить в методах использовать
[05:16.480 --> 05:22.000]  не честные градиенты, честные значения функций, честные значения гессианов, а их стокастические
[05:22.000 --> 05:29.280]  версии. В чем суть? Ну, например, поступает нам какой-то сэмпл из природы данных, ну по-другому
[05:29.280 --> 05:33.800]  никак, если у нас никаких данных не будет из этой природы, как мы что-то под нее будем подстраивать.
[05:33.800 --> 05:39.680]  Поступил сэмпл, мы в онлайн режиме можем посчитать соответственно функцию потери уже конкретно на
[05:39.680 --> 05:45.840]  этом сэмпле. Просто как раз взять эту функцию потери, взять модель, продеференцировать, получить
[05:45.840 --> 05:51.920]  конкретный градиент, но это будет не тот градиент честный, а то как на конкретном сэмпле кси, который
[05:51.920 --> 05:56.460]  мы с вами получили. Вот. Единственное хорошее предположение, которое мы можем сделать в данном
[05:56.460 --> 06:02.020]  случае, это сказать то, что у нас вот тот стокастический градиент на сэмпле конкретном,
[06:02.020 --> 06:10.500]  который к нам прилетел, он будет не смещенным. Ну понятно, потому что объекты нам прилетают из
[06:10.500 --> 06:18.420]  природы, поэтому в среднем мы действительно будем иметь честный градиент. Понятно, что какие-то
[06:18.420 --> 06:23.380]  более сложные случаи, когда у нас вдруг возникает какой-то байс при сэмплировании, рассматривать,
[06:23.380 --> 06:28.780]  ну просто слишком сложно, потому что если есть байс, то кажется нарушается какая-то природа,
[06:28.780 --> 06:35.540]  и вы сэмплируете просто не из этого распределения, из какого-то другого. Поэтому здесь понятно
[06:35.540 --> 06:39.900]  предполагаем, что из этого распределения нам прилетают такие объекты, и мы по ним считаем
[06:39.900 --> 06:48.860]  градиенты. Вот. Окей. Но часто в вашем машинном обучении, в том числе в вашем, в домашних заданиях вы
[06:48.860 --> 06:55.260]  сталкивались вот с такой постановкой, то есть там не было интегралов, там была какая-то конечная
[06:55.260 --> 06:59.900]  сумма. Вот такого вида, что та же самая функция потерь, та же самая модель, те же самые веса,
[06:59.900 --> 07:05.660]  и вот объекты некоторые из этого распределения D, которые вам уже были изначально даны. Вот.
[07:05.660 --> 07:11.420]  Такая постановка называется off-line постановкой, потому что в онлайн варианте вы как бы начинаете
[07:11.420 --> 07:16.380]  абсолютно с нуля, а у вас нет ничего, у вас нет никаких данных, вам просто говорят, ну сейчас
[07:16.380 --> 07:21.180]  начну данные поступать, давай в режиме реального времени начнем их обрабатывать, обучать нашу модель.
[07:21.180 --> 07:26.340]  Такие постановки бывают, и это довольно общие постановки, когда вы предполагаете, что у вас
[07:26.340 --> 07:33.300]  просто ничего нет. В онлайн постановке вам соответственно уже дана какая-то выборка довольно
[07:33.300 --> 07:41.420]  большого размера часто из нужного вам распределения D. Но возникает вопрос, а как вот такая постановка,
[07:41.420 --> 07:47.300]  которая записана здесь, вообще связана с изначальной постановкой. Изначальная постановка,
[07:47.300 --> 08:00.260]  которая была в виде математического ожидания. Да, все тут-тут почти правильно, потому что на самом деле,
[08:00.260 --> 08:06.700]  как вы знаете, интеграл можно считать нечестно. Нечестно, не знаю, не знаете, не знаете, не знаете,
[08:06.700 --> 08:12.580]  там самые простые способы. Те суммы дарбу, которые вы считали, это один из вариантов. А можно,
[08:12.580 --> 08:21.100]  например, считать интегралы с помощью сэмплирования. И всякого рода подходы рандомизированные,
[08:21.100 --> 08:26.740]  основанные на сэмплировании, называются Монте-Карло подходами. Ну, например, какой-нибудь примерчик
[08:26.740 --> 08:30.620]  сейчас. Монте-Карло подходы на самом деле разные бывают, я какой-нибудь простенький приведу,
[08:30.620 --> 08:35.180]  чтобы было понятно. Ну вот есть у меня какой-то интеграл, я хочу посчитать его плотно, площадь под
[08:35.180 --> 08:40.220]  графиком получается. Но я начну сэмплировать в этот квадратик, ну и как часто у меня будет попадать
[08:40.220 --> 08:46.780]  под кривую. В таком процентном соотношении у меня значение, ну частоты попадания под кривую
[08:46.780 --> 08:53.580]  будут стремиться к интегралу. Ну площадь получается площадь вот этой кривой, площадь интегралов,
[08:53.580 --> 08:58.100]  которые находятся под графиком, можно будет найти как отношение, умножив просто на площадь
[08:58.100 --> 09:03.100]  того квадрата, в который я сэмплирую. Вот. Монте-Карло подход. Здесь то же самое. Вы сэмплируете
[09:03.100 --> 09:10.180]  данные из распределения, довольно большое количество. И говорите, что насэмплировав
[09:10.180 --> 09:14.500]  вот из этого распределения, усреднив вот эти потери по всем точкам, которые я наделал,
[09:14.500 --> 09:20.500]  у меня это будет неплохо так опроксимировать реальный интеграл. Вариант-вариант действительно
[09:20.500 --> 09:26.660]  можно показать, что при большом n и при некоторых условиях на функцию потери на модель это всё
[09:26.660 --> 09:32.780]  безобразие действительно будет неплохо опроксимировать исходную задачу. Вот. Поэтому часто действительно
[09:32.780 --> 09:38.460]  на практике, да и не только задачу обучения подменяют, ну вот общую задачу обучения,
[09:38.460 --> 09:42.860]  которую мы видели сами до этого, подменяют вот такую вот задачу вида конечной суммы, уходят
[09:42.860 --> 09:50.420]  от интеграла. Вот, это мы с вами обсудили. Вот, у меня такой вопрос тогда возникает. Кажется,
[09:50.420 --> 09:57.940]  что теперь в офлайн-постановке мы полный градиент вроде как считать можем. Вот, но почему мы это не
[09:57.940 --> 10:04.940]  делаем? То есть машинное обучение, думаю, вы знаете, что полные градиенты люди часто не считают. Почему?
[10:04.940 --> 10:12.900]  Дорого, дорого и долго. Дорого и долго. На самом деле, это первая проблема, почему это не делают.
[10:12.900 --> 10:18.940]  Выборка большая, посчитать полный градиент, даже распределено то, что вам вчера рассказывал на
[10:18.940 --> 10:25.260]  семинарию, это очень дорого. То есть даже в распределенном сетапе мы считаем стахастические
[10:25.260 --> 10:29.180]  градиенты, то есть по куску данных. То как выбираем какой-то случайный кусочек данных,
[10:29.180 --> 10:34.780]  ну или не случайный, вот, и по ним считаем градиент. Даже там, используя 100 вычислителей,
[10:34.780 --> 10:43.140]  мы полный градиент не считаем. Вот. Ну, этого есть на самом деле и не только какие-то подходы с
[10:43.140 --> 10:48.620]  точки зрения дороговизны, с точки зрения машинного обучения. Иногда вот такой подход стахастически
[10:48.620 --> 10:53.860]  добавляет вам рабасности, потому что стахастик это в любом случае какие-то асцеляции, которые мы
[10:54.060 --> 10:59.640]  увидим, которые добавляют случайность в ваш процесс обучения. Чтобы не переобучиться,
[10:59.640 --> 11:04.460]  а с точки зрения именно машинного обучения это важно, с точки зрения оптимизации как раз
[11:04.460 --> 11:08.980]  переобучение это всё хорошо, это вы нашли точный минимум задачи, которые вы ставили. А,
[11:08.980 --> 11:14.060]  с точки зрения машинного обучения это не очень хорошо, поэтому стахастика может быть даже
[11:14.060 --> 11:17.820]  играть какую-то хорошую роль с точки зрения именно машинного обучения, но не оптимизации.
[11:17.820 --> 11:25.580]  Вот, окей, поэтому вот сегодня мы с вами будем рассматривать вот такой вот сетап,
[11:25.580 --> 11:31.720]  когда у нас вместо полного градиента будет вычисляться градиент по случайному
[11:31.720 --> 11:38.380]  сэмплу xe, который независимо и равномерно генерируется вот из нашего вот этого
[11:38.380 --> 11:44.980]  распределения. Либо из D, прямо говорим, что он генерируется, какой-то сэмпл D,
[11:44.980 --> 11:51.540]  либо, если мы уже говорим про finite самопостановку, то он будет генерироваться просто равномерно из тех
[11:51.540 --> 11:57.740]  индексов, которые у нас есть. Тогда нам уже не нужно ждать, что придет что-то из распределения,
[11:57.740 --> 12:05.340]  мы просто из того, что есть, будем минимизировать. Вот, окей, здесь постановка понятна. Все, супер,
[12:05.340 --> 12:11.820]  тогда идем дальше. Опять же, очень просто модифицируется метод, с которым мы с вами
[12:11.820 --> 12:17.540]  работали. Самый первый метод, который у нас есть – градиентный спуск. Пожалуйста, в прошлый раз мы
[12:17.540 --> 12:24.220]  добавили в него субградиенты, разрешили считать субградиенты вместо честных градиентов и
[12:24.220 --> 12:30.500]  минимизировать негладкие задачи. Здесь мы говорим, что теперь давайте вместо полного градиента,
[12:30.500 --> 12:36.820]  честного детерминистического, я буду использовать какую-то его стахастическую версию. Каждый раз,
[12:36.820 --> 12:43.220]  ну, для простоты здесь, соответственно, предполагается, что мы генерируем случайность
[12:43.220 --> 12:48.260]  независимо. Ну и, как на прошлом слайде было сказано, как-то равномерно. То есть, это приходит
[12:48.260 --> 12:56.100]  либо из D равномерно, либо приходит из номеров от 1 до N тоже равномерно. Вот, получается вот такой
[12:56.100 --> 13:01.820]  вот метод. Ничего такого сверхъестественного у него нет. Давайте попробуем подоказывать его
[13:01.820 --> 13:08.500]  сходимость. Но перед тем подоказывать его сходимость, ведем вспомогательный объект,
[13:08.500 --> 13:13.900]  который называется условным математическим ожиданием. Работали с ним на тервере, супер,
[13:13.900 --> 13:21.380]  вводили его, водится не так тривиально. Здесь, поэтому, я надеюсь, вы понимаете его суть. И по факту,
[13:21.380 --> 13:26.740]  здесь просто нужно понимать, что, чтобы анализировать поведение метода на текущей
[13:26.740 --> 13:32.820]  итерации, я буду фиксировать всю случайность, которая у меня произошла до этой итерации. Вот,
[13:32.820 --> 13:39.260]  то есть, я вот это условно-математическое ожидание порождаю сигма-алгеброй, которая,
[13:39.260 --> 13:45.260]  соответственно, сама порождается, начальная точка, которая может выбираться как-то рандомизированно,
[13:45.260 --> 13:52.300]  плюс теми случайностями, которые у меня произошли до этого, до текущей итерации в этом методе. То есть,
[13:52.300 --> 13:57.860]  те кси-иты, которые я генерировал в ходе работы метода. Вот, эту случайность я фиксирую,
[13:57.860 --> 14:03.420]  как вы понимаете, в условно-математическом ожидании. На время, понятно. И, соответственно,
[14:03.420 --> 14:07.460]  все это безобразие начинает ожидать только по той случайности, которая у меня произойдет на этой
[14:07.460 --> 14:14.500]  итерации. Вот. Ну, и у меня к вам вопрос просто на такое вот понимание. Вот это условно-математическое
[14:14.500 --> 14:20.180]  ожидание, что, какого рода объект возвращает? Детерминистический или случайный? Случайный.
[14:20.180 --> 14:24.660]  Потому что, как только вы выходите за пределы этого математического ожидания, вы его посчитали,
[14:24.660 --> 14:29.620]  у вас то, что вылезло на наружу, теперь зависит от вот этих всех случайных величин,
[14:29.620 --> 14:33.980]  поэтому это случайный объект. Чтобы это сделать, соответственно, не случайным, тут нужно просто
[14:33.980 --> 14:38.620]  это все приравнять каким-то значением конкретным. Тогда это у вас уже будет какая-то детерминистическая
[14:38.620 --> 14:43.180]  функция, зависящая от этих объектов, но это можно и не делать. Это, скорее, такие формальные вещи,
[14:43.180 --> 14:49.140]  которые вам объясняли, когда вы эти все объекты вводили сами по себе. Там вы, скорее всего, вводили
[14:49.140 --> 14:53.860]  сначала от ожидания относительно разбиения условное, потом относительно сигма-алгебры,
[14:53.860 --> 14:58.300]  потом относительно случайной величины. Здесь сигма-алгебра просто нужна, но главное понимать
[14:58.300 --> 15:02.900]  суть, что мы фиксируем всю предысторию и говорим, что пока мы будем смотреть на только ту случайность,
[15:02.900 --> 15:31.420]  которая у нас была до этого. Не, это в жизни у вас может возникать, то есть если мы не говорим про
[15:31.980 --> 15:35.980]  постановку, когда вы просто можете строчки в матрице выбирать, которые соответствуют сэмплу.
[15:35.980 --> 15:41.700]  Ну вот в Reinforcement Learning, про которые будет сегодня рассказывать Никита, онлайн-постановки
[15:41.700 --> 15:46.980]  супер популярны, потому что там у вас никакой выборки нет, вы взаимодействуете со средой. Среда,
[15:46.980 --> 15:52.540]  с которой вы можете делать какие-то действия. Это просто игра. То есть вы можете выбирать какое-то
[15:52.540 --> 15:57.900]  действие и говорить, давайте я попробую вот это действие. Что мне выдаст среда? Даст какой-то бонус
[15:57.900 --> 16:04.300]  или скажет, что я, например, сделал неправильно и она меня оштрафует. И в среде, понятно, зашита
[16:04.300 --> 16:09.700]  какая-то случайность. И там вы конкретно взаимодействуете просто со случайными объектами.
[16:09.700 --> 16:15.140]  И никакой выборки у вас нет. Более того, ждать, когда вы накопите эту полную выборку, просто
[16:15.140 --> 16:19.940]  бессмысленно. Вам нужно уже подстраивать алгоритм в режиме реального времени. Копить эту выборку
[16:19.940 --> 16:25.180]  может уходить дни, потому что среда может быть разная в Reinforcement Learning, она может вам день
[16:25.180 --> 16:31.420]  отвечать. Это могут быть какие-то отклики клиентов на конкретный ваш. Вы там подобрали им какую-то
[16:31.420 --> 16:36.700]  выборку музыки, они вам дали отклик. Поставили эти оценки, понятно, которые зависят от их настроения,
[16:36.700 --> 16:45.020]  зависят от их конкретного текущего состояния. Они случайные. При этом, понятно, отклик вам
[16:45.020 --> 16:49.660]  приходит немгновенно. И вам нужно это адаптировать в реальном времени. Поэтому здесь вот все эти игрушки
[16:49.660 --> 16:54.220]  Reinforcement Learning это все онлайн постановки. Понятно, что часто возникают и оффлайн постановки,
[16:54.220 --> 16:58.620]  когда выборка есть. Там, понятно, случайность просто генерируется, вы случайно выбираете индекс.
[16:58.620 --> 17:03.740]  В вашем случае, там вот эта матрица, датасета грибы, берете строчку, которая соответствует конкретному
[17:03.740 --> 17:12.460]  грибу, и по ней считаете градиент вместо полной матрицы. Вот, так, случайная величина. И вот такое
[17:12.460 --> 17:19.340]  еще свойство, так называемое Tower Property, не знаю, с ним знакомились или нет, то что у вас полное
[17:19.340 --> 17:24.620]  математическое ожидание сжирает просто условное математическое ожидание. Знакомо, да, с ним?
[17:24.620 --> 17:30.820]  Супер. Это прям хорошо, обычно его приходится объяснять. Вот. Тоже, на самом деле, супер естественное
[17:30.820 --> 17:36.180]  свойство, потому что случайная величина y, ну, можно ее рассматривать как в некотором смысле
[17:36.180 --> 17:44.180]  параметризацию случайной величины x, и вы сначала берете ожидание по y, потом берете уже условное
[17:44.180 --> 17:49.060]  ожидание по y, потом уже берете, то есть из x вы как бы уходите, потом берете ожидание по y,
[17:49.060 --> 17:53.900]  это по факту эквивалентно, потому что вы берете по x. Не знаю, мне какой-то пример нравится в духе того,
[17:53.900 --> 18:00.100]  что вы считаете условно-математическое ожидание, ну, например, считаете средний вес всего населения
[18:00.100 --> 18:05.420]  Земли, предполагая, что это некоторая случайная величина. Вот. Ну, и вот это просто изначально вот
[18:05.420 --> 18:10.420]  такое значение, а потом вы говорите, что давайте, я еще знаю случайную величину, такую как рост
[18:10.420 --> 18:15.180]  человека. Понятно, что x и y, при этом это коррелированные случайные величины. Понятно,
[18:15.180 --> 18:19.660]  что если человек выше, то средний его вес, скорее всего, просто больше, чем у человека,
[18:19.660 --> 18:26.300]  который просто пониже. Вот. Ну, и вы сначала говорите, что давайте я посчитаю условно-
[18:26.300 --> 18:31.700]  математическое ожидание, когда у меня есть вот вес при условии какого-то y, но когда вы потом
[18:31.700 --> 18:37.300]  это заново просуммируете уже, возьмете полно-математическое ожидание, получится ровно то же самое. То есть,
[18:37.300 --> 18:42.340]  тут неважно, как считать средний вес. Просто средний вес или сначала с учетом роста, а потом уже
[18:42.340 --> 18:49.140]  и эту зависимость от роста поглотить еще одним математическим ожиданием. Вот. От физика тут очень
[18:49.140 --> 18:55.780]  понятно этого всего безобразия, поэтому идем дальше. Давайте подоказываем сходимость. Что хочется
[18:55.780 --> 19:00.580]  предположить? Опять же, первое предположение, это у нас стандартное наше предположение, то, что у
[19:00.580 --> 19:06.500]  нас стахастический градиент несмещенный, обговорили, что понятно, без него будет жить очень сложно.
[19:06.500 --> 19:12.820]  Без него будет жить очень сложно, и за пределы выходить не хочется. Второе предположение, то, что у меня
[19:12.820 --> 19:19.540]  дисперсия стахастического градиента ограничена. Не самое часто физичное свойство, но давайте пока
[19:19.540 --> 19:26.020]  оставим ее, она такая базовая, почему бы его не рассматривать. Окей, дальше я делаю ровно все те же
[19:26.020 --> 19:33.900]  манипуляции. Первая манипуляция, она очень простая. Так, градиент воспуска, подставил, расписал квадрат.
[19:33.900 --> 19:40.540]  Дальше, соответственно, нужно уже что-то делать, потому что до этого мы с вами, мы знаем только то,
[19:40.540 --> 19:48.700]  что у меня функция f гладкая и мюсильно выпуклая. Но здесь у нас функции f так-таковой нету, поэтому ее
[19:48.700 --> 19:54.380]  нужно сначала добыть из того, что у нас написано. Но чтобы добыть ее, давайте возьмем условное
[19:54.380 --> 20:00.380]  математическое ожидание, я же не зря его вводил. Относительно давайте как раз x, фиксирую всю
[20:00.380 --> 20:06.580]  случайность, которая у меня происходила до текущей итерации. Что тогда из выражения выше будет
[20:06.580 --> 20:17.460]  случайными величинами? Относительно такое. xк плюс 1 будет случайной величиной, супер, что еще?
[20:17.460 --> 20:29.580]  Градиент вот этот будет случайной величиной, так, и вот этот, да? Что-то еще? Нет, xк мы как раз
[20:29.580 --> 20:35.460]  фиксируем, то есть то, что та случайность, которая xк, посчитана с помощью xк-1, ну и так далее,
[20:35.460 --> 20:41.740]  там всех остальных. Это мы все зафиксировали. Ну гамма-каты, давайте все же брать детерминистические
[20:41.740 --> 20:48.180]  шаги, которые не зависят в случайности. Понятно, что можно, конечно, поизвращаться, там адаптивность какую-то
[20:48.180 --> 20:55.180]  добавить и сказать, что давайте подобираем. Вот вся командами тоже так делают. У вас там как раз шаг,
[20:55.180 --> 21:00.460]  ну даже матрица шкалирования, она зависит от градиента стихистического. Но здесь мы сделаем попроще,
[21:00.460 --> 21:05.340]  и давайте берем тогда это условно-математическое ожидание от того безобразия, что у нас написано.
[21:05.340 --> 21:13.860]  Я его сразу буду оставлять только там, где у меня величины случайные, вот, а все, что, ну,
[21:13.860 --> 21:19.300]  относительно этого математического ожидания. А все остальное я буду просто оставлять без него.
[21:25.180 --> 21:53.380]  Так, ну что, тогда давайте смотреть, что теперь нам дают эти все замечательные вещи,
[21:53.380 --> 22:02.540]  которые я написал. Про это что можем сказать? Чему будет равно? Это условно-математическое ожидание от градиента.
[22:02.540 --> 22:10.220]  Да, просто честному градиенту ровно вот из этого предположения, потому что я здесь
[22:10.220 --> 22:15.380]  ожидаю только по кси, а это на самом деле оно и есть, потому что та случайность, которая осталась
[22:15.380 --> 22:20.260]  после того, как мы все зафиксировали, это только кси. Только кси, поэтому вот то, что у меня здесь
[22:20.260 --> 22:28.860]  записано, просто вырождается в честный градиент. И это хорошо, потому что сейчас мы приходим к тому,
[22:28.860 --> 22:35.780]  что вот даже скалярное произведение в итоге оказалось таким, которое у нас было в детерминистическом методе.
[22:35.780 --> 22:42.340]  Вопрос теперь к дисперсии. Вопрос теперь к дисперсии. Давайте я перелесну на слайд. Вот,
[22:42.340 --> 22:56.180]  на следующий. Здесь я вот про вот это как раз написал. Да, смотрите, то есть у вас здесь вот
[22:56.180 --> 23:01.900]  случайность, которая здесь есть, математическое ожидание берется по случайной величине кси. А здесь
[23:01.900 --> 23:07.300]  в условном математическом ожидании вы фиксируете все, что происходило до этого. То есть вы фиксируете
[23:07.300 --> 23:14.020]  х0, вы фиксируете все ксишки до притекущей. По факту вся случайность, которая действует сейчас вот
[23:14.020 --> 23:20.220]  на этот вектор, это кси кт. Больше никакой случайности нет, все остальное фиксировано. Поэтому вот это
[23:20.220 --> 23:25.060]  условно-математическое ожидание, оно эквивалентно тому, что вы просто берете от ожидания по кси.
[23:25.060 --> 23:32.380]  Вот, теперь давайте разберемся с дисперсией. С дисперсией просто поинтереснее.
[23:32.380 --> 23:45.980]  Точнее не с дисперсией со вторым моментом, вот. Окей, предположение у нас было только на дисперсию,
[23:45.980 --> 23:54.700]  поэтому нужно сделать умный ноль и добавить и вычесть в скобочках дисперсию. Не дисперсию, а честный
[23:54.700 --> 24:00.460]  градиент, чтобы дальше уже можно было это все безобразие использовать. Так,
[24:02.380 --> 24:13.460]  вот. Хорошо, дальше я это, давайте, выражение под ожиданием раскрою, вот эти квадраты, и у меня
[24:13.460 --> 24:29.140]  сначала получится как раз дисперсия. Так, дисперсия, плюс еще удвоенный просто градиент. Смотрите,
[24:29.140 --> 24:33.420]  с градиента я сразу сниму условно-математическое ожидание, потому что эта величина будет
[24:33.420 --> 24:38.420]  не случайной. С нормы градиента я сразу снял условно-математическое ожидание. Согласны,
[24:38.420 --> 24:43.660]  что я могу снять его, да? xk, это у меня вещь не случайная относительно этого условно-математического
[24:43.660 --> 24:51.540]  ожидания, поэтому я его снял. Так, дальше остается у меня что-то вот такое. Скалярное произведение xk,
[24:51.540 --> 25:05.740]  xk, xk. Так, и здесь f от xk градиент. Так, и это условно-математическое ожидание. Хорошо, с первым
[25:05.740 --> 25:10.900]  кусочком понятно как разбираться. Ограниченная дисперсия, опять же, по предположению все делается.
[25:10.900 --> 25:18.180]  Хорошо, здесь возникает просто sigma в квадрате. Sigma в квадрате. Дальше второй кусочек, опять же,
[25:18.180 --> 25:24.260]  остается, он у нас возникал и в сходимости обычного градиентного метода. Что делать с последним,
[25:24.260 --> 25:36.980]  кто быстренько понимает, что там будет происходить? Ноль. Почему ноль? Да, смотрите,
[25:36.980 --> 25:43.540]  у вас, опять же, с точки зрения условно-математического ожидания случайным является только вот это
[25:43.540 --> 25:49.100]  выражение. Даже точнее так, вот только вот этот конкретный градиент стахастический. Поэтому вы
[25:49.100 --> 25:54.020]  условно-матоматическое ожидание можете занести под скалярное произведение, промотождать вот это
[25:54.020 --> 26:00.900]  все безобразие. Он вам даст, опять же, по нашему предположению, честный градиент, поэтому честный
[26:00.900 --> 26:06.460]  градиент минус честный градиент это просто ноль. Все, поэтому здесь у вас получается вот такое вот выражение.
[26:06.460 --> 26:19.220]  Согласны? Все, супер. Так, окей, здесь мы промотожидали, занесли, выразили, все. Дальше у
[26:19.220 --> 26:25.340]  нас получилось вот такое вот безобразие. Первое доказали, второе доказали, но первое, то, с чего
[26:25.340 --> 26:31.740]  мы начинали, верхняя строчка. Поэтому это все можно объединить и получится вот такое вот. То есть,
[26:31.740 --> 26:38.900]  кажется, что вообще все хорошо, все выглядит очень похоже на то, что у нас было в обычном градиентном
[26:38.900 --> 26:44.340]  спуске. Единственное, что болтается вот этот кусочек гамма-к в квадрате на сигма в квадрате,
[26:44.340 --> 26:48.860]  гамма-к в квадрате на сигма в квадрате. Дальше, поэтому делаются абсолютно те же самые шаги,
[26:48.860 --> 26:54.180]  которые мы с вами делали. Применяли сильную выпуклость для скалярного произведения и
[26:54.180 --> 26:58.460]  применяли гладкость для нормы. То есть, здесь нужно было вычесть сначала умный ноль,
[26:58.460 --> 27:04.900]  значение градиента в оптимуме, потом применить гладкость именно для выпуклых функций. У вас там
[27:04.900 --> 27:14.500]  это расписывается через разность функций точки xk и x звездой. Это здесь сделано и поэтому вот
[27:14.500 --> 27:18.740]  раз у вас получается так, это у вас просто использована гладкость и сильная выпуклость.
[27:18.740 --> 27:24.940]  Дальше слагаемые сгруппированы, и получился опять же тот же самый результат. Единственное,
[27:24.940 --> 27:31.460]  что только болтается вот этот сигма в квадрате на гамма-к в квадрате. Ну окей, болтается и болтается.
[27:31.460 --> 27:35.940]  Дальше опять же выбираем гамма так, чтобы у вас все было хорошо с точки зрения вот,
[27:35.940 --> 27:41.100]  чтобы убить вот этот кусочек. Чтобы убить этот кусочек, вы выбираете гамма меньше,
[27:41.100 --> 27:45.620]  чем 1 делить на L, и получается вот такая вот сходимость. Вот такая вот сходимость.
[27:45.620 --> 27:55.900]  Да, как-то вот так. Возникает вопрос, что с этой сходимостью можно поделать. Будет ли это
[27:55.900 --> 28:06.820]  такая же сходимость, как у градиентного спуска? Будьте здоровы. Сигма мешает, сигма мешает. Ну давайте
[28:06.820 --> 28:12.580]  попробуем даже с постоянным шагом что-нибудь поделать. Я уберу вот эти к здесь, уберу здесь эти к. Во-первых,
[28:12.580 --> 28:17.140]  у меня возникает вопрос. Градиент на спуске, чтобы получить сходимость, мы запускали рекурсию.
[28:17.140 --> 28:22.700]  У нас справа стояло какое-то выражение, слева. Здесь я могу сразу же запустить рекурсию.
[28:22.700 --> 28:34.860]  Да, у нас слева с мотожданием стоит, а справа без мотождания стоит. Поэтому как бы, там же суть
[28:34.860 --> 28:41.140]  рекурсии то, что у меня была всегда оценка на выражение справа еще. Спасибо. Вот, на выражение
[28:41.140 --> 28:46.980]  справа была оценка. Я его оценивал по рекурсии и дальше мне это все схлопалось. Пум-пум-пум-пум. Здесь у
[28:46.980 --> 28:53.700]  меня мотождание стоит и поэтому оценка на xk минус x звездой в подрати у меня тоже будет только
[28:53.700 --> 29:01.460]  по мотожданию. Вот, условному. Поэтому что-то нужно сделать. Сделать тут все довольно просто. Так как
[29:01.460 --> 29:07.180]  нужно мотождание, давайте возьмем полное мотождание. Вот, набросим полное мотождание на обе части,
[29:07.180 --> 29:15.180]  тогда оно нам сожрет условным от ожидания, а здесь возникнет как раз полное мотождание. Все. И вот в
[29:15.180 --> 29:20.260]  таком виде рекурсию уже запускать можно. В таком виде рекурсию уже запускать можно. Когда вы возьмете
[29:20.260 --> 29:26.580]  полное мотождание, рекурсия уже запускается. Ну и давайте запустим. Давайте запустим. Тогда у меня
[29:26.580 --> 29:35.780]  что здесь будет? 1 минус гамма мю в квадрате xk минус 1 x звездой плюс гамма в квадрате сигма в
[29:35.780 --> 29:41.220]  квадрате плюс то, что у меня вылезло еще после того, как я подставил xk минус x звездой. А здесь у меня
[29:41.220 --> 29:51.060]  вылезет гамма мю гамма в квадрате сигма в квадрате. Согласны? Вот. Запускаю, запускаю дальше,
[29:51.060 --> 29:56.420]  дальше, дальше. И я в какой-то момент приду, соответственно, понятно, в нулевую точку.
[29:56.420 --> 30:07.100]  Мотождание только забыл. Мотождание нулевой точки. Вот так вот. Но здесь у меня будет болтаться
[30:07.100 --> 30:13.020]  вот этот гамма, который я насуммировал. Гамма в квадрате сигма в квадрате, который у меня
[30:13.020 --> 30:25.700]  вот суммируется. Здесь что будет? И от нуля до ка 1 минус гамма мю в степени и. Согласны, да,
[30:25.700 --> 30:30.660]  что так получится? То есть, когда я подставлю еще раз, у меня здесь вылезет k минус 2. Ну и,
[30:30.660 --> 30:36.140]  соответственно, гамма в квадрате сигма в квадрате со степенью 1 минус гамма мю в квадрате. Ну и вот эти
[30:36.140 --> 30:41.140]  степени у меня здесь и будут постоянно вылезать, вылезать, вылезать. Как теперь можно оценить вот этот
[30:41.140 --> 30:49.180]  второй член? Как можно? Геометрическая прогрессия. То есть, здесь она усеченная. Но можно загрубить и
[30:49.180 --> 30:55.300]  сказать, что здесь давайте я поставлю бесконечность. И тогда вот эта сумма будет равна как 1 делить на
[30:55.300 --> 31:02.500]  гамма мю. Вот. И в итоге получается результат. Результат получается вот такого духа. 1 минус гамма
[31:02.500 --> 31:13.420]  мю х нулевого х звездой в квадрате. От ожидания можно поставить. Вот. Плюс гамма сигма в квадрате
[31:13.420 --> 31:25.580]  делить на мю. Вот. Чего забыл? А, k плюс 1, да. Вот. То есть получается по факту что-то очень похоже. Если
[31:25.580 --> 31:30.380]  сигма равно нулю, то это вообще сходимость просто градиентного спуска, честно. Сигма не равна нулю.
[31:30.380 --> 31:37.380]  Получается что у вас? У вас опять же есть линейная сходимость. Пока у вас первый член мажорирует
[31:37.380 --> 31:42.100]  второй у вас есть линейная сходимость. Как только они начинают быть одного порядка,
[31:42.100 --> 31:50.340]  та оценка, которую мы получили говорит о том, что ну вы можете достигнуть какой-то точности. Но
[31:50.340 --> 31:57.500]  глобально вот это вам говорит, что вот это какая-то достижимая константа, ниже которой вы не опуститесь.
[31:57.500 --> 32:04.100]  Вот это может быть меньше, меньше, меньше, меньше. Но вот это у вас некоторый барьер. Ну не тот,
[32:04.100 --> 32:09.020]  который мы проходили, а просто какой-то барьер сходимости, который вам говорит о том, что ну какая-то
[32:09.020 --> 32:14.260]  константа, дальше которой никаких гарантий нету. Никаких гарантий нету, это просто константа,
[32:14.260 --> 32:19.980]  она не уменьшается. Если у вас шаг фиксирован, сигма фиксирована, мю фиксирована, то есть никак
[32:19.980 --> 32:27.700]  вы не играетесь шагом, а сигма и мю тут поиграться как-то сложно. Вот. То у вас будет про факту сначала
[32:27.700 --> 32:32.020]  линейная сходимость. Линейная сходимость пока вы далеко от решения, вы действительно падаете к
[32:32.020 --> 32:37.260]  нему, а потом начинаются асцеляции. И это на практике наблюдается. То есть, ну метод просто
[32:37.260 --> 32:41.720]  не сходится. То есть сначала есть линейной сходимости, потом начинаются просто колебания
[32:41.720 --> 32:46.800]  вокруг оптимума из-за того, что вот есть эта дисперсия, которая не компенсируется. И понятно,
[32:46.800 --> 32:51.740]  что степень этих, ну как бы, высота этих колебаний, то есть на графике это обычно выглядит вот так.
[32:51.740 --> 32:57.060]  Вот те графики, которые вы строите. Сначала линейная сходимость, потом начинается вот так. Вот.
[32:57.060 --> 33:03.260]  Глубина того, насколько вы погружаетесь вот здесь зависит, понятно, от гаммы, от сигмы, от мю.
[33:03.260 --> 33:09.260]  Меньше шаг, соответственно, медленнее сходимость, но глубже погружение.
[33:09.260 --> 33:14.760]  Вот. Понятно, дисперсия тоже, на самом деле, можно поиграть, это мы сейчас обсудим.
[33:14.760 --> 33:22.760]  Вот. Но пока вот такой вот эффект, то, что у вас есть эта окрестность, которая не улучшаема.
[33:22.760 --> 33:32.960]  Не улучшаема, и это как бы проблема метода SGD, и здесь у нас у меня все это, да, вот это мы с вами уже выбили,
[33:32.960 --> 33:37.960]  оценили через геометрическую прогрессию, получили вот такую сходимость.
[33:37.960 --> 33:44.960]  Линейная сходимость к решению, второй кусочек, это вот та осцилляция, которая у вас будет в любом случае.
[33:44.960 --> 33:52.460]  Вот. Обсудим, как это можно решать. Первый вариант, это то, что я сказал, это просто уменьшение шага.
[33:52.660 --> 33:59.860]  Но опять же, уменьшение шага, если шаг фиксирован, он вам не дает точную сходимость,
[33:59.860 --> 34:03.360]  в любом случае опускаетесь к решению и в какой-то момент вы начинаете осциллировать.
[34:03.360 --> 34:10.360]  Да, глубже, да, дальше. То есть, медленнее сходитесь линейно, но, соответственно, глубже с точки зрения качества решения.
[34:10.360 --> 34:18.360]  Вот. Есть еще один вариант, это брать, например, уменьшающийся шаг, 1 делить на k или 1 делить на корень из k плюс 1.
[34:19.260 --> 34:29.260]  Это то, к чему обычно прибегает машинное обучение, когда у вас шаг уменьшается, выставляют всякие шуделеры, выставляют или просто прописывают их уменьшающимися шаги,
[34:29.260 --> 34:39.260]  чтобы как раз бороться с тахастикой. Чтобы бороться с тахастикой, чтобы у вас как раз эта окрестность становилась еще ниже, ниже, ниже.
[34:40.160 --> 34:51.160]  Но этого всего безобразия, я думаю, уже понятен и плюс, и минус. Окрестность глубже, то есть вы даже вот здесь можете достигнуть точного решения, это доказуемо.
[34:51.160 --> 35:02.160]  Но в силу того, что ваш шаг становится непостоянным, вы теряете линейную сходимость на начальных итерациях, вы теряете линейную сходимость.
[35:03.060 --> 35:15.060]  Ну вот такие вот плюсы, соответственно, можно как бы на практике стараются и линейную сходимость, то есть наибыстрейшую, которая возможна для данной задачи, не терять,
[35:15.060 --> 35:26.060]  и при этом выгребать качество с точки зрения точности, поэтому вот эти шаг уменьшаются как-то поэтапно. То есть сначала берут какой-то постоянный, до какого-то момента его держат,
[35:26.960 --> 35:32.960]  потом опять же его начинают переключать, через эпоху или через несколько эпох его переключают, делают чуть меньше.
[35:32.960 --> 35:43.960]  Вот, и таким вот образом как бы у вас получается сходимость не сильно портится, вот, и при этом как бы стахастика начинает по чуть-чуть убиваться, по чуть-чуть убиваться за счет уменьшения шага.
[35:45.960 --> 35:51.960]  Вопрос, на самом деле с сигмой тоже можно поиграться, тоже ее сделать меньше, вот, ну как?
[35:52.860 --> 35:56.860]  Сигма, а пожалуйста.
[36:09.860 --> 36:15.860]  Вот, правильно, то есть есть у меня некоторая, в некотором смысле реализация случайной величины,
[36:16.760 --> 36:21.760]  вот, я с помощью этой реализации говорю, что я пытаюсь опроксимировать реальный градиент.
[36:21.760 --> 36:28.760]  Говорю, что у меня стахастический градиент, возможно похож на реальный градиент, но есть дисперсия, то есть по мотожданию там вообще не смещенная оценка,
[36:28.760 --> 36:32.760]  ну а есть какая-то дисперсия, которую, ну, мне бы хотелось побороться.
[36:32.760 --> 36:37.760]  Как побороться? Ну, это вы все знаете, еще есть Тиар Вера, всякие предельные Тиаремы.
[36:38.660 --> 36:40.660]  Как уменьшить дисперсию?
[36:40.660 --> 36:44.660]  У Красном Борис уже сказал, взять побольше сэмплов.
[36:44.660 --> 36:49.660]  Особенно, если у нас сэмплы между собой, что? Независимы.
[36:49.660 --> 36:55.660]  Если вы берете, соответственно, давайте, так называемый batch сэмплов.
[36:55.660 --> 36:57.660]  Batch сэмплов.
[36:57.660 --> 37:01.660]  Машинное учение, это ровно так и называется, batch.
[37:01.660 --> 37:06.660]  Так, тут же, xij, вот.
[37:07.560 --> 37:12.560]  То вот такой градиент, утверждается, будет лучше опроксимировать с точки зрения дисперсии.
[37:12.560 --> 37:18.560]  Здесь мощность s, b, то есть вы сэмплируете b индексов, вы сэмплируете b индексов,
[37:18.560 --> 37:31.560]  и все индексы из s сэмплируются независимо, и там, не знаю, из xi равномерно, из xd равномерно, либо берется там равномерно из z.
[37:32.460 --> 37:36.460]  Понятно, что это дороже, потому что тогда у нас стахистический градиент,
[37:36.460 --> 37:41.460]  нужно считать просто по большему числу данных, вместо одной точки у вас там нужно брать b точек.
[37:41.460 --> 37:45.460]  В самом деле, часто так и делают, потому что одна точка, это, конечно, слишком агрессивно,
[37:45.460 --> 37:51.460]  она может быть очень так, слишком большая дисперсия будет у стахистического градиента.
[37:51.460 --> 37:56.460]  Ну и тоже, когда вы говорите про какую-то природу, можно подождать пару сэмплов,
[37:56.460 --> 37:58.460]  хотя не во всех задачах так можно.
[37:59.360 --> 38:05.360]  Ну и, как верно подметили, то есть здесь у вас вот эту дисперсию, которая у вас получается, можно оценить вот так.
[38:05.360 --> 38:08.360]  Понятно, что по математическому ожиданию здесь будет все хорошо,
[38:08.360 --> 38:14.360]  вы просто промотожидаете и скажете, что в силу линейности вы заносите математическое ожидание под сумму,
[38:14.360 --> 38:19.360]  каждый отдельный сэмпл в среднем это у вас действительно честный градиент,
[38:19.360 --> 38:21.360]  и получается в среднем это тоже честный градиент.
[38:21.360 --> 38:25.360]  Дисперсии чуть посложнее, но тоже несложно.
[38:26.260 --> 38:28.260]  Добавлю сюда, соответственно, вот этот честный градиент,
[38:28.260 --> 38:32.260]  который нам обычно нужно было вычитать при оценке дисперсии.
[38:34.260 --> 38:35.260]  Ну и что здесь получается?
[38:35.260 --> 38:39.260]  У меня под знаком суммы стоит, что я могу сказать про эти случайные величины?
[38:39.260 --> 38:41.260]  Они независимы, да?
[38:41.260 --> 38:46.260]  Получается у меня сумма считается дисперсия независимых случайных величин.
[38:46.260 --> 38:52.260]  Для них мы знаем, что дисперсия суммы равна сумме дисперсий.
[38:53.160 --> 38:59.160]  Бэшку вынес за норму, получилось b², дальше выношу сумму дисперсии.
[39:01.160 --> 39:06.160]  Здесь у меня возникает эта сумма, точнее разность градиент стахистического и честного,
[39:06.160 --> 39:12.160]  ну и я ее оцениваю как сигмой, и у меня получается сигма в квадрате делить на b.
[39:12.160 --> 39:19.160]  Потому что вот здесь вот этот оценил сигмой, суммирование сожрет одну бэшку, одна бэшка останется.
[39:20.060 --> 39:27.060]  Тут важно то, что мы берем все это безобразие независимо друг от друга, и получается вот такой вот батч-эффект.
[39:27.060 --> 39:30.060]  Больше сэмплируете, меньше дисперсии.
[39:30.060 --> 39:36.060]  То есть можно бороться за сходимость не только шагом, но можно вот таким вот способом
[39:36.060 --> 39:41.060]  с помощью увеличения количества данных, которые вы используете в стахистическом градиенте.
[39:47.060 --> 39:49.060]  Что значит большим количеством индексов?
[39:50.060 --> 39:59.060]  Здесь мы предполагаем, что у нас вот один сэмпл – это просто один индекс, это конкретная точка данных.
[39:59.060 --> 40:06.060]  Понятно, что можно уже изначально всю эту задачу, которая видит суммы, разбить на бачи.
[40:06.060 --> 40:09.060]  Обычно так и сразу и делается.
[40:09.060 --> 40:14.060]  Понятно, что именно с вычислительной точки зрения до какого-то момента это все неравномерно считается.
[40:14.960 --> 40:19.960]  График не совсем равномерный. С увеличением бача подсчет градиента растет не так.
[40:19.960 --> 40:23.960]  Вот здесь есть какие-то эффекты, типа наитонициализацию и так далее,
[40:23.960 --> 40:27.960]  что подсчет какого-то небольшого бача в принципе имеет место.
[40:27.960 --> 40:31.960]  И он стоит столько же, сколько и подсчет бача по одному сэмплу.
[40:31.960 --> 40:34.960]  Поэтому можно изначально выборку, понятно, разделить на кусочки.
[40:34.960 --> 40:38.960]  То есть не обязательно один сэмпл – это вот один, он у меня пойдет в бач.
[40:38.960 --> 40:42.960]  Бач можно изначально задать, а потом уже делать бачи из бачей.
[40:43.860 --> 40:45.860]  Тоже вариант.
[40:48.860 --> 40:50.860]  Понятно здесь идея.
[40:50.860 --> 40:54.860]  Окей, это мы с вами обсудили.
[40:54.860 --> 40:57.860]  Так, это тоже обсудили.
[40:57.860 --> 41:03.860]  А, ну и давайте как раз этот сюжет с вами закончим.
[41:04.760 --> 41:06.760]  В итоге.
[41:06.760 --> 41:08.760]  Так, это что?
[41:08.760 --> 41:10.760]  А нет, тут корня этого нет.
[41:10.760 --> 41:12.760]  Это для SGD.
[41:12.760 --> 41:14.760]  Сначала для SGD.
[41:14.760 --> 41:16.760]  Оценка вот такая вот без корня.
[41:16.760 --> 41:20.760]  Получается, вот можно так вот сделать.
[41:20.760 --> 41:24.760]  Первое – это ровно то, что мы имели для градиентного спуска.
[41:24.760 --> 41:28.760]  Линейная сходимость – показателем уделить на L.
[41:28.760 --> 41:30.760]  Второй кусочек, соответственно, стахастический.
[41:31.660 --> 41:34.660]  Тут такой результат немного искусственный,
[41:34.660 --> 41:38.660]  потому что здесь нужно подбирать шаг довольно хитро.
[41:38.660 --> 41:40.660]  То есть его от номера итерации не только там уменьшать,
[41:40.660 --> 41:43.660]  но иногда брать постоянным, равным 1 делить на L,
[41:43.660 --> 41:45.660]  потом на более поздних итерациях уменьшать.
[41:45.660 --> 41:47.660]  Тут надо знать свойства функций.
[41:47.660 --> 41:50.660]  Но в теории вот такой вот результат можно получить,
[41:50.660 --> 41:52.660]  как бы линейная сходимость к решению,
[41:52.660 --> 41:56.660]  плюс сублинейная сходимость из-за стахастики.
[41:56.660 --> 41:58.660]  Понятно, стахастики нет, у вас просто градиентный спуск.
[41:58.660 --> 42:03.660]  Стахастика есть, у вас проявляется эффект сублинейности в какой-то момент.
[42:03.660 --> 42:05.660]  То есть у вас будет как раз сначала линейная сходимость,
[42:05.660 --> 42:07.660]  а потом сублинейная сходимость.
[42:07.660 --> 42:12.660]  В зависимости, опять же, как соотносится расстояние изначального до решения
[42:12.660 --> 42:15.660]  и как получается, какого размера у вас сигма.
[42:15.660 --> 42:18.660]  Сигма большая, понятно, сразу начнется с сублинейной сходимости.
[42:22.660 --> 42:24.660]  Да-да-да, но там очень хитро подбирается.
[42:24.660 --> 42:28.660]  То есть изначально на начальных итерациях оно берется 1 на L.
[42:28.660 --> 42:31.660]  Дальше говорится, давайте начнем уменьшать его.
[42:31.660 --> 42:33.660]  Ну там необычный вариант.
[42:33.660 --> 42:37.660]  То есть там не так просто такой результат получить.
[42:37.660 --> 42:39.660]  То есть там не просто надо брать уменьшающийся шаг.
[42:39.660 --> 42:41.660]  Там чтобы как раз вот это была линейная сходимость,
[42:41.660 --> 42:44.660]  в оценке нужно изначально брать правильный шаг.
[42:44.660 --> 42:46.660]  1 делить на L.
[42:46.660 --> 42:49.660]  Оказывается, что этот результат еще и ускоряется.
[42:49.660 --> 42:53.660]  То есть понятно, к градиентному спуску мы можем применить нестерва.
[42:53.660 --> 42:57.660]  К стахастическому градиентному спуску это тоже безобразие делается.
[42:57.660 --> 43:03.660]  При этом можно заметить, что 1 кусочек, который отвечал за терминетистическую сходимость,
[43:03.660 --> 43:09.660]  он ускоряется, как в принципе и в случае, когда вообще стахастики никакой не было.
[43:09.660 --> 43:11.660]  А 2 кусочек нет.
[43:11.660 --> 43:15.660]  Оказывается вот этот шумовой эффект, который возникает из-за того,
[43:15.660 --> 43:19.660]  что стахастический градиент имеет какую-то дисперсию ограниченную.
[43:19.660 --> 43:22.660]  Он не ускоряется, у вас в любом случае получится сублинейная сходимость.
[43:22.660 --> 43:24.660]  1 делить на K.
[43:24.660 --> 43:27.660]  Тут как работа не работа, и вот эта оценка не улучшаема.
[43:27.660 --> 43:29.660]  То есть это доказуемо не улучшаемо.
[43:29.660 --> 43:32.660]  Есть нижние оценки на то, что вот этот кусок такой и есть.
[43:32.660 --> 43:36.660]  То есть у вас есть линейная сходимость, плюс сублинейная сходимость,
[43:36.660 --> 43:40.660]  причем ровно такая же сублинейная, как у градиентного спуска.
[43:40.660 --> 43:44.660]  То есть стахастика в этом плане враждебна к ускорению.
[43:44.660 --> 43:47.660]  В принципе, это в некотором смысле ожидаемый эффект,
[43:47.660 --> 43:52.660]  потому что в негладком случае нету тоже ускорения.
[43:52.660 --> 43:57.660]  А стахастик – это ограниченная дисперсия, чем-то похожа на ограниченность субградиента.
[43:57.660 --> 44:02.660]  Поэтому если в негладком случае нет ускорения, то и здесь ожидаемого его тоже нет.
[44:02.660 --> 44:08.660]  Оно есть пока у вас свою роль не начнет играть стахастика,
[44:08.660 --> 44:12.660]  но потом, когда вы дойдете до того момента, что у вас сигма,
[44:12.660 --> 44:17.660]  вот эта слагаемость сравняется примерно вот с этим, в какой-то момент работа алгоритма,
[44:17.660 --> 44:22.660]  то у вас сигма уже будет играть роль, и тогда получится линейная сходимость,
[44:22.660 --> 44:25.660]  которая не отличается от градиентного спуска.
[44:33.660 --> 44:38.660]  Можем, можем, можем. Нет, такая сходимость уже не получится.
[44:38.660 --> 44:42.660]  Просто кажется, что 1 делить на k в квадрате уже не особо выгодно брать.
[44:42.660 --> 44:46.660]  То есть правильные оценки, да и на практике 1 на k работает нормально,
[44:46.660 --> 44:49.660]  1 на k в квадрате слишком медленно, он слишком быстро убывает.
[44:49.660 --> 44:54.660]  Да, вы будете, может быть, в итоге в какой-то асимптотике получать хорошее решение,
[44:54.660 --> 44:58.660]  потому что шаг еще меньше, поэтому сойдетесь еще глубже.
[44:58.660 --> 45:04.660]  Но просто потому что шаг убывает слишком быстро, эта асимптотика слишком плохая.
[45:04.660 --> 45:10.660]  Когда вы дойдете до этой бесконечности, 1 на k достаточно, 1 на корень из k достаточно часто.
[45:10.660 --> 45:16.660]  Поэтому, конечно, можно брать супермаленькие шаги, но не рекомендуется.
[45:16.660 --> 45:18.660]  Вот здесь такая идея.
[45:18.660 --> 45:22.660]  Вот первая часть лекции, все, вопросы?
[45:22.660 --> 45:24.660]  Здесь корня нету.
[45:24.660 --> 45:29.660]  Первая для градиентного спуска, первая для градиентного спуска, я зачем-то его бахну в лобби.
[45:29.660 --> 45:34.660]  Почему?
[45:34.660 --> 45:39.660]  Ну, смотрите, mu на l, это меньше единицы, например, 1 сотая.
[45:39.660 --> 45:42.660]  Здесь получается, в скобочке, что?
[45:42.660 --> 45:44.660]  0,99 степени k.
[45:44.660 --> 45:47.660]  А здесь что будет получаться, если я корень возьму?
[45:47.660 --> 45:49.660]  0,9 степени k.
[45:49.660 --> 45:52.660]  Что быстрее убывает?
[45:52.660 --> 45:54.660]  Второе быстрее убывает.
[45:54.660 --> 45:57.660]  Понятно, что этого становится сильно меньше.
[45:57.660 --> 46:01.660]  Через 10 итераций, то есть 0,9 степени 10, это супермало уже.
[46:01.660 --> 46:03.660]  А 0,99 степени 10.
[46:03.660 --> 46:06.660]  Ну так, по-моему, он как раз до 0,9 и дойдет.
[46:06.660 --> 46:09.660]  Поэтому градиентный спуск тут медленнее, понятно.
[46:09.660 --> 46:12.660]  Это тот же самый результат, что у нас и был.
[46:12.660 --> 46:15.660]  Вот, по первой части все.
[46:15.660 --> 46:19.660]  Дальше пойдем уже к лобби.
[46:19.660 --> 46:22.660]  К дополнительным техникам, что можно делать в стокастическом случае.
[46:22.660 --> 46:25.660]  Вопросы?
[46:25.660 --> 46:28.660]  Ну вот мы как раз это и обсудим.
[46:28.660 --> 46:31.660]  Обсудим обзорно.
[46:31.660 --> 46:34.660]  Катя вам, конечно, больше рассказала.
[46:34.660 --> 46:37.660]  В том числе частично с доказательствами.
[46:37.660 --> 46:40.660]  Вот, я скорее просто идейно это обсужу.
[46:40.660 --> 46:43.660]  Нет, мы сейчас посмотрим.
[46:43.660 --> 46:46.660]  Посмотрим, потому что, например,
[46:46.660 --> 46:49.660]  Те результаты, которые дает SGD, они не улучшаем.
[46:49.660 --> 46:52.660]  Потому что, опять же, выборки нет.
[46:52.660 --> 46:55.660]  Ничего нет лучше, чем просто ждать сэмпл и считать по нему 100 градиентов.
[46:55.660 --> 46:58.660]  Вот.
[46:58.660 --> 47:01.660]  Но, когда и выборка есть, появляются альтернативы.
[47:01.660 --> 47:04.660]  Вот.
[47:04.660 --> 47:07.660]  Окей, тогда перерыв и возвращаемся.
[47:07.660 --> 47:10.660]  И как раз разбираемся с Variance Reduction.
[47:10.660 --> 47:13.660]  Все, все, продолжаем.
[47:13.660 --> 47:16.660]  Давайте разбираться, что там у нас происходит дальше.
[47:16.660 --> 47:19.660]  Вот.
[47:19.660 --> 47:22.660]  Так, эти мы способы с вами обговорили.
[47:22.660 --> 47:25.660]  Пам-пам-пам.
[47:25.660 --> 47:28.660]  Так.
[47:28.660 --> 47:31.660]  Почему работает градиентный спуск,
[47:31.660 --> 47:34.660]  и почему не до конца сходится SGD?
[47:34.660 --> 47:37.660]  То есть, кажется, что на начальном этапе
[47:37.660 --> 47:40.660]  происходит ровно то, что обычно происходило с градиентным спуском.
[47:40.660 --> 47:43.660]  То есть, в итоге X стремится как-то к X звездой.
[47:43.660 --> 47:46.660]  Процесс как-то сходится к решению.
[47:46.660 --> 47:49.660]  Но потом начинаются вдруг асцеляции,
[47:49.660 --> 47:52.660]  хотя у градиентного спуска они не наблюдались.
[47:52.660 --> 47:55.660]  Какая проблема появилась в физике метода?
[47:55.660 --> 47:58.660]  Потому что градиент стокастически не объясняет до конца физику.
[47:58.660 --> 48:01.660]  Вот в чем может быть конкретная проблема?
[48:01.660 --> 48:04.660]  Почему один сходится и, начиная там просто в одной точке,
[48:04.660 --> 48:07.660]  задерживается до конца?
[48:07.660 --> 48:10.660]  А второй, ну, скачет между разными точками.
[48:10.660 --> 48:13.660]  Причем примерно с одинакового размера шагами.
[48:13.660 --> 48:16.660]  Почему так происходит?
[48:16.660 --> 48:19.660]  Особенно вот интересно с точки зрения машинного обучения.
[48:25.660 --> 48:28.660]  А почему он слишком сильный?
[48:28.660 --> 48:31.660]  Вот у градиентного спуска он не сильный.
[48:31.660 --> 48:34.660]  Вроде бы размер шага один на L и там и там.
[48:34.660 --> 48:37.660]  Вроде бы размер шага один на L и там и там.
[48:37.660 --> 48:40.660]  Градиентному спуску норм.
[48:52.660 --> 48:55.660]  Это описывает то, что происходит.
[48:55.660 --> 48:58.660]  Почему градиентный спуск начинает в решении задерживаться?
[48:58.660 --> 49:01.660]  Почему он там остается?
[49:01.660 --> 49:04.660]  Потому что у вас, когда X стремится к X звездой,
[49:04.660 --> 49:07.660]  у вас и градиент, который вы используете,
[49:07.660 --> 49:10.660]  стремится к нулю.
[49:10.660 --> 49:13.660]  То есть шаги у вас уменьшаются за счет того, что
[49:13.660 --> 49:16.660]  не сам степ-сайз становится меньше,
[49:16.660 --> 49:19.660]  а просто потому что, приближаясь к решению,
[49:19.660 --> 49:22.660]  у вас градиент приближается к нулю, а значит,
[49:22.660 --> 49:25.660]  оттуда он и не выходит.
[49:25.660 --> 49:28.660]  А что происходит в случае, когда вы используете SGD?
[49:28.660 --> 49:31.660]  Вы можете сказать про градиент, даже несмотря на то,
[49:31.660 --> 49:34.660]  что вы стремитесь к X звездой, вы это можете предположить,
[49:34.660 --> 49:37.660]  например, сказать, что да, я стремлюсь к X звездой.
[49:37.660 --> 49:40.660]  Будет ли градиент на каком-то сэмпле равен нулю?
[49:40.660 --> 49:43.660]  Не обязательно. Не обязательно, потому что
[49:43.660 --> 49:46.660]  в этом в том числе и суть машинного обучения,
[49:46.660 --> 49:49.660]  потому что один сэмпл – это одна точка данных,
[49:49.660 --> 49:52.660]  либо какой-то кусочек данных небольшой.
[49:52.660 --> 49:55.660]  Градиент отражает функцию потерь модели
[49:55.660 --> 49:58.660]  А большой градиент отражает функцию потерь
[49:58.660 --> 50:01.660]  на всех данных. И то, что
[50:01.660 --> 50:04.660]  итоговая модель, которая у вас получилась,
[50:04.660 --> 50:07.660]  которая X звездой, которая нам нужна,
[50:07.660 --> 50:10.660]  это потери для всех данных,
[50:10.660 --> 50:13.660]  они могут быть не очень хорошими для конкретного кусочка данных,
[50:13.660 --> 50:16.660]  потому что, опять же, когда мы говорим про линейную модель,
[50:16.660 --> 50:19.660]  например, у вас набросаны какие-то точки
[50:19.660 --> 50:22.660]  на плоскость, и вы говорите, что да,
[50:22.660 --> 50:25.660]  я там построил линейную модель для этих всех точек,
[50:25.660 --> 50:28.660]  вот она вот так выглядит. Но потом вам говорят,
[50:28.660 --> 50:31.660]  окей, давай мы что-нибудь попробуем сказать
[50:31.660 --> 50:34.660]  про вот эти точки. Стахастический градиент
[50:34.660 --> 50:37.660]  посчитаем для этих точек. Но тогда вы говорите,
[50:37.660 --> 50:40.660]  кажется, синяя модель не очень для него,
[50:40.660 --> 50:43.660]  потому что вот такая модель будет лучше.
[50:43.660 --> 50:46.660]  Она прямо по этим точкам пройдется.
[50:46.660 --> 50:49.660]  Ровно через них пройдет. Поэтому
[50:49.660 --> 50:52.660]  оптимум, который дает синяя модель,
[50:52.660 --> 50:55.660]  синяя модель вам дает X звездой.
[50:55.660 --> 50:58.660]  Он не является оптимумом для конкретного боча,
[50:58.660 --> 51:01.660]  конкретного боча. Поэтому даже несмотря на то,
[51:01.660 --> 51:04.660]  что вы дошли до оптимума,
[51:04.660 --> 51:07.660]  ну или приближаетесь к нему, вам никто не гарантирует,
[51:07.660 --> 51:10.660]  что на конкретном сэмпле этот оптимум будет стремиться к нулю.
[51:10.660 --> 51:13.660]  Просто потому, что так устроено машинное обучение,
[51:13.660 --> 51:16.660]  общая функция потери, она может быть неравна,
[51:16.660 --> 51:19.660]  teachings, параметры модели μанимируя общую функцию,
[51:19.660 --> 51:22.660]  параметры модели, которые минимизируют общую функцию comparing
[51:22.660 --> 51:25.660]  модели, которые минимизируют общую функцию, потерь, они
[51:25.660 --> 51:28.660]  могут быть неравны параметрам, которые минимизируют
[51:28.660 --> 51:31.660]  какую-то выборку. При этом на самом деле часто наблюдается
[51:31.660 --> 51:34.660]  эффект, ну не часто, но иногда наблюдается эффект,
[51:34.660 --> 51:37.660]  когда вы все же можете гарантировать, что у вас модель,
[51:37.660 --> 51:40.660]  которая получена, altenna вас минимизирует
[51:40.660 --> 51:43.660]  и частную выборку, и общую.
[51:43.660 --> 51:50.220]  Как мне построить модель, которая минимизирует и все потери, и которая
[51:50.220 --> 51:53.340]  минимизирует частные потери?
[51:53.340 --> 51:58.980]  Да, можно, например, построить что-то вот такое.
[51:59.620 --> 52:07.780]  Вот. Почему нет? Ну, а почему нет? Может быть, реально, жизнь такая. Вы дальше
[52:07.780 --> 52:12.460]  сейчас точки будете напихивать, и они вам будут попадать ровно в эту кривую.
[52:12.460 --> 52:17.260]  Понятно, что, конечно, это в порядке бреда, и, скорее всего, вам в каком-нибудь машинном
[52:17.260 --> 52:21.700]  обучении рассказывали, что это просто называется переобучением. Вот. Вы сделали
[52:21.700 --> 52:26.740]  слишком жесткую модель, задали ее слишком большим числом параметров, в том числе
[52:26.740 --> 52:30.580]  в данном случае эти параметры отвечают за коэффициенты полинома. Вот. И этот
[52:30.580 --> 52:36.460]  полином в итоге у вас подстроился так, что вот тупо в точки и попали. Вот. Когда
[52:36.460 --> 52:39.780]  начнете накидывать новые точки, ну, полином просто окажется не совсем
[52:39.780 --> 52:44.940]  корректным. Вот. Такое бывает, соответственно, такая вещь называется
[52:44.940 --> 52:49.500]  оверпараметризацией. И в каких-то простых случаях, ну, то есть, параметров слишком
[52:49.500 --> 52:54.940]  много, что вы можете подстроиться под любую выборку, которая у вас есть, даже
[52:54.940 --> 52:59.300]  под выборку, которая у вас есть. Причем, на самом деле, для простых моделей
[52:59.300 --> 53:03.060]  какие-то это говорят, что это плохо, ну, как-то вот показывают, это как пример того,
[53:03.060 --> 53:08.420]  что вы выбрали слишком сложную модель и нужно упрощать ее, вы по факту просто
[53:08.420 --> 53:13.580]  переобучаетесь. Но в нейронках эффект оверпараметризации часто довольно
[53:13.580 --> 53:18.740]  хороший, на самом деле. То есть, там этих весов слишком много и даже вот эта
[53:18.740 --> 53:22.820]  слишком большая модель для конкретного датасета может быть давать и хороший
[53:22.820 --> 53:27.300]  результат. То есть, там может быть ситуация лучше, чем вот это я нарисовал.
[53:27.300 --> 53:32.620]  Поэтому вот эта физика, почему SGD вообще перестает сходиться. Потому что у вас то,
[53:32.620 --> 53:37.260]  что вы используете в качестве градиента, ну, то, почему вы делаете шаг, он к нулю не
[53:37.260 --> 53:42.180]  стремится, он к нулю не стремится. Вот, и поэтому начинаются эти асцеляции и они,
[53:42.180 --> 53:47.380]  соответственно, пропорциональны тому, насколько у вас градиент по конкретному
[53:47.380 --> 53:52.740]  сэмплу в оптимальной точке для всей выборки не равен нулю, насколько он там
[53:52.740 --> 54:02.060]  большим будет. Вот, соответственно, возникает идея, если я хочу использовать
[54:02.060 --> 54:09.460]  что-то, получить хорошую сходимость, мне нужно, получается, в SGD использовать
[54:09.460 --> 54:15.340]  какой-то стахастический градиент, который будет стремиться к нулю. Вот, я хочу
[54:15.340 --> 54:20.420]  потребовать вот это. Если у меня x стремится к x звездой, ну, вот так вот у меня устроен
[54:20.420 --> 54:24.300]  процесс обучения, там решение задачи оптимизации, я могу предположить, что у меня x
[54:24.300 --> 54:28.340]  стремится к x звездой. Я бы хотел, чтобы у меня есть стахастический градиент,
[54:28.340 --> 54:33.060]  стремился к реальному градиенту, который в оптимуме просто равен нулю, в оптимуме
[54:33.060 --> 54:36.940]  равен нулю. Ну, и дополнительно там можно потребовать какие-то вещи в духе того,
[54:36.940 --> 54:42.140]  чтобы у меня в среднем это был честный градиент, либо просто в среднем по полному
[54:42.140 --> 54:45.540]  математическому ожиданию, либо даже по условному математическому ожиданию, понятно,
[54:45.540 --> 54:53.220]  что. Тут еще только вот так надо вбахать, потому что xk тоже случайная величина. Вот,
[54:53.220 --> 55:00.740]  как-то вот так вот можно сделать. Попробовать сосконструировать такой метод. Ну, и давайте
[55:00.740 --> 55:06.140]  попробуем это сделать. Единственное, что сразу нужно отговорить то, что, ну, как я говорил,
[55:06.140 --> 55:10.580]  в онлайн-сеттинге, когда у вас нет никакой выборки, вам просто поступают сэмплы, вы ничего не можете
[55:10.580 --> 55:16.500]  копить, вы тупо можете обрабатывать текущий сэмпл и шагать по нему. Ну, там не получается что-то
[55:16.500 --> 55:21.780]  как-то вот с редукцией дисперсии полностью устранить эту сигму. Полностью устранить. Да,
[55:21.780 --> 55:28.500]  там можно сделать чуть-чуть эффективнее, но сигма полностью не устраняется. Вот. А вот в случае,
[55:28.500 --> 55:34.580]  когда у вас задача, целевая функция в задаче уже представляет собой вид суммы, вот такой вот,
[55:34.580 --> 55:40.180]  ну, то есть сказать loss функция просто по каждому из отдельных сэмплов, там уже появляются интересные
[55:40.180 --> 55:44.500]  эффекты, которые мы сейчас с вами и пою следом. Катя вам про это рассказывала. Я расскажу просто
[55:44.500 --> 55:50.660]  интуицию, физику, ну, и те методы, которые она не упомянула. Она скорее-то пошла вглубь, вот. А я
[55:50.660 --> 55:56.460]  скорее-то в ширь немного про это, поэтому всему безобразию пройдусь. Вот. Здесь мы, соответственно,
[55:56.460 --> 56:00.940]  в этих методах будем предполагать, что стахастика будет как раз исходить от индекса. И вот эта
[56:00.940 --> 56:07.700]  ксиката, это будет эквивалентно тому, что я просто выбираю случайный индекс из того, из того набора
[56:07.700 --> 56:18.780]  FIT, который у меня есть. Вот. Понятно, да, идея? Чего? Ну, это индекс, я же здесь его и обозначил,
[56:18.780 --> 56:27.980]  здесь тоже и, ну, просто с к. Он мне сэмплируется равномерно из от 1 до n. Вот, каждую итерацию.
[56:27.980 --> 56:35.780]  Да-да-да, ну, то есть, вот, я просто вот это обозначение, до этого я обводил ксишку, ксишку,
[56:35.780 --> 56:41.940]  потому что, ну, кси могла быть из d, кси могла быть как раз этим индексом. А здесь я честно говорю,
[56:41.940 --> 56:49.220]  что у меня кси отвечает за этот индекс. Вот. Вот, потому что теперь я уже ограничился только вот этим
[56:49.220 --> 57:02.340]  сетапом конечной суммы. Да. Еще раз, ровно потому, как мы начинали с вами, с примера с машинного
[57:02.340 --> 57:12.820]  обучения. Вот, у вас, пожалуйста, представляет собой вот функция сумма потерян на всех сэмплах,
[57:12.820 --> 57:20.140]  которые у вас есть в выборке. Вот. Ну, вы же можете посчитать градиенты. Чем хорошо, опять же, то,
[57:20.140 --> 57:26.500]  что вам вчера рассказывал Саша. Ну, Ёж, то, что у вас по факту вот процесс подсчета полного градиента
[57:26.500 --> 57:32.220]  вот этого всего безобразия, он делится. Потому что вы к одному даете один кусочек, к другому даете
[57:32.220 --> 57:37.620]  другой кусочек, и каждый считает градиент по своему куску данных. Просумировали, получили полный
[57:37.620 --> 57:43.820]  градиент. Вот. Поэтому здесь я могу просто посчитать кусок по куску данных градиент, и в среднем,
[57:43.820 --> 57:50.260]  потому что я выбираю этот кусок данных равномерно, у меня это будет честный градиент. Вот. Вся суть. Так.
[57:56.500 --> 58:03.540]  Вот. Первый метод, ну, он исторически тоже первым появился, метод называется saga. Метод называется
[58:03.540 --> 58:10.820]  saga. Вот. И идея у него довольно естественная и понятная. Смотрите, в чем суть. В силу того,
[58:10.820 --> 58:17.220]  что я на каждой итерации считаю какой-то стахастический градиент по какому-то индексу И,
[58:17.220 --> 58:25.980]  и к этому случайному. Вот. Получается, ну, в SGD я просто забываю, что я считал этот стахастический
[58:25.980 --> 58:31.740]  градиент по этому индексу. Вот. А saga говорит, окей, если я считал этот градиент, то давайте я его просто
[58:31.740 --> 58:37.700]  запомню. Вот. У меня есть некоторый набор переменных y, которые тоже размерности градиента. Вот.
[58:37.700 --> 58:42.820]  Размерности градиента. Вот. И в каждую y я изначально записываю нолики. Ничего там нет.
[58:42.820 --> 58:57.300]  Пусть на текущей итерации, на текущей итерации, я посчитал градиент f и kt. Вот. Тогда, смотрите,
[58:57.300 --> 59:03.860]  что я делаю. Я его запишу в соответствующий y. То есть вот я знаю, что для этого индекса я
[59:03.860 --> 59:10.820]  посчитал, я посчитал градиент. Я его записываю. В y запоминается тот градиент, который я считал.
[59:10.820 --> 59:16.580]  Вот. Остальные y остаются неизменными. Вот. И получается, что тогда вот изначально понятно,
[59:16.580 --> 59:22.580]  что у меня здесь ничего нет, у y вообще все нули. Я просто делаю что-то типа в духе SGD. Вот. У меня
[59:22.580 --> 59:29.420]  просто градиент по конкретному f и тому и kt выбирается. Окей. Но потом со временем, когда я
[59:29.420 --> 59:38.060]  повыбираю множество разных индексов, у меня вот в этой сумме будет в некотором смысле, так,
[59:38.060 --> 59:43.820]  лениво, что ли, с задержкой копится градиент. То есть когда я уже повыбираю все индексы,
[59:43.820 --> 59:49.860]  у меня реально все y будут не нулевыми. Будут не нулевыми. И там будет лежать какой-то градиент
[59:49.860 --> 59:58.180]  для конкретной функции fg. Fg. Единственная проблема, то что тут точка будет какая-то, ну, явно не текущая.
[59:58.180 --> 01:00:04.300]  Явно не текущая, а какая-то другая. Вот. Может быть значительно позже, значительно раньше, чем
[01:00:04.300 --> 01:00:13.060]  текущая точка, то есть дальше от решения. Вот. Но суть такая, то что у меня y в некотором смысле так вот
[01:00:13.060 --> 01:00:19.700]  лениво, заторможенно, запоздало копят себе градиент. Полный градиент, он будет нечестный, он будет
[01:00:19.700 --> 01:00:28.780]  такой вот немного покореженный, но это будет градиент. Какой-то похожий на правду. Вот. Ну и в чем
[01:00:28.780 --> 01:00:33.940]  идея? В чем идея? Ну, можно доказать, что там в силу того, что вы выбираете индекс независимо и
[01:00:33.940 --> 01:00:39.940]  равномерно, у вас вот g будет равномерно. По мотожданию равен этому честному градиенту. Ну,
[01:00:39.940 --> 01:00:46.420]  давайте проверим. Вот. Если вы индекс выбираете равномерно, то у вас вот здесь вот берется
[01:00:46.420 --> 01:00:55.620]  мотождание по и-катому. Тогда что здесь получается? 1 на n сумма f и тых, x-катых. Это я просто
[01:00:55.620 --> 01:01:00.860]  выписал определение математического ожидания для первого слагаемого. Каждая из слагаемых я
[01:01:00.860 --> 01:01:06.140]  выбираю с вероятностью 1 на n. 1 на n я просто вынес за пределы этой вероятности. Вот. Ну и
[01:01:06.140 --> 01:01:15.500]  аналогично с y. С y там будет 1 на n сумма y. Ну и сумма y между собой сократятся. Она вот здесь есть и
[01:01:15.500 --> 01:01:20.620]  здесь есть. Все, они между собой сократятся, а вот эта сумма это же просто честный градиент. Вот.
[01:01:20.620 --> 01:01:26.060]  Поэтому здесь получается по мотожданию честный градиент. При этом вот опять же,
[01:01:26.060 --> 01:01:31.500]  как я сказал, y это запоздалая версия. Вот. При этом что можно сказать? Что здесь хорошего можно
[01:01:31.500 --> 01:01:38.860]  сказать? Когда у меня x стремится к, как со звездой, ну я предполагаю, что это так. Это можно, конечно,
[01:01:38.860 --> 01:01:43.420]  доказать отдельно. То есть мы не будем доказывать уже сходимость этих всех алгоритмов. Вот. Просто
[01:01:43.420 --> 01:01:52.540]  смотрим на физику. Вот. У меня в y тоже будут в итоге, через много-много итераций, когда я обновлюсь,
[01:01:52.540 --> 01:01:59.260]  обновлюсь, обновлюсь несколько раз, градиент стремится к градиенту в точке x со звездой. Да,
[01:01:59.260 --> 01:02:05.620]  мы знаем, что этот градиент не равен нулю. Не равен нулю. Но в тумме-то это будет стремиться к
[01:02:05.620 --> 01:02:11.820]  полному градиенту. Поэтому вот то, что у меня хранится в сумме y, это 0. То, что у меня хранится в
[01:02:11.820 --> 01:02:17.400]  сумме y, это 0. При этом, что я еще знаю, вот это будет стремиться к 0, вот это будет стремиться
[01:02:17.400 --> 01:02:27.020]  к градиенту f и kT x со звездой, и это будет тоже стремиться к градиенту f и kT x со звездой,
[01:02:27.020 --> 01:02:38.360]  потому что x kT
[01:02:38.360 --> 01:02:43.600]  что по мотоожиданию равно несмещённой оценке градиента,
[01:02:43.600 --> 01:02:45.360]  то есть несмещённая оценка градиента G,
[01:02:45.360 --> 01:02:47.040]  просто нашего целого градиента,
[01:02:47.040 --> 01:02:54.440]  но при этом будет стремиться к нулю при исходимости к оптимуму.
[01:02:54.440 --> 01:02:56.440]  То есть SGD тоже давал несмещённую оценку,
[01:02:56.440 --> 01:02:59.040]  но у него дисперсия была некомпенсируема,
[01:02:59.040 --> 01:03:00.520]  она давала ассоциации.
[01:03:00.520 --> 01:03:03.360]  Тут, в связи с тем, что дисперсия будет в итоге стремиться к нулю,
[01:03:03.360 --> 01:03:06.560]  потому что у меня G-кат стремится к реальному градиенту,
[01:03:06.560 --> 01:03:09.560]  я и буду получать эффект, который называется редукция дисперсии,
[01:03:09.560 --> 01:03:12.560]  то есть уменьшение и устранение дисперсии.
[01:03:12.560 --> 01:03:13.560]  Вот вся физика.
[01:03:13.560 --> 01:03:17.560]  SGD, соответственно, такой вот метод, который первый это сделал.
[01:03:20.560 --> 01:03:23.560]  Минус, понятный тоже, это лишняя память.
[01:03:23.560 --> 01:03:29.560]  В силу того, что вам нужно для каждого индекса хранить свой Y, это дорого.
[01:03:29.560 --> 01:03:32.560]  Понятно, что если задача машинного обучения довольно большая,
[01:03:32.560 --> 01:03:36.560]  ну там какие-то регрессии, то есть какие-то маршрумсы, которые вы обучаете,
[01:03:36.560 --> 01:03:38.560]  это всё, конечно, просто.
[01:03:38.560 --> 01:03:40.560]  Там это можно ещё хранить.
[01:03:40.560 --> 01:03:43.560]  Но когда вы обучаете неровную сетку,
[01:03:43.560 --> 01:03:48.560]  лишняя память размера количества сэмплов умножить на размер сетки,
[01:03:48.560 --> 01:03:50.560]  это всё замечательно.
[01:03:50.560 --> 01:03:53.560]  То, что вам рассказывала Ёж про миллиарды, это правда.
[01:03:53.560 --> 01:03:56.560]  Даже если у вас там количество бачей, которые вы используете,
[01:03:56.560 --> 01:03:59.560]  ну десятки тысяч, то есть десятки тысяч на миллиарды,
[01:03:59.560 --> 01:04:02.560]  это прям огромное число, это невозможно хранить.
[01:04:06.560 --> 01:04:08.560]  Поэтому появляется ещё одна модификация,
[01:04:08.560 --> 01:04:10.560]  ну друг у друга просто подход,
[01:04:10.560 --> 01:04:12.560]  так называемый SVRG,
[01:04:12.560 --> 01:04:14.560]  Stochastic Variance Reduction Gradient,
[01:04:14.560 --> 01:04:16.560]  ну в честь него как раз редукция дисперсии,
[01:04:16.560 --> 01:04:19.560]  ну появилось вот это понятие редукция дисперсии.
[01:04:19.560 --> 01:04:20.560]  Вот.
[01:04:20.560 --> 01:04:21.560]  Тут подход следующий.
[01:04:21.560 --> 01:04:22.560]  Давайте я скажу так.
[01:04:22.560 --> 01:04:26.560]  У меня есть некоторая референсная точка V.
[01:04:26.560 --> 01:04:29.560]  Это то, что вам рассказывала Катя, должна была рассказывать вчера.
[01:04:29.560 --> 01:04:30.560]  Вот.
[01:04:30.560 --> 01:04:33.560]  Вы в этой референсной точке считаете полный градиент.
[01:04:33.560 --> 01:04:34.560]  Вот.
[01:04:34.560 --> 01:04:35.560]  А дальше суть следующая.
[01:04:35.560 --> 01:04:39.560]  Вы опроксимацию градиента делаете с помощью этой референсной точки.
[01:04:39.560 --> 01:04:43.560]  Вы как бы из полного градиента вычленяете тот кусочек,
[01:04:43.560 --> 01:04:45.560]  который соответствует текущему бачу,
[01:04:45.560 --> 01:04:47.560]  ну текущему индексу, который вы выбрали,
[01:04:47.560 --> 01:04:53.560]  и заменяете его на тот градиент,
[01:04:53.560 --> 01:04:55.560]  по этому индексу, который посчитан в текущей точке.
[01:04:55.560 --> 01:04:56.560]  Вот.
[01:04:56.560 --> 01:04:58.560]  Вот такая вот идея.
[01:04:58.560 --> 01:05:02.560]  То есть тут вот все завязывается на референсную точку.
[01:05:02.560 --> 01:05:05.560]  Вы очень редко считаете полный градиент.
[01:05:05.560 --> 01:05:09.560]  А дальше с помощью этого градиента в некотором смысле апроксимируете,
[01:05:09.560 --> 01:05:11.560]  апроксимируете полный градиент.
[01:05:11.560 --> 01:05:15.560]  То есть по-хорошему вам бы хотелось, конечно, считать вот что-то вот такое.
[01:05:15.560 --> 01:05:16.560]  Вот.
[01:05:16.560 --> 01:05:21.560]  Но здесь вы видите как бы из-за того, что вы здесь берете разность,
[01:05:21.560 --> 01:05:23.560]  вы можете в некотором смысле сказать,
[01:05:23.560 --> 01:05:29.560]  что вы сюда привнесли какой-то клад от реально текущей точки,
[01:05:29.560 --> 01:05:31.560]  ну и они могут быть действительно похожи.
[01:05:31.560 --> 01:05:35.560]  Более того, если вы, мы так вот просто порассуждаем,
[01:05:35.560 --> 01:05:38.560]  если у меня x-кате стремится к x звездой,
[01:05:38.560 --> 01:05:43.560]  тогда у меня и v тоже будет стремиться к x звездой.
[01:05:43.560 --> 01:05:47.560]  А получается то, что и g будут стремиться к x звездой.
[01:05:47.560 --> 01:05:48.560]  Почему?
[01:05:48.560 --> 01:05:54.560]  Потому что вот это стремится у меня к f, it, kt, x звездой,
[01:05:54.560 --> 01:05:59.560]  и это у меня стремится к f, it, kt, x звездой,
[01:05:59.560 --> 01:06:05.560]  что не ноль, но хорошо, без разницы, это не страшно, они друг друга сейчас убьют.
[01:06:05.560 --> 01:06:07.560]  А это у меня будет стремиться к нулю,
[01:06:07.560 --> 01:06:11.560]  потому что v стремится к x звездой, а полный градиент будет стремиться к нулю.
[01:06:11.560 --> 01:06:12.560]  Вот.
[01:06:12.560 --> 01:06:14.560]  Здесь вот такая вот идея.
[01:06:15.560 --> 01:06:17.560]  Полный градиент через вот эту разность,
[01:06:17.560 --> 01:06:20.560]  через такую-такую разность, через референсную точку.
[01:06:20.560 --> 01:06:21.560]  Вот.
[01:06:21.560 --> 01:06:23.560]  И здесь уже никакой памяти лишней не нужно,
[01:06:23.560 --> 01:06:25.560]  как это было в сага.
[01:06:25.560 --> 01:06:26.560]  Вот.
[01:06:26.560 --> 01:06:28.560]  Здесь вы просто вот, к сожалению,
[01:06:28.560 --> 01:06:30.560]  это тоже минус, вы сейчас считаете полный градиент,
[01:06:30.560 --> 01:06:34.560]  потому что вроде как мы вообще начинали делать SGD,
[01:06:34.560 --> 01:06:37.560]  чтобы не считать полный градиент, потому что дорого.
[01:06:37.560 --> 01:06:38.560]  Вот.
[01:06:38.560 --> 01:06:40.560]  Здесь вас заставляют делать,
[01:06:41.560 --> 01:06:45.560]  ну, как раз в некотором смысле это минус этого подхода,
[01:06:45.560 --> 01:06:49.560]  то есть в одном случае у вас память страдает,
[01:06:49.560 --> 01:06:51.560]  в другом случае вычислительные ресурсы,
[01:06:51.560 --> 01:06:53.560]  потому что полный градиент дороговато.
[01:06:53.560 --> 01:06:55.560]  Так, это мы с вами обсудили.
[01:06:55.560 --> 01:07:00.560]  Опять же, можно доказать, что по математическому ожиданию это будет честный градиент.
[01:07:00.560 --> 01:07:03.560]  Сходимость к нулю.
[01:07:03.560 --> 01:07:06.560]  Ну и вот из минусов, опять же, полный градиент.
[01:07:06.560 --> 01:07:09.560]  Плюс, каждую турацию вам нужно считать два раза градиент
[01:07:09.560 --> 01:07:11.560]  по текущему бачу.
[01:07:11.560 --> 01:07:13.560]  В точке, соответственно, XK
[01:07:13.560 --> 01:07:15.560]  и в референционной точке V.
[01:07:15.560 --> 01:07:17.560]  Вот.
[01:07:17.560 --> 01:07:19.560]  Ну, что, в принципе, тоже чуть-чуть
[01:07:19.560 --> 01:07:21.560]  удорожает ваш метод.
[01:07:21.560 --> 01:07:23.560]  Удорожает метод.
[01:07:23.560 --> 01:07:25.560]  Вот. Окей.
[01:07:25.560 --> 01:07:27.560]  Со SWRG разобрались.
[01:07:27.560 --> 01:07:29.560]  И третий метод, который тоже является популярным
[01:07:29.560 --> 01:07:32.560]  и, на самом деле, наверное, самым таким рабастым,
[01:07:32.560 --> 01:07:35.560]  ну, и с точки зрения сходимости пантовым,
[01:07:35.560 --> 01:07:37.560]  это SARA.
[01:07:37.560 --> 01:07:39.560]  Вот.
[01:07:39.560 --> 01:07:41.560]  Здесь суть похожа на SWRG.
[01:07:41.560 --> 01:07:45.560]  Вам тоже нужно в некотором смысле иногда считать какое-то референсное значение.
[01:07:45.560 --> 01:07:47.560]  Референсное значение, полный градиент.
[01:07:47.560 --> 01:07:49.560]  Но здесь, соответственно,
[01:07:49.560 --> 01:07:51.560]  авторы предлагают сделать
[01:07:51.560 --> 01:07:53.560]  update чуть мягче.
[01:07:53.560 --> 01:07:55.560]  Чуть мягче, потому что
[01:07:55.560 --> 01:07:57.560]  там вам предлагали всегда отсылаться
[01:07:57.560 --> 01:07:59.560]  к этой референсной точке.
[01:07:59.560 --> 01:08:01.560]  Всегда говорить, окей, я буду считать значение,
[01:08:01.560 --> 01:08:03.560]  иметь градиент референсной точки,
[01:08:03.560 --> 01:08:05.560]  который я сохранил.
[01:08:05.560 --> 01:08:07.560]  Вот.
[01:08:07.560 --> 01:08:09.560]  И, соответственно, считать стахастический градиент в этой референсной точке
[01:08:09.560 --> 01:08:11.560]  и вычислять эту аппроксимацию.
[01:08:11.560 --> 01:08:13.560]  Здесь они говорят, окей,
[01:08:13.560 --> 01:08:15.560]  давайте сделаем чуть помягче
[01:08:15.560 --> 01:08:17.560]  и будем сдвигать
[01:08:17.560 --> 01:08:19.560]  вот этот референсный градиент.
[01:08:19.560 --> 01:08:21.560]  То есть, там он фиксирован вместе с точкой.
[01:08:21.560 --> 01:08:23.560]  А здесь он обновляется следующим образом.
[01:08:23.560 --> 01:08:25.560]  То есть, вот у вас референсный градиент,
[01:08:25.560 --> 01:08:27.560]  вы из него считаете значение...
[01:08:29.560 --> 01:08:31.560]  Тут плюс.
[01:08:31.560 --> 01:08:33.560]  Вы вычитаете значение
[01:08:33.560 --> 01:08:35.560]  по текущему бачу в предыдущей точке.
[01:08:35.560 --> 01:08:37.560]  Вот.
[01:08:37.560 --> 01:08:39.560]  И добавляете опять же значение градиента
[01:08:39.560 --> 01:08:41.560]  по текущему бачу в текущей точке.
[01:08:41.560 --> 01:08:43.560]  И вот у вас этот референсный градиент
[01:08:43.560 --> 01:08:45.560]  в некотором смысле
[01:08:45.560 --> 01:08:47.560]  вот так вот обновляется.
[01:08:47.560 --> 01:08:49.560]  Чем-то похоже и на SVRG,
[01:08:49.560 --> 01:08:51.560]  потому что есть референсная точка.
[01:08:51.560 --> 01:08:53.560]  Чем-то похоже и на
[01:08:53.560 --> 01:08:55.560]  SAGA, потому что у вас
[01:08:55.560 --> 01:08:57.560]  здесь
[01:08:57.560 --> 01:08:59.560]  есть какой-то эффект
[01:08:59.560 --> 01:09:01.560]  сохранения, но при этом нет вот этой
[01:09:01.560 --> 01:09:03.560]  большой памяти.
[01:09:03.560 --> 01:09:05.560]  Но при этом вы как-то перезаписываете
[01:09:05.560 --> 01:09:07.560]  значение градиента
[01:09:07.560 --> 01:09:09.560]  по текущему бачу.
[01:09:09.560 --> 01:09:11.560]  То есть, вот такая вот идея.
[01:09:11.560 --> 01:09:13.560]  Чуть более мягкая, чем SVRG.
[01:09:13.560 --> 01:09:15.560]  И действительно, на практике
[01:09:15.560 --> 01:09:17.560]  по траектории исходимости у SAR
[01:09:17.560 --> 01:09:19.560]  она более плавненькая получается,
[01:09:19.560 --> 01:09:21.560]  потому что референсная точка может быть
[01:09:21.560 --> 01:09:23.560]  очень далеко, особенно на начальных
[01:09:23.560 --> 01:09:25.560]  итерациях.
[01:09:25.560 --> 01:09:27.560]  Поэтому градиент, который вы там считаете,
[01:09:27.560 --> 01:09:29.560]  довольно неприятный становится.
[01:09:29.560 --> 01:09:31.560]  А здесь
[01:09:31.560 --> 01:09:33.560]  с силой того, что вы вот так вот
[01:09:33.560 --> 01:09:35.560]  J пересчитываете итеративно,
[01:09:35.560 --> 01:09:37.560]  референсный градиент становится более плавным.
[01:09:37.560 --> 01:09:39.560]  Исходится это лучше.
[01:09:41.560 --> 01:09:43.560]  Названо, кстати, в честь ребенка.
[01:09:43.560 --> 01:09:45.560]  Это вот у
[01:09:45.560 --> 01:09:47.560]  сейчас помню.
[01:09:47.560 --> 01:09:49.560]  У вьетнамца
[01:09:49.560 --> 01:09:51.560]  Лама, дочка SAR маленькая,
[01:09:51.560 --> 01:09:53.560]  родилась, когда он придумал этот метод.
[01:09:53.560 --> 01:09:55.560]  Кстати, парень заканчивал аспирантуру МГУ.
[01:09:55.560 --> 01:09:57.560]  Нет, магистратуру МГУ.
[01:09:57.560 --> 01:09:59.560]  А потом поехал в Америку
[01:09:59.560 --> 01:10:01.560]  учиться там на PHD.
[01:10:01.560 --> 01:10:03.560]  Сейчас он, по-моему, работает.
[01:10:03.560 --> 01:10:05.560]  Толь в Sony, по-моему,
[01:10:05.560 --> 01:10:07.560]  он работает.
[01:10:07.560 --> 01:10:09.560]  Учился он у Мартина Такача,
[01:10:09.560 --> 01:10:11.560]  у профессора,
[01:10:11.560 --> 01:10:13.560]  к которому я часто езжу, поэтому знаю
[01:10:13.560 --> 01:10:15.560]  историю этого метода.
[01:10:15.560 --> 01:10:17.560]  SVRG, соответственно, про него
[01:10:17.560 --> 01:10:19.560]  тоже более плавненько.
[01:10:19.560 --> 01:10:21.560]  Там уже нет
[01:10:21.560 --> 01:10:23.560]  не смещенности по условным отожданию,
[01:10:23.560 --> 01:10:25.560]  но есть смещенность по
[01:10:25.560 --> 01:10:27.560]  полным отожданию.
[01:10:27.560 --> 01:10:29.560]  Идея ровно такая же.
[01:10:29.560 --> 01:10:31.560]  Почему это стремится
[01:10:31.560 --> 01:10:33.560]  к чему-то хорошему?
[01:10:33.560 --> 01:10:35.560]  Потому что у вас разность стремится к нулю.
[01:10:35.560 --> 01:10:37.560]  Потому что если X стремится к
[01:10:37.560 --> 01:10:39.560]  X звездой, то вот это стремится к X звездой.
[01:10:39.560 --> 01:10:41.560]  Значит, разность стремится к нулю.
[01:10:41.560 --> 01:10:43.560]  Но при этом получается,
[01:10:43.560 --> 01:10:45.560]  что ZHK у вас стремится все же
[01:10:45.560 --> 01:10:47.560]  к константе.
[01:10:47.560 --> 01:10:49.560]  SVRG вроде как было получше.
[01:10:49.560 --> 01:10:51.560]  Здесь у вас в рамках
[01:10:51.560 --> 01:10:53.560]  одной эпохи
[01:10:53.560 --> 01:10:55.560]  ZHK просто застопорится.
[01:10:55.560 --> 01:10:57.560]  То есть в вашем методе вы его запустите,
[01:10:57.560 --> 01:10:59.560]  инициализируете как-то ZHK.
[01:10:59.560 --> 01:11:01.560]  Дальше он у вас пойдет, пойдет, пойдет,
[01:11:01.560 --> 01:11:03.560]  и ZHK в итоге станет какой-то константой,
[01:11:03.560 --> 01:11:05.560]  потому что не меняется.
[01:11:05.560 --> 01:11:07.560]  Но в силу того, что вы делаете этот апдейт,
[01:11:07.560 --> 01:11:09.560]  вы обновляете ZHK,
[01:11:09.560 --> 01:11:11.560]  то она у вас в итоге
[01:11:11.560 --> 01:11:13.560]  становится, тоже будет стремиться
[01:11:13.560 --> 01:11:15.560]  к нулю, потому что здесь
[01:11:15.560 --> 01:11:17.560]  вот точка, которая в итоге встает сюда,
[01:11:17.560 --> 01:11:19.560]  она же стремится к X звездой.
[01:11:19.560 --> 01:11:21.560]  Поэтому и ZHK будет стремиться к X звездой.
[01:11:21.560 --> 01:11:23.560]  В рамках одной эпохи константов.
[01:11:29.560 --> 01:11:31.560]  Ну и давайте кратенько вывод
[01:11:31.560 --> 01:11:33.560]  по тому, что здесь происходило.
[01:11:33.560 --> 01:11:35.560]  Методы редукции диспетчер
[01:11:35.560 --> 01:11:37.560]  вообще хорошая альтернатива, особенно для задач
[01:11:37.560 --> 01:11:39.560]  конечной суммы. Сходимость
[01:11:39.560 --> 01:11:41.560]  будет, тут соответственно, доказана линейная.
[01:11:41.560 --> 01:11:43.560]  То есть тут нет никаких эффектов
[01:11:43.560 --> 01:11:45.560]  от того, что у вас есть стахастика
[01:11:45.560 --> 01:11:47.560]  в градиенте. Более того,
[01:11:47.560 --> 01:11:49.560]  чем классна сходимость всех этих трех методов
[01:11:49.560 --> 01:11:51.560]  в сильно выпуклом случае,
[01:11:51.560 --> 01:11:53.560]  выпуклом она уже отличается, и вне выпуклом
[01:11:53.560 --> 01:11:55.560]  тоже. Но в сильно выпуклом случае
[01:11:55.560 --> 01:11:57.560]  итерационная сложность
[01:11:57.560 --> 01:11:59.560]  общая, то есть учитывая
[01:11:59.560 --> 01:12:01.560]  и внутренние, и внешние циклы, она
[01:12:01.560 --> 01:12:03.560]  будет вот такая вот. На какую
[01:12:03.560 --> 01:12:05.560]  итерационную сложность она похожа?
[01:12:07.560 --> 01:12:09.560]  Какой это метод? Если
[01:12:09.560 --> 01:12:11.560]  N-ку особенно отсюда уберу.
[01:12:11.560 --> 01:12:13.560]  Обычный градиентный спуск, правда?
[01:12:13.560 --> 01:12:15.560]  Вот. А то
[01:12:15.560 --> 01:12:17.560]  какая сложность у градиентного спуска?
[01:12:17.560 --> 01:12:19.560]  У него стоит одна итерация.
[01:12:19.560 --> 01:12:21.560]  Он должен посчитать полный градиент,
[01:12:21.560 --> 01:12:23.560]  а это будет в N раз
[01:12:23.560 --> 01:12:25.560]  дороже, чем здесь.
[01:12:25.560 --> 01:12:27.560]  То есть здесь итерация стоит, условно, единицу,
[01:12:27.560 --> 01:12:29.560]  потому что нужно посчитать только один
[01:12:29.560 --> 01:12:31.560]  сэппл, ну либо два, вот единицы, будем так
[01:12:31.560 --> 01:12:33.560]  говорить. Вот.
[01:12:33.560 --> 01:12:35.560]  Ну и на эту N-ку вообще можно забить, то есть
[01:12:35.560 --> 01:12:37.560]  там у градиента спуска оценка будет вот такая
[01:12:37.560 --> 01:12:39.560]  вообще, на сложность именно
[01:12:39.560 --> 01:12:41.560]  если мы не говорим не на итерационную
[01:12:41.560 --> 01:12:43.560]  сложность, а именно сложность по количеству сэмплов,
[01:12:43.560 --> 01:12:45.560]  то тут будет сложность вот такая. Понятно, что
[01:12:45.560 --> 01:12:47.560]  на эту N-ку L на μ это у вас больше единицы,
[01:12:47.560 --> 01:12:49.560]  поэтому на эту N-ку можно вообще
[01:12:49.560 --> 01:12:51.560]  не смотреть. Вот.
[01:12:51.560 --> 01:12:53.560]  Получается, что он в N раз дешевле,
[01:12:53.560 --> 01:12:55.560]  а делает столько же итераций,
[01:12:55.560 --> 01:12:57.560]  теории. Но это супер.
[01:12:57.560 --> 01:12:59.560]  На самом деле получается, вы за бесплатно
[01:12:59.560 --> 01:13:01.560]  имеете метод, который
[01:13:01.560 --> 01:13:03.560]  сходится значительно
[01:13:03.560 --> 01:13:05.560]  так же, но дешевле. Но дешевле,
[01:13:05.560 --> 01:13:07.560]  поэтому
[01:13:07.560 --> 01:13:09.560]  welcome. Но проблема в том, что
[01:13:09.560 --> 01:13:11.560]  опять же
[01:13:11.560 --> 01:13:13.560]  есть свои недостатки, во-первых,
[01:13:13.560 --> 01:13:15.560]  это память,
[01:13:15.560 --> 01:13:17.560]  усаги, подсчет полного градиента
[01:13:17.560 --> 01:13:19.560]  УСВРГ и Сары.
[01:13:19.560 --> 01:13:21.560]  С этим пока не понятно,
[01:13:21.560 --> 01:13:23.560]  как иметь дело.
[01:13:23.560 --> 01:13:25.560]  Community не знает на это ответ.
[01:13:25.560 --> 01:13:27.560]  Нет такого метода, который
[01:13:27.560 --> 01:13:29.560]  эти недостатки полностью уничтожает.
[01:13:29.560 --> 01:13:31.560]  Но, в принципе, рабочие варианты
[01:13:31.560 --> 01:13:33.560]  для задачи именно суммы,
[01:13:33.560 --> 01:13:35.560]  задачи минимизации
[01:13:35.560 --> 01:13:37.560]  имперического риска. Есть ускоренные версии,
[01:13:37.560 --> 01:13:39.560]  Катя вам про них рассказывала.
[01:13:39.560 --> 01:13:41.560]  В том числе называется
[01:13:41.560 --> 01:13:43.560]  Катюша. Придумала, кстати, китаец
[01:13:43.560 --> 01:13:45.560]  из Майкрософта. Что-то у него не знаю, но
[01:13:45.560 --> 01:13:47.560]  женские имена его тянет.
[01:13:47.560 --> 01:13:49.560]  У него есть Наташа, есть Катюша,
[01:13:49.560 --> 01:13:51.560]  что-то у него еще есть.
[01:13:53.560 --> 01:13:55.560]  Получается, что для ускоренной методы
[01:13:55.560 --> 01:13:57.560]  свои оценки, понятно, они похожи
[01:13:57.560 --> 01:13:59.560]  на нести русские. Вот такая вот идея.
[01:13:59.560 --> 01:14:01.560]  Все, спасибо, у нас как раз время
[01:14:01.560 --> 01:14:03.560]  произошло к танцу.

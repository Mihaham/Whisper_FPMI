[00:00.000 --> 00:12.120]  Так, ну что, ребята, я вас всех приветствую. Меня зовут Шаробоков Максим Геннадьевич. Я буду
[00:12.120 --> 00:21.920]  вести у вас лекции по случайным процессам. Ну, это предмет второй в вероятностном цикле вашем
[00:21.920 --> 00:28.120]  с кафедромоу. Первое это теория вероятностей, второе это случайные процессы, и в следующем
[00:28.120 --> 00:35.240]  семестре у вас будет мат-статистика. Что вас ждет, в принципе? Ну, сначала про организационные моменты
[00:35.240 --> 00:46.640]  поговорим. Вы должны будете сдать два задания. Возможно, в середине семестра будет контрольная
[00:46.640 --> 00:52.840]  работа. Ну, если у нас будут лекции, семинары там мощными оставаться, то, скорее всего, будет
[00:52.840 --> 00:58.560]  потоковая большая контрольная работа. Ну, а если там как-то мы перейдем на дистанционку,
[00:58.560 --> 01:07.640]  ну, там тогда на месте разберемся. И экзамен в конце. Вот. Так что два задания, контрольный экзамен.
[01:07.640 --> 01:24.840]  Так, что еще сказать? Ну, есть сайт, на котором лежат все мои материалы. Это они находятся на
[01:24.840 --> 01:32.240]  Notion. Ну, там ссылка сложная, но если вы загуглите просто Notion, там Шарабоков, случайные процессы,
[01:32.240 --> 01:42.000]  вы найдете. И там, вот на странице, посвященном случайным процессам, вы дадете очень много всего.
[01:42.000 --> 01:52.640]  Там лекции, PDF-видеозаписи, семинары, PDF-видеозаписи, программа курса, список литературы, список вопросов
[01:52.640 --> 02:00.360]  там на понимание, которые я часто задаю там на экзаменах, где-то на сдачах и так далее. Там же
[02:00.360 --> 02:12.920]  будет появляться список задач для сдачи заданий. И, в общем, очень полезная вещь. Так что вы,
[02:12.920 --> 02:21.960]  как можно раньше, с этим ознакомьтесь, туда зайдите, посмотрите, что там за материалы, и это должно
[02:21.960 --> 02:30.440]  стать вашим рабочим ресурсом. Вот эта вещь, потому что там все уже лежит. Ну, там уже есть какие-то
[02:30.440 --> 02:35.320]  лекции, я первый раз их прочитал в прошлом году, и я не думаю, что будут какие-то очень существенные
[02:35.320 --> 02:40.520]  изменения в этом году. Ну, может быть, где-то там файн-тюнинг какой-то проведу в отдельных лекциях,
[02:40.520 --> 02:56.680]  в отдельные моменты. Кто-то пытается сюда на улицу выйти. Ну так вот, хорошо, это что касается
[02:56.680 --> 03:04.800]  организации. Значит, куда писать вопросы? Вы можете мне писать вопросы по предмету. Значит,
[03:04.800 --> 03:25.280]  на почту. Вот, можете писать мне в ВК. Кому как удобно. Ну, я в ВК сейчас стало что-то редко
[03:25.280 --> 03:32.560]  появляться, и уведомления у меня нет, во всяком случае. А сюда можете спокойно мне писать. Вот,
[03:32.560 --> 03:39.320]  если возникают какие-то вопросы по организации, по предмету, по науке, по всему. Вот, и я всегда
[03:39.320 --> 03:46.040]  отвечу, помогу. Так, ну, наверное, по организации все основное, что я хотел сказать. Если у вас
[03:46.040 --> 03:57.560]  вопросы какие-то. Так более-менее понятно, что происходит. Ну, хорошо. Значит, еще раз обращаясь
[03:57.560 --> 04:03.520]  к списку литературы. Мой курс предполагает, что вам ничего не надо, кроме моих лекций. То есть,
[04:03.520 --> 04:10.120]  лекции абсолютно самодостаточны. Литература, по случайным процессам, ее очень много. Просто
[04:10.120 --> 04:16.440]  невероятное количество. Но предмет сам по себе нетривиальный, сложный. И все эти книжки,
[04:16.440 --> 04:23.080]  они написаны по-разному, разным языком. И вряд ли вы найдете книжку, которая очень близка к нашему
[04:23.080 --> 04:28.600]  курсу. Но единственное, что может быть, книжку, которую мы написали в соавторстве, у нас там коллектив был,
[04:28.600 --> 04:37.280]  Гасников, я и там еще несколько наших товарищей. Вот. Наверное, эта книжка близка. Ну, наверное,
[04:37.280 --> 04:44.560]  самая близкая, на сто процентов близкая, это то, что мои лекции ПДФ, которые я выкладываю. Вот. А так,
[04:44.560 --> 04:52.280]  если вы туда зайдете, вы увидите, что я там даже загруппировал некоторые литературу, там литературу,
[04:52.280 --> 04:55.920]  в которой много примеров, с которой надо начинать читать литературу, которые фундаментальные,
[04:55.920 --> 05:03.160]  классические книжки. Вот. Специализированная литература. В общем, там вы все найдете. А я,
[05:03.160 --> 05:09.240]  наверное, перейду уже к самому предмету, если вопросов по организации нет. Значит, смотрите,
[05:09.240 --> 05:16.640]  случайные процессы. Про что это вообще? Есть такие явления, которые развиваются во времени,
[05:16.640 --> 05:24.960]  предсказать поведение которых мы не можем. Почему мы не можем предсказать? По разным причинам. Мы
[05:24.960 --> 05:31.040]  можем перезапускать один и тот же процесс, если у нас есть такая возможность, и наблюдать разные
[05:31.040 --> 05:36.520]  его реализации. Сейчас он пошел так, потом он пошел всяк. Например, вы взяли монетку, подкидываете ее,
[05:36.520 --> 05:41.800]  да, и следите за результатами. Орел, решка, там, решка, решка, орел, решка. И если вы заново
[05:41.800 --> 05:46.560]  перезапустите этот процесс, то вы увидите другую последовательность. Вот. И вы не можете
[05:46.560 --> 05:51.720]  предсказать заранее, что выпадет потом. На самом деле, процесс может быть детерминированный. То есть,
[05:51.720 --> 05:56.600]  если напрячься и его описать, то, может быть, и можно понять, как он будет дальше развиваться. Но
[05:56.600 --> 06:02.800]  есть такие детерминированные процессы, которые очень трудно описать. И проще к ним относиться,
[06:02.800 --> 06:09.800]  как к случайным процессам. Вот. Неформально говоря, вот такие развивающиеся во времени случайные
[06:09.800 --> 06:16.800]  явления, когда есть какая-то элемент неопределенности будущего поведения, вот такие явления мы будем
[06:16.800 --> 06:24.280]  называть случайными процессами. И встречаются они просто вообще повсюду. Вот я просто из разных
[06:24.280 --> 06:29.680]  областей науки возьму примеры. Вот, как я говорил, там, допустим, подбрасывание монетки. Если вы
[06:29.680 --> 06:37.400]  приходите на остановку, ждете автобус, и вы не знаете, когда он придет, если вы рассмотрите там
[06:37.400 --> 06:43.520]  функцию, значит, индикатор наличия автобуса на остановке, то какое-то случайное время его нет,
[06:43.520 --> 06:48.640]  потом какое-то случайное время он стоит, потом его снова какого-то случайное время нет. Такая
[06:48.640 --> 06:55.880]  случайная функция получается. В науке, например, в теории сигналов радиосигнал приходит вам с
[06:55.880 --> 07:02.200]  какими-то помехами. Эти помехи делают его случайным. В машинном обучении, когда вы обучаете какую-то
[07:02.320 --> 07:11.920]  модель и следите за качеством модели как функции времени. Вот такая спадающая величина. Если вы
[07:11.920 --> 07:15.360]  процесс перезапустите, у вас оптимизатор стахастический, то он пойдет по другому пути,
[07:15.360 --> 07:25.000]  тоже случайная кривая. Какие еще можно привести там примеры? Из физики, например, если вы возьмете
[07:25.000 --> 07:29.560]  какую-то жидкость, возьмете очень маленький подкрашенный шарик, поместите его в эту жидкость,
[07:29.560 --> 07:34.480]  будьте за ним под микроскопом следить, то вы увидите, как он трясется за счет того,
[07:34.480 --> 07:40.000]  что молекулы ударяют по этому шарику. То есть координат этого шарика тоже можно описывать как
[07:40.000 --> 07:49.040]  какие-то случайные функции. В общем, повсюду там курс валюты, курс акций, вот эти все кривые,
[07:49.040 --> 07:54.560]  так как мы не знаем будущего, как они себя поведут, это к ним тоже можно относиться как к случайным
[07:54.560 --> 07:58.760]  каким-то функциям. В общем, повсюду вот эта случайность, которая развивается во времени,
[07:58.760 --> 08:05.640]  она присутствует. И что мы должны сразу уяснить по этому поводу? И самое первое, что вы должны
[08:05.640 --> 08:11.360]  запомнить, то что мы не всегда можем предсказать, как будет дальше развиваться процесс. На самом деле
[08:11.360 --> 08:16.520]  почти всегда мы этого не можем. То есть мы не можем однозначно сказать, посмотрев на предысторию
[08:16.520 --> 08:21.960]  случайного процесса, как он будет развиваться в дальнейшем. Все, эту мысль вы можете сразу себе
[08:21.960 --> 08:26.320]  запомнить. Если вы не имеете возможности максимально точно, детерминированными моделями,
[08:26.320 --> 08:31.680]  описать поведение процесса, все, вы его не предскажете, даже смотря на его историю,
[08:31.680 --> 08:41.880]  что будет дальше. Ну что мы можем делать? Не все так грустно, хотя мы не можем предсказать,
[08:41.880 --> 08:46.480]  что будет дальше, как будет развиваться процесс. Мы можем попробовать оценить,
[08:46.640 --> 08:52.280]  вычислить вероятность того, что процесс поведет себя тем или иным образом, вероятность того,
[08:52.280 --> 08:58.040]  что процесс будет возрастать в каком-то участке времени, или убывать, или сначала убывать,
[08:58.040 --> 09:03.280]  потом возрастать, или посчитать какие-то численные характеристики, связанные процессом,
[09:03.280 --> 09:09.840]  там средние процессы, как ведет себя процесс в среднем, каков его размах. Но для того,
[09:09.840 --> 09:16.800]  чтобы довести до числа, чтобы понять, что будет происходить с процессом, хотя бы в этих терминах,
[09:16.800 --> 09:22.160]  вероятностных терминах, чтобы довести до числа, чтобы вычислить эти характеристики, нам нужна модель,
[09:22.160 --> 09:28.280]  подходящая модель описания случайного явления, случайного процесса. Вот, и вот тут начинается
[09:28.280 --> 09:35.440]  математика. Как мы будем это моделировать? Мы построим модели случайных явлений, случайных
[09:35.440 --> 09:41.360]  процессов на основе теории вероятности. У вас была теория вероятности, много чего там было,
[09:41.360 --> 09:45.840]  вам сказали, что такое случайные величины, функция распределения, плотность, мат ожидания,
[09:45.840 --> 09:51.680]  дисперсия, формулы все эти, хар-функция и так далее, всякие корреляции, независимость и так далее.
[09:51.680 --> 09:57.240]  Вот это у вас все было, и мы попробуем применить весь вот этот аппарат к изучению уже случайных
[09:57.240 --> 10:02.720]  функций, не случайных величин. Когда ты касаешься к величине, она тебе выдает какое-то значение
[10:02.720 --> 10:08.120]  мгновенно, снова коснулся до нее, она тебе снова какое-то значение выдала, а теперь все это
[10:08.120 --> 10:13.080]  разворачивается во времени. Но как перейти от случайных величин, стационардных, независящих от
[10:13.080 --> 10:18.960]  времени, вещей, сущностей, к изависищим от времени, давайте мы добавим просто некий свободный
[10:18.960 --> 10:26.280]  детерминированный параметр t, и что мы получим? Вот давайте смотреть. Сегодня на этой лекции мы
[10:26.280 --> 10:32.240]  попробуем с вами вот этот фундамент случайных процессов построить и посмотрим, какие из него
[10:32.240 --> 10:37.240]  будут следствия. Первое определение, которое мы дадим, это определение случайного процесса, ведь про это
[10:37.240 --> 10:47.720]  же наш курс. Случайным процессом, я буду сокращать, случайным процессом обозначается он вот так вот,
[10:47.720 --> 10:57.440]  я все поясню сейчас t, значит с какого-то множества t большое, как под множество r. Случайным
[10:57.440 --> 11:10.720]  процессом называется параметрическое семейство случайных величин, определенных на одном
[11:10.720 --> 11:25.360]  вероятностном пространстве. Давайте мы его сразу обозначим, Омега ФП. Смотрите, тут сложные слова,
[11:25.360 --> 11:30.080]  да, я понимаю, но смотрите, в чем суть. Случайная величина это функция исхода, это кси от Омега,
[11:30.080 --> 11:37.320]  вы это должны прекрасно знать. Здесь что? Мы добавляем еще одну вещь, которую называем временем,
[11:37.320 --> 11:44.440]  хотя физически это может не обязательно быть время, но его люди называют временем,
[11:44.440 --> 11:50.360]  это t. Оно может быть дискретным, может быть непрерывным, неважно. И у нас появляются две
[11:50.360 --> 11:56.760]  переменные. Если мы зафиксируем t, то по определению мы получим случайную величину.
[11:56.760 --> 12:09.160]  Вот, t значит время, давайте так, Омега исход. Значит, если мы фиксируем время, t0, допустим,
[12:09.160 --> 12:19.120]  обозначим, то мы получим случайную величину, она называется сечением процесса. Это случайная
[12:19.120 --> 12:29.240]  величина. Наоборот, если мы зафиксируем исход, мы получаем обычную детерминированную функцию,
[12:29.240 --> 12:34.400]  обычную функцию экспонента, синус, косинус, всякие линейные функции, всякое может здесь быть.
[12:34.400 --> 12:47.880]  Эта функция называется реализацией процесса. Еще и называют траекторией. Траектория процесса.
[12:47.880 --> 12:56.200]  Вот, зафиксировали время, получили сечение, случайную величину. Она принимает различные
[12:56.200 --> 13:01.640]  свои значения в зависимости от того, какой исход будет. Если мы зафиксируем исход, у нас
[13:01.640 --> 13:09.320]  фиксируется вся эта кривая, она определена, это детерминированная кривая. Ну вот, сечение и
[13:09.320 --> 13:15.560]  реализация. И теперь смотрите, хорошо, мы ввели такую модель, теперь вернемся к нашей проблеме,
[13:15.560 --> 13:20.280]  прогнозы, предсказания поведения процессов. Вот рассмотрим какой-нибудь процесс, например,
[13:20.280 --> 13:27.320]  там курс акций, он как-то развивается, и пусть вот этот момент времени Т настоящее. И есть где-то
[13:27.320 --> 13:33.960]  Т-будущее. Все, что нам дано, это прошлое и настоящее. В будущий момент времени мы не знаем,
[13:33.960 --> 13:40.880]  что будет. Мы не знаем, что будет. Процесс может принять вот такое значение. Такое, такое, такое.
[13:40.880 --> 13:47.040]  Видите, может разные значения принимать. Где-то точки могут быть плотнее расположены, то есть вероятность
[13:47.040 --> 13:52.720]  попасть туда больше. Вот это отвечает распределение соответствующей случайной величины. Вот эта
[13:52.720 --> 13:57.320]  случайная величина, которая принимает вот эти значения в этот момент времени, это есть сечение в
[13:57.320 --> 14:05.040]  этот момент времени. Вот это вот Си от Омега Т-будущее. Т-будущее мы зафиксировали, видите,
[14:05.040 --> 14:12.600]  расцекли сечение. А вот эти различные реализации отвечают просто различным исходам. И проблема
[14:12.600 --> 14:22.080]  здесь состоит вся в том, что в общем случае различным исходам может соответствовать одна и та же
[14:22.080 --> 14:29.520]  предыстория. То есть если у вас вот это вот Омега ноль, и для реализации Омега ноль процесс развивается
[14:29.520 --> 14:37.360]  вот так, то для какой-нибудь другой Омеги один, прошлое и настоящее может быть одинаковым, а будущее
[14:37.360 --> 14:45.200]  разным. И вы не знаете, какая Омега на самом деле реализовалась. То есть когда, смотрите, как вот на
[14:45.200 --> 14:49.800]  это можно посмотреть. Вот вы смотрите на курс акции, вот растет вот эта кривая. Как это можно
[14:49.800 --> 14:55.240]  интерпретировать? Что мы реально наблюдаем? Мы реально наблюдаем некоторую реализацию. Реально
[14:55.240 --> 15:01.320]  мы наблюдаем некую реализацию. Мы видим ее историю, прошлое и настоящее. Мы не знаем ее
[15:01.320 --> 15:07.480]  будущее. Почему? Где-то как будто где-то во Вселенной реализовался этот исход для некоторой функции
[15:07.480 --> 15:14.280]  кси Омега Т, и вот мы наблюдаем эту конкретную реализацию. Но мы не знаем эту функцию, как функцию
[15:14.280 --> 15:20.200]  двух переменных, и мы не знаем эту реализацию, и поэтому мы не знаем будущее. И бывает так, что у нас
[15:20.200 --> 15:27.840]  может быть одно и то же прошлое и настоящее, но разное будущее для разных исходов. Мы не знаем,
[15:27.840 --> 15:32.240]  какой исход. Мы не можем по предыстории понять, что будет дальше, потому что предыстория одинакова
[15:32.240 --> 15:41.760]  для разных исходов. Видите, в чем проблема? Вот такая проблема. Но что мы можем делать? Мы
[15:41.760 --> 15:45.240]  можем попробовать вычислять вероятности какие-нибудь. Например, вероятность того,
[15:45.240 --> 15:51.680]  что процесс попадет в какой-нибудь интервал. Мы здесь опираемся на случайные величины,
[15:51.680 --> 15:58.240]  здесь мы рассматриваем вот такую случайную величину. Это случайная величина, у нее есть какая-то
[15:58.240 --> 16:03.000]  функция распределения. А если есть функция распределения, значит, потенциально теоретически
[16:03.000 --> 16:06.760]  мы можем вычислить вероятность попадания в какой-нибудь интервал. Можем посчитать среднее
[16:06.760 --> 16:13.080]  математическое ожидание и так далее. Но для того, чтобы это делать, нам нужно обобщить понятие
[16:13.080 --> 16:17.920]  функции распределения на процессы. Что такое функция распределения для случайной величины,
[16:17.920 --> 16:24.640]  мы знаем. А что такое функция распределения у случайного процесса? Это самая главная
[16:24.640 --> 16:30.960]  характеристика случайного процесса. Давайте мы это посмотрим, как это можно определить. Смотрите,
[16:30.960 --> 16:37.720]  что мы сделаем, чтобы это определить. Мы рассмотрим с вами несколько сечений процесса, то есть
[16:37.720 --> 16:48.440]  случайные величины в разные моменты времени. Например, кси от омега t1, кси от омега t2 и так далее.
[16:48.440 --> 16:56.560]  Конечное число их рассмотрим. Омега tn. t у них могут, в принципе, у кого-то даже совпадать, но давайте
[16:56.560 --> 17:02.200]  для простоты считать, что они все разные. Омеги везде одинаковые. Вот это все сечения, взятые в
[17:02.200 --> 17:06.960]  разный момент времени. Это все случайные величины. И так как по определению они заданы на одном
[17:06.960 --> 17:13.440]  вероятностном пространстве, это значит, что они образуют случайный вектор. Вектор случайных
[17:13.440 --> 17:17.880]  величин, заданных на одном вероятностном пространстве, это случайный вектор. А у случайного вектора есть
[17:17.880 --> 17:25.920]  функция распределения. Мы ее знаем. Я ее буду обозначать вот так. f кси x1 и так далее xn, точка
[17:25.920 --> 17:32.440]  запятая t1 и так далее tn, чтобы почернуть, в какие моменты были взяты эти сечения. Это есть
[17:32.440 --> 17:45.480]  вероятность на множестве исходов таких, что кси от омега t1 меньше x1 и так далее кси от омега tn
[17:45.480 --> 17:55.600]  меньше xn. Вот такую вещь рассмотрим. Они определены все на одном вероятностном пространстве. Каждый из
[17:55.600 --> 18:03.040]  них является случайной величиной. Множество омег, которые удовлетворяют этому условию, оно измеримо,
[18:03.040 --> 18:09.560]  потому что это случайная величина, измеримая функция исхода, и каждая из них это функция измеримая.
[18:09.560 --> 18:14.880]  Здесь стоит конечное пересечение множества сигма алгебра, поэтому их пересечение тоже является
[18:14.880 --> 18:21.080]  функцией сигма алгебра, поэтому эта вероятность определена. Ну хорошо, такую вещь мы вести можем.
[18:21.080 --> 18:26.440]  Вот она называется н-мерной функцией ей распределения случайного процесса.
[18:26.440 --> 18:44.560]  Н-мерная функция распределения случайного процесса кси. Здесь n больше либо равно 1, t1
[18:45.040 --> 19:03.680]  и тн произвольные из вот этого t большого. Вот множество таких вот функций f кси от x1 и т.д., xn, t1 и т.д., tn при n
[19:03.680 --> 19:11.200]  больше либо равных единице и t1, tn из t, если мы проварьируем, по n и по t. Вот множество
[19:11.200 --> 19:16.600]  вот этих вот функций называется семейством конечномерных распределений,
[19:16.600 --> 19:34.080]  семейством конечномерных распределений случайного процесса. Вот это будет нашей
[19:34.080 --> 19:44.160]  основной величиной, которая будет, с помощью которой можно вычислять вероятности различных событий.
[19:44.160 --> 19:54.920]  Ну, например, просто пример, переду, как мы можем этим пользоваться. Допустим, нам нужно в какой-то
[19:54.920 --> 19:59.520]  будущий момент времени посчитать вероятность попадания в некоторый интервал. Тогда для этого нам
[19:59.520 --> 20:07.720]  нужна одномерная функция распределения f кси. Допустим, мы ее знаем, f xt, которая равна вероятности
[20:07.720 --> 20:16.720]  по всем исходам таким, что и кси от ωt меньше xа. Тогда как нам у момента t будущее, допустим,
[20:16.720 --> 20:23.680]  посчитать вероятность того, что мы попадаем в некоторый интервал, скажем, ab интервал. Вероятность
[20:23.680 --> 20:35.040]  того, что кси от ωt, будущее, попадает в какой-то интервал. Вот это есть просто разница. Функция
[20:35.040 --> 20:44.800]  распределения кси в точке b в момент t, будущее, минус функция распределения кси а в момент t,
[20:44.800 --> 20:50.400]  будущее. Ведь если мы функцию f xt, как функцию двух переменных, знаем, то просто подставляем туда
[20:50.400 --> 20:55.680]  наши b, a и t, будущее, вычисляем, и то число, которое мы получим, это есть вероятность того,
[20:55.680 --> 21:05.880]  что в этот момент мы попадем в этот интервал. Но можно, конечно, и какие-то более сложные
[21:05.880 --> 21:12.800]  вероятности считать со использованием f. Это я просто привел пример. Теперь движемся дальше,
[21:12.800 --> 21:18.640]  немножко более глубоко посмотрим, какими же свойствами обладает эта f и действительно ли
[21:18.640 --> 21:24.720]  ее достаточно для описания вероятностных свойств процесса. Потому что, видите, я тут ввел
[21:24.720 --> 21:30.400]  почему-то конечное число t, почему конечное, почему он конечный. Оно сколько угодно большое может
[21:30.400 --> 21:39.200]  быть, но оно конечное. Почему я не бесконечное взял? Давайте рассмотрим поглубже свойства вот
[21:39.200 --> 21:58.200]  этой функции f семейства, конечно, мерных распределений. Какими свойствами обладает,
[21:58.200 --> 22:11.120]  ну, значит, свойство мерных функций распределения случайных процессов. Первое свойство очевидное,
[22:11.120 --> 22:18.560]  так как f, что бы это ни было, f это вероятность. Вероятность у нас лежит в интервале от нуля до
[22:18.560 --> 22:24.640]  единицы. Значит, никуда не деться, эта функция тоже обязана лежать в интервале от нуля до единицы.
[22:24.640 --> 22:45.720]  Для любых n, x и t. Второе, значит, эти функции, я не буду это переписывать, вот это все,
[22:45.720 --> 23:05.960]  эти функции непрерывны слева по каждой переменной x. То есть, если мы зафиксируем все x кроме одного и
[23:05.960 --> 23:12.320]  посмотрим на эту функцию как функции этого x, то эта функция будет непрерывна слева. Ну, вы можете
[23:12.320 --> 23:15.960]  какую-нибудь другую книжку открыть и увидеть, что там написано непрерывно справа, а я говорю,
[23:15.960 --> 23:23.480]  непрерывна слева. Почему? А потому что вот почему. У нас в курсах во всех вероятность и функция
[23:23.480 --> 23:29.440]  распределения, вернее, она определяется через строгое неравенство. Видите, меньше. Если здесь
[23:29.440 --> 23:35.400]  меньше функция распределения, то тогда будет непрерывность слева. Если здесь меньше либо равно,
[23:35.400 --> 23:42.080]  то тогда будет функция непрерывная справа. Ну, такая тонкость есть. Ну, не то чтобы это было
[23:42.080 --> 23:49.280]  где-то принципиально важным, но как бы конкретно здесь вы можете не понять, почему где-то написано
[23:49.280 --> 23:53.160]  справа, хотя у меня было слева. Вот все дело в том, как определить функцию распределения,
[23:53.160 --> 24:00.040]  строгий там знак поставить или нет. Если строгий, то слева. Так, ну, это следствие из теории вероятности,
[24:00.040 --> 24:08.120]  я не буду на этом останавливаться. Вот, следует из свойств непрерывности вероятности. Так,
[24:08.120 --> 24:19.160]  еще одно свойство. Если хотя бы одна временная эксцита стремится к минус бесконечности, а остальные
[24:19.160 --> 24:30.440]  постоянно, ну или никуда не стремятся, а остальные постоянно, то тогда что получается? Если эксцита
[24:30.440 --> 24:36.360]  стремится к минус бесконечности, например, x1, тогда вот это событие становится все более невозможным,
[24:36.360 --> 24:40.600]  оно пересекается с другими, неважно с какими, туда эта вероятность будет стремиться просто к нулю,
[24:40.600 --> 24:53.320]  по свойству непрерывности вероятности. Это значит, что f к c, x1 и так далее, xn, t1, tn стремится к нулю. Вот,
[24:53.320 --> 25:01.160]  при эксцитам стремящимся к минус бесконечности. Вот, а если все, здесь все, а здесь хотя бы одна
[25:01.160 --> 25:11.880]  переменная, если все эксциты стремятся к плюс бесконечности, тогда, если все эксы стремятся к плюс
[25:11.880 --> 25:17.080]  бесконечности, все эти события стремятся быть более достоверными. Вот, так что эта вероятность будет
[25:17.080 --> 25:25.320]  стремиться к единице. Вот, то тогда, тут, наверное, внизу не видно, я вот тут продолжил, f к c от x1, ну и
[25:25.360 --> 25:31.800]  так далее. Все это стремится к единице, когда все они стремятся к плюс бесконечности, все эксы
[25:31.800 --> 25:38.400]  стремятся к плюс бесконечности. А здесь, когда один стремится к минус бесконечности, тогда это стремится
[25:38.400 --> 25:47.800]  к нулю. Вот. Тут такие свойства, которые лежат на поверхности. Еще одно свойство, такое как бы не
[25:47.800 --> 25:56.440]  не очень тривиальная, не очень очевидная. То, что эта функция, она монотонна в некотором смысле.
[25:56.440 --> 26:13.400]  В следующем смысле. Смотрите, ну давайте я пока так напишу и поясню. Значит, дельта 1 это
[26:13.400 --> 26:20.440]  оператор троеточие дельта n, n операторов, которые действуют на f к си, оператор конечной разности,
[26:20.440 --> 26:30.540]  x1 и так далее, xn, t1 и так далее, tn. Вот эта вещь больше либо равна нуля. Где дельта и это
[26:30.540 --> 26:40.080]  линейный оператор, который, ну да, звучит очень заумно на самом деле, но за этим тривианщина просто
[26:40.080 --> 26:45.840]  скрывается. Смотрите, линейный оператор, который действует так. Если дельта ита действует на f к си,
[26:45.840 --> 26:56.320]  я не буду здесь все переписывать, то это получается функция, которая вычислена до х и минус 1, и к х и
[26:56.320 --> 27:03.480]  прибавляется h иты больше либо равен ноль. Дальше идет и плюс один и так далее до конца. То есть мы
[27:03.480 --> 27:16.480]  иты x повысили на h итую и вычли, как если бы ничего не делали. Вот. Просто подействовали на итую
[27:16.480 --> 27:23.280]  компоненту, повысили ее на h итую. Вот. И то, что здесь стоит, это означает, что мы подействовали на x
[27:23.280 --> 27:33.440]  с n таким образом. Получили разность. Потом дельта n-1 подействовала на эту разность. То есть получилось,
[27:33.440 --> 27:41.920]  что в этой разности мы должны уже x n-1 увеличить и отнять, когда он не увеличен. И мы получим уже
[27:41.920 --> 27:48.440]  четыре слагаемых. А потом дельта n-2 подействует на эти четыре слагаемых. Мы должны снова вот такую
[27:48.440 --> 27:54.040]  разность посчитать. Понятно, что это такое? Вот. Сложная такая вещь. Так вот, это всегда больше либо
[27:54.040 --> 28:01.000]  равно нуля. Чуть позже я поясню геометрический смысл этой вещи, а пока просто зафиксировали
[28:01.000 --> 28:13.520]  для себя, что есть такое свойство монотонности. Вот. Так. Ну, еще пара тривиальных свойств.
[28:13.520 --> 28:39.440]  Еще пара свойств. Там, по-моему, про перестановку. Да. Пятое. Смотрите, какое еще свойство есть.
[28:39.440 --> 28:45.080]  Здесь стоят пересечения событий. Но если мы их переставим местами, ничего не изменится. Если в
[28:45.080 --> 28:49.360]  пересечении переставлять множество местами, пересечение не меняется. Так что, как бы ты не
[28:49.360 --> 29:04.720]  переставлял, функция от этого не поменяется. Поэтому для любой перестановки элементов множества 1n,
[29:04.720 --> 29:13.360]  для любой перестановки, как это обозначается, k1, kn этого множества, мы получаем, что fкс,
[29:13.360 --> 29:23.200]  xk1 и так далее, xkn, когда мы x переставим местами и, соответственно, мы переставим t местами точно
[29:23.200 --> 29:37.960]  таким же образом, то тогда это будет то же самое, как до перестановки. Вот. x и t мы не меняем. Мы
[29:37.960 --> 29:43.080]  просто их переставляем местами. Как бы вы их не переставляли, мы получим одну и ту же функцию.
[29:43.080 --> 29:54.840]  Вот такое свойство, тоже кривиальное. И еще одно свойство, если мы возьмем вот эту функцию и,
[29:54.840 --> 30:02.480]  начиная с некоторого, скажем, k плюс 1 до конца, все x возьмем равными плюс бесконечности, то тогда то,
[30:02.480 --> 30:12.760]  что мы получим, будет являться функцией распределения, но уже k-мерной. Вот как. То есть fкс,
[30:12.760 --> 30:21.560]  x1 и так далее, xk плюс бесконечность и так далее, плюс бесконечность, t1 и так далее, tn. Здесь
[30:21.560 --> 30:29.280]  их n было. Но вот это, это k плюс 1 и так далее до n. Здесь мы заменили на плюс бесконечность. То это
[30:29.280 --> 30:45.960]  есть fкс от x1 и так далее, xk, t1 и так далее, tk. Вот. Вот такие шесть свойств. Это свойство
[30:45.960 --> 30:52.200]  согласования. То есть вот эти функции, смысл в чем? Вот эти функции, n-мерные функции распределения,
[30:52.200 --> 31:00.240]  они не могут быть всякими. Они обязаны удовлетворять вот этим свойствам. От нуля до единицы,
[31:00.240 --> 31:08.640]  непрерывность слева, стремление к нулю к единице, когда x и там, соответственно, куда-то стремятся,
[31:08.640 --> 31:16.800]  монотонность, перестановки и вот такая согласованность между этими f. Вот это свойство
[31:16.800 --> 31:23.080]  семейства конечномерных распределений. Мы с вами уже получали что-то подобное в теории вероятности,
[31:23.080 --> 31:27.640]  когда изучали свойства функции распределения. Помните, она тоже была сейчас от нуля до единицы,
[31:27.640 --> 31:36.240]  тоже была непрерывна слева, была монотонна. Так. Тоже там стремилась к нулю при х, стремящемся к
[31:36.240 --> 31:43.440]  минус бесконечности и к единице при х, стремящемся к плюс бесконечности. И так получалось, что вот
[31:43.480 --> 31:49.520]  функция распределения обладает этими свойствами. Если вы возьмете просто какую-то функцию,
[31:49.520 --> 31:58.360]  которая обладает этими свойствами, то тогда можно доказать, что она отвечает некоторой случайной
[31:58.360 --> 32:02.240]  величине, что она на самом деле является функцией распределения некоторой случайной величины.
[32:02.240 --> 32:10.480]  И вот теперь смотрите, здесь то же самое. Семейство конечномерных распределений обладает всеми этими
[32:10.480 --> 32:18.240]  свойствами. Теперь наоборот, если вы возьмете семейство каких-то функций, каких-то, которые
[32:18.240 --> 32:24.680]  обладают всеми этими свойствами, то на самом деле это семейство функций является семейством
[32:24.680 --> 32:30.640]  конечномерных распределений для некоторого процесса. Вот как. И то, что я назвал, это теорема
[32:30.640 --> 32:42.840]  Колмогорова. Сейчас я ее запишу, у вас вопрос был. Да, да, именно ТК. Ну потому что плюс бесконечность,
[32:42.840 --> 32:52.560]  меньше Х, там неважно, там пропадает уже это Т. Так что все. Вот то, что я сейчас сказал, это есть
[32:52.560 --> 33:03.000]  теорема Колмогорова. Это первая наша с вами теорема в этом курсе. Она пойдет без доказательства.
[33:03.000 --> 33:19.760]  Теорема Колмогорова. Звездочка я обозначу, потому что без доказательства. Сейчас я ее точно
[33:19.760 --> 33:37.600]  сформулирую. Значит, пусть задано семейство функций, семейство функции Ф. Я к себе не пишу, потому что это
[33:37.600 --> 33:48.000]  просто какое-то семейство функций вот стольких переменных при n больше либо равных единице t и tn,
[33:48.000 --> 34:14.920]  удовлетворяющие условиям 1.6. Тогда существует вероятностное пространство Омега ФП и случайный
[34:14.920 --> 34:31.600]  процесс, и случайный процесс, вот. Определенный на этом пространстве.
[34:31.600 --> 34:50.400]  Ну, семейство конечномерных распределений,
[34:50.400 --> 35:04.760]  которого совпадает с F. А, подождите, тут у меня сигма-алгебра F, и там F не очень это удачно. Давайте мы
[35:04.760 --> 35:13.320]  сигма-алгебру будем такой функции каллиграфической F писать, а здесь я лучше как-нибудь попрямее,
[35:13.320 --> 35:23.120]  попрямее сделаю. Это семейство функций, а там сигма-алгебра. Вот так. Еще у меня t большой где-то отсутствует,
[35:23.120 --> 35:31.760]  вот здесь вот. Вот так. Пусть задано семейство функций со свойствами 1.6. Тогда найдется вероятностное
[35:31.760 --> 35:38.640]  пространство и процесс на нем, у которого семейство конечномерных распределений совпадает с F. Важное
[35:38.640 --> 35:45.320]  здесь дополнение, такое пространство и процесс могут быть не единственные. Вот. Не обязательно одно
[35:45.320 --> 35:54.200]  найдется такое, но обязательно найдется. Вот так вот. Обязательно найдется. И теперь смотрите,
[35:54.200 --> 36:03.840]  зачем вообще эта теория нужна. Это очень важно. Я ввел определение случайного процесса как функции
[36:03.840 --> 36:11.840]  исхода и времени двух независимых переменных. Исхода и времени. Это один из способов задания процессов.
[36:11.840 --> 36:21.200]  Точно так же как задание функции исхода, это один из способов задания случайной величины. Но гораздо
[36:21.200 --> 36:28.120]  удобнее случайные величины задавать не какими-то функциями исходов, а распределениями. Мы говорим там,
[36:28.120 --> 36:32.320]  дали нам случайную величину с нормальным распределением. Ну какая разница какая там,
[36:32.320 --> 36:38.280]  кси, атомеха? Она существует. Какая-то она существует. Какую-то можно подобрать. Ну какая разница?
[36:38.280 --> 36:44.080]  Тебе нужны от случайной величины вероятностные численные характеристики. Тебе функция распределения
[36:44.080 --> 36:52.880]  важна. Вот так и здесь. Мы не будем задавать случайные процессы как функции исхода и времени, хотя это
[36:52.880 --> 36:59.560]  их определение. На практике это не нужно. На практике гораздо важнее вероятностные свойства. И вот эта
[36:59.560 --> 37:06.960]  теорема говорит о том, что процесс можно задать через задание семейства конечномерных распределений.
[37:06.960 --> 37:12.600]  Для случайной величины мы можем задать функцию распределения. И все, мы работаем с ней как случайной
[37:12.600 --> 37:18.560]  величиной. А здесь одной функции не обойтись, но вот семейству нужны функции. Семейство конечномерных
[37:18.560 --> 37:24.440]  распределений. И это будет наш как бы основной подход. Мы процессы будем определять по семействам
[37:24.440 --> 37:29.920]  конечномерных распределений. Потому что согласно этой теореме нам главное, чтобы 6 свойств выполняли,
[37:29.920 --> 37:36.440]  чтобы было согласовано. Семейство было согласованным. Если это так, то вот это абстрактное пространство,
[37:36.440 --> 37:44.760]  вот эта функция, они найдутся. Все. Так что вот в чем польза вот этой теоремы. Мы можем задавать
[37:44.760 --> 37:54.440]  процессы, задавая их функцией распределения н-мерные, а не функцию времени и исходов, что неудобно.
[37:54.440 --> 38:02.640]  Еще одна польза этой теоремы, которую мы правда сейчас не прочувствовали, потому что ее не доказывал,
[38:02.640 --> 38:08.920]  то, что на самом деле доказательство этой теоремы конструктивно. То есть реально не просто
[38:08.920 --> 38:13.520]  знать, что теорема существования существует, а пример ты привести не можешь. Нет. Если посмотреть
[38:13.520 --> 38:18.920]  доказательства, то можно увидеть, как именно строится это пространство и как строится процесс как
[38:18.920 --> 38:25.560]  функция исхода и времени. Так что вот польза этой теоремы, она еще и такая. Если тебе все-таки
[38:25.560 --> 38:29.960]  хочется построить этот процесс, открываешь доказательства этой теоремы и смотришь, как
[38:29.960 --> 38:36.080]  он устроится в самом общем случае. Там прямо алгоритм есть. Так что очень полезная теорема,
[38:36.080 --> 38:40.160]  но она очень большая и очень долго тяжело доказывать, поэтому я это делать не буду,
[38:40.160 --> 38:54.880]  потому что наш курс, он не про это. Так, хорошо, об этом я сказал. Следующая вещь,
[38:54.880 --> 39:03.560]  которую здесь надо пояснить. Здесь написано, что семейству распределений соответствует обязательно
[39:03.560 --> 39:10.040]  какой-то процесс. По этому семейству можно построить процесс. Возможно не единственный.
[39:10.040 --> 39:17.720]  Вот за счет того, что у вас могут быть две случайные величины с одним и тем же распределением,
[39:17.720 --> 39:24.320]  у вас могут быть точно также и два случайных процесса с одним и тем же распределением,
[39:24.320 --> 39:31.200]  с одним и тем же семейством конечномерных распределений. То есть как функции исхода и
[39:31.200 --> 39:38.120]  времени, они разные. Функции амигиты, они разные. Вот распределение у них одинаковое.
[39:38.120 --> 39:49.040]  И вот для этого, сейчас я определение веду и на перерыв идем. Для этого вводят такое вот
[39:49.040 --> 39:52.800]  полезное определение. Оно нам пригодится потом на протяжении почти всего курса.
[39:52.880 --> 40:10.920]  Понятие об эквивалентных случайных процессах. Еще одно определение ведем.
[40:10.920 --> 40:26.400]  То есть случайные процессы кси-т и это, естественно, определенные на одном вероятностном пространстве.
[40:26.400 --> 40:44.720]  Определенные на одном вероятностном пространстве и удовлетворяющие свойства вот такому,
[40:44.720 --> 41:00.680]  что для любых ТСТ вероятность исходов, на которых кси от омега Т равняется это от омега Т равно единице,
[41:00.680 --> 41:20.120]  называются стахастически эквивалентными или просто эквивалентными. И в этом случае,
[41:20.120 --> 41:28.880]  если для кси это вот это выполнено, то кси называется модификацией еты, а это называется
[41:28.880 --> 41:49.280]  модификацией кси. Вот такая терминология. Модификация еты. А это модификация кси.
[41:49.280 --> 41:57.280]  Обратите внимание, что квантор для любого стоит вне вероятности, не внутри. Если мы это поместим
[41:57.280 --> 42:02.160]  внутрь, это будет другое. Такое тоже есть определение, тоже в некотором смысле эквивалентность,
[42:02.160 --> 42:07.960]  но она нам не пригодится. Для любого ТСТ стоит снаружи. Вот это важный момент. Обратите на это
[42:07.960 --> 42:17.480]  внимание, не внутри. Вот такое определение запомним, оно нам еще пригодится. Пойдем на перерыв.
[42:17.480 --> 42:33.320]  Ладно, ребята, давайте мы продолжим. Здесь вот логическая линия пока завершилась, которая вышла
[42:33.320 --> 42:42.360]  из этого семейства теоремы Калмогорова. Давайте я вернусь теперь вот к этой вещи. Что это такое
[42:42.360 --> 42:50.760]  значит геометрически. Вот если n равняется единице, я показал, что это такое. Тут эта вот разность.
[42:50.760 --> 43:10.280]  И если мы рассмотрим, например, дельта 1 f х1 t1, это будет f х1 плюс h1 t1 минус f х1 t1. А разность
[43:10.280 --> 43:17.880]  функции распределений, это вероятность того, что хи, давайте я омеги опущу, как мы для случайных
[43:17.880 --> 43:30.160]  величин опускаем, так и я и здесь опущу. Значит в t1 принадлежит, значит вот так по-моему, х1 х1
[43:30.160 --> 43:37.760]  плюс h1. Вот вероятность. Она больше либо равна нуля. Видите, то есть 1 дельта больше либо равна нуля.
[43:37.760 --> 43:47.480]  Теперь давайте рассмотрим двумерный случай дельта 1, дельта 2. Дельта это вероятность попадания
[43:47.480 --> 44:01.600]  процесса в интервал от х1 до х1 плюс h1. Теперь что такое дельта 1, дельта 2 f от х1 х2 t1 t2.
[44:01.600 --> 44:13.840]  Ну сначала подействуем дельтой 2. Дельта 1, подействуя, что получим f, значит х1 х2 плюс h2 t1 t2
[44:13.840 --> 44:32.000]  минус f х1 х2 t1 t2. Вот, и скобочку еще закрываем. Вот я подействовал дельтой 2. Видите, х1 на месте
[44:32.000 --> 44:40.800]  находится. Дельта 2 действует только на второй аргумент, только на х2. Вот, а теперь дельта 1
[44:40.800 --> 44:45.560]  действует по определению от линейного оператора. То есть дельта разности равняется разности дельт.
[44:45.560 --> 44:52.000]  Вот, можно так на это посмотреть. Ну либо можно целиком ко всей этой штуке, всю эту штуку
[44:52.000 --> 44:58.240]  увеличивать на х1. Давайте так, наверное, поступим. Сначала я увеличу х1, а потом вычту то, что у нас было.
[44:58.240 --> 45:19.200]  х1 плюс h1, х2 плюс h2, t1 t2, минус f х1 плюс h1, х2 t1 t2. И минус то, что было до этого. То есть когда
[45:19.200 --> 45:34.440]  мы х1 не трогаем. То есть надо просто вычесть вот эту скобку, которая идет выше. Минус f х1 х2 плюс h2 t1 t2. Ну и минус на минус даст плюс здесь.
[45:34.440 --> 45:46.400]  Видите, f х1 х2 t1 t2. Вот какая вещь. Ну и что это такое? Так, где тряпка-то?
[45:46.400 --> 46:05.920]  Давайте мы это обозначим. Вот у нас есть плоскость х1 х2. Вот этих двух переменных. И у нас здесь есть
[46:05.920 --> 46:20.080]  где-то точка х1 плюс h1, х1, ну давайте так, есть точка х2 и х2 плюс h2. Вот у нас четыре точки на плоскости.
[46:20.080 --> 46:33.440]  Что такое первое f? От х1 плюс h1 х2 плюс h2. Это вероятность того, что хи t1 меньше вот этого,
[46:33.440 --> 46:45.120]  а хи t2 меньше вот этого. Меньше этого, меньше этого. Вот на плоскости это есть вероятность попадания вот в этот угол.
[46:45.120 --> 46:54.080]  Вот в этот все, вот в этот угол. Вероятность попадания вот сюда. Так? Вот эта вещь.
[46:54.080 --> 47:07.600]  Вероятность попадания в этот угол. Здесь стоит минус. Вот такая вероятность. А это что такое?
[47:07.600 --> 47:18.520]  Значит у нас здесь х2 и х1 плюс h1. Минус вероятность попадания вот в этот угол.
[47:18.520 --> 47:31.720]  Видите? То есть та вероятность. Минус эта вероятность. Идем ниже. Х1, х2 плюс h2.
[47:31.720 --> 47:37.280]  И это уже вероятность попадания вот в этот угол. Она тоже вычитается.
[47:37.280 --> 47:50.160]  А потом идет плюс вероятность вот этого угла х1 х2. Потому что, видите, мы его два раза отняли.
[47:50.160 --> 47:56.400]  Мы его отняли, когда вы читали этот угол и отняли, когда вы читали этот угол. Так что вот этот
[47:56.400 --> 48:02.720]  меньший угол мы отняли два раза. А здесь мы его прибавили один раз. Так что в сумме получается так,
[48:02.720 --> 48:09.240]  что мы получаем вероятность попадания в этот прямоугольник. Вот. Вот каков геометрический
[48:09.240 --> 48:18.880]  смысл вот этой конструкции. Это есть вероятность попадания в тот прямоугольник. Вероятность того,
[48:18.880 --> 48:37.400]  что кси t1 принадлежит х1 х1 плюс h1, запятая, кси2 принадлежит х2 х2 плюс h2. Вот. А раз это вероятность,
[48:37.400 --> 48:44.960]  она не отрицательна. Поэтому и дельта 1 дельта 2 тоже не отрицательна. Вот. И в общем случае можно
[48:44.960 --> 48:52.480]  показать, что вот эта серия дельт, когда n произвольна, это есть вероятность попадания в какой-то
[48:52.480 --> 48:58.440]  там параллелепипед в инмерном пространстве. А раз это вероятность, то она не отрицательна. Вот
[48:58.440 --> 49:04.400]  смысл геометрический вот этой вот конструкции. Хотя, да, выглядит она сложно. Вот видите, даже для n
[49:04.400 --> 49:10.720]  равно 2 тут уже пришлось постараться, чтобы ее выписывать. А для n равно 3 там будет еще больше слагаемых.
[49:10.720 --> 49:34.240]  Вот. Понятно это? Ну, вроде так. А? А как это вероятность попадания отрицательна? Это как?
[49:40.720 --> 50:06.400]  Ну, что-то мне не очень понятно, что вы имеете в виду. То, что здесь какие-то скачки возможны,
[50:06.400 --> 50:12.680]  ну и пусть они не ухудшат. Если прямоугольник станет больше, вероятность не уменьшится. Как она
[50:12.680 --> 50:17.920]  может уменьшиться? Потому что вероятность попадания в маленький прямоугольник здесь,
[50:17.920 --> 50:22.560]  она тоже положительна. Если вы прямоугольник расширите, то вы не уменьшите вероятность.
[50:22.560 --> 50:43.360]  Потому что какая разница? Что-то мне не очень понятно. К минус бесконечности?
[50:43.360 --> 50:50.720]  Ну, получим угол большой.
[50:50.720 --> 51:11.720]  Ну, не может быть вероятность отрицательна.
[51:11.720 --> 51:23.120]  Ну, это невозможно. Ну, как вероятность отрицательна может быть даже на каких-то прямых?
[51:23.120 --> 51:27.520]  Это непонятно, что вы имеете в виду. Что значит на прямой вероятность отрицательна?
[51:27.520 --> 51:34.760]  Какого события и вероятность надо посмотреть? Ну, давайте мы потом с вами поговорим после этого.
[51:34.760 --> 51:43.320]  Я это все закончил, потом разберемся. Это я прокомментировал этот четвертый пункт. Дальше едем.
[51:43.320 --> 51:56.920]  Дальше едем. Что еще можно сказать? По поводу астахастических эквивалентностей. А, нет, это мы
[51:56.920 --> 52:04.120]  закрыли. Теперь давайте поговорим о том, каким может быть Т, каким может быть время и о проблемах,
[52:04.120 --> 52:13.280]  которые там возникают. Что мы сейчас с вами поняли? Итак, мы предложили определение случайного
[52:13.280 --> 52:21.000]  процесса. Окей. Поняли, что слишком сильно оно нам не помогает с точки зрения предсказания, но мы
[52:21.000 --> 52:26.080]  можем попробовать вычислять вероятности. Для этого мы ввели понятие семейства конечномерных
[52:26.080 --> 52:31.000]  распределений, как и функция распределения при случайной величины. Семейство конечномерных
[52:31.000 --> 52:35.680]  распределений — это такая важная характеристика, которая все говорит о процессе. Мы посмотрели
[52:35.680 --> 52:40.560]  свойства этого семейства распределений и установили, что эти свойства необходимы и достаточно для того,
[52:40.560 --> 52:45.800]  чтобы быть семейством конечномерных распределений какого-то случайного процесса. Это теорема
[52:45.800 --> 52:53.800]  Колмогорова. Выяснили, что в принципе могут быть два разных процесса, как разные функции — омеге и Т,
[52:53.800 --> 53:00.240]  но у которых семейство конечномерных распределений совпадает. Я тут единственный только забыл
[53:00.480 --> 53:08.840]  сказать, что если два процесса стахастически эквалентны в этом смысле, то тогда у них семейство
[53:08.840 --> 53:13.760]  конечномерных распределений совпадает. Я не буду это доказывать, но это очень легко доказать,
[53:13.760 --> 53:19.520]  как бы я это вам оставляю в качестве упражнения. Но вы, если пока не знаете, как это делается,
[53:19.520 --> 53:23.720]  ничего страшного, просто пока запомните, что если они стахастически эквалентны, значит у них
[53:23.720 --> 53:34.000]  совпадают распределения, семейство конечномерных распределений. Теперь, когда мы ввели вот эту Т в новое
[53:34.000 --> 53:42.480]  время, то мы на самом деле получили не только от этого пользу, но и некоторые проблемы. Какие могут
[53:42.480 --> 53:48.520]  быть Т? Вот Т-большое я впереди вводил. Т-маленькое принадлежит Т-большому. А что это за множество Т такое?
[53:48.520 --> 53:57.160]  Каким оно может быть? Если Т одноэлементно, то есть в момент времени всего один, то по сути никакого
[53:57.160 --> 54:01.720]  семейства нет. В твоем семействе только одна случайная величина. Значит, твой процесс – это
[54:01.720 --> 54:09.960]  случайная величина. То есть если один элемент, следовательно, случайный процесс – это случайная
[54:09.960 --> 54:20.360]  величина, по сути. Если там N элементов, конечно их число, то по сути ты имеешь дело со случайным вектором.
[54:20.360 --> 54:27.120]  Тебя несколько случайных величин, заданных на одной вероятности пространства, не образуют вектор.
[54:27.120 --> 54:32.280]  Поэтому твой процесс, по сути, это вектор. Но вот эти две ситуации – это то, что вы изучали в теории
[54:32.280 --> 54:41.520]  вероятностей. А нас интересует то, что идет дальше. Если там, если T равняется N множеству натуральных
[54:41.520 --> 54:49.240]  чисел или множеству целых чисел, то тогда это уже бесконечная множество случайных величин. Такие
[54:49.240 --> 54:56.520]  случайные процессы называются случайными последовательностями. В общем-то, вам эта вещь
[54:56.520 --> 55:00.840]  тоже известна. Вы изучали по теории вероятностей там сходимость в разных смыслах, случайных
[55:00.840 --> 55:07.120]  последовательностей. Так что вот на самом деле вы уже работали со случайными процессами. И еще один
[55:07.120 --> 55:14.200]  случай, который уже новый для вас. T может быть равно R, может быть R+, я имею в виду от нуля до плюс
[55:14.200 --> 55:19.240]  бесконечности, включая ноль. Такое полезное обозначение у нас будет встречаться. Это может быть какой-то
[55:19.240 --> 55:27.320]  интервал от A до B. Такие случайные процессы называются случайными функциями.
[55:27.320 --> 55:37.400]  Потому что время здесь непрерывно. Вот здесь время дискретно. Во всех первых трех случаях время
[55:37.400 --> 55:45.040]  дискретно. Здесь так оно вообще конечно. Здесь время непрерывно. И вот в этом случае возникают
[55:45.040 --> 55:53.640]  определенного рода проблемы. Сейчас я вам скажу, какие. Смотрите, мы в дальнейшем захотим вычислять
[55:53.640 --> 56:01.520]  вероятности различных событий, которые связаны с процессом. Одно дело, вопрос типа посчитать
[56:01.520 --> 56:08.400]  вероятность того, что процесс в какой-то момент времени принадлежит отрезку A-B. Это одно. А вот
[56:08.400 --> 56:16.160]  как насчет таких событий? Типа найти вероятность того, что процесс возрастает на некотором интервале.
[56:16.160 --> 56:23.120]  Или даже еще проще. Вероятность того, что на всем интервале процесс строго больше нуля.
[56:23.120 --> 56:34.880]  То есть это событие, оно касается этого процесса во всех точках для любых T. Если у тебя T непрерывно,
[56:34.880 --> 56:40.720]  то получается, что это событие, которое связано с процессом, внесет на множество точек.
[56:40.720 --> 56:45.920]  Вот определены ли такие вероятности? Вот давайте мы об этом поговорим.
[56:53.120 --> 57:12.880]  Рассмотрим такую проблему. Пусть дан некий процесс, и нам нужно найти вероятность вот такую.
[57:12.880 --> 57:22.000]  Пусть им Supremum по T, принадлежащим от нуля до единицы, некоторого процесса x и от T, и мы
[57:22.000 --> 57:30.400]  учитываем вот такое событие. Меньше x. Ну как бы вероятность того, что Supremum процесса на отрезке
[57:30.400 --> 57:37.920]  не превышает какое-то значение x. То есть процесс на отрезке не превзошел x на этом интервале.
[57:37.920 --> 57:41.960]  Нормальное такое событие, правильно? Может такое быть? Может такое быть? Почему бы и нет?
[57:41.960 --> 57:55.280]  Вопрос, определена ли это вероятность? Во-первых, надо понять, а в чем здесь, собственно, проблема.
[57:55.280 --> 58:05.680]  Смотрите, для каждого фиксированного T, x и от T это случайная величина. Вот такое событие,
[58:05.680 --> 58:14.280]  оно обладает вероятностью, оно измеримо. Множество амих таких, что вот это, оно измеримо,
[58:14.280 --> 58:19.600]  над ним определенная вероятность. Это элемент sigma-алгебра f, на котором определены случайные
[58:19.600 --> 58:28.840]  величины x. Правильно? Просто по определению случайных величин. Вот такое событие, скажем,
[58:29.240 --> 58:41.480]  Assassins' не можно здесь, вот по каким-то причинам, пересечь sting historicum,
[58:41.480 --> 58:49.120]  тау и тау, возьму там, SAM. versioning.com, вот это тоже элемент sigma-алгебры. Тоже множество таких f
[58:49.120 --> 58:55.120]  подходят, потому что это элемент sigma-алгебры, это элемент sigma-алгебры, ну, и конечное пересечение
[58:55.120 --> 59:01.120]  Вероятность всякого события, которое состоит из конечных пересечений вот таких штук.
[59:01.120 --> 59:04.120]  А теперь посмотрим сюда, что вот это такое.
[59:04.120 --> 59:07.120]  Ведь для того, чтобы супремум был меньше х,
[59:07.120 --> 59:12.120]  необходимо и достаточно, чтобы в каждой точке t он был меньше х.
[59:12.120 --> 59:23.120]  То есть пересечение для всех t от нуля до единицы множество кси от t меньше х.
[59:23.120 --> 59:26.120]  Но здесь не счетное число пересечений.
[59:26.120 --> 59:31.120]  А если мы не счетное число раз пересечем элементы сигма-алгебры,
[59:31.120 --> 59:34.120]  мы не обязательно получим элемент сигма-алгебры.
[59:34.120 --> 59:36.120]  Поэтому ответ такой.
[59:36.120 --> 59:40.120]  В общем случае, когда мы ничего не знаем априори про процесс,
[59:40.120 --> 59:46.120]  про его реализации траектории, мы не можем сказать, определена эта вероятность или нет.
[59:46.120 --> 59:48.120]  Она может быть определена.
[59:48.120 --> 59:52.120]  Вдруг так совпало для этого процесса всё хитро,
[59:52.120 --> 59:57.120]  что на самом деле вероятность определена на такой штуке.
[59:57.120 --> 01:00:00.120]  Она может быть, в принципе, определена.
[01:00:00.120 --> 01:00:03.120]  Мы не можем поставить следствие, что она определена,
[01:00:03.120 --> 01:00:05.120]  потому что эти определены.
[01:00:05.120 --> 01:00:08.120]  Но в принципе, может оказаться, что это событие измеримое,
[01:00:08.120 --> 01:00:10.120]  вероятность о нём определена.
[01:00:10.120 --> 01:00:12.120]  А в каких-то случаях может так не быть.
[01:00:12.120 --> 01:00:14.120]  В каких-то случаях нет.
[01:00:14.120 --> 01:00:19.720]  на самом деле, потому что ты там такой весь из себя, значит, строишь свои модели, потом там
[01:00:19.720 --> 01:00:24.560]  пытаешься вычитать такие вероятности, а потом такой раз, и ты задумываешься, а что ты вообще
[01:00:24.560 --> 01:00:31.160]  считаешь? То, что ты считаешь, оно вообще хотя бы существует, потому что так-то вообще нехилая
[01:00:31.160 --> 01:00:34.960]  задачка, вот такие вот вероятности считать, и может быть, ты тратишь много времени, а на самом
[01:00:34.960 --> 01:00:39.040]  деле эта вероятность просто не существует, у тебя просто ее в принципе не получится вычислить.
[01:00:39.040 --> 01:00:45.720]  Когда мы имели дело с случайными величинами, векторами, последовательностями, там не было
[01:00:45.720 --> 01:00:52.080]  таких проблем, там все счетное, не более чем счетное, и как ни пересекаю, у тебя все существует и прекрасно,
[01:00:52.080 --> 01:00:58.800]  все эти вероятности, какие себе можно помыслить для практики, они все существуют, а вот здесь вот
[01:00:58.800 --> 01:01:05.120]  такая вот тонкость есть, что вот несчетное пересечение, поэтому вероятность может не существовать.
[01:01:05.120 --> 01:01:12.680]  Ну а смотрите, в каких случаях мы можем точно сказать, что эта вещь будет существовать, ну хотя
[01:01:12.680 --> 01:01:21.520]  бы вот это вот, этот supremum. Например, если мы уверены, что мы имеем дело с процессом, у которого
[01:01:21.520 --> 01:01:31.880]  все траектории непрерывны, если все траектории процесса непрерывны, то вот это событие равносильно,
[01:01:31.880 --> 01:01:39.000]  ну во-первых, вот этому, а во-вторых, пересечению по всем рациональным точкам отрезка 0,1, а их уже счетно,
[01:01:39.000 --> 01:01:51.720]  то есть равно для случая, когда все траектории непрерывны, пересечению уже Т принадлежит из 0,1
[01:01:51.720 --> 01:02:00.440]  пересечь с Q, а это счетное множество, значит оно измеримое, значит вероятность определена,
[01:02:00.760 --> 01:02:06.000]  но другой вопрос, как посчитать, пока его не берем, пока только принципиальные вещи рассматриваем,
[01:02:06.000 --> 01:02:12.600]  потому что если мы уверены, что все траектории непрерывны, тогда да, тогда абсолютно точно говорим,
[01:02:12.600 --> 01:02:17.600]  все можно считать без проблем, по крайней мере вот такую вероятность, а если мы этого не знаем,
[01:02:17.600 --> 01:02:22.280]  ну тогда хрен его знает, надо-то осмотреть, более глубоко изучать процесс, с чем мы имеем дело,
[01:02:22.280 --> 01:02:30.200]  и это уже зависит от задачи и того, с чем мы имеем дело. Вот, теперь, одна из удивительных вещей
[01:02:30.200 --> 01:02:39.760]  случайных процессов стоит в том, что один и тот же процесс можно определить так, что у него множество
[01:02:39.760 --> 01:02:48.200]  траекторий произвольно и множество траекторий непрерывно. Представляете, процесс вы определили
[01:02:48.680 --> 01:02:55.160]  одним образом, так что множество его траекторий всякое, но может так оказаться, что почти все его
[01:02:55.160 --> 01:03:00.160]  траектории непрерывны, или что-то можно сделать, какое-то другое вероятностное пространство задать
[01:03:00.160 --> 01:03:07.400]  так, чтобы на новом вероятностном пространстве этот процесс имел непрерывные траектории, а семейство
[01:03:07.400 --> 01:03:13.280]  конечномерных распределений было такоже, как у исходного. То есть, понимаете, вот на практике нас,
[01:03:13.280 --> 01:03:18.160]  опять же повторяю, не волнует, на каких абстрактных пространствах вероятностных задан случайный
[01:03:18.160 --> 01:03:27.520]  процесс Омега-ФП. Нас интересует только семейство конечномерных распределений, и мы вольны делать
[01:03:27.520 --> 01:03:32.680]  процессом всё, что угодно, лишь бы его семейство конечномерных распределений оставалось одним
[01:03:32.680 --> 01:03:39.920]  и тем же, потому что именно его мы используем для вычисления каких-то характеристик. Так вот, может
[01:03:39.920 --> 01:03:47.320]  оказаться так, что для вашего процесса существует непрерывная модификация. Модификация в том смысле,
[01:03:47.320 --> 01:03:53.800]  в котором я написал. Вот, может существовать непрерывная модификация, то есть на том же
[01:03:53.800 --> 01:03:59.720]  самом вероятностном пространстве задать другой процесс, у которого будут другие, у которого будут
[01:03:59.720 --> 01:04:09.920]  те же самые характери... свойства вероятностные. И есть теорема, тоже Колмогорова, о том, как выяснить,
[01:04:09.920 --> 01:04:15.600]  какие процессы, у каких процессов существует непрерывная модификация. Значит, смотрите,
[01:04:15.600 --> 01:04:28.240]  ещё одна теорема на сегодня, последняя теорема, тоже Колмогорова, Колмогорова. Значит,
[01:04:28.240 --> 01:04:38.320]  пусть кси от Т из Т большого, это случайный процесс, и пусть Т большое, это есть отрезок
[01:04:38.320 --> 01:04:49.240]  от А до Б, конечный. Вот, конечный отрезок. Тогда, если существуют числа А больше нуля,
[01:04:49.240 --> 01:04:55.720]  Б больше нуля, С, ну С меньше бесконечности, ну число и С меньше бесконечности, неважно,
[01:04:55.720 --> 01:05:11.680]  такие, что для любых Т и Т плюс H, H больше либо равно нуля, вероятность... Сейчас, дайте,
[01:05:11.680 --> 01:05:27.560]  я напишу, а потом вы напишите. Я сейчас не сразу вспомню. По-моему, по-моему, я не наврал.
[01:05:27.560 --> 01:05:34.680]  А, подождите, А, Б плохо, у меня А и Б вот здесь. Да, видите, давайте я вот тут А и обозначу
[01:05:34.680 --> 01:05:49.400]  Бетой. А, значит, там пусть будет С, здесь А, здесь Б. По-моему, по-моему, так. Да. А, ну, кстати говоря, тут
[01:05:49.400 --> 01:05:58.920]  даже может быть отрицать, раз модули стоят. Это тоже давайте сотрем, неважно. Вот, то кси от Т имеет
[01:05:58.920 --> 01:06:25.280]  непрерывную модификацию, то есть модификацию с почти всеми непрерывными траекториями. Вот. И уже как
[01:06:25.280 --> 01:06:30.280]  следствие этой теоремы, можно создать другое вероятностное пространство и определить этот процесс,
[01:06:30.280 --> 01:06:37.680]  переопределить процесс кси на нем так, чтобы у него уже все траектории были непрерывны. И тогда на практике
[01:06:37.680 --> 01:06:44.160]  вы работаете уже, по сути, не с исходного пространства на плохом, на котором эта вероятность
[01:06:44.160 --> 01:06:50.280]  может быть не определена, а на другом хорошем, на котором все траектории уже непрерывны. И там уже
[01:06:50.280 --> 01:07:10.480]  все хорошо, и вы уже можете вычислять такие вероятности. Да. Не-не-не, смотрите, вот процессы надо
[01:07:10.480 --> 01:07:17.920]  отличать от траекторий. У нас здесь почти всеми непрерывными траекториями, а не процесс. Траектория
[01:07:17.920 --> 01:07:28.320]  это реализация процесса. Да, это конкретная кривая. Вот. Сам процесс это как бы совокупность всех
[01:07:28.320 --> 01:07:33.920]  реализаций. Процесс совокупность реализаций. А реализация это какая-то одна кривая среди всего
[01:07:33.920 --> 01:07:41.800]  этого множества. Почти всеми это означает, что почти все по исходам. Вот, если вы рассмотрите кси от
[01:07:41.800 --> 01:07:48.200]  омега т, то что такое почти все непрерывные? Это значит, что есть некое множество омег,
[01:07:48.200 --> 01:08:03.040]  меры единица. Вот. Что все омеги оттуда, для всех омег оттуда траектория непрерывна. Всюду по т.
[01:08:03.040 --> 01:08:11.920]  Почти всюду по исходам, всюду по т. Вот. Знаете, вот когда случайные процессы, тут надо думать о двух
[01:08:11.920 --> 01:08:20.800]  сразу вещах. Вот. Процесс это функция двух вещей. Исхода и времени. И поэтому вот все время приходится
[01:08:20.800 --> 01:08:26.600]  оперировать вот этими двумя вещами. Когда у нас будут там понятия предела, производной, то мы
[01:08:26.600 --> 01:08:31.080]  будем ко времени относиться как к параметру. Знаете, такие в школе были задачи с параметрами.
[01:08:31.080 --> 01:08:38.640]  Когда у тебя что-то там при определенных значениях параметра. Вот здесь у нас как бы теория вероятности,
[01:08:38.640 --> 01:08:45.280]  это как бы случайные процессы, это теория вероятности с параметром. Вот. В зависимости от параметра
[01:08:45.280 --> 01:08:50.680]  что-то может быть. Поэтому-то надо быть аккуратным, что для одних значений параметра одно, для других
[01:08:50.680 --> 01:08:58.640]  другое. Вот я так, к слову. Вообще вот такая теорема. Вот. А вообще такая теорема. Это полезная теорема,
[01:08:58.680 --> 01:09:04.880]  мы будем ей потом пользоваться. Я ее тоже привожу без доказательства, потому что у нас будут такие процессы,
[01:09:04.880 --> 01:09:11.920]  для которых мы захотим считать что-то вот такое, но про которое мы сходу не скажем, как бы существуют
[01:09:11.920 --> 01:09:16.520]  эти вероятности или нет. Но мы сможем для них установить, что у них существует непрерывная
[01:09:16.520 --> 01:09:23.680]  модификация. И тогда отсюда следует, что рассматривая непрерывную модификацию процесса,
[01:09:23.680 --> 01:09:30.520]  мы можем вычислять такие вещи. Вероятно, существует, что все места конечномерных распределений
[01:09:30.520 --> 01:09:35.920]  совпадают. А раз совпадает, значит вероятность это существует. Все, профит. Таким образом мы
[01:09:35.920 --> 01:09:49.360]  избегаем вот этой вот сложной проблемы, связанной с несчетностью пересечения. Вот. Так. Ну что еще?
[01:09:49.360 --> 01:09:58.400]  Давайте еще раз оглянемся назад. Что мы сегодня уже сделали? Мы сегодня подвели большой фундамент
[01:09:58.400 --> 01:10:05.840]  подо всем. Первое, ввели определение случайного процесса сечения траектории. Второе, ввели
[01:10:05.840 --> 01:10:12.560]  важнейшую характеристику семейства конечномерных распределений. Посмотрели свойства этого
[01:10:12.560 --> 01:10:18.040]  семейства и выяснили, что их необходимы достаточно для того, чтобы они задавали какой-то
[01:10:18.040 --> 01:10:24.600]  процесс. Так. Процессов, отвечающих одному и тому же семейству распределений, может быть много.
[01:10:24.600 --> 01:10:31.320]  Поэтому нам понадобится понятие о модификации или о стахастической эквивалентности процессов.
[01:10:31.320 --> 01:10:38.480]  Так. Это мы рассмотрели. И мы с вами выяснили, что вот в этих вот случаях, когда мы имеем дело
[01:10:38.480 --> 01:10:45.880]  с случайными функциями непрерывным множеством времени, оказывается, что мы не всегда можем
[01:10:45.880 --> 01:10:50.920]  вычислить какую-нибудь вероятность, потому что то, что стоит под вероятностью, не всегда события в
[01:10:50.920 --> 01:10:58.120]  формальном смысле этого слова. Вот. С этим мы ничего в общем случае не можем поделать. Могут быть
[01:10:58.120 --> 01:11:02.040]  ситуации, когда мы не знаем, реально не знаем, существует вероятность или нет, и мы не можем это
[01:11:02.040 --> 01:11:09.320]  никак выяснить. Вот такое может быть. Но в некоторых случаях мы можем посчитать. Например, когда все
[01:11:09.320 --> 01:11:16.840]  траектории непрерывны, и мы сводим утверждение о несчетном числе точек к утверждению насчетном,
[01:11:16.840 --> 01:11:22.160]  всюдоплотном множестве точек. Для этого нам помогает вот эта теорема о существовании непрерывной
[01:11:22.160 --> 01:11:27.880]  модификации. Если это выполнено, значит у процесса существуют непрерывные модификации, значит
[01:11:27.880 --> 01:11:36.240]  вероятность от такого рода уже посчитать можно. Вот. И мы будем потом этим пользоваться. Вот. Я
[01:11:36.240 --> 01:11:42.440]  понимаю, да, сегодняшняя лекция, она получилась очень абстрактная такая, да. Мы вспомнили множество
[01:11:42.440 --> 01:11:51.360]  всяких, ну нам вернее потребовалось вспомнить множество фактов из теории вероятности. Видите,
[01:11:51.360 --> 01:11:56.880]  тут какой язык, сигма-алгебра, вот эти все абстрактные. Но я хочу сказать, что дальше ничего этого не будет,
[01:11:56.880 --> 01:12:01.960]  по большому счету. То есть мы не будем работать там сигма-алгебрами, заморачиваться ими. Мы будем
[01:12:01.960 --> 01:12:11.360]  решать такие более насущные проблемы. Вот. А то, что я здесь рассказал, это как бы вот введение во всю эту
[01:12:11.360 --> 01:12:16.960]  проблематику, что тут тоже не все так просто. Нельзя, знаете, вот просто так взять, ввести параметр t,
[01:12:16.960 --> 01:12:21.880]  и у тебя, значит, все прекрасно, все считается, все вероятности, никаких проблем ни в чем ты не
[01:12:21.880 --> 01:12:26.360]  испытываешь. Нет. Вот есть определенные проблемы. Это только верхушка асберга,
[01:12:26.360 --> 01:12:34.600]  которая я показал здесь. Вот. Но все, что здесь есть, нам потом приходится решать конкретные задачи и
[01:12:34.600 --> 01:12:43.000]  в дальнейшем выстраивать теорию. Вот. Как я уже сказал, мы не будем определять процессы как функции
[01:12:43.000 --> 01:12:47.760]  исхода и времени. Это неудобно. Так же, как и неудобно, определять случайные величины как
[01:12:47.760 --> 01:12:55.760]  функции исходов. Мы будем задавать процессы через семейство распределений. Вот. Через распределение.
[01:12:55.760 --> 01:13:02.960]  В дальнейшем мы рассмотрим с вами, ну что мы будем дальше делать, пусть только мы фундамент навели.
[01:13:02.960 --> 01:13:10.640]  Мы посмотрим, что такое математическое ожидание, дисперсия процесса. Новые для вас понятия
[01:13:10.640 --> 01:13:18.480]  возникнет корреляционная функция, кавариационная функция процесса. Мы рассмотрим некоторые просто
[01:13:18.480 --> 01:13:27.000]  сверх важные, важнейшие виды случайных процессов, которые проникают во все области деятельности. Это
[01:13:27.000 --> 01:13:36.200]  пуассоновский процесс и виннеровский процесс. Гауссовские процессы. Потом у нас пойдут так
[01:13:36.200 --> 01:13:41.880]  называемые стационарные процессы, оргодические процессы. Это все некие классы процессов с хорошими
[01:13:41.880 --> 01:13:48.920]  свойствами, которые хорошо глубоко изучены, которые встречаются там в науке и технике повсюду и про
[01:13:48.920 --> 01:13:55.280]  которые надо все хорошо знать, потому что они помогают понимать, что происходит и решать задачи.
[01:13:55.280 --> 01:14:01.240]  Мы не будем с вами иметь дела никогда с какими-то совсем произвольными процессами. Мы будем о них
[01:14:01.240 --> 01:14:07.560]  что-то подразумевать и получать из этих свойств, изначальных аксиоматических про эти процессы,
[01:14:07.560 --> 01:14:12.560]  какие-то следствия далеко идущие. Вот такие модели, которые основаны на теории вероимости,
[01:14:12.560 --> 01:14:19.320]  они позволяют эти далеко идущие следствия получать. Так что как бы это абстрактно здесь не выглядело,
[01:14:19.320 --> 01:14:24.840]  это только на начальном этапе. А дальше будем уже заниматься более конкретными вещами.
[01:14:24.840 --> 01:14:38.440]  Наверное, это все, что я хотел сегодня рассказать. Смотрите там мои ПДФки. В моих ПДФках иногда
[01:14:38.440 --> 01:14:45.360]  может быть что-то немножко больше, чем я рассказываю. Так что имейте это в виду.
[01:14:45.360 --> 01:14:52.040]  Я сегодня, может быть, только не коснулся вторичного вероятностного пространства.
[01:14:52.040 --> 01:14:59.840]  Прочитайте в ПДФке, а пока просто на словах расскажу. Смотрите, случайные величины, случайный
[01:14:59.840 --> 01:15:05.520]  процесс, мы их определяем на вероятностных пространствах. Вероятностное пространство это
[01:15:05.520 --> 01:15:13.680]  тройка, ωfp, пространство исходов, сигма-алгебра, некая структура над множеством исходов и
[01:15:13.680 --> 01:15:19.440]  вероятностная мера, которая определена на элементах сигма-алгебра. Это пространство может быть
[01:15:19.440 --> 01:15:27.560]  абстрактным, ω. Это мог быть не только числа, это мог быть строки, слова, кошки, собаки, не знаю,
[01:15:27.560 --> 01:15:39.640]  все что угодно. Омегой может быть. Но на практике используют не это абстрактное вероятностное
[01:15:39.640 --> 01:15:44.520]  пространство, на котором определено, а другое, то, что называется вторичное вероятностное
[01:15:44.520 --> 01:15:50.840]  пространство или выборочное вероятностное пространство. Вот есть у нас исходная ωfp и есть
[01:15:50.840 --> 01:15:59.640]  вторичная множество значений случайной величины, например, R. Все R. Потом сигма-алгебра над R, это,
[01:15:59.640 --> 01:16:05.680]  как правило, баррельская сигма-алгебра. И вероятностная мера на баррельской сигма-алгебре такая,
[01:16:05.680 --> 01:16:13.840]  чтобы она соответствовала P из исходного вероятностного пространства. Вот. Для случайных
[01:16:13.840 --> 01:16:21.480]  процессов тоже можно определить вторичное вероятностное пространство. В этом случае пространством
[01:16:21.480 --> 01:16:27.800]  исходов будет являться множество функций. Для случайной величины это множество ее значений. А
[01:16:27.800 --> 01:16:34.160]  что такое значение процесса? Это его реализация. Значит, вторичным вероятностным пространством для
[01:16:34.160 --> 01:16:43.900]  случайного процесса служит тройка. Первая ω, уже другая ω, это множество функций. Вот это
[01:16:43.900 --> 01:16:51.640]  наше выборочное пространство. Второе сигма-алгебра, уже построенная на множестве функций определенным
[01:16:51.640 --> 01:16:57.160]  образом, как почитаете. Вот. И вероятностная мера, которая соответствует исходной вероятностной мере.
[01:16:57.160 --> 01:17:07.920]  Вот. Тогда, смотрите, тут два подхода возникают к взгляду на случайные процессы. Если вы работаете
[01:17:07.920 --> 01:17:13.600]  в терминах исходного пространства, вы зафиксировали какую-то ω, вы получили конкретную реализацию
[01:17:13.600 --> 01:17:22.440]  функцию. Как си, ω от t. Омега ноль, запятая t. Кривая вам дана. В терминах вторичного пространства,
[01:17:23.160 --> 01:17:30.000]  когда вы фиксируете точку в пространстве функций, вы по сути фиксируете функцию. Вот это и есть
[01:17:30.000 --> 01:17:36.640]  реализация процесса. Так что, что там вы зафиксировали омегу ноль и получили кси от омега ноль t как
[01:17:36.640 --> 01:17:42.480]  реализацию. А здесь вы ткнули сразу в пространство функций. И вот эта функция, в которую вы ткнули,
[01:17:42.480 --> 01:17:48.600]  это и есть реализация процесса. Очень удобно. Вот, например, теорема Калмагорова, первая,
[01:17:49.000 --> 01:17:56.280]  вот она доказывается как раз так. Там рассматривается вторичное вероятностное пространство множества
[01:17:56.280 --> 01:18:01.280]  функций. Сигма алгебры на этом множестве функций. Вероятностная мера, все это определяется.
[01:18:01.280 --> 01:18:12.720]  Ну, это вот просто к слову, что обобщения, они есть полезные. И еще у меня чуть-чуть
[01:18:12.760 --> 01:18:19.760]  время остается. Случайные процессы, это не единственный способ обобщить случайные величины.
[01:18:19.760 --> 01:18:25.480]  Ведь мы добавили один параметр, но никто не запрещает добавлять два параметра, три параметра,
[01:18:25.480 --> 01:18:30.320]  много параметров, бесконечно множество параметров, счетное и несчетное множество параметров.
[01:18:30.320 --> 01:18:39.560]  Понимаете? Если добавляется, допустим, несколько параметров не случайных, то это уже называется
[01:18:39.560 --> 01:18:46.760]  случайными полями. У нас есть случайный процесс, то, что мы T интерпретируем как время, хотя оно вовсе не
[01:18:46.760 --> 01:18:52.360]  обязано быть временем, в таком привычном понимании слова, это не обязано быть физическое время. Это
[01:18:52.360 --> 01:18:58.080]  просто мы назвали T временем, а так мало ли что это может быть. А когда у вас много параметров,
[01:18:58.080 --> 01:19:07.080]  ну какое же это время, если оно двумерное? Это поле, то есть на плоскости у вас загораются какие-то точки
[01:19:07.080 --> 01:19:15.440]  случайно, в случайных местах. Это вот называется случайными полями. Мы в нашем курсе не будем
[01:19:15.440 --> 01:19:22.320]  работать со случайными полями, но я просто говорю, что об общении, они тут самые разные есть и тоже
[01:19:22.320 --> 01:19:29.440]  очень интересные. И про вот эти случайные поля чуть-чуть вы можете почитать вот этой книжке,
[01:19:29.440 --> 01:19:37.600]  которую мы писали в соавторстве с Гастеньковым Александром. Вообще, эту книжку я вам советую
[01:19:37.600 --> 01:19:45.120]  обязательно посмотреть. Сейчас, кстати, выходит переиздание ее в ОРСС, по-моему, называется,
[01:19:45.120 --> 01:19:51.000]  издательство. Там мы много опечаток исправили, но вам, наверное, электронная версия уже без
[01:19:51.000 --> 01:19:59.560]  запечаток должна быть доступна. И там есть теория. Моя теория, наверное, ближе к моему курсу,
[01:19:59.560 --> 01:20:04.920]  чем та, которая там описана, хотя корреляция сильная. Но чем эта книжка особенно цена,
[01:20:04.920 --> 01:20:11.680]  с моей точки зрения, то, что там описано множество приложений случайных процессов в реальности.
[01:20:11.680 --> 01:20:19.560]  Например, там описана задача пейдж ранг гугловская, как гугл ранжирует страницы. Это мы будем проходить
[01:20:19.880 --> 01:20:27.600]  во второй части нашего курса, когда будут марковские цепи пойдут. Там всякие процессы марковские
[01:20:27.600 --> 01:20:33.040]  с непрерывным временем. В общем, несколько приложений там описано, и будет здорово,
[01:20:33.040 --> 01:20:37.240]  если вы туда посмотрите. Они написаны на таком, где-то на научном языке, где-то на
[01:20:37.240 --> 01:20:44.240]  научном популярном языке. На соавторов было много, у всех язык разный. Обязательно полистайте,
[01:20:44.240 --> 01:20:50.440]  почитайте, так вы увидите, где вот это все реально прямо сейчас живет. И как все это применяется,
[01:20:50.440 --> 01:20:58.320]  потому что тогда вы поймете, зачем все вот это вообще надо. Не просто там вы... Ведь кому нужны
[01:20:58.320 --> 01:21:03.560]  просто определения, какие-то теоремы, ведь нужно же это все применять. И вот в этой книжке вы
[01:21:03.560 --> 01:21:11.120]  найдете все эти применения во всей своей красе. Так что читайте, смотрите, интересуйтесь. У меня
[01:21:11.120 --> 01:21:11.720]  все на сегодня.

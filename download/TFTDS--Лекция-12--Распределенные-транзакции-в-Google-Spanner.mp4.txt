[00:00.000 --> 00:14.000]  Помните, когда-то перед лекцией про масштабируемость, на которой мы занимались поиском узких мест в распределенных файловых системах и киваревых хранилищах,
[00:14.000 --> 00:23.000]  я говорил, что эта лекция, во-первых, новая была, во-вторых, очень важная, потому что все, что мы делали до этой лекции,
[00:24.000 --> 00:31.000]  было нужно для того, чтобы в какой-то момент взять и аккумулировать их в некоторые дизайны, настоящие промышленные системы, увидеть, как это все работает вместе.
[00:31.000 --> 00:38.000]  Вот сегодня скажу то же самое, что сегодняшняя лекция про то, чтобы собрать вместе все, что мы изучали три месяца.
[00:38.000 --> 00:44.000]  Сегодня мы воспользуемся нашими знаниями про multiprocess, про версионируемое хранилище,
[00:44.000 --> 00:54.000]  про Colossus, про TrueTime, про протоколы транзакций и соберем это все в одну большую систему, которая будет называться Google Spanner.
[00:54.000 --> 01:01.000]  Это будет наша основная иллюстрация, ну, по крайней мере, на первую половину занятия. Я надеюсь, что это все окажется нам полезным.
[01:01.000 --> 01:09.000]  Поводом поговорить про Google Spanner является наша сегодняшняя тема, это распределенные транзакции.
[01:22.000 --> 01:28.000]  Помните, я многократно уже рисовал такую слоеную архитектуру, где у нас в основании был слой хранения,
[01:28.000 --> 01:33.000]  потом был слой репликации, потом был слой шардирования, ну и вот наконец транзакции.
[01:33.000 --> 01:37.000]  Сегодня мы до этого слоя доберемся и соберем всю эту конструкцию вместе.
[01:37.000 --> 01:44.000]  Но чтобы перейти к распределенным транзакциям, нужно вспомнить про транзакции нераспределенные,
[01:44.000 --> 01:50.000]  потому что это было достаточно давно уже. Итак, я напомню, у нас есть такая задача.
[01:51.000 --> 01:59.000]  Есть хранилище, которое пусть умеет выполнять простые операции.
[01:59.000 --> 02:09.000]  Мы можем в него что-то записать и можем из этого хранилища потом прочитать то, что мы записали.
[02:09.000 --> 02:17.000]  Читаем и пишем мы по ключам. Мы сейчас ничего не говорим про то, распределенное это хранилище,
[02:17.000 --> 02:22.000]  или это локальное хранилище, или это может быть даже просто память в компьютере.
[02:22.000 --> 02:28.000]  Нам это не важно. Но нам важно, что это хранилище является атомарным, в смысле линейризуемым.
[02:28.000 --> 02:32.000]  С ним можно работать конкурентно, и конкурентно исполнено этим хранилищем.
[02:32.000 --> 02:39.000]  О нем можно думать как у последовательного, который при этом еще уважает порядок операции в реальном времени.
[02:39.000 --> 02:42.000]  И мы над таким хранилищем хотим выполнять транзакции.
[02:42.000 --> 02:48.000]  Интуитивно транзакции – это операция, которая трогает атомарно сразу несколько ключей.
[02:48.000 --> 02:59.000]  В нашем случае транзакции – это интерактивные программы, которые начинаются со служебной операции startTransaction,
[02:59.000 --> 03:07.000]  за которой следует серия записей и чтений, которые организованы какой-то логикой.
[03:07.000 --> 03:12.000]  Мы читаем один ключ, смотрим на результат, и на основе того, что мы прочитали, делаем какую-то запись.
[03:12.000 --> 03:21.000]  И завершается эта интерактивная программа, эта транзакция, либо служебной операции commitTransaction,
[03:21.000 --> 03:29.000]  которая означает, что пользователь, клиент нашей системы хочет зафиксировать те изменения, которые он сделал в хранилище,
[03:29.000 --> 03:32.000]  или же abortTransaction.
[03:32.000 --> 03:40.000]  Пользователь проверил какие-то инварианты в своей транзакции, они не сошлись, поэтому транзакция откатывается.
[03:40.000 --> 03:53.000]  И пользователь адресовал все эти операции, все эти шаги транзакции компоненту, который назывался планировщик.
[03:53.000 --> 03:57.000]  Планировщик находился между клиентами, которые выполняют транзакции, и хранилищем.
[03:57.000 --> 04:06.000]  Он получал от каждого клиента, пусть последовательно, все его операции, и его задача была перенаправлять их в уровень хранилища таким образом,
[04:06.000 --> 04:15.000]  чтобы с одной стороны транзакции исполнялись параллельно, а с другой стороны пользователь получал какую-то понятную модель исполнения этих транзакций.
[04:15.000 --> 04:22.000]  Можно представить себе очень простую модель, где все планировщики выполняют транзакции просто последовательно.
[04:22.000 --> 04:27.000]  У него есть внутри один глобальный Mutex, и он упорядочивает все транзакции.
[04:27.000 --> 04:36.000]  Это было бы делать очень неэффективно, зато мы бы знали, что планировщик порождает, чтобы транзакции исполняются незатейливо,
[04:36.000 --> 04:44.000]  и не нужно думать про то, как они конкурируют. Так делать, конечно, не нужно, но нужно исполнить транзакции параллельно.
[04:44.000 --> 04:49.000]  Но вместе с этим нужно предоставить пользователю какую-то понятную модель изоляции.
[04:58.000 --> 05:03.000]  Конечно же планировщик хочет запускать параллельно чтение и записи в хранилище.
[05:03.000 --> 05:16.000]  Между планировщиком и хранилищем рождаются конкурентные истории, состоящие из чтений и записи отдельных транзакций.
[05:16.000 --> 05:24.000]  А поскольку само хранилище является линейризуемым, то об исполнении планировщиком всех записей и чтений
[05:24.000 --> 05:30.000]  можно думать как о некоторой последовательной истории из отдельных записей и отдельных чтений.
[05:30.000 --> 05:35.000]  То есть любая конкурентная история, о ней можно думать как о последовательной.
[05:35.000 --> 05:44.000]  И можно сказать, что планировщик порождает вот такие последовательные истории, или как они называются в этом контексте расписания.
[05:44.000 --> 05:50.000]  И модель согласованности говорит пользователю о том, какие расписания может порождать планировщик.
[05:50.000 --> 05:58.000]  Вот простой планировщик, который устроен внутри так, что он берет глобальный мьютекс, он порождает только серийные расписания.
[05:58.000 --> 06:03.000]  То есть расписания, где все операции каждой транзакции просто идут группой.
[06:03.000 --> 06:11.000]  Но пользователю это не нужно, потому что это такая избыточная гарантия, потому что пользователь не наблюдает вот этих самых расписаний.
[06:11.000 --> 06:21.000]  Он работает только с планировщиком и смотрит и видит только те результаты, которые он получает из чтений, из планировщика.
[06:21.000 --> 06:26.000]  Он не наблюдает самих расписаний, которые рождаются между планировщиком и хранилищем.
[06:26.000 --> 06:31.000]  Поэтому мы сказали, что нам достаточно, нам не нужно строить в планировщике только серийные расписания.
[06:31.000 --> 06:36.000]  Нам достаточно строить расписания, которые неотличимы от серийных.
[06:38.000 --> 06:42.000]  И такие расписания мы назвали бью-сериализуемыми.
[06:42.000 --> 06:52.000]  А моделизация мы назвали сериализуемость.
[06:53.000 --> 07:02.000]  Ну или даже строгая сериализуемость, если мы дополнительно требуем, чтобы планировщик порождал,
[07:02.000 --> 07:07.000]  во-первых, такие расписания, которые неотличимы от серийных, от последовательных,
[07:07.000 --> 07:11.000]  и к тому же он бы уважал предшествование транзакций в реальном времени.
[07:11.000 --> 07:18.000]  Если одна транзакция зафиксировалась до старта другой, то вот в этой сериализации транзакции шли бы в том же самом порядке.
[07:19.000 --> 07:27.000]  Но мы на лекции про транзакции выяснили, что вот такие вот расписания, которые являются сериализуемыми,
[07:27.000 --> 07:31.000]  которые неотличимы для наблюдателя от серийных, довольно сложно устроены,
[07:31.000 --> 07:35.000]  что задача тестирования таких расписаний на сериализуемость, она, наверное, полная.
[07:35.000 --> 07:40.000]  Поэтому мы не смогли бы построить такой планировщик, который порождает все такие расписания.
[07:40.000 --> 07:43.000]  Поэтому мы ограничились некоторым под классом.
[07:48.000 --> 08:03.000]  Мы сказали, что хорошо, пусть планировщик не может порождать все подобные расписания, не слишком сложно устроенные,
[08:03.000 --> 08:09.000]  пусть он порождает только расписания, которые неотличимы от серийных в том смысле,
[08:09.000 --> 08:15.000]  что можно получить из нашего расписания серийные путем свопов соседних неконфликтующих операций.
[08:15.000 --> 08:21.000]  Скажем, двух чтений одного и того же ключа или двух записей произвольных ключей.
[08:21.000 --> 08:27.000]  Но, скажем, переставлять местами запись и чтение одного и того же ключа в разных транзакциях нельзя,
[08:27.000 --> 08:32.000]  потому что, очевидно, это повлияет на результат этих самых чтений.
[08:32.000 --> 08:38.000]  А если мы переставляем две записи по одному и тому же ключу, то это повлияет на конечное состояние базы данных.
[08:38.000 --> 08:46.000]  Для такого определения сериализуемости мы нашли критерий.
[08:46.000 --> 08:52.000]  Мы сказали, что можно построить по расписанию граф конфликтов, в котором вершины транзакций в расписании
[08:52.000 --> 08:59.000]  а дуги соединяют две транзакции, если в расписании есть две операции, одна из которых предшествует другое,
[08:59.000 --> 09:02.000]  они конфликтуют. Из этих двух транзакций, которые конфликтуют, одна предшествует другой.
[09:02.000 --> 09:09.000]  То есть граф конфликтов у нас задавал систему ограничений на потенциальную сериализацию в расписании.
[09:09.000 --> 09:16.000]  И, пользуясь этим критерием, мы построили протокол, построили планировщик, который гарантированно порождает
[09:16.000 --> 09:23.000]  конфликтно-сериализуемое расписание, то есть расписание не отличимое для пользователя от серийных,
[09:23.000 --> 09:27.000]  ну и пользователей этого будет достаточно. Этот протокол назывался двухфазной блокировки.
[09:32.000 --> 09:47.000]  И, коротко, 2PL. Как он был устроен? Мы связывали с каждым ключом в нашем хранилище блокировку.
[09:47.000 --> 09:56.000]  Когда транзакция приходила в планировщик с первой своей операцией, то планировщик,
[09:56.000 --> 10:05.000]  перед тем как обслужить запись либо чтение, сначала брал блокировку на соответствующий ключ.
[10:05.000 --> 10:13.000]  Это мы рисовали вот такой вертикальной стрелочкой. И после этого уже, скажем, выполнял запись.
[10:13.000 --> 10:20.000]  После этого пользователь, например, начинал чтение, присылал нам соответствующий ключ с операцией,
[10:20.000 --> 10:26.000]  мы для него брали блокировку и выполняли чтение. Важно, чтобы блокировки после чтения и записи
[10:26.000 --> 10:44.000]  мы не отпускали, а накапливали в транзакции и делали так до тех пор, пока транзакция не решала закоммититься.
[10:44.000 --> 10:49.000]  После этого, в момент коммита транзакции, планировщик должен был где-то надежно зафиксировать,
[10:49.000 --> 10:53.000]  что транзакция завершилась, что она хочет сохранить свои изменения.
[10:53.000 --> 10:59.000]  Если мы говорим про базу данных локальной, то можно представить себе журнал на жестком диске,
[10:59.000 --> 11:07.000]  где мы с каждой записью логируем либо откат транзакции, либо новый ключ, который мы хотим записать.
[11:07.000 --> 11:12.000]  И в момент коммита мы пишем служебную запись в этот журнал, что все, транзакция надежно сохранена.
[11:12.000 --> 11:19.000]  И даже после рестарта системы мы сможем этот журнал проиграть, повторить и восстановить состояние.
[11:19.000 --> 11:25.000]  Но после того, как мы запись в журнал сделали, после того, как мы зафиксировали надежно, то логи можно отпускать.
[11:28.000 --> 11:31.000]  То есть это протокол двухфазных блокировок. И вот почему он так назывался?
[11:31.000 --> 11:40.000]  Эта фаза называлась фаза роста, а эта фаза сжатия.
[11:42.000 --> 11:52.000]  Вот это все происходило в разрезе одной конкретной транзакции.
[11:52.000 --> 11:56.000]  И планировщик так поступил с каждой отдельной транзакцией.
[11:56.000 --> 12:00.000]  И если транзакции не пересекались, скажем, по множеству ключей, которым они обращаются,
[12:00.000 --> 12:02.000]  то планировщик мог исполнять их параллельно.
[12:02.000 --> 12:07.000]  Если транзакции пересекались по ключам, то одна транзакция, видимо, должна ждать другую.
[12:07.000 --> 12:15.000]  Мы показали, что такой планировщик порождает только серилизуемые расписания,
[12:15.000 --> 12:21.000]  показав, что в графе конфликтов для любого расписания, который порождает такой планировщик, не может быть циклов.
[12:21.000 --> 12:25.000]  То есть мы воспользовались тем критерием, которое мы построили для вот такого определения серилизуемости.
[12:25.000 --> 12:28.000]  А дальше мы сделали пару наблюдений.
[12:28.000 --> 12:33.000]  Ну, во-первых, если мы берем блокировку на чтение, то мы можем брать блокировку разделяемую.
[12:33.000 --> 12:35.000]  Но для записей нам нужно брать эксклюзивную.
[12:35.000 --> 12:40.000]  То есть если две транзакции читают разные ключи, то они могут читать параллельно.
[12:40.000 --> 12:45.000]  Но если одна транзакция читает ключи другая и пишет, то они исключают друг друга,
[12:45.000 --> 12:50.000]  исключают параллельное исполнение друг друга, потому что одна из блокировок будет эксклюзивной.
[12:50.000 --> 12:58.000]  Мы столкнулись с проблемой двухфазной блокировки, которая была связана с тем, что транзакции интерактивные.
[12:58.000 --> 13:03.000]  Планировщик заранее не знает, каким ключам будет обращаться пользователь своей транзакции,
[13:03.000 --> 13:06.000]  поэтому он не может гарантировать, что локи будут браться монотонно.
[13:06.000 --> 13:08.000]  Это значит, что возникают дедлоки.
[13:08.000 --> 13:12.000]  Поэтому нужен механизм, который позволяет из этих дедлоков как-то выбираться.
[13:12.000 --> 13:17.000]  И мы рассмотрели для этого стратегию, которая называлась Soundweight.
[13:20.000 --> 13:26.000]  Мы говорили, пусть каждая транзакция на старте выбирает себе временную метку.
[13:26.000 --> 13:28.000]  Они не обязательно должны быть строгими монотонно.
[13:28.000 --> 13:31.000]  Они должны всего лишь коррелировать с возрастом транзакции.
[13:31.000 --> 13:34.000]  Чем раньше транзакция родилась, тем меньше у нее временная метка.
[13:34.000 --> 13:45.000]  И если какая-то транзакция текущая хочет взять лок, а этим локом уже владеет другая транзакция,
[13:52.000 --> 13:54.000]  то мы сравним их в временные метки.
[13:54.000 --> 14:01.000]  И если оказывается, что наша транзакция старше, чем транзакция, которая владеет блокировкой,
[14:01.000 --> 14:06.000]  то мы транзакцию, которая владеет блокировкой, кикаем.
[14:06.000 --> 14:09.000]  Она откатывается, и мы блокировку подбираем.
[14:09.000 --> 14:14.000]  Если же оказывается, что мы младшая транзакция, то есть наш timestamp больше,
[14:14.000 --> 14:20.000]  то мы дожидаемся, пока транзакция владеет блокировкой и ее не отпустит.
[14:20.000 --> 14:25.000]  Такой подход гарантирует, что дедлоков не будет, потому что дедлок, по определению,
[14:25.000 --> 14:29.000]  этот цикл в графе конфликтов, в графе, где дугами соединены транзакции,
[14:29.000 --> 14:32.000]  если они друг друга ожидают, если одна ожидает другой.
[14:32.000 --> 14:37.000]  И в этой схеме видно, что если дуга есть, то это дуга монотонная.
[14:37.000 --> 14:41.000]  То есть мы ждем, только если мы младшая транзакция.
[14:41.000 --> 14:44.000]  Таким образом, циклов быть не может. Значит, не может быть дедлоков.
[14:44.000 --> 14:48.000]  Но с другой стороны, ровно из-за этой стратегии, из-за этой стратегии,
[14:48.000 --> 14:52.000]  которая помогает нам сбегать дедлоков, у нас появляются откаты транзакций,
[14:52.000 --> 14:57.000]  которые происходят не по воле самой транзакции, потому что в ней не выполнились какие-то варианты,
[14:57.000 --> 15:02.000]  а просто потому, что наш планировщик иначе зависает.
[15:02.000 --> 15:11.000]  Может показаться, что это незначительный момент, но вот сегодня он будет для нас принципиально важен.
[15:11.000 --> 15:14.000]  Это первый протокол, который мы рассмотрели.
[15:14.000 --> 15:21.000]  Но у этого протокола был не то что быизъян, некоторое несовершенство.
[15:21.000 --> 15:29.000]  Вот предположим, что среди наших транзакций есть какие-то небольшие транзакции,
[15:29.000 --> 15:31.000]  которые берут локи, что-то делают и уходят.
[15:31.000 --> 15:37.000]  А есть транзакции, которые длятся долго, и они долго что-то читают.
[15:37.000 --> 15:39.000]  Вот можно себе представить разные примеры.
[15:39.000 --> 15:44.000]  Можно представить себе, что вы хотите сделать бэкап вашей базы данных.
[15:44.000 --> 15:48.000]  Она там внутри может быть даже отказоустойчивая, реплицированная,
[15:48.000 --> 15:54.000]  а может быть это локальная база данных, и вы просто хотите зафиксировать текущее состояние,
[15:54.000 --> 16:00.000]  записать куда-то на диск и безопасно сохранить где-то в холодном хранилище.
[16:00.000 --> 16:05.000]  Для этого вы хотите прочитать всю базу данных, какое-то ее согласованное состояние,
[16:05.000 --> 16:10.000]  моментальное состояние. При этом вы не хотите на это время брать блокировки на чтение,
[16:10.000 --> 16:14.000]  потому что вы заблокируете все апдейты, и вам нужно работать.
[16:14.000 --> 16:17.000]  Ну или может быть, если мы говорим про распределенные системы,
[16:17.000 --> 16:23.000]  может быть у вас данных так много, и вы хотите их прочитать, ну скажем, в целые мы продюс-операции.
[16:23.000 --> 16:28.000]  То есть вы хотите даже чтение распараллелись, потому что оно супер долгое и супер большое.
[16:28.000 --> 16:33.000]  Вот в этом случае такой подход неприменим, он слишком пессимистичен.
[16:33.000 --> 16:41.000]  И альтернативный вариант называется изоряция Snapshot.
[16:51.000 --> 16:58.000]  Если мы хотим одновременно и читать, и записывать новые данные в системе в нашем хранилище,
[16:58.000 --> 17:03.000]  то очевидно мы должны поддерживать просто разные версии данных.
[17:03.000 --> 17:08.000]  То есть если в двухфазных блокировках, когда мы пишем что-то транзакцию,
[17:08.000 --> 17:13.000]  то мы перезаписываем ключи, то сейчас мы хотим, чтобы каждая транзакция,
[17:13.000 --> 17:18.000]  когда она что-то пишет, когда она комитится, порождала бы новую версию хранилища.
[17:18.000 --> 17:36.000]  И вот эти версии, они были бы имутабельными, и каждая версия адресовалась бы некоторым числом.
[17:36.000 --> 17:45.000]  Когда транзакция стартовала бы, она получала бы на старте временной метку,
[17:45.000 --> 17:52.000]  которая бы адресовала версию хранилища, относительно которой транзакция будет выполнять все свои чтения.
[17:52.000 --> 18:00.000]  Давайте я нарисую транзакцию.
[18:00.000 --> 18:10.000]  Вот транзакция стартовала, и в момент старта, на шаге Start Transaction, получила себе read timestamp.
[18:11.000 --> 18:18.000]  Этот read timestamp подрисует, ну давайте пронумеруем это, нулевая версия, это первая версия.
[18:18.000 --> 18:24.000]  Не то чтобы мы требуем, чтобы версии нумировались подрятыдущими натуральными числами,
[18:24.000 --> 18:28.000]  этого как раз мы не требуем, но для картинки подойдет.
[18:28.000 --> 18:35.000]  Мы получили read timestamp, допустим один, и вот относительно этой версии мы собираемся выполнять все свои чтения.
[18:35.000 --> 18:43.000]  Когда же мы делаем запись в транзакции, то мы эту запись просто буферизируем на клиенте.
[18:43.000 --> 18:48.000]  То есть мы пока не сообщаем об этой записи в хранилище.
[18:48.000 --> 18:54.000]  И когда мы решаем транзакцию в хранилище зафиксировать, когда мы хотим сделать commit timestamp,
[18:54.000 --> 19:01.000]  то мы просто вливаем автомарно каким-то магическим образом все наши записи в хранилище, порождая в нем новую версию.
[19:05.000 --> 19:15.000]  Вот это все напоминает система контроля версий.
[19:15.000 --> 19:20.000]  Здесь мы очипили ветку, здесь мы сделали какие-то локальные комиты, пока не говоря об этом мастеру.
[19:20.000 --> 19:25.000]  И вот здесь мы собираемся влить изменения в мастер, породить там новое звено.
[19:25.000 --> 19:29.000]  Но, как вы понимаете, в системе контроля версии ничего даже не так гладко бывает.
[19:29.000 --> 19:35.000]  Когда вы хотите сделать мерч, то он не всегда происходит, потому что случаются конфликты с другими разработчиками,
[19:35.000 --> 19:37.000]  потому что есть конкуренция.
[19:37.000 --> 19:40.000]  Но вот здесь у нас в транзакциях, конечно же, тоже есть конкуренция.
[19:40.000 --> 19:43.000]  Представим, что у нас была другая транзакция.
[19:43.000 --> 19:49.000]  Я не сказал, что когда мы комитим транзакцию, мы выбираем себе новую версию, она называется commit timestamp.
[19:49.000 --> 19:55.000]  И вот под этой версией мы порождаем новое состояние хранилищ.
[19:55.000 --> 19:58.000]  Как происходит конкуренция транзакции?
[19:58.000 --> 20:02.000]  Вот, допустим, у нас была другая синяя транзакция.
[20:02.000 --> 20:07.000]  Она на старте получила временную метку, скажем, 2.
[20:07.000 --> 20:12.000]  Потом сделала какие-то свои записи.
[20:12.000 --> 20:17.000]  И тоже решила влить их в мастер.
[20:17.000 --> 20:27.000]  Но так получилось, что какие-то записи, например, в 2 и в 2 штрих, обращались, перезаписывали один и тот же ключ.
[20:27.000 --> 20:32.000]  Тогда получается, что с момента старта этой транзакции, с момента отщепления ветки,
[20:32.000 --> 20:39.000]  в мастере ключ, который эта транзакция перезаписала, тоже был перезаписан.
[20:39.000 --> 20:47.000]  И когда мы пытаемся закомитить транзакцию, то мы этот конфликт должны обнаружить и транзакцию откатить.
[20:47.000 --> 20:58.000]  Если какие-то две операции конфликтуют, две операции записи, то комит этой транзакции будет неуспешен.
[20:58.000 --> 21:08.000]  Это мы называли правилом first-committer wins.
[21:08.000 --> 21:17.000]  У нас здесь конфликтует только записи, поэтому это такой частный случай определения конфликта,
[21:17.000 --> 21:21.000]  когда две записи обращаются к одному и тому же ключу просто-напросто.
[21:21.000 --> 21:26.000]  Если между стартом и попыткой комит транзакции была другая транзакция,
[21:26.000 --> 21:31.000]  которая успела закомитить свои изменения, то наша транзакция закомититься не может, должна откатываться.
[21:31.000 --> 21:36.000]  И мы должны ее рестартовать.
[21:36.000 --> 21:41.000]  Я забыл сказать, что если мы сохраняем timestamp при рестарте, то транзакциям гарантируется прогресс.
[21:41.000 --> 21:47.000]  Рано или поздно транзакция станет самой старшей, ее уже некому будет вытеснить, и она завершится.
[21:47.000 --> 21:55.000]  То есть мы просто ретравимся.
[21:55.000 --> 22:01.000]  Такой подход не требует от чтения никаких блокировок.
[22:01.000 --> 22:06.000]  Этот подход позволяет нам гораздо больше параллелизма иметь.
[22:06.000 --> 22:09.000]  Но с другой стороны, у него есть своя цена.
[22:09.000 --> 22:14.000]  А именно, мы лишаемся нашей гарантии, сериализуемости.
[22:14.000 --> 22:15.000]  Почему?
[22:15.000 --> 22:22.000]  Может показаться, что история изменений хранилища, история вот этих версий, это и есть сериализация.
[22:22.000 --> 22:26.000]  Каждая версия это комит некоторые транзакции, они в каком-то порядке выстроены.
[22:26.000 --> 22:33.000]  Но я напомню, что этот протокол позволял себе генерировать такие исполнения и такие истории,
[22:33.000 --> 22:37.000]  которые не объяснялись никаким последовательным применением транзакций.
[22:37.000 --> 22:41.000]  Мы рассматривали сценарий, который называется writes queue.
[22:41.000 --> 22:44.000]  На языке транзакций это называлось аномалией.
[22:44.000 --> 22:54.000]  Когда у нас были две транзакции, и одна из них, скажем, читала ключ Х.
[22:54.000 --> 22:57.000]  Короче, напишу.
[22:57.000 --> 23:15.000]  Другая симметрично читала ключ Y, и если видела в нем ноль, то писала в X.
[23:15.000 --> 23:24.000]  И если мы применяли две такие транзакции, хранилища, где изначально X и Y были равны нулю,
[23:24.000 --> 23:34.000]  то если вот эти две транзакции отщепились от начальной версии, то они обе прочитали эти два ключа,
[23:34.000 --> 23:37.000]  увидели нули, и обе комитятся.
[23:37.000 --> 23:41.000]  Комитятся в каком-то порядке, выбирая себе какие-то commit timestamp.
[23:41.000 --> 23:46.000]  Они должны быть разные, это уже деталь реализации.
[23:46.000 --> 23:51.000]  Важно, что на стадии комита вот это правило никаких конфликтов не обнаружит.
[23:51.000 --> 23:54.000]  Просто потому что конфликтов на writesets нет.
[23:54.000 --> 23:56.000]  Транзакции пишут разные ключи.
[23:56.000 --> 24:02.000]  В итоге после комита двух этих транзакций в хранилище у нас остаются значения для X единицы, для Y единицы.
[24:02.000 --> 24:11.000]  И понятно, что такой результат не соответствует никакому порядку сериализации, ни одному, ни другому.
[24:11.000 --> 24:16.000]  Неприятно, но зато параллельно.
[24:16.000 --> 24:19.000]  Вот два таких подхода.
[24:19.000 --> 24:22.000]  Да, еще маленькое замечание.
[24:22.000 --> 24:30.000]  Двухфазные блокировки давали нам, конечно, строгую сериализуемость.
[24:30.000 --> 24:35.000]  Словом можно добавить сюда.
[24:35.000 --> 24:40.000]  Ну вот, два подхода к изоляции транзакций.
[24:40.000 --> 24:47.000]  Это я коротко повторил предшествующие занятия, предшествующие логические занятия.
[24:47.000 --> 24:51.000]  Вот здесь нет ни слова про распределенность, про темы нашего курса.
[24:51.000 --> 24:56.000]  Мы здесь не говорим ничего про отказы, про шардирование, про какие-то рестарты.
[24:56.000 --> 25:02.000]  Нас здесь все это не волнует, потому что все это посвящено модели изоляции.
[25:02.000 --> 25:05.000]  Или иначе говоря, конкуренции транзакций.
[25:05.000 --> 25:09.000]  Мы совсем не беспокоились о том, как устроено хранилище.
[25:09.000 --> 25:14.000]  Что там могут быть какие-то шарды, что у нас узлы могут перезагружаться,
[25:14.000 --> 25:20.000]  перезагружаться могут узлы системы, перезагружаться могут сами клиенты или вообще отказывать.
[25:20.000 --> 25:26.000]  Во всем этом мы не заботились, потому что мы говорили, что в конце концов оба этих протокола можно переносить,
[25:26.000 --> 25:31.000]  можно использовать как локально на одной машине, можно даже использовать внутри процессора.
[25:31.000 --> 25:37.000]  Я, кажется, рассказывал про то, как с помощью такого протокола можно с помощью такого протокола на уровне,
[25:37.000 --> 25:42.000]  реализовав блокировки и конфликты на уровне протокола к гирянности кашей,
[25:42.000 --> 25:46.000]  добиться транзакционности вращениях ячейком памяти.
[25:46.000 --> 25:51.000]  Так вот, все это работало в самом разном масштабе.
[25:51.000 --> 25:55.000]  Но вот сегодня мы хотим поговорить именно про распределенность,
[25:55.000 --> 25:58.000]  про то, как эти протоколы можно реализовать в распределенной системе.
[25:58.000 --> 26:03.000]  И про то, что вот на этом уровне уже становится важно, что наше хранилище не монолитное,
[26:03.000 --> 26:09.000]  что в нем есть какие-то невидимые границы шардов, что клиенты могут отказывать, что узлы могут пересгружаться.
[26:09.000 --> 26:13.000]  И вот все это мы рассмотрим на двух больших иллюстрациях.
[26:13.000 --> 26:18.000]  Мы сегодня поговорим про две системы, про Google Spanner, но это самая большая база в данных мире,
[26:18.000 --> 26:22.000]  вполне уместно про нее поговорить, и это Яндекс.дб.
[26:22.000 --> 26:26.000]  Почему две системы? Почему именно такие, почему две?
[26:26.000 --> 26:33.000]  Потому что две эти системы используют очень разный подход к реализации распределенных транзакций.
[26:33.000 --> 26:40.000]  Google Spanner использует с одной стороны классический подход, двухфазный коммит, про который еще пойдет чуть позже,
[26:40.000 --> 26:46.000]  но в то же время добавляет туда нечто особенное, а именно TrueTime.
[26:46.000 --> 26:54.000]  А Яндекс.дб, который... Google Spanner статья была опубликована в 2012 или 2013 году,
[26:54.000 --> 26:58.000]  статья, по которой начали писать Яндекс.дб была опубликована в 2013 году,
[26:58.000 --> 27:04.000]  так вот эта система использует совершенно другой подход к транзакциям и пытается исправить
[27:04.000 --> 27:09.000]  некоторую фундаментальную проблему, которая заложена в двухфазном коммите и в дизайне спаннера.
[27:09.000 --> 27:16.000]  И почему... Этим, конечно, все не исчерпывается, двухфазным коммитом и детерминированными транзакциями,
[27:16.000 --> 27:22.000]  но все же я бы сказал, что в промышленных системах в реальном мире это два главных подхода.
[27:22.000 --> 27:29.000]  Ну и я обращаю внимание, что наша цель сегодня все же не про конкретные системы говорить, это всего лишь иллюстрации,
[27:29.000 --> 27:36.000]  а увидеть, как на примере этих систем, как в дизайне этих систем возникают те или иные задачи,
[27:36.000 --> 27:43.000]  и как они там решаются, с помощью каких алгоритмов. Ну а побочный эффект, обсуждая дизайн спаннера,
[27:43.000 --> 27:50.000]  мы, конечно же, соберем вместе все, что мы к этому моменту накопили с вами в курсе.
[27:50.000 --> 27:59.000]  Ну что, давайте поговорим про спаннер. И давайте для этого телепортируемся в проектор.
[27:59.000 --> 28:05.000]  Итак, по списку авторов видно, что система Google Spanner довольно сложная,
[28:05.000 --> 28:12.000]  поэтому нам потребуется много времени.
[28:12.000 --> 28:19.000]  Итак, что из себя представляет спаннер? Спаннер представляет из себя базу данных,
[28:19.000 --> 28:33.000]  то есть он предоставляет пользователю табличную модель, там есть строчки, колонки, ячейки, типы, схемы,
[28:33.000 --> 28:37.000]  распределенные SQL запросы, то есть все, что вы ожидаете от базы данных.
[28:37.000 --> 28:43.000]  Но для наших целей можно думать о спаннере как о распределенном кивелю хранилище.
[28:43.000 --> 28:51.000]  И вот давайте посмотрим, как организованы те уровни архитектуры, про которые мы уже много раз говорили,
[28:51.000 --> 28:57.000]  шардирование, репликация и хранение.
[28:57.000 --> 29:09.000]  Вот на этой картинке изображен отдельный шарт кивелю хранилища, над которым строятся таблицы.
[29:09.000 --> 29:14.000]  Каждый шарт образован разными, несколькими репликами, тремя репликами здесь.
[29:14.000 --> 29:25.000]  Каждая реплика для повышения доступности находится в отдельном датацентре.
[29:25.000 --> 29:31.000]  Реплики между собой реплицируют данные с помощью протокола multipax.
[29:31.000 --> 29:40.000]  Состояние каждой реплики это данные таблицы, то есть некоторого фрагмента какой-то большой таблицы.
[29:40.000 --> 29:49.000]  И что примечательно, что каждая реплика в этом multipax хранит данные не на локальном каком-то жестком диске,
[29:49.000 --> 29:54.000]  который может отказать, и тогда реплика, тогда мы потеряем целиком реплику.
[29:54.000 --> 29:57.000]  Нет, каждая реплика для хранения данных, своей копии данных,
[29:57.000 --> 30:04.000]  использует распределенную файловую систему, колосус, который находится, напомню, в каждом датацентре,
[30:04.000 --> 30:13.000]  которая своя в каждом датацентре. У нас есть 3DC, в каждом свой колосус, и реплика хранит данные в этом колосусе.
[30:13.000 --> 30:20.000]  Если вдруг какая-то реплика, какой-то узел, который реализуется в logarитме multipax вдруг откажет,
[30:20.000 --> 30:26.000]  то это не значит, что система потеряет реплику навсегда, и что нужно будет это сложным образом чинить все это.
[30:26.000 --> 30:33.000]  Мы можем перенести реплику на другую машину, и при этом она сохранит свое персистентное состояние,
[30:33.000 --> 30:39.000]  потому что оно было в колосусе, оно было там реплицировано, и там могут использоваться как-то не тройная репликация,
[30:39.000 --> 30:45.000]  а может там используются erasure-коды. Помните, я про это как-то рассказывал.
[30:45.000 --> 30:52.000]  И что нам сейчас особенно важно, в этом multipax есть стабильный reader.
[30:52.000 --> 30:59.000]  Мы его каким-то образом выбираем, и он живет, мы рассчитываем, что долго.
[30:59.000 --> 31:03.000]  Нам сейчас не нужно сильно погружаться в устройство колосуса, в устройство multipax,
[31:03.000 --> 31:09.000]  окажется, мы по отдельности все это обсудили, и сейчас мы можем пользоваться этим как просто отдельными кубиками.
[31:09.000 --> 31:16.000]  Ну вот, так организован отдельный шард. И в нашей системе много таких шардов,
[31:16.000 --> 31:23.000]  и транзакции потенциально задевают тоже много таких шардов, ну, какое-то подможество из всех шардов,
[31:23.000 --> 31:30.000]  которые обслуживают данную таблицу.
[31:30.000 --> 31:38.000]  Если слой хранения репликации шардирования понятен, то можно приступить к разговору о транзакциях.
[31:38.000 --> 31:45.000]  Почему мы вообще должны обо всем этом думать, когда мы говорим про реализацию транзакций с панели?
[31:45.000 --> 31:56.000]  Итак, Spanner в первом приближении реализует протокол транзакции двухфазной блокировки.
[31:56.000 --> 32:00.000]  Вот видите, тут подсвечено.
[32:00.000 --> 32:10.000]  И для того, чтобы избегать дедлоков, Spanner использует стратегию ваунвейт.
[32:10.000 --> 32:18.000]  То есть пока ничего нового. Пока мы все это знаем, и непонятно, зачем мы продолжаем эту лекцию, хотя вроде по частям все известно.
[32:18.000 --> 32:22.000]  Давайте объясню общую трудность.
[32:22.000 --> 32:30.000]  Ну, потому что есть отказы. Потому что клиент может отказывать, и потому что могут перезагружаться реплики какого-то шарда.
[32:30.000 --> 32:37.000]  Ну вот мы эти проблемы сначала изучим в контексте транзакций, которые касаются только одного шарда.
[32:37.000 --> 32:48.000]  Вот пусть наша транзакция работает с несколькими ключами, но все эти ключи находятся в пределах одного таблета и обслуживаются одним набором реплик.
[32:48.000 --> 32:52.000]  Вот я утверждаю, что это сильно упрощает задачу.
[32:52.000 --> 32:55.000]  Вот ограничимся такой картинкой.
[32:55.000 --> 33:00.000]  Как же вот поверх вот этой всей конструкции реализуется протокол двухфазных блокировок?
[33:00.000 --> 33:08.000]  Ну, я уже говорил, что в двухфазных блокировках нужно связывать с каждым ключом в хранилище блокировку, собственно.
[33:08.000 --> 33:14.000]  Не то, что в этом UTX, это просто некоторая служебная запись, что вот для ключа вся та блокировка.
[33:14.000 --> 33:18.000]  И все эти записи находятся в структуре под названием Logtable.
[33:18.000 --> 33:22.000]  И этот Logtable обслуживается компонентом, который называется Transaction Manager.
[33:22.000 --> 33:30.000]  Вот Transaction Manager это примерно планировщик, который был изображен вот на доске, где-то здесь.
[33:30.000 --> 33:42.000]  Вот этот Transaction Manager получает записи, чтения от клиентов, берет блокировки, записывает их в эту таблицу и дальше работает с данными таблета.
[33:42.000 --> 33:51.000]  Что любопытно, что Transaction Manager живет на лидере и блокировки хранит тоже только на нем.
[33:51.000 --> 33:56.000]  Вот таблица с блокировками – это не персистентное состояние.
[33:56.000 --> 34:04.000]  Если RSM, то есть вот этот набор реплик, этот мультипаксус перезагрузится в смысле, там, не знаю, перевыберется лидер,
[34:04.000 --> 34:08.000]  потому что этот лидер отказал и пришлось выбрать нового.
[34:09.000 --> 34:16.000]  Вообще говоря, отказ узла, но я буду говорить, что это перезагрузка RSM, потому что RSM не отказывает совсем, скорее всего.
[34:16.000 --> 34:24.000]  Но он может передвигать роль лидера, и при перевыборе лидера сгорает вот это состояние.
[34:24.000 --> 34:29.000]  Так вот, этот RSM может перезагрузиться, и тогда таблица блокировок будет потеряна.
[34:29.000 --> 34:33.000]  А еще может клиент отказать. И то, и другое является проблемой.
[34:33.000 --> 34:36.000]  Давайте подумаем, как мы с этим живем.
[34:36.000 --> 34:41.000]  Во-первых, что делать, если отказал клиент? Он же, в конце концов, блокировки берет.
[34:41.000 --> 34:49.000]  И, как мы уже знаем, если клиент умирает и блокировка остается навсегда за ним, то кажется, что система остается недоступной больше.
[34:52.000 --> 34:56.000]  Как мы решаем эту проблему? Мы это обсуждали уже?
[34:57.000 --> 35:00.000]  Мы вводим понятие «сессии» для клиента.
[35:00.000 --> 35:08.000]  Клиент, когда он начинает свою транзакцию в операции StartTransaction, он приходит в TransactionManager и говорит «вот я стартую транзакцию».
[35:08.000 --> 35:15.000]  И TransactionManager заводит для него некоторую запись, что вот есть клиент с транзакцией с ID таким-то.
[35:15.000 --> 35:18.000]  И с этим клиентом ассоциируется некоторый таймер.
[35:18.000 --> 35:24.000]  И клиент, видимо, должен посылать какие-то хардбиты, чтобы эту сессию поддерживать в живом состоянии.
[35:24.000 --> 35:32.000]  Если клиент набирает блокировки, не успевает сделать коммит и отказывает, что вполне возможно, потому что от клиента мы отказаустойчивости не ожидаем,
[35:32.000 --> 35:39.000]  то на TransactionManager в какой-то момент протухает этот таймер, и он просто отзывает все блокировки и откатывает транзакцию.
[35:42.000 --> 35:46.000]  Маленькая деталь. Когда клиент что-то читает, он блокировку берет сразу.
[35:46.000 --> 35:50.000]  Когда клиент что-то пишет, он буферизирует запись у себя.
[35:50.000 --> 35:57.000]  И когда он говорит коммит, то отправляет все записи TransactionManager. TransactionManager забирает оставшиеся блокировки на запись.
[35:57.000 --> 36:08.000]  Ну и если никаких конфликтов не возникло, если никто не отменился, то фиксирует запись, то есть коммит транзакции через multipax надежно.
[36:08.000 --> 36:14.000]  Сам shard, блокировки снимает, и вот транзакция завершается.
[36:14.000 --> 36:22.000]  Сейчас, подожди, мы здесь реализуем 2PL.
[36:22.000 --> 36:28.000]  Аномалия RightSqueue, которую я писал, это аномалия, которая возникает при конкуренции транзакций.
[36:28.000 --> 36:38.000]  В 2PL их не возникает. Мы это доказали, что все расписания будут неотличимы от реализуемых для пользователя.
[36:38.000 --> 36:46.000]  И сейчас мы говорим просто про реализацию 2PL. Мы говорим, что возьмем блокировку. Я сейчас объясняю, что такое взять блокировку.
[36:46.000 --> 36:50.000]  Мы переходим к TransactionManager, и TransactionManager делает запись в Logtable.
[36:50.000 --> 36:55.000]  Ну, либо он видит, что... Ну ладно, пусть пока так. Твой вопрос теперь.
[36:55.000 --> 37:00.000]  Вы сейчас сказали, что клиент сначала буферизирует все записи, а потом отслабит их с коммитом, правильно?
[37:00.000 --> 37:06.000]  Да, но это же не важно. То есть мы просто откладываем запись на последнюю часть транзакции.
[37:06.000 --> 37:08.000]  Протокол-то от этого не меняется.
[37:11.000 --> 37:14.000]  Да блокировки можно тоже позже брать.
[37:15.000 --> 37:22.000]  Но мы просто в транзакции переупорядочиваем часть операции. Это же не влияет на происходящее никак.
[37:24.000 --> 37:26.000]  Нет, непонятно.
[37:26.000 --> 37:31.000]  Вот мы, если x равен 0, то y записать y.
[37:31.000 --> 37:35.000]  Ну вот тут происходит сначала чтение, потом запись. А может быть мы прочли, потом записали.
[37:35.000 --> 37:38.000]  Потом прочли другой ключ, потом записали третий ключ.
[37:38.000 --> 37:43.000]  Ну вот мы же можем запись в конец транзакции просто отложить. То есть просто перегруппировать нашу программу.
[37:43.000 --> 37:46.000]  От этого ее семантика не поменяется.
[37:51.000 --> 37:55.000]  То есть мы запись откладываем, но здесь есть некоторые технические нюансы.
[37:55.000 --> 37:58.000]  Наверное, он сейчас нам не очень важен. Может быть, я только тебе запутаю этим.
[37:58.000 --> 38:01.000]  Но по смыслу это все равно, это тот же самый 2PL.
[38:03.000 --> 38:06.000]  Ладно, можно же об этом забыть и считать, что вот буквально 2PL.
[38:07.000 --> 38:15.000]  Просто пока транзакция не закомитилась, транзакшн-менеджер нигде не фиксирует персистентно ее записи.
[38:17.000 --> 38:20.000]  То есть мы через мультипаксы, через аппликацию ничего пока не пишем.
[38:21.000 --> 38:25.000]  Мы просто запоминаем в лидере, что вот мы взяли какие-то блокировки.
[38:28.000 --> 38:31.000]  Вот давайте теперь подумаем, что происходит, когда клиент умирает.
[38:32.000 --> 38:38.000]  Сессия его протухает, транзакшн-менеджер отзывает блокировки, и другие транзакции могут продолжить работу.
[38:39.000 --> 38:41.000]  Никаких следов транзакция пока не оставила.
[38:42.000 --> 38:49.000]  Но с другой стороны, когда у нас есть один узел клиент, другой узел лидер, и между ними есть какие-то хардбиты,
[38:49.000 --> 38:54.000]  мы всегда можем ожидать, что кто-то зависнет, кто-то будет работать медленно, какие-то хардбиты вовремя не долетят.
[38:54.000 --> 38:58.000]  И сессия на транзакшн-менеджере протухнет, даже если клиент все еще жив.
[39:00.000 --> 39:07.000]  Но это неприятно, но это не страшно. То есть клиент интерактивный, он выполняет чтение, выполняет чтение, выполняет чтение,
[39:07.000 --> 39:11.000]  и вдруг на третьем чтении транзакшн-менеджер может говорить, что твоя сессия вообще-то протухла,
[39:11.000 --> 39:14.000]  поэтому мы считаем тебя мертвым, и все твои локи отозваны.
[39:14.000 --> 39:17.000]  Поэтому клиент должен перезапустить и попробовать снова.
[39:18.000 --> 39:23.000]  Вот в случае, когда мы говорим про распределенность, у нас возникает еще одна причина,
[39:23.000 --> 39:26.000]  по которой транзакция может откатиться и притравиться.
[39:26.000 --> 39:32.000]  Это тайм-аут сессии. Ну потому что иначе с блокировками работать нельзя, их нужно ограничивать во времени.
[39:34.000 --> 39:39.000]  А что делать, если перезагрузился РСМ, то есть если был перевыбран лидер?
[39:39.000 --> 39:45.000]  Ну опять не страшно, все эти записи сгорят, сгорят все сессии,
[39:45.000 --> 39:51.000]  поэтому просто отменятся все транзакции, которые еще не успели сгромиться и которые работали с этим шардом.
[39:54.000 --> 39:56.000]  Что скажете?
[39:56.000 --> 40:11.000]  Ну как именно организован этот слой тут?
[40:11.000 --> 40:15.000]  Даже не очень важно, нет, не обязательно большой объем данных,
[40:15.000 --> 40:19.000]  но представь, как выглядит райт-охэт лог в базе,
[40:19.000 --> 40:22.000]  ну даже в локальной базе данных.
[40:22.000 --> 40:30.000]  Вот когда ты в двухфазном блокировках делаешь какую-то запись,
[40:30.000 --> 40:36.000]  ты пишешь в райт-охэт лог запись, что вот раньше по ключу х сохранилось значение 7
[40:36.000 --> 40:43.000]  и перезаписываешь значение 8, и вот так они копятся, копятся, копятся,
[40:43.000 --> 40:47.000]  а потом ты в момент комитта транзакции добавляешь служебную запись,
[40:47.000 --> 40:50.000]  что вот транзакция с таким ID закомичена,
[40:50.000 --> 40:54.000]  и когда ты вдруг перезагружаешься, должен накатить этот лог,
[40:54.000 --> 40:57.000]  и если, ну это не то чтобы одна запись про комит транзакции,
[40:57.000 --> 41:02.000]  это много маленьких и сообщения комит.
[41:02.000 --> 41:05.000]  Ну честно говоря, я точно не знаю, как сделаны здесь,
[41:05.000 --> 41:14.000]  но это можно по-разному себе представить, но вполне можно представить каким-то таким образом.
[41:14.000 --> 41:21.000]  Вот, кажется, что мы сделали два PL поверх одного шарда,
[41:21.000 --> 41:29.000]  и мы переживаем как перезагрузку РСМ, перевыбор лидера, так и отказ клиента.
[41:29.000 --> 41:32.000]  Да?
[41:32.000 --> 41:39.000]  Тогда вот кусочек транзакции готов.
[41:39.000 --> 41:44.000]  А теперь посмотрим на случай, когда транзакция трогает ключи,
[41:44.000 --> 41:48.000]  принадлежащие обслуживаемые разными шардами.
[41:48.000 --> 41:53.000]  Вот утвердается, что сложность распределенных транзакций как раз в таком сценарии,
[41:53.000 --> 41:55.000]  который называется кроссшардовой транзакции.
[41:55.000 --> 42:03.000]  Вот здесь действительно просто два PL плюс сессии, плюс сгорание всех этих сессий при рестарте РСМ.
[42:03.000 --> 42:06.000]  Вот в кроссшардовых транзакциях появляется новая проблема,
[42:06.000 --> 42:10.000]  которая и составляет всю сложность распределенных транзакций.
[42:10.000 --> 42:12.000]  Вот ровно ради этого мы сегодня и собрались.
[42:12.000 --> 42:19.000]  Давайте подумаем, в чем разница.
[42:19.000 --> 42:21.000]  Давайте еще раз повторю, что происходит.
[42:21.000 --> 42:24.000]  Мы, клиент, когда мы стартуем транзакцию, мы приходим в Transaction Manager,
[42:24.000 --> 42:29.000]  регистрируемся в нем со своим идентификатором, идентификатором своей транзакции.
[42:29.000 --> 42:35.000]  Открывается сессия, и мы в рамках этой сессии читаем, пишем, берем блокировки здесь,
[42:35.000 --> 42:42.000]  и в момент комитта транзакции мы все свои изменения сбрасываем в РСМ, в таблет, надежно их фиксируем.
[42:42.000 --> 42:45.000]  Что может пойти не так?
[42:45.000 --> 42:48.000]  Может протухнуть сессия у клиента по непонятным причинам.
[42:48.000 --> 42:52.000]  Просто у клиента началась сборка мусора, потому что он на джаве написан.
[42:52.000 --> 42:58.000]  Потому что перевыбран лидер, или просто на уровне 2PL произошел конфликт.
[42:58.000 --> 43:01.000]  Мы сказали, что для того, чтобы 2PL блокировок дедлоков избегал,
[43:01.000 --> 43:05.000]  нам нужно периодически откатывать транзакции даже против их воли.
[43:05.000 --> 43:08.000]  Ну вот три причины, которые могут привести к откату транзакций,
[43:08.000 --> 43:15.000]  но вроде бы ни одна из этих причин не смертельна.
[43:15.000 --> 43:26.000]  А теперь мы работаем не с одним шардом, а с несколькими шардами.
[43:26.000 --> 43:34.000]  Да, вот беда именно на стадии комита.
[43:34.000 --> 43:39.000]  Мы что-то читали, читали, читали, на разных шардах брали блокировки на чтение,
[43:39.000 --> 43:44.000]  копили записи, мы клиенты, и решаем транзакцию закомитить.
[43:44.000 --> 43:47.000]  Вот теперь в комите транзакции участвуют разные шарты,
[43:47.000 --> 43:51.000]  и мы должны каждому отправить сообщение комита своими записями.
[43:51.000 --> 43:56.000]  И каждый шард, независимо от других, получается, принимает решение,
[43:56.000 --> 43:58.000]  его этот комит устраивает или нет.
[43:58.000 --> 44:01.000]  Может быть, ему прилетели записи, может быть, шарду прилетели записи,
[44:01.000 --> 44:04.000]  которые по блокировкам конфликтуют с другими.
[44:04.000 --> 44:07.000]  Может быть, сессия на этом шарде протухла.
[44:07.000 --> 44:09.000]  Может быть, шард за это время перезагрузился.
[44:09.000 --> 44:13.000]  И вот все это могло происходить независимо на разных шардах.
[44:13.000 --> 44:17.000]  В итоге у нас могут быть, у нас могут расходиться мнения относительно того,
[44:17.000 --> 44:19.000]  нужно транзакцию комитить или не нужно.
[44:19.000 --> 44:24.000]  Один шард голосует за то, что он готов, точнее, один шард ее комитит успешно
[44:24.000 --> 44:27.000]  и фиксирует изменения вот здесь, на уровне RSM,
[44:27.000 --> 44:30.000]  а другой шард отказывается, потому что протух тайм-аут.
[44:30.000 --> 44:33.000]  И в итоге мы теряем атомарность транзакций.
[44:33.000 --> 44:37.000]  Она применилась частично.
[44:37.000 --> 44:42.000]  Вот эта задача, которая здесь естественным образом возникла
[44:42.000 --> 44:47.000]  в случае кроссшардовых транзакций, называется задачей атомарного комита.
[44:47.000 --> 44:51.000]  И формулируется она в общем случае следующим образом.
[45:01.000 --> 45:02.000]  Получилось.
[45:17.000 --> 45:25.000]  Пусть у нас есть какие-то ресурс-менеджеры, чтобы это не значило.
[45:25.000 --> 45:28.000]  И есть некоторые координаторы транзакции.
[45:28.000 --> 45:32.000]  Вот координатор транзакции хочет, чтобы его изменение,
[45:32.000 --> 45:35.000]  его транзакция применилась на всех ресурс-менеджерах.
[45:35.000 --> 45:39.000]  И каждый ресурс-менеджер независимо от других принимает локальное решение
[45:39.000 --> 45:41.000]  о том, устраивает его транзакция или нет.
[45:41.000 --> 45:43.000]  Готов он ее за комиссию или нет.
[45:43.000 --> 45:49.000]  Задача стоит в том, чтобы либо все ресурс-менеджеры применили транзакцию,
[45:49.000 --> 45:52.000]  если все не согласны, либо, если хотя бы один не согласен,
[45:52.000 --> 45:55.000]  чтобы транзакция откатилась на всех ресурс-менеджерах.
[45:55.000 --> 46:02.000]  Это просто такая каноническая постановка задач еще из прошлого тысячеретия.
[46:02.000 --> 46:07.000]  В нашем случае ресурс-менеджеры это отдельные шарды спаннера.
[46:07.000 --> 46:11.000]  Как же добиться атомарного коммита?
[46:11.000 --> 46:16.000]  В случае одношардовых транзакций у нас был коммит то, что называется однофазный.
[46:16.000 --> 46:19.000]  Вы просто отправили коммит, и он был либо успешен,
[46:19.000 --> 46:21.000]  либо шард нам отвечал, что он не успешен,
[46:21.000 --> 46:23.000]  тогда транзакция откатывалась целиком.
[46:23.000 --> 46:25.000]  Этого было достаточно для атомарности.
[46:25.000 --> 46:28.000]  Сейчас мы так делать не можем в одну фазу,
[46:28.000 --> 46:31.000]  просто отправив команду коммит.
[46:31.000 --> 46:33.000]  Что же нужно сделать?
[46:33.000 --> 46:35.000]  Видимо, нужно сделать две фазы.
[46:35.000 --> 46:43.000]  Мы говорим сейчас про двухфазный коммит.
[46:43.000 --> 46:48.000]  Это такой канонический способ сделать распределенные транзакции.
[46:48.000 --> 46:55.000]  У нас есть узел-координатор,
[46:55.000 --> 47:06.000]  и есть ресурс-менеджеры.
[47:06.000 --> 47:11.000]  Каждый из них принимает независимое решение о коммите или откате транзакции.
[47:11.000 --> 47:13.000]  Мы не знаем, что он выберет,
[47:13.000 --> 47:18.000]  поэтому мы сначала отправляем его в коммит.
[47:19.000 --> 47:22.000]  Это первая фаза.
[47:22.000 --> 47:26.000]  Мы просим ресурс-менеджера подготовить транзакцию к коммиту.
[47:26.000 --> 47:28.000]  То есть либо откати ее,
[47:28.000 --> 47:33.000]  либо пообещай нам, что ты ее гарантированно применишь.
[47:33.000 --> 47:36.000]  И мы можем сделать это.
[47:36.000 --> 47:38.000]  И мы можем сделать это.
[47:38.000 --> 47:40.000]  И мы можем сделать это.
[47:40.000 --> 47:42.000]  И мы можем сделать это.
[47:42.000 --> 47:49.000]  И либо пообещай нам, что ты ее гарантированно применишь.
[47:49.000 --> 47:52.000]  Вот если нам ресурс-менеджер пообещал,
[47:52.000 --> 47:54.000]  что он транзакцию применит,
[47:54.000 --> 47:58.000]  он больше не вправе от этого решения отказаться.
[47:58.000 --> 48:00.000]  То есть если мы говорим про шарт-спандере,
[48:00.000 --> 48:02.000]  мы отправляем ему препэр и просим.
[48:02.000 --> 48:06.000]  Шарт, если тебе транзакция устраивает,
[48:06.000 --> 48:10.000]  то ты ни в коем случае не должен про нее больше забыть.
[48:10.000 --> 48:14.000]  И вот мы собираем ответы от всех ресурс-менеджеров,
[48:14.000 --> 48:17.000]  которые затрагивают транзакции, от всех шардов.
[48:17.000 --> 48:21.000]  И если каждый шарт согласен за комитет транзакции,
[48:21.000 --> 48:23.000]  он ее подготовил,
[48:23.000 --> 48:25.000]  то мы принимаем решение комитета транзакции
[48:25.000 --> 48:29.000]  и посылаем его всем ресурс-менеджерам.
[48:29.000 --> 48:32.000]  Если хотя бы один шарт отказал,
[48:32.000 --> 48:35.000]  то мы посылаем команду Abort.
[48:40.000 --> 48:45.000]  Вот такая общая схема.
[48:45.000 --> 48:48.000]  И обычно начинают рассуждать про отказоустойчивость.
[48:48.000 --> 48:52.000]  У нас ресурс-менеджеры это шарды спандера.
[48:52.000 --> 48:54.000]  Это отказоустойчивые сущности.
[48:54.000 --> 48:56.000]  Они могут перезагружаться логически,
[48:56.000 --> 48:58.000]  перевыбирая мастера.
[48:58.000 --> 49:00.000]  Но если они пообещали клиенту,
[49:00.000 --> 49:02.000]  что они транзакцию зафиксируют,
[49:02.000 --> 49:04.000]  то они этого уже забыть не могут.
[49:04.000 --> 49:07.000]  Поэтому когда я говорю препэр шарду спандера,
[49:07.000 --> 49:11.000]  то давайте нарисуем уже
[49:11.000 --> 49:14.000]  двухфазный комит в спандере.
[49:14.000 --> 49:17.000]  В двухфазном комите координатором является
[49:22.000 --> 49:24.000]  клиент.
[49:30.000 --> 49:34.000]  Есть data-shart1, data-shart2.
[49:37.000 --> 49:40.000]  Маловато. Давайте больше.
[50:00.000 --> 50:02.000]  Мы отправляем препэр.
[50:02.000 --> 50:06.000]  Каждый шарт получает препэр для транзакции.
[50:06.000 --> 50:08.000]  Если сессия не протухла,
[50:08.000 --> 50:11.000]  если конфликтов с локами никаких нет,
[50:11.000 --> 50:14.000]  если лидер все еще тот же и у него есть
[50:14.000 --> 50:16.000]  собственно запись о сессии,
[50:16.000 --> 50:20.000]  то шарт, получая препэр, надежно сохраняет
[50:20.000 --> 50:22.000]  все взятые блокировки
[50:22.000 --> 50:24.000]  в персидентное хранилище.
[50:24.000 --> 50:26.000]  То есть блокировки для транзакций,
[50:26.000 --> 50:28.000]  которые подготовлены шардом, находятся уже
[50:28.000 --> 50:30.000]  не только в памяти лидера,
[50:30.000 --> 50:32.000]  в оперативной памяти.
[50:32.000 --> 50:34.000]  Они еще сброшены в пакса,
[50:34.000 --> 50:36.000]  где они уже ногами находятся.
[50:42.000 --> 50:44.000]  Мы получаем ответы от каждого шарда
[50:44.000 --> 50:46.000]  и посылаем комит.
[50:49.000 --> 50:51.000]  Если у нас все устраивает.
[50:55.000 --> 50:57.000]  Если хотя бы один шарт отказался,
[50:57.000 --> 50:59.000]  потому что там протухла сессия,
[50:59.000 --> 51:01.000]  то нужно уведомить остальные шарды,
[51:01.000 --> 51:03.000]  что транзакция за комитство
[51:04.000 --> 51:06.000]  нужно откатить, то есть отпустить
[51:06.000 --> 51:09.000]  все взятые надежный персидент на блокировке.
[51:12.000 --> 51:14.000]  Почему это не работает?
[51:15.000 --> 51:17.000]  Каждый шарт,
[51:19.000 --> 51:21.000]  все нужно говорить про то,
[51:21.000 --> 51:23.000]  что случается при отказах и здесь, и здесь.
[51:23.000 --> 51:26.000]  Если шарт еще не успел получать препэр
[51:26.000 --> 51:28.000]  и перезагрузился или тайм-аут истек,
[51:28.000 --> 51:30.000]  то ничего страшного.
[51:30.000 --> 51:32.000]  Он ответит на препэр отказом
[51:32.000 --> 51:34.000]  и клиент отменит транзакцию
[51:34.000 --> 51:36.000]  на других шардах.
[51:36.000 --> 51:38.000]  Если шарт прошел через препэр,
[51:38.000 --> 51:40.000]  то перезагрузка уже не имеет значения,
[51:40.000 --> 51:42.000]  потому что все данные надежно
[51:42.000 --> 51:44.000]  сохранены в мультипаксос
[51:44.000 --> 51:46.000]  и при смене ридера они восстановятся.
[51:46.000 --> 51:48.000]  Блокировки мы не забудем уже.
[51:50.000 --> 51:52.000]  Но вот беда, если клиент пропадет.
[51:52.000 --> 51:54.000]  Потому что если какой-то шарт
[51:54.000 --> 51:56.000]  пообещал клиенту,
[51:56.000 --> 51:58.000]  что он подготовил транзакцию,
[51:58.000 --> 52:00.000]  что он надежно сохранил все локи этого клиента
[52:00.000 --> 52:02.000]  и не потеряет их уже и не забудет
[52:02.000 --> 52:04.000]  никакому тайм-ауту.
[52:04.000 --> 52:06.000]  А клиент после этого отказал
[52:06.000 --> 52:08.000]  и не отправил шарду сообщения
[52:08.000 --> 52:10.000]  ни коммит, ни аборт,
[52:10.000 --> 52:12.000]  то локи остались висеть навечно.
[52:14.000 --> 52:16.000]  И тут уже никаким тайм-аутом
[52:16.000 --> 52:18.000]  проблема не решается,
[52:18.000 --> 52:20.000]  потому что вот до того, как ответить на препэр,
[52:20.000 --> 52:22.000]  шарт имел право принять локальные решения
[52:22.000 --> 52:24.000]  о коммите транзакции.
[52:24.000 --> 52:26.000]  Он мог ее отменить один.
[52:26.000 --> 52:28.000]  После препэра он отменить ее не может,
[52:28.000 --> 52:30.000]  потому что не понимает, на что решились другие шарды.
[52:32.000 --> 52:34.000]  Как же это починить?
[52:38.000 --> 52:40.000]  Вот мы хотим избавиться в этой схеме
[52:40.000 --> 52:42.000]  от, это вообще фундаментальная проблема
[52:42.000 --> 52:44.000]  двухвазного коммита,
[52:44.000 --> 52:46.000]  а именно блокировка при отказе координатора.
[52:46.000 --> 52:48.000]  У лэмпорта есть целая статья
[52:48.000 --> 52:50.000]  про то, как сделать это все,
[52:50.000 --> 52:52.000]  как это все скреить с паксосом.
[52:54.000 --> 52:56.000]  Мы про это не будем говорить,
[52:56.000 --> 52:58.000]  но мы можем сделать координатор
[52:58.000 --> 53:00.000]  отказоустойчивым очень легко,
[53:00.000 --> 53:02.000]  просто перенеся его внутри системы,
[53:02.000 --> 53:04.000]  потому что внутри системы у нас уже есть отказоустойчивые существа,
[53:04.000 --> 53:06.000]  это сами шарды.
[53:06.000 --> 53:08.000]  Поэтому что мы сделаем?
[53:18.000 --> 53:20.000]  Клиент, когда он
[53:20.000 --> 53:22.000]  переходит к фазе коммита
[53:22.000 --> 53:24.000]  своей кросс-шардовой транзакции,
[53:24.000 --> 53:26.000]  выбирает среди
[53:26.000 --> 53:28.000]  шардов, которые
[53:28.000 --> 53:30.000]  он трогает
[53:30.000 --> 53:32.000]  главный координатор.
[53:32.000 --> 53:34.000]  И посылает
[53:34.000 --> 53:36.000]  каждому шарду
[53:36.000 --> 53:38.000]  по-прежнему команду Prepr.
[53:44.000 --> 53:46.000]  Но плюс к этому сообщает, что
[53:46.000 --> 53:48.000]  координатором транзакции
[53:48.000 --> 53:50.000]  является, скажем, S1.
[53:50.000 --> 53:52.000]  И каждый шард,
[53:52.000 --> 53:54.000]  подготовив транзакцию,
[53:54.000 --> 53:56.000]  отвечает о том,
[53:56.000 --> 53:58.000]  что он подготовил ее
[53:58.000 --> 54:00.000]  не к клиенту уже,
[54:00.000 --> 54:02.000]  а вот этому координатору.
[54:06.000 --> 54:08.000]  Вот здесь вот происходят
[54:10.000 --> 54:12.000]  записи
[54:14.000 --> 54:16.000]  в мультипаксос.
[54:16.000 --> 54:18.000]  В мультипаксос.
[54:20.000 --> 54:22.000]  Это позже нам понадобится.
[54:22.000 --> 54:24.000]  Координатор,
[54:24.000 --> 54:26.000]  шард номер один,
[54:26.000 --> 54:28.000]  собрав все Prepr
[54:28.000 --> 54:30.000]  успешные, ну либо получив
[54:30.000 --> 54:32.000]  хотя бы один неуспешный, принимает решение
[54:32.000 --> 54:34.000]  о комите и работе
[54:34.000 --> 54:36.000]  об откате транзакции.
[54:38.000 --> 54:40.000]  Но важно, у нас же шард очень
[54:40.000 --> 54:42.000]  отказоустойчивый, но он умеет перезагружаться.
[54:42.000 --> 54:44.000]  И нужно, чтобы он после
[54:44.000 --> 54:46.000]  перезагрузки ни в коем случае не забыл принятое решение.
[54:48.000 --> 54:50.000]  Поэтому он сначала принимает решение,
[54:50.000 --> 54:52.000]  пишет его
[54:52.000 --> 54:54.000]  еще раз в свой мультипаксос
[54:56.000 --> 54:58.000]  и уже после этого
[54:58.000 --> 55:00.000]  отправляет
[55:00.000 --> 55:02.000]  другим шардам, которые участвовали в транзакции
[55:02.000 --> 55:04.000]  результат,
[55:04.000 --> 55:06.000]  финальную команду
[55:06.000 --> 55:08.000]  commit или abort.
[55:14.000 --> 55:16.000]  Это уже
[55:16.000 --> 55:18.000]  почти окончательный вариант.
[55:18.000 --> 55:20.000]  Нужно лишь подумать
[55:20.000 --> 55:22.000]  о кое-каких деталях.
[55:22.000 --> 55:24.000]  Вот скажем,
[55:24.000 --> 55:26.000]  кто здесь у нас может отказывать?
[55:26.000 --> 55:28.000]  Целиком отказывать, разве что клиент может.
[55:28.000 --> 55:30.000]  И вот будет ли страшно,
[55:30.000 --> 55:32.000]  если клиент успел
[55:32.000 --> 55:34.000]  отправить Prepr
[55:34.000 --> 55:36.000]  скажем, на шард
[55:36.000 --> 55:38.000]  на какие-то шарды,
[55:38.000 --> 55:40.000]  а на какие-то не успел?
[55:40.000 --> 55:42.000]  Где все это
[55:42.000 --> 55:44.000]  восстановится?
[55:44.000 --> 55:46.000]  Кто отменит транзакцию?
[55:46.000 --> 55:48.000]  Если хотя бы
[55:48.000 --> 55:50.000]  ни один шард не получил
[55:50.000 --> 55:52.000]  Prepr, значит транзакция
[55:52.000 --> 55:54.000]  не закомитилась вообще, все протухнет
[55:54.000 --> 55:56.000]  и отменится самой собой.
[55:56.000 --> 55:58.000]  Если хотя бы
[55:58.000 --> 56:00.000]  один шард получил Prepr
[56:00.000 --> 56:02.000]  и принял решение,
[56:02.000 --> 56:04.000]  что транзакция должна быть
[56:04.000 --> 56:06.000]  на какие-то шарды,
[56:06.000 --> 56:08.000]  а на какие-то не успел,
[56:08.000 --> 56:10.000]  и принял решение, что транзакция
[56:10.000 --> 56:12.000]  должна быть подготовлена и зафиксировала
[56:12.000 --> 56:14.000]  ее надежно в своем мультипаксисе,
[56:14.000 --> 56:16.000]  то он
[56:16.000 --> 56:18.000]  об этом напишет S1
[56:18.000 --> 56:20.000]  координатору,
[56:20.000 --> 56:22.000]  координатор это запомнит,
[56:22.000 --> 56:24.000]  и он будет знать,
[56:24.000 --> 56:26.000]  что в транзакции участвует
[56:26.000 --> 56:28.000]  еще, скажем, два шарда.
[56:28.000 --> 56:30.000]  И если он от них не дождется решения
[56:30.000 --> 56:32.000]  за некоторое время,
[56:32.000 --> 56:34.000]  то он в праве уже принять сам
[56:34.000 --> 56:36.000]  локально любое решение.
[56:36.000 --> 56:38.000]  Он может выбрать комит, только если он получил
[56:38.000 --> 56:40.000]  все положительные ответы,
[56:40.000 --> 56:42.000]  а если он получил хотя бы один
[56:42.000 --> 56:44.000]  отрицательный ответ или просто таймаут,
[56:44.000 --> 56:46.000]  то он может консервативно транзакцию все отменить.
[56:46.000 --> 56:48.000]  Это всегда безопасно.
[56:48.000 --> 56:50.000]  Так что
[56:50.000 --> 56:52.000]  смерть клиента
[56:52.000 --> 56:54.000]  вот где-то на этом участке
[56:54.000 --> 56:56.000]  она ни на что не влияет,
[56:56.000 --> 56:58.000]  и против рестартов
[56:58.000 --> 57:00.000]  РСМ мы боремся тем, что мы пишем
[57:00.000 --> 57:02.000]  надежно в хранилище,
[57:02.000 --> 57:04.000]  пишем мультипаксис.
[57:04.000 --> 57:06.000]  Вот тут происходит
[57:06.000 --> 57:08.000]  в комите транзакции
[57:08.000 --> 57:12.000]  три последовательные записи.
[57:14.000 --> 57:16.000]  Сначала мы надежно сохраняем блокировки,
[57:16.000 --> 57:18.000]  потом мы надежно параллельно
[57:18.000 --> 57:20.000]  на разных шардах, потом мы на координаторе
[57:20.000 --> 57:22.000]  надежно сохраняем решение комит транзакции
[57:22.000 --> 57:24.000]  и потом мы параллельно
[57:24.000 --> 57:26.000]  пишем данные
[57:26.000 --> 57:28.000]  в само хранилище, которое мы реплицируем.
[57:34.000 --> 57:36.000]  Ну конечно,
[57:36.000 --> 57:38.000]  если координатор,
[57:38.000 --> 57:40.000]  там можно ответить, всегда клиент.
[58:04.000 --> 58:06.000]  Ну если клиент
[58:06.000 --> 58:08.000]  прям отказал
[58:08.000 --> 58:10.000]  и если его
[58:10.000 --> 58:12.000]  так настолько, ему настолько
[58:12.000 --> 58:14.000]  важен результат транзакции,
[58:14.000 --> 58:16.000]  наверное это его собственная забота.
[58:16.000 --> 58:18.000]  То есть можно
[58:20.000 --> 58:22.000]  если он не готов
[58:22.000 --> 58:24.000]  восстановиться от, то есть ему дают
[58:24.000 --> 58:26.000]  гарантию, что транзакция либо
[58:26.000 --> 58:28.000]  целиком накатывается, либо целиком
[58:28.000 --> 58:30.000]  откатывается, и что она
[58:30.000 --> 58:32.000]  серилизуется относительно других транзакций,
[58:32.000 --> 58:34.000]  то это то, что может сделать система.
[58:34.000 --> 58:36.000]  А если клиенту важно понимать,
[58:36.000 --> 58:38.000]  все-таки успел он сделать что-то или нет,
[58:38.000 --> 58:40.000]  когда он умер, то это его забота.
[58:40.000 --> 58:42.000]  Мне кажется, он на уровне транзакции может это сделать.
[58:42.000 --> 58:44.000]  Я не знаю, скажем,
[58:44.000 --> 58:46.000]  это не то чтобы про транзакции,
[58:46.000 --> 58:48.000]  речь про базы данных,
[58:48.000 --> 58:50.000]  но вот в ZooKeeper у тебя есть
[58:50.000 --> 58:52.000]  апдейт, где ты фактически
[58:52.000 --> 58:54.000]  выполняешь операцию CAS. Если у тебя
[58:54.000 --> 58:56.000]  текущая версия какого-то узла
[58:56.000 --> 58:58.000]  равна чему-то, то я перезаписываю ее на
[58:58.000 --> 59:00.000]  другую версию.
[59:02.000 --> 59:06.000]  Так каждый
[59:06.000 --> 59:08.000]  препер,
[59:08.000 --> 59:10.000]  нет, конечно, может, но просто
[59:10.000 --> 59:12.000]  если shard2 получил препер,
[59:12.000 --> 59:14.000]  он же получил еще и указание,
[59:14.000 --> 59:16.000]  кто является координатором, поэтому он
[59:16.000 --> 59:18.000]  сообщит координатору, поэтому координатор
[59:18.000 --> 59:20.000]  о транзакции узнает.
[59:20.000 --> 59:22.000]  Но если он сам не получил сообщение
[59:22.000 --> 59:24.000]  от клиента, он не знает свою порцию
[59:24.000 --> 59:26.000]  транзакции, то есть свои записи,
[59:26.000 --> 59:28.000]  то просто
[59:28.000 --> 59:30.000]  транзакцию вправе отклинить, потому что
[59:30.000 --> 59:32.000]  он голосует против.
[59:34.000 --> 59:36.000]  Голосовать против всегда
[59:36.000 --> 59:38.000]  легально до тех пор, пока ты
[59:38.000 --> 59:40.000]  получил препер. Вот как только ты его получил
[59:40.000 --> 59:42.000]  и согласился, все, ты
[59:42.000 --> 59:44.000]  закомитился, что транзакция,
[59:44.000 --> 59:46.000]  что ты транзакцию по своей воле
[59:46.000 --> 59:48.000]  уже не отменишь. Если ты не получил препер,
[59:48.000 --> 59:50.000]  значит ты можешь говорить что угодно,
[59:50.000 --> 59:52.000]  ну, говорить нет.
[01:00:00.000 --> 01:00:02.000]  Ну что, двухфазный комит,
[01:00:02.000 --> 01:00:04.000]  разобрались? Вот, пожалуйста,
[01:00:04.000 --> 01:00:06.000]  я понимаю, что здесь
[01:00:06.000 --> 01:00:08.000]  есть очень похожие
[01:00:08.000 --> 01:00:10.000]  названия,
[01:00:10.000 --> 01:00:12.000]  двухфазный комит, двухфазные блокировки,
[01:00:12.000 --> 01:00:14.000]  но это же совершенно
[01:00:14.000 --> 01:00:16.000]  проразное.
[01:00:16.000 --> 01:00:18.000]  Вот двухфазные блокировки,
[01:00:18.000 --> 01:00:20.000]  вот Spanner, он использует
[01:00:20.000 --> 01:00:22.000]  двухфазные блокировки
[01:00:22.000 --> 01:00:24.000]  для изоляции транзакций,
[01:00:24.000 --> 01:00:26.000]  то есть мы здесь говорим про конкуренцию,
[01:00:26.000 --> 01:00:28.000]  а двухфазный комит,
[01:00:28.000 --> 01:00:30.000]  то есть это про много транзакций,
[01:00:30.000 --> 01:00:32.000]  которые одновременно в системе работают,
[01:00:32.000 --> 01:00:34.000]  а двухфазный комит,
[01:00:34.000 --> 01:00:36.000]  он про отказоустойчивость и он
[01:00:36.000 --> 01:00:38.000]  про одну отдельную транзакцию.
[01:00:38.000 --> 01:00:40.000]  То есть это вот разные
[01:00:40.000 --> 01:00:42.000]  проблемы,
[01:00:42.000 --> 01:00:44.000]  тоже две фазы, у нас везде две фазы просто в курсе,
[01:00:44.000 --> 01:00:46.000]  и это некоторая фундаментальная
[01:00:46.000 --> 01:00:48.000]  особенность всего происходящего, но
[01:00:48.000 --> 01:00:50.000]  вот здесь задачи разные, хоть
[01:00:50.000 --> 01:00:52.000]  решения называются похожим образом,
[01:00:52.000 --> 01:00:54.000]  задачи разные,
[01:00:54.000 --> 01:00:56.000]  одно про отказоустойчивость, другое про
[01:00:56.000 --> 01:00:58.000]  конкуренцию, про изоляцию.
[01:00:58.000 --> 01:01:00.000]  Вот, пожалуйста,
[01:01:00.000 --> 01:01:02.000]  не путайтесь.
[01:01:06.000 --> 01:01:08.000]  Специфика
[01:01:08.000 --> 01:01:10.000]  двухфазного комита в Spanner
[01:01:10.000 --> 01:01:12.000]  пока довольно несущественная,
[01:01:12.000 --> 01:01:14.000]  мы перенесли картинатор внутри системы,
[01:01:14.000 --> 01:01:16.000]  сказали, что один из
[01:01:16.000 --> 01:01:18.000]  шардов системы является,
[01:01:18.000 --> 01:01:20.000]  в Spanner такой шард-участник
[01:01:20.000 --> 01:01:22.000]  называется Participant.
[01:01:22.000 --> 01:01:24.000]  Где бы это написать здесь?
[01:01:34.000 --> 01:01:36.000]  А шард-координатор
[01:01:38.000 --> 01:01:40.000]  называется Participant-Leader.
[01:01:46.000 --> 01:01:48.000]  Вот если получится
[01:01:48.000 --> 01:01:50.000]  быстро вернуться на
[01:01:52.000 --> 01:01:54.000]  статью, на экран,
[01:01:54.000 --> 01:01:56.000]  получится ведь?
[01:01:58.000 --> 01:02:00.000]  Давайте я так покажу, потому что лень.
[01:02:00.000 --> 01:02:02.000]  Participant-Leader это как раз
[01:02:02.000 --> 01:02:04.000]  роль, что этот шард является координатором
[01:02:04.000 --> 01:02:06.000]  транзакции, он отвечает за
[01:02:06.000 --> 01:02:08.000]  глобальный, за атомарный комит.
[01:02:08.000 --> 01:02:10.000]  Вот есть и другие, вот с ними
[01:02:10.000 --> 01:02:12.000]  мы общаемся.
[01:02:12.000 --> 01:02:14.000]  Итак, значит,
[01:02:14.000 --> 01:02:16.000]  эта конструкция вроде бы
[01:02:16.000 --> 01:02:18.000]  полностью разобрана,
[01:02:18.000 --> 01:02:20.000]  и мы готовы перейти к
[01:02:20.000 --> 01:02:22.000]  следующему шагу реализации
[01:02:22.000 --> 01:02:24.000]  транзакции в Spanner,
[01:02:24.000 --> 01:02:26.000]  а именно мы хотим оптимизировать
[01:02:30.000 --> 01:02:32.000]  чтение. Мы хотим оптимизировать
[01:02:32.000 --> 01:02:34.000]  транзакции, которые только читают.
[01:02:36.000 --> 01:02:38.000]  Да, Spanner использует двухфазные
[01:02:38.000 --> 01:02:40.000]  блокировки для того, чтобы
[01:02:40.000 --> 01:02:42.000]  реализовать транзакции, которые
[01:02:42.000 --> 01:02:44.000]  и читают, и пишут.
[01:02:44.000 --> 01:02:46.000]  Но вот если транзакция только читает,
[01:02:46.000 --> 01:02:48.000]  то Spanner хочет действовать эффективнее.
[01:02:48.000 --> 01:02:50.000]  Мы видели, что для
[01:02:50.000 --> 01:02:52.000]  редон или транзакций
[01:02:52.000 --> 01:02:54.000]  блокировки в изоляции
[01:02:54.000 --> 01:02:56.000]  снапшотов вообще не требуется.
[01:02:56.000 --> 01:02:58.000]  Вот забудем пока про двухфазные блокировки,
[01:02:58.000 --> 01:03:00.000]  вспомним про изоляцию снапшотов.
[01:03:00.000 --> 01:03:02.000]  Здесь для транзакций, которые
[01:03:02.000 --> 01:03:04.000]  читают, никакие блокировки,
[01:03:04.000 --> 01:03:06.000]  никакие конфликты, все это не требовалось.
[01:03:06.000 --> 01:03:08.000]  Они просто запускаются, получают себе временную метку
[01:03:08.000 --> 01:03:10.000]  и читают свою версию
[01:03:10.000 --> 01:03:12.000]  хранилища мультиверсионного.
[01:03:14.000 --> 01:03:16.000]  С одной стороны, им хорошо,
[01:03:16.000 --> 01:03:18.000]  а с другой стороны они
[01:03:18.000 --> 01:03:20.000]  не читают, они читают что-то сомнительное,
[01:03:20.000 --> 01:03:22.000]  потому что вот вся эта
[01:03:22.000 --> 01:03:24.000]  история,
[01:03:24.000 --> 01:03:26.000]  все эти версии хранилища,
[01:03:26.000 --> 01:03:28.000]  это не сериализация транзакций.
[01:03:28.000 --> 01:03:30.000]  То есть мы можем породить версии,
[01:03:30.000 --> 01:03:32.000]  которые не объясняются
[01:03:32.000 --> 01:03:34.000]  никакой сериализацией.
[01:03:34.000 --> 01:03:36.000]  То есть, чтение без блокировок
[01:03:36.000 --> 01:03:38.000]  из мультиверсионного хранилища,
[01:03:38.000 --> 01:03:40.000]  это хорошо, а вот
[01:03:40.000 --> 01:03:42.000]  First Committer Wins и
[01:03:42.000 --> 01:03:44.000]  поиск конфликтов только
[01:03:44.000 --> 01:03:46.000]  про айцетом, это путь к
[01:03:46.000 --> 01:03:48.000]  аномалии.
[01:03:48.000 --> 01:03:50.000]  Так вот, что решает Spanner?
[01:03:50.000 --> 01:03:52.000]  Что можно совместить два подхода.
[01:03:52.000 --> 01:03:54.000]  Можно взять двухфазные блокировки
[01:03:56.000 --> 01:03:58.000]  для транзакций, которые
[01:03:58.000 --> 01:04:00.000]  только читают,
[01:04:00.000 --> 01:04:02.000]  которые и читают,
[01:04:02.000 --> 01:04:04.000]  и пишут. Но если транзакция
[01:04:04.000 --> 01:04:06.000]  только читает,
[01:04:06.000 --> 01:04:08.000]  давайте я что-то уничтожу.
[01:04:14.000 --> 01:04:16.000]  Для транзакций,
[01:04:16.000 --> 01:04:18.000]  которые и читают, и пишут,
[01:04:18.000 --> 01:04:20.000]  мы будем использовать 2PL.
[01:04:20.000 --> 01:04:22.000]  А вот для транзакций,
[01:04:22.000 --> 01:04:24.000]  которые только читают,
[01:04:24.000 --> 01:04:26.000]  мы будем использовать изоляцию
[01:04:26.000 --> 01:04:28.000]  снапшотов.
[01:04:30.000 --> 01:04:32.000]  Что я имею под этим в виду?
[01:04:32.000 --> 01:04:34.000]  Потому что пока непонятно. То есть нельзя просто взять
[01:04:34.000 --> 01:04:36.000]  два подхода и сказать, что я хочу получить
[01:04:36.000 --> 01:04:38.000]  сильные стороны каждого
[01:04:38.000 --> 01:04:40.000]  и игнорировать слабости.
[01:04:40.000 --> 01:04:42.000]  То есть мы можем
[01:04:42.000 --> 01:04:44.000]  изоляцию снапшотов каждого
[01:04:44.000 --> 01:04:46.000]  и игнорировать слабости.
[01:04:48.000 --> 01:04:50.000]  Когда мы
[01:04:50.000 --> 01:04:52.000]  серилизуем порядок
[01:04:52.000 --> 01:04:54.000]  изоляции снапшотов, хорошо то, что
[01:04:54.000 --> 01:04:56.000]  чтение происходит без блокировок относительно стабильной версии
[01:04:56.000 --> 01:04:58.000]  некоторой, но плохо то, что
[01:04:58.000 --> 01:05:00.000]  история изменений
[01:05:02.000 --> 01:05:04.000]  не объясняется никакой
[01:05:04.000 --> 01:05:06.000]  серилизацией.
[01:05:06.000 --> 01:05:08.000]  С другой стороны, если мы используем 2PL,
[01:05:08.000 --> 01:05:10.000]  то внутри 2PL серилизация
[01:05:10.000 --> 01:05:12.000]  не объясняется.
[01:05:12.000 --> 01:05:14.000]  Другое дело, что она неявная.
[01:05:14.000 --> 01:05:16.000]  То есть когда мы запускаем
[01:05:16.000 --> 01:05:18.000]  этот протокол, берем там какие-то блокировки,
[01:05:18.000 --> 01:05:20.000]  то мы знаем, что для
[01:05:20.000 --> 01:05:22.000]  исполнения в 2PL
[01:05:22.000 --> 01:05:24.000]  существует некоторая
[01:05:24.000 --> 01:05:26.000]  серилизация,
[01:05:26.000 --> 01:05:28.000]  но какая именно, мы не говорим. То есть мы нигде явно
[01:05:28.000 --> 01:05:30.000]  ее не строим. Мы просто говорим, что в графе
[01:05:30.000 --> 01:05:32.000]  конфликтов для расписания, который порождается
[01:05:32.000 --> 01:05:34.000]  в 2PL, не будет циклов, поэтому
[01:05:34.000 --> 01:05:36.000]  по критерию в расписании
[01:05:36.000 --> 01:05:38.000]  расписание будет конфликтно-серилизуемо, значит
[01:05:38.000 --> 01:05:40.000]  отключимо для пользователя от серийного.
[01:05:40.000 --> 01:05:42.000]  Но конкретно расписание
[01:05:42.000 --> 01:05:44.000]  мы не знаем.
[01:05:44.000 --> 01:05:46.000]  Так вот,
[01:05:46.000 --> 01:05:48.000]  давайте мы научимся
[01:05:50.000 --> 01:05:52.000]  материализовывать вот эту серилизацию,
[01:05:52.000 --> 01:05:54.000]  которая возникает в 2PL.
[01:05:54.000 --> 01:05:56.000]  Когда мы коммитим транзакцию,
[01:05:56.000 --> 01:05:58.000]  давайте не перезаписывать ключи
[01:05:58.000 --> 01:06:00.000]  в хранилище, а
[01:06:00.000 --> 01:06:02.000]  давайте порождать новую версию
[01:06:02.000 --> 01:06:04.000]  хранилища.
[01:06:04.000 --> 01:06:06.000]  Вот мы хотим сделать хранилище
[01:06:06.000 --> 01:06:08.000]  версионным.
[01:06:10.000 --> 01:06:12.000]  Потом покажу это в статье Spanner.
[01:06:12.000 --> 01:06:14.000]  В Spanner хранилище, которое реализуется
[01:06:14.000 --> 01:06:16.000]  через шардирование, через
[01:06:16.000 --> 01:06:18.000]  multipax, через колоссусы, это на самом деле
[01:06:18.000 --> 01:06:20.000]  версионируемое хранилище. То есть мы по паре
[01:06:20.000 --> 01:06:22.000]  ключ-таймстэмп
[01:06:22.000 --> 01:06:24.000]  храним значение.
[01:06:24.000 --> 01:06:26.000]  Ровно потому, что в коммите транзакции,
[01:06:26.000 --> 01:06:28.000]  когда мы пишем данные
[01:06:28.000 --> 01:06:30.000]  в хранилище, мы хотим не
[01:06:30.000 --> 01:06:32.000]  перезаписывать, а генерировать временную
[01:06:32.000 --> 01:06:34.000]  метку и записывать данные под ней.
[01:06:36.000 --> 01:06:38.000]  Вот мы из 2PL возьмем сериализацию
[01:06:38.000 --> 01:06:40.000]  и материализуем
[01:06:40.000 --> 01:06:42.000]  ее в виде истории изменений
[01:06:42.000 --> 01:06:44.000]  хранилища.
[01:06:44.000 --> 01:06:46.000]  И таким образом,
[01:06:46.000 --> 01:06:48.000]  когда транзакция будет только читать,
[01:06:48.000 --> 01:06:50.000]  если транзакция собирается только читать,
[01:06:50.000 --> 01:06:52.000]  то она будет просто читать с хранилища
[01:06:52.000 --> 01:06:54.000]  напрямую.
[01:06:54.000 --> 01:06:56.000]  А если транзакция будет и читать, и писать,
[01:06:56.000 --> 01:06:58.000]  то пусть она использует протокол 2PL
[01:06:58.000 --> 01:07:00.000]  и сериализуется относительно других
[01:07:00.000 --> 01:07:02.000]  читающих, пишущих транзакций.
[01:07:02.000 --> 01:07:04.000]  Идея такова.
[01:07:08.000 --> 01:07:10.000]  Но
[01:07:10.000 --> 01:07:12.000]  если бы так все было просто, все бы так
[01:07:12.000 --> 01:07:14.000]  давно и сделали.
[01:07:14.000 --> 01:07:16.000]  Давайте я объясню,
[01:07:16.000 --> 01:07:18.000]  в чем проблема.
[01:07:18.000 --> 01:07:20.000]  В том, что
[01:07:20.000 --> 01:07:22.000]  для транзакций, которые собираются
[01:07:22.000 --> 01:07:24.000]  только читать,
[01:07:24.000 --> 01:07:26.000]  порядок
[01:07:26.000 --> 01:07:28.000]  записей — это порядок
[01:07:28.000 --> 01:07:30.000]  на временных метках.
[01:07:30.000 --> 01:07:32.000]  Транзакции, которые
[01:07:32.000 --> 01:07:34.000]  используют 2PL,
[01:07:34.000 --> 01:07:36.000]  упорядочиваются с помощью блокировок.
[01:07:38.000 --> 01:07:40.000]  Вот для разных транзакций
[01:07:40.000 --> 01:07:42.000]  упорядочивание выглядит по-разному.
[01:07:42.000 --> 01:07:44.000]  И, разумеется, оно должно быть согласовано.
[01:07:44.000 --> 01:07:46.000]  Вот пусть у нас есть две транзакции,
[01:07:48.000 --> 01:07:50.000]  которые конкурируют.
[01:07:52.000 --> 01:07:54.000]  И пусть они работают
[01:07:54.000 --> 01:07:56.000]  с пересекающимся набором ключей.
[01:07:56.000 --> 01:07:58.000]  Они читают и пишут,
[01:07:58.000 --> 01:08:00.000]  пересекаются по набору ключей,
[01:08:00.000 --> 01:08:02.000]  поэтому
[01:08:02.000 --> 01:08:04.000]  используют протокол 2PL.
[01:08:04.000 --> 01:08:06.000]  И это означает, что они набирают локи.
[01:08:06.000 --> 01:08:08.000]  И вот давайте я сейчас обозначу
[01:08:08.000 --> 01:08:10.000]  красным отрезке, где
[01:08:12.000 --> 01:08:14.000]  собраны все локи.
[01:08:16.000 --> 01:08:18.000]  Вот здесь
[01:08:18.000 --> 01:08:20.000]  взят последний лок,
[01:08:20.000 --> 01:08:22.000]  и здесь
[01:08:22.000 --> 01:08:24.000]  локи начинают
[01:08:24.000 --> 01:08:26.000]  отпускаться.
[01:08:26.000 --> 01:08:28.000]  Здесь взят последний лок,
[01:08:28.000 --> 01:08:30.000]  и локи начали отпускаться.
[01:08:30.000 --> 01:08:32.000]  Если транзакции конфликтуют
[01:08:32.000 --> 01:08:34.000]  по ключам,
[01:08:34.000 --> 01:08:36.000]  то они берут какие-то общие локи,
[01:08:36.000 --> 01:08:38.000]  и вот этот отрезок
[01:08:38.000 --> 01:08:40.000]  между
[01:08:40.000 --> 01:08:42.000]  взятием
[01:08:44.000 --> 01:08:46.000]  всех локов
[01:08:46.000 --> 01:08:48.000]  и отпусканием всех локов,
[01:08:48.000 --> 01:08:50.000]  вот эти отрезки
[01:08:50.000 --> 01:08:52.000]  для двух этих транзакций не пересекаются.
[01:08:52.000 --> 01:08:54.000]  Потому что есть
[01:08:54.000 --> 01:08:56.000]  какой-то общий лок, который они берут.
[01:08:56.000 --> 01:08:58.000]  То есть, если
[01:08:58.000 --> 01:09:00.000]  две транзакции используют 2PL,
[01:09:00.000 --> 01:09:02.000]  то вот они как-то упорядочились.
[01:09:02.000 --> 01:09:04.000]  Вот где-то здесь происходит
[01:09:04.000 --> 01:09:06.000]  коммит.
[01:09:06.000 --> 01:09:08.000]  Где-то здесь мы надежно
[01:09:08.000 --> 01:09:10.000]  фиксируем на диске,
[01:09:10.000 --> 01:09:12.000]  в RSA, о том, что транзакция закоммичена.
[01:09:12.000 --> 01:09:14.000]  В 2PL
[01:09:14.000 --> 01:09:16.000]  в 2PL
[01:09:16.000 --> 01:09:18.000]  в 2PL вот так вот рождается
[01:09:18.000 --> 01:09:20.000]  стерилизация.
[01:09:20.000 --> 01:09:22.000]  Мы просто нигде ее физически
[01:09:22.000 --> 01:09:24.000]  не представляем никак.
[01:09:26.000 --> 01:09:28.000]  Но теперь
[01:09:28.000 --> 01:09:30.000]  коммит – это запись
[01:09:30.000 --> 01:09:32.000]  версионируемое хранилище
[01:09:32.000 --> 01:09:34.000]  по некоторой временной меткой.
[01:09:34.000 --> 01:09:36.000]  Так вот, чего мы хотим?
[01:09:36.000 --> 01:09:38.000]  Мы хотим, чтобы если вот этот
[01:09:38.000 --> 01:09:40.000]  интервал в транзакции T1
[01:09:40.000 --> 01:09:42.000]  предшествовал этому интервалу
[01:09:42.000 --> 01:09:44.000]  в транзакции T2,
[01:09:44.000 --> 01:09:46.000]  то из этого бы следовало,
[01:09:46.000 --> 01:09:48.000]  что транзакция T1
[01:09:48.000 --> 01:09:50.000]  запишет
[01:09:50.000 --> 01:09:52.000]  свои записи
[01:09:52.000 --> 01:09:54.000]  в хранилище под временной меткой,
[01:09:54.000 --> 01:09:56.000]  которая будет обязательно меньше,
[01:09:56.000 --> 01:09:58.000]  чем временная метка, которая выберет
[01:09:58.000 --> 01:10:00.000]  для коммита транзакции T2.
[01:10:04.000 --> 01:10:06.000]  Потому что читающие транзакции
[01:10:06.000 --> 01:10:08.000]  хотят смотреть на временные метки
[01:10:08.000 --> 01:10:10.000]  и не знают ничего про блокировки,
[01:10:10.000 --> 01:10:12.000]  а 2PL работает с блокировками,
[01:10:12.000 --> 01:10:14.000]  упорядочивается через них.
[01:10:16.000 --> 01:10:18.000]  Вот такая вот задача – согласовать
[01:10:18.000 --> 01:10:20.000]  два этих порядка.
[01:10:26.000 --> 01:10:28.000]  Кажется, что мы это умеем делать, правда?
[01:10:32.000 --> 01:10:34.000]  Ровно
[01:10:34.000 --> 01:10:36.000]  ради этого момента
[01:10:36.000 --> 01:10:38.000]  мы этому и учились когда-то.
[01:10:42.000 --> 01:10:44.000]  И снова появляется экран.
[01:10:52.000 --> 01:10:54.000]  Итак, это слайды с
[01:10:54.000 --> 01:10:56.000]  самой первой презентации Spanner в 2012 году.
[01:11:00.000 --> 01:11:02.000]  Как статья, эта презентация
[01:11:02.000 --> 01:11:04.000]  целиком посвящена транзакциям.
[01:11:08.000 --> 01:11:10.000]  И давайте обсудим, как решать такую задачу.
[01:11:12.000 --> 01:11:14.000]  Кто-то поторопился, ну ладно.
[01:11:14.000 --> 01:11:16.000]  Откатывает назад лениво.
[01:11:18.000 --> 01:11:20.000]  Итак, задача – выбирать
[01:11:20.000 --> 01:11:22.000]  монотонные временные метки.
[01:11:22.000 --> 01:11:24.000]  У нас есть два интервала в двух транзакциях
[01:11:24.000 --> 01:11:26.000]  между взятием всех логов
[01:11:26.000 --> 01:11:28.000]  и отпусканием всех логов.
[01:11:28.000 --> 01:11:30.000]  И если два интервала не пересекаются во времени,
[01:11:30.000 --> 01:11:32.000]  если один предшествует другому,
[01:11:32.000 --> 01:11:34.000]  то в первом интервале в момент коммита
[01:11:34.000 --> 01:11:36.000]  должна быть выбрана временная метка
[01:11:36.000 --> 01:11:38.000]  строго меньше, чем во втором интервале.
[01:11:38.000 --> 01:11:40.000]  Как такую задачу можно решать?
[01:11:40.000 --> 01:11:42.000]  Эту задачу решают обычно.
[01:11:42.000 --> 01:11:44.000]  Обычно строят вспомогательный сервис,
[01:11:44.000 --> 01:11:46.000]  который называется Timestamp Oracle.
[01:11:46.000 --> 01:11:48.000]  Оракул временных меток.
[01:11:48.000 --> 01:11:50.000]  Оракул времени.
[01:11:50.000 --> 01:11:52.000]  Чем он занимается? К нему можно прийти
[01:11:52.000 --> 01:11:54.000]  с запроса, и он скажет, вот тебе временная метка.
[01:11:54.000 --> 01:11:56.000]  И он дает их монотонно.
[01:11:56.000 --> 01:11:58.000]  Вопрос.
[01:11:58.000 --> 01:12:00.000]  Как такого оракула построить?
[01:12:00.000 --> 01:12:02.000]  Действительно, можно взять просто RSM.
[01:12:02.000 --> 01:12:04.000]  Мы берем RSM,
[01:12:04.000 --> 01:12:06.000]  который реплицирует просто атомик
[01:12:06.000 --> 01:12:08.000]  с операцией FetchEd.
[01:12:08.000 --> 01:12:10.000]  Мы приходим к нему, говорим FetchEd,
[01:12:10.000 --> 01:12:12.000]  он увеличивает на единиц, возвращает нам значение.
[01:12:12.000 --> 01:12:14.000]  Все монотонно, отказы устойчиво.
[01:12:14.000 --> 01:12:16.000]  Ну, конечно же, если мы собираемся
[01:12:16.000 --> 01:12:18.000]  выполнять, там не знаю,
[01:12:18.000 --> 01:12:20.000]  десятки, сотен тысяч транзакций в секунду,
[01:12:20.000 --> 01:12:22.000]  то мы будем использовать
[01:12:22.000 --> 01:12:24.000]  фичет.
[01:12:24.000 --> 01:12:26.000]  Ну, конечно же, если мы собираемся
[01:12:26.000 --> 01:12:28.000]  выполнять, там не знаю,
[01:12:28.000 --> 01:12:30.000]  десятки, сотен тысяч транзакций в секунду,
[01:12:30.000 --> 01:12:32.000]  то мы не сможем прийти
[01:12:32.000 --> 01:12:34.000]  сто тысяч раз или миллион раз
[01:12:34.000 --> 01:12:36.000]  в этот мультипак со следа рафта сказать FetchEd.
[01:12:36.000 --> 01:12:38.000]  Он лопнет от количества команд.
[01:12:38.000 --> 01:12:40.000]  Поэтому
[01:12:40.000 --> 01:12:42.000]  как мы поступим на самом деле?
[01:12:42.000 --> 01:12:44.000]  Мы на месте этого timestamp
[01:12:44.000 --> 01:12:46.000]  оракула будем открывать
[01:12:46.000 --> 01:12:48.000]  каждые пять миллисекунд окно.
[01:12:48.000 --> 01:12:50.000]  И копить все запросы, которые
[01:12:50.000 --> 01:12:52.000]  к нам приходят.
[01:12:52.000 --> 01:12:54.000]  Вот ждем пять миллисекунд,
[01:12:54.000 --> 01:12:56.000]  набираем, скажем, десять тысяч
[01:12:56.000 --> 01:12:58.000]  инкрементов.
[01:12:58.000 --> 01:13:00.000]  И отправляем в мультипак
[01:13:00.000 --> 01:13:02.000]  со СВСМ команду
[01:13:02.000 --> 01:13:04.000]  FetchEd десять тысяч.
[01:13:06.000 --> 01:13:08.000]  Получаем старое значение.
[01:13:08.000 --> 01:13:10.000]  И раздаем
[01:13:10.000 --> 01:13:12.000]  десять тысяч разных ответов
[01:13:12.000 --> 01:13:14.000]  вот в интервале от полученного значения
[01:13:14.000 --> 01:13:16.000]  до полученного значения плюс число десять тысяч
[01:13:16.000 --> 01:13:18.000]  разным клиентам.
[01:13:18.000 --> 01:13:20.000]  Понятная идея?
[01:13:20.000 --> 01:13:22.000]  То есть
[01:13:22.000 --> 01:13:24.000]  количество запросов
[01:13:24.000 --> 01:13:26.000]  в этот timestamp oracle
[01:13:26.000 --> 01:13:28.000]  в принципе
[01:13:28.000 --> 01:13:30.000]  может пережить любое количество
[01:13:30.000 --> 01:13:32.000]  инкрементов.
[01:13:32.000 --> 01:13:34.000]  Просто потому что чем больше инкрементов,
[01:13:34.000 --> 01:13:36.000]  количество операций над RSM,
[01:13:36.000 --> 01:13:38.000]  количество команд в RSM не увеличивается.
[01:13:38.000 --> 01:13:40.000]  Просто растет
[01:13:40.000 --> 01:13:42.000]  значение FetchEd.
[01:13:46.000 --> 01:13:48.000]  Хорошо.
[01:13:48.000 --> 01:13:50.000]  То есть мы можем построить
[01:13:50.000 --> 01:13:52.000]  себе такой компонент.
[01:13:52.000 --> 01:13:54.000]  Но Google почему-то делает не так.
[01:13:56.000 --> 01:13:58.000]  Но многие системы делают так.
[01:13:58.000 --> 01:14:00.000]  Скажем, если вы пойдете в Яндекс работать,
[01:14:00.000 --> 01:14:02.000]  там система в IT, там есть транзакции, динамические таблицы,
[01:14:02.000 --> 01:14:04.000]  и у них есть компонент stamp oracle,
[01:14:04.000 --> 01:14:06.000]  который генерирует временные метки для транзакций.
[01:14:06.000 --> 01:14:08.000]  Вот Spanner так не делает.
[01:14:10.000 --> 01:14:12.000]  Ясно ли почему?
[01:14:18.000 --> 01:14:20.000]  Мы смотрели на картинку
[01:14:20.000 --> 01:14:22.000]  и там были реплики,
[01:14:22.000 --> 01:14:24.000]  которые находились в разных датацентрах.
[01:14:24.000 --> 01:14:26.000]  И датацентров Google довольно много,
[01:14:26.000 --> 01:14:28.000]  и они находятся вообще на разных континентах,
[01:14:28.000 --> 01:14:30.000]  но очень далеко друг от друга.
[01:14:30.000 --> 01:14:32.000]  Беда здесь в том, что
[01:14:32.000 --> 01:14:34.000]  если мы построим один компонент
[01:14:34.000 --> 01:14:36.000]  timestamp oracle,
[01:14:36.000 --> 01:14:38.000]  это точка централизации.
[01:14:38.000 --> 01:14:40.000]  Все транзакции должны посещать его.
[01:14:42.000 --> 01:14:44.000]  А если мы говорим про multipax,
[01:14:44.000 --> 01:14:46.000]  то timestamp oracle,
[01:14:46.000 --> 01:14:48.000]  там несколько реплик,
[01:14:48.000 --> 01:14:50.000]  они будут устойчивы сам по себе.
[01:14:50.000 --> 01:14:52.000]  И реплики могут находиться далеко друг от друга
[01:14:52.000 --> 01:14:54.000]  для большей доступности.
[01:14:54.000 --> 01:14:56.000]  Но в конце концов,
[01:14:56.000 --> 01:14:58.000]  среди них есть лидер,
[01:14:58.000 --> 01:15:00.000]  и всех пойдет в него.
[01:15:00.000 --> 01:15:02.000]  И даже если он переживает любой рейд запросов,
[01:15:02.000 --> 01:15:04.000]  то все равно со всего глобуса
[01:15:04.000 --> 01:15:06.000]  вы стекаетесь к какой-то конкретной машине.
[01:15:06.000 --> 01:15:08.000]  То есть кому-то повезет, а для кого-то она будет далеко.
[01:15:08.000 --> 01:15:10.000]  Поэтому, что делает Google?
[01:15:10.000 --> 01:15:12.000]  Они говорят, что поскольку у нас система
[01:15:12.000 --> 01:15:14.000]  геораспределенная, она масштабируется на весь глобус,
[01:15:14.000 --> 01:15:16.000]  мы хотим
[01:15:16.000 --> 01:15:18.000]  выбирать глобально
[01:15:18.000 --> 01:15:20.000]  монотонные временные метки
[01:15:20.000 --> 01:15:22.000]  по возможности, избегая координации,
[01:15:22.000 --> 01:15:24.000]  чтобы транзакции где-нибудь
[01:15:24.000 --> 01:15:26.000]  в Антарктиде и на Северном полюсе, они
[01:15:26.000 --> 01:15:28.000]  друг с другом не коммуницировали
[01:15:28.000 --> 01:15:30.000]  с общими узлами.
[01:15:30.000 --> 01:15:32.000]  Для этого у Google
[01:15:32.000 --> 01:15:34.000]  есть компания, которая называется TrueTime.
[01:15:34.000 --> 01:15:36.000]  В чем
[01:15:36.000 --> 01:15:38.000]  идея? Выбрать
[01:15:38.000 --> 01:15:40.000]  временную метку для ком... Простите
[01:15:40.000 --> 01:15:42.000]  этот пример,
[01:15:42.000 --> 01:15:44.000]  почему не стоит делать темные слайды
[01:15:44.000 --> 01:15:46.000]  и там оранжевые надписи.
[01:15:46.000 --> 01:15:48.000]  Свет выключить, хорошая идея.
[01:15:50.000 --> 01:15:52.000]  Но стало лучше
[01:15:52.000 --> 01:15:54.000]  все же.
[01:15:54.000 --> 01:15:56.000]  Задача сделать так, чтобы
[01:15:56.000 --> 01:15:58.000]  если два отрезка не пересекаются
[01:15:58.000 --> 01:16:00.000]  в двух транзакциях, то временные метки были бы монотонными.
[01:16:00.000 --> 01:16:02.000]  Кажется, про это даже есть
[01:16:02.000 --> 01:16:04.000]  отдельный слайд, можно его и
[01:16:04.000 --> 01:16:06.000]  показать.
[01:16:14.000 --> 01:16:16.000]  Ну ладно.
[01:16:20.000 --> 01:16:22.000]  Идея в том,
[01:16:22.000 --> 01:16:24.000]  что можно не думать про
[01:16:24.000 --> 01:16:26.000]  монотонность,
[01:16:26.000 --> 01:16:28.000]  можно не думать
[01:16:28.000 --> 01:16:30.000]  про координацию и про две транзакции,
[01:16:30.000 --> 01:16:32.000]  можно думать только про одну транзакцию,
[01:16:32.000 --> 01:16:34.000]  если в качестве временных меток
[01:16:34.000 --> 01:16:36.000]  вы используете не
[01:16:36.000 --> 01:16:38.000]  какие-то монотонные числа,
[01:16:38.000 --> 01:16:40.000]  а прямо физическое время.
[01:16:40.000 --> 01:16:42.000]  То есть понятно, что если вы
[01:16:42.000 --> 01:16:44.000]  выберете в качестве временной метки
[01:16:44.000 --> 01:16:46.000]  для записи
[01:16:46.000 --> 01:16:48.000]  now между точкой, где вы
[01:16:48.000 --> 01:16:50.000]  взяли последний лок, где вы первый отпустили,
[01:16:50.000 --> 01:16:52.000]  то монотонность будет соблюдена.
[01:16:52.000 --> 01:16:54.000]  Но поскольку now
[01:16:54.000 --> 01:16:56.000]  вы не можете использовать, потому что
[01:16:56.000 --> 01:16:58.000]  часы могут быть рассинхронизированы,
[01:16:58.000 --> 01:17:00.000]  вы используете что-то сложнее,
[01:17:02.000 --> 01:17:04.000]  вы используете true time, который
[01:17:04.000 --> 01:17:06.000]  возвращает вам вместо now интервал
[01:17:06.000 --> 01:17:08.000]  вот earliest to latest
[01:17:08.000 --> 01:17:10.000]  и говорит, что время, что точка
[01:17:10.000 --> 01:17:12.000]  вопроса находится в этом интервале,
[01:17:12.000 --> 01:17:14.000]  или интервал вопроса пересекается с интервалом этой
[01:17:14.000 --> 01:17:16.000]  операции. И что
[01:17:16.000 --> 01:17:18.000]  приятно, что вот этот сервис
[01:17:18.000 --> 01:17:20.000]  true time now в этом
[01:17:20.000 --> 01:17:22.000]  вызове никакой коммуникации с другими узлами
[01:17:22.000 --> 01:17:24.000]  не выполняет.
[01:17:24.000 --> 01:17:26.000]  Коммуникация там, конечно, есть, он раз в 30 секунд
[01:17:26.000 --> 01:17:28.000]  общается, но про это тоже написано
[01:17:28.000 --> 01:17:30.000]  в слайдах спаннера.
[01:17:34.000 --> 01:17:36.000]  Это система, которая развернута внутри
[01:17:36.000 --> 01:17:38.000]  датацентра. В каждом
[01:17:38.000 --> 01:17:40.000]  датацентре есть GPS
[01:17:40.000 --> 01:17:42.000]  антенны, есть узлы
[01:17:42.000 --> 01:17:44.000]  таймастера с атомарными часами.
[01:17:44.000 --> 01:17:46.000]  И раз в 30 секунд
[01:17:46.000 --> 01:17:48.000]  каждый узел
[01:17:48.000 --> 01:17:50.000]  в кластере общается с разными
[01:17:50.000 --> 01:17:52.000]  таймастерами, получает от них очень
[01:17:52.000 --> 01:17:54.000]  точное время,
[01:17:54.000 --> 01:17:56.000]  а потом 30 секунд с ним
[01:17:56.000 --> 01:17:58.000]  живет.
[01:17:58.000 --> 01:18:00.000]  То есть он знает текущее время
[01:18:00.000 --> 01:18:02.000]  с очень маленькой, с очень высокой точностью,
[01:18:02.000 --> 01:18:04.000]  а потом 30 секунд живет
[01:18:04.000 --> 01:18:06.000]  по своим собственным
[01:18:06.000 --> 01:18:08.000]  часам, просто закладывая в них большой
[01:18:08.000 --> 01:18:10.000]  дрейф.
[01:18:10.000 --> 01:18:12.000]  Когда у него спрашивают нау, он смотрит,
[01:18:12.000 --> 01:18:14.000]  насколько давно
[01:18:14.000 --> 01:18:16.000]  он синхронизировался с таймастерами
[01:18:16.000 --> 01:18:18.000]  и сдвигает интервал вправо.
[01:18:18.000 --> 01:18:20.000]  Но поскольку он не знает,
[01:18:20.000 --> 01:18:22.000]  сколько времени в точности прошло между текущим
[01:18:22.000 --> 01:18:24.000]  запросом клиента, точкой синхронизации,
[01:18:24.000 --> 01:18:26.000]  он закладывает в эту величину,
[01:18:26.000 --> 01:18:28.000]  он закладывает еще дрейф
[01:18:28.000 --> 01:18:30.000]  возможный, поэтому расширяет интервал.
[01:18:30.000 --> 01:18:32.000]  Но даже в такой схеме,
[01:18:32.000 --> 01:18:34.000]  если мы закладываем
[01:18:34.000 --> 01:18:36.000]  дрейф в 200 микро секунд в секунду,
[01:18:36.000 --> 01:18:38.000]  это в 10 раз хуже,
[01:18:38.000 --> 01:18:40.000]  чем очень плохие кварцевые часы,
[01:18:40.000 --> 01:18:42.000]  то даже в такой схеме
[01:18:42.000 --> 01:18:44.000]  этого достаточно, чтобы получить
[01:18:44.000 --> 01:18:46.000]  ширину окна примерно
[01:18:46.000 --> 01:18:48.000]  6 миллисекунд. То есть этот сервис довольно
[01:18:48.000 --> 01:18:50.000]  точный, но у них еще есть здесь
[01:18:50.000 --> 01:18:52.000]  графики,
[01:18:54.000 --> 01:18:56.000]  что вот, не знаю, 90
[01:18:56.000 --> 01:18:58.000]  процентов
[01:18:58.000 --> 01:19:00.000]  времени True Time
[01:19:00.000 --> 01:19:02.000]  оценивает
[01:19:02.000 --> 01:19:04.000]  вот этот Эпсел,
[01:19:04.000 --> 01:19:06.000]  ширину интервала примерно в ноль вообще.
[01:19:06.000 --> 01:19:08.000]  Но если мы говорим про 99
[01:19:08.000 --> 01:19:10.000]  процентов времени, то на вот
[01:19:10.000 --> 01:19:12.000]  10 миллисекунд.
[01:19:12.000 --> 01:19:14.000]  Но это все равно, даже
[01:19:14.000 --> 01:19:16.000]  если мы
[01:19:16.000 --> 01:19:18.000]  берем вот
[01:19:18.000 --> 01:19:20.000]  такой процент, то все равно
[01:19:20.000 --> 01:19:22.000]  это намного лучше,
[01:19:22.000 --> 01:19:24.000]  чем коммуникация
[01:19:24.000 --> 01:19:26.000]  через очень большие расстояния.
[01:19:26.000 --> 01:19:28.000]  То есть мы ее делаем
[01:19:28.000 --> 01:19:30.000]  все равно, это коммуникацию, но мы делаем это в фоне
[01:19:30.000 --> 01:19:32.000]  через GPS,
[01:19:34.000 --> 01:19:36.000]  либо общаясь
[01:19:36.000 --> 01:19:38.000]  только с локальными атомными часами
[01:19:38.000 --> 01:19:40.000]  в своем ДЦ, либо общаясь со спутниками
[01:19:40.000 --> 01:19:42.000]  через радиосигнал.
[01:19:44.000 --> 01:19:46.000]  Мы с спутниками не общаемся, это они с нами
[01:19:46.000 --> 01:19:48.000]  общаются.
[01:19:50.000 --> 01:19:52.000]  И имея такой сервис,
[01:19:52.000 --> 01:19:54.000]  который, вообще говоря, не очень-то просто
[01:19:54.000 --> 01:19:56.000]  повторить, кажется, что никто
[01:19:56.000 --> 01:19:58.000]  и не пытался больше. Мы можем
[01:19:58.000 --> 01:20:00.000]  легко построить процедуру,
[01:20:00.000 --> 01:20:02.000]  которая генерирует вот эти самые
[01:20:02.000 --> 01:20:04.000]  временные метки.
[01:20:04.000 --> 01:20:06.000]  Мы берем
[01:20:06.000 --> 01:20:08.000]  true time now, правую границу,
[01:20:08.000 --> 01:20:10.000]  потому что мы знаем, что она точно правее,
[01:20:10.000 --> 01:20:12.000]  чем точка, где мы взяли локи.
[01:20:12.000 --> 01:20:14.000]  А потом дожидаемся,
[01:20:14.000 --> 01:20:16.000]  пока S окажется в прошлом,
[01:20:16.000 --> 01:20:18.000]  чтобы попасть в интервал.
[01:20:18.000 --> 01:20:20.000]  Правую границу мы управляем, точка, когда мы
[01:20:20.000 --> 01:20:22.000]  отпускаем локи. Так вот, локи мы отпускаем
[01:20:22.000 --> 01:20:24.000]  только тогда, когда мы от true time now
[01:20:24.000 --> 01:20:26.000]  получили интервал, левая граница, которого
[01:20:26.000 --> 01:20:28.000]  оказалось не меньше, чем
[01:20:28.000 --> 01:20:30.000]  вот эта выбранная S.
[01:20:30.000 --> 01:20:32.000]  Но это вы, кажется, знаете, если вы делали
[01:20:32.000 --> 01:20:34.000]  первую домашнюю работу и не скипали
[01:20:34.000 --> 01:20:36.000]  часть про true time.
[01:20:44.000 --> 01:20:46.000]  Таким образом, когда транзакция
[01:20:46.000 --> 01:20:48.000]  коммитится, она
[01:20:48.000 --> 01:20:50.000]  выбирает себе timestamp, она с помощью
[01:20:50.000 --> 01:20:52.000]  вот этой процедуры генерирует
[01:20:52.000 --> 01:20:54.000]  на тонную временную метку, заменяя коммуникацию
[01:20:54.000 --> 01:20:56.000]  на ожидания, и после этого
[01:20:56.000 --> 01:20:58.000]  делает запись. Таким образом, мы
[01:20:58.000 --> 01:21:00.000]  генерируем
[01:21:00.000 --> 01:21:02.000]  в 2P или версии хранилища
[01:21:02.000 --> 01:21:04.000]  явно,
[01:21:04.000 --> 01:21:06.000]  и читающая транзакция теперь
[01:21:06.000 --> 01:21:08.000]  может читать вообще без блокировок
[01:21:08.000 --> 01:21:10.000]  и читать именно
[01:21:10.000 --> 01:21:12.000]  из истории сервизованных транзакций.
[01:21:14.000 --> 01:21:16.000]  Правда, тут есть некоторые технические нюансы,
[01:21:16.000 --> 01:21:18.000]  совсем маленький,
[01:21:18.000 --> 01:21:20.000]  иностранный клиент. Давайте я
[01:21:20.000 --> 01:21:22.000]  его покажу.
[01:21:24.000 --> 01:21:26.000]  Время поговорить
[01:21:26.000 --> 01:21:28.000]  про
[01:21:28.000 --> 01:21:30.000]  тонячий клиент для спандера
[01:21:30.000 --> 01:21:32.000]  почему-то. Смотрите,
[01:21:32.000 --> 01:21:34.000]  у нас есть два механизма исполнения
[01:21:34.000 --> 01:21:36.000]  транзакции. Если транзакция
[01:21:36.000 --> 01:21:38.000]  RedOnly, то она должна идти по одному
[01:21:38.000 --> 01:21:40.000]  пути, то есть получать read timestamp и читать
[01:21:40.000 --> 01:21:42.000]  без блоков из хранилища напрямую.
[01:21:42.000 --> 01:21:44.000]  Если у нас транзакция
[01:21:44.000 --> 01:21:46.000]  пишет,
[01:21:46.000 --> 01:21:48.000]  то она должна использовать
[01:21:48.000 --> 01:21:50.000]  двухфазные блокировки и вот
[01:21:50.000 --> 01:21:52.000]  выполнить протокол, который описан был до этого.
[01:21:52.000 --> 01:21:54.000]  Но транзакция у нас интерактивная, поэтому
[01:21:54.000 --> 01:21:56.000]  у нее на лбу заранее не написано, что она собирается
[01:21:56.000 --> 01:21:58.000]  делать, будет ли она писать или нет.
[01:21:58.000 --> 01:22:00.000]  Так вот, на уровне клиентской
[01:22:00.000 --> 01:22:02.000]  библиотеки транзакция фактически анонсируется
[01:22:02.000 --> 01:22:04.000]  с теми, как она собирается работать.
[01:22:04.000 --> 01:22:06.000]  Просто для одного и другого
[01:22:06.000 --> 01:22:08.000]  типа транзакции есть отдельные API.
[01:22:08.000 --> 01:22:10.000]  Если мы говорим про транзакции,
[01:22:10.000 --> 01:22:12.000]  которые только читают, то
[01:22:12.000 --> 01:22:14.000]  в клиентской библиотеке они выглядят так.
[01:22:14.000 --> 01:22:16.000]  Вот здесь context manager,
[01:22:16.000 --> 01:22:18.000]  мы берем снапшот хранилища,
[01:22:18.000 --> 01:22:20.000]  то есть фактически
[01:22:20.000 --> 01:22:22.000]  выбираем некоторую временную метку для чтения
[01:22:22.000 --> 01:22:24.000]  и потом через этот снапшот читаем.
[01:22:24.000 --> 01:22:26.000]  Вот мы здесь дальше не можем
[01:22:26.000 --> 01:22:28.000]  ничего писать, у нас просто нет вызовов таких
[01:22:28.000 --> 01:22:30.000]  в объекте снапшот.
[01:22:30.000 --> 01:22:32.000]  Если же мы
[01:22:32.000 --> 01:22:34.000]  собираемся писать транзакции,
[01:22:34.000 --> 01:22:36.000]  то мы должны воспользоваться
[01:22:36.000 --> 01:22:38.000]  другими API,
[01:22:38.000 --> 01:22:40.000]  написать тело транзакции
[01:22:42.000 --> 01:22:44.000]  и передать его функцию
[01:22:44.000 --> 01:22:46.000]  runTransaction.
[01:22:46.000 --> 01:22:48.000]  Мы передаем свое тело
[01:22:48.000 --> 01:22:50.000]  транзакции, и нам этот runTransaction
[01:22:50.000 --> 01:22:52.000]  вызывает
[01:22:52.000 --> 01:22:54.000]  этот UnitOfWork,
[01:22:54.000 --> 01:22:56.000]  передавая ему объект транзакции.
[01:22:58.000 --> 01:23:00.000]  И дальше мы через него
[01:23:00.000 --> 01:23:02.000]  делаем какие-то апдейты, удаления и так далее.
[01:23:02.000 --> 01:23:04.000]  Почему API такое?
[01:23:04.000 --> 01:23:06.000]  Потому что
[01:23:06.000 --> 01:23:08.000]  эта транзакция рестартовать не может.
[01:23:08.000 --> 01:23:10.000]  Просто нет повода
[01:23:10.000 --> 01:23:12.000]  транзакцию откатывать.
[01:23:12.000 --> 01:23:14.000]  Она просто читает из некоторого
[01:23:14.000 --> 01:23:16.000]  мгновенного состояния.
[01:23:16.000 --> 01:23:18.000]  Вот эта транзакция откатываться может,
[01:23:18.000 --> 01:23:20.000]  и ретраи они завернуты
[01:23:20.000 --> 01:23:22.000]  вот в эту вспомогательную функцию.
[01:23:24.000 --> 01:23:26.000]  То есть это тело может ретраиться,
[01:23:26.000 --> 01:23:28.000]  но и поэтому важно, чтобы здесь не было каких-то сайд-эффектов еще.
[01:23:30.000 --> 01:23:32.000]  Так вот, используя либо одно, либо другое
[01:23:32.000 --> 01:23:34.000]  API, вы сразу системе объявляете,
[01:23:34.000 --> 01:23:36.000]  как будет работать ваша транзакция,
[01:23:36.000 --> 01:23:38.000]  и она исполняется либо одним, либо другим
[01:23:38.000 --> 01:23:40.000]  способом.
[01:23:40.000 --> 01:23:42.000]  Окей,
[01:23:42.000 --> 01:23:44.000]  почти готово.
[01:23:44.000 --> 01:23:46.000]  Мы почти
[01:23:46.000 --> 01:23:48.000]  добрались до конца.
[01:23:48.000 --> 01:23:50.000]  Но кое-что еще осталось.
[01:23:50.000 --> 01:23:52.000]  Давайте подумаем вот над чем.
[01:23:52.000 --> 01:23:54.000]  А как выполнять
[01:23:54.000 --> 01:23:56.000]  чтение вашей транзакции?
[01:23:56.000 --> 01:23:58.000]  Как выполнять чтение в вашей системе?
[01:23:58.000 --> 01:24:00.000]  Вот вы клиент,
[01:24:00.000 --> 01:24:02.000]  вы хотите выполнить транзакцию,
[01:24:02.000 --> 01:24:04.000]  которая будет только читать.
[01:24:04.000 --> 01:24:06.000]  У вас для этого есть
[01:24:06.000 --> 01:24:08.000]  вот
[01:24:08.000 --> 01:24:10.000]  вот
[01:24:10.000 --> 01:24:12.000]  потерялся вот такой API.
[01:24:12.000 --> 01:24:14.000]  Здесь вы сгенировали некоторую
[01:24:14.000 --> 01:24:16.000]  временную метку.
[01:24:16.000 --> 01:24:18.000]  В чем ваш замысел?
[01:24:18.000 --> 01:24:20.000]  Ваша временная метка, конечно же, должна быть
[01:24:20.000 --> 01:24:22.000]  не меньше, чем временная метка
[01:24:22.000 --> 01:24:24.000]  каждой закомиченной к этому времени
[01:24:24.000 --> 01:24:26.000]  транзакции.
[01:24:26.000 --> 01:24:28.000]  Как выбрать такую временную метку?
[01:24:32.000 --> 01:24:34.000]  Ну точно так же можно выбрать.
[01:24:34.000 --> 01:24:36.000]  То есть снова можно воспользоваться
[01:24:36.000 --> 01:24:38.000]  TrueTime на машине спаннера.
[01:24:38.000 --> 01:24:40.000]  И он возьмет
[01:24:40.000 --> 01:24:42.000]  TrueTime Latest.
[01:24:42.000 --> 01:24:44.000]  И вы будете по этой временной метке читать.
[01:24:44.000 --> 01:24:46.000]  Она будет не меньше, чем любая транзакция,
[01:24:46.000 --> 01:24:48.000]  которая уже закомитилась.
[01:24:48.000 --> 01:24:50.000]  Но смотрите, в чем сложность.
[01:24:52.000 --> 01:24:54.000]  Вы приходите с этой временной меткой
[01:24:54.000 --> 01:24:56.000]  на какой-то шарт.
[01:24:58.000 --> 01:25:00.000]  Да, я, кстати, не показал вам,
[01:25:00.000 --> 01:25:02.000]  когда говорил про мультиверсионное
[01:25:02.000 --> 01:25:04.000]  хранилище,
[01:25:04.000 --> 01:25:06.000]  что вот спаннер действительно
[01:25:06.000 --> 01:25:08.000]  его реализует.
[01:25:08.000 --> 01:25:10.000]  То есть он хранит отображение
[01:25:10.000 --> 01:25:12.000]  из ключей и таймстэмпов
[01:25:12.000 --> 01:25:14.000]  в какие-то строчки.
[01:25:14.000 --> 01:25:16.000]  Так вот, вы приходите на шарт, у вас временная метка,
[01:25:16.000 --> 01:25:18.000]  и вы говорите, хочу читать из снэпшота.
[01:25:18.000 --> 01:25:20.000]  Ну снэпшот — это же то,
[01:25:20.000 --> 01:25:22.000]  что мы на самом деле в уме
[01:25:22.000 --> 01:25:24.000]  придумали себе.
[01:25:24.000 --> 01:25:26.000]  Никаких снэпшотов в системе нет.
[01:25:26.000 --> 01:25:28.000]  В системе просто есть
[01:25:28.000 --> 01:25:30.000]  мультиверсионное хранилище, и туда можно
[01:25:30.000 --> 01:25:32.000]  написать пары ключ-таймстэмп значения.
[01:25:34.000 --> 01:25:36.000]  Так вот, когда вы приходите в шарт
[01:25:36.000 --> 01:25:38.000]  с временной меткой, шарт же не знает,
[01:25:38.000 --> 01:25:40.000]  есть ли в нем все
[01:25:40.000 --> 01:25:42.000]  записи с этим...
[01:25:42.000 --> 01:25:44.000]  Верно ли, что он в себе уже
[01:25:44.000 --> 01:25:46.000]  содержит все записи с таймстэмпом
[01:25:46.000 --> 01:25:48.000]  клиента и со всеми
[01:25:48.000 --> 01:25:50.000]  меньшими таймстэмпами?
[01:25:50.000 --> 01:25:52.000]  Или туда еще могут прилететь какие-то другие транзакции,
[01:25:52.000 --> 01:25:54.000]  которые еще просто не успели
[01:25:54.000 --> 01:25:56.000]  доставить в него свои записи,
[01:25:56.000 --> 01:25:58.000]  какие-то незакомичные транзакции?
[01:25:58.000 --> 01:26:00.000]  Понятна проблема?
[01:26:00.000 --> 01:26:02.000]  То есть пока снэпшот — это только то,
[01:26:02.000 --> 01:26:04.000]  что мы на доске нарисовали, а физически
[01:26:04.000 --> 01:26:06.000]  они как-то не представлены.
[01:26:06.000 --> 01:26:08.000]  Так вот, если вы клиент и вы приходите в шарт,
[01:26:08.000 --> 01:26:10.000]  то вы должны каким-то образом дождаться,
[01:26:10.000 --> 01:26:12.000]  дождаться того момента,
[01:26:12.000 --> 01:26:14.000]  когда вы будете уверены, что в шарде
[01:26:14.000 --> 01:26:16.000]  не появятся новых записей с таймстэмпом
[01:26:16.000 --> 01:26:18.000]  меньшим или равным вашему.
[01:26:18.000 --> 01:26:20.000]  И для этого
[01:26:20.000 --> 01:26:22.000]  в спаннере
[01:26:22.000 --> 01:26:24.000]  снова есть некоторые ухищрения.
[01:26:24.000 --> 01:26:26.000]  А именно каждый шарт,
[01:26:26.000 --> 01:26:28.000]  но вот об этом написано в статье,
[01:26:28.000 --> 01:26:30.000]  и добрая часть статьи этому посвящена.
[01:26:36.000 --> 01:26:38.000]  Каждый шарт поддерживает
[01:26:38.000 --> 01:26:40.000]  вот такую величину,
[01:26:40.000 --> 01:26:42.000]  он называется save time.
[01:26:42.000 --> 01:26:44.000]  Это оценка
[01:26:44.000 --> 01:26:46.000]  снизу на возможные
[01:26:46.000 --> 01:26:48.000]  таймстэмпы транзакций,
[01:26:48.000 --> 01:26:50.000]  которые могут этому шарду
[01:26:50.000 --> 01:26:52.000]  еще прийти.
[01:26:52.000 --> 01:26:54.000]  Если вы приходите с таймстэмпом
[01:26:54.000 --> 01:26:56.000]  больше, чем save time шарда,
[01:26:56.000 --> 01:26:58.000]  то вы должны ждать,
[01:26:58.000 --> 01:27:00.000]  пока этот save time не дорастет
[01:27:00.000 --> 01:27:02.000]  до значения большего,
[01:27:02.000 --> 01:27:04.000]  чем вашего.
[01:27:04.000 --> 01:27:06.000]  И каким образом оно поддерживается?
[01:27:06.000 --> 01:27:08.000]  Давайте я покажу картинку,
[01:27:08.000 --> 01:27:10.000]  я в детали углубляться не буду,
[01:27:10.000 --> 01:27:12.000]  потому что можно запутаться.
[01:27:12.000 --> 01:27:14.000]  Но все это встраивается
[01:27:14.000 --> 01:27:16.000]  в двухфазный комит.
[01:27:16.000 --> 01:27:18.000]  Каждый шарт поддерживает свою
[01:27:18.000 --> 01:27:20.000]  нижнюю оценку,
[01:27:20.000 --> 01:27:22.000]  и всем своим записям
[01:27:22.000 --> 01:27:24.000]  он их монотонно генерирует,
[01:27:24.000 --> 01:27:26.000]  потому что каждый шарт
[01:27:26.000 --> 01:27:28.000]  он в целом атомарен,
[01:27:28.000 --> 01:27:30.000]  он может генировать монотонные временные метки,
[01:27:30.000 --> 01:27:32.000]  даже с учетом рестартов и смены лидера.
[01:27:32.000 --> 01:27:34.000]  И когда шарду приходит
[01:27:34.000 --> 01:27:36.000]  команда Prepr от координатора,
[01:27:36.000 --> 01:27:38.000]  он пишет ее, он сохраняет ее надежно,
[01:27:38.000 --> 01:27:40.000]  сохраняет локи и генерирует
[01:27:40.000 --> 01:27:42.000]  временную метку и отправляет ее координатору.
[01:27:42.000 --> 01:27:44.000]  А координатор
[01:27:44.000 --> 01:27:46.000]  в итоге выбирает
[01:27:46.000 --> 01:27:48.000]  временную метку комита
[01:27:48.000 --> 01:27:50.000]  как максимум из true time latest,
[01:27:50.000 --> 01:27:52.000]  true time now latest
[01:27:52.000 --> 01:27:54.000]  и нижние границы,
[01:27:54.000 --> 01:27:56.000]  которые прислали шарды.
[01:27:58.000 --> 01:28:00.000]  Это такой механизм
[01:28:00.000 --> 01:28:02.000]  ставить границу снизу
[01:28:02.000 --> 01:28:04.000]  на возможные записи в данный шарт,
[01:28:04.000 --> 01:28:06.000]  и таким образом
[01:28:08.000 --> 01:28:10.000]  мы можем
[01:28:10.000 --> 01:28:12.000]  сейфтайм на шарде поддерживать
[01:28:12.000 --> 01:28:14.000]  и помогать клиенту дожидаться,
[01:28:14.000 --> 01:28:16.000]  и помогать клиенту
[01:28:16.000 --> 01:28:18.000]  читать только действительно снимки состояния.
[01:28:18.000 --> 01:28:20.000]  Если он читает, то он уверен, что
[01:28:20.000 --> 01:28:22.000]  никаких записей с меньшим таймстемпом в этот шарт уже не придет.
[01:28:22.000 --> 01:28:24.000]  И снова никакой коммуникации не нужно,
[01:28:24.000 --> 01:28:26.000]  мы просто ждем.
[01:28:36.000 --> 01:28:38.000]  Итак, мне кажется, что
[01:28:38.000 --> 01:28:40.000]  я помодлю некоторых
[01:28:40.000 --> 01:28:42.000]  оптимизаций, которые я
[01:28:42.000 --> 01:28:44.000]  наверное уже не в силах
[01:28:44.000 --> 01:28:46.000]  рассказать.
[01:28:54.000 --> 01:28:56.000]  Спаннер разобрал.
[01:28:56.000 --> 01:28:58.000]  Итак, напомню всю конструкцию.
[01:28:58.000 --> 01:29:00.000]  Спаннер это
[01:29:00.000 --> 01:29:02.000]  шардированное мультиверсионное кивалио-хранилище.
[01:29:02.000 --> 01:29:04.000]  Каждый шарт представляет собой
[01:29:04.000 --> 01:29:06.000]  набор реплик мультипаксиса,
[01:29:06.000 --> 01:29:08.000]  которые хранят свои данные
[01:29:08.000 --> 01:29:10.000]  в колоссе и на жестком диске.
[01:29:10.000 --> 01:29:12.000]  И над
[01:29:12.000 --> 01:29:14.000]  этими шардами исполняются
[01:29:14.000 --> 01:29:16.000]  распределенные транзакции.
[01:29:16.000 --> 01:29:18.000]  В первом приближении транзакции используют двухфазные блокировки.
[01:29:18.000 --> 01:29:20.000]  И если
[01:29:20.000 --> 01:29:22.000]  транзакция касается только одного шарда,
[01:29:22.000 --> 01:29:24.000]  то каждый шарт хранит
[01:29:24.000 --> 01:29:26.000]  эти блокировки в
[01:29:26.000 --> 01:29:28.000]  оперативной памяти лидера.
[01:29:28.000 --> 01:29:30.000]  И если
[01:29:30.000 --> 01:29:32.000]  транзакция касается только одного шарда,
[01:29:32.000 --> 01:29:34.000]  то в момент коммита
[01:29:34.000 --> 01:29:36.000]  клиент отправляет этому шарду
[01:29:36.000 --> 01:29:38.000]  все свои записи, которые он накопил.
[01:29:38.000 --> 01:29:40.000]  Шарт проверяет.
[01:29:40.000 --> 01:29:42.000]  Трансакшн менеджер на лидере шарда проверяет,
[01:29:42.000 --> 01:29:44.000]  что сессия не стекла,
[01:29:44.000 --> 01:29:46.000]  что локтейбл
[01:29:46.000 --> 01:29:48.000]  не конфликтует,
[01:29:48.000 --> 01:29:50.000]  что сессия
[01:29:50.000 --> 01:29:52.000]  вообще ему
[01:29:52.000 --> 01:29:54.000]  известно,
[01:29:54.000 --> 01:29:56.000]  что локтейбл не конфликтует, в смысле ваундвейта.
[01:29:56.000 --> 01:29:58.000]  И тогда транзакцию фиксирует.
[01:29:58.000 --> 01:30:00.000]  Если же мы говорим про транзакции,
[01:30:00.000 --> 01:30:02.000]  которые касаются нескольких шардов,
[01:30:02.000 --> 01:30:04.000]  то для того, чтобы автоматно
[01:30:04.000 --> 01:30:06.000]  на всех транзакцию либо за коммит, либо откатить,
[01:30:06.000 --> 01:30:08.000]  мы используем протокол двухфазного коммита,
[01:30:08.000 --> 01:30:10.000]  но чтобы побороться с отказами клиента,
[01:30:10.000 --> 01:30:12.000]  мы переносим координацию
[01:30:12.000 --> 01:30:14.000]  транзакции на
[01:30:14.000 --> 01:30:16.000]  узел, точнее на компонент системы
[01:30:16.000 --> 01:30:18.000]  отказа и устойчивость на один из шардов.
[01:30:18.000 --> 01:30:20.000]  А дальше мы говорим, что здорово
[01:30:20.000 --> 01:30:22.000]  было бы редон для транзакций пооптимизировать,
[01:30:22.000 --> 01:30:24.000]  и поэтому, когда мы пишем
[01:30:24.000 --> 01:30:26.000]  в 2PL
[01:30:26.000 --> 01:30:28.000]  записи в хранилище,
[01:30:28.000 --> 01:30:30.000]  то мы порождаем, мы не перезаписываем значение
[01:30:30.000 --> 01:30:32.000]  в хранилище, мы порождаем новую версию.
[01:30:32.000 --> 01:30:34.000]  Но для того, чтобы
[01:30:34.000 --> 01:30:36.000]  это делать согласовано, чтобы
[01:30:36.000 --> 01:30:38.000]  порядок на версиях был согласован с порядком
[01:30:38.000 --> 01:30:40.000]  в 2PL, мы используем TrueTime
[01:30:40.000 --> 01:30:42.000]  для распределенной генерации
[01:30:42.000 --> 01:30:44.000]  временных меток.
[01:30:44.000 --> 01:30:46.000]  Ну и вот в итоге
[01:30:46.000 --> 01:30:48.000]  получается
[01:30:54.000 --> 01:30:56.000]  вот такая общая схема.
[01:30:56.000 --> 01:30:58.000]  Итого, кажется,
[01:30:58.000 --> 01:31:00.000]  мы использовали здесь мультиверсионность, мы использовали
[01:31:00.000 --> 01:31:02.000]  Multipax, мы использовали
[01:31:02.000 --> 01:31:04.000]  Colossus, мы использовали транзакции,
[01:31:04.000 --> 01:31:06.000]  мы использовали TrueTime.
[01:31:06.000 --> 01:31:08.000]  Вот все вместе это собирается.
[01:31:10.000 --> 01:31:12.000]  Что здесь принципиально нового?
[01:31:12.000 --> 01:31:14.000]  На самом деле не так уж много.
[01:31:14.000 --> 01:31:16.000]  Вот Spanner — это пример системы,
[01:31:16.000 --> 01:31:18.000]  это пример хорошего дизайна,
[01:31:18.000 --> 01:31:20.000]  в смысле
[01:31:20.000 --> 01:31:22.000]  взять много всего
[01:31:22.000 --> 01:31:24.000]  известного и собрать во что-то большое
[01:31:24.000 --> 01:31:26.000]  и работающее.
[01:31:26.000 --> 01:31:28.000]  Вот из таких вот инноваций прямо в Spanner
[01:31:28.000 --> 01:31:30.000]  это именно TrueTime, то есть
[01:31:30.000 --> 01:31:32.000]  способ генерации распределенных временных меток.
[01:31:32.000 --> 01:31:34.000]  2PL — это что-то
[01:31:34.000 --> 01:31:36.000]  из далекого-далекого прошлого,
[01:31:36.000 --> 01:31:38.000]  когда компьютеров еще не было.
[01:31:38.000 --> 01:31:40.000]  Двухфазный Комит тоже — это какие-то
[01:31:40.000 --> 01:31:42.000]  70-80-е годы, наверное.
[01:31:44.000 --> 01:31:46.000]  Изоляция снайпшотов — это чуть попозже придумано,
[01:31:46.000 --> 01:31:48.000]  но все равно.
[01:31:48.000 --> 01:31:50.000]  То есть мы просто собрали
[01:31:50.000 --> 01:31:52.000]  очень много идей и построили из них
[01:31:52.000 --> 01:31:54.000]  большую-большую систему. Ну и вот это
[01:31:54.000 --> 01:31:56.000]  повторить довольно сложно.
[01:31:58.000 --> 01:32:00.000]  Все-таки есть тут специфика самого Google,
[01:32:00.000 --> 01:32:02.000]  потому что не каждому,
[01:32:02.000 --> 01:32:04.000]  не каждой системе, не каждой компании
[01:32:04.000 --> 01:32:06.000]  просто нужно решать вот такие
[01:32:06.000 --> 01:32:08.000]  задачи, как глобальные,
[01:32:08.000 --> 01:32:10.000]  глобальные монотонные временные метки.
[01:32:10.000 --> 01:32:12.000]  Если вы живете в одном дата-центре, то, может быть, вам
[01:32:12.000 --> 01:32:14.000]  это все не нужно, потому что вам достаточно
[01:32:14.000 --> 01:32:16.000]  вот этого самого Timestep Oracle,
[01:32:16.000 --> 01:32:18.000]  который использует RAFT.
[01:32:18.000 --> 01:32:20.000]  Вот Google недостаточно.
[01:32:20.000 --> 01:32:22.000]  Хорошо, тогда
[01:32:22.000 --> 01:32:24.000]  со Spanner
[01:32:24.000 --> 01:32:26.000]  и что главное мы должны вынести,
[01:32:26.000 --> 01:32:28.000]  наверное, из этого, что сложность,
[01:32:28.000 --> 01:32:30.000]  которая у нас возникла здесь по пути,
[01:32:30.000 --> 01:32:32.000]  сложность в распределенных транзакциях,
[01:32:32.000 --> 01:32:34.000]  она возникает именно из-за распределенности
[01:32:34.000 --> 01:32:36.000]  и из-за многошардовости. И вот тут
[01:32:36.000 --> 01:32:38.000]  рождается двухфазный коммит. Такая абсолютно
[01:32:38.000 --> 01:32:40.000]  фундаментальная идея, которая много где
[01:32:40.000 --> 01:32:42.000]  используется, но и вообще
[01:32:42.000 --> 01:32:44.000]  не знаю,
[01:32:44.000 --> 01:32:46.000]  можно было бы даже сказать, что
[01:32:46.000 --> 01:32:48.000]  сам алгоритм ПАКСС похож
[01:32:48.000 --> 01:32:50.000]  на некоторый отказоустойчивый вариант
[01:32:50.000 --> 01:32:52.000]  двухфазного коммита. Короче,
[01:32:52.000 --> 01:32:54.000]  многие можно параллели проводить.
[01:32:54.000 --> 01:32:56.000]  Так вот, что мы сделаем дальше?
[01:32:56.000 --> 01:32:58.000]  После перерыва мы поговорим
[01:32:58.000 --> 01:33:00.000]  про другую систему,
[01:33:00.000 --> 01:33:02.000]  про другой подход к транзакциям,
[01:33:02.000 --> 01:33:04.000]  совершенно другой, и про
[01:33:04.000 --> 01:33:06.000]  систему Яндекс.ДБ, которая его использует.
[01:33:06.000 --> 01:33:08.000]  И в этом подходе
[01:33:08.000 --> 01:33:10.000]  мы все еще хотим
[01:33:10.000 --> 01:33:12.000]  делать распределенные транзакции, наша цель,
[01:33:12.000 --> 01:33:14.000]  собственно, но мы хотим
[01:33:14.000 --> 01:33:16.000]  побороться с двухфазным коммитом.
[01:33:16.000 --> 01:33:18.000]  Нам нравится однофазный коммит,
[01:33:18.000 --> 01:33:20.000]  который был в одношардовых транзакциях,
[01:33:20.000 --> 01:33:22.000]  когда мы просто говорим «коммит».
[01:33:22.000 --> 01:33:24.000]  Здесь появляется лишняя фаза, и мы разберемся,
[01:33:24.000 --> 01:33:26.000]  каким причинам эта фаза появляется,
[01:33:26.000 --> 01:33:28.000]  насколько они фундаментальные,
[01:33:28.000 --> 01:33:30.000]  и как можно, если мы
[01:33:30.000 --> 01:33:32.000]  исправим эти причины, что-то пооптимизировать.
[01:33:32.000 --> 01:33:34.000]  Это совершенно альтернативный
[01:33:34.000 --> 01:33:36.000]  дизайн, который был придуман, ну, примерно
[01:33:36.000 --> 01:33:38.000]  в то же время,
[01:33:38.000 --> 01:33:40.000]  и на свете пока очень мало мест,
[01:33:40.000 --> 01:33:42.000]  где его смогли применить. Вот Яндекс —
[01:33:42.000 --> 01:33:44.000]  это как раз то место, где такой дизайн
[01:33:44.000 --> 01:33:46.000]  написали.
[01:33:46.000 --> 01:33:48.000]  Ну и, что забавно,
[01:33:48.000 --> 01:33:50.000]  Яндекс.ДБ и Google Spanner — они
[01:33:50.000 --> 01:33:52.000]  как открытые системы,
[01:33:52.000 --> 01:33:54.000]  недоступен исходный код,
[01:33:54.000 --> 01:33:56.000]  но при этом и ту, и другую
[01:33:56.000 --> 01:33:58.000]  систему можно использовать в облаках.
[01:33:58.000 --> 01:34:00.000]  Яндекс.ДБ — это система хранения для Яндекс.Облака,
[01:34:00.000 --> 01:34:02.000]  Google Spanner — это система хранения
[01:34:02.000 --> 01:34:04.000]  для Google Cloud.
[01:34:08.000 --> 01:34:10.000]  Так что тут много параллелей между ними.
[01:34:10.000 --> 01:34:12.000]  Ну, давайте сделаем прерыв, после этого
[01:34:12.000 --> 01:34:14.000]  мы продолжим.

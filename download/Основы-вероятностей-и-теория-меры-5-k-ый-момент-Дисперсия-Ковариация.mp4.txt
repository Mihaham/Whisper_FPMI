[00:00.000 --> 00:10.600]  Что мы с вами не успели доказать в прошлый раз, неравенство
[00:10.600 --> 00:22.440]  к ошибке Николаевскому для математического ожидания,
[00:22.440 --> 00:25.920]  оно было в таком виде, что математическое ожидание
[00:25.920 --> 00:29.240]  модуля произведения случайных причин в квадрате будет
[00:29.240 --> 00:33.360]  меньше либо равно, чем произведение мат ожиданий
[00:33.360 --> 00:35.080]  квадратов этих случайных величин.
[00:35.080 --> 00:37.400]  В соответствии на первый случай мы с вами аккуратно
[00:37.400 --> 00:46.800]  разобрали, когда ни один из этих квадратов не равен
[00:46.800 --> 00:47.800]  нулю.
[00:47.800 --> 00:49.440]  Мы там рассматривали специальные случайные величины, которые
[00:49.440 --> 00:56.280]  получались из ксиэты делением на корень вот из этих величин.
[00:56.360 --> 01:00.400]  Мы с вами недоразобрали случай, когда кто-то равен
[01:00.400 --> 01:02.600]  нулю, но без ограничения общности можно считать то,
[01:02.600 --> 01:07.160]  что пускай от ожидания квадрата кси у нас будет равно нулю.
[01:07.160 --> 01:11.400]  Верно следующее замечание.
[01:11.400 --> 01:17.720]  Оно почти очевидное, но давайте это приговорим.
[01:17.720 --> 01:21.840]  Что если математическое ожидание случайной величины
[01:21.840 --> 01:26.200]  z равно нулю, и при этом случайная величина z у вас больше
[01:26.200 --> 01:30.240]  либо равна нуля, то что можно утверждать про случайную
[01:30.240 --> 01:33.240]  величину z?
[01:33.240 --> 01:39.080]  Ну, почти.
[01:39.080 --> 01:43.600]  Причем это почти имеет в соответствии почти наверное.
[01:43.600 --> 01:47.320]  То есть вы должны осознать сейчас, что почти наверное
[01:47.320 --> 01:48.320]  это научный термин.
[01:48.320 --> 01:53.080]  Обычно народ так радуется, когда впервые слышат это
[01:53.080 --> 01:54.760]  понятие, что оно означает.
[01:55.760 --> 01:58.320]  То есть вероятность того, что случайная величина
[01:58.320 --> 02:00.800]  z равна нулю, равна единице.
[02:00.800 --> 02:09.440]  И в будущем у нас всегда будет, то есть что-то выплывано
[02:09.440 --> 02:11.760]  почти наверное, это значит, что вероятность того, что
[02:11.760 --> 02:16.920]  выплывано то, что утверждается, это один.
[02:16.920 --> 02:20.000]  Особенно заключается в том, что никакими средствами
[02:20.000 --> 02:23.720]  теории вероятности вы не сможете почувствовать
[02:23.720 --> 02:26.320]  вещь, которая изменена почти наверное.
[02:26.320 --> 02:29.640]  Изменена почти наверное как-то безграмотно звучит.
[02:29.640 --> 02:31.280]  Наверное, это по сути.
[02:31.280 --> 02:32.280]  Ну, понятно, да?
[02:32.280 --> 02:34.640]  То есть, предположим, у вас есть две случайные величины,
[02:34.640 --> 02:35.960]  которые одинаковые почти наверное.
[02:35.960 --> 02:38.920]  То есть множество омек, на которых они отличаются,
[02:38.920 --> 02:39.920]  оно равно нулю.
[02:39.920 --> 02:41.280]  Так вот, никакими средствами теории вероятности вы не
[02:41.280 --> 02:42.520]  сможете это почувствовать.
[02:42.520 --> 02:43.520]  Идея ясна?
[02:43.520 --> 02:46.800]  Поэтому в будущем такие случайные величины вы будете
[02:46.800 --> 02:49.040]  называть эквивалентными, в теории вам все это будет
[02:49.040 --> 02:51.720]  рассказываться.
[02:51.720 --> 02:53.600]  Встрекайте к этим понятиям, кивайте.
[02:53.600 --> 02:54.600]  Хорошо?
[02:54.600 --> 02:55.600]  Замечательно.
[02:55.600 --> 02:56.600]  Так, доказательства.
[02:56.600 --> 03:01.520]  Вот эта история, она простейшая, поскольку, я напомню, мы
[03:01.520 --> 03:04.600]  умеем с вами строить сейчас только дискретные модели.
[03:04.600 --> 03:07.720]  Ну, другие не научились, мы выяснили то, что математического
[03:07.720 --> 03:09.800]  аппарата нам для этого недостаточно.
[03:09.800 --> 03:14.400]  Поэтому для нас с вами математическое ожидание случайных Z это
[03:14.400 --> 03:20.440]  бесконечный ряд, который состоит из слагаемых,
[03:20.520 --> 03:23.040]  каждая из которых имеет вид такого произведения.
[03:23.040 --> 03:25.480]  Ну, то есть значение случайных значений в данной точке
[03:25.480 --> 03:27.760]  омега на вероятность данной точки омега.
[03:27.760 --> 03:28.760]  Ага.
[03:28.760 --> 03:33.760]  Теперь, что мы видим, поскольку у нас случайно, господи,
[03:33.760 --> 03:36.040]  извините меня за это, но поскольку у меня случайная
[03:36.040 --> 03:41.200]  значения не отрицательная, вот здесь вот у меня тоже
[03:41.200 --> 03:45.600]  больше либо равно нуля, ну вот, то что мы получаем?
[03:45.600 --> 03:49.600]  То есть у нас сумма не отрицательных слагаемых, которая оказалась
[03:49.680 --> 03:50.680]  равна нулю.
[03:50.680 --> 03:55.680]  Как такое возможно?
[03:55.680 --> 03:56.680]  Все слагаемые ноль.
[03:56.680 --> 03:57.680]  Все.
[03:57.680 --> 03:58.680]  Поэтому у нас два варианта.
[03:58.680 --> 04:02.880]  Ну вот, либо у вас Z равно нулю, либо у вас вероятность
[04:02.880 --> 04:03.880]  равна нулю.
[04:03.880 --> 04:06.800]  Можно я не буду вот отсюда получать вот это?
[04:06.800 --> 04:10.440]  Ну, то есть как бы мы собираем вместе все омега маленькие
[04:10.440 --> 04:13.800]  такие, что Z у вас больше нуля, вероятность каждого
[04:13.800 --> 04:16.240]  такого омега маленького ноль, их счетное число,
[04:16.240 --> 04:17.880]  ну значит и сумма их тоже будет ноль.
[04:17.880 --> 04:20.800]  То есть вероятность, когда Z строго больше нуля, она
[04:20.800 --> 04:21.800]  ноль.
[04:21.800 --> 04:24.320]  Но это значит, что вероятность, когда Z равна нулю, она единичка,
[04:24.320 --> 04:25.880]  потому что всегда выполнил вот это.
[04:25.880 --> 04:27.280]  Все понял, что я сказал?
[04:27.280 --> 04:28.280]  Покевайте как-то.
[04:28.280 --> 04:29.280]  Ну, просто.
[04:29.280 --> 04:30.280]  Окей.
[04:30.280 --> 04:34.320]  Соответственно, что мы можем сказать, то что если в силу
[04:34.320 --> 04:37.000]  этого замечания, если математическое ожидание квадрат случайной
[04:37.000 --> 04:39.920]  значения равна нулю, значит Z квадрат может, извините,
[04:39.920 --> 04:40.920]  Прокси квадрат.
[04:40.920 --> 04:41.920]  Мы что можем сказать?
[04:41.940 --> 04:48.320]  Она почти, наверное, ноль.
[04:48.320 --> 04:52.200]  Ну а теперь давайте посмотрим на вот это вот математическое
[04:52.200 --> 04:53.200]  ожидание.
[04:53.200 --> 04:55.920]  Если случайно значена Кси, почти, наверное, ноль, то
[04:55.920 --> 04:58.480]  что можно сказать про это случайно значение?
[04:58.480 --> 05:01.960]  Она тоже почти, наверное, ноль.
[05:01.960 --> 05:03.240]  Это ясно?
[05:03.240 --> 05:06.520]  Потому что всегда, когда Кси 0, это произведение тоже
[05:06.520 --> 05:08.540]  ноль, то есть тут может только увеличится количество
[05:08.540 --> 05:10.720]  ситуации, когда это произведение равно нулю.
[05:10.720 --> 05:15.240]  А если у вас случайно начинает почти наверно ноль, то ее математическое ожидание, очевидно, равно нолю.
[05:17.680 --> 05:20.880]  Четыре д. Тут ноль, тут ноль. Ура! Все доказали.
[05:25.240 --> 05:30.480]  В тервере будет отдельное обсуждение вот этих вот вещей почти наверно.
[05:30.480 --> 05:32.800]  Во-первых, вы должны привыкнуть к терму, почти наверно.
[05:34.800 --> 05:39.880]  Не в тервере, а в теории меры это будет почти всюду. То есть свойство, которое выполнено.
[05:41.120 --> 05:42.560]  Везде.
[05:42.560 --> 05:47.120]  Свойство, которое выполнено почти всюду, это будет свойство, что там, где оно не выполнено, равно нулю.
[05:47.400 --> 05:50.120]  Потому что у нас в терверах хорошо, у нас единичка, а
[05:51.160 --> 05:54.520]  меры же, она не обязательно единичка будет. Сейчас понимаете, что я говорю, да?
[05:55.920 --> 06:04.000]  У вас такие недоуменные лица. Слушайте, у меня огромная просьба. Смотрите, вот я чего-то говорю, говорю, говорю, говорю, потом все, да, закончили, ко мне подходит толпа людей.
[06:04.000 --> 06:06.000]  Слушайте, мы вот там не поняли.
[06:06.000 --> 06:07.720]  Понимаете, есть как бы
[06:07.720 --> 06:15.360]  базовые вещи. Если вы чего-то не поняли, есть точно еще один человек, который точно не понял, он вам будет благодарен, если вы зададите вопрос по поводу этого непонятного. Это ясно?
[06:17.640 --> 06:24.600]  Поэтому, пожалуйста, задавайте вопросы по ходу. Договорились с Киваем. Отлично.
[06:26.880 --> 06:28.880]  Напросился, да.
[06:28.880 --> 06:33.880]  Вы говорите почти нереально. Почему вы не впрасно говорили, что это равно 0?
[06:33.880 --> 06:43.880]  Ну, потому что это неправда. То есть у тебя может получиться история, то, что есть какая-то одна точечка омега, то есть вот у тебя есть омега большое, у тебя есть какая-то одна точка.
[06:43.880 --> 06:45.880]  Вот вероятность этой точечки равна нулю.
[06:47.880 --> 06:53.880]  В дискретных случаях у нас такое не бывало, но у нас это бывало в ситуации, когда мы в качестве омега рассматриваем отрезок от 0 до 1.
[06:54.880 --> 07:03.880]  То есть ты можешь рассматривать, мы не допостроили здесь математическую модель, мы не очень понимаем, что такое множество событий, но мы же это сделаем рано или поздно.
[07:03.880 --> 07:12.880]  Вы мне сейчас поверите, что если я рассматриваю две случайные величины, кси тождественной равны нулю на отрезке, и это, которая есть в функции дирекля.
[07:13.880 --> 07:18.880]  Что у нас там было? Единичка в рациональных или иррациональных?
[07:19.880 --> 07:23.880]  Один в рациональных.
[07:24.880 --> 07:26.880]  Это функция дирекля.
[07:27.880 --> 07:32.880]  То есть она принимает значение 0 в иррациональных точках.
[07:37.880 --> 07:39.880]  Единичка в рациональных точках.
[07:39.880 --> 07:41.880]  Естественно пересеченные с 0 и 1.
[07:48.880 --> 07:53.880]  С точки зрения меры Лебега, мера рациональных точек у нас равна нулю.
[07:53.880 --> 07:58.880]  То есть получается, что вот эта кси равна эти почти наверно.
[07:59.880 --> 08:08.880]  И мысль в том, что никакими средствами теории вероятности, тем аппаратам, которые будут построены, вы не сможете почувствовать их отличия друг от друга.
[08:08.880 --> 08:11.880]  Вероятности там все будут одинаковые.
[08:11.880 --> 08:16.880]  Отсюда все характеристики, которые завязаны на вероятностях на распределениях, они будут одинаковые.
[08:16.880 --> 08:18.880]  Никак не почувствуете их друг от друга.
[08:18.880 --> 08:20.880]  При этом это сильно разные штуки.
[08:21.880 --> 08:22.880]  Идея ясна?
[08:22.880 --> 08:27.880]  Поэтому просто написать равна нулю мы не можем, потому что это неправда.
[08:28.880 --> 08:30.880]  Сейчас нормально, да? Хорошо.
[08:32.880 --> 08:34.880]  Но еще раз, центральная мысль заключается в чем?
[08:35.880 --> 08:43.880]  Нас именно во всей этой истории теория вероятности кси как функции от омега маленького не интересует.
[08:43.880 --> 08:48.880]  То есть мы с ней работаем как функции от омега маленького, но просто она так устроена.
[08:48.880 --> 08:51.880]  Но она нас не интересует, нас всегда интересует вероятность.
[08:51.880 --> 08:56.880]  Смысл заключается в том, что если у вас омега маленькая известна, то есть эксперимент уже имел место быть,
[08:56.880 --> 09:02.880]  то телевизора тут не нужна, уже все, вы в ДТП или вы в чем-то плохом, или наоборот в чем-то хорошем, оно уже есть.
[09:04.880 --> 09:06.880]  Поэтому нас всегда интересует вероятность.
[09:09.880 --> 09:11.880]  Так, спасибо за вопрос больше.
[09:11.880 --> 09:14.880]  Вот я уверен, что были люди, которые тоже хотели это услышать.
[09:16.880 --> 09:20.880]  Соответственно, что дальше? Следующий объект, который возникает,
[09:20.880 --> 09:24.880]  ну понятие, это катый момент, случайная величина.
[09:26.880 --> 09:33.880]  Потому что ненавиственно у Каши Банюковского мы с вами закрывали вопрос о свойствах математического ожидания от случайной величины.
[09:36.880 --> 09:39.880]  Соответственно, что касается катого момента.
[09:43.880 --> 09:45.880]  Так, первая вещь на сегодня.
[09:45.880 --> 09:47.880]  Так, первая вещь на сегодня.
[09:47.880 --> 09:49.880]  Катый момент.
[09:50.880 --> 09:52.880]  То есть на определение.
[09:52.880 --> 09:54.880]  Катый момент, извините,
[09:57.880 --> 09:59.880]  случайной величиной кси
[10:01.880 --> 10:04.880]  называется математическое ожидание кси в степени К.
[10:04.880 --> 10:07.880]  Ну как так, не очень интеллектуально.
[10:09.880 --> 10:13.880]  То есть обычный мат ожидания – это первый момент нашей случайной величины.
[10:13.880 --> 10:15.880]  Какие замечания?
[10:15.880 --> 10:17.880]  Замечания первые.
[10:17.880 --> 10:21.880]  То, что мы с вами об этом говорили, я еще раз обращу на это внимание.
[10:21.880 --> 10:31.880]  То, что катый момент у вас существует тогда и только тогда, когда у вас существует мат ожидания кси в степени К под модулем.
[10:32.880 --> 10:39.880]  Мы это с вами обсуждали, причем это верно будет не только в случае, когда мы с вами разговариваем,
[10:39.880 --> 10:47.880]  мы это с вами обсуждали, причем это верно будет не только в случае, когда мы с вами работаем с дискретными вероятностными моделями.
[10:47.880 --> 10:49.880]  Это будет верно всегда.
[10:49.880 --> 10:53.880]  Если вы расписываете математическое ожидание, у вас получается ряд, который не индексирован.
[10:54.880 --> 10:56.880]  Сейчас подкивайте, чтобы понятно.
[10:57.880 --> 11:02.880]  А раз он не индексирован, то вопрос об условной исходимости ряда вообще не стоит, поэтому вот так.
[11:03.880 --> 11:06.880]  Это первая мысль, которая есть, и вторая мысль, которая есть.
[11:10.880 --> 11:16.880]  Все ли понимают, что у нас мат ожидания может не существовать?
[11:17.880 --> 11:23.880]  Ну, если у нас бесконечное число значений случайной величины, этот ряд может, ну, во-первых, он может зайти из бесконечности,
[11:23.880 --> 11:26.880]  во-вторых, он может как-то условно сходиться, а значит, он нам не подходит.
[11:26.880 --> 11:28.880]  Это всем понятно, да?
[11:28.880 --> 11:33.880]  Возникает вопрос о том, о том, что как связано вообще существование катых моментов между собой.
[11:33.880 --> 11:38.880]  Тут приходит на помощь неравенство промо от ожидания.
[11:38.880 --> 11:39.880]  У нас их будет много.
[11:39.880 --> 11:43.880]  То, которое я сейчас формулирую, доказывать не буду, вы его докажете на тервере.
[11:44.880 --> 11:48.880]  Но вот, я вас формулирую для того, чтобы закрыть вопрос о том, как связаны катые моменты между собой.
[11:49.880 --> 11:51.880]  Называется оно неравенство Лепунова.
[11:57.880 --> 12:02.880]  Лепунов – это наш терверщик советский, у него там блестящие результаты, там было все здорово.
[12:03.880 --> 12:09.880]  Соответственно, выглядит так, что если у вас чиселки s и t связаны с таким соотношением,
[12:09.880 --> 12:12.880]  то мот ожидания будут связаны с следующим соотношением.
[12:12.880 --> 12:18.880]  Мот ожидания кси в степени s в степени 1s t будет меньше либо равно
[12:18.880 --> 12:24.880]  мот ожидания модуль кси в степени t в степени 1t t.
[12:24.880 --> 12:33.880]  Ну, понятно то, что если кто-то из них в какой-то момент не существует,
[12:33.880 --> 12:35.880]  у вас просто соответствующее значение уйдет в бесконечность.
[12:39.880 --> 12:41.880]  Какой отсюда вывод мы можем сделать?
[12:47.880 --> 12:49.880]  Какой вывод мы отсюда можем сделать?
[12:54.880 --> 13:04.880]  Если существует каждая катая момент, то существуют все моменты порядками ниже.
[13:04.880 --> 13:06.880]  Это понятно?
[13:06.880 --> 13:17.880]  То есть, если существует катый момент, следовательно, существует k минус этой моменты.
[13:21.880 --> 13:23.880]  Покрупнее я буду стараться.
[13:25.880 --> 13:29.880]  Ну, смотри, предположим, у тебя существует пятый момент.
[13:29.880 --> 13:31.880]  Ну, то есть, ряд сошел в свой раз.
[13:31.880 --> 13:34.880]  Значит, четвертый, третий, второй и первый тоже существуют.
[13:38.880 --> 13:40.880]  Почему по неравенству?
[13:40.880 --> 13:43.880]  То есть, смотри, ты получаешь, берешь вот здесь s и t.
[13:43.880 --> 13:46.880]  То есть, почему у тебя может не получиться какой-то момент?
[13:46.880 --> 13:48.880]  У тебя расходится соответствующий ряд.
[13:48.880 --> 13:50.880]  Ну, то есть, расходится, уходит в бесконечность.
[13:50.880 --> 13:52.880]  У нас тут модули.
[13:52.880 --> 13:53.880]  О чем говорит вот это неравенство?
[13:53.880 --> 14:01.880]  Если у тебя при t равном 5 вот этот ряд сошелся, значит, при s равном 4, 3, 2 и так далее это тоже сойдется,
[14:01.880 --> 14:03.880]  потому что вера на это неравенство.
[14:03.880 --> 14:05.880]  Она не может себе позволить уйти в бесконечность.
[14:06.880 --> 14:07.880]  Хорошо?
[14:07.880 --> 14:10.880]  K минус i.
[14:13.880 --> 14:17.880]  Я могу даже написать i от единички до k минус 1.
[14:18.880 --> 14:19.880]  Так? Хорошо.
[14:20.880 --> 14:21.880]  Ну, догадывайте.
[14:22.880 --> 14:23.880]  Ну вот.
[14:32.880 --> 14:35.880]  s, 1s, t, t, 1t, t.
[14:37.880 --> 14:40.880]  Это какие-то чиселки. s и t это какие-то фиксированные чиселки.
[14:40.880 --> 14:42.880]  Будет выполнено такое неравенство.
[14:42.880 --> 14:44.880]  Я тут...
[14:45.880 --> 14:46.880]  Я же в школе работаю.
[14:46.880 --> 14:48.880]  У меня сейчас не с ними тема.
[14:48.880 --> 14:52.880]  Мы проходим векторы меткоординат в 9 классе во второй школе.
[14:53.880 --> 14:59.880]  Я как-то пытаюсь какую-то наукообразность всю эту школьную историю навести,
[14:59.880 --> 15:02.880]  чтобы хоть как-то оправдать, что я там вузовский преподаватель, работаю в школе.
[15:03.880 --> 15:04.880]  Я зашел с чего?
[15:04.880 --> 15:06.880]  Я им дал общее определение линейного пространства.
[15:07.880 --> 15:08.880]  Ну там...
[15:10.880 --> 15:12.880]  Сейчас я что-то не понял, к чему это.
[15:12.880 --> 15:14.880]  Ну ладно, опустим этот смех.
[15:14.880 --> 15:15.880]  Опустим этот смех.
[15:16.880 --> 15:17.880]  Ну вот.
[15:17.880 --> 15:20.880]  Я говорю, чуваки, ну вот у вас будет линал, вы будете заниматься линейным пространством.
[15:20.880 --> 15:21.880]  Линейное пространство, что там?
[15:21.880 --> 15:24.880]  Абстрактное множество, две операции, которые утворяют вот этим свойством, аксиомом.
[15:25.880 --> 15:30.880]  Мы с вами разберем там, ну вот, конкретный пример линейного пространства, все будем проверять.
[15:31.880 --> 15:32.880]  Проходит месяц.
[15:32.880 --> 15:33.880]  Ну я же понимаю, что они ни фига не поняли.
[15:34.880 --> 15:35.880]  Ну вот.
[15:35.880 --> 15:36.880]  Я это еще раз проговариваю.
[15:36.880 --> 15:39.880]  Они говорят, Иван Георгиевич, вы вообще что-то нам...
[15:39.880 --> 15:41.880]  Все, вы доказываете аксиомы?
[15:42.880 --> 15:48.880]  Ну то есть они не поняли, почему мы те свойства, которые зашиты в определении линейного пространства,
[15:48.880 --> 15:52.880]  почему мы для множества направленных отрезков на плоскости почему-то начали доказывать.
[15:52.880 --> 15:53.880]  Это же аксиом.
[15:54.880 --> 15:55.880]  Ну вот.
[15:55.880 --> 15:56.880]  К чему я это?
[15:57.880 --> 16:03.880]  Все присутствующие понимают то, что когда мы с вами аккуратно построим весь необходимый математический аппарат,
[16:04.880 --> 16:07.880]  измеряемых пространств и измеряемых функций, на нем интегра, олибега,
[16:07.880 --> 16:11.880]  и когда вы придете на теорию вероятностей, с чего начнется тервер.
[16:11.880 --> 16:15.880]  Он скажет, пусть у нас есть вероятностное пространство,
[16:15.880 --> 16:17.880]  Омега ФП, произвольное.
[16:18.880 --> 16:22.880]  И погнали формулировать все результаты теории вероятностей на произвольном линейном пространстве.
[16:23.880 --> 16:28.880]  Все, что делаем сейчас мы, мы делаем в частном случае на дискретном вероятностном пространстве.
[16:29.880 --> 16:30.880]  Это частный случай.
[16:30.880 --> 16:32.880]  Но результаты, естественно, все будут переноситься туда.
[16:34.880 --> 16:35.880]  Это же вы понимаете, да?
[16:36.880 --> 16:40.880]  В прошлый раз просто кто-то задал вопрос о том, что, а вот то, что вы формулируете, там будет работать?
[16:40.880 --> 16:41.880]  Ну да, конечно.
[16:42.880 --> 16:43.880]  Хорошо?
[16:44.880 --> 16:45.880]  Ладно.
[16:46.880 --> 16:48.880]  Так, с катом моментом все.
[16:49.880 --> 16:52.880]  Следующий объект, который нам нужен, это дисперсия случайной величины.
[16:54.880 --> 16:56.880]  Дисперсия случайной величины.
[16:57.880 --> 17:01.880]  В чем целеполагание введения такого объекта?
[17:02.880 --> 17:05.880]  Можно рассмотреть две случайные величины с такими распределениями.
[17:06.880 --> 17:10.880]  Я напомню то, что математическое ожидание это характеристика распределения случайной величины,
[17:11.880 --> 17:13.880]  а не случайной величины как функции от Омега.
[17:14.880 --> 17:16.880]  Надеюсь, все поняли, что я имел в виду.
[17:17.880 --> 17:22.880]  Поэтому, когда мы говорим о моментах, нас интересует всегда распределение.
[17:23.880 --> 17:26.880]  И поэтому вы же уже решали, скорее всего, задачки на случайную величину.
[17:27.880 --> 17:30.880]  Потому что всегда в условиях сказано, дана такая случайная величина с таким-то распределением.
[17:31.880 --> 17:34.880]  И редко вам задает случайную величину как функцию от Омега маленького.
[17:35.880 --> 17:38.880]  Но такое будет для упражнений, чтобы вы поработали.
[17:39.880 --> 17:43.880]  И в середине курса нам нужно, чтобы мы сигма-алгебр построили порожденные измеримыми функциями.
[17:44.880 --> 17:45.880]  А так всегда задают только распределение.
[17:46.880 --> 17:47.880]  Давайте рассмотрим две.
[17:48.880 --> 17:51.880]  То есть у вас случайная величина 1 принимает значение 1-1 с вероятностями 1-2.
[17:52.880 --> 17:56.880]  И случайная величина эта принимает значение 100 и минус 100 с вероятностями тоже 1-2.
[17:57.880 --> 18:00.880]  Понятно, что мат ожидания этих случайных величин одинаковый.
[18:02.880 --> 18:03.880]  Это очевидно.
[18:07.880 --> 18:10.880]  При этом видно, что разброс данных случайных величин огромный.
[18:12.880 --> 18:16.880]  И в этот момент кажется, что мат ожидания так себе характеристика.
[18:17.880 --> 18:22.880]  То есть у вас две сильно разные случайные начины, которые принимают разные значения, сильно отличающие друг друга.
[18:23.880 --> 18:25.880]  При этом эта характеристика у них одинаковая.
[18:26.880 --> 18:31.880]  И в этот момент логично возникает мысль о том, что хочется заиметь характеристику,
[18:32.880 --> 18:35.880]  которая будет показывать степень разброса случайной величины от своего среднего.
[18:37.880 --> 18:38.880]  Понятная история.
[18:39.880 --> 18:43.880]  Потому что если эта характеристика мала, это значит, что случайная величина от этой чиселки,
[18:44.880 --> 18:45.880]  которая есть мат ожидания, отстоит не сильно.
[18:46.880 --> 18:47.880]  Если она большая, значит она отстоит сильно.
[18:48.880 --> 18:49.880]  Уже какие-то выводы о случайной величине можно делать.
[18:50.880 --> 18:56.880]  Соответственно, мы вводим такую характеристику, называется она дисперсию случайной кси.
[18:57.880 --> 18:58.880]  И определяться она будет как?
[18:59.880 --> 19:00.880]  То есть вы берете свою случайную величину.
[19:03.880 --> 19:04.880]  Вам же нужен как?
[19:05.880 --> 19:06.880]  Степень разброса от среднего.
[19:07.880 --> 19:08.880]  Поэтому вы из нее вычитаете среднее.
[19:09.880 --> 19:10.880]  Дальше разброс.
[19:11.880 --> 19:12.880]  То есть вас интересует абсолютное значение.
[19:13.880 --> 19:15.880]  Поэтому я здесь возведу в квадрат, чтобы решить этот вопрос.
[19:16.880 --> 19:17.880]  Разброс влево, разброс вправо.
[19:17.880 --> 19:18.880]  Разброс вправо.
[19:19.880 --> 19:20.880]  И возьму среднее.
[19:21.880 --> 19:22.880]  То есть возьму от ожидания.
[19:23.880 --> 19:24.880]  Одна дисперсия случайной величины определяется так.
[19:25.880 --> 19:28.880]  Какие эмоции в этот момент должны возникнуть у слушателей?
[19:29.880 --> 19:30.880]  Ну это первое.
[19:31.880 --> 19:32.880]  Да.
[19:33.880 --> 19:34.880]  Ну муторно.
[19:35.880 --> 19:41.880]  Ну слушай, ну понимаешь, что-то на физтехе учиться не аясь, ездить далеко муторно.
[19:42.880 --> 19:43.880]  А что делать?
[19:44.880 --> 19:45.880]  Вопрос к Сталину.
[19:45.880 --> 19:46.880]  Вопрос к Сталину.
[19:47.880 --> 19:48.880]  Ну вот.
[19:49.880 --> 19:50.880]  Это одна история.
[19:51.880 --> 19:53.880]  А вторая история то, что это правильно.
[19:54.880 --> 19:55.880]  Почему квадрат?
[19:56.880 --> 19:57.880]  Логичнее всего модуль.
[19:58.880 --> 19:59.880]  Причем вслед за дисперсией.
[20:00.880 --> 20:03.880]  Водится понятие средне квадратического отклонения.
[20:05.880 --> 20:08.880]  Средне квадратическое отклонение.
[20:08.880 --> 20:11.880]  Которое определяется как корень дисперсии с такой аргументацией.
[20:12.880 --> 20:14.880]  То, что видно, что размерность не сходится.
[20:15.880 --> 20:16.880]  Ну так.
[20:17.880 --> 20:19.880]  То есть у вас и у ксиблок есть какая-то размерность,
[20:20.880 --> 20:22.880]  то в квадрате у вас выбираете среднее, то размерность не та.
[20:23.880 --> 20:24.880]  И надо извлечь квадратный корень.
[20:25.880 --> 20:31.880]  Ну и кажется историей то, что ну как-то действительно было логичнее навесить модуль.
[20:32.880 --> 20:34.880]  Ну потому что модуль это и в той же ситуации.
[20:34.880 --> 20:36.880]  Действительно было логичнее навесить модуль.
[20:37.880 --> 20:38.880]  Ну потому что модуль это и в точности есть.
[20:39.880 --> 20:41.880]  Расстояние кси от этой точки.
[20:42.880 --> 20:43.880]  И вы берете среднее из этих вот расстояний.
[20:45.880 --> 20:47.880]  Ответ на этот вопрос.
[20:48.880 --> 20:49.880]  Типа так математику устроено.
[20:50.880 --> 20:51.880]  Этот объект оказывается самым удобным.
[20:52.880 --> 20:54.880]  Сейчас, когда мы сформулируем его свойства,
[20:55.880 --> 20:56.880]  вы увидите прелести всего этого объекта,
[20:57.880 --> 21:00.880]  которых вообще нету у объекта, когда вы считаете математическое ожидание модуля.
[21:01.880 --> 21:03.880]  Интересно вслед за дисперсией еще вводится понятие
[21:04.880 --> 21:05.880]  аналогичных к этому моменту.
[21:06.880 --> 21:08.880]  Это центрированный катый момент.
[21:09.880 --> 21:11.880]  Центрированный катый момент.
[21:18.880 --> 21:19.880]  Определяется он...
[21:20.880 --> 21:21.880]  Так обозначается он...
[21:23.880 --> 21:24.880]  О господи!
[21:25.880 --> 21:26.880]  Я не помню.
[21:26.880 --> 21:30.880]  Ну, определяется он, соответственно, как мат ожидания кси
[21:31.880 --> 21:33.880]  минус мат ожидания кси в степени к.
[21:34.880 --> 21:35.880]  Ну, соответственно.
[21:36.880 --> 21:37.880]  Нет, там какое-то было обозначение.
[21:38.880 --> 21:42.880]  Там что-то типа того, что мат ожидания кси в степени к с черточкой.
[21:43.880 --> 21:45.880]  Слушайте, я боюсь наводять.
[21:46.880 --> 21:47.880]  Но с черточкой это выборочно.
[21:48.880 --> 21:50.880]  Без обозначений, в общем.
[21:51.880 --> 21:52.880]  Хорошо?
[21:52.880 --> 21:53.880]  Ну, погнали.
[21:54.880 --> 21:55.880]  Теперь свойства дисперсии.
[21:56.880 --> 21:57.880]  Свойства этого объекта, который есть.
[21:58.880 --> 21:59.880]  Да.
[22:00.880 --> 22:01.880]  Давайте последовательно.
[22:02.880 --> 22:03.880]  Вопрос.
[22:04.880 --> 22:05.880]  Да.
[22:06.880 --> 22:07.880]  Да.
[22:09.880 --> 22:10.880]  Давайте последовательно.
[22:11.880 --> 22:12.880]  Вопрос.
[22:29.880 --> 22:30.880]  В степень возводится функционально.
[22:30.880 --> 22:31.880]  Возводится функционально.
[22:32.880 --> 22:34.880]  У вас же изначально, случайно, величина, это что?
[22:35.880 --> 22:36.880]  Это функция.
[22:37.880 --> 22:38.880]  Из-за мега вл, правильно?
[22:39.880 --> 22:41.880]  То есть, когда вы определяете кси в кубе, вы что делаете?
[22:42.880 --> 22:43.880]  Вы сначала делаете вот так вот, да?
[22:44.880 --> 22:45.880]  А потом эту чиселку возводите в куб.
[22:54.880 --> 22:56.880]  Ну, у тебя может получиться так то, что там будут накладывать.
[22:57.880 --> 22:59.880]  Может быть, вот, например, я возьму кси в квадрате, да?
[23:00.880 --> 23:01.880]  У тебя могут накладываться.
[23:02.880 --> 23:06.880]  Ну, предположим, кси принимала значение плюс-минус единичка с вероятностями 1 четверть.
[23:07.880 --> 23:08.880]  Она будет принимать значение 1 с вероятностями 1 вторая.
[23:09.880 --> 23:12.880]  Но, идейно, как бы, это изначально функция.
[23:13.880 --> 23:14.880]  И когда вы возводите в степень, это функционально возводится.
[23:15.880 --> 23:17.880]  Ну, то есть, для того, чтобы посчитать значение кси в кубе,
[23:18.880 --> 23:19.880]  вам сначала надо посчитать значение кси, а потом позвести в куб.
[23:20.880 --> 23:21.880]  От конкретной точки омега.
[23:22.880 --> 23:24.880]  А как-то другой вопрос какой был?
[23:24.880 --> 23:26.880]  Мюката.
[23:29.880 --> 23:30.880]  Мюката, да, точно.
[23:31.880 --> 23:32.880]  Что-то такое было.
[23:33.880 --> 23:34.880]  Мюката.
[23:35.880 --> 23:38.880]  В общем, я не могу сказать, что это нужная вещь, если честно.
[23:50.880 --> 23:53.880]  Смотрите, теперь зачем вот эта история нужна?
[23:54.880 --> 24:01.880]  Потому что, если мат ожидания и дисперсия это вещи хорошие и понятные,
[24:02.880 --> 24:06.880]  но вот их физический смысл, то моменты, нафиг эта фигня нужна.
[24:07.880 --> 24:08.880]  Ну, то есть, первая реакция такая.
[24:09.880 --> 24:10.880]  Ну вот, технически.
[24:11.880 --> 24:13.880]  То есть, они, то есть, там вылезают вводе того,
[24:14.880 --> 24:17.880]  что вы иногда, когда будете раскладывать там в ряд специальные функции,
[24:18.880 --> 24:22.880]  у вас там коэффициенты будут, вот эти каты и моменты.
[24:24.880 --> 24:25.880]  Но это и на тервере вы будете происходить.
[24:26.880 --> 24:27.880]  То есть, смотрите.
[24:30.880 --> 24:34.880]  Сейчас я текст придумываю, как тут аккуратно сказать.
[24:45.880 --> 24:48.880]  Нет примеров просто в той математике, которую вот вы делаете.
[24:49.880 --> 24:53.880]  Я когда, то есть, у меня самая такая вещь, когда я больше всего прокачался в математике,
[24:54.880 --> 24:56.880]  я к аспирантуре готовил госэкзамен себе с дачи.
[24:57.880 --> 24:59.880]  Потому что нам там нужно было вот весь тервер,
[25:00.880 --> 25:02.880]  который рассказывали в университете и потом в аспирантуре,
[25:03.880 --> 25:04.880]  вот его весь выучить.
[25:05.880 --> 25:06.880]  А вы же, как люди, которые проходят через сессию,
[25:07.880 --> 25:09.880]  вы понимаете, что когда вы садитесь, готовитесь к экзамену,
[25:10.880 --> 25:12.880]  у вас все вот эти вот вещи, которые вы тут поняли, там поняли,
[25:13.880 --> 25:15.880]  они складываются в единую картину дисциплины.
[25:16.880 --> 25:20.880]  Это, собственно, моя главная претензия к Выше школы экономики заключается в том,
[25:20.880 --> 25:23.880]  что они немножечко вот убили сессию.
[25:24.880 --> 25:25.880]  Именно как вот момент, когда вы садитесь,
[25:26.880 --> 25:30.880]  и 4 дня занимаетесь одной дисциплиной, и у вас она складывается в единое целое.
[25:35.880 --> 25:37.880]  И в тот момент, извините, я просто издалека,
[25:38.880 --> 25:39.880]  я не знаю, как по-другому объяснить.
[25:42.880 --> 25:45.880]  Там вот был один раздел, я прям видел, как развивалась,
[25:46.880 --> 25:49.880]  поскольку тот тервер, который учился, он в рамках XX века шел,
[25:50.880 --> 25:53.880]  я видел, как люди приходили к той мысли, вот постепенно,
[25:54.880 --> 25:56.880]  к тем результатам, которые они сделали,
[25:57.880 --> 25:58.880]  и там было все понятно, как они пришли к этой мысли.
[25:59.880 --> 26:01.880]  Так, кивайте. Хорошо? Окей.
[26:02.880 --> 26:06.880]  Ну вот, потом я открываю, есть такая книжка Булинского-Шираева,
[26:07.880 --> 26:08.880]  «Случайные процессы», скорее всего, она будет вам рекомендована
[26:09.880 --> 26:10.880]  в списке литературы.
[26:11.880 --> 26:14.880]  Шираев – это тот, кто ученик Кумогорову, про которую я уже говорил,
[26:15.880 --> 26:17.880]  Булинский, это тоже у нас профессор тервера.
[26:17.880 --> 26:21.880]  И там как, там идет теорема, и там доказательство теоремы,
[26:22.880 --> 26:25.880]  оно такое техничное и очень быстрое, но совершенно непонятно,
[26:26.880 --> 26:28.880]  как люди придумали, то есть это доказательство совершенно
[26:29.880 --> 26:32.880]  не раскрывает содержание тех понятий, которые там заложены.
[26:33.880 --> 26:36.880]  Ну то есть, как вот математика развивалась, а потом говорят,
[26:37.880 --> 26:39.880]  чуваки, слушайте, я придумал классное доказательство этого же факта,
[26:40.880 --> 26:43.880]  оно быстрее, лучше, за счет того, что оно быстрее и лучше,
[26:43.880 --> 26:46.880]  оно уходит в массы, и студент начинает учить,
[26:47.880 --> 26:51.880]  и студентов начинают через это учить, но при этом теряется
[26:52.880 --> 26:54.880]  понимание, как люди пришли к этим результатам.
[26:55.880 --> 26:56.880]  Сейчас понятно немножечко стало?
[26:57.880 --> 27:01.880]  Вот теперь смотрите, в теории вероятности есть отдельные
[27:02.880 --> 27:06.880]  математические приемы, с помощью которых эти результаты доказываются.
[27:07.880 --> 27:09.880]  Например, метод характеристических функций, с помощью которых
[27:10.880 --> 27:12.880]  очень легко и быстро доказывается центральная предельная теорема.
[27:13.880 --> 27:15.880]  О ней, я надеюсь, сегодня не успею, в конце буду говорить.
[27:16.880 --> 27:18.880]  Мы сегодня должны с тервером, по идее, закончить.
[27:19.880 --> 27:22.880]  Ну вот, изначально там центральная предельная теорема,
[27:23.880 --> 27:26.880]  она доказывалась там, сложно, тяжело, вот для медумиальной схемы,
[27:27.880 --> 27:30.880]  ну вот, и прямо вот ты, так прочувствовав это доказательство,
[27:31.880 --> 27:34.880]  понимаешь этот результат, а через метод характеристических функций
[27:35.880 --> 27:37.880]  ты получаешь все очень быстро, легко, при этом мало понимаешь
[27:38.880 --> 27:39.880]  содержание результата. Объяснил?
[27:39.880 --> 27:42.880]  То есть мысль в чем? Тервер очень техничная наука,
[27:43.880 --> 27:49.880]  поэтому, и вот эти, в частности, карты и моменты, они нужны для этой техники.
[27:50.880 --> 27:53.880]  Не очень понятно, какой кроется, может быть, за ними смысл
[27:54.880 --> 27:56.880]  и почему доказательства идут именно так, как они идут,
[27:57.880 --> 28:00.880]  но я объяснил, почему так будет происходить в тервере. Хорошо?
[28:01.880 --> 28:07.880]  Ладно. Это просто технический аппарат, который нам нужен.
[28:07.880 --> 28:11.880]  Результаты, результаты, а дальше, понимаешь, мы же математики,
[28:12.880 --> 28:15.880]  нам эти результаты надо доказать, причем так, чтобы все было аккуратно.
[28:16.880 --> 28:19.880]  Ну вот, а дальше как? То есть есть тяжелые доказательства,
[28:20.880 --> 28:23.880]  но которые, оно выставлено поколениями, и понятно, почему оно такое.
[28:24.880 --> 28:26.880]  А потом кто-то придумал, слушайте, а зачем мы так мучаемся,
[28:27.880 --> 28:32.880]  можно вот так вот зайти. То есть математики сейчас вообще адовая история,
[28:32.880 --> 28:36.880]  тот вот сейчас Александр Скрипченко, она стала деканом матфака вышки,
[28:37.880 --> 28:39.880]  может быть, кто-то слышал. Я ее даже учил какой-то момент
[28:40.880 --> 28:42.880]  во второй школе. Ну вот, она занимается теорией узлов.
[28:43.880 --> 28:47.880]  Оказывается, что эта теория узлов, она связана со всеми разделами математики,
[28:48.880 --> 28:50.880]  которая вообще никакого, казалось бы, теория узлов,
[28:51.880 --> 28:55.880]  на мой взгляд, это вообще фигня какая-то. Вот эти узлы, можно так, а можно так.
[28:56.880 --> 28:59.880]  То есть детям рассказывать, она очень популярная, она детям рассказывать,
[28:59.880 --> 29:02.880]  шикарно рассказывать, все здорово. Там какая-то высокая математика,
[29:03.880 --> 29:05.880]  которая потом увязана в различные разделы математики.
[29:06.880 --> 29:10.880]  Я не понимаю, как это происходит. Ну как бы я наукой давно не занимаюсь,
[29:11.880 --> 29:15.880]  и мне сложно представить, как они умудряются связывать различные разделы математики,
[29:16.880 --> 29:18.880]  которые, казалось бы, совсем друг с другом не связаны.
[29:19.880 --> 29:24.880]  С помощью узлов, да, понятно. Ну вот. То есть это, на самом деле,
[29:24.880 --> 29:30.880]  я говорил в самом начале об этом, о том, что математика,
[29:31.880 --> 29:36.880]  если философски опускаться в эту историю, это очень сложно.
[29:37.880 --> 29:44.880]  Я ответил на вопросы, хорошо? Ладно, все, я сейчас уйду далеко,
[29:45.880 --> 29:49.880]  была боль в стороне. Давайте, свойство дисперсии.
[29:49.880 --> 29:53.880]  Свойство дисперсии. Так, соответственно, первое.
[29:54.880 --> 29:59.880]  Понятно, что считать дисперсию по определению сложно, просто потому что,
[30:00.880 --> 30:01.880]  ведь для того, чтобы посчитать математическое ожидание,
[30:02.880 --> 30:04.880]  мы с вами говорили, нам нужно знать распределение случайной величины.
[30:05.880 --> 30:07.880]  У вас как получилась та случайная величина, которая стоит под скобочкой?
[30:08.880 --> 30:11.880]  Вызвали свою случайную величину, вычили чиселку и возвели в квадрат.
[30:12.880 --> 30:14.880]  Ну, как это происходит? Ну, как это происходит?
[30:14.880 --> 30:16.880]  Ну, просто чтобы найти распределение, вот это измененное распределение
[30:17.880 --> 30:19.880]  новой случайной величины, это будет тяжко.
[30:20.880 --> 30:22.880]  Никто так, естественно, не делает.
[30:23.880 --> 30:26.880]  И есть достаточно простая форма, мы ее сейчас выведем, и это будет результат.
[30:27.880 --> 30:29.880]  То есть дисперсия определяется вот так.
[30:30.880 --> 30:33.880]  Соответственно, здесь у вас под мат ожидания находятся квадраты разности,
[30:34.880 --> 30:36.880]  начиная с седьмого, наверное, класса.
[30:37.880 --> 30:39.880]  Ну, кто когда, вы знаете.
[30:39.880 --> 30:41.880]  Формула для квадраты разности.
[30:42.880 --> 30:44.880]  Соответственно, здесь мы можем, смотрим.
[30:45.880 --> 30:47.880]  Поскольку математическое ожидание, это у нас есть линейный оператор
[30:48.880 --> 30:50.880]  на множестве случайных величин,
[30:51.880 --> 30:53.880]  то мат ожидания от суммы, это сумма мат ожиданий.
[30:54.880 --> 30:57.880]  При этом мы видим, что вот эта случайная величина,
[30:58.880 --> 31:00.880]  вот эта случайная величина, это чиселка и это чиселка.
[31:00.880 --> 31:03.880]  Мы видим, что вот эта случайная величина, вот эта случайная величина,
[31:04.880 --> 31:06.880]  это чиселка и это чиселка.
[31:09.880 --> 31:11.880]  Давно я был в седьмом классе.
[31:14.880 --> 31:16.880]  И это тоже чиселка.
[31:17.880 --> 31:19.880]  Поэтому у нас здесь получается мат ожидания кси в квадрате,
[31:20.880 --> 31:23.880]  минус двойку вынесли, мат ожидания кси вынесли,
[31:24.880 --> 31:26.880]  а вот мат ожидания кси осталось.
[31:27.880 --> 31:31.880]  Дальше здесь у нас получается мат ожидания от мат ожидания кси в квадрате.
[31:35.880 --> 31:37.880]  Кого с кем?
[31:38.880 --> 31:40.880]  Кого с кем?
[31:41.880 --> 31:43.880]  Независимость, это там должна быть пара случайных величин.
[31:44.880 --> 31:45.880]  Тут нет, она одна.
[31:46.880 --> 31:48.880]  Поэтому нет независимости, не задумывайте.
[31:49.880 --> 31:51.880]  Я напомню то, что я пользуюсь линейностью математического ожидания.
[31:52.880 --> 31:54.880]  То есть когда мы считаем мат ожидания вот такой вот,
[31:54.880 --> 31:56.880]  получается а, мат ожидания кси плюс b.
[32:20.880 --> 32:22.880]  Ну, смотри, мы берем мат ожидания вот от этого произведения.
[32:22.880 --> 32:23.880]  Что у нас здесь?
[32:24.880 --> 32:26.880]  Двойка чиселка, мат ожидания кси чиселка.
[32:27.880 --> 32:28.880]  Вот они вынеслись, две штуки.
[32:29.880 --> 32:33.880]  А вот это вот кси получается мы берем от ожидания от одного вот этого множителя.
[32:34.880 --> 32:35.880]  Вот у нас.
[32:36.880 --> 32:37.880]  Третьим слагаемым давайте посмотрим.
[32:38.880 --> 32:39.880]  То есть мы берем от ожидания от чего?
[32:40.880 --> 32:41.880]  От чиселки.
[32:42.880 --> 32:43.880]  Мат ожидания и чиселки к чему равно?
[32:44.880 --> 32:45.880]  Самой этой чиселки.
[32:46.880 --> 32:49.880]  В результате мы видим то, что у нас получается мат ожидания кси в квадрате,
[32:49.880 --> 32:56.880]  в квадрате плюс мотождание икси в квадрате.
[32:57.080 --> 33:00.280]  Ну, дальше мы проводим подобные слагаемые, это, наверное, класс
[33:00.280 --> 33:04.400]  шестой, пятый.
[33:04.400 --> 33:06.840]  И получаем вот такую форму.
[33:06.840 --> 33:09.800]  То есть, для того, чтобы посчитать дисперсию случайной величины икси,
[33:09.800 --> 33:12.840]  нам нужно посчитать мотождание его возрасте в квадрат и посчитать мотождание
[33:12.840 --> 33:16.840]  от квадрата случайной величины.
[33:16.840 --> 33:18.920]  А эта история нормальная, потому что вот
[33:18.960 --> 33:22.760]  ну, мотождание мы умеем считать, а насчет мотождания квадрата мы с вами
[33:22.760 --> 33:27.240]  помним, что если мы считаем математическое ожидание функции,
[33:27.240 --> 33:30.440]  какой-то функции от случайной величины, при этом мы знаем распределение
[33:30.440 --> 33:34.400]  случайной величины икси, то есть, у нее распределение принимает значение
[33:34.400 --> 33:38.400]  хкат и с вероятностью мипкат, что посчитать вот такое мотождание
[33:38.400 --> 33:41.400]  достаточно посчитать вот такой вот ряд.
[33:49.920 --> 33:54.920]  Поэтому считать дисперсию мы всегда будем вот по этой формуле.
[33:54.920 --> 33:58.920]  Я напоминаю то, что определение, можно было в принципе так и
[33:58.920 --> 34:02.920]  определять, но я напоминаю, что определение должно скрывать суть объекта,
[34:02.920 --> 34:06.920]  а вот отсюда вообще ничего не понятно. Хотя считать удобнее, конечно.
[34:10.920 --> 34:14.920]  Что для того, чтобы посчитать мотождание откси в квадрате,
[34:14.920 --> 34:17.920]  можно воспользоваться вот таким результатом, который мы...
[34:26.920 --> 34:30.920]  А, там тоже был комментарий? Да, этого нет, извините.
[34:30.920 --> 34:33.920]  Это продолжение доказательств.
[34:35.920 --> 34:38.920]  А так сейчас кто-то пишет?
[34:38.920 --> 34:42.920]  Нет, по-моему, ставят просто свыше и в строку пишут, нет?
[34:45.920 --> 34:48.920]  Так, погнали дальше.
[34:48.920 --> 34:51.920]  Второе.
[34:52.920 --> 34:56.920]  Дисперсия случайночной кси строго больше нуля,
[34:56.920 --> 34:59.920]  но это очевидно, просто все его определения,
[34:59.920 --> 35:03.920]  поскольку это мотождание от неотрицательной случайночной.
[35:03.920 --> 35:08.920]  И что важно, что если дисперсия случайночной оказалась равна нулю,
[35:08.920 --> 35:11.920]  то это значит, что у нас есть суть,
[35:11.920 --> 35:14.920]  а если случайночная оказалась равна нулю,
[35:14.920 --> 35:17.920]  то что в этом случае можно сказать о случайночной?
[35:23.920 --> 35:26.920]  Констант «почти наверное».
[35:31.920 --> 35:34.920]  Потому что это термин действительно в анализе.
[35:34.920 --> 35:37.920]  А у нас теория вероятности. Там «почти наверное».
[35:37.920 --> 35:40.920]  Одно и частный случай другого.
[35:40.920 --> 35:43.920]  То есть «почти наверное» – это частный случай почти всюду.
[35:45.920 --> 35:49.920]  То есть мы с вами будем, когда в следующем разделе работать с мерами,
[35:49.920 --> 35:53.920]  меры у нас могут принимать, во-первых, конечные меры,
[35:53.920 --> 35:57.920]  то есть они могут… мера всего может быть любым числом, положительным, конечно.
[35:57.920 --> 36:00.920]  Плюс у нас будут с вами сигмы – конечные меры,
[36:00.920 --> 36:03.920]  когда у нас наше все пространство может иметь бесконечную меру,
[36:03.920 --> 36:06.920]  но в чем сигма заключается?
[36:06.920 --> 36:09.920]  Вы можете нарезать на счетное число непересекающихся кусочков,
[36:09.920 --> 36:12.920]  и мера каждого кусочка конечна.
[36:12.920 --> 36:15.920]  Ну, как прямая. Прямую можно нарезать на счетное число кусков,
[36:15.920 --> 36:18.920]  и мера каждого конечна.
[36:24.920 --> 36:27.920]  Нет, дисперсия – чиселка. Дисперсия – это…
[36:27.920 --> 36:30.920]  Еще раз, смотрите, это характеристики, это числа.
[36:30.920 --> 36:33.920]  То есть одна из проблем со случайными величинами,
[36:33.920 --> 36:36.920]  которая заключается, непонятно, как с ней работать,
[36:36.920 --> 36:39.920]  потому что у нее аргумент непонятный,
[36:39.920 --> 36:42.920]  а мы хотим какие-то понятные вещи, которые в Excel-ку загнать можно.
[36:42.920 --> 36:45.920]  Характеристики по этому – это все числовые истории.
[36:45.920 --> 36:48.920]  Главная характеристика распределения,
[36:48.920 --> 36:51.920]  которая у вас будет – это функция распределения,
[36:51.920 --> 36:54.920]  просто в рамках нашей истории дискретной.
[36:54.920 --> 36:57.920]  Функция распределения – интересная история,
[36:57.920 --> 37:00.920]  там всегда будет кусочек линейной истории ступенчатая.
[37:00.920 --> 37:03.920]  В серверу вы подробно с ними поработаете.
[37:16.920 --> 37:19.920]  Потому что «почти наверное» – это значит,
[37:19.920 --> 37:22.920]  что там, где она не константа, вероятность ноль.
[37:22.920 --> 37:27.920]  То есть вопрос заключается, как доказать отсюда и сюда.
[37:27.920 --> 37:30.920]  Давайте докажем. Я не планировал, но давайте докажем.
[37:30.920 --> 37:33.920]  Пускай у вас кси – констант «почти наверное».
[37:33.920 --> 37:38.920]  Давайте попробуем сначала подумать про математическое ожидание кси.
[37:38.920 --> 37:41.920]  То есть у вас кси равно с почти наверное.
[37:41.920 --> 37:44.920]  Что мы тогда можем сказать про математическое ожидание кси?
[37:44.920 --> 37:50.920]  Ведь это что? Сумма по Омего, П от Омего на кси от Омего.
[37:50.920 --> 37:53.920]  Но что значит, что кси равно с почти наверное?
[37:53.920 --> 37:55.920]  Это там, где она не с, вероятности ноль.
[37:55.920 --> 37:57.920]  Но это значит обнуляться эти слагаемые.
[37:57.920 --> 37:59.920]  Поэтому я могу суммировать только по таким Омего,
[37:59.920 --> 38:01.920]  что кси от Омего равно с.
[38:01.920 --> 38:03.920]  С на П от Омего.
[38:05.920 --> 38:09.920]  С вынесется за скобочку. У меня получится сумма по всем Омего таким,
[38:09.920 --> 38:10.920]  что кси от Омего равно с.
[38:10.920 --> 38:14.920]  Но в силу этого замечания сумма вероятности таких Омего маленьких – один.
[38:14.920 --> 38:16.920]  То есть я получил С.
[38:18.920 --> 38:19.920]  Дальше что у меня получается?
[38:19.920 --> 38:23.920]  У меня получается, что я считаю математическое ожидание кси
[38:23.920 --> 38:25.920]  минус мат ожидания кси в квадрате.
[38:25.920 --> 38:27.920]  При этом что я могу сказать об этой разности?
[38:27.920 --> 38:29.920]  Она равна нулю почти наверное.
[38:29.920 --> 38:32.920]  То есть если я сейчас аккуратно выпишу ряд
[38:34.920 --> 38:43.920]  по всем Омего, П от Омего на кси от Омего минус с в квадрате,
[38:43.920 --> 38:47.920]  то получается у меня либо этот множитель ноль, либо этот множитель ноль.
[38:49.920 --> 38:52.920]  То есть кси минус мат ожидания кси, который есть с.
[38:53.920 --> 38:54.920]  Оно ноль почти наверное.
[38:54.920 --> 38:55.920]  Но это что значит?
[38:55.920 --> 38:57.920]  Что там, где не ноль, вероятность Омега ноль.
[38:58.920 --> 39:01.920]  Получается либо этот множитель ноль, либо этот множитель ноль.
[39:02.920 --> 39:04.920]  По-моему я чрезмерно подробно рассказываю.
[39:06.920 --> 39:07.920]  Ну и все, получился нолик.
[39:10.920 --> 39:12.920]  Ну в обратную сторону так же.
[39:16.920 --> 39:20.920]  Так что с дисперсией тут надо как-то чтобы это в кровь вошло.
[39:20.920 --> 39:22.920]  Потому что если дисперсия нолик,
[39:22.920 --> 39:24.920]  это значит, что случайная величина у вас констант.
[39:24.920 --> 39:26.920]  Только не говорите пожалуйста, что на ноль.
[39:26.920 --> 39:32.920]  Третье это, что происходит с дисперсией,
[39:32.920 --> 39:35.920]  когда у нас есть линейные преобразования случайной величины.
[39:35.920 --> 39:36.920]  Какой ответ?
[39:36.920 --> 39:38.920]  Ну тут сообразить несложно.
[39:42.920 --> 39:43.920]  Еще?
[39:44.920 --> 39:45.920]  А в квадрате?
[39:47.920 --> 39:48.920]  Б уходит.
[39:48.920 --> 39:50.920]  Ну давайте идейно сначала.
[39:50.920 --> 39:52.920]  То есть что произошло?
[39:52.920 --> 39:54.920]  То есть параметр масштаба.
[39:54.920 --> 39:56.920]  То есть если мы кси увеличили в несколько раз,
[39:56.920 --> 39:59.920]  во сколько же раз у вас увеличится им от ожидания кси,
[39:59.920 --> 40:00.920]  ну это понятно.
[40:01.920 --> 40:03.920]  А значит вот этот квадрат увеличится в квадрат раз.
[40:04.920 --> 40:06.920]  Ну и собственно дисперсия увеличится в квадрат.
[40:06.920 --> 40:08.920]  Теперь параметр сдвига.
[40:08.920 --> 40:09.920]  То есть что мы сделали?
[40:09.920 --> 40:10.920]  Мы сдвинули кси на б.
[40:10.920 --> 40:11.920]  Всю кси на б.
[40:12.920 --> 40:14.920]  Понятно, что им от ожидания так же съедет на б.
[40:14.920 --> 40:16.920]  Поэтому их разность не изменится.
[40:18.920 --> 40:21.920]  И поэтому соответственно средняя тоже не изменится.
[40:23.920 --> 40:25.920]  Можно доказательство сами.
[40:25.920 --> 40:26.920]  Ладно?
[40:26.920 --> 40:28.920]  Надо просто преобразовать вот эту вот историю.
[40:28.920 --> 40:30.920]  Убедиться, что b здесь сокращается.
[40:31.920 --> 40:33.920]  А сначала выносится вот здесь за скобочку.
[40:33.920 --> 40:36.920]  Потом в квадрате и вылезает под знаком от ожидания.
[40:37.920 --> 40:38.920]  Хорошо?
[40:38.920 --> 40:39.920]  Я не буду доказывать.
[40:42.920 --> 40:43.920]  Пока все.
[40:43.920 --> 40:44.920]  Дисперсии пока все.
[40:44.920 --> 40:46.920]  Переходим к следующему объекту.
[40:47.920 --> 40:49.920]  Это ковариация двух случайных величин.
[40:50.920 --> 40:52.920]  То есть пока у нас с вами была характеристика.
[40:52.920 --> 40:56.920]  Мы говорили о характеристиках одной случайной величины.
[40:59.920 --> 41:02.920]  Теперь у нас с вами есть пара случайных величин кси и b.
[41:04.920 --> 41:06.920]  Так, первые, вторые, третьи свойства дисперсии.
[41:06.920 --> 41:08.920]  Дисперсия была вторым, значит сейчас третья.
[41:08.920 --> 41:10.920]  Ковариация случайных величин.
[41:11.920 --> 41:14.920]  Ковариация.
[41:17.920 --> 41:18.920]  Случайных величин.
[41:18.920 --> 41:22.920]  То есть отличие в том, что у вас их сейчас есть две кси и b.
[41:24.920 --> 41:25.920]  Соответственно, определение.
[41:27.920 --> 41:29.920]  Ковариация пары случайных величин
[41:30.920 --> 41:31.920]  будет называться такой объект.
[41:31.920 --> 41:33.920]  Вы считаете мат ожидания
[41:34.920 --> 41:38.920]  от произведения центрированной кси и центрированной b.
[41:39.920 --> 41:41.920]  Что такое операция центрирования?
[41:42.920 --> 41:44.920]  Вы из своей случайной величины
[41:44.920 --> 41:47.920]  путем вычитания чиселки делаете
[41:47.920 --> 41:49.920]  те случайные величины мат ожидания, которые ноль.
[41:50.920 --> 41:52.920]  То есть понятно, что если возьмете мат ожидания
[41:52.920 --> 41:54.920]  вот этой дырани, это будет ноль.
[41:54.920 --> 41:55.920]  Это понятно?
[41:55.920 --> 41:57.920]  Ну и отдельно этого тоже ноль.
[41:57.920 --> 41:59.920]  Дальше вы их умножаете
[42:00.920 --> 42:01.920]  и берете средние.
[42:02.920 --> 42:04.920]  Вот это вот называется ковариация двух случайных величин.
[42:05.920 --> 42:07.920]  Соответственно, какие свойства?
[42:09.920 --> 42:10.920]  Свойства первые.
[42:10.920 --> 42:12.920]  Опять же то, что считать вот эту вещь,
[42:12.920 --> 42:14.920]  даже когда вам дано совместное распределение
[42:14.920 --> 42:16.920]  двух случайных величин, это сдохнуть.
[42:17.920 --> 42:19.920]  Поэтому есть простая форму.
[42:20.920 --> 42:22.920]  Для того, чтобы посчитать ковариацию,
[42:22.920 --> 42:24.920]  вам нужно посчитать математическое ожидание
[42:24.920 --> 42:26.920]  произведения двух случайных величин
[42:26.920 --> 42:28.920]  и вычесть из них произведение математических ожиданий.
[42:29.920 --> 42:31.920]  Доказательства в точности такой же
[42:31.920 --> 42:33.920]  можно и я не буду это делать.
[42:33.920 --> 42:35.920]  Отличие только в том, что мы здесь раскрывали квадрат,
[42:35.920 --> 42:37.920]  а там нужно просто раскрыть скобки.
[42:37.920 --> 42:39.920]  А так все тоже самое будет.
[42:39.920 --> 42:40.920]  Окей?
[42:43.920 --> 42:45.920]  Второе.
[42:46.920 --> 42:48.920]  Так, после формулы у нас
[42:48.920 --> 42:50.920]  еще что было вот здесь вот.
[42:50.920 --> 42:52.920]  Что происходит с линейными прообразованиями?
[42:53.920 --> 42:55.920]  Я не буду расписывать,
[42:55.920 --> 42:57.920]  а с вами скажу, что ковариация
[42:57.920 --> 42:59.920]  есть белинейная форма.
[43:02.920 --> 43:04.920]  Есть белинейная форма
[43:05.920 --> 43:07.920]  на множестве случайных величин.
[43:08.920 --> 43:10.920]  На множестве случайных величин.
[43:12.920 --> 43:14.920]  Ковариация есть билинейная форма.
[43:14.920 --> 43:16.920]  Это было же, да?
[43:16.920 --> 43:18.920]  Это чего? Аугиброгеометрия, да?
[43:18.920 --> 43:20.920]  А кой второй семестр? Первый?
[43:20.920 --> 43:22.920]  Это второй семестр.
[43:22.920 --> 43:24.920]  Ну, то есть это значит то, что
[43:24.920 --> 43:26.920]  если я вот здесь вот
[43:26.920 --> 43:28.920]  поставлю какую-то линейную комбинацию,
[43:28.920 --> 43:30.920]  давайте одну вещь распишу,
[43:30.920 --> 43:32.920]  ну, то есть если я здесь делаю
[43:32.920 --> 43:34.920]  альфа-1, кси-1 плюс альфа-2,
[43:34.920 --> 43:36.920]  то есть если я здесь делаю альфа-1,
[43:36.920 --> 43:38.920]  кси-1 плюс альфа-2, кси-2,
[43:38.920 --> 43:40.920]  это,
[43:41.920 --> 43:43.920]  то это будет что такое?
[43:43.920 --> 43:45.920]  альфа-1 на ковариацию,
[43:45.920 --> 43:47.920]  кси-1 это,
[43:47.920 --> 43:49.920]  плюс альфа-2 на ковариацию,
[43:49.920 --> 43:51.920]  кси-2 это.
[43:51.920 --> 43:53.920]  Ну, это как-то пример.
[43:55.920 --> 43:56.920]  Что такое?
[43:56.920 --> 43:58.920]  Будет линейность по первому адгументу,
[43:58.920 --> 44:00.920]  ну, по второму тоже.
[44:06.920 --> 44:08.920]  Ну, и какое тут замечание, кажется, логично
[44:08.920 --> 44:10.920]  в этот момент?
[44:24.920 --> 44:26.920]  Дисперсия оказывается
[44:26.920 --> 44:28.920]  квадратичной формой,
[44:28.920 --> 44:30.920]  порожденной этой билинейной формы.
[44:30.920 --> 44:32.920]  У нас ведь всегда, когда есть
[44:32.920 --> 44:34.920]  билинейная форма,
[44:34.920 --> 44:36.920]  она автоматически порождает квадратичную форму,
[44:36.920 --> 44:38.920]  когда вы берете билинейную
[44:38.920 --> 44:40.920]  вектор от самого себя.
[44:40.920 --> 44:42.920]  У нас ведь квадратичное
[44:42.920 --> 44:44.920]  на линейное пространство определялось, да?
[44:44.920 --> 44:46.920]  Ну, да.
[44:46.920 --> 44:48.920]  Поэтому у вас получается
[44:48.920 --> 44:50.920]  третье замечание, что
[44:52.920 --> 44:54.920]  у вас дисперсия,
[44:54.920 --> 44:56.920]  это есть ковариация,
[44:56.920 --> 44:58.920]  дисперсия это
[44:58.920 --> 45:00.920]  билинейная форма, которая,
[45:00.920 --> 45:02.920]  дисперсия это квадратичная форма,
[45:02.920 --> 45:04.920]  которая соответствует этой билинейной форме.
[45:06.920 --> 45:08.920]  Типа, ну и что такого, да?
[45:08.920 --> 45:10.920]  Тут это подводочка,
[45:10.920 --> 45:12.920]  вы не думайте, что это просто так.
[45:14.920 --> 45:16.920]  Смотрите.
[45:16.920 --> 45:18.920]  Четвертое.
[45:20.920 --> 45:22.920]  Когда я
[45:22.920 --> 45:24.920]  хочу посчитать дисперсию
[45:24.920 --> 45:26.920]  суммы двух случайных величин?
[45:28.920 --> 45:30.920]  Пока я ничего там не знаю
[45:30.920 --> 45:32.920]  о их независимости.
[45:32.920 --> 45:34.920]  Что утверждается?
[45:34.920 --> 45:36.920]  Что для того, чтобы посчитать дисперсию суммы двух,
[45:36.920 --> 45:38.920]  нужно посчитать дисперсию первой плюс дисперсию
[45:38.920 --> 45:40.920]  второго плюс удвоенную
[45:40.920 --> 45:42.920]  ковариацию первого на второе.
[45:46.920 --> 45:48.920]  Доказательство этой истории, оно такое же
[45:48.920 --> 45:50.920]  техническое, давайте его проведем.
[45:50.920 --> 45:52.920]  То есть, что такое дисперсия
[45:52.920 --> 45:54.920]  кси плюс это?
[45:54.920 --> 45:56.920]  По определению. Это математическое
[45:56.920 --> 45:58.920]  ожидание квадрата
[45:58.920 --> 46:00.920]  разности. Чего?
[46:00.920 --> 46:02.920]  Вашей случайной величины и ее математического
[46:02.920 --> 46:04.920]  ожидания.
[46:04.920 --> 46:06.920]  Ну, соответственно, тут можно перегруппировать
[46:06.920 --> 46:08.920]  слагаемые под знаком квадрата.
[46:08.920 --> 46:10.920]  У нас получится мат ожидания
[46:10.920 --> 46:12.920]  кси минус мат ожидания
[46:12.920 --> 46:14.920]  кси плюс это
[46:14.920 --> 46:16.920]  минус мат ожидания эт.
[46:20.920 --> 46:22.920]  Дальше опять седьмой кваз,
[46:22.920 --> 46:24.920]  в котором, как мы выяснили, я не
[46:24.920 --> 46:26.920]  силен.
[46:26.920 --> 46:30.920]  То есть это
[46:30.920 --> 46:32.920]  квадрат первого слагаемого,
[46:32.920 --> 46:34.920]  но если мы возьмем от него мат ожидания, мы
[46:34.920 --> 46:36.920]  получим дисперсию. Дальше квадрат второго
[46:36.920 --> 46:38.920]  слагаемого, если мы возьмем от него
[46:38.920 --> 46:40.920]  мат ожидания, мы получим дисперсию эт.
[46:40.920 --> 46:42.920]  И дальше будет удвоенное
[46:42.920 --> 46:44.920]  произведение первого на второе. Если мы возьмем
[46:44.920 --> 46:46.920]  от него мат ожидания, там двойка вынесется
[46:46.920 --> 46:50.920]  и мы получим две ковырятся.
[46:50.920 --> 46:52.920]  История ясна?
[46:52.920 --> 46:54.920]  Хорошо.
[46:54.920 --> 46:58.400]  Ну, легко понять, что когда вы будете считать дисперсию
[46:58.400 --> 47:05.600]  суммы случайных величин, то вам нужно будет просуммировать
[47:05.600 --> 47:17.520]  дисперсии этих случайных величин и еще прибавить
[47:17.520 --> 47:28.960]  две суммы всевозможной кавариации, ксиито и ксижито.
[47:28.960 --> 47:36.280]  Ну, понятно, что доказываться будет точно так же, точно
[47:36.280 --> 47:38.440]  так же вы будете раскрывать здесь квадрат, у вас будет
[47:38.440 --> 47:39.440]  сколько-то слагаемых.
[47:40.040 --> 47:42.040]  Вот так вот.
[48:04.040 --> 48:06.040]  Прям интерриг, да?
[48:09.640 --> 48:12.600]  Ну, то есть он мне даже помогает, он нагнетает, да?
[48:12.600 --> 48:14.600]  К чему же все это идет?
[48:14.600 --> 48:20.600]  Пункт пятый.
[48:20.600 --> 48:23.600]  Давайте посмотрим на эту кавариацию.
[48:23.600 --> 48:25.680]  Давайте вот, смотрите, вы как люди, которые хорошо
[48:25.680 --> 48:28.400]  знают прошлую лекцию, что можете сказать вот о таком
[48:28.400 --> 48:29.400]  выражении?
[48:29.400 --> 48:33.000]  Ну, какой-то ассоциативный ряд у вас возникает?
[48:33.000 --> 48:34.960]  Мотожнание произведений, произведение математических
[48:34.960 --> 48:35.960]  ожиданий.
[48:35.960 --> 48:40.120]  Ну, что-то там было про независимость, да?
[48:40.120 --> 48:42.000]  То есть мы говорили, что когда две случайные величины
[48:42.000 --> 48:46.120]  независимы, то произведение, то мат ожидания произведения
[48:46.120 --> 48:48.240]  случайных величин равно произведению мат ожиданий.
[48:48.240 --> 48:49.520]  Было такое свойство.
[48:49.520 --> 48:51.880]  Отсюда вывод, что если у вас случайная величина
[48:51.880 --> 48:55.120]  независима, отсюда следует то, что кавариация двух
[48:55.120 --> 48:58.600]  случайных величин равна нулю.
[48:58.600 --> 49:08.680]  В ясности, я говорю, она к деталям все сильнее.
[49:08.680 --> 49:13.120]  То есть если случайная величина независима, то кавариация
[49:13.120 --> 49:14.120]  равна нулю.
[49:14.120 --> 49:17.320]  Как вы думаете, что будет в обратную сторону?
[49:17.320 --> 49:22.960]  То есть если кавариация равна нулю, то...
[49:22.960 --> 49:23.960]  Ни фига подобного.
[49:23.960 --> 49:27.680]  Во-первых, почти, наверное, независима – это неправильное
[49:27.680 --> 49:28.680]  вещество.
[49:28.680 --> 49:34.920]  Такого не бывает.
[49:34.920 --> 49:38.000]  Независимость – это отношение, там только да или нет.
[49:38.000 --> 49:48.440]  В обратную сторону это неверно.
[49:48.440 --> 49:52.800]  Ну потому что что значит, что кавариация у вас равна
[49:52.800 --> 49:53.800]  нулю?
[49:53.800 --> 49:56.680]  Это какое-то там математическое ожидание ноль.
[49:56.680 --> 49:59.120]  На самом деле понятно, что можно подкрутить значение
[49:59.120 --> 50:02.160]  случайной величины и добиться того, чтобы математическое
[50:02.160 --> 50:03.800]  ожидание у вас стало равно нулю.
[50:03.800 --> 50:07.800]  Что-то вы меня так смотрите, как будто не верите.
[50:07.800 --> 50:15.280]  А можно пример какой-нибудь?
[50:15.280 --> 50:18.080]  Можно я его сделаю на окружности.
[50:18.080 --> 50:20.040]  Вы меня простите, не дискретный пример.
[50:20.040 --> 50:21.040]  Хорошо?
[50:21.040 --> 50:24.040]  То есть вы в процессе изучения тервера будете сталкиваться
[50:24.040 --> 50:26.360]  с этими вот постоянно.
[50:26.360 --> 50:34.600]  Я его не помню, я сейчас буду подгонять, так что я заранее
[50:34.600 --> 50:35.600]  прошу прощения.
[50:35.600 --> 50:45.240]  Так, ну давайте, пускай Омега у нас будет от 0 до 2 пипа.
[50:45.240 --> 50:50.280]  Вам придется поверить мне, что те как бы… А, че, у нас
[50:50.280 --> 50:52.080]  тогда мат ожидания – это интегралы.
[50:52.080 --> 50:53.080]  Это тяжело, нет?
[50:53.240 --> 51:06.480]  Ну что, нам нужно, чтобы… То есть че я сейчас буду добиваться?
[51:06.480 --> 51:09.360]  Я буду добиваться, чтобы все эти три хрени были нулями.
[51:09.360 --> 51:12.960]  Ну просто, мне же нужно в итоге 0, чтобы получить 0,
[51:12.960 --> 51:15.640]  чтобы просто все три должны быть нулями.
[51:15.640 --> 51:16.640]  Хорошо?
[51:16.640 --> 51:19.640]  Ну давайте.
[51:20.640 --> 51:26.080]  Ну пускай у меня Си это будет косинус от Омега, а это
[51:26.080 --> 51:30.520]  это будет синус от Омега.
[51:30.520 --> 51:36.400]  Ну интуитивно понятно, что среднее должно быть 0.
[51:36.400 --> 51:39.360]  Ну и у косинуса, и у синуса, потому что у вас косинус
[51:39.360 --> 51:44.480]  симметрично относительно горизонтали, то есть у него
[51:44.480 --> 51:47.040]  как бы одинаковые значения все время, поэтому средним
[51:47.040 --> 51:49.960]  образом, если мы будем считать по всей окружности,
[51:49.960 --> 51:50.960]  у нас получится 0.
[51:50.960 --> 51:51.960]  Да?
[51:51.960 --> 51:56.080]  То есть мат ожидания Кси – это у нас 0, мат ожидания
[51:56.080 --> 51:59.040]  Эта, ну по тем же соображениям тоже будет 0.
[51:59.040 --> 52:00.040]  Ну вот.
[52:00.040 --> 52:03.040]  А теперь давайте подумаем, что такое произведение
[52:03.040 --> 52:04.400]  двух случайных величин.
[52:04.400 --> 52:08.400]  Косинус от Омега умножить на синус от Омега.
[52:08.400 --> 52:15.400]  Это че такое?
[52:15.400 --> 52:18.080]  Синус 2 Омега делись пополам.
[52:18.080 --> 52:19.080]  Правильно?
[52:19.080 --> 52:22.920]  А теперь посмотрим, то есть когда Омега у вас пробегает
[52:22.920 --> 52:27.400]  от 0 до 2 пи, 2 Омега у вас че пробегает?
[52:27.400 --> 52:35.280]  Ну вот, поэтому мат ожидания Кси на это – это тоже будет
[52:35.280 --> 52:36.280]  0.
[52:36.280 --> 52:44.480]  Ну то есть ковидриация их оказала все равно 0, потому
[52:44.480 --> 52:46.920]  что просто там все 0 по формуле.
[52:46.920 --> 52:50.400]  Вот это вот первый пункт, что в дисперсии, что в ковидриации
[52:50.400 --> 52:52.440]  – это формула вычисления.
[52:52.440 --> 52:55.040]  Теперь давайте подумаем, зависимы вот эти две хрени
[52:55.040 --> 52:56.040]  или независимы.
[52:56.040 --> 53:00.400]  Ведь тут проблема в том, что это непрерывные, да,
[53:00.400 --> 53:01.400]  я так нехорошо делаю.
[53:01.400 --> 53:08.000]  Ну, Косинус Омега и Синус Омега зависимы или нет,
[53:08.000 --> 53:11.000]  как вы думаете?
[53:11.000 --> 53:14.080]  Не, ну точку брать бесполезно, потому что вероятность
[53:14.080 --> 53:17.040]  будет равна нулю, нам нужны множества.
[53:17.040 --> 53:19.800]  Кто предложит множество?
[53:19.800 --> 53:29.200]  Опять хорошо с нулями играться, да, ведь, ну то есть мысль
[53:29.200 --> 53:30.200]  в чем?
[53:30.200 --> 53:33.920]  Предполагается, подобрать два таких множества, извините,
[53:33.920 --> 53:38.320]  множества, что не выполнено вот это равенство.
[53:38.320 --> 53:46.280]  Для этого что надо сделать?
[53:46.280 --> 53:48.840]  Я предлагаю проще с нулями, проще всего играться.
[53:48.840 --> 53:51.200]  Давайте сделаем так, чтобы вот эти два были не ноль,
[53:51.200 --> 53:52.200]  а вот это было ноль.
[53:52.200 --> 53:53.200]  Хорошо?
[53:53.200 --> 53:56.280]  А чтобы вот это было ноль, надо чтобы вот это событие
[53:56.280 --> 53:57.280]  было каким?
[53:57.280 --> 53:58.280]  Проще всего пустым.
[53:58.280 --> 53:59.280]  Правильно.
[53:59.280 --> 54:02.040]  Кого взять в качестве А и Б?
[54:02.040 --> 54:05.240]  То есть нужно так, чтобы, ну, чтобы вероятности были
[54:05.240 --> 54:08.360]  положительными, да, но при этом чтобы одномоментно
[54:08.360 --> 54:09.360]  они не могли быть.
[54:09.360 --> 54:14.640]  Понятно, что имеется в виду, да?
[54:14.640 --> 54:17.200]  Ну то есть мы как бы говорим то, что вот у нас наша пина
[54:17.200 --> 54:20.440]  четыре, но нам нужно говорить, что косинус попадает вот
[54:20.440 --> 54:24.000]  сюда, а синус попадает вот сюда.
[54:24.000 --> 54:26.480]  Пересечение этих двух даже можно взять больше или
[54:26.480 --> 54:28.600]  бы равно, но будет по точке.
[54:28.600 --> 54:31.480]  Но вероятность точки у нас ноль, то есть А это у
[54:31.500 --> 54:34.480]  нас единица на корень из двух до единички.
[54:35.360 --> 54:38.520]  Б – это единица на корень из двух до единички.
[54:38.520 --> 54:42.780]  Тогда вот это у вас там будет...
[54:42.780 --> 54:45.320]  Че, пИнадово, поучается.
[54:45.580 --> 54:47.880]  Здесь пИнадово, здесь вот это будет пИнадово..
[54:47.880 --> 54:51.820]  Че хрен не с Heh, пИнадово делить на 2 пИ.
[54:51.820 --> 54:56.240]  ПИнадово делить на 2 пИ, пИнадово делить на 2 П.
[54:56.240 --> 54:58.380]  То есть по четверти, да?
[54:58.380 --> 54:59.380]  Да.
[54:59.380 --> 55:00.380]  А это ноль.
[55:00.380 --> 55:02.740]  потому что одномоментно эти события невозможны.
[55:02.740 --> 55:06.420]  Вот, пожалуйста, вес случайной величины, который очевидно
[55:06.420 --> 55:10.260]  зависимый и тихо нам подобрал такие А и Б, что вероятность
[55:10.260 --> 55:12.340]  пересечения не равна произведению вероятностей.
[55:12.340 --> 55:17.060]  При этом коваряция получилась нулем.
[55:17.060 --> 55:20.260]  Просто мы там все нули получились.
[55:20.260 --> 55:21.260]  Это же мот ожидания.
[55:21.260 --> 55:22.940]  Сделать так, чтобы оно было нулем не так сложно.
[55:22.940 --> 55:27.100]  В дискретном случае это тоже можно сделать, но
[55:27.100 --> 55:30.180]  последние два года я говорил, типа, поверьте, все говорили,
[55:30.180 --> 55:31.180]  ладно.
[55:31.180 --> 55:32.180]  И как-то выходило.
[55:32.180 --> 55:33.180]  Вы удовлетворены примером?
[55:33.180 --> 55:34.180]  Ну, плюс-минус.
[55:34.180 --> 55:35.180]  Хорошо.
[55:35.180 --> 55:39.460]  В дискретном тоже можно подобрать, это несложно.
[55:39.460 --> 55:40.460]  Да, Оля.
[55:40.460 --> 55:45.460]  Мы же обычно в независимых случайных причинах делали
[55:45.460 --> 55:48.660]  для одного исхода, для значения одного.
[55:48.660 --> 55:52.900]  Ну, потому что это непрерывный случай, и у тебя, если ты
[55:52.900 --> 55:54.900]  берешь равно конкретные чиселки, у тебя всегда
[55:54.900 --> 55:56.860]  будут нули, и это не годится.
[55:56.860 --> 56:01.140]  Ну, то есть, я не люблю в любых случаях поверить,
[56:01.140 --> 56:03.140]  что они здесь, но это другое?
[56:03.140 --> 56:04.140]  Да, конечно.
[56:04.140 --> 56:05.140]  Еще раз.
[56:05.140 --> 56:06.140]  Все, что мы делали, мы делали в дискретном.
[56:06.140 --> 56:08.140]  Слушайте, в дискретном тоже подбирается.
[56:08.140 --> 56:11.780]  Просто я так экспромтом не уверен, что я...
[56:11.780 --> 56:13.260]  Мысль заключается в чем?
[56:13.260 --> 56:14.260]  Надо подобрать.
[56:14.260 --> 56:15.780]  Да, числа подобрать.
[56:15.780 --> 56:18.700]  Просто здесь я понял, что сразу эти можно брать,
[56:18.700 --> 56:20.260]  и все хорошо получится.
[56:20.260 --> 56:22.540]  Вы понимаете, что на самом деле можно взять вероятность
[56:22.540 --> 56:25.100]  на пространство с четырьмя точками и там так подобрать
[56:25.100 --> 56:27.260]  значение вероятности, то что в результате все будет
[56:27.260 --> 56:28.260]  хорошо.
[56:28.260 --> 56:29.260]  Ну да, да.
[56:29.260 --> 56:30.260]  Хорошо.
[56:30.260 --> 56:31.260]  Все.
[56:31.260 --> 56:41.340]  Так, кто понял, что сказали, ребята, и что это доводимо
[56:41.340 --> 56:43.340]  до результата?
[56:43.340 --> 56:44.340]  Никто не понял.
[56:44.340 --> 56:45.340]  Ну и ладно.
[56:45.340 --> 56:46.340]  Ладно.
[56:46.340 --> 56:50.900]  В общем, мысль заключается в том, что если у вас случайно
[56:50.900 --> 56:53.660]  величины независимы, только вариация у них равна нулю,
[56:53.660 --> 56:54.980]  то в обратную сторону неверно.
[56:54.980 --> 56:55.980]  В связи с этим...
[56:55.980 --> 56:59.980]  Но, теперь смотрите, интрига, мы подходим к развязке.
[56:59.980 --> 57:01.660]  Что мы с вами получаем?
[57:01.660 --> 57:02.660]  Шестое.
[57:02.660 --> 57:09.020]  Что если квадрация двух случайных величин равна нулю,
[57:09.020 --> 57:11.100]  что выполняется, например, когда случайная величина
[57:11.100 --> 57:15.980]  независима, то дисперсия суммы случайных величин
[57:16.060 --> 57:17.060]  равна сумме дисперсии.
[57:17.060 --> 57:29.820]  И вот в этот момент должно стать...
[57:29.820 --> 57:33.660]  Вот, смотрите, вы же как смотрите, вот у вас математика
[57:33.660 --> 57:36.420]  развивается, и, в принципе, все там логично происходит,
[57:36.420 --> 57:37.420]  более или менее.
[57:37.420 --> 57:40.220]  Вот в этот момент должен произойти ступор.
[57:40.220 --> 57:41.220]  Почему?
[57:41.220 --> 57:44.220]  Вот в этот момент.
[57:44.460 --> 57:45.460]  Погромче.
[57:45.460 --> 57:48.460]  По формулке светит просто.
[57:48.460 --> 57:50.460]  Это да, это хорошо, но идейно.
[57:50.460 --> 57:53.460]  Вы точно сейчас форму накируете.
[57:53.460 --> 57:54.460]  Конечно.
[57:54.460 --> 57:55.460]  Ведь у нас что получилось?
[57:55.460 --> 57:56.460]  Все было здорово.
[57:56.460 --> 57:57.460]  Что такое квадрация?
[57:57.460 --> 57:59.900]  На самом деле, почему мы ввели квадрацию?
[57:59.900 --> 58:03.100]  Потому что это та белинейная форма, которую у вас продуцирует
[58:03.100 --> 58:05.980]  квадратичную форму, коей является дисперсия.
[58:05.980 --> 58:06.980]  И все понятно.
[58:06.980 --> 58:10.020]  И дисперсия себя ведет как квадратичную форму
[58:10.020 --> 58:12.980]  хорошо, квадрация как белинейная, все здорово.
[58:12.980 --> 58:15.460]  А потому что получается, что если мы накладываем
[58:15.460 --> 58:20.900]  на слагаемые какое-то странное требование, то, что квадрация
[58:20.900 --> 58:23.820]  ноль, оно вообще непонятно, но давайте возьмем независимость.
[58:23.820 --> 58:26.820]  Если слагаемые независимы, то почему-то дисперсия
[58:26.820 --> 58:30.060]  начинает вести себя линейно, хотя изначально это квадратичная
[58:30.060 --> 58:31.060]  форма.
[58:31.060 --> 58:32.060]  Что за фигня?
[58:32.060 --> 58:33.060]  Ну, объективно.
[58:33.060 --> 58:35.580]  И тут включается вот именно то, что я вам говорил.
[58:35.580 --> 58:38.700]  То есть, Богачев, который сейчас ведет на продвинутом
[58:38.700 --> 58:41.860]  потоке, он же мой семер по действительному анализу,
[58:41.900 --> 58:43.860]  он сотрудник арте, теории, функции и функционального
[58:43.860 --> 58:44.860]  анализа.
[58:44.860 --> 58:46.420]  Он совершенно искренне считает, что самая главная наука
[58:46.420 --> 58:48.660]  в математике это функциональный анализ, а все остальное
[58:48.660 --> 58:50.500]  братье меньше, включая тервер.
[58:50.500 --> 58:53.820]  И он нам это на полном серьезе, нам в группе тервера этого
[58:53.820 --> 58:56.380]  объявлял на семинарах.
[58:56.380 --> 58:59.140]  Потом он делал такую паузу и говорил, ну да, вот в тервере
[58:59.140 --> 59:03.020]  есть одно, вот это понятие независимости, и оно делает
[59:03.020 --> 59:07.100]  тервер как бы отдельной дисциплины, в нем суть.
[59:07.100 --> 59:08.780]  И вот здесь это явно выражается.
[59:09.100 --> 59:12.300]  У вас будут постоянные отсылки тервера к функциональному
[59:12.300 --> 59:13.300]  анализу.
[59:13.300 --> 59:14.620]  Понятно, что там действительно анализ, это такой подкласс
[59:14.620 --> 59:17.700]  функционального анализа, но все время будут отсылки.
[59:17.700 --> 59:19.980]  Тоже самое не анализ Вакаши Буниковского, это же по сути
[59:19.980 --> 59:20.980]  то же самое.
[59:20.980 --> 59:25.060]  Но когда влезает независимость, а если вы будете заниматься
[59:25.060 --> 59:29.540]  тервером, там разные виды зависимости, там, слабая,
[59:29.540 --> 59:31.500]  сильная зависимость между случайными величинами,
[59:31.500 --> 59:33.700]  там как бы вся вот эта математика, которая была до этого, она
[59:33.700 --> 59:37.500]  летит, летит в плане логичного представления о структуре.
[59:37.500 --> 59:39.780]  Вы осознали?
[59:39.780 --> 59:40.780]  Окей.
[59:40.780 --> 59:44.420]  Теперь смотрите, в силу пятого у нас что получается?
[59:44.420 --> 59:47.660]  У нас получается то, что квариация является понятием
[59:47.660 --> 59:51.620]  более слабым, чем независимость, но при этом его достаточно
[59:51.620 --> 59:54.180]  для того, чтобы дисперсия вот так вот вела себя прикольно.
[59:54.180 --> 59:55.180]  Хорошо?
[59:55.180 --> 59:59.060]  Поэтому вот в этот момент водится отдельное понятие
[59:59.060 --> 01:00:01.100]  некоррелированных случайных величин.
[01:00:08.500 --> 01:00:10.180]  Так, определение.
[01:00:10.180 --> 01:00:13.900]  То есть случайные величины ксиэт, случайные е, величины
[01:00:13.900 --> 01:00:25.220]  ксиэт называются, называются некоррелированными, некоррелированными,
[01:00:25.220 --> 01:00:30.180]  если их квариация равна нулю.
[01:00:30.180 --> 01:00:34.140]  Понятие корреляции у нас сейчас тоже будет, не пугайтесь.
[01:00:34.140 --> 01:00:43.980]  В конце я сегодня постараюсь успеть объяснить, зачем
[01:00:43.980 --> 01:00:46.740]  вот такое отдельное понятие некоррели...
[01:00:46.740 --> 01:00:48.380]  Сейчас конспект, момент.
[01:01:04.140 --> 01:01:09.820]  Знаете, там во время всяких публичных выступлений
[01:01:09.820 --> 01:01:11.340]  надо всегда ссылаться на классику.
[01:01:11.340 --> 01:01:16.220]  Но это удобно, потому что с классиками невозможно
[01:01:16.220 --> 01:01:19.900]  пользоваться, поэтому я сейчас сошлюсь на величайшую
[01:01:19.900 --> 01:01:21.460]  художественную произведение человечества, это серия
[01:01:21.460 --> 01:01:22.460]  его папины дочки.
[01:01:22.460 --> 01:01:26.900]  Там в какой-то момент, Галина Сергеевна, вы наверно в
[01:01:26.900 --> 01:01:30.940]  ваше поколение вообще не знаете, что это такое, да?
[01:01:30.940 --> 01:01:31.940]  Ну это...
[01:01:31.940 --> 01:01:32.940]  Да?
[01:01:32.940 --> 01:01:36.700]  Ну вот, в какой-то момент Галина Сергеевна, когда
[01:01:36.700 --> 01:01:39.180]  Маша поступила в Баунку, она там говорит, что Маша
[01:01:39.180 --> 01:01:42.020]  это девушка абсолютно некоррелированная из физики.
[01:01:42.020 --> 01:01:45.060]  Понятно, что сценаристы использовали это для того,
[01:01:45.060 --> 01:01:48.580]  чтобы как-то придать наукообразности речи Галины
[01:01:48.580 --> 01:01:49.580]  Сергеевны.
[01:01:49.580 --> 01:01:50.580]  Ну вот.
[01:01:50.580 --> 01:01:55.780]  Но вот эта вот история, она очень болезненная для
[01:01:55.780 --> 01:01:56.780]  прикладников.
[01:01:56.780 --> 01:01:58.780]  В чем заключается история?
[01:01:58.780 --> 01:02:02.220]  То понятие независимых случайных величин именно
[01:02:02.220 --> 01:02:05.380]  с точки зрения математики этой истории очень тяжелая.
[01:02:05.380 --> 01:02:08.580]  Вот мы сейчас с Олей дискутируем, потому что я, правда, тамстер,
[01:02:08.580 --> 01:02:13.140]  то что если случайные величины у вас, они будут в общем случае
[01:02:13.140 --> 01:02:18.020]  определены, то как будет определяться независимость?
[01:02:18.020 --> 01:02:20.940]  Для любых баррельских множеств А и Б, этих множеств вот
[01:02:20.940 --> 01:02:21.940]  до черта.
[01:02:21.940 --> 01:02:24.860]  Для любых этих множеств вероятность того, что кси
[01:02:24.860 --> 01:02:27.220]  принадлежит А, это принадлежит Б, должно распадаться в
[01:02:27.300 --> 01:02:28.300]  произведение вероятностей.
[01:02:28.300 --> 01:02:32.100]  Ну то есть проверить вот эту историю, это ну просто
[01:02:32.100 --> 01:02:34.700]  сдохнуть, потому что А и Б, таких А и Б у вас очень
[01:02:34.700 --> 01:02:35.700]  много.
[01:02:35.700 --> 01:02:38.180]  Вопрос некролированности случайной величины кажется
[01:02:38.180 --> 01:02:39.180]  очень простым.
[01:02:39.180 --> 01:02:41.460]  Вопрос-то надо посчитать какое-то там математическое
[01:02:41.460 --> 01:02:42.460]  ожидание.
[01:02:42.460 --> 01:02:44.980]  Вы будете этим много заниматься на семинарах, и это не будет
[01:02:44.980 --> 01:02:46.340]  представлять никакой проблемы.
[01:02:46.340 --> 01:02:48.060]  Надо просто знать совместное распределение этих двух
[01:02:48.060 --> 01:02:49.060]  случайных величин.
[01:02:49.060 --> 01:02:51.540]  Вы посчитаете это матожедание, посчитаете это, посчитаете
[01:02:51.540 --> 01:02:53.340]  это, чиселку получится.
[01:02:53.340 --> 01:02:55.340]  Это первая привлекательность к вариации.
[01:02:55.340 --> 01:02:56.940]  Вторая привлекательность к вариации заключается
[01:02:56.940 --> 01:03:00.980]  в том, что статистически эта вещь оцениваема, то
[01:03:00.980 --> 01:03:03.500]  есть потом, когда после тервера у вас будет математическая
[01:03:03.500 --> 01:03:09.180]  статистика, там будет прям целый раздел, как вот эти
[01:03:09.180 --> 01:03:14.060]  вот характеристики по наблюдениям примерно прикинуть.
[01:03:14.060 --> 01:03:16.620]  И будет всякая теория, то есть насколько точно вы
[01:03:16.620 --> 01:03:19.700]  своими вот этими оценками их приближаете.
[01:03:19.700 --> 01:03:23.220]  Будет такое понятие выборочное среднее, то есть у вас вот
[01:03:23.300 --> 01:03:25.180]  это вот математическое ожидание, а можно посчитать
[01:03:25.180 --> 01:03:27.700]  выборочное математическое ожидание по тем наблюдениям,
[01:03:27.700 --> 01:03:28.700]  которые вы получили.
[01:03:28.700 --> 01:03:31.180]  Что происходит дальше?
[01:03:31.180 --> 01:03:36.220]  Сидят эти несчастные прикладники, химики, биологи всякие,
[01:03:36.220 --> 01:03:39.140]  для которых вся эта математика нафиг не нужна, всё это грустно
[01:03:39.140 --> 01:03:40.140]  и скучно.
[01:03:40.140 --> 01:03:45.900]  И как проверять независимость вообще непонятно, то есть
[01:03:45.900 --> 01:03:47.900]  вот у них есть какие-то показатели, которые они
[01:03:47.900 --> 01:03:48.900]  снимают.
[01:03:48.900 --> 01:03:53.740]  И вот им надо понять, вот эти показатели друг от
[01:03:53.740 --> 01:03:56.780]  друга зависят или они друг от друга не зависят.
[01:03:56.780 --> 01:04:01.140]  Очень поверхностная задачка же, вот как и как это делать.
[01:04:01.140 --> 01:04:06.820]  И они считают так называемый коэффициент корреляции.
[01:04:06.820 --> 01:04:08.620]  Выборочный, конечно, не наш теоретический, который
[01:04:08.620 --> 01:04:09.620]  мы считаем.
[01:04:09.620 --> 01:04:12.540]  Через год вам объяснят, что такое выборочный, поэтому
[01:04:12.540 --> 01:04:13.540]  сейчас давайте его введём.
[01:04:13.540 --> 01:04:21.660]  Они их по наблюдениям считают.
[01:04:21.660 --> 01:04:24.620]  То есть, предположим, вот у них есть, смотрите, история
[01:04:24.620 --> 01:04:25.620]  какая.
[01:04:25.620 --> 01:04:29.180]  В XL, везде есть вот эти вот коэффициенты, вот если
[01:04:29.180 --> 01:04:33.940]  вы XL откроете, господи, вы же второй курс, вы слишком
[01:04:33.940 --> 01:04:34.940]  мелкий.
[01:04:34.940 --> 01:04:41.900]  Когда был, в какой-то момент, Яндекс сделал свою олимпиаду
[01:04:41.900 --> 01:04:44.020]  на ФКН.
[01:04:44.020 --> 01:04:48.380]  Для нас это был удар под дых, понятно, пишу, в общем,
[01:04:48.380 --> 01:04:50.820]  ФПМИ это было тогда очень больно.
[01:04:50.820 --> 01:04:53.740]  Мы сказали, слушайте, давайте на ФПМИ, Яндекс тоже сделает
[01:04:53.740 --> 01:04:54.740]  свою стипендию.
[01:04:54.740 --> 01:04:57.300]  Они говорят, окей, давайте сделаем, но придумайте нам
[01:04:57.300 --> 01:05:00.140]  какую-нибудь вот такую интересную стипендию, мы не хотим просто
[01:05:00.140 --> 01:05:02.580]  давать там завсе раз, сколько можно.
[01:05:02.580 --> 01:05:05.780]  Всех этих стипендий завсе раз, если сложить там столько
[01:05:05.780 --> 01:05:07.780]  солей получается в месяц.
[01:05:07.780 --> 01:05:10.020]  Но придумайте что-нибудь интересное.
[01:05:10.020 --> 01:05:13.020]  Они сказали придумывать мне, я придумал там какую-то
[01:05:13.020 --> 01:05:16.180]  комплексную историю, то, что на входе за олимпиады
[01:05:16.180 --> 01:05:18.820]  за школьные достижения, а потом там типа вот ребята
[01:05:18.820 --> 01:05:24.460]  учатся, и там нарастающая идет оценка, то есть прибавляются
[01:05:24.460 --> 01:05:29.900]  ваши активности в учебе, в преподавании, в публикациях
[01:05:29.900 --> 01:05:33.860]  и во всяких олимпиадах типа ACPC, и будет вот эта вот накопительная
[01:05:33.860 --> 01:05:34.860]  история.
[01:05:34.860 --> 01:05:36.020]  И моя цель заключалась в чем?
[01:05:36.020 --> 01:05:41.140]  Мне нужно было показать, почему надо учитывать олимпиады
[01:05:41.140 --> 01:05:42.140]  при дальнейшей оценке.
[01:05:42.140 --> 01:05:44.820]  Я говорил, что если человек крутой олимпиадник, он потом
[01:05:44.820 --> 01:05:46.060]  будет хорошо учиться.
[01:05:46.060 --> 01:05:47.460]  И я что сделал?
[01:05:47.460 --> 01:05:53.340]  Я просто посчитал коэффициент корреляции между баллами.
[01:05:53.340 --> 01:05:55.860]  Мы на приемке, у нас такой есть ваш рейтинг, вы его
[01:05:55.860 --> 01:05:57.740]  наверное не видели, он такой тайный, ну то есть там за
[01:05:57.740 --> 01:06:00.740]  каждую олимпиадку назначаются баллы и суммируются.
[01:06:00.740 --> 01:06:03.660]  И у каждого абитуриента есть балл, ну то есть числовая
[01:06:03.660 --> 01:06:04.660]  история хорошая.
[01:06:05.060 --> 01:06:09.660]  И потом у нас мы выкачали, как вы учитесь, ну не вы,
[01:06:09.660 --> 01:06:13.020]  а как бы те ребята, потому что олимпиад уже там сколько,
[01:06:13.020 --> 01:06:15.260]  три года, степень уже три года существует.
[01:06:15.260 --> 01:06:19.940]  И дальше выборка, а выборка огромная, это ФПМИ, господи,
[01:06:19.940 --> 01:06:21.980]  но тех, кто поступает по конейми по бюджету, там
[01:06:21.980 --> 01:06:25.940]  300 человек, репрезентативность хорошая.
[01:06:25.940 --> 01:06:29.460]  И я считал коэффициенты корреляции, я показывал,
[01:06:29.460 --> 01:06:32.900]  говорю, смотрите, как хорошо зависят успехи ребят потом
[01:06:32.900 --> 01:06:36.540]  во время учебы от их олимпиадной активности, а от проектной
[01:06:36.540 --> 01:06:41.620]  зависит вообще от отрицательной корреляции была, да, типа
[01:06:41.620 --> 01:06:46.180]  те, кто пошли в бизнес, пускай там и остаются, нефиг им учиться
[01:06:46.180 --> 01:06:47.180]  и в разработку.
[01:06:47.180 --> 01:06:51.300]  То есть это история, которая считается по результатам,
[01:06:51.300 --> 01:06:54.500]  то есть было 300 наблюдений и две чиселки, понятно,
[01:06:54.500 --> 01:06:59.860]  да, рейтинг и потом дальнейший, нет, там было даже, там как-то
[01:06:59.860 --> 01:07:02.260]  я смотрел, как учатся потом в течение времени, там было
[01:07:02.260 --> 01:07:03.260]  несколько семестров.
[01:07:03.260 --> 01:07:06.820]  Так, корреляция двух случайных величин.
[01:07:06.820 --> 01:07:21.340]  Так, определяется это как, то есть корреляция случайных
[01:07:21.340 --> 01:07:25.620]  величин кси, это определяется, как частные корреляции
[01:07:25.620 --> 01:07:30.260]  их делить на корень из дисперсии кси.
[01:07:30.740 --> 01:07:34.820]  То есть понятно, что эта вещь определяется, когда
[01:07:34.820 --> 01:07:37.540]  дисперсии не равны нулю, ни та, ни другая.
[01:07:41.540 --> 01:07:42.540]  Ну, давайте...
[01:07:46.540 --> 01:07:48.540]  А какое обозначение корреляции?
[01:07:48.540 --> 01:07:53.660]  Корреляция с двумя R, я прошу прощения, дикция.
[01:07:53.660 --> 01:07:57.100]  Соответственно, основная мысль, которая есть в этой
[01:07:57.100 --> 01:08:04.700]  истории, это то, что теорема, это что корреляция двух
[01:08:04.700 --> 01:08:10.620]  случайных величин по модулю меньше или равна 1.
[01:08:10.620 --> 01:08:16.460]  При этом И выполнило следующее.
[01:08:16.460 --> 01:08:22.420]  Корреляция будет равна 1 тогда, когда существуют
[01:08:22.420 --> 01:08:28.460]  чиселки А и В, причем А больше нуля, что кси равняется
[01:08:28.460 --> 01:08:35.580]  А это плюс В, и корреляция равна минус 1 тогда и только
[01:08:35.580 --> 01:08:38.740]  тогда, когда существуют чиселки А В, где А меньше
[01:08:38.740 --> 01:08:43.180]  нуля, что кси равняется А это плюс В.
[01:08:43.180 --> 01:08:45.900]  То есть если корреляция принимает свои крайние
[01:08:45.900 --> 01:08:48.940]  значения, они просто линейно зависимы, одна выражается
[01:08:48.940 --> 01:08:50.420]  через другую, причем неважно кто.
[01:08:50.420 --> 01:08:55.660]  Какие-то есть, это сдвиг, он ни на что не влияет.
[01:08:55.660 --> 01:09:00.820]  Вот в этот момент складывается очень симпатичная картинка.
[01:09:00.820 --> 01:09:03.860]  Смотрите, если случайная величина независимая, то
[01:09:03.860 --> 01:09:08.380]  корреляция 0, значит корреляция тоже 0, то есть мы посерединке.
[01:09:08.380 --> 01:09:13.380]  Если у вас лучшая степень зависимости, линейная зависимость,
[01:09:13.380 --> 01:09:15.860]  то корреляция принимает свои крайние значения.
[01:09:15.860 --> 01:09:20.380]  И складываются вообще очень приятные истории.
[01:09:20.380 --> 01:09:24.100]  То есть, господи, мы получили числовую характеристику
[01:09:24.100 --> 01:09:27.940]  зависимости, степень независимости двух случайных величин.
[01:09:27.940 --> 01:09:31.060]  Если мы посерединке, они независимы, вообще кайф.
[01:09:31.060 --> 01:09:33.780]  А если мы по краям, то зависимость такая, самая контовая, но
[01:09:33.780 --> 01:09:36.780]  то есть линейная.
[01:09:36.780 --> 01:09:39.180]  Кайф?
[01:09:39.180 --> 01:09:40.180]  Но это не так.
[01:09:40.180 --> 01:09:44.900]  Только что мы с вами просто сходу, причем таких примеров
[01:09:44.900 --> 01:09:47.700]  будет очень много, показали, что если корреляция равна
[01:09:47.820 --> 01:09:51.900]  нулю, это не значит, что случайные величины независимы.
[01:09:51.900 --> 01:09:55.020]  То есть они даже зависимы, понятно, вот косинус и синус,
[01:09:55.020 --> 01:09:58.420]  понятно, что это зависимые характеристики две.
[01:09:58.420 --> 01:10:01.980]  Но беда заключается в чем?
[01:10:01.980 --> 01:10:05.340]  То, что вот эта вещь считается и оценивается, и в статистике
[01:10:05.340 --> 01:10:07.140]  вы будете заниматься оценками.
[01:10:07.140 --> 01:10:11.620]  И есть эти несчастные прикладники, которые очень хотят найти
[01:10:11.620 --> 01:10:13.700]  вот эту вот степень зависимости.
[01:10:13.820 --> 01:10:19.020]  Будете, когда заниматься этим своим машинным обучением,
[01:10:19.020 --> 01:10:21.860]  ну вот, и вы будете там в частности проходить историю
[01:10:21.860 --> 01:10:22.860]  с регрессией.
[01:10:22.860 --> 01:10:23.860]  Ведь что такое регрессия?
[01:10:23.860 --> 01:10:28.660]  Это то, что у вас линейная зависимость между признаками.
[01:10:28.660 --> 01:10:36.060]  Но беда состоит в том, что это неверно.
[01:10:36.060 --> 01:10:41.300]  Есть следующий корреляцион, то есть что неверно?
[01:10:41.300 --> 01:10:45.580]  Неверно говорить о том, что если у вас корреляция
[01:10:45.580 --> 01:10:49.660]  близка к нулю, это значит то, что у вас эти ваши характеристики
[01:10:49.660 --> 01:10:51.380]  независимы между собой.
[01:10:51.380 --> 01:10:52.380]  Это неправильно.
[01:10:52.380 --> 01:10:55.420]  Они поэтому и придумали отдельный термин «не коррелированный».
[01:10:55.420 --> 01:10:59.420]  Что в этом случае, то есть, я же разговаривал всегда
[01:10:59.420 --> 01:11:01.860]  у меня родители доктора химических наук, я у них
[01:11:01.860 --> 01:11:03.740]  спрашивал, я говорю, вы эти штуки читаете?
[01:11:03.740 --> 01:11:06.500]  Он говорит, ну да, как же, мы строим зависимости,
[01:11:06.500 --> 01:11:07.500]  там все хорошо.
[01:11:07.500 --> 01:11:08.500]  Я говорю, а как вы это объясняете?
[01:11:08.860 --> 01:11:12.020]  Ну вот они, то, что они там зависимы слабо или сильно
[01:11:12.020 --> 01:11:14.580]  зависимы в зависимости от того, какой вот коэффициент
[01:11:14.580 --> 01:11:17.260]  корреляции, близкий к нулю или близкий к единиц.
[01:11:17.260 --> 01:11:20.260]  Я же тоже, когда, ну я не Бунин и, конечно, писал в тот
[01:11:20.260 --> 01:11:25.540]  момент Евгением, Женем, в общем, которые всеми образовательными
[01:11:25.540 --> 01:11:27.980]  проектами Яндекса, я и посчитал их коэффициентом корреляции,
[01:11:27.980 --> 01:11:32.020]  говорю, вот, смотрите, больше половинки, значит хорошо.
[01:11:32.020 --> 01:11:33.820]  Ну то есть такая зависимость хорошая.
[01:11:33.820 --> 01:11:41.660]  Ну вот, и вот тут, на самом деле, это очень иронично,
[01:11:41.660 --> 01:11:44.460]  то есть ты открываешь даже википедию, я сегодня, когда
[01:11:44.460 --> 01:11:47.980]  готовился, я открыл википедию, открываете статью корреляции
[01:11:47.980 --> 01:11:50.740]  и читаете, и там написано, то есть если корреляция
[01:11:50.740 --> 01:11:53.580]  ноль, это не значит то, что они независимы.
[01:11:53.580 --> 01:11:54.580]  Вообще нет.
[01:11:54.580 --> 01:11:56.980]  Ну вот, и если корреляция близка к единице, это не
[01:11:56.980 --> 01:11:59.540]  значит то, что один зависим на другом, там может быть
[01:11:59.540 --> 01:12:01.100]  очень сложная история.
[01:12:01.100 --> 01:12:05.020]  Но как они это объясняют, и как я это понимаю, то,
[01:12:05.020 --> 01:12:07.820]  что прикладники, я же как бы не прикладник, они
[01:12:07.820 --> 01:12:11.700]  говорят, что если корреляции такие, то это повод для
[01:12:11.700 --> 01:12:14.220]  нас задуматься, что зависимость должна быть такой.
[01:12:14.220 --> 01:12:17.220]  Идея ясна?
[01:12:17.220 --> 01:12:18.740]  Я это к чему?
[01:12:18.740 --> 01:12:21.780]  К тому, что будьте, пожалуйста, аккуратны, то есть всякий
[01:12:21.780 --> 01:12:25.220]  раз, когда вы будете сталкиваться с вот этими двумя понятиями
[01:12:25.220 --> 01:12:30.060]  независимости и некоррелированности, во-первых, четко их отличайте,
[01:12:30.060 --> 01:12:32.740]  понимаете, почему все носятся с этой некоррелированностью,
[01:12:32.740 --> 01:12:34.220]  потому что с ней работать легко.
[01:12:34.220 --> 01:12:37.940]  Ее и посчитать можно в теоретическом случае очень
[01:12:37.940 --> 01:12:43.180]  просто, а в случае прикладном, там вот эти выборочные
[01:12:43.180 --> 01:12:46.300]  моменты можно посчитать, ее оценить можно тоже достаточно
[01:12:46.300 --> 01:12:47.300]  легко.
[01:12:47.300 --> 01:12:49.660]  Эксельку открываете, там есть этот выборочный коэффициент
[01:12:49.660 --> 01:12:53.060]  корреляции, но это не независимость.
[01:12:53.060 --> 01:12:57.620]  Так, теперь нам вот этот вот результат надо доказать,
[01:12:57.620 --> 01:12:58.820]  что это да, это все правда.
[01:12:58.820 --> 01:13:06.300]  Я сегодня опять не успею.
[01:13:06.300 --> 01:13:19.740]  Так, погнали, доказательства.
[01:13:19.740 --> 01:13:21.700]  Водим две случайно вечные.
[01:13:21.700 --> 01:13:26.060]  Кси-штрих, который есть, что мы делаем, мы кси сначала
[01:13:26.060 --> 01:13:28.260]  отцентрируем, а потом нормируем.
[01:13:28.260 --> 01:13:35.180]  И это штрих тоже самое, мы ее сначала центрируем,
[01:13:35.180 --> 01:13:41.620]  а потом нормируем.
[01:13:41.620 --> 01:13:43.620]  Поднимите руку, кто четко понимает, что кроется за
[01:13:43.620 --> 01:13:46.340]  свалами, центрируем и нормируем, и что я действительно это
[01:13:46.340 --> 01:13:49.140]  сделал, что я отцентрирую, отнормировал.
[01:13:49.140 --> 01:13:52.140]  Что-то не все поднимаю.
[01:13:52.140 --> 01:13:55.540]  Ну давайте посмотрим, чему равно от ожидания кси-штрих.
[01:13:56.020 --> 01:13:58.660]  Эта чиселка просто выносится за знак математического
[01:13:58.660 --> 01:13:59.660]  ожидания.
[01:13:59.660 --> 01:14:00.660]  У нас получается вот так.
[01:14:00.660 --> 01:14:03.660]  Ну это ноль.
[01:14:03.660 --> 01:14:04.660]  Да?
[01:14:04.660 --> 01:14:05.660]  Окей.
[01:14:05.660 --> 01:14:11.660]  Теперь, когда мы считаем дисперсию кси-штрих, соответственно,
[01:14:11.660 --> 01:14:14.140]  для того, чтобы посчитать дисперсию, нам что нужно?
[01:14:14.140 --> 01:14:18.500]  Нам нужно посчитать математическое ожидание квадрата случайно
[01:14:18.500 --> 01:14:20.980]  ввечной минус ее от ожидания, но от ожидания у нас ноль,
[01:14:20.980 --> 01:14:24.460]  поэтому можно сразу писать, переписывать просто случайно
[01:14:25.380 --> 01:14:26.380]  Да?
[01:14:26.380 --> 01:14:27.380]  Теперь смотрим.
[01:14:27.380 --> 01:14:33.980]  Это же чиселка, да, а мы с вами знаем то, что чиселка
[01:14:33.980 --> 01:14:36.500]  сначала выносится из-под квадрата, уходит к коренью, а потом
[01:14:36.500 --> 01:14:38.860]  она выносится из-под мат ожидания, будет просто единицей
[01:14:38.860 --> 01:14:39.860]  дисперсию кси.
[01:14:39.860 --> 01:14:44.580]  Так, у нас получается просто мат ожидания кси-мат ожидания
[01:14:44.580 --> 01:14:45.580]  кси в квадрате.
[01:14:45.580 --> 01:14:48.300]  Так это и есть дисперсия, она сократилась, получилась
[01:14:48.300 --> 01:14:49.300]  1.
[01:14:49.300 --> 01:14:51.300]  То есть мы добились того, что вот этой процедурой
[01:14:51.300 --> 01:14:53.540]  центризования мы сделали так, чтобы мат ожидания
[01:14:53.540 --> 01:14:58.220]  было ноль, а нормирование, чтобы дисперсия была 1.
[01:14:58.220 --> 01:14:59.220]  Хорошо.
[01:14:59.220 --> 01:15:04.300]  Дальше давайте посчитаем дисперсию кси штрих плюс
[01:15:04.300 --> 01:15:05.300]  минус это штрих.
[01:15:05.300 --> 01:15:11.900]  Как только бы недавно мы с вами выяснили то, что
[01:15:11.900 --> 01:15:14.340]  дисперсия суммы в общем случае тут же про независимость
[01:15:14.340 --> 01:15:17.900]  ничего не сказано, но мы можем расписать по той формуле,
[01:15:17.900 --> 01:15:18.900]  что было.
[01:15:18.900 --> 01:15:22.100]  То есть это дисперсия кси штрих плюс дисперсия
[01:15:22.100 --> 01:15:29.300]  это штрих, плюс минус две кавариации, кси штрих это
[01:15:29.300 --> 01:15:32.300]  штрих.
[01:15:32.300 --> 01:15:37.260]  Ну дисперсия эта и эта это двойки, два я вынесу за
[01:15:37.260 --> 01:15:40.420]  скобку, у меня получается единица, плюс минус.
[01:15:40.420 --> 01:15:42.860]  Теперь давайте смотреть вот на эту кавариацию.
[01:15:42.860 --> 01:15:46.140]  Ну вот, соответственно, чтобы посчитать кавариацию
[01:15:46.140 --> 01:15:47.980]  двух случайных величин, мне что нужно?
[01:15:47.980 --> 01:15:50.340]  Мне нужно мат ожидания произведения, из него вычесть
[01:15:50.980 --> 01:15:53.580]  произведение мат ожидания, но мат ожидания каждой штуки
[01:15:53.580 --> 01:15:54.580]  ноль.
[01:15:54.580 --> 01:16:04.780]  Поэтому вот этой штуки не будет, просто мат ожидания
[01:16:04.780 --> 01:16:05.780]  произведения.
[01:16:05.780 --> 01:16:10.180]  А кавариация и белинейная форма они вынесятся из-под
[01:16:10.180 --> 01:16:11.180]  кавариации.
[01:16:11.180 --> 01:16:13.360]  И останется мат ожидания произведения вот этих штук,
[01:16:13.360 --> 01:16:16.780]  а это что?
[01:16:16.780 --> 01:16:18.440]  Это кавариация.
[01:16:18.440 --> 01:16:23.040]  То есть у нас получилась здесь ковариация кси и эт,
[01:16:23.040 --> 01:16:28.040]  делить на корень с дисперсией кси, корень с дисперсией эт.
[01:16:29.040 --> 01:16:30.040]  Да?
[01:16:31.040 --> 01:16:33.040]  Ой, это ж она!
[01:16:35.040 --> 01:16:40.040]  То есть у нас получилось 2 единицы плюс-минус корреляция
[01:16:42.040 --> 01:16:43.040]  кси и эт.
[01:16:45.040 --> 01:16:48.040]  Понятно, что первое утверждение теоремы я доказал.
[01:16:49.040 --> 01:16:52.040]  Поднимите руки, кто видит, что первое утверждение теоремы я доказал.
[01:16:54.040 --> 01:16:58.040]  Ну, у нас же дисперсия, дисперсия штука не отрицательная.
[01:16:59.040 --> 01:17:00.040]  Всегда.
[01:17:00.040 --> 01:17:01.040]  Правильно?
[01:17:02.040 --> 01:17:04.040]  А здесь у нас что получается?
[01:17:04.040 --> 01:17:09.040]  Единицы плюс-минус и плюс-минус, как бы это я обоих вариантов расписывал.
[01:17:09.040 --> 01:17:13.040]  Но понятно, что корреляция не может вылезти за пределы отрезка,
[01:17:13.040 --> 01:17:15.040]  потому что если она больше единицы,
[01:17:15.040 --> 01:17:17.040]  единицы минус корреляция становится отрицательным,
[01:17:17.640 --> 01:17:21.640]  а если она меньше минус единицы, единицы плюс корреляция будет отрицательной.
[01:17:21.640 --> 01:17:22.640]  Всё.
[01:17:22.640 --> 01:17:27.640]  Поэтому первый результат, что корреляция по модулю меньше равна 1, мы показали.
[01:17:28.640 --> 01:17:31.640]  Но куда интересен вот этот вот второй результат?
[01:17:31.640 --> 01:17:32.640]  Потому что чем он интересен?
[01:17:32.640 --> 01:17:37.640]  Именно с тем, что это единственная ситуация, когда мы хоть что-то можем утверждать наверняка.
[01:17:37.640 --> 01:17:42.640]  То есть если у вас корреляция оказалась равна 1, значит они зависимы, даже мы знаем как, линейно.
[01:17:43.640 --> 01:17:46.640]  То есть это единственная история, когда у нас что-то мы можем утверждать.
[01:17:47.240 --> 01:17:49.240]  Так, погнали.
[01:17:53.240 --> 01:17:54.240]  Ну, смотрим.
[01:17:54.240 --> 01:17:59.240]  Я, с вашего позволения, разберу один вариант с единичкой давайте.
[01:17:59.240 --> 01:18:01.240]  Или минус единичкой?
[01:18:02.240 --> 01:18:04.240]  Ну, не важно, на самом деле.
[01:18:04.240 --> 01:18:05.240]  Пускай корреляция равна 1.
[01:18:05.240 --> 01:18:06.240]  Это что значит?
[01:18:06.240 --> 01:18:09.240]  Это значит, что единицы минус корреляция 0.
[01:18:09.240 --> 01:18:10.240]  Правильно?
[01:18:10.240 --> 01:18:12.240]  Это значит, что дисперсия разности,
[01:18:12.240 --> 01:18:13.240]  она равна 0.
[01:18:13.240 --> 01:18:14.240]  Согласны?
[01:18:15.240 --> 01:18:19.240]  А мы с вами недавно выяснили, что дисперсия у нас 0, тогда и только тогда, когда что.
[01:18:26.240 --> 01:18:28.240]  Ну, это не правда, С.
[01:18:28.240 --> 01:18:29.240]  Констант.
[01:18:32.240 --> 01:18:34.240]  Почти, наверное, правильно?
[01:18:35.240 --> 01:18:39.240]  Ну, кстати, я его там наврал, вот здесь надо было естественно записать, почти наверное.
[01:18:41.240 --> 01:18:45.240]  На самом деле всегда в теории вероятности, когда вы будете утверждать равенство или не равенство,
[01:18:45.240 --> 01:18:47.240]  вот что-то по случайной величины,
[01:18:47.240 --> 01:18:49.240]  там всегда будет почти наверное,
[01:18:49.240 --> 01:18:54.240]  потому что вы измените случайную величину в каком-то количестве точек мира,
[01:18:54.240 --> 01:18:57.240]  которых 0, и вы никак это не сможете почувствовать.
[01:18:57.240 --> 01:19:00.240]  У вас все такие значения есть, и вы не сможете их почувствовать.
[01:19:00.240 --> 01:19:03.620]  точек меры, которых ноль, и вы никак это не сможете почувствовать.
[01:19:03.620 --> 01:19:07.800]  У вас все характеристики, которые вы пишете, останутся теми же.
[01:19:07.800 --> 01:19:14.060]  Давайте смотреть, что такое кси с чертой. Это есть кси, минус мотоожидание кси,
[01:19:14.060 --> 01:19:18.940]  делительная коренность дисперсии кси, это у нас это, минус мотоожидание это,
[01:19:18.940 --> 01:19:24.780]  делительная коренность дисперсии это и это с. Можно я не буду выписывать вот те а и b.
[01:19:24.780 --> 01:19:38.260]  Что такое будет а? Это будет с, домноженный на дисперсию кси, плюс, понятно? Видно,
[01:19:38.260 --> 01:19:42.380]  что кси и это выражается, потому что случайная величина здесь только вот это и вот это.
[01:19:42.380 --> 01:19:47.580]  Все остальное это чиселки. То же самое будет, если у вас будет минус единица,
[01:19:47.580 --> 01:20:13.180]  у вас здесь будет плюс, здесь будет плюс, и все. Что ты дашь? Вот здесь вот? Потому что,
[01:20:14.100 --> 01:20:19.660]  коверяция расписываешь, аккуратно. Что такое коверяция? Это мотоожидание произведения
[01:20:19.660 --> 01:20:22.720]  минус произведение мотоожидания, но
[01:20:22.720 --> 01:20:25.920]  Вот мы его здесь посчитали.
[01:20:25.920 --> 01:20:30.040]  Поэтому это есть просто мотождание, кси минус мотождание
[01:20:30.040 --> 01:20:34.480]  кси, делить на конец дисперсии кси, это минус мотождание
[01:20:34.480 --> 01:20:37.680]  это, делить на конец дисперсии это.
[01:20:37.680 --> 01:20:38.680]  Всё.
[01:20:38.680 --> 01:20:43.680]  А вот эта вот штука, извините, вот тут умножить, умножить.
[01:20:43.680 --> 01:20:46.600]  Это же чиселки, множители, они выносятся из-под знака
[01:20:46.600 --> 01:20:47.600]  мотождания.
[01:20:47.600 --> 01:20:50.920]  Ну и всё, у тебя остается просто мотождание, произведение
[01:20:50.920 --> 01:20:51.960]  вот этих двух разностей.
[01:20:51.960 --> 01:20:54.640]  Это просто определение коваряции.
[01:20:54.640 --> 01:20:55.640]  Вот.
[01:20:55.640 --> 01:20:59.320]  А эти штуки остались в знаменащинах и неожиданно
[01:20:59.320 --> 01:21:00.320]  получили корреляцию.
[01:21:00.320 --> 01:21:01.560]  Ну то есть, ну подогнали.
[01:21:01.560 --> 01:21:08.880]  Вот если честно, я когда, ну сколько-то лет назад
[01:21:08.880 --> 01:21:11.800]  я готовился, ну то есть как бы вот всё, что я вам говорю,
[01:21:11.800 --> 01:21:14.360]  я просто вот как с куста гладлю, да, а я в какой-то
[01:21:14.360 --> 01:21:17.120]  момент готовился к лекции и я думал, чёрт, а как это
[01:21:17.120 --> 01:21:18.120]  доказывать?
[01:21:18.120 --> 01:21:20.480]  Ну вот именно линейность, да, вот эту вот.
[01:21:20.840 --> 01:21:22.320]  И я так думал, а как?
[01:21:22.320 --> 01:21:25.080]  Вот именно то, что равно почти, наверное, блин.
[01:21:25.080 --> 01:21:28.000]  И меня пришлось в Шираево, в Шираево лезть.
[01:21:28.000 --> 01:21:33.200]  Ну вот, это как бы с доказательствами Шираева, оно же такое непрозрачное
[01:21:33.200 --> 01:21:34.200]  абсолютно же.
[01:21:34.200 --> 01:21:37.600]  Ну то есть реально, что мы сделали, да, мы выписали,
[01:21:37.600 --> 01:21:40.200]  у нас тут типа получилось два плюс два, плюс-минус
[01:21:40.200 --> 01:21:42.000]  два, вынесли за скобку, всё хорошо.
[01:21:42.000 --> 01:21:46.680]  Ну догадаться до такого решения, ну как его придумать,
[01:21:46.680 --> 01:21:47.680]  я не знаю.
[01:21:48.480 --> 01:21:53.680]  А как объяснить по поводу мотождания ксиш-3, почему
[01:21:53.680 --> 01:21:54.680]  ноль получается?
[01:21:54.680 --> 01:21:57.280]  Ну потому что у тебя вот эта скобочка, мотождание
[01:21:57.280 --> 01:22:00.400]  этой скобочки ноль, у тебя же мотождание линейное,
[01:22:00.400 --> 01:22:03.280]  да, ты берёшь мотождание это, минус мотождание это,
[01:22:03.280 --> 01:22:07.840]  но это число, поэтому мотождание, оно само, у тебя получается
[01:22:07.840 --> 01:22:13.960]  екси минус екси, это ноль, вот, теперь зачем всё это
[01:22:13.960 --> 01:22:14.960]  нужно?
[01:22:15.360 --> 01:22:19.360]  Так, я надеюсь, про некоррелированность вы осознали.
[01:22:26.760 --> 01:22:30.880]  Я тут тоже смотрю, каждый повышает настроение как
[01:22:30.880 --> 01:22:33.880]  можно, я начал смотреть, пересматривать детство
[01:22:33.880 --> 01:22:38.480]  Шелдона, там Шелдон доказывал этому пастеру о том, что
[01:22:38.480 --> 01:22:39.480]  он говорит.
[01:22:39.480 --> 01:22:42.400]  Вероятно из того, что Бог есть, 50 процентов, Шелдон
[01:22:42.400 --> 01:22:43.400]  говорит ха-ха.
[01:22:44.960 --> 01:22:52.920]  Вот, теперь зачем всё это надо, это всё, да, у нас
[01:22:52.920 --> 01:22:55.480]  остались законы больших чисел и центральная преддельная
[01:22:55.480 --> 01:22:59.880]  теорема, я прошу прощения, я что-то опять балаболил,
[01:22:59.880 --> 01:23:02.560]  надо было терверы закончить, но мы его закончим, у нас
[01:23:02.560 --> 01:23:04.680]  остался закон больших чисел и центральная преддельная
[01:23:04.680 --> 01:23:05.680]  теорема.
[01:23:05.680 --> 01:23:08.760]  То есть, говорят, то, ради чего весь тервер и существует,
[01:23:08.760 --> 01:23:14.000]  который обосновывает применение в жизни.
[01:23:14.000 --> 01:23:15.240]  Спасибо огромное, что пришли.

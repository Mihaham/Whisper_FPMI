[00:00.000 --> 00:09.600]  Всем доброго дня, мы с вами продолжаем. Сегодня у нас последняя лекция по куди, как ни
[00:09.600 --> 00:18.680]  странно. Да, Данил этому рад, видимо, несказанно. Вот, и мы с вами, у нас сегодня большой план. Во-первых,
[00:18.680 --> 00:26.040]  мы должны с вами разобрать, в чем заключается задача подсчета суммы на префиксе. Еще раз
[00:26.040 --> 00:33.080]  детально разобраться и немного поговорить про конкарнси, как конкарнси устроен в видеокарте.
[00:33.080 --> 00:40.200]  Вот наш план на сегодня. Итак, давайте вспомним, какую мы задачу с вами решали в прошлый раз. В
[00:40.200 --> 00:47.000]  прошлый раз у нас была такая задача. У нас с вами есть массив, а мы, кстати, не хотим эти шторы
[00:47.000 --> 01:00.120]  открыть. У нас с вами есть массив, и в нем есть элементы. Допустим, в нем 15 элементов. Нам
[01:00.120 --> 01:05.880]  нужно посчитать сумму на префиксе. Значит, и по идее у нас должно быть в сумме один из двух видов
[01:05.880 --> 01:14.120]  массивов. Значит, задача варьируется. В каком-то месте она называется inclusive scan. Это означает,
[01:14.160 --> 01:21.960]  что мы берем элементы с нулевого по 15-й складывы. Есть exclusive scan? Что означает exclusive
[01:21.960 --> 01:39.920]  scan? Это означает, что у нас сумма сначала 0, потом 0, потом 0.1, после этого 0.14 и 0.15. Мы с вами уже в
[01:39.920 --> 01:46.280]  прошлый раз раз разобрались, как считать сумму в простом случае. То есть, когда у нас с вами есть
[01:46.280 --> 01:52.040]  один блок, и мы хотим посчитать сумму внутри одного блока. Мы с вами рассмотрели один из алгоритмов.
[01:52.040 --> 01:58.640]  Один из алгоритмов заключается в том, что мы берем наши элементы и складываем их последовательно.
[01:58.640 --> 02:04.440]  То есть, складываем суммы по одному и складываем суммы по двойке. Вот такое вот решение мы с вами
[02:04.440 --> 02:12.480]  рассмотрели. То есть, у нас есть элементы а0 и т.д., а7 или x0 и т.д., x7. И мы складываем элементы.
[02:12.480 --> 02:17.520]  Сначала складываем по одному, потом складываем по двое, потом складываем со сдвигом 4,
[02:17.520 --> 02:24.200]  потом делаем сдвиг на 8. Да, то есть, в итоге у нас получается итоговый результат в виде сумм чисел
[02:24.200 --> 02:30.600]  на префиксе. Чем этот алгоритм неудачный? Давайте вспомним, что мы с вами обсуждали. Какая симптотика
[02:30.600 --> 02:48.480]  у него будет? Ну, не квадрат будет. У нас высота этой... Да, а симптотика этого алгоритма будет
[02:48.480 --> 02:55.760]  аналоган, если его делать в тупую. Значит, если мы будем воспроизводить это для всех элементов
[02:56.080 --> 03:04.720]  массива. После этого мы сказали следующее, что если мы с вами научимся вычислять сумму на блоке,
[03:04.720 --> 03:16.520]  а дальше научимся каким-то образом эти суммы агрегировать, желательно золотые,
[03:16.520 --> 03:26.320]  то оказывается, что а симптотика алгоритма резко уменьшается. Ну, не так сильно она уменьшается,
[03:26.320 --> 03:32.600]  но в целом ее можно будет оценить. Мы понимаем с вами, что на самом деле количество операций,
[03:32.600 --> 03:42.080]  которое мы делаем для того, чтобы из N сделать N делить на блок size, у нас с вами сколько? У нас
[03:42.080 --> 03:49.080]  с вами количество операций будет равное N на логарифм блок size. Потому что количество операций,
[03:49.080 --> 03:54.680]  которые у нас идут, это N логарифм блок size. Но в итоге кажется, что мы с вами будем получать
[03:54.680 --> 04:03.480]  тот же самый N логан, просто чуть-чуть хитрее. То есть за счет атомарных операций мы будем
[04:03.480 --> 04:10.640]  работать чуть-чуть быстрее. Значит, мы будем повторять эти операции. То есть у нас N на логарифм
[04:10.640 --> 04:18.120]  блок size плюс N делить на блок size, на логарифм блок size плюс 1, плюс и так далее. Значит,
[04:18.120 --> 04:29.760]  если мы посчитаем эту сумму, то сколько у нас будет? Мне кажется, что будет сколько? Рубрика
[04:29.760 --> 04:47.680]  арифметика. Так, один. Помогите мне, пожалуйста. Чего равняется эта сумма приблизительно? Мне
[04:47.680 --> 04:59.680]  кажется, что эта сумма равняется... Что, правда? Нет, не сумма обратных, это сумма арифметической
[04:59.680 --> 05:07.880]  прогрессии, о геометрической прогрессии. Да, в целом можно сказать, что для нашей задачи это будет
[05:07.880 --> 05:18.120]  приблизительно 1. Вот, и плюс еще останется N на логарифм N по основанию блок size. Да, то есть мы
[05:18.120 --> 05:23.520]  с вами экономим симптотику. Более того, что мы можем с вами сказать про константу этого алгоритма?
[05:23.520 --> 05:29.640]  Поскольку у нас все действия будут с разделяемой памяти, то константа здесь будет меньше. Более того,
[05:29.640 --> 05:36.400]  мы с вами помним, что если мы работаем с вами с нашими алгоритмами на видеокарте, то мы все это
[05:36.400 --> 05:43.920]  делим на C, где C количество кудоядер. Ну, на самом деле не на C, но в какой-то степени мы будем это
[05:43.920 --> 05:49.520]  делить. Вот, и оказывается, что, как ни странно, вот эта вот симптотика, она оказывается, даже с учетом
[05:49.520 --> 05:55.080]  того, что нам приходится копировать наши данные с ЦПУ на ГПУ и обратно, она будет работать быстрее,
[05:55.080 --> 06:01.600]  чем просто посчитать последнюю сумму элементов масси. Вот, поэтому этот алгоритм замечательный,
[06:01.600 --> 06:09.440]  но если мы с вами научимся считать сумму чисел между блоками. И давайте как раз я еще раз напомню,
[06:09.440 --> 06:14.080]  как считать сумму чисел между блоками. На самом деле алгоритм достаточно такой не сложный,
[06:14.080 --> 06:27.400]  но, грубо говоря, в коде он нам позволит посчитать несколько более сложных операций. Так, заодно мы
[06:27.400 --> 06:36.800]  этот алгоритм можем использовать и не только для этих задач. Так, стираем все с доски и получаем
[06:36.800 --> 06:41.800]  следующее. Значит, представьте себе, что у нас были массивы. Я буду обозначать их сразу элементами.
[06:41.800 --> 07:02.080]  0, 1, 2, 3, 4, 5, 6, 7. Вот, и давайте считать, что мы будем с вами считать эксклюзивный скан,
[07:02.080 --> 07:08.560]  то есть считать суммы, начиная с нулевого элемента, и при этом последнее число будет
[07:08.560 --> 07:15.040]  возвращаться как сумма чисел в блоке. Тогда смотрите, что у нас получается после выполнения
[07:15.040 --> 07:30.720]  каждой операции. Значит, здесь у нас получается 0. Так, точнее сначала здесь пустота. Дальше здесь
[07:30.720 --> 07:52.160]  еще раз пустота. Так, дальше у нас получается 4, 5, 6. Дальше еще одна пустота и еще одна пустота.
[07:52.160 --> 08:06.360]  Так, здесь у нас будет сумма всех 12-15, здесь у нас будет сумма 8-11, здесь у нас будет сумма
[08:06.360 --> 08:17.000]  4,7. Значит, теперь как у этого всего посчитать сумму чисел? Мы делаем следующее. Мы берем вот
[08:17.000 --> 08:34.560]  эти вот штуки, загоняем в отдельный массив, получаем 0-3, 7, 12-15. И давайте для этой штуки тоже
[08:34.560 --> 08:43.120]  прогоним эксклюзивный скан. Собственно, что у нас получится? У нас получится пустота. Дальше
[08:43.120 --> 08:50.920]  получим 0-3. Этот с этим просуммируется, получим 0-7. Здесь получим 0-11. И на выходе мы получим
[08:50.920 --> 08:59.160]  сумму чисел в массиве. Сколько у нас тут будет? У нас тут будет сумма с 0 по 15. А теперь что... То есть
[08:59.160 --> 09:06.400]  у нас тут еще этот скан. Что мы с вами можем сделать? Мы можем сделать хитрую вещь. Мы можем
[09:06.400 --> 09:13.320]  взять и сделать операцию суммирования каждого числа с номером потока, в котором он находится.
[09:13.320 --> 09:19.960]  То есть здесь у нас будет сумма чисел 0-0-1-0-2. Значит, когда мы проинжектим вот этот блок,
[09:19.960 --> 09:33.320]  вставим, мы получаем с вами 0-6. Значит, когда мы вставим вот эту сумму с 0-7, мы получим сумму
[09:33.320 --> 09:39.240]  уже начиная 0-7, 0-8 и так далее. И когда мы вставим с вами последний блок, у нас будет общая сумма
[09:39.240 --> 09:47.120]  чисел массива. То есть тем самым мы с вами реализовали опцию скан. То есть вот одна дополнительная
[09:47.120 --> 09:54.160]  симпатика ОАТН, которая у нас получалась. Вот этот вот инжект. Это как раз у нас ОАТН.
[09:54.160 --> 10:07.360]  Так, это я еще раз разъяснил тот алгоритм, который мы в прошлый раз делали. Так, понятен ли этот
[10:07.360 --> 10:21.600]  алгоритм? Вот, отлично. Ну, спрашивается, а почему бы это все не реализовать за ОАТН?
[10:21.600 --> 10:32.640]  В сумме. Кажется, что у нас не очень большое количество операций. То есть мы дублируем лишние
[10:32.640 --> 10:38.520]  операции, потому что на слайде кажется, что много из этого не надо. Но давайте подумаем,
[10:38.520 --> 10:41.600]  что мы можем сделать для того, чтобы попробовать не дублировать операции.
[10:41.600 --> 11:08.240]  Давайте порисуем немного, что у нас в массиве находится. Так, мы будем с вами считать эксклюзивный
[11:08.240 --> 11:34.280]  скан. Сразу скажу. И давайте вот представим себе, что у нас с вами есть числа. Так, 07. Хотим мы в сумме
[11:34.280 --> 12:01.600]  посчитать следующую сумму. Пустое множество. Так, 0. 04. Да, что-то я промахнулся с количеством
[12:01.600 --> 12:09.480]  операций. Вот так как-то. Лучше будем считать, что я ровно нарисовал каждую ячейку под каждой. Так,
[12:09.480 --> 12:17.680]  ну давайте подумаем. Значит, что хотелось бы получить? Тут нужно сделать следующее. Давайте
[12:17.680 --> 12:30.080]  разделим массив пополам. И поймем, что у нас есть в правой половине нашего разделения того,
[12:30.120 --> 12:42.280]  чего нет в левой половине. Что есть общего у всех вот этих вот сумм, и нету вот этих сумм.
[12:42.280 --> 12:55.840]  Нет, сумма 0.2 тоже здесь есть. Тройка, да. Смотрите, здесь у нас есть сумма 0.3. А где мы ее получим
[12:55.880 --> 13:04.200]  здесь, в верхней части экрана? Наверное, мы ее получим вот таким образом. Если мы посмотрим
[13:04.200 --> 13:13.840]  наш алгоритм классический, если мы уберем лишние стрелочки, то мы сумму чисел получим здесь,
[13:13.840 --> 13:22.880]  за два шага. Вот, и каким-то образом нам нужно будет попытаться перенести эту сумму в правую
[13:22.880 --> 13:32.320]  часть. Давайте подумаем, чего еще у нас есть в этой сумме. Дальше, если мы детально раскинем
[13:32.320 --> 13:38.280]  нашу картинку, то мы увидим здесь следующее в водораздел. Что у этой половинки есть сумма чисел
[13:38.280 --> 13:45.440]  от 4 до 5, а здесь нету суммы чисел от 4 до 5. То есть нам нужно каким-то образом еще посчитать
[13:45.440 --> 13:54.000]  сумму чисел от 4 до 5. Вот, она каким-то образом накапливается. И давайте мы делаем следующее,
[13:54.000 --> 14:01.120]  что мы будем накапливать только конкретно свою сумму, которая необходима. То есть у нас получается
[14:01.120 --> 14:11.760]  с вами сумма чисел 4,5, 6,7. Дальше мы получаем сумму от 4 до 7. Вот, а теперь давайте подумаем,
[14:12.000 --> 14:20.560]  что мы с этим можем с вами сделать. Поскольку у нас эксклюзивный скан, я хочу подчеркнуть,
[14:20.560 --> 14:26.240]  что у нас скан эксклюзивный, то, в принципе, наверное, что-то с этим можно сделать. Давайте
[14:26.240 --> 14:46.720]  я здесь выполню проекцию. 0, 1, 2, 4, 4, 5, 6, 4, 7. Давайте думать. Значит, идея такая, а давайте попробуем
[14:46.720 --> 14:55.360]  те части, которые были делим наш массив пополам и перекидываем левую часть направо.
[14:56.960 --> 15:03.760]  То есть нам нужно перекинуть левую часть направо. А с правой частью мы поступим хитрее. Смотрите,
[15:03.760 --> 15:11.000]  здесь первый шаг нетривиальный. Мы забываем про вот этот вот элемент и прокидываем пустоту. То
[15:11.000 --> 15:18.120]  есть смотрите, идея такая, что здесь сумма чисел от 0 до 3, здесь у нас пустой элемент. Что такое вот
[15:18.120 --> 15:27.520]  это? Вот это 0,3 плюс пустота, вот это 0,3 плюс 4, вот это 0,3 плюс 4,5, это 0,3 плюс 4,6. А здесь пустота 0,
[15:27.520 --> 15:33.400]  0,1, 0,2. Соответственно, мы делаем следующую вещь. Мы берем и прокидываем пустоту сюда.
[15:33.400 --> 15:40.680]  То есть получаем здесь пустоту, а здесь сумму чисел от 0 до 3.
[15:40.680 --> 15:50.320]  Ну смотрите, что у нас получается. У нас на самом деле в какой-то момент алгоритм перестанет
[15:50.320 --> 15:59.480]  работать. Давайте посмотрим внимательно. Если бы мы получили бы элемент здесь и оставили бы его,
[15:59.480 --> 16:07.040]  не ставили пустотой, то дальше бы мы эту сумму забыли затереть. Но давайте посмотрим внимательно.
[16:07.040 --> 16:15.920]  Здесь у нас сумма чисел 4,5 и хочется, что нам сделать? У нас есть сумма 4,5 и давайте мы ее
[16:15.920 --> 16:25.400]  сагрегируем. То есть сложим ее здесь и получим сумму чисел от 0 до 5. А здесь мы прокидываем число
[16:25.400 --> 16:33.000]  от 0 до 3. То есть у нас получается такой перекрестный алгоритм сложения. То есть идея такая,
[16:33.000 --> 16:41.480]  что левую часть мы перекидываем вправую и с ней складываем. А вправой мы берем и складываем ее,
[16:41.480 --> 16:47.120]  правую часть просто перекидываем налево. То есть получается у нас такое перекрестное сложение.
[16:47.120 --> 17:04.720]  Здесь у нас идет перекрестное сложение, здесь идет перекид. Вот такой алгоритм интересный.
[17:04.720 --> 17:11.560]  У нас с вами получается так. Последняя стадия это перекрест. Я сейчас покажу слайд. Я просто
[17:11.560 --> 17:17.800]  хотел показать идею того, как это работает. Никакую структуру данных вам это не напоминает?
[17:17.800 --> 17:37.440]  Ну не совсем дерево отрезков. Другое дерево. Да, это дерево фенвика. Да, это ровно дерево
[17:37.440 --> 17:44.160]  фенвика. Именно то, как в нем сумму вычисляется. Сумма числа на префикс. То есть она неявно
[17:44.160 --> 17:50.200]  позволяет нам вычислить сумму числа на префикс. Кажется алгоритм замечательный. Да, он выполняет
[17:50.200 --> 17:58.600]  обольшое аттен действий. Да. Но давайте подумаем, в чем здесь может быть проблема?
[17:58.600 --> 18:13.280]  Во-первых, сколько нам действия придется теперь делать на каждый блок? Во-первых,
[18:13.280 --> 18:21.160]  нам нужно будет сделать удвоенный алгоритм блок size действий. Ладно, это еще ничего страшного.
[18:21.160 --> 18:27.440]  Вопрос. Сколько варпов при этом у нас будет задействовано?
[18:35.640 --> 18:40.280]  В целом даже можно сказать, что у нас будет задействовано порядка от варпов, потому что,
[18:40.280 --> 18:46.400]  наверное, если мы будем складывать сумму чисел 0 и первого элемента, неплохо было бы и делать
[18:46.400 --> 18:55.240]  нулевым потоком. Да, а вот это сумму чисел было бы неплохо складывать первым потоком. Так,
[18:55.240 --> 19:05.560]  но здесь у нас опять возникает некоторая проблема. Кто видит проблему? Мы в прошлой лекции переместили
[19:05.560 --> 19:11.200]  арифметику сложения наших элементов, и внезапно наш код начал работать медленно.
[19:16.400 --> 19:27.000]  Кто помнит по какой причине? Да, у нас бан конфликт возникает, когда мы в одну
[19:27.000 --> 19:34.000]  ячейку в разделяемой памяти внутри одного варпа пишем с двух разных потоков. Давайте
[19:34.000 --> 19:50.720]  посмотрим, где у нас тут варп конфликт наблюдается. Конфликт здесь наблюдается
[19:50.720 --> 20:01.880]  в данном моменте времени. Представьте себе, что у нас с вами есть сложение нулевого и первого
[20:01.880 --> 20:09.640]  элемента массива. Берется нулевой поток, и он складывает значение в первую ячейку массива.
[20:09.640 --> 20:20.120]  Параллельно где-то есть у нас 32 и 33 ячейка массива, в которой складываются элементы с 16 потоком.
[20:20.120 --> 20:30.400]  И запись идет в 33 ячейку. Завечу, что по модулю 32 эти ячейки имеют одинаковый индекс,
[20:30.400 --> 20:41.200]  это означает, что мы с вами получаем варп конфликт. Казалось бы, как его можно попробовать
[20:41.200 --> 20:48.400]  решить. Замечу, что нумерацию своим индексом делать тоже плохо, потому что тогда у нас
[20:48.400 --> 20:59.920]  большое количество варпов начинает работать. Есть ли у кого-то еще вариант?
[21:18.400 --> 21:33.400]  Ну можешь попробовать. Так. Да, я проспойлерил, черт. Жалко. Верите ли вы в паранормальное явление,
[21:33.400 --> 21:42.720]  в мистику? Да. Кстати, как-то странно, я эту мистику показал ровно таким образом,
[21:42.720 --> 21:55.840]  как она происходит. Явление 25-го кадра, слышали? А, блин, какое 25-е кадр сейчас расширение частота
[21:55.840 --> 22:04.840]  мониторов 60 и 140 Гц. Ну да, 61-й кадр получается. То есть идея такая, что давайте вместо того,
[22:04.840 --> 22:13.000]  чтобы... Какие мысли были? Что людям, чтобы внедрить какую-то рекламу или какое-то мнение,
[22:13.000 --> 22:20.720]  нужно показывать один дополнительный кадр по телеку. Вот. Для того, чтобы им сомну шанс.
[22:20.720 --> 22:36.840]  Ну типа того. Вот. А давайте мы как раз поступим здесь ровно таким же образом. Мы с вами введем
[22:36.840 --> 22:49.760]  дополнительный кадр. Только у нас это будет не получается, не 25-й кадр, а 33-й кадр.
[22:49.760 --> 22:58.720]  Значит, в чем суть? Суть в том, что вы берете вот этот вот элемент, вот этот наш чистый массив,
[22:58.720 --> 23:11.600]  и закрашите каждый 32-й кадр. Тогда смотрите, как у нас меняется индексация. То есть у нас
[23:11.600 --> 23:17.280]  получается, что теперь вот этот вот элемент меняется 32-м актуальным. Но на самом деле номер
[23:17.280 --> 23:24.600]  кадра 33-й. Поэтому 16-й поток, несмотря на то, что он будет складывать элементы 32-й и 33-й,
[23:24.600 --> 23:36.880]  он будет складывать на самом деле со сдвигом на один. То есть поток-то 16-й, кладет-то он тоже
[23:36.880 --> 23:43.800]  в 33-ю ячейку массива. Но на самом деле это будет не 33-й элемент массива, а 34-й элемент массива.
[23:43.800 --> 23:55.920]  Да, потому что у нас двиг произошел на одну ячейку. Более того, этот эффект будет повторяться в
[23:55.920 --> 24:12.480]  дальнейшем. Какой-то момент оно опять совпадет. Ну через сколько она совпадет? Ну да, то есть на
[24:12.480 --> 24:25.080]  самом деле номер актуального потока будет следующим. Н плюс Н делить на 32. Да, и как ни
[24:25.080 --> 24:30.800]  странно, можно определить минимальное значение элемента И, в котором И поделить на 32 будет
[24:30.800 --> 24:43.440]  равняться 32. Вопрос, чего тогда равняется И? Ой, внезапно, а у нас максимальный размер блока 1024.
[24:43.440 --> 24:56.480]  Все. То есть сдвиг работает. На семинарских примерах, по-моему, там выигрыш идет... Сколько
[24:56.480 --> 25:03.600]  будет 30 на 25? 20 процентов дает выигрыш. Вот этот вот способ. И несмотря на это, вот даже решение
[25:03.600 --> 25:13.680]  задачи при помощи 33-го кадра будет на 20 процентов медленнее, чем тупое решение сложить все совсем.
[25:13.680 --> 25:28.120]  Ну, то есть сделать N-логан действий, но при этом с хорошей симптотикой. Так. Есть ли вопросы
[25:28.120 --> 25:35.640]  по этому алгоритму? Ну, собственно, если немножко раскурить и подумать, какие алгоритмы позволяют
[25:35.640 --> 25:40.400]  решать дерево фенвика, то на видеокарте мы понимаем, что у нас есть параллельное дерево
[25:40.400 --> 25:47.920]  фенвика под капотом. Вот. И в принципе его можно как раз использовать в параллель. Вот. Но это на самом
[25:47.920 --> 25:55.360]  деле полезный момент. Да. Кстати, вот еще раз один индексации, что у нас с индексацией меняется.
[25:55.360 --> 26:04.440]  Это мы тоже с вами посудили. Но можно сделать еще и так. Собственно, что делается здесь? Здесь
[26:04.440 --> 26:10.560]  происходит наш хитрый момент, который заключается в том, что 1024 это 32 в квадрате.
[26:10.560 --> 26:18.840]  Ну, а поэтому почему бы нам не использовать большое количество синхронизаций, а попробовать
[26:18.840 --> 26:29.280]  сделать подсчет суммы внутри каждого варпа, потом раскинуть сумму на префиксе и посчитать
[26:29.280 --> 26:36.280]  сумму еще один раз. Да, в итоге опять же мы с вами экономим количество операций синхронизации,
[26:36.280 --> 26:44.240]  которого у нас имеется. И вот эта реализация алгоритма является одной из самых оптимальных
[26:44.240 --> 26:52.760]  реализаций нашего кода. Да, тут говорится как раз, как распределяются вот эти вот агрегированные
[26:52.760 --> 26:58.640]  суммы. То есть как раз, смотрите, у нас считается сумма на префиксе, как это у нас до этого было.
[26:59.600 --> 27:05.240]  Дальше мы говорим с вами, что вот сумма чисел на префиксе как раз распределяется на свои собственные
[27:05.240 --> 27:11.840]  потоки. Как раз вот тот алгоритм, который мы с вами рассказали, и потом считается сумма еще раз.
[27:11.840 --> 27:22.880]  Вот такой вот хитрый алгоритм, который я сейчас реализую. Так, это мы с вами разобрали задачу
[27:22.880 --> 27:27.600]  scan. Давайте поговорим про ее применение. Первая задача — это задача compaction,
[27:27.600 --> 27:35.280]  кстати, которая вам предлагается реализовать в задании. Да, это последняя задача — функция
[27:35.280 --> 27:43.360]  filter. В чем она заключается? Она заключается в том, что нужно оставить те элементы массива,
[27:43.360 --> 27:48.800]  которые ультворяют определенному фильтру. То есть те значения, которые меньше какого-то
[27:48.800 --> 27:56.400]  определенного х. И второй способ — это сортировка массива. Если вы понимаете,
[27:56.400 --> 28:01.280]  как фильтровать значения меньше определенного, то какого алгоритма можно запустить?
[28:01.280 --> 28:08.320]  Ну, quick sort, может быть даже merge sort. То есть, в принципе, на видеокарте можно запустить
[28:08.320 --> 28:14.960]  merge sort. Более того, если вы умнеете правильно считать сумму на префиксе, то можно запустить
[28:14.960 --> 28:22.960]  radix sort. То есть, разные виды сортировок можно применять. Причем radix sort с битовой арифметикой.
[28:22.960 --> 28:28.480]  Ой, не с битовой, а с большой арифметикой. В принципе, можно будет подумать над этим.
[28:28.480 --> 28:34.880]  Значит, quick sort по разряду сортировок. Значит, как работает compaction? Представьте себе, что у нас
[28:34.880 --> 28:40.880]  есть исходный массив, и нам нужно отфильтровать массивы, которые ультворяют условия. То есть,
[28:40.960 --> 28:46.560]  это массивы с элементом массива P. Дальше, что мы делаем? Мы говорим метки массивов. Значит,
[28:46.560 --> 28:54.720]  у нас с вами получаются элементы 1 и 0. А после этого мы считаем сумму на префиксе. И смотрите,
[28:54.720 --> 29:00.560]  что вы можете сказать? Мы можем взять те элементы, в которых значение единичка, посмотреть на их
[29:00.560 --> 29:06.400]  индексы и взять как раз элементы с определенным индексом. То есть, образно говоря, смотрите,
[29:06.400 --> 29:15.160]  мы берем вот эту и вот эту ячейку, единичку и пятерку. Значит, вот этот элемент массива должен
[29:15.160 --> 29:22.040]  переехать на пятое место. Вот он переехал на пятое место у нас. Ну, если считать в один
[29:22.040 --> 29:26.320]  нумерации. Если считать ноль нумерации, то он точно переедет на свое собственное место.
[29:26.320 --> 29:31.440]  То есть, в зависимости от того типа скана, который вы делаете, inclusive или exclusive,
[29:31.440 --> 29:38.880]  немного меняется индексация. То есть, в принципе, функции, как мы поняли с вами,
[29:38.880 --> 29:46.160]  функция фильтр будет работать быстрее. На видеокарте. Да, опять же битовую маску тоже
[29:46.160 --> 29:51.200]  можно считать достаточно быстро. То есть, здесь нужно будет реализовать скорее всего либо два
[29:51.200 --> 30:02.040]  ядра, либо одно ядро. Так, понятно ли суть решения этой задачи? Отлично. Быстрая сортировка. Как
[30:02.040 --> 30:14.520]  реализовать? Быстрая сортировка. Да, выбираем случайный пивот, но главное дальше делать
[30:14.520 --> 30:19.560]  compaction in place одновременно для условий меньше и больше, чтобы не городить лишнюю память.
[30:19.560 --> 30:24.840]  Потому что, я помню, в какие-то года мы давали задания, связанные с быстрой сортировкой, и у
[30:24.840 --> 30:28.480]  людей оказывалось, что быстрая сортировка работает медленнее, чем обычная сортировка на
[30:28.480 --> 30:35.400]  обычном компьютере. Более того, было такое, что типа quick sort вообще не работал. Там что-то
[30:35.400 --> 30:40.160]  работал внутри одного блока, а между блоками не работает. Вот поэтому можно запускать рекурсию.
[30:40.160 --> 30:48.040]  Вот. То есть, это что касается быстрой сортировки наших массивов. Значит, из таких интересных
[30:48.280 --> 30:56.480]  вещей касательно вот этих алгоритмов. Мы рассмотрели с вами классические алгоритмы работы на видеокарте.
[30:56.480 --> 31:07.680]  Из алгоритмов, которые не надо реализовывать самостоятельно. Самостоятельно не стоит
[31:07.680 --> 31:20.400]  реализовать быстрое преобразование фурье. Значит, есть ее аналог в мире геометрии.
[31:20.400 --> 31:29.520]  Задача. То есть, быстрое преобразование фурье на видеокарте это то же самое, что некоторая
[31:29.520 --> 31:33.880]  геометрическая задача. Это задача построения правильного многоугольника с циркулем линейкой.
[31:33.880 --> 31:47.480]  Как это ни странно. Объясняю почему. Вот мы с вами в концепции редакшн и скан пытались избавиться от
[31:47.480 --> 31:55.720]  бан конфликтов. И дальше делается следующее, что если вы попробуете реализовывать быстрое
[31:55.720 --> 32:02.120]  преобразование фурье, то для достаточно большого количества чисел вы нарветесь на такое большое
[32:02.200 --> 32:06.360]  количество бан конфликтов, что если вы попробуете их реализовывать, вы получите еще больше бан
[32:06.360 --> 32:14.000]  конфликтов. В итоге реализация становится неэффективной. И вот тут если мы пойдем с
[32:14.000 --> 32:21.960]  вами и посмотрим библиотеку, связанную с быстрым преобразованием фурье. КУФФТ называется библиотека.
[32:32.120 --> 32:59.800]  Вам говорят т-т-т-т. Все замечательно. Где оно? Здесь есть прямо этот. Во, нашел. Вот
[32:59.800 --> 33:07.160]  прочитаем этот полк. Алгоритм хорошо сильно оптимизирован для размеров, которые представляют
[33:07.160 --> 33:10.800]  собой произведение двоих, троих, пятерок, семерок. При этом, если это степень двойки,
[33:10.800 --> 33:16.840]  у вас все замечательно. Если у вас эта степень двойки и тройки произведите, то чуть медленнее.
[33:16.840 --> 33:24.280]  Да, если у вас внезапно вы хотите вычислить быстрое преобразование фурье на числах,
[33:24.280 --> 33:33.400]  которые кратны 11 на размерах массива, извините, вам придется потерпеть. Вам вас переключат на
[33:33.400 --> 33:41.160]  классическую реализацию. Просто потому что количество синхронизации будет достаточно большим. Это что
[33:41.160 --> 33:48.240]  касается как раз видеокарты, библиотек на вот этих вещей. И сразу давайте скажу про те библиотеки,
[33:48.240 --> 33:53.320]  которые есть на видеокарте. На самом деле их очень много. Их много всяких разных. И самая
[33:53.320 --> 33:57.880]  классическая библиотека, которую можно попробовать использовать, это библиотека Кубласс. Собственно,
[33:57.880 --> 34:06.920]  это библиотека для линейного алгебра. Как ни странно, если вы пользуетесь каким-нибудь пакетом
[34:06.920 --> 34:14.720]  нампай, сцепай, либо еще что-нибудь, и внезапно перемножаете матрицы, то скорее всего вы
[34:14.720 --> 34:21.200]  перемножаете их при помощи Бласса, определенной реализации. Это наиболее эффективное вычисление
[34:21.200 --> 34:27.040]  операции в видеокарте, это при помощи Кубласса. Единственный момент, сразу скажу следующее,
[34:27.040 --> 34:32.360]  что если вы попытаетесь реализовать какую-нибудь линейную алгебру при помощи Бласса, то скорее
[34:32.360 --> 34:38.760]  всего код у вас будет сильно медленнее, если вы будете использовать чисто ее. То есть код на Куде
[34:38.760 --> 34:43.440]  все-таки намного быстрее. Но при этом там есть удобные механизмы того, что вы можете алоцировать
[34:43.440 --> 34:48.680]  массивы, заполнять их определенными элементами. Но правда, как обычно там сишный код, большое
[34:48.680 --> 34:56.000]  количество параметров. Следующая библиотека это QFFT, после этого есть CUDA Mass Library, то есть это
[34:56.000 --> 35:02.240]  стандартная библиотека математическая, то есть можно внутри кода ядер писать произвольные функции.
[35:02.240 --> 35:13.080]  После этого есть библиотека Curant, которая позволяет запускать псевдослучайный генератор. И из таких
[35:13.080 --> 35:19.960]  библиотек еще две есть. Интересных это Solver, который умеет решать линейные уравнения. Да,
[35:19.960 --> 35:26.480]  как это ни странно, на видеокарте достаточно быстро. А мы понимаем, что решение уравнений на видеокарте
[35:26.480 --> 35:35.240]  это полезная операция. Но я правда не знаю, фан может быть не сразу стоит, понятно, но там любая
[35:35.240 --> 35:41.480]  физика, аэродинамика и так далее, это решение линейных уравнений. Да, в частных производных. Ну,
[35:41.960 --> 35:47.520]  для этого как раз можно использовать видеокарту. И есть Qsparce, которая работает для Sparse Matrix,
[35:47.520 --> 35:55.560]  правда, так сказать, она не всегда эффективна. Поскольку у нас появились операции для работы
[35:55.560 --> 36:01.000]  с тензорами, то появилась как раз библиотека, связанная с тензорами. И есть еще внешняя библиотека,
[36:01.000 --> 36:07.320]  которая позволяет запаковать все в определенный набор, это AMGX. Это что касается математики,
[36:07.320 --> 36:13.240]  но куда же без параллельных вычислений, как говорится, не только на одном узле. И для этого есть
[36:13.240 --> 36:24.080]  тоже две библиотеки. Первая это спецификация OpenShme, OpenSharedMemory, NVIDIA SharedMemory,
[36:24.080 --> 36:28.080]  которая позволяет разделить общую память между несколькими видеокартами. Но на самом деле,
[36:28.080 --> 36:38.600]  зачастую это не используется. Зачастую используется сеть библиотека по названиям NCCL. То есть она как
[36:38.600 --> 36:45.840]  раз позволяет реализовывать коллективные операции в MPI на уровне видеокарт. То есть си реализовывает
[36:45.840 --> 36:51.000]  распределенное обучение. А по факту, если вы хотите обучать нейросети, то как дистранда вам нужно
[36:51.000 --> 36:57.440]  просто уметь считать средний градиент по больнице. Вот, между всеми вычислительными узлами. Ну и как
[36:57.440 --> 37:02.880]  раз эта библиотека вполне спокойно это позволяет сделать. Так, кода и искусственный интеллект.
[37:02.880 --> 37:09.360]  Куда же без него? Здесь есть библиотеки связанные с первой, две известные. Первая это QDNN,
[37:09.360 --> 37:19.560]  Kuda Deep Neural Networks. Значит потихоньку от нее отходят от библиотеки QDNN. То есть ее еще
[37:19.560 --> 37:25.160]  редко увидеть в классических реализациях. Но в свое время она была достаточно популярной. То есть
[37:25.160 --> 37:32.960]  насколько я знаю современные библиотеки Torch, они могут работать и без QDNN. Вот. И вторая библиотека
[37:32.960 --> 37:40.040]  это библиотека TanzRRT. Она позволяет сильно ускорить производство нейросети. То есть прогон нейросети
[37:40.040 --> 37:47.080]  в режиме Production. Вот. Ну и на самом деле, если вы пользуетесь большим количеством плагинов,
[37:47.080 --> 37:52.920]  то в принципе тоже могли замечать, что основная обработка голоса проводится на уровне видеокарты
[37:52.920 --> 38:02.000]  и так далее. Значит если касаться того, какие есть аналоги Kuda. Потому что Kuda везде доступна.
[38:02.000 --> 38:10.880]  Один из основных форматов это OpenCL. То есть это аналог Open Graphical Language для других видеокарт.
[38:10.880 --> 38:19.360]  Да. И как ни странно, писать код на OpenCL это интересно. Можно зайти в репозиторию TanzRflow
[38:19.360 --> 38:24.400]  и посмотреть, как реализован фреймворк TanzRflow Lite, который позволяет запускать
[38:24.400 --> 38:29.400]  нейросети на мобильных устройствах. И там по факту идет просто огромный свитч на то,
[38:29.400 --> 38:39.040]  какой у вас графический ускоритель. И после этого запускается. Значит. Конечно же у разных
[38:39.040 --> 38:44.680]  сейчас производителей появляются тоже свои собственные механизмы. Если мы говорим про Apple,
[38:44.680 --> 38:57.840]  то это механизм под названием MPS. Его поддерживает сейчас, кстати, Kuda. И если мы говорим про
[38:57.840 --> 39:07.440]  AMD видеокарты, то это RockM. То есть у каждых движков есть свои собственные тоже механизмы. Но,
[39:07.440 --> 39:13.680]  как ни странно, Kuda стала достаточно популярной и поэтому используют ее. Ну в принципе OpenCL
[39:13.680 --> 39:19.000]  тоже поддерживается. Значит есть вообще, если вы вообще не хотите писать код на видеокарте ни в
[39:19.000 --> 39:27.240]  какой степени, то для этого есть библиотека OpenACC. Значит на семинарах кто-то из вас проходил
[39:27.240 --> 39:34.120]  библиотеку OpenMP, в которой все при помощи Pragma написано. Прагма OMP Parallel4, а под капотом вам
[39:34.120 --> 39:42.680]  TreadLoop создается. Вот. Такое же самое можно организовать и на видеокарте. Значит из таких
[39:42.680 --> 39:49.080]  дополнительных вещей тоже, значит, про библиотеки, если вы внезапно не хотите писать на питоне,
[39:49.080 --> 39:56.760]  ой, писать на питоне, да, то, в общем, есть несколько разноуровневых механизмов. Ну,
[39:56.800 --> 40:08.040]  первый это из пушки по воробьям. Вы не поверите. Нет, сам высокий уровень. Взять PyTorch, короче говоря.
[40:08.040 --> 40:18.600]  Просто взять PyTorch и взять операцию с тензорами, которая у него там имеется. Работать будет медленно,
[40:18.600 --> 40:25.080]  но будет работать. Тем более матрицу перемножать она сможет и на тензорных ядрах. Значит дальше
[40:25.240 --> 40:29.960]  спускаться на уровень библиотек, то в питоне есть библиотека под названием Numba,
[40:29.960 --> 40:38.520]  которая умеет работать с видеокартами. Если спускаться еще ниже, то здесь обычно существуют
[40:38.520 --> 40:45.960]  две параллельных библиотеки, которые дружат друг с другом. Первая это называется PyCuda. Она
[40:45.960 --> 40:53.200]  имеет как low-level interface, связанная с тем, что вы пишете ядро на C, потом вы компилируете,
[40:53.600 --> 41:02.600]  и дальше из питона дергаете. И дополнительно есть набор примочек, связанных к ней. Это библиотека
[41:02.600 --> 41:11.880]  Рейкна. В ней есть некоторые другие ядра, которые можно использовать. И в принципе в них как раз
[41:11.880 --> 41:19.760]  уже задача редакшена и скана уже реализована. То есть по факту их с нуля обычно не приходится
[41:19.760 --> 41:26.360]  писать. Так, это что касается задач редакшен скан и библиотек, которые мы с вами можем
[41:26.360 --> 41:45.000]  использовать для работы со всем этим чудом. Так, давайте вопрос. Хорошо. Я на самом деле
[41:45.000 --> 41:51.560]  подготовил Google Collab с этим с материалами по PyCuda, так что возможно на семинарах в этом
[41:51.560 --> 41:59.800]  году как раз эти примеры будут разбираться. А мы поедем к следующей лекции. И как ни странно,
[41:59.800 --> 42:07.600]  мы еще не все с вами разобрали с видеокартами. Последний блок будет посвящен тем, а как же
[42:07.600 --> 42:14.400]  работают на самом деле операции на видеокарте. Мы с вами говорили следующее, что операция CUDA
[42:14.400 --> 42:25.720]  memcpi у нас является... Какой? Блокирующей. Что произойдет, если ее сделать не блокирующей?
[42:25.720 --> 42:34.120]  Не, на самом деле печаль не будет. Просто нужно будет понимать каким образом работает конвертность
[42:34.120 --> 42:39.840]  видеокарте. И как раз для этого нам нужно будет разобрать нашу преамплу. Кажется,
[42:39.840 --> 42:45.800]  что мы уже научились все делать на видеокарте. Но как ни странно, мы с вами до сих пор используем
[42:45.800 --> 42:54.960]  всего лишь три параметра вызова ядра. А их четыре. Всего параметров вызова ядра четыре. И хотелось бы
[42:54.960 --> 43:03.120]  разобраться, каким образом можно использовать четвертый параметр вызова ядра. Но не все так просто.
[43:03.120 --> 43:08.560]  Так, смотрите. Вот обычно у нас выглядит вот набор команд выглядит вот таким образом. То есть у нас
[43:08.560 --> 43:14.160]  есть с вами CUDA memcpi. Более того, мы можем ее использовать синхронно и можем использовать
[43:14.160 --> 43:20.440]  ассинхронно. Если ассинхронную операцию CUDA memcpi async использовать, то у вас все будет выглядеть
[43:20.440 --> 43:28.320]  ровно таким же образом, как в классическом коде на видеокарте. То есть вы можете этот async просто
[43:28.320 --> 43:35.280]  убрать и у вас будет все то же самое. Изначально вот такой у нас поток команд. У нас время выполнения
[43:35.280 --> 43:42.280]  команд ровно такое. Сразу скажу, что здесь скорость замера программы будет осуществляться и с учетом
[43:42.280 --> 43:49.720]  операции копирования. Вот это очень важно. То есть мы делаем еще учет операции копирования. Можно
[43:49.720 --> 43:57.920]  сделать так. Смотрите, мы делаем четыре потока, четыре стрима объекта и делаем следующее. У нас
[43:57.920 --> 44:04.240]  у нас есть ядро один и после него мы сразу вызываем CUDA memcpi device to host синие стрелочки. И
[44:04.720 --> 44:13.760]  оказывается, что поскольку у нас на видеокарте строится граф зависимости по коду, то в конвейере
[44:13.760 --> 44:23.160]  получается так, что на видеокарте memcpi имеет свой собственный поток управления. А это означает,
[44:23.160 --> 44:30.200]  что в принципе device to host, копирование хостана девайса с девайса на хост может осуществляться
[44:30.200 --> 44:36.840]  параллельно. Но опять же главное, чтобы у нас стояла зависимость по данным, потому что если у нас нет
[44:36.840 --> 44:45.200]  зависимости по данным, то контроля у нас не будет. Можно сделать вот так. Что мы говорим? Давайте мы
[44:45.200 --> 44:54.120]  попробуем с вами использовать concurrency на уровне как раз потоков исчислений, потоков. То есть что
[44:54.120 --> 45:02.480]  получается? Мы можем сделать еще одну операцию, так называемую freeway concurrency. Когда мы копировать
[45:02.480 --> 45:10.920]  данные будем в одном стриме, потом сразу на нем будем выполнять код, выполнять код еда, а дальше
[45:10.920 --> 45:19.480]  потом копировать снова. В итоге у нас получается вот такая пирамидка, называется freeway concurrency.
[45:19.480 --> 45:26.560]  Но в принципе, если мы с вами умные люди, то можем понять, что при желании мы можем с
[45:26.560 --> 45:35.880]  вами и четвертый поток исполнения использовать. Это поток исполнения на нашем ЦПУ. То есть мы
[45:35.880 --> 45:42.080]  закинули код выполнения ядра, у нас все операции синхронные. Это означает, что наш основной поток
[45:42.080 --> 45:48.760]  исполнения выполняется. Может выполнять какие-то действия. И дополнительно можно после этого
[45:48.760 --> 45:55.160]  попробовать все это дело еще и атомизировать. То есть выполнять не одно ядро целиком, а разбить его
[45:55.160 --> 46:02.200]  на поднаборы ядер. Тем самым у нас пирамидка увеличивается. То есть это сколько способов
[46:02.200 --> 46:08.480]  осуществить concurrency у нас есть. И оказывается следующее, что если посмотреть на количество
[46:08.480 --> 46:14.840]  операций в секунду, то на ЦПУ это все работает порядка 40 гигафлопс. Да, это кстати бенчмарки
[46:14.840 --> 46:21.160]  до этого. А теперь смотрите, классический ГПУ нам от силы даст 120 гигафлопс с учетом операции
[46:21.160 --> 46:27.400]  копирования туда-обратно. Но если попробовать использовать freeway concurrency, то есть скопировать
[46:27.400 --> 46:32.760]  в одном месте, а потом копировать по факту исполнения обратно, то мы уже получаем ускорение в
[46:32.760 --> 46:39.000]  полтора раза. Третье, что у нас есть, это если мы еще и копирование в одну сторону сделаем,
[46:39.000 --> 46:44.960]  то мы ускоримся еще где-то практически в полтора раза. Если мы подключим поток исполнения на ЦПУ,
[46:44.960 --> 46:51.000]  мы конечно не сильно преувеличим, но в принципе можем использовать наши операции. Вот, и в целом
[46:51.000 --> 46:58.880]  в бесконечности в пределе у нас получается 330 гигафлопс в секунду. 330 гигафлопс. Тут, знаете,
[46:58.880 --> 47:04.760]  есть какая абстракция. Значит, по-хорошему есть такая физическая задача. Значит, у вас есть
[47:04.760 --> 47:13.240]  кирпич. Поверх него вы ставите еще один кирпич, чтобы первый кирпич не упал, максимально выпирающий.
[47:13.240 --> 47:19.960]  Потом вы ставите второй кирпич таким образом, чтобы эти кирпичи не выпирали, не падала эта
[47:19.960 --> 47:26.600]  конструкция. И повторяете так до победного. Вопрос, типа, а какое максимальное отклонение вы
[47:26.600 --> 47:34.920]  можете получить? Вот, с этой конструкцией кирпичей, чтобы ее не перевернуло. Вот,
[47:34.920 --> 47:40.160]  в принципе задача здесь ровно такая же. Слышали про такую физическую задачу?
[47:40.160 --> 48:08.560]  Да, да, да. Сейчас, тут можно даже попробовать это. Ну да. Не, не, не, до задачи.
[48:08.680 --> 48:11.320]  Вот она.
[48:22.880 --> 48:30.720]  В двойке стремится ответ. Ну вот, здесь тоже самое на видеокарте мы получаем. Тот же самый эффект.
[48:30.720 --> 48:36.680]  Вот, а это значит, что нам нужно познакомиться с этим объектом, который нам позволит это сделать.
[48:36.680 --> 48:43.280]  И это объект, откуда стримы. Значит, говорить следующее, что если задача легкая,
[48:43.280 --> 48:49.800]  почему бы ее не спустить в отдельном стриме? Сразу скажу, что поток и стрим, трет и стрим,
[48:49.800 --> 48:55.320]  это две разных концепции. Более того, я скажу следующее, что если вы попробуетесь воспользоваться
[48:55.320 --> 49:02.560]  библиотекой Рейкна, то там стрим называется поток. То есть, там стрим называется как трет.
[49:02.560 --> 49:20.120]  Ну не знаю. Видимо не все. Вот. Чего? Да я не уверен.
[49:20.120 --> 49:32.880]  Значит, смотрите, по умолчанию мы работаем с вами в нулевом потоке. В нулевом стриме, так сказать.
[49:32.880 --> 49:37.880]  Извините, я что-то говорил. Вот говорит стрим. Вот. И этот стрим по умолчанию синхронный к хосту
[49:37.880 --> 49:43.000]  и девайсу. То есть, как бы все операции, которые осуществляются в нулевом стриме,
[49:43.000 --> 49:49.480]  они по умолчанию являются синхронным. То есть, как бы у вас could be spying a sync в нулевом стриме
[49:49.480 --> 49:57.640]  будет вести себя ровно так же, как просто обычный could be spying. Вот. Значит, если вы не указываете
[49:57.640 --> 50:05.560]  параметры, то у вас параметр равен нулю. Хорошо. Значит, да, синхронный на хосту и девайс.
[50:05.560 --> 50:17.040]  Значит, в чему синхронный? Значит, стрим синхронен. Значит, следующая вещь. К запуску гидра. Раз. Дальше.
[50:17.040 --> 50:25.080]  К операции could be spying. Два. Третья операция could be spying, но если вы копируете память с девайса
[50:25.080 --> 50:32.840]  на девайс. То есть, вам не нужно синхронизировать хостай девайса. И как ни странно, стримы синхронны
[50:32.840 --> 50:40.880]  еще к операции could be spying хоста на девайс. Да, потому что вы отправляете буферизированный вывод
[50:40.880 --> 50:46.720]  на видеокарту. То есть, по факту мы даже, кажется, про это говорили, когда мы говорили, что мы с вами
[50:46.720 --> 50:53.280]  можем и спин в памяти отправлять наши данные. То есть, нам достаточно сделать аналог МАП в виртуальной
[50:53.280 --> 51:00.480]  памяти. То есть, пригвоздить нашу виртуальную память к реальной и сделать копирование. Все это
[51:00.480 --> 51:05.840]  происходит на уровне одинкаширования. Так, вот он код. Пример этого кода. То есть, мы делаем
[51:05.840 --> 51:13.560]  could be malloc, а дальше мы делаем could be memcpy. Это вот синхронное ядро. То есть, это то, как мы
[51:13.560 --> 51:21.880]  раньше писали все. Грит блок ноль. А теперь смотрите, как у нас меняется код. Собственно, у нас
[51:21.880 --> 51:28.920]  получается следующее, что при желании вот эти вот два кода, то есть, выполнение ядра два и выполнение
[51:28.920 --> 51:36.600]  метода на CPU, они являются у нас потенциально пересекающимися друг с другом. То есть, они
[51:36.600 --> 51:41.400]  выполняются параллельно. А теперь сделаем вот такую вещь. То есть, мы создадим объект вида
[51:41.400 --> 51:49.000]  kuda-stream-t. Создадим kuda-stream-create. Сделаем malloc-host. То есть, привяжем нашу память. Это важно
[51:49.000 --> 51:55.360]  после того, чтобы мы могли сделать memcpy. И в итоге получается следующее, что все эти операции
[51:55.360 --> 52:02.280]  потенциально могут выполняться параллельно. Каждый в своем стриме. То есть, операции внутри стрима
[52:02.280 --> 52:10.200]  выполняются последовательно. Между стримами нет никакой гарантии. Но как тогда определить,
[52:10.200 --> 52:18.600]  когда у нас данные, грубо говоря, два ядра будут выполняться последовательно друг за другом? Есть
[52:18.600 --> 52:30.280]  мысли? Ну, первое это барьер поставить, да. Не, но когда точно нам нужно сказать, что одно ядро точно
[52:30.280 --> 52:41.040]  будет выполняться после второго. Но если есть общие элементы массива или общие просто нотации,
[52:41.040 --> 52:46.280]  общие указатели на память. Да, мы в принципе на стадии статического анализа кода это можем выяснить.
[52:46.280 --> 52:57.880]  Так что у нас типа тут у нас массив x, здесь у нас массив y. Это было в момент времени t1,
[52:57.880 --> 53:04.600]  это было в момент времени t2. То есть, если у нас y зависит от x, да, какой-то степени. Желательно,
[53:04.600 --> 53:11.320]  почему он мог зависеть? Либо у нас и произошло копирование из y в x где-нибудь, либо у нас
[53:11.320 --> 53:17.400]  собственно в этом моменте времени t1 у нас тоже был в ядре параметр y. То есть, получается у нас
[53:17.400 --> 53:22.760]  момент времени t2 зависит от момента t1. И даже вне зависимости от того, что у нас здесь один стрим,
[53:22.800 --> 53:31.600]  а здесь второй стрим, они все равно будут выстраиваться последовательно. То есть,
[53:31.600 --> 53:38.200]  все равно здесь неявный барьер будет, неявное ожидание получения результатов будет здесь,
[53:38.200 --> 53:49.360]  здесь относительно этого. В принципе порядок не надо нарушать. Эта видеокарта будет учитывать.
[53:49.360 --> 53:58.160]  Так, хорошо. Единственное, смотрите, нужно привязать нашу память, сделать ее пинт.
[53:58.160 --> 54:04.280]  Ох, так дайте я остановлюсь здесь и спрошу, понятно ли то, что здесь произошло.
[54:04.280 --> 54:27.920]  Ну и в целом мы можем создавать большое количество стримов. Но не все так просто. На самом деле в
[54:27.920 --> 54:34.120]  видеокарте можно понять, возможна ли concurrent kernel execution, то есть можно ли выполнять их
[54:34.120 --> 54:39.480]  параллельно. И по умолчанию, если у вас есть concurrent kernel execution, то от силы вы можете
[54:39.480 --> 54:47.840]  выполнять два ядра параллельно. Больше вы никак не сможете. Ну как же нам все-таки решить нашу
[54:47.840 --> 54:54.240]  проблему? Для этого как раз нам нужно понять, что есть отдельная очередь копирования. То есть,
[54:54.240 --> 55:01.600]  у нас с вами, когда выполняется код на видеокарте, то у нас с вами есть очередь копирования с их
[55:01.600 --> 55:08.280]  host на device. Дальше у нас с вами есть очередь для вычислений и после этого у нас есть очередь
[55:08.280 --> 55:14.000]  копирования с device на host. Важный аспект заключается в том, что если вы какую-то операцию уже
[55:14.000 --> 55:20.360]  запланировали, после этого кажется, что вы выполняете какую-то дополнительную операцию,
[55:20.360 --> 55:26.360]  которая не должна зависеть от предыдущей, но вы ее поставили позже по времени, то она у вас
[55:26.360 --> 55:31.480]  поставится позже по времени. То есть, здесь временная шкала является актуальной. Сейчас мы
[55:31.480 --> 55:38.600]  это тоже посмотрим на примере. Смотрите, вот у нас есть две последние операции. Мы с вами делаем
[55:38.600 --> 55:48.160]  следующее. У нас порядок вызова наших операций. У нас есть host device для массива A1, дальше host
[55:48.160 --> 55:54.960]  device для массива B1, дальше мы выполняем kernel с первым элементом массива, после этого мы
[55:54.960 --> 56:01.080]  выполняем device to host с первым элементом массива и после этого выполняем device to host для второго
[56:01.080 --> 56:08.360]  элемента массива. И смотрите, что происходит. У нас получается следующее. У нас с вами DH2 никаким
[56:08.360 --> 56:14.600]  образом не зависит от предыдущих операций, но из-за того, что DH2 стоит после всех операций,
[56:14.600 --> 56:21.520]  он должен дождаться, пока выполнится DH1, потому что DH1 в очередь поставился раньше, чем DH2.
[56:21.520 --> 56:29.640]  А все операции после этого являются блокирующим друг друга. Перед тем, как выполнить ядро K1,
[56:29.640 --> 56:35.880]  нам нужно скопировать с host device массива A1 и B1. А потом, чтобы скопировать обратно,
[56:35.880 --> 56:40.840]  нам нужно дождаться времени. В итоге у нас получается неприятная картина. То, что те массивы,
[56:40.840 --> 56:51.000]  которые у нас с вами не зависят друг от друга, блокируют друг друга. Вопрос, каким образом можно
[56:51.120 --> 57:08.880]  переставить порядок действий, чтобы код пополнялся на 25% быстрее. В общем, на самом деле все просто.
[57:08.880 --> 57:16.440]  Нужно DH2 вынести в самое начало. Тогда у нас вот эта пирамидка, связанная с DH2,
[57:16.440 --> 57:27.200]  провалится наверх. Важно, что если мы поставим ее где-то раньше, ну, допустим, после DH1.
[57:27.200 --> 57:32.760]  Смотрите, тут важный момент, что если вы поставите ее сразу после выполнения HD1,
[57:32.760 --> 57:43.520]  то она окажется параллельна с ней. Если вы поставите после HDB1, то она окажется параллельна
[57:43.520 --> 57:52.520]  с HDB1. То есть время операции будет учитываться. То есть мы с вами строим граф порядка управления.
[57:52.520 --> 58:00.560]  Вот, то есть еще раз давайте я нарисую, что если у нас, собственно, вот такая вот штука.
[58:00.560 --> 58:13.520]  1, 2, 3, 4 и операция 5. Значит, если мы поставим, смотрите, 1, 5 штрих, то 5 штрих окажется здесь.
[58:13.520 --> 58:33.520]  Если мы при этом укажем с вами другой порядок. Мы укажем с вами 1, 2, 5, 2 штриха,
[58:33.520 --> 58:47.400]  то тогда операция окажется здесь. Ну, а если мы укажем 1, 2, 5, 3 штриха, то 5,
[58:47.400 --> 58:58.320]  3 штриха окажется здесь. Оно не зависит, но зависит от того, в какой момент времени мы поставили,
[58:58.320 --> 59:08.000]  то есть когда мы его задекларировали. Нет, нет, вообще никак не зависит. Они поэтому разными цветами
[59:08.000 --> 59:20.000]  подсвечены. Так, хорошо. Смотрите, мы с вами разобрались. Вот, да, то есть если мы HDB2 переместим,
[59:20.000 --> 59:28.360]  то мы перенесем в самое начало. Еще один кейс. Смотрите, значит, мы можем поставить наши массивы
[59:28.360 --> 59:36.800]  вот таким образом. То есть KA1, KB1, то есть у нас в последствии операции. Но при этом у нас с
[59:36.800 --> 59:43.280]  вами есть массивы, которые взаимодействуют с операциями, с массивом A и делаются несколько
[59:43.280 --> 59:49.840]  разных операций. То есть у нас есть операция A1, у нас есть операция B1. И как раз когда мы объявляем
[59:49.840 --> 59:55.800]  операцию A2 с нашим массивом, то есть у нас массив, допустим, используется чисто для чтения и больше
[59:55.800 --> 01:00:00.880]  с ним ничего не делается, то как раз порядок выполнения операции становится таким, что у нас
[01:00:00.880 --> 01:00:15.600]  A2 становится параллельно B1. С этим все замедляется на 50%. То есть мы ждем пока у нас B1 блокирует A1.
[01:00:15.600 --> 01:00:22.640]  То есть это как раз стрим-исполнение у нас первый, а здесь у нас стрим-исполнение второй.
[01:00:22.640 --> 01:00:35.240]  У нас A2 будет после D1. Если мы поменяем порядок, и оказывается, что в стриме A1 и A2 мы можем
[01:00:35.240 --> 01:00:41.640]  работать с массивом A в режиме чтения. В принципе, ядро это может быстро достаточно
[01:00:41.640 --> 01:00:49.720]  проанализировать. Это делается на уровне статического анализа. Вот, ну мы эти ядра можем поставить
[01:00:49.720 --> 01:00:56.760]  параллельно. И исполнение можем поставить параллельно, и все у нас будет замечательно. То есть время
[01:00:56.760 --> 01:01:04.320]  работы будет хорошо. То есть смотрите, цель следующая, что старайтесь параллельная операция располагать
[01:01:04.320 --> 01:01:10.920]  именно параллельно друг к другу, причем в коде исполнения. То есть если у вас есть операция A и операция B,
[01:01:10.920 --> 01:01:16.240]  которая ей параллельна, нужно сразу запустить и A и B, потому что если мы запустим C, которая идет
[01:01:16.240 --> 01:01:26.800]  после A, то, извините, B уже будет выполняться параллельно с C, а не параллельно с A. Так, эта
[01:01:26.800 --> 01:01:40.000]  идея понятна, да? И последняя вещь, значит как не стоит писать операцию. То есть, как говорится,
[01:01:40.000 --> 01:01:49.960]  концепция напиши чистый и хороший код, на видеокарте не работает. То есть представьте себе идею такую,
[01:01:49.960 --> 01:01:57.560]  что вы сначала хотите скопировать все с хаста на девайс, потом выполнить ядро, а потом все массивы
[01:01:57.560 --> 01:02:04.520]  скопировать с девайса на хост. Ну, логично. Сделайте так. Ну, смотрите, что происходит. Мы
[01:02:04.520 --> 01:02:13.600]  копируем с хаста на девайс. Даже все хорошо. Дальше выполняем ядро. Тоже кажется все замечательно.
[01:02:13.600 --> 01:02:20.960]  А потом оказывается интересный момент времени, что когда мы копируем все с девайса на хост,
[01:02:20.960 --> 01:02:26.640]  у нас с вами осуществляется блокирующая операция. То есть мы должны дождаться,
[01:02:26.640 --> 01:02:37.480]  пока у нас ядро закончит работу. И тут как раз написано, что сигналы между запусками ядр у нас
[01:02:37.480 --> 01:02:43.960]  отложены. То есть как бы хотелось бы, чтобы у нас операция, которая идет ДН1, она как бы выполнила
[01:02:43.960 --> 01:02:50.880]  сразу после КА1. Но из-за задержки, из-за задержки в очередях у нас получается,
[01:02:50.880 --> 01:02:58.320]  что образуется некоторая задержка по времени. И в итоге сигнал о том, что у нас в очередь стоит
[01:02:58.320 --> 01:03:05.240]  ДН1, будет ставиться только после того, как у нас идет копирование выполнения ядра КА3. То есть мы
[01:03:05.280 --> 01:03:14.080]  будем дожидаться результатом исполнения ядра КА3. Упс, неприятный момент. Как это можно ускорить?
[01:03:14.080 --> 01:03:23.320]  Помните порядок по программе. То есть желтые-желтые, зеленые к зеленым,
[01:03:23.320 --> 01:03:29.920]  синий к синим. Вот, если они друг от друга не зависят. То есть стараемся как раз писать все
[01:03:29.920 --> 01:03:39.880]  операции таким образом, чтобы они у нас с вами не выполнялись последним. Так, ага. Теперь выводы
[01:03:39.880 --> 01:03:46.480]  давайте произнесем для этой части. Значит у нас с вами есть три очереди. Это device to host, host device
[01:03:46.480 --> 01:03:53.560]  и очередь ядра. И concurrency есть и в GPU. То есть мы с вами поняли, что код можно ускорять и в режиме
[01:03:53.560 --> 01:03:59.400]  concurrency. Значит сразу скажу, какие темы мы с вами не прошли по коде. Мы с вами не прошли работу
[01:03:59.480 --> 01:04:07.840]  с тензорными ядрами. Пока новая история. Возможно, что с ней будет в какое-то время работа в будущем.
[01:04:07.840 --> 01:04:16.760]  И мы не прошли всякие мало битовые операции, которые сейчас полезны для всяких больших языковых моделей.
[01:04:16.760 --> 01:04:23.920]  Сейчас, грубо говоря, из-за того, что вот эти вот большие языковые модели а-ля чат ГПТ, они очень
[01:04:23.920 --> 01:04:29.280]  большие и хотелось бы их помещать в оперативную память. Желательно в оперативной памяти видеокарты,
[01:04:29.280 --> 01:04:35.800]  их пережимают. Естественно там возникает новый способ вычисления операций с массивами. То есть
[01:04:35.800 --> 01:04:43.560]  там даже дошло до того, что в современных версиях видеокарты можно использовать специальный тип данных.
[01:04:43.560 --> 01:04:51.920]  То есть есть float16, который позволяет вам двухбайтный float иметь, а есть bfloat16. Который
[01:04:51.920 --> 01:04:58.560]  позволяет там решить проблему перемножения нескольких флотовых чисел, не учитывая, грубо
[01:04:58.560 --> 01:05:06.240]  говоря, более аккуратно учитываем антису, которая появится в данных. Вот так. Это что касается
[01:05:06.240 --> 01:05:11.200]  куды. Значит следующие разы уже начнется блок по параллельным вычислением, по распределенным
[01:05:11.200 --> 01:05:16.640]  вычислением. То есть я по факту завершил блок по параллельным вычислением. Вот. И небольшая
[01:05:16.640 --> 01:05:24.680]  прямула к тому, что будет. Значит мы будем с вами в курсе проходить историю, связанную с тем,
[01:05:24.680 --> 01:05:32.320]  а что будет, если вы внезапно выходите в реальный мир из учебной лаборатории, так сказать. Мы пока
[01:05:32.320 --> 01:05:37.680]  находились с вами в какой-то учебной лаборатории, и все условия у нас были идеальными. Мы были
[01:05:37.680 --> 01:05:42.920]  заточены именно под ускорение программ. А вот следующие разы как раз мы уже пойдем в реальный
[01:05:42.920 --> 01:05:47.320]  мир и поймем, а что же будет происходить, если внезапно у вас один из компьютеров вылетит из
[01:05:47.320 --> 01:05:58.360]  строя. Или один из компьютеров перестанет принимать сообщения по сети. Просто он прологал. То есть
[01:05:58.360 --> 01:06:05.080]  всякие разные кейсы бывают, и как раз вот эти кейсы уже будут рассматриваться. Так что следующая
[01:06:05.080 --> 01:06:11.920]  лекция у нас будет про то, как устроены блокчные файловые системы распределенные. А после того
[01:06:11.920 --> 01:06:21.600]  будет обработка поверх блокчных распределенных систем. Ладно, всем спасибо тогда и до встречи
[01:06:21.600 --> 01:06:24.160]  на следующих лекциях, семинарах и так далее.

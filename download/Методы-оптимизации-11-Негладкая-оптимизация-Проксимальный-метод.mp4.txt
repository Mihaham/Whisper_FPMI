[00:00.000 --> 00:09.000]  Да, давайте тогда с организационного момента и начнем, я хотел в перерыве это сказать,
[00:09.000 --> 00:15.240]  но раз вопрос возник сейчас, то давайте разберемся. На следующей неделе все семинары будут проходить в
[00:15.240 --> 00:24.080]  формате онлайн. Это будут семинары по выбору, 7 разных спикеров на разные темы вокруг оптимизации,
[00:24.080 --> 00:31.360]  в том числе какие-то прикладные вещи, то есть часть семинаров прочтут ваши классические семинаристы,
[00:31.360 --> 00:37.280]  но по темам, которые выходят за пределы курса, а часть ребят это приглашенные ребята, в том числе,
[00:37.280 --> 00:42.080]  например, команда Яндекс.Ресеча, которая занимается распределенной оптимизацией,
[00:42.080 --> 00:46.760]  будут одних семинаров. То есть в течение недели, там вечером в понедельник, вечером во вторник,
[00:46.760 --> 00:53.120]  среду, четверг, пятницу, иногда по два семинара в день, можно посетить один семинар, который наиболее
[00:53.120 --> 01:00.880]  интересен. И вот на этом семинаре, в конце либо в начале, будет проведен последний двенадцатый тест в
[01:00.880 --> 01:05.720]  формате онлайн. Это будет какой-то такой вот мини-контест. Вы как раз сейчас на этой неделе
[01:05.720 --> 01:13.000]  проходите библиотеку CVXPy, ну и вот, соответственно, по этой библиотеке у вас и будет последняя вот эта
[01:13.000 --> 01:19.280]  работа на семинаре. И она будет в формате онлайн такого небольшого, совсем быстренького контеста.
[01:19.280 --> 01:29.360]  Решите одну небольшую задачку. Вот, как-то так, как-то так. Сегодня форму для записи на эти семинары
[01:29.360 --> 01:35.200]  выкачу. Нужно будет выбрать один семинар как обязательный, ну и если хотите посетить больше,
[01:35.200 --> 01:47.560]  выбираете еще один тем семинар, может записаться. Тесты пишутся один раз, в итоге считается по
[01:47.560 --> 01:52.440]  худшему результату. То есть, если хотите написать два, будем считать по худшему. Поэтому пишите один раз,
[01:52.440 --> 01:57.520]  ну там, на первом семинаре, который посетили или там, если хотите посетить три семинара, хоть на
[01:57.520 --> 02:04.440]  последнем, пишите. Главное один раз. Задачи будут отличаться от семинара к семинару. Вот, как-то так.
[02:04.440 --> 02:11.200]  Это, соответственно, по тому, как будут устроены семинары на следующей неделе. А через неделю они
[02:11.200 --> 02:18.800]  тоже будут в формате онлайн, только уже в качестве небольшой такой конференции от тех, кто разбирает
[02:18.800 --> 02:26.080]  статьи, от тех, кто разбирает статьи. Что еще? Что еще? Ну, как-то так. А лекции у вас до конца, вот еще у нас
[02:26.080 --> 02:32.200]  осталось, кроме этой три лекции, в таком формате они продолжаются. Ну и получается, на неделе до зачетной
[02:32.200 --> 02:38.600]  у вас начинаются уже колоквиумы. Большие группы разбиваются на две части и семинарист, плюс какие-то
[02:38.600 --> 02:45.280]  приглашенные гости у вас принимают колоквиум. На зачетной неделе и за неделю до этого. Как разбить
[02:45.280 --> 02:51.280]  группу на две части, ну это вы сами внутри группы решаете. Кто пойдет раньше, кто пойдет позже. Программу
[02:51.280 --> 02:57.680]  колоквиума я до воскресенья постараюсь выкатить. Окей, это по организационным вопросам. Ну тогда
[02:57.680 --> 03:05.800]  поехали разбираться с сегодняшней темой. На семинарах вы уже в некотором смысле освоили аппарат,
[03:05.800 --> 03:12.240]  который нам сегодня будет нужен. Но я вас сразу же спрошу. Является ли функция модуль x выпуклой?
[03:12.240 --> 03:21.760]  Да, безусловно, она является выпуклой. Но возникает вопрос, является ли она дифференцируемой и гладкой.
[03:21.760 --> 03:27.640]  Такими функциями мы работали до этого все лекции. Является ли она дифференцируемой и гладкой?
[03:27.640 --> 03:38.200]  Пусочно. Она является дифференцируемой везде, кроме точки ноль. Там возникают проблемы. Но
[03:38.200 --> 03:46.000]  выполнена ли гладкость даже вне точки ноль? Давайте, например, возьмем что-то такое. Ну и если у нас
[03:46.000 --> 03:52.960]  x и y брать из окрестностей 0, 1 справа, 1 слева, то это разница всегда будет равняться двоечке.
[03:52.960 --> 03:57.920]  Это разница всегда будет равняться двоечке. И тут нету ничего такого замечательного в
[03:57.920 --> 04:04.640]  духе того, что тут ограничена какая-то константа l умножить на норму. К сожалению,
[04:04.640 --> 04:10.400]  вот такие проблемы возникают в связи с тем, что даже самые простые выпуклые функции не всегда
[04:10.400 --> 04:18.720]  являются дифференцируемыми и не всегда при этом являются еще и гладкими. В связи с этим мы будем
[04:18.720 --> 04:24.960]  рассматривать на этом семинаре вот такое ограничение на функции. То есть выпуклость мы оставляем,
[04:24.960 --> 04:31.040]  но убираем гладкость, добавляем липшицевость в самой функции, с которой мы в принципе уже сталкивались
[04:31.040 --> 04:38.000]  на первом семинаре, когда разбирали сложные задачи оптимизации не выпуклой для липшицевых
[04:38.000 --> 04:42.600]  функций. То есть тут суть такая, что у нас теперь просто значение функции будет вот так вот ограничено,
[04:42.600 --> 04:50.160]  некоторые константы m, на приращение аргумента. На самом деле вот это все,
[04:50.160 --> 04:55.920]  это определение, как и в случае гладкости, можно обобщить на ситуацию, когда у вас функция задана
[04:55.920 --> 05:02.840]  не на всем пространстве rd, когда вы работаете на каком-то там ограниченном множестве x, хотя бы
[05:02.840 --> 05:08.960]  потому, что если мы говорим вообще про сильно выпуклые задачи, при этом которые целевые функции,
[05:08.960 --> 05:17.960]  которых являются еще и m липшицевыми, вообще таких функций не существует на rd. Почему? Кто понимает,
[05:17.960 --> 05:28.200]  почему не существует на rd таких функций? Да, все правильно. То есть липшицевость предполагает,
[05:28.200 --> 05:33.760]  то что у вас функция сверху ограничена линейной, то есть растет не быстрее, чем линейная на
[05:33.760 --> 05:42.200]  бесконечности. Если у вас функция сильно выпуклась, вам утверждают о том, что у вас
[05:42.200 --> 05:48.440]  функция растет не медленнее, чем квадратичная на бесконечности, ну и с этим у вас, понятно,
[05:48.440 --> 05:54.600]  возникает противоречие. И поэтому, понятно, чаще всего, конечно, такие задачи рассматриваются на
[05:54.600 --> 05:59.200]  каких-то ограниченных множествах, на каких-то выпуклых компактах, но сегодня, чтобы не
[05:59.200 --> 06:06.600]  загромождать анализ всеми этими проекциями и так далее, будем рассматривать пока rd. Понятно,
[06:06.600 --> 06:14.560]  это все более чем легко обобщается тем аппаратом, с которым мы с вами уже знакомы. Так, это мы с вами
[06:14.560 --> 06:26.680]  обсудили. Ну хорошо, если у нас функция не дифференцируема, градиента нету, что может
[06:26.680 --> 06:33.000]  существовать место градиента? Субградиент, с которым вы, соответственно, познакомились на
[06:33.000 --> 06:41.080]  предыдущем семинаре. Вот его определение. Отмечу, напомню скорее о том, что субградиент вообще
[06:41.080 --> 06:46.280]  определяется только для выпуклых функций и как раз исходя из определения выпуклых функций,
[06:46.280 --> 06:52.600]  потому что вот здесь вот у нас обычно стоял градиентик, градиентик в точке x, вот, вместо
[06:53.040 --> 06:58.920]  g. Ну а теперь мы, соответственно, мы ставим туда на вектор, который будет продолжать для
[06:58.920 --> 07:05.200]  выпуклых функций удовлетворять вот этому определению выпуклости. Теперь, соответственно,
[07:05.200 --> 07:10.040]  функции не дифференцируемы, градиента нет, мы рассматриваем все возможные такие же. Ну
[07:10.040 --> 07:14.320]  и все возможные g в этой точке X, которые удовлетворяют этому соотношению выпуклости,
[07:14.320 --> 07:18.760]  называется субдифференциалом, то есть, множество субградиентов в этой точке называется
[07:18.760 --> 07:24.760]  с субдифференциалом. С этим вы познакомились, как искать субдифференциалы, их свойства вы уже знаете.
[07:24.760 --> 07:33.760]  Вы уже знаете. Хорошо, ну давайте тогда обсудим, как конструировать нам методы, но перед тем, как конструировать
[07:33.760 --> 07:41.760]  методы, давайте разберемся с условиям оптимальности. Оптимальности выпуклых функций, которые, соответственно,
[07:41.760 --> 07:47.760]  у нас уже являются в общем случае недиференцируемой и негладкой. Условие оптимальности выглядит
[07:47.760 --> 07:53.760]  следующим образом. х звездой это у нас минимум выпуклой функции f. Здесь, понятно, слово глобально пропущено,
[07:53.760 --> 07:59.760]  потому что вроде как договорились пропускать это. Тогда и только тогда, когда у нас 0 принадлежит
[07:59.760 --> 08:05.760]  субдифференциалу точки х звездой функции, субдифференциала функции f точки х звездой.
[08:05.760 --> 08:13.760]  Окей, давайте посмотрим, что тут можно сделать. Ну давайте в сторону, сначала в ту докажем, вправо,
[08:13.760 --> 08:20.760]  то есть у меня 0 принадлежит субдифференциалу. Что я тогда могу сделать? Опять же, по определению
[08:20.760 --> 08:32.760]  выпуклости я пишу следующее. Так g х-х со звездой. Согласны здесь? Это просто из определения
[08:32.760 --> 08:39.760]  выпуклости g принадлежит субдифференциалу d f х с звездой. Просто из предыдущей вот этой строчечки.
[08:39.760 --> 08:49.760]  Так? Вот. Но так как это в g у меня лежит, в том числе, в качестве g я могу брать 0,
[08:49.760 --> 08:56.760]  в качестве g я могу брать 0, поэтому здесь я в качестве g этот 0 и кладу, и у меня здесь получается
[08:56.760 --> 09:02.760]  просто f от х со звездой. Ну и что получается? Получается ровно то, что нам нужно определение
[09:02.760 --> 09:06.760]  глобального минимума функции. Глобальную минимум функции для любой точки х, которую бы я не
[09:06.760 --> 09:10.760]  рассмотрел, у меня значение в точке х будет больше либо равно, чем значение в точке х со звездой.
[09:10.760 --> 09:18.760]  Это мы вам в ту сторону доказали. Теперь давайте докажем в обратную сторону. Пусть у меня x это
[09:18.760 --> 09:25.760]  глобальный минимум. X со звездой это глобальный минимум. Когда у меня выполнено вот такое
[09:25.760 --> 09:34.760]  соотношение для любого х? Для любого х. Хорошо. Хорошо. Тогда как я могу воспользоваться выпуклостью?
[09:34.760 --> 09:40.760]  Как я могу воспользоваться выпуклостью? Ну давайте посмотрим. Если у меня выполнено это соотношение,
[09:40.760 --> 09:52.760]  то тогда и выполнено и вот это соотношение. 0 х минус х со звездой. Согласны? Вот. Просто 0 к умной
[09:52.760 --> 09:58.760]  добавляю. Ну а значит отсюда у меня ровно следует, что у меня 0 лежит в суб дифференциале ровно из
[09:58.760 --> 10:02.760]  определения суб дифференциала, потому что вот это выполнено для любого х, как мы только что
[10:02.760 --> 10:08.760]  сказали. Все. Получается, что условие оптимальности очень простое. 0 должен лежать в суб дифференциале
[10:08.760 --> 10:19.760]  нашей функции. Вот. Хорошо. Хорошо. Здесь это все доказано. Так. Теперь хочется доказать следующее
[10:19.760 --> 10:25.760]  свойство. То есть липчество. То есть это конечно хорошо. Вроде бы понятны свойства. Не быстрее, чем
[10:25.760 --> 10:31.760]  линейно растет. Но можно доказать эквивалентную вещь. Оказывается, что у нас функция является
[10:31.760 --> 10:39.760]  выпуклой тогда и только тогда, когда все суб дифференциалы, все суб градиенты этой функции ограничены.
[10:39.760 --> 10:47.760]  Ограничены. В принципе, с чем-то похожим мы сталкивались и когда говорили про гладкость, у нас
[10:47.760 --> 10:53.760]  тогда был как липшицевость градиента и ограничивалась уже вторая производная. То есть
[10:53.760 --> 11:00.760]  гисян. Здесь получается похожая ситуация в силу того, что липчество сама функция, то ограничен ее
[11:00.760 --> 11:08.760]  градиент. В данном случае субградиент. Ограничен субградиент. Ну хорошо. Давайте попробуем это доказать.
[11:08.760 --> 11:15.760]  Доказываем сначала. Получается со стороны у нас есть выпуклость и липчецевость. Хотим тогда показать,
[11:15.760 --> 11:23.760]  что у нас будет ограничен субградиент. Будет ограничен субградиент. Окей. Давайте попробуем
[11:23.760 --> 11:32.760]  выписать как-то нашу выпуклость, так как у нас только кроме выпуклости в определении суб дифференциала
[11:32.760 --> 11:47.760]  ничего и нету. Ничего и нету. g y минус x. Так, ну давайте я чуть по-другому это. Так? Чуть-чуть. Чего-чего?
[11:47.760 --> 11:58.760]  x минус y. Ну давайте вернемся на всякий случай. Так, как я поставил там в определении? А, f от y?
[11:58.760 --> 12:08.760]  Да, все правильно, все правильно. Хорошо. Это про это молодцом. Так, вот так вот. Хорошо, хорошо. Ну и что я тогда
[12:08.760 --> 12:18.760]  знаю про это выражение g? У меня суб дифференциал в какой точке? Субградиент в какой точке? x или y?
[12:18.760 --> 12:29.760]  Хорошо. Хорошо. Ну тогда это мне должно выполняться для любого x. Согласны? Для любого x, в том числе для
[12:29.760 --> 12:43.760]  x, который равен x, равен y плюс g. y плюс g. Окей. Смотрим, что получается. Меньше либо равно, чем f от y.
[12:43.760 --> 12:51.760]  А здесь, когда я подставлю, соответственно, y минус g, y плюс g вместо x, что у меня получится?
[12:51.760 --> 12:58.760]  Скалярное произведение двух g. А это просто, ой, это просто у меня будет норма g в квадрате.
[12:58.760 --> 13:06.760]  Евклидова норма g в квадрате. А теперь я могу что сказать? Что у меня g в квадрате меньше либо равен, чем f от x
[13:06.760 --> 13:18.760]  минус f от y. Пользуюсь чем? Липшицевостью функции и получаю, что у меня m x минус y евклидова норма
[13:18.760 --> 13:26.760]  евклидова норма. Ну а дальше я подставляю снова x, который мне равен x плюс g. И здесь получается m g
[13:26.760 --> 13:34.760]  евклидова норма. Ну и в итоге получается, что евклидова норма моего субградиента ограничена
[13:34.760 --> 13:40.760]  в силу того, что мы брали произвольный субградиент в произвольной точке y, это будет выполняться всегда.
[13:40.760 --> 13:46.760]  Хорошо, все, тут доказали, в эту сторону понятно. Так, так, так.
[13:46.760 --> 13:57.760]  Окей, давайте будем действовать в обратную сторону. Снова записываю определение того, что у меня g это
[13:57.760 --> 14:05.760]  субградиент, что у меня тогда будет g здесь, это из субдифференциала.
[14:05.760 --> 14:13.760]  Так, и здесь я хочу со знаком минус, то есть вот тут я брал со знаком плюс, тут я брал со знаком плюс, вот здесь вот
[14:13.760 --> 14:21.760]  это выражение брал со знаком плюс, я теперь хочу его взять со знаком минус, поэтому мне нужно поменять местами y и x.
[14:22.760 --> 14:31.760]  Минус y, минус x. Так, x минус y, сейчас.
[14:32.760 --> 14:41.760]  x минус y должно что ли быть? Нет, только что у нас было x. Было вот так, x минус y.
[14:41.760 --> 14:51.760]  Нет, f-то я оставил здесь такую, y. Я вот здесь только знак поменял. Так, все норм?
[14:57.760 --> 15:03.760]  А, и да, я поменял еще тут местами, да. Окей, вот так.
[15:03.760 --> 15:11.760]  Господи, что-то у меня сегодня выпукло, что бы уже проблемы. Так, так норм? Вот, так норм.
[15:11.760 --> 15:22.760]  Хорошо, так норм. Что у нас известно? У нас известно, что наш субградиент ограничен,
[15:22.760 --> 15:27.760]  субградиент ограничен, ну тогда что у нас получается? Я потаскаю это теперь
[15:27.760 --> 15:37.760]  чуть-чуть в разные стороны. Сколько у меня тут y минус x? Переношу вправо, у меня здесь получается f от y
[15:37.760 --> 15:45.760]  минус f от x. Ну а дальше что? Дальше я делаю Кашибуниковского здесь, у меня получается g
[15:45.760 --> 15:53.760]  Евклидова норма, y минус x Евклидова норма. Вот, Евклидова норма. Ну и получается, что у меня
[15:53.760 --> 16:03.760]  f от y минус f от x меньше либо равно, чем g, так, y минус x. Понятно, что я могу поменять x и y
[16:03.760 --> 16:13.760]  местами и получить, что у меня f от x минус f от y меньше либо равно, чем g, y минус x. Вот.
[16:13.760 --> 16:23.760]  Тогда я ставлю модуль и получаю то, что от меня и требовалось. Так. Пам-пам-пам. Так. Окей.
[16:23.760 --> 16:29.760]  Ну теперь давайте поговорим про метод. Поговорим про метод. В принципе, то есть мы рассматриваем какую-то
[16:29.760 --> 16:36.760]  негладкую задачу. Функция у нас является выпуклой, m липшицовой. Понятно, в общем случае там нет никакой
[16:36.760 --> 16:44.760]  гладкости, поэтому градиента нету. Ну и довольно простое обобщение, простая модификация, как
[16:44.760 --> 16:50.760]  выкрутиться из этого случая. Давайте тогда вместо градиента буду брать субградиент. Какой-то
[16:50.760 --> 16:58.760]  субградиент из моего суб дифференциала в текущей точке x. Как вы знаете, у вас если функция все хорошо,
[16:58.760 --> 17:04.760]  непрерывно дифференцируемая, там будет просто градиент стоять, он единственный. Более того, не знаю,
[17:04.760 --> 17:10.760]  обсуждали ли на всех семинарах это или нет, суб дифференциал у вас для выпуклой замкнутой функции
[17:10.760 --> 17:18.760]  существует всегда на внутренности. На внутренности множество определений этой функции. То есть в принципе,
[17:18.760 --> 17:24.760]  если могут возникнуть проблемы с существованием суб дифференциала, что он будет пуст, то только на
[17:24.760 --> 17:29.760]  каких-то границах множества, на котором определена функция. То есть в принципе суб дифференциал
[17:29.760 --> 17:35.760]  существует, и мы оттуда можем какие-то вытягивать векторы. Ну и соответственно в качестве теперь
[17:35.760 --> 17:50.760]  градиента мы берем субградиент. Окей? Да, да, да, да. Ну это ровно так же, как и в случае вычисления
[17:50.760 --> 18:01.760]  градиента. Автодифференцирование, это правда, да. Так, окей, давайте разбираться. Давайте разбираться
[18:01.760 --> 18:07.760]  здесь, как доказывать. В принципе, я делаю то же самое, что мы делаем с вами обычно. Расписываю
[18:07.760 --> 18:13.760]  расстояние до решения. Расписываю расстояние до решения, и дальше расписываю то, что у меня вот
[18:13.760 --> 18:20.760]  здесь вот происходит. Что у меня здесь происходит? Здесь у меня вынес текущее расстояние,
[18:20.760 --> 18:26.760]  скалярное произведение и норма в квадрате. Дальше я пользуюсь чем? Я знаю то, что у меня
[18:26.760 --> 18:32.760]  субградиенты ограничены, поэтому я могу ограничить это безобразие, которое у меня здесь написано
[18:32.760 --> 18:40.760]  гаммой на m в квадрате. Гамма на m в квадрате. Плюс про вот это что-то я знаю тоже. Что я про это знаю?
[18:40.760 --> 18:46.760]  Про скалярное произведение субградиента на x минус x звездой. Знаю, что у меня вот этот вектор
[18:46.760 --> 18:54.760]  лежит в суб дифференциале. Точки xk в нашей функции. Что я могу сказать про скалярное произведение?
[18:54.760 --> 18:58.760]  Как я его могу оценить?
[18:58.760 --> 19:08.760]  Я могу его оценить через выпуклость. Я могу его оценить через выпуклость f от x звездой
[19:08.760 --> 19:16.760]  минус f от xk. Просто опять же через определение субградиента функции в точке xk.
[19:16.760 --> 19:24.760]  Получаем следующую оценку. xk плюс 1 минус x звездой. Это вторая вещь, которую мы оценили
[19:24.760 --> 19:29.760]  по выпуклости.
[19:29.760 --> 19:38.760]  xk минус x звездой в квадрате в квадрате плюс 2 гамма f от x звездой
[19:38.760 --> 19:46.760]  минус f от xk плюс гамма в квадрате m в квадрате.
[19:46.760 --> 19:56.760]  Теперь делаю небольшую перестановку. Вправо переношу f. Нормы x переношу наоборот влево.
[19:56.760 --> 20:05.760]  Ровно то же самое, что мы делали в выпуклом случае. Получаю вот такое вот.
[20:05.760 --> 20:19.760]  Что дальше делать? Просуммировать по всем k и усреднить на всей итерации.
[20:19.760 --> 20:28.760]  Так, окей, давайте это сделаем.
[20:28.760 --> 20:37.760]  Просуммировали. Здесь у меня останется нулевое минус последнее.
[20:37.760 --> 20:44.760]  Что довольно приятно делить на k. Ну а когда я суммирую до t г в квадрате m в квадрате,
[20:44.760 --> 20:50.760]  потом усредняю, у меня оно же и остается. Просто потому что я k раз просуммировал и потом разделил на k.
[20:50.760 --> 20:56.760]  Хорошо, здесь что еще можно убрать? Можно убрать вот этот кусочек, потому что я пишу оценку сверху,
[20:56.760 --> 21:02.760]  а он не положительный, потому что берет со знака минус. Ну и в принципе, а, ну еще гамму то забыл,
[21:02.760 --> 21:09.760]  гамма в квадрате m в квадрате. Вот, и что у меня в итоге получается? Что у меня в итоге получается?
[21:10.760 --> 21:19.760]  Так, давайте вот здесь у меня все уже просуммировано. Вот, получилось следующее выражение, я его,
[21:19.760 --> 21:27.760]  здесь давайте его и оставим. Вот такое вот выражение. Как прийти к какому-то критерию сходимости?
[21:27.760 --> 21:35.760]  Потому что в левой части у нас стоит что-то странное. Вот, как мы приходили к критерию сходимости в левой части?
[21:36.760 --> 21:42.760]  Да, ну на самом деле в выпуклом случае, когда мы гладки рассматривали случай, там можно вообще показать то,
[21:42.760 --> 21:49.760]  что у вас f от xk меньше чем f от xk минус 1. В гладком выпуклом случае справедливо вот такая вот вещь.
[21:49.760 --> 21:55.760]  Поэтому здесь можно было вообще обойтись без Янсона и сказать то, что у вас значение функции монотонного
[21:55.760 --> 22:02.760]  бывает, поэтому можно брать сходимость по последней точке. Но это в гладком случае. Здесь такое вы себе
[22:02.760 --> 22:08.760]  позволить не можете. Хотя бы потому что, ну давайте представим опять же нашу функцию модуль.
[22:08.760 --> 22:15.760]  Нашу функцию модуль. Ну и в случае, когда у вас там все дифференцируемо бы было, вы бы оказались в нуле,
[22:15.760 --> 22:20.760]  производная в минимуме равна нулю, и вы из нее не уйдете. Последняя точка, она и остается последней.
[22:20.760 --> 22:26.760]  Сейчас мы говорим то, что вот оказавшись даже в нуле, мы должны взять что? Какой-то вектор из субдифференциала,
[22:26.760 --> 22:32.760]  который у вас по факту может оказаться ни разу не нулевым. Вы можете взять какой-то случайный вектор.
[22:32.760 --> 22:40.760]  Кстати, с какого значения до какого у модуля? От минус одного до одного и соответственно чуть-чуть
[22:40.760 --> 22:46.760]  выйти за пределу, ну то есть из этого нуля выйти. Поэтому никакой сходимости по последней точке у вас здесь
[22:46.760 --> 22:51.760]  не будет. Это такая естественная вещь, поэтому ее здесь искать и не нужно. Просто потому что
[22:52.760 --> 22:57.760]  субдифференциал это такая неприятная вещь, которая у вас даже находясь в оптимальной точке, может из него
[22:57.760 --> 23:03.760]  выпнуть. Поэтому будет сходимость только по средней точке, и как вы правильно сказали, мы вот это
[23:03.760 --> 23:09.760]  с вами оцениваем по Янсону, по Янсону, потому что f у вас выпуклая функция, ну и тогда мы это можем
[23:09.760 --> 23:24.760]  оценить как f по средней точке. Вот так вот. Согласны здесь? Вот, получили вот такую вот сходимость,
[23:24.760 --> 23:32.760]  еще называется органическая сходимость по средней точке. Вот, дальше возникает вопрос, как подбирать
[23:32.760 --> 23:37.760]  в правой части шаг, потому что на меня есть и в знаменателе, и в числителе. Возьму очень маленький
[23:37.760 --> 23:43.760]  шаг, сходимость пропадет, возьму очень большой шаг, она тоже снова пропадет, потому что в одном случае
[23:43.760 --> 23:50.760]  будет расти вот этот кусочек, левый, когда я буду уменьшать гамма, а когда буду увеличивать гамма,
[23:50.760 --> 23:57.760]  будет расти правый кусочек. Нужно найти что-то, в некотором смысле, какой-то баланс между вот этими
[23:57.760 --> 24:08.760]  двумя вещами. Как его найти? Да, здесь соответственно это можно все безобразие оптимизировать.
[24:08.760 --> 24:14.760]  Как это оптимизировать? Чему будет равен оптимальный шаг? Ну, давайте возьмем эту производную,
[24:14.760 --> 24:26.760]  ну что здесь будет? x0-x звездой в квадрате в квадрате, минус 2 гамма в квадрате k, плюс m в квадрате 2,
[24:26.760 --> 24:37.760]  равно нулю. Ну, ищем отсюда гамму, ищем отсюда гамму, гамма получается x0-x звездой делить на m
[24:37.760 --> 24:45.760]  к корень. Согласно, просто выразил отсюда гамму, просто выразил отсюда гамму, ее можно соответственно
[24:45.760 --> 24:54.760]  подставить, можно подставить выражение, здесь мы подбираем шаг, ну и подставляем выражение, получаем вот такую вот сходимость.
[24:54.760 --> 24:59.760]  Понятно, что такой подбор шага не самый практичный, потому что вам нужно знать расстояние до решения,
[24:59.760 --> 25:04.760]  что не самая очевидная вещь, если вы знаете расстояние до решения, то скорее всего вы знаете,
[25:04.760 --> 25:11.760]  где оно расположено, знать константу липшица функции, но горизонт итерации это уже не так страшно,
[25:11.760 --> 25:18.760]  поэтому в реальности, когда вы решаете негладкие задачки, можно применять просто шаг 1 делить на корень из k,
[25:18.760 --> 25:27.760]  1 делить на корень из k и соответственно получать точно такие же оценки сходимости, точно такие же оценки сходимости.
[25:28.760 --> 25:37.760]  Корень из количества итераций. Нет, вот здесь, вот здесь можно брать как раз по номеру итерации.
[25:37.760 --> 25:43.760]  Вот, более практичная вещь, потому что в принципе на практике вы не знаете ни расстояния до решения,
[25:43.760 --> 25:48.760]  ни m, ну и горизонт итерации вы тоже в некотором смысле не задаете, вы же просто говорите, я запускаю метод,
[25:48.760 --> 25:53.760]  хочу, чтобы он сошелся до какой-то точности, поэтому просто можно брать уменьшающийся шаг.
[25:53.760 --> 26:00.760]  В принципе, вы это и пробовали, даже для гладких задач эта вещь работает, ну и для негладких тоже.
[26:00.760 --> 26:09.760]  Для негладких тоже. Так, вот здесь соответственно дана теорияма сходимости, вот такая вот оценка.
[26:09.760 --> 26:15.760]  Получается, для выпуклой задачи 1 делить на корень из k, но если в терминах епсилон, то будет епсилон в квадрате.
[26:15.760 --> 26:21.760]  Понятно, что вот вы кладете равным епсилон и отсюда выражаете k, вылезет епсилон в квадрате.
[26:22.760 --> 26:26.760]  Кто помнит, какая оценка справедлива для гладкого случая?
[26:30.760 --> 26:32.760]  Да, для градиентного спуска.
[26:38.760 --> 26:43.760]  Это сильно выпуклый, да? Сильно выпуклый. Сильно выпуклый, там у нас вообще линейная сходимость.
[26:43.760 --> 26:53.760]  Линейная сходимость в духе 1, ну давайте так вот опишу, q в степени k.
[26:53.760 --> 26:56.760]  А если выпуклый случай, какая будет сходимость в гладком?
[27:02.760 --> 27:09.760]  О, 1 делить на епсилон или 1 делить на k? 1 делить на k, если мы говорим в терминах k.
[27:09.760 --> 27:20.760]  Тогда что у нас получается? Получается, что оказывается, вот негладкий случай оказался в этом плане сложнее, чем гладкий,
[27:20.760 --> 27:25.760]  потому что здесь у нас в выпуклом случае сходимость 1 делить на корень из k, что медленнее, чем 1 делить на k.
[27:25.760 --> 27:38.760]  В выпуклом случае у вас будет оценка для негладкой задачи порядка 1 делить на k, что понятно хуже, чем линейная сходимость в гладком случае.
[27:39.760 --> 27:48.760]  Получается, что негладкий случай в данном случае просто сложнее, просто сложнее вычислительно.
[27:48.760 --> 27:57.760]  Ну и давайте быстренько просуммируем итог. Просуммируем итог. У нас получилось обобщение градиентного спуска на негладкие задачи.
[27:57.760 --> 28:02.760]  С помощью применения субградиента вместо честного градиента.
[28:02.760 --> 28:12.760]  То, что мы с вами сейчас обсудили, у нас сходимость стала хуже по сравнению с гладким случаем.
[28:12.760 --> 28:20.760]  Еще давайте такой вопрос. Как думаете, можно ли улучшить результат в этом случае? Будьте здоровы.
[28:20.760 --> 28:30.760]  Оказывается, нет. То есть, если в случае гладкой задачи у нас можно было ускорить градиентный спуск с помощью моментных членов,
[28:30.760 --> 28:40.760]  в данном случае для негладких задач нижние оценки говорят о том, что градиентный спуск, субградиентный спуск оказывается оптимальным методом.
[28:40.760 --> 28:47.760]  И лучше, чем он ничего и не придумать. Ну вот такая вот ситуация, соответственно.
[28:47.760 --> 28:54.760]  Ну, с ней приходится жить. Получается, что вот здесь оптимальный метод довольно простой.
[28:54.760 --> 29:06.760]  А в невыпаклом случае, что вы мне скажете? Там-то у нас вообще было возможно было ускорение в гладком случае или нет? Кто помнит?
[29:06.760 --> 29:13.760]  На самом деле, да, это, кстати, интересное замечание.
[29:13.760 --> 29:25.760]  Интересное замечание, потому что в реальности, то есть, использование субградиентного метода вам не ускорит в оптимальном случае решение негладкой задачи.
[29:25.760 --> 29:35.760]  Ну, опять же, если вы учитываете какую-то дополнительную специфику задачи, например, то, что на малой размерности можно решать эту задачу быстрее, быстрее методами нулевого порядка.
[29:35.760 --> 29:43.760]  Методами нулевого порядка разного рода. То есть некоторые мы посмотрим через лекцию. Не на следующий, а через одну.
[29:43.760 --> 29:53.760]  А есть еще всякие методы эллипсоидов, методы плоскостей. И они тоже решают маломерные задачи, в том числе негладкие.
[29:53.760 --> 30:01.760]  И в некотором смысле успешнее чем субградиентный спуск.
[30:01.760 --> 30:05.760]  Ну, в общем случае, оценка субградиентного спуска не улучшаема.
[30:05.760 --> 30:13.760]  А в невыпуклом случае, как вы помните, в некотором смысле мы закольцовываем историю. В гладком вы можете гарантировать сходимость какой-то стационарной точки.
[30:13.760 --> 30:21.760]  А в негладком случае, ну, мы с вами разбирали. На самой первой лекции лучше чем полный перебор, придумать ничего нельзя.
[30:21.760 --> 30:27.760]  То есть невыпуклый, негладкий случай это довольно такой неприятный случай, неприятный кейс.
[30:27.760 --> 30:33.760]  Даже когда функция у вас является липшицовой.
[30:33.760 --> 30:35.760]  Окей, окей.
[30:35.760 --> 30:51.760]  Так, ну и то, что я говорил в принципе в начале, то, что для выпуклых и сильно выпуклых задач, ну, особо рассматривать негладкие задачи на неограниченных множествах не имеет смысла.
[30:51.760 --> 31:01.760]  Чаще всего эти задачи возникают в виде какого-то, ну, на каком-то ограниченном множестве, возможно хорошем, с которыми с вами уже взаимодействовали.
[31:01.760 --> 31:13.760]  Поэтому здесь можно применять и методы проекции, методы проекции, либо вот соответственно зеркальный спуск, где вы вместо евклидового расстояния используете дивергенцию Брегмана.
[31:13.760 --> 31:24.760]  К сожалению, специфика Франко Вульфа, если мы говорим про его для простых множеств, не позволяет вообще доказать для негладких задач что-то хорошее.
[31:24.760 --> 31:34.760]  Не позволяет доказать что-то хорошее просто потому, что вот такой вот метод, который у которого доказательство сильно опирается на наличие гладкости, на наличие гладкости.
[31:34.760 --> 31:43.760]  Окей, так, ну давайте быстренько перейдем к следующей теме. Сколько у нас прошло от лекции? Полчаса.
[31:43.760 --> 31:46.760]  Давайте перерыв сделаем, а потом уже к следующей теме тогда.
[31:46.760 --> 31:58.760]  Давайте продолжать. В общем-то мы поняли то, что в принципе гладкие задачи в некотором смысле с точки зрения теории сложнее, ой, негладкие задачи сложнее, чем гладкие.
[31:58.760 --> 32:10.760]  Оценки исходимости методов хуже, никакого ускорения нету, поэтому вообще взаимодействовать с какими-то негладкими вещами получается хотелось бы избегать.
[32:10.760 --> 32:21.760]  Вот, и в некотором смысле, ну ту негладкость, которая есть в задаче, можно было бы как-то в некотором смысле спрятать под ковер, спрятать под ковер.
[32:21.760 --> 32:29.760]  И чтобы сделать это, давайте ведем вот такой вот объект, который называется проксимальным оператором.
[32:29.760 --> 32:41.760]  Проксимальным оператором пусть у меня есть функция R, которая у меня в общем случае негладкая, тогда проксимальный оператор для нее определяется следующим образом.
[32:41.760 --> 32:48.760]  Это вот такое вот выражение. Такое вот выражение. Почему оно нам именно в таком виде нужно будет, мы поймем чуть-чуть позже.
[32:48.760 --> 32:55.760]  Мы поймем чуть-чуть позже, но тут предполагается, что вот такой проксимальный оператор мы можем считать бесплатно.
[32:55.760 --> 33:01.760]  Бесплатно для функции R. А что будет, если мы его можем в некотором смысле считать только платно?
[33:01.760 --> 33:03.760]  Ну, обсудим в самом конце.
[33:03.760 --> 33:11.760]  В общем, пока предполагаем, что вот если существует проксимальный оператор определяется следующим образом, мы его можем считать бесплатно.
[33:11.760 --> 33:15.760]  Что вы можете сказать про функцию, которого у вас стоит под проксимальным оператором?
[33:16.760 --> 33:21.760]  Что вы можете сказать про функцию, которая superst feature под проксимальным оператором если R является выпуклый?
[33:21.760 --> 33:29.200]  тоже выпукла не просто выпукла какая она сильно выпукла да а что мы знаем про
[33:29.200 --> 33:34.440]  сильно выпуклые задачи кто помнит что у них про про решение сильно выпуклых задач мы знаем
[33:34.440 --> 33:43.520]  да у них она единственная причем еще и уникальная вот получается что можно доказать что у нас
[33:43.520 --> 33:52.480]  проксимальный оператор проксимальный оператор это однозначный оператор то есть он не возвращает
[33:52.480 --> 33:56.760]  многозначные значения единственное что нужно дополнительно предположить что хоть в какой-то
[33:56.760 --> 34:03.000]  точке функция r не принимает значение плюс бесконечность если она всегда плюс бесконечность
[34:03.000 --> 34:07.920]  то там смысла вообще в проксимальном операторе нет у нас просто всегда равен там значение под ним
[34:07.920 --> 34:12.520]  всегда равно плюс бесконечности вот никакого минимума там искать не получится если у вас
[34:12.520 --> 34:20.320]  какой-то точки есть конечное значение то понятно что тогда уже работают все эти вещи которые
[34:20.320 --> 34:25.440]  мы с вами обсуждали про сильно выпуклые функции и у нее существует единственное решение поэтому
[34:25.440 --> 34:33.920]  оператор всегда возвращает одно число проксимальный вот это соответственно здесь у меня и формализована
[34:33.920 --> 34:40.320]  окей так почему вообще хочется биться за проксимальный оператор ну хотя бы потому что
[34:40.320 --> 34:49.960]  для каких-то не гладких функций он определяется в замкнутой форме то есть можно просто выписать
[34:49.960 --> 34:59.000]  в явном виде в явном виде но вот соответственно пример l 1 нормы l 1 нормы по факту это же это
[34:59.000 --> 35:05.960]  пример просто вашего модуля просто в одном случае многомерный многомерный и там легко удостовериться
[35:05.960 --> 35:12.240]  что проксимальный оператор будет вычисляться следующим образом причем в некотором смысле так
[35:12.240 --> 35:18.520]  сепарабельно по каждой из координат он считается отдельно по каждой из координат он считается
[35:18.520 --> 35:26.800]  отдельно хорошо и на самом деле вот часто вот такой проксимальный оператор ну вот с таким с
[35:26.800 --> 35:34.120]  такой функцией r которая определена первой нормой первой нормой называют threshold threshold
[35:34.120 --> 35:39.560]  почему потому что нужно посмотреть как это все безобразие безобразие выглядит
[35:39.560 --> 35:51.640]  выглядит у вас на графике у вас на графике ну давайте посмотрим посмотрим со срезками
[35:51.640 --> 35:57.840]  мы с вами работали да со срезками с вами работали ну вот давайте что в нулест происходит с этой
[35:57.840 --> 36:11.760]  функции со с этим значением prox r x и так что в нуле ну 0 просто сигнум равен нулю тут четко
[36:11.760 --> 36:20.200]  0 если чуть-чуть это иду от нуля но не за пределы лямбды не за пределы лямбды что тогда будет
[36:20.200 --> 36:26.680]  происходить с значением функции вот в этой точке да потому что смотрите у меня по модулю x будет
[36:27.320 --> 36:32.200]  значение будет меньше чем lambda значение будет отрицательным а срезка у меня как раз дело то
[36:32.200 --> 36:37.040]  что если значение отрицательно она его просто в 0 переводит поэтому я пока не дойду до лямбды
[36:37.040 --> 36:43.560]  так не дойду до лямбды мне здесь просто 0 у меня здесь просто 0 а потом соответственно функция
[36:43.560 --> 36:50.180]  начнет подниматься вот так вот линейно и абсолютно симметричная ситуация здесь абсолютно симметричная
[36:50.180 --> 36:58.820]  до минус лямбды значение отрицательное, а потом начнет это все успешно подниматься наверх,
[36:58.820 --> 37:04.940]  то есть уже начинает приобретать положительные значения. Хорошее свойство, на самом деле,
[37:04.940 --> 37:09.460]  мы его обсудим чуть попозже, когда дойдем до метода, который использует проксимальный оператор.
[37:09.460 --> 37:17.500]  То есть пока запомним то, что проксимальный оператор по такой вот функции помогает отсекать
[37:17.500 --> 37:25.620]  в некотором смысле какие-то значения координат, которые меньше порогового значения лямбда. Поэтому
[37:25.620 --> 37:31.660]  и трешхолдом называется пороговое значение лямбда, вы по нему отсекаете координату. Если она,
[37:31.660 --> 37:37.580]  соответственно, ниже этого порогового значения, вы ее тупо зануляете, тупо зануляете. Это хорошее
[37:37.580 --> 37:43.940]  свойство, но чуть попозже. Для квадратичной нормы в квадрате, для эвклидовой нормы в квадрате,
[37:43.940 --> 37:53.140]  проксимальный оператор определяется вот так вот. И оказывается, что для индикаторной функции
[37:53.140 --> 37:58.860]  множества х, проксимальный оператор тоже можно определить. Как вы думаете, чему он будет равен?
[37:58.860 --> 38:13.100]  Давайте я выпишу. Прокс, R, X. А? Чему-чему? Правильно, проекции. То есть, давайте я
[38:13.100 --> 38:20.580]  для тех, кто, для остальных выпишу, у нас все получается вот так вот. Так, вместо R я пишу индикатор,
[38:20.580 --> 38:33.460]  и здесь будет одна вторая х в квадрате. Понятно, что у меня вот этот арг-минимум определен не на
[38:33.460 --> 38:41.460]  все множестве R, Rd, потому что просто индикатор у меня улетает в бесконечность за пределами. Поэтому
[38:41.460 --> 38:48.180]  это же просто эквивалентно арг-минимуму на множестве х. На некотором выпукло множестве х,
[38:50.580 --> 38:58.420]  а это у нас, ну, так как это арг-минимум, я еще одну вторую тут уберу, одну вторую уберу. Но это же
[38:58.420 --> 39:02.540]  ровно с евклидовой проекции. Оказывается, проксимальный оператор в некотором смысле это
[39:02.540 --> 39:08.740]  вообще не евклидовой проекции. Существует довольно много различных результатов для
[39:08.740 --> 39:13.500]  проксимальных операторов, частных случаев, как они преобразуются, когда вы работаете
[39:13.500 --> 39:20.540]  суммами функций, когда вы работаете сепарабельными функциями, когда вы делаете какие-то аффинные
[39:20.540 --> 39:25.980]  преобразования внутри аргумента. Довольно много существует результатов, как считать проксимальные
[39:25.980 --> 39:32.260]  операторы. Как много считать проксимальные операторы в частных случаях. Но та фишка, которая нам нужна,
[39:32.260 --> 39:39.700]  это то, что проксимальный оператор, ну, вот, например, здесь в некотором смысле может вам скушать за
[39:39.700 --> 39:46.900]  бесплатно негладкую функцию. Негладкую функцию, то есть здесь у вас была какая-то вроде бы плохая
[39:46.900 --> 39:51.900]  функция негладкая, которая там не дифференцируема в нуле, вот, но проксимальный оператор ее умеет
[39:51.900 --> 39:58.420]  обрабатывать за бесплатно. В явном виде, в замкнутой форме он выдает вам ответ. Где, соответственно,
[39:58.420 --> 40:04.620]  это можно использовать, ну, поймем чуть попозже. Проекцию мы с вами поняли. Окей, давайте
[40:04.620 --> 40:12.060]  подоказываем пока некоторые свойства проксимальных операторов, чтобы в дальнейшем уже с ними успешно
[40:12.060 --> 40:18.800]  взаимодействовать. Ну, первое свойство, следующее, опять же, у нас r выпукла функция, для нее определен
[40:18.800 --> 40:25.100]  проксимальный оператор. Тогда для любых x и y следующие условия эквивалентны. y у нас равен
[40:25.100 --> 40:32.140]  проксимальному оператору функции r в точке x. Тогда и только тогда, когда у нас x минус y,
[40:32.140 --> 40:41.860]  ой, только тут r, r и не f, вот, когда у нас x минус y принадлежит суб дифференциалу точки y,
[40:41.860 --> 40:49.740]  точки y, вот. Ну как, откуда это взять? Проксимальный оператор, вспоминаем, это опять же у нас задача
[40:49.740 --> 40:56.540]  некоторой минимизации. Только что мы это выписывали, давайте выпишем еще раз. Некоторая задача
[40:56.540 --> 41:02.420]  минимизации. Для этой задачи мы с вами что, соответственно, знаем? Что мы про нее знаем?
[41:02.420 --> 41:10.100]  Прокс, ну давайте тут x tilde уже буду ставить. Что мы для нее можем выписать для этой задачи?
[41:10.100 --> 41:17.380]  x, r. Условие оптимальности. А как оно будет выглядеть? Как оно будет выглядеть для вот этой задачки,
[41:17.380 --> 41:25.500]  которая у меня стоит под argmin? Как она будет выглядеть? Условие оптимальности. Ноль лежит
[41:25.500 --> 41:37.740]  суб дифференциале вот этой функции r от x плюс, тут только с tilde. Вот, давайте я сразу поставлю
[41:37.740 --> 41:56.060]  оптимальное. r x минус x и здесь x оптимальное. А нет, еще суб дифференциал я не взял. Вот,
[41:56.060 --> 42:05.300]  одна вторая, x, ну давайте вот так. Вот так запишу. Суб дифференциал того, что у меня здесь записано,
[42:05.300 --> 42:16.300]  минус x в квадрате. В точке получается x tilde равно x звездой, которое, ну вот решение
[42:16.300 --> 42:23.220]  проксимального оператора. Решение проксимального оператора, ну или я могу сразу тут y написать,
[42:23.220 --> 42:30.900]  у меня же y это решение проксимального оператора. Окей, так, чему равен суб дифференциал? Давайте
[42:30.900 --> 42:46.300]  выпишем dr точки y плюс y минус x. Согласны? Ну и тогда что у меня получается? y минус x,
[42:46.300 --> 43:01.940]  так, с минусом, принадлежит dr точки y. Согласны? Просто перенес влево. Вот, вот это же вектор
[43:01.940 --> 43:09.220]  у меня, я его могу вычесть из правой и из левой части. Окей, что получается? Что получается? Ну то,
[43:09.220 --> 43:14.140]  что нужно получается, то есть вот отсюда сюда действует. Но так как у меня условия оптимальности,
[43:14.140 --> 43:21.460]  это вообще критерий, то получается то, что у меня x минус y принадлежит суб дифференциалу,
[43:21.460 --> 43:29.260]  эквивалентно тому, что на самом деле у меня, и вот это будет правда, в силу того, что у меня вот
[43:29.260 --> 43:36.540]  это критерий, условия оптимальности, оно действует в обе стороны. Поэтому первое и второе вещи
[43:36.540 --> 43:49.560]  эквивалентные. Осталось показать, ну давайте, эквивалентность второго и третьего. Второго
[43:49.560 --> 43:54.540]  и третьего здесь мы показали, что первое и второе эквивалентные. Теперь осталось показать
[43:54.540 --> 44:01.580]  эквивалентность второго и третьего. Что у нас тут для произвольного z? Ну, что тут похоже?
[44:01.580 --> 44:08.240]  Похоже на что тут похоже, но раз для произвольного задания, видимо, надо писать какую-то выпуклость.
[44:08.240 --> 44:12.080]  Какую-то выпуклость, она здесь в некотором смысле и записана, вот.
[44:12.080 --> 44:20.800]  Соответственно, если я знаю, что у меня х-у лежит в субдифференциале R, тогда,
[44:20.800 --> 44:27.560]  так как х-у это у меня субградиент, для него должно быть выполнено условие выпуклости.
[44:27.560 --> 44:53.960]  Согласны? Просто из определения субградиента и субдифференциала, субградиент и субдифференциала.
[44:53.960 --> 45:02.000]  В обратную сторону, что я могу сказать, в обратную сторону у меня для любой точки z выполнено следующее,
[45:02.000 --> 45:10.760]  х-у, z-у меньше либо равно, чем R от z, минус R от y, для любой точки z.
[45:10.760 --> 45:17.080]  Получается, что вот тогда у меня х-у просто по определению лежит в субдифференциале точки y.
[45:17.080 --> 45:21.800]  Согласны? Все. То есть тут довольно просто.
[45:21.800 --> 45:24.360]  Получается, все эти три условия эквивалентны.
[45:24.360 --> 45:26.000]  Все эти три условия эквивалентны.
[45:26.000 --> 45:30.320]  Вот. На самом деле мне скорее будут, сейчас понадобится второе условие,
[45:30.320 --> 45:34.040]  ой, третье, третье условие. Вот. Мы к нему и шли. Мы к нему и шли.
[45:34.040 --> 45:45.240]  Окей. Так. Так, это доказательство. Бум-бум-бум. Бум-бум. Так.
[45:45.240 --> 45:50.960]  Мне вот такое вот понадобится. Я хочу доказать вот еще одно свойство
[45:50.960 --> 45:56.320]  проксимального оператора. То, что у меня опять же, если проксимальный оператор задан выпуклой функцией,
[45:56.320 --> 46:03.640]  то тогда вот выполнены такие свойства. Первое называется firmly non-expansive-ness,
[46:03.640 --> 46:10.080]  firmly, ну, создавайте слабое, не расширяемость. А второе просто называется non-expansive-ness.
[46:10.080 --> 46:14.480]  То есть не расширяемость. Где мы с ней уже сталкивались?
[46:14.480 --> 46:21.040]  В проекции. Ну, в принципе, вещь ожидаемая. То есть у нас проекция была не расширяемая,
[46:21.040 --> 46:25.960]  а, как мы понимаем, уже проксимальный оператор, это такое обобщение проекции.
[46:25.960 --> 46:30.440]  Поэтому то, что это свойство здесь всплыло, это очень даже хорошо. Это очень даже хорошо.
[46:30.440 --> 46:35.880]  Вот. Соответственно, мы с ним сможем потом провозаимодействовать. Окей.
[46:35.880 --> 46:43.040]  Давайте подоказываем. Ну, давайте первое свойство докажем, второе, потому что,
[46:43.040 --> 46:48.720]  ну, там совсем очевидно будет. Пусть у меня это у. Проксимальный оператор r в точке x,
[46:48.720 --> 47:01.360]  v это проксимальный оператор r в точке y. В точке y. Что я тогда могу сказать? Что я тогда могу
[47:01.360 --> 47:06.480]  сказать? Исходя из того свойства, которое у меня было доказано до этого. Давайте,
[47:06.480 --> 47:14.080]  вот я вот на него отмотаю. Вот это свойство. Ну, давайте его выпишем. Давайте его просто
[47:14.080 --> 47:29.880]  выпишем в таком виде, в котором оно у меня тут есть. Сейчас. x-y, z-y меньше либо равно,
[47:29.880 --> 47:39.080]  чем r от z минус r от y. Вот. Хорошо. Потому что x-y у меня вроде как есть. Мне бы нужно
[47:39.080 --> 47:46.080]  где-то накопать теперь проксимальные операторы. Проксимальные операторы. Вот. Ну, давайте вот напишу
[47:46.080 --> 47:54.520]  здесь z1 и запишу это условие еще раз. y-x, только теперь поменяем местами. y и x.
[47:59.880 --> 48:05.760]  Согласны? Согласны? Так.
[48:05.760 --> 48:23.000]  Так, хорошо. Хорошо. Ну, что тогда теперь сделаем? Давайте попробуем как-то подобрать z1 и z2 так,
[48:23.000 --> 48:30.200]  чтобы у меня в некотором смысле что-то, сейчас давайте гляну, я сам подзабыл,
[48:30.200 --> 48:37.680]  что тут надо аккуратненько схлопнуть. А, вот так. Да, ладно, давайте вот так сделаем.
[48:37.680 --> 48:49.520]  Вот так вот запишем в силу того, что у меня точка u. Для произвольной точки z должно выполняться z1.
[48:49.520 --> 48:56.800]  Вот. У же у меня должно быть проксимальным оператором. Я не ту точку подставил. Я не ту точку подставил.
[48:56.800 --> 49:05.600]  Там же было так, что у меня в теореме u это проксимальный под x. Поэтому если у меня здесь u это
[49:05.600 --> 49:12.120]  прокс x, тогда у меня здесь должно быть u, а здесь v, потому что v прокс y. Вот. Сейчас, сейчас, сейчас все норм.
[49:12.120 --> 49:18.760]  Вот. Тогда у меня получится вот такое соотношение. z1 и z2 я могу подставлять любыми.
[49:18.760 --> 49:27.480]  z1 и z2 я могу подставлять любыми. В частности, подставлю их равными z1, подставлю равным v,
[49:27.480 --> 49:40.320]  а z2 подставлю равным u. Ну, еще посмотрю, что получается. x-u, z1 это v-u, меньше либо равно, чем rv-ru.
[49:40.320 --> 49:52.800]  А 2-ое это неравенство r-v, z2 это у-v, меньше либо равно, чем ru-rv.
[49:52.800 --> 50:00.200]  Складываю эти два неравенства и что получаю? Справа у меня будет четко стоять 0, а слева у меня
[50:00.200 --> 50:13.400]  будет стоять x-u-y плюс v, v-u, меньше либо равно 0, меньше либо равно 0. Ну, тогда что я выделяю?
[50:13.400 --> 50:28.520]  x-y, v на u, и здесь у меня что получается? Минус v-u в квадрате, меньше либо равно 0, меньше либо равно 0.
[50:28.520 --> 50:38.280]  Все хорошо, с плюсом вырезала. И тогда у меня что получается? v-u в квадрате,
[50:38.280 --> 50:53.160]  меньше либо равно, чем x-y, u-v. Подставляю, получается prox x-prox y меньше либо равен,
[50:53.160 --> 51:06.520]  чем prox x-prox y, x-y. Все, получили вот это свойство ферменон экспансивенни, слабую сжимаемость.
[51:06.520 --> 51:18.600]  Как из нее тут же доказать, тут только квадрат, как из нее доказать, что prox-prox будет меньше,
[51:18.600 --> 51:26.840]  чем x-y. Как из нее быстренько доказать это? kbh, то есть здесь мы применяем kbh и получаем,
[51:26.840 --> 51:37.000]  что у нас здесь prox-prox умножить на x-y больше либо равен, чем вот это выражение сколярного
[51:37.000 --> 51:44.160]  произведения. Делим на один из проксов, потому что слева у нас два прокса стоит, и получаем ровно
[51:44.160 --> 51:53.680]  то, что нужно. Вот это то свойство, которое мы с вами уже наблюдали, когда говорили про проекции.
[51:53.680 --> 52:02.960]  Так, пум-пум-пум. Так, хорошо, свойства доказали. Теперь вообще хочется посмотреть на задачи,
[52:02.960 --> 52:08.520]  где нам эти проксимальные операторы понадобятся. Вот, рассмотрим вот такую задачу минимизации
[52:08.520 --> 52:15.080]  функции, где у меня есть, по факту, моя целевая функция состоит из двух частей, из двух компонентов.
[52:15.080 --> 52:21.800]  Такую задачу называют композитной, потому что два слагаемых. Ну и предположим, что у меня как бы в
[52:21.800 --> 52:30.360]  данном случае f отвечает за гладкую часть этой функции, а r отвечает за негладкую, но
[52:30.360 --> 52:34.200]  проксимально дружественную. Это означает то, что у меня, я могу посчитать от r,
[52:34.200 --> 52:39.080]  проксимальный оператор бесплатно. Ну то есть в явном виде я могу просто записать формулу и
[52:39.080 --> 52:44.720]  соответственно выписать. Ну вот такая вот постановка довольно общая. То есть раньше мы
[52:44.720 --> 52:50.120]  рассматривали случаи, когда у нас r равно нулю. Тождественно равно нулю просто гладкая функция была.
[52:50.120 --> 52:55.800]  До этого, то есть в начале этой лекции рассматривали случаи, когда у нас f равно
[52:55.800 --> 53:01.720]  тождественно нулю, и у нас просто есть минимизация некоторой функции r, которая является негладкой. Но
[53:01.720 --> 53:06.000]  про проксимальную дружественность мы не говорили. Проксимальную дружественность мы не говорили.
[53:06.000 --> 53:13.600]  Вот, но здесь вот такая вот общая постановка. Так, окей. Ну и предлагается следующий метод для
[53:13.600 --> 53:20.040]  решения этой задачи. То есть в силу того, что у меня f, теперь вещь дифференцируемая,
[53:20.040 --> 53:25.920]  непрерывно дифференцируемая, гладкая, я от нее беру градиент и делаю следующий шаг. То есть вроде как
[53:25.920 --> 53:32.400]  делаю градиентный шаг, делаю градиентный шаг, но потом этот шаг оборачиваю в проксимальный оператор.
[53:32.400 --> 53:37.880]  В принципе, интуиция даже понятна, когда мы смотрим на оператор проекции. То есть в частный
[53:37.880 --> 53:43.880]  случай оператор проекции. То же самое было шаг градиента, потом накидываем на это все безобразие
[53:43.880 --> 53:51.240]  проекцию. Супер. Но теперь давайте в общем случае хочется понять, что вообще происходит. То есть когда у
[53:51.240 --> 53:57.120]  нас, например, r дифференцируемо, вот этот проксимальный оператор, который у нас arg- минимум,
[53:57.120 --> 54:06.320]  как для него выписываются условия оптимальности? Давайте его еще раз вот. Гамма r, x. Так, плюс одна
[54:06.320 --> 54:32.800]  вторая. Здесь у нас будет x, k минус гамма. Вот так. Минус x в квадрате. Как запишется
[54:32.800 --> 54:40.600]  оптимальная условия, если у меня r дифференцируемо? Градиент просто этого выражения равен нулю.
[54:40.600 --> 54:49.200]  Согласны? Вот. Ну давайте запишу. Запишу, а потом поймем, что мы в итоге выписали. Гамма градиент r от
[54:49.200 --> 55:04.440]  x плюс x минус x, k плюс гамма f от x, k равно нулю. Ну так как я потом это оптимальное значение
[55:04.440 --> 55:11.720]  подставляю в x, k плюс 1, поэтому я здесь его и подставлю. Вот. Ну и что здесь получается? Чему
[55:11.720 --> 55:28.840]  равно x, k плюс 1? Это x, k минус гамма f от x плюс r, x, k плюс 1. Вот. Ну вот такой вот метод в
[55:28.840 --> 55:34.760]  реальности там записан. Здесь я его записал, так скажем, в неявном виде, потому что в таком
[55:34.760 --> 55:39.400]  виде, понятно, вы метод записать не можете. Как найти точку x, k плюс 1, когда вам нужно посчитать
[55:39.400 --> 55:46.280]  градиент точки r в функции r в точке x, k плюс 1. Но вот в реальности реальный градиентный спуск
[55:46.280 --> 55:51.880]  честный. Если бы мы сказали, что у нас вот есть функция f плюс r, и мы бы не смотрели на ее специфику,
[55:51.880 --> 55:56.440]  что она состоит из двух частей. Просто сказали, есть дифференцируемая функция, там градиент f,
[55:56.440 --> 56:01.440]  градиент r, тупо берем градиент p и градиент p, суммируем, получаем общий градиент, делаем шаг
[56:01.440 --> 56:06.560]  градиентного спуска. Здесь бы стояла точка x, k просто. Ну а в случае вот проксимального,
[56:06.560 --> 56:12.400]  когда мы можем посчитать от r проксимальный оператор бесплатно, функция будет выглядеть
[56:12.400 --> 56:17.600]  соответственно следующим образом. Шаг метода будет выглядеть следующим образом. Это неявный
[56:17.600 --> 56:24.000]  вид. Явный вид вот там. То есть у вас вот этот неявный шаг, он зашит в вычисление проксимального
[56:24.000 --> 56:29.800]  оператора. В случае проекции это там выбор нужной точки, ну а в случае каких-то простых
[56:29.800 --> 56:38.560]  регулизаторов, это получается у вас просто вычисление этого прокса. Вот. И здесь, кстати,
[56:38.560 --> 56:44.800]  как раз хочется поговорить, где в первую очередь применяется проксимальный алгоритм. Это как раз
[56:44.800 --> 56:50.440]  в случае, когда у вас задача машинного обучения. f это обычно целевая функция. Там, например,
[56:50.440 --> 56:55.760]  ваша логистическая регрессия. r это регулизатор. Вы с ним, в принципе, взаимодействовали,
[56:55.760 --> 57:02.040]  добавляли к логистической регрессии регулизатор. Было такое в домашнем задании. l2 регулизатор
[57:02.040 --> 57:06.800]  совсем простой, тоже гладкий, спокойно дифференцируемый, и вы как раз не использовали
[57:06.800 --> 57:12.560]  никаких проксов. Но можно добавлять и более хитрые регулизаторы, например, или один регулизатор.
[57:12.560 --> 57:20.040]  И тут сразу же следует его важное свойство. То, что я помню, показывал вам Threshold, то что он у
[57:20.040 --> 57:24.920]  вас после шага. То есть что происходит в реальности алгоритма, когда вы, например, используете для
[57:24.920 --> 57:30.160]  вашей логистической регрессии или один регулизатор. Вы делаете шаг градиента воспуска,
[57:30.160 --> 57:36.320]  а потом у вас включается проксимальный оператор. И проксимальный оператор что вам сделает? Он вам
[57:36.320 --> 57:46.040]  отсечет те координаты, которые имеют маленькое значение по модулю. Был такой эффект у вас уже
[57:46.040 --> 57:55.480]  в домашнем задании. Был. Помните, как раз вы работали с методом Франко Вульфа и решали с
[57:55.480 --> 58:02.320]  помощью него задачку на l1 шарике. И у вас там как раз был такой классный эффект, что если вы
[58:02.320 --> 58:10.680]  стартуете из нуля, то количество не нулевых компонент не будет быстро увеличиваться. То есть
[58:10.680 --> 58:18.800]  в итоге в решении процентов 10, 15 и нулевых компонент. Здесь тот же самый эффект. Похожий,
[58:18.800 --> 58:26.040]  похоже, потому что природа регулизатора и множество похожие, потому что l1 нормой
[58:26.040 --> 58:31.760]  порождается. Но здесь просто в l1 норма вам просто запрещало выходить, когда у вас было множество. А
[58:31.760 --> 58:38.960]  здесь вам разрешается выходить за пределы множества просто потому что регулизатор. Ну и соответственно
[58:38.960 --> 58:45.880]  природа похожая. То есть здесь трэш холпом отсеивает компоненты, которые близки к нулю. И там
[58:45.880 --> 58:51.640]  вы отсеивали довольно большое число компонент за счет того, что проецировались на множество. Вот.
[58:51.640 --> 58:57.480]  Вот такая вот идея, почему это может быть полезно, в том числе в машинном обучении,
[58:57.480 --> 59:05.080]  причем бесплатно. Сейчас мы это поймем, что это действительно бесплатно. Вот. Окей,
[59:05.080 --> 59:11.160]  выписал явный вид, явный вот эту схему. Что нам еще нужно? Нужно доказать вот такое вот свойство.
[59:11.160 --> 59:17.040]  Такое вот свойство, что x звездой у нас является стационарной точкой нашего вот этого
[59:17.040 --> 59:24.120]  проксимального оператора плюс градиентного шага. Градиентного шага. Ну тут все довольно
[59:24.120 --> 59:30.160]  стандартно. Опять же, выписываем условия оптимальности. Ну, то есть прокса у нас
[59:30.760 --> 59:42.160]  в данном случае гамма r от x плюс одна вторая x минус x звездой плюс гамма f от x звездой.
[59:42.160 --> 59:51.800]  Условия оптимальности выписываю. Тут только тут я уже выписываю через субдифференциал. То есть
[59:51.800 --> 01:00:03.080]  0 должен принадлежать субдифференциалу. Вот. Что получается? Что получается? Гамма r от x плюс одна
[01:00:03.080 --> 01:00:12.840]  вторая x. Тоже самое переписывается. Дальше мне просто нужно взять субдифференциал. Что получается?
[01:00:12.840 --> 01:00:31.400]  Гамма субдифференциал r плюс x минус x со звездочкой плюс гамма f от x. 0 принадлежит. Ну и заметим,
[01:00:31.400 --> 01:00:38.640]  что если я поставлю x со звездочкой, то у меня все выполнится. То есть x и вот здесь сократятся. Ну,
[01:00:38.640 --> 01:00:44.960]  а я знаю из условий оптимальности моей исходной задачи то, что у меня субдифференциал r плюс
[01:00:44.960 --> 01:00:50.960]  градиент f от x со звездой. Ну f от x у меня дифференцирован. Получится общий субдифференциал
[01:00:50.960 --> 01:00:55.680]  исходной функции. 0 ему должен принадлежать в оптимальной точке. Вот. В силу того, что опять же
[01:00:55.680 --> 01:01:00.760]  условия оптимальности это у меня критерий. Я это могу провернуть в обе стороны. То есть сначала
[01:01:00.760 --> 01:01:07.080]  подставить сюда x со звездой, вытащить вот так в таком неявном виде. Ну вот так вот просто сказать.
[01:01:07.080 --> 01:01:12.080]  Ну давайте я вот так вот рассмотрю. Дальше, соответственно, в силу того, что это у меня критерий,
[01:01:12.080 --> 01:01:17.040]  поэтому я поднимаюсь наверх. Говорю, что это 0 принадлежит субдифференциалу вот такой вот функции.
[01:01:17.040 --> 01:01:22.720]  Ну а дальше еще выше поднимаюсь. Вот. В силу того, что опять же у меня критерий, вот это выполнено.
[01:01:22.720 --> 01:01:32.960]  Вот это условие выполнено. Окей. Окей. Ну я как раз вот здесь от условий оптимальности и иду. Говорю,
[01:01:32.960 --> 01:01:39.280]  что у меня вот это условие оптимальности. Дальше, умножаю на гамма, добавляю незначащий x.
[01:01:39.280 --> 01:01:44.560]  И здесь у меня что получается? Вот это как раз точка, а это точка, в которой я считаю
[01:01:44.560 --> 01:01:54.240]  проксимальный оператор. Так. И здесь у меня как раз получается х со звездой равен проксимальному оператору.
[01:01:54.240 --> 01:02:06.080]  Ну вот это то условие как раз у меня x было x-y принадлежит dr. На данном случае гамма dr х со звездой.
[01:02:06.080 --> 01:02:12.280]  Вот. Второе условие из теоремы свойств проксимального оператора. Вот. Я им пользуюсь,
[01:02:12.280 --> 01:02:16.960]  она у меня тоже критерий. Она у меня тоже критерий, что прокс равен вот этому значению.
[01:02:16.960 --> 01:02:22.600]  Поэтому в обе стороны эти рассуждения верны. Окей. Здесь разобрались. Здесь разобрались.
[01:02:22.600 --> 01:02:32.800]  Получили вот два таких свойства. Нерасширяемость и то, что у меня стационарная точка вот этого моего шага
[01:02:32.800 --> 01:02:38.960]  метода, это точка х со звездой. Точка х со звездой. Где у нас, кстати, такая же стационарность была?
[01:02:38.960 --> 01:02:48.800]  Кто помнит? Снова в проекции. То есть, в принципе, вот здесь вот это очень похожие свойства на те,
[01:02:48.800 --> 01:02:53.560]  которые мы с вами встречали в проекции. Они прям один в один. Один в один. Поэтому
[01:02:53.560 --> 01:03:02.040]  доказательства будут тоже один в один. х ка плюс один моя точка. Как я ее считаю,
[01:03:02.040 --> 01:03:08.400]  как проксимальный оператор только тут функции r. Только функции r. Проксимальный оператор от шага
[01:03:08.400 --> 01:03:17.480]  градиентного спуска. Окей. Дальше что знаю? Я знаю то, что мне х со звездой. Это прокс гамма r х со
[01:03:17.480 --> 01:03:26.560]  звездой минус гамма f от х со звездой. Вот. Окей. Подставляю сюда. Подставляю сюда. У меня получается
[01:03:26.560 --> 01:03:48.640]  разность двух проксов. Разность двух проксов гамма r х со звездой минус градиент f от х со звездой.
[01:03:48.640 --> 01:03:54.680]  Так-так. Все. То есть, здесь воспользовались свойством, что х со звездой это стационарная
[01:03:54.680 --> 01:04:02.040]  точка и она равна просто проксу от шага градиента. Окей. Дальше нерасширяемость прокса. У меня здесь
[01:04:02.040 --> 01:04:17.880]  останется только х ка минус гамма f от х ка минус х со звездой плюс гамма f от х со звездой. Вот. Что мы
[01:04:17.880 --> 01:04:24.920]  можем сказать про градиентов от х со звездой? Быстренько. Что-то можем сказать или нет?
[01:04:30.600 --> 01:04:31.480]  Можем или нет?
[01:04:41.000 --> 01:04:46.000]  Ну, тут ответ нет тоже подойдет на самом деле. То есть, можем или не получается или нет что-то
[01:04:46.000 --> 01:04:56.680]  сказать. Будто равен нулю или нет. Правильно. Потому что на самом деле у вас задача f от х плюс r от х.
[01:04:56.680 --> 01:05:02.120]  Так. Даже если у вас там дифференцируемая функция r, у вас получается, что вот градиент суммы
[01:05:02.120 --> 01:05:08.160]  должен равен в оптимальной точке быть равен нулю. Так. При этом каждый из компонентов может быть
[01:05:08.160 --> 01:05:13.760]  не равняться нулю. Согласны? Ну, в сумме ноль, но каждый из компонентов, понятно, может быть нулевой.
[01:05:13.760 --> 01:05:19.320]  Вот. Поэтому про градиент с точки х со звездой вы сказать примерно ничего не можете. Вот. Но так же было,
[01:05:19.320 --> 01:05:24.720]  в принципе, когда мы работали с проекциями. Как когда мы работали с проекциями, у нас же тоже там
[01:05:24.720 --> 01:05:32.360]  условия оптимальности было такое, то что, ну если х со звездой, которая минимум на множестве,
[01:05:32.360 --> 01:05:38.600]  не совпадает с глобальным минимумом на все rd, то там градиент может быть и не нулевым. Он вам
[01:05:38.600 --> 01:05:45.120]  просто должен указывать за пределы множества. За пределы множества всегда. Артагонально множество,
[01:05:45.120 --> 01:05:51.160]  это мы тоже с вами выписывали. Вот. А здесь, здесь, соответственно, тоже ничего нельзя сказать,
[01:05:51.160 --> 01:05:56.480]  как и в случае проекции. Потому что, опять же, prox это обобщение проекции. Вот. Выписали такое
[01:05:56.480 --> 01:06:03.920]  безобразие, опять же расписали расстояние. Вот это. Теперь можно раскатать по гладкости. Раскатать
[01:06:03.920 --> 01:06:09.520]  по гладкости. Я, на самом деле, доказательства скопировал просто из проекции, потому что одно и
[01:06:09.520 --> 01:06:17.040]  тоже мы применили свойства prox, прочь на prox заменил просто. Вот. А здесь, кто помнит,
[01:06:17.040 --> 01:06:24.440]  куда девается х со звездой? f от x, градиент f от x со звездой. Куда он там делся?
[01:06:24.440 --> 01:06:31.360]  Он же там не пропадает, но куда-то девается. Куда мы его там запихивали?
[01:06:33.920 --> 01:06:40.360]  Давайте я вам просполирую тогда. Дальше, что тут гладкость расписывается? Ну да, вот здесь
[01:06:40.360 --> 01:06:45.600]  сразу допишется, через дивергенцию Брегмана это мы расписывали. То есть, в случае, когда у вас
[01:06:45.600 --> 01:06:52.640]  градиент f от y, ну в данном случае y равен х со звездой, он просто равен нулю, то у вас
[01:06:52.640 --> 01:06:58.000]  дивергенция Брегмана вот эта выпадала, и у вас получалось что-то в духе xk, то что у нас раньше
[01:06:58.000 --> 01:07:04.840]  получалось х со звездой. Так, в силу того, что теперь у вас градиент не выпадает, потому что не нулевый,
[01:07:04.840 --> 01:07:09.920]  вот, ну тогда дивергенция Брегмана у вас в некотором смысле выглядит более полно. То есть,
[01:07:09.920 --> 01:07:16.400]  скалярное произведение в ней остается, и его за собой нужно тянуть. Но как мы с вами знаем,
[01:07:16.400 --> 01:07:21.400]  дивергенция Брегмана у вас всегда, это вещь положительная, в силу того, что ее порождает
[01:07:21.400 --> 01:07:28.640]  выпуклая функция. Ее порождает выпуклая функция, поэтому это в любом случае некоторый измеритель
[01:07:28.640 --> 01:07:35.240]  расстояния, поэтому это вещь положительная. Ну и когда вы это все распишете, у вас отсюда выпадает
[01:07:35.240 --> 01:07:42.280]  эта дивергенция Брегмана, ну и ровно такой же член, который был. Член, который был, правильно
[01:07:42.280 --> 01:07:48.360]  подбор гамма, его уничтожает, его уничтожает. Вы знаете, что дивергенция не отрицательна, больше
[01:07:48.360 --> 01:07:53.480]  либо равна нуля, гамма соответственно меньше, чем один делить на L, вас спасает. Вас спасает, вы
[01:07:53.480 --> 01:07:59.280]  убиваете второй кусочек и получаете линейную сходимость. Ровно все то же самое, что было в
[01:07:59.280 --> 01:08:07.040]  проекции. Все? Супер. Ну что можно сказать? Что можно сказать, что в некотором смысле
[01:08:07.040 --> 01:08:13.680]  проаксимальный градиентный спуск это обобщение метода с проекцией плюс обобщение градиентного
[01:08:13.680 --> 01:08:17.520]  спуска на случай, когда вы работаете с композитной задачей, где R является
[01:08:17.520 --> 01:08:24.840]  проаксимально дружественной функцией. Причем, что самое классное, то есть в те время сходимости,
[01:08:24.840 --> 01:08:30.240]  которые я сейчас получил, у меня будут фигурировать только mu, константа сильной выпуклости всей функции,
[01:08:30.240 --> 01:08:36.600]  ну или функции f, например, если она сильно выпуклая, и L, константа гладкости функции f. При этом,
[01:08:36.600 --> 01:08:42.160]  то, что у меня функция R не гладкая, вообще никак не влияет, но только влияет в том случае,
[01:08:42.160 --> 01:08:47.720]  когда у меня… не влияет, если у меня R проаксимально дружественная. То есть я вот этим
[01:08:47.720 --> 01:08:52.560]  проаксимальным оператором, который я могу считать в явном виде для некоторых простых функций,
[01:08:52.560 --> 01:08:59.280]  я сжираю эту негладкость и получается правильный метод с точки зрения сходимости. Ну как правильный,
[01:08:59.280 --> 01:09:05.640]  имеется в виду, что если бы я просто посмотрел на мою исходную задачу и сказал, ну так как это
[01:09:05.640 --> 01:09:11.200]  сумма гладкой и негладкой функции, то лучше, чем субградиентный метод, я применить ничего не
[01:09:11.200 --> 01:09:20.680]  могу. Это прямой взгляд, вижу негладкую функцию, применяю субградиентный метод. Тогда всплыли бы
[01:09:20.680 --> 01:09:29.720]  негладкие свойства функции R, и тогда бы сходимость была соответствующая плохая. Но если опять же я
[01:09:29.720 --> 01:09:34.240]  добавляю специфику в задачу, говорю о том, что, например, функция R является проаксимально
[01:09:34.240 --> 01:09:40.080]  дружественной, то я могу скушать вот эти свойства, скушать свойства негладкости и оставить только
[01:09:40.080 --> 01:09:47.320]  хорошие свойства гладкости моей исходной целевой функции F. Все. Ну тут еще написано, что,
[01:09:47.320 --> 01:09:58.160]  что тут еще у меня в выводах написано. Да, R скушается в случае, когда... Во, интересный вопрос.
[01:09:58.160 --> 01:10:05.160]  Кажется, когда, если я у меня F положу равной нулю, я могу вот так вот отрешивать любую негладкую
[01:10:05.160 --> 01:10:10.640]  задачу. Проаксимально дружественную. И это правда будет быстрее. Правда будет быстрее, но опять же
[01:10:10.640 --> 01:10:16.120]  проаксимальная дружественность. То есть по факту я могу находить минимум функций R за бесплатно. Ну,
[01:10:16.120 --> 01:10:22.160]  неинтересно. Но я же могу сказать, что, как мы с вами обсуждали, у нас тоже часто возникали какие-то
[01:10:22.160 --> 01:10:29.160]  вспомогательные arg-минимумы, я вам всегда говорил, что на самом деле их можно отрешивать неточно. То
[01:10:29.160 --> 01:10:34.240]  есть брать численный метод какой-то дополнительный и, пожалуйста, мы его отрешим, появляется небольшая
[01:10:34.240 --> 01:10:43.000]  неточность, но ее можно просто учесть дополнительно потом при анализе. Окей. Здесь, если я, соответственно,
[01:10:43.000 --> 01:10:52.360]  даю вот эту возможность отрешивать задачу неточно. Вот. Даю возможность, я действительно могу теперь
[01:10:52.360 --> 01:10:58.680]  решать любую негладкую задачу. Но на отрешивание arg-минимума мне нужны дополнительные ресурсы.
[01:10:58.680 --> 01:11:01.480]  Будет ли это лучше, чем субградиентный метод?
[01:11:04.240 --> 01:11:12.080]  В общем случае. Ну, ответ вроде очевиден. Давайте, подскажите мне его, может быть,
[01:11:12.080 --> 01:11:20.920]  понимаете уже ответ. Я его уже в некотором смысле давал. Да, с чего его? Потому что нижние оценки
[01:11:20.920 --> 01:11:25.880]  говорят, что субградиентный метод оптимален. Здесь мы тоже просто используем субградиенты и какую-то
[01:11:25.880 --> 01:11:31.160]  комбинацию точек дополнительных. Мы же Prox начинаем вот эту arg-минимум отрешивать с субградиентным
[01:11:31.160 --> 01:11:37.760]  методом в том числе. Вот. Да, это вспомогательная задача. Негладкая вспомогательная задача. Вот.
[01:11:37.760 --> 01:11:41.720]  Которую нужно решить несколько раз. Ну и, соответственно, оценка у вас в итоге будет ровно
[01:11:41.720 --> 01:11:45.640]  такая же, как для субградиентного метода. Никакого там улучшения не будет. Ну да,
[01:11:45.640 --> 01:11:51.600]  с максимальным методом можно отрешивать задачи негладкие. На практике иногда работает даже
[01:11:51.600 --> 01:11:56.360]  лучше. То есть, берете Prox, у вас из-за того, что там есть эта сильная выпуклость, каждая конкретная
[01:11:56.360 --> 01:12:03.360]  задачка у вас решается быстрее. Вот. Но вам ее нужно решить не один раз. Вот. И в итоге там,
[01:12:03.360 --> 01:12:11.880]  в теории, это никакого улучшения не дает. На практике может дать. Окей. Что еще? А все. Все,
[01:12:11.880 --> 01:12:19.000]  у меня презентация закончилась, поэтому ваши вопросы? Так. Все, вопросов нету. Тогда сегодня мы
[01:12:19.000 --> 01:12:24.760]  с вами даже заранее закончили. Супер. Все, спасибо. Увидимся через неделю.

[00:00.000 --> 00:13.320]  Ну ладно, тогда давайте тихонько начинать. Запись там пошла. Звук есть, дёргается. Огонь. Если
[00:13.320 --> 00:20.920]  будут какие-то проблемы, пожалуйста, говорите. Итак, ребят, лучший технический вуз, как вы
[00:20.920 --> 00:26.920]  понимаете. Техника работает как надо. Итак, сначала пара организационных моментов. Спасибо,
[00:26.920 --> 00:32.320]  что большинство из вас записались на курс. Там сейчас 321 анкета. Собственно,
[00:32.320 --> 00:36.280]  касательно семинарский групп. Там, как и предполагалось, половина хочет ходить чисто в
[00:36.280 --> 00:41.200]  лекционную аудиторию на семинары после лекции. Вопросов нет. Это абсолютно рабочий вариант.
[00:41.200 --> 00:46.520]  Семинарский групп, собственно, семинарские занятия предполагаются по вторникам и средам,
[00:46.520 --> 00:53.200]  в 18-30 вечером, очные. То есть туда можно записаться. Мы скинем опросик, чтобы понимать,
[00:53.200 --> 00:56.840]  кто куда, когда пойдет. Уже маленький, просто в чатике. Пожалуйста, отметись.
[00:56.840 --> 01:04.240]  Онлайн занятия, скорее всего, будут в районе пятницы, потому что так сложилось. То есть на них
[01:04.240 --> 01:17.360]  тоже можно прийти. Вопрос. А кто хочет сегодня ходить в другую семинарскую группу? А вы точно
[01:17.360 --> 01:23.400]  на нее ходить будете? Если да, то можно. Просто ранее получалось так, что ставили семинары
[01:23.400 --> 01:28.520]  параллельно с лекционным потоком. Туда приходило два человека. В итоге они просто-напросто
[01:28.520 --> 01:36.080]  переставали работать. Если есть необходимость, можно поставить в понедельник. Прошу. Сюда ходить
[01:36.080 --> 01:40.400]  можно всем по умолчанию. Вопрос в том, кто-то будет в 18-30 по понедельникам ходить не сюда.
[01:40.400 --> 01:48.200]  Тоже и черни. Смотрите, все семинары по умолчанию будут в 18-30, если не оговорено обратно. Потому
[01:48.200 --> 01:52.440]  что все, как правило, преподаватели – это действующие специалисты, которые работают,
[01:52.440 --> 01:58.880]  и они все это делают после работы. Так, здесь какие-то вопросы есть? Окей, смотрите. Тогда еще
[01:58.880 --> 02:05.360]  пара организационных моментов. В репозитории появилась, соответственно, информация про все
[02:05.360 --> 02:09.640]  плюс-минус, что необходимо. Там появилась примерная программа экзамена, там появилась табличка,
[02:09.640 --> 02:15.280]  где будут лежать все записи лекций. Запись первой лекции там доступна. Второй пока обрабатывается,
[02:15.280 --> 02:20.440]  потому что мы нанимаем нового монтажера, но появится на канале буквально сегодня-завтра,
[02:20.440 --> 02:25.400]  я надеюсь. Там же есть список необходимых знаний, скажем так, необытимых фактов,
[02:25.400 --> 02:31.720]  пререквизитов для курса. Короче, списочек из формата прочитать и понять, знаете ли вы из
[02:31.720 --> 02:37.480]  этого списка все или нет. Намек, все, что без звездочки знать должны, наверное, все вот прям совсем,
[02:37.480 --> 02:42.800]  и причем сходу в 3 часа ночи разбуди, чтобы вы это помнили. Поэтому, пожалуйста, проверьте сами себя.
[02:42.800 --> 02:58.960]  Вот. И что еще? Там же появилось первое домашнее задание, его можно решать. Оно максимально простое
[02:58.960 --> 03:05.000]  прямолинейное, там все описано. Сама проверяющая система сейчас подымется, мне надо деньги закинуть
[03:05.000 --> 03:09.560]  на ту виртуалку, в которой она крутится. Чтобы сдать домашку, вам достаточно будет, собственно,
[03:09.560 --> 03:14.960]  загрузить домашку, так как указано в инструкции, в проверяющую систему, она вам скажет, сколько у
[03:14.960 --> 03:20.240]  вас баллов. Один, это сто процентов. Короче, вероятность единицы, то есть вы все решили. Один,
[03:20.240 --> 03:24.800]  значит, полный балл. Ноль, значит, не полный балл, нулевой балл. Ноль пять, значит, половина решили,
[03:24.800 --> 03:28.400]  какие тесты у вас не прошли, вам напишут. Вопросы, пожелания, комментарии есть?
[03:35.000 --> 03:39.720]  Окей, надо открыть, видимо, надо будет написать, видимо, более явную инструкцию. Короче, там,
[03:39.720 --> 03:44.800]  как правило, всегда есть в папке ноутбук, который называется Assignment, вот это ваша домашка. Все
[03:44.800 --> 03:49.280]  остальное, вспомогательные файлы, внутри ноутбука Assignment написано, вся инструкция, что надо делать.
[03:49.280 --> 03:53.800]  Конкретно в данном случае у вас есть ноутбук Assignment, там, по сути, какие-то вопросы, что-то,
[03:53.800 --> 03:59.000]  что вам необходимо сделать. По факту, вам необходимо что? Реализовать класс кнн в
[03:59.600 --> 04:06.800]  а чтобы у вас проверяющая система отработала и дала вам, скажем так, развернутый комментарий,
[04:06.800 --> 04:11.720]  пока что это немного на костылях, но вам нужно весь текст этого файла, когда все работает,
[04:11.720 --> 04:23.760]  запихнуть в template и сдать его. Там инструкция написана. Не спать. И вы и ноутбук. Пока нет
[04:23.760 --> 04:28.360]  дедлайна, да, после дедлайна можете доздавать, но учитываться будет то, что вы получили до дедлайна.
[04:28.360 --> 04:38.120]  Да, ну то есть все, что... А? Да, да, да, да. Там написано, чем нельзя пользоваться, то есть словом,
[04:38.120 --> 04:43.640]  вам говорят написать кнн, использовать кнн за скалерно нельзя, вам надо его написать. Ну и так
[04:43.640 --> 04:50.080]  далее. Ну оно там ограничено каким-то здоровым числом, пока что за всю историю никто его не
[04:50.080 --> 04:54.240]  перебрал. Ну то есть там нельзя сделать, полностью не ограничить, там какой-то констант стоит, то ли
[04:54.720 --> 05:01.200]  тысяча, короче, у вас раза с пятого, я думаю, зайдет. Ну если не идет там, например, четвертая
[05:01.200 --> 05:09.800]  домашняя сложная, она раз с пятнадцатым может зайти. Это нормально, она примерно, это просто то,
[05:09.800 --> 05:21.120]  что сдавали люди весной 21 года, поэтому она примерно. Смотрите, то, что вам написал бот,
[05:21.120 --> 05:25.720]  я прошу прощения, это мой ассистент закинул разболовку раньше, чем мы ее согласовали,
[05:25.720 --> 05:32.080]  на это пока не смотрите, я ее сейчас удалю. То, что там написано, это то, что было год назад. Оно
[05:32.080 --> 05:36.960]  сейчас немножко поменялось, поэтому то, что там написано, это неактуально. Ну что ж, еще вопрос,
[05:36.960 --> 05:44.400]  комментарии есть? Окей, смотрите, я на всякий случай напоминаю вот про этот файлик пререквизитов,
[05:44.400 --> 05:49.960]  я про него еще раз в чате напишу, но по сути это то, что вам хорошо бы знать, это то, что мы с вас
[05:49.960 --> 05:57.560]  спросим где-то в районе 19 сентября, в районе 3 октября мы скорее всего с вас спросим вот эти
[05:57.560 --> 06:02.400]  факты. То есть хорошо бы их вспомнить, запомнить, заботать и так далее. Вы из них большинство
[06:02.400 --> 06:08.720]  знаете, но опыт показывает, что многие эти вещи, что такое финное преобразование, далеко не сразу
[06:08.720 --> 06:18.880]  люди помнят. Да, то есть ну представьте себе, что это какой-то потоковый экзамен, кстати,
[06:18.960 --> 06:23.080]  если будет соответственно переполнение, такое может быть, мы просто еще одну аудиторию возьмем,
[06:23.080 --> 06:31.280]  типа 123 ГК и туда часть выгрузим, ну чтобы не сажать тут людей прям совсем как сосиски. Это будет на
[06:31.280 --> 06:35.080]  семинар, то есть лекция пройдет как обычно, потом кусочек семинара мы обсудим то, что необходимо,
[06:35.080 --> 06:39.440]  там коротенький семинар специально сделан, а потом где-то час будет на вот эту контрольную.
[06:39.440 --> 06:47.600]  Ну соответственно на нее явка плюс-минус обязательно, то есть неявка в формате заболел-уехал,
[06:47.600 --> 06:52.160]  заболел-уехало принимается, но это надо тогда отдельно согласовывать. То есть в принципе у нас
[06:52.160 --> 07:00.720]  посещение прям так по желанию, но на контрольную, как вы понимаете, надо прийти. Не, погодите,
[07:00.720 --> 07:04.160]  вот там это будет конкретно просто в этот день контрольная, грубо говоря. Можете считать,
[07:04.160 --> 07:06.600]  что она фиксированная для всех, это к семинарской группе не имеет отношения.
[07:06.600 --> 07:22.520]  Решим. Просто я изначально предполагал, что в понедельникам нет других семинаров,
[07:22.520 --> 07:27.000]  поэтому сейчас вы меня немного поставили в тупик. Хорошо. Так, окей, еще вопросы есть?
[07:27.000 --> 07:33.160]  Ну ладно, тогда собственно вспоминаем, что у нас было в прошлый раз. Мы с вами чуть-чуть поговорили
[07:33.160 --> 07:38.320]  про линейную алгебру, вспомнили какие-то базовые понятия, что-то починили, что-то разобрали,
[07:38.320 --> 07:42.960]  где-то вы меня даже поправили. Большое спасибо, поправляйте меня абсолютно свободно. Сегодня нам
[07:42.960 --> 07:47.960]  это дело пригодится, мы с вами поговорим про ужас линейную регрессию. Но те, у кого была статистика,
[07:47.960 --> 07:52.280]  прекрасно понимают, что это такое. Те, кто ходили на лабо по общей физу, тоже прекрасно понимают,
[07:52.280 --> 07:56.840]  что это такое. В принципе штука максимально простая, но при этом и очень красивая. Итак,
[07:56.840 --> 08:01.160]  сегодня в меню. Во-первых, мы с вами поговорим о том, что такое линейная модель. В принципе,
[08:01.280 --> 08:07.640]  как они работают, зачем они нужны и почему они используются повсеместно. Во-вторых,
[08:07.640 --> 08:12.280]  мы поговорим о том, какие именно подходы к регрессии с точки зрения линейных моделей есть,
[08:12.280 --> 08:18.600]  и какие есть подходы для получения аналитического решения и, собственно, какого, агрегентного
[08:18.600 --> 08:23.840]  решения. Вот. Потом поговорим про теорему Гаусса Маркова. Классная теорема, одна из немногих
[08:23.840 --> 08:28.840]  теорем машинного обучения, поэтому, пожалуйста, не забивайте на вот те там 3-4 теоремы, которые у
[08:28.840 --> 08:35.080]  нас есть. Теорема Экрата Янга, теорема Гаусса Маркова и так далее. Штука классная. Да,
[08:35.080 --> 08:39.200]  на экзамене это тоже понадобится и, в принципе, это понадобится, чтобы понимать. То есть вот,
[08:39.200 --> 08:43.920]  если я говорю, обратите внимание на теорему, да, ее лучше заботать. Причем заботать не формально
[08:43.920 --> 08:50.000]  о траторике, а понять то, что регулярно люди не понимают чуть такое. Потом поговорим про
[08:50.000 --> 08:53.720]  регуляризацию. По сути, первый раз в таком более-менее явном виде. Сегодня у нас будет
[08:53.720 --> 08:57.920]  регуляризация по-тихому, но на самом деле она много где еще применяется и называется
[08:57.920 --> 09:03.800]  по-разному Way Decay и так далее. И поговорим про то, как понять, что ваша модель нехорошая.
[09:03.800 --> 09:09.200]  Сказала бы по-другому, но мы все-таки в университете. Итак, если вопрос есть,
[09:09.200 --> 09:14.960]  задавайте. Рук поднимаете, задаете. Поехали. На предыдущей лекции, предыдущей, которая не по
[09:14.960 --> 09:22.040]  линалу, были разговоры у нас про что? Про понятие в машинном обучении, про то, что такое наивный
[09:22.040 --> 09:27.280]  байсский классификатор, про теорему байса. Коротенько мы поговорили про КНН на прошлом занятии. Это
[09:27.280 --> 09:30.680]  все, в принципе, работало. На всякий случай эта камера точно меня видит, а то он куда-то
[09:30.680 --> 09:37.720]  спорит. Хорошо, спасибо. Вот. И сегодня мы переходим к линейным моделям. Как вы уже помните,
[09:37.720 --> 09:41.880]  линейная модель это так, кто разводит линейное отображение из пространства параметров,
[09:41.880 --> 09:48.240]  ой, признаков в пространство ответа. Вот то, что мы с вами на прошлые недели разбирали, как линейные,
[09:48.240 --> 09:51.880]  по сути, отображения с почвы матричек, это, по сути, все есть линейная модель. Все,
[09:51.880 --> 09:56.560]  что можно представить в виде линейной комбинации наших признаков, является линейной моделью и
[09:56.560 --> 10:02.400]  так далее. Бывают линейные модели в регрессии, понятное дело, есть точки, провели палку. Бывают
[10:02.400 --> 10:07.120]  в классификации, есть точки, разделили палкой. Бывают линейные модели даже там, где у нас не
[10:07.120 --> 10:13.400]  стоит задача обучения с учителем, например, в задаче снижения размеров. Это unsupervised. Задача,
[10:13.400 --> 10:17.800]  но на самом деле переходим в супервайс режим, то есть наша задача, у нас есть данные в размерности
[10:17.800 --> 10:23.800]  25, мы хотим использовать не 25 признаков, а 3 признака. Как нам понить размеров, потеряв наименьшее
[10:23.800 --> 10:27.400]  количество информации. Понятное дело, полностью мы не сможем сохранить информацию, потому что
[10:27.400 --> 10:33.160]  нас размер с 25 на 3 не отображается, если не выражено. Но, тем не менее. Сегодня мы с вами поговорим
[10:33.160 --> 10:37.920]  про регрессию классификации, это следующая лекция, а снижение размерности это через одну лекцию,
[10:37.920 --> 10:41.880]  там поговорим про PCA. Но, я думаю, про метод главных компонентов тоже из вас многие слышали,
[10:41.880 --> 10:47.000]  правильно? Кто слышал? Понял, не многие слышали. А что такое сингулярное разложение? Может,
[10:47.000 --> 10:52.480]  больше слышали? Хорошо, а что такое разложение по собственным векторам? Может, больше слышали?
[10:52.480 --> 10:58.400]  О, вот. Ну, по сути, почти все одно и то же. То есть, из разложений по собственным векторам можно дойти
[10:58.400 --> 11:04.720]  до PCA, причем достаточно быстро. Ну, что ж. И также стоит сказать про линейной модели, что это
[11:04.720 --> 11:09.760]  крайне важная штуковина по двум причинам. Первая. Наверное, в доброй половине задач,
[11:09.760 --> 11:14.280]  которые решается машинным обучением, вам достачена линейная модель. Вот серьезно. Вам не нужны
[11:14.280 --> 11:19.160]  Uber, Neuron, огромные бустинги и так далее. Вы берете линейную модель, вставляете ее, она быстрая,
[11:19.160 --> 11:25.520]  устойченная, интерпретируемая, легко реализуется, короче, дешевая, сплошные плюсы. И дает хорошее
[11:25.520 --> 11:30.120]  качество. Второе. Без понимания того, как работают линейные модели, понять, как работают все эти
[11:30.120 --> 11:34.400]  новомодные нейронки, ну, в принципе, нельзя. То, что нейронки — это такие линейные модели на
[11:34.400 --> 11:39.320]  стероидах. В них понапихали нелинейные функции активации, придумали какие-то, скажем так,
[11:39.320 --> 11:47.040]  обоснованные некоторыми свойствами наших данных и какими-то мыслями о симметрии в наших данных
[11:47.040 --> 11:51.040]  преобразованиях. Например, свертки — это, по сути, тоже в некотором смысле линейное преобразование,
[11:51.040 --> 11:54.720]  его можно так представить и так далее. Поэтому понимание линейных моделей — это вообще
[11:54.720 --> 11:59.040]  кругольный, наверное, камень понимания двух третей всего машинного обучения. Поэтому,
[11:59.040 --> 12:04.240]  пожалуйста, не пренебрегайте им. И начнем мы с линейной регрессии. Что такое линейная
[12:04.240 --> 12:08.600]  регрессия? Это задача регрессии. В данном случае будем рассматривать одномерную, то есть мы просто
[12:08.600 --> 12:12.880]  напросто предсказываем одну чиселку. Также есть векторная регрессия, линейная регрессия, где мы
[12:12.880 --> 12:17.680]  предсказываем много чиселок. То есть мы отображаем наши признаки в одно число. По сути,
[12:17.680 --> 12:25.080]  у нас есть датсет, пара x и y, x и y — наши объекты, y — целевые переменные, y приходят из R. Если у нас,
[12:25.080 --> 12:30.560]  в общем, в случае это будет векторная регрессия, то из R, как из RK, где RK — это какая-то камерная
[12:30.560 --> 12:36.400]  векторная штуковина. И, соответственно, наша задача — найти некоторую линейную комбинацию наших
[12:36.400 --> 12:41.280]  признаков, найти наши признаки, чтобы получить ответ. Ну или можно записать вот в таком виде
[12:41.280 --> 12:47.120]  ω0, это наш свободный член, плюс взвешенная сумма всех x, xкатый — это именно катый признак,
[12:47.120 --> 12:52.400]  не ката-объект на всякий случай, но ωk, где у нас всего p признаков. Или что то же самое,
[12:52.400 --> 12:59.280]  так как математики, народ, скажем так, который любит писать все коротко и понятно, очень часто
[12:59.280 --> 13:05.320]  переписывают вот в таком формате. xу, скажем так, виртуально добавляют вот нашему, каждому признаку,
[13:05.320 --> 13:10.840]  каждому объекту добавляют пективный признак на первое место, который всегда равен единиц.
[13:10.840 --> 13:16.160]  Ну тогда у нас, соответственно, вектор x — это 1, а потом x1, 2, 3, та-та-та. Зачем это надо? Ну,
[13:16.160 --> 13:20.440]  потому что тогда мы можем с вами записать экспонированная ω, вот наша с вами линейная
[13:20.440 --> 13:24.800]  модель. Не надо никого в плюс b и так далее, все максимально красиво, понятно записано. Хорошо?
[13:24.800 --> 13:29.280]  Но еще раз обращай внимание, это чисто для удобства записи, потому что, когда мы начнем
[13:29.280 --> 13:34.040]  говорить про регуляризацию, вот здесь объявится большая проблема, и об нее можно споткнуться.
[13:34.040 --> 13:40.360]  ω — наш вектор весов, ω0 — свободный член, соответственно, он может его в себя включить,
[13:40.360 --> 13:45.280]  или же мы можем просто добавить со всей матрицы объект-признак, столбец из единичек, и тогда у
[13:45.280 --> 13:50.000]  нас только будет ω-вектор весов, где ω0 — это краски свободных членов. С ним вроде все понятно.
[13:50.000 --> 13:55.960]  И задача оптимизации, которую мы решаем, это минимизация какого-то функционала, например,
[13:55.960 --> 14:01.360]  функции ошибки, например, средней квадратичной, где у нас есть наши y и есть наша оценка y,
[14:01.360 --> 14:05.120]  то есть предсказание нашей модели. Мы можем с вами минимизировать средние квадратичные
[14:05.120 --> 14:08.480]  отклонения, можем минимизировать средние абсолютные, можете придумать там какую-нибудь
[14:08.480 --> 14:13.760]  квантильную функцию, что придумаете, то и будет, и это от вас зависит. На всякий случай. Вот это
[14:13.760 --> 14:19.440]  конкретная постановка задачи МНК методом на меньше квадратов она решается. Тут вопросы есть,
[14:19.440 --> 14:26.720]  все понятно? 3, 2, 1, хорошо. Ну и, соответственно, с этой задачей можно работать максимально
[14:27.200 --> 14:33.760]  почему? Потому что это один из единственных случаев, где в машинном обучении у нас есть
[14:33.760 --> 14:38.320]  аналитическое решение. Не какая-то там попытка аппроксимировать непонятно что, непонятно как
[14:38.320 --> 14:43.600]  градиатными методами, а просто получить аналитическое решение. На всякий случай вот этот вывод тоже
[14:43.600 --> 14:48.080]  хорошо бы уметь делать, потому что, во-первых, хорошо бы уметь матрички дифференцировать, во-вторых,
[14:48.080 --> 14:53.640]  на экзамене тоже спросим. Смотрите, вот наша функция, наш функционал, средней квадратичной
[14:53.640 --> 14:58.360]  ошибки, правильно? Ну или, как вы еще можете назвать, наша функция эмпирического риска может
[14:58.360 --> 15:03.000]  быть записана таким образом. Это квадрат отклонений, правильно? Ну или что то же самое,
[15:03.000 --> 15:12.080]  мы пытаемся посчитать вектор отклонений и посчитать квадрат его второй нормы. Согласны? Тогда можем
[15:12.080 --> 15:17.360]  это переписать в каком виде? Собственно, как мы с вами на прошлой неделе разбирали, y-xω транспонированный
[15:17.360 --> 15:22.680]  на y-xω. Или что же самое, вторая норма вектора отклонений в квадрате. Почему, кстати, в квадрате,
[15:22.680 --> 15:30.040]  как вы думаете, почему бы это еще под квадратный корень не засунуть? Да, так проще считать. А решение
[15:30.040 --> 15:37.200]  задачи оптимизации поменяется, если под корень засунем? Нет, все это понимают. Все помнят, что корень
[15:37.200 --> 15:47.480]  у нас, как бы, какая величина функция точнее, а везде монотонная. Молодцы, на области определения.
[15:47.480 --> 15:51.600]  Потому что, если вы будете корень от отрицательного числа доставать, то бескомплексная плоскость
[15:51.600 --> 15:59.200]  как-то будет печальна. Хорошо. Все, соответственно, вот наш x. На всякий случай здесь сектами абсолютно
[15:59.200 --> 16:04.080]  все равно, или у нас не свободная члена, или мы сюда вот включили наш столбец из единичек,
[16:04.080 --> 16:08.560]  поэтому у нас свободный член сидит в матрице весов. Пока все нормально. Ну и замечательно то,
[16:08.560 --> 16:12.960]  что мы с вами знаем, что в точке у оптима у нас производная равна нулю. Вообще говоря,
[16:12.960 --> 16:18.320]  квадратичная функция потерь парабол у нас выпуклая. Правильно? Минимум там один. Поэтому мы с
[16:18.320 --> 16:24.040]  вами можем найти решение для данного, для данной оптимизационной задачи. Приравниваем нулю, знаем,
[16:24.040 --> 16:30.560]  что у нас там минимум равна нулю, и получается простенький вывод, что омега это x транспонировано x
[16:30.560 --> 16:36.120]  минус 1, x транспонировано y. На всякий случай еще раз проверьте себя вот сейчас визуально,
[16:36.120 --> 16:45.880]  какие размерности должны быть у x и у y. Вот я поэтому и говорю, сейчас, пожалуйста, внимательно
[16:45.880 --> 16:56.200]  проверьте размерности. Давайте вот сейчас все 10 секунд подумать, я пока горло смочу. А потом,
[16:56.200 --> 17:15.200]  соответственно, мы с вами скажем, что да как. Нет, размерность в смысле матрицы. Вот матрица
[17:15.200 --> 17:21.040]  x у нас в какой размерности? Матрица y и матрица омега, соответственно, которую мы проравниваем. Это
[17:21.040 --> 17:25.640]  просто, чтобы вы в уме это все проверили и поняли, что здесь нигде ошибки нет. Потому что, когда вы
[17:25.640 --> 17:29.840]  начнете это руками вводить, у третьей, наверное, появятся проблемы с тем, что краски и ничего не
[17:29.840 --> 17:39.360]  сходится. Да? Ну, смотрите, здесь как раз-таки вот вопрос, если она выраженная. Вопрос. Пока считаем,
[17:39.360 --> 17:46.600]  что x транспонирован на y, не выраженная, все в порядке. Ну, давайте, хорошо. Собственно,
[17:46.600 --> 18:00.800]  матрица x, какая у нее сейчас здесь размерности? Ну, что такое m, что такое n? Объектов? А p,
[18:00.800 --> 18:06.880]  количество признаков. Хорошо, тогда это у нас n на p, тогда x транспонирован на x, размерность какая
[18:06.880 --> 18:14.240]  будет? p на p. Все согласны? Эта матрица вам, кстати, ничего не напоминает? Ну, вопрос к тем, кто статистику
[18:14.240 --> 18:22.800]  изучал. Хорошо, тогда потом про это. Хорошо, предполагаем, что она у нас не выраженная, мы ее можем
[18:22.800 --> 18:30.160]  обратить. Потом опять x транспонирован на y, p на p, на p на n. Правильно? То есть, здесь получается p на n
[18:30.160 --> 18:36.280]  итоговая матрица. А y у нас в какой размерности? y это наша целевые переменные. По сути, это матрица
[18:36.280 --> 18:46.120]  n на 1. Получается, размерность какая остается? p на n на n на 1. Вот наш вектор весов получился.
[18:46.120 --> 18:51.040]  Все правильно, все сошлось, ничего не перепутали. Я почему на это обращаю внимание, когда это начнете
[18:51.040 --> 18:55.240]  в коде писать, а вы начнете это в коде писать, это будет во второй домашке, можно перепутать
[18:55.240 --> 18:58.880]  матрицу, допустим, и транспонировать, тогда размерность матрицы весов окажется какая-то
[18:58.880 --> 19:03.920]  неправильная и так далее. И плюс, собственно, ваш вопрос, а что делать, если у нас свободный
[19:03.920 --> 19:08.640]  член добавился? А нам вообще все равно у нас размерность y зависит от размера обучающей выборки.
[19:08.640 --> 19:15.640]  Размерность x это p на n, n на p точнее, где n это количество объектов, а p количество признаков
[19:15.640 --> 19:21.520]  с фиктивным, без фиктивного. Вы можете туда хоть еще 10 добавить, он все равно склопится. Уловили?
[19:21.520 --> 19:26.960]  Все поняли. И заметьте, мы с вами получили аналитическое решение для задачи линейной
[19:26.960 --> 19:30.760]  регрессии со средней квадратической функцией потерь. Все, вот оно у нас есть, на самом деле,
[19:30.760 --> 19:35.160]  можете его запомнить. Просто полезно его помнить, чтобы резко кому-нибудь ответить на собеседование,
[19:35.160 --> 19:40.000]  вам скажут, ух ты, классно. Вывести тоже можно за 20 секунд, так что выводить тоже полезно.
[19:40.000 --> 19:45.120]  Но, собственно, вопрос на экране. А что делать, если матрица x транспонирована x выраженная? Что
[19:45.120 --> 19:50.200]  такое выраженная матрица? Все помнят, я надеюсь. Правильно? Обратить мы ее не можем, если она прям
[19:50.200 --> 19:54.840]  выраженная, но, на самом деле, даже если она плохо обусловленная, то есть ее детерминат близок к
[19:54.840 --> 19:59.360]  нулю, то у нас с обращением будут большие проблемы. Потому что, если на детерминат близок к нулю,
[19:59.360 --> 20:04.320]  то, когда мы с вами будем пытаться посчитать обратную матрицу, нам придется делить на
[20:04.320 --> 20:10.120]  определитель, это деление на очень маленькое число, это увеличение вычислительной ошибки и так далее,
[20:10.120 --> 20:16.520]  так далее, так далее. С этим все понятно. Хорошо, что делать? А главное, ладно, что делать, я вам скажу,
[20:16.520 --> 20:27.160]  а в каком случае это может возникнуть? Так, линии независимых элементов. Хорошо.
[20:27.160 --> 20:40.960]  Признаков или объектов? Ну вот, признаков или объектов повторяющихся? То есть, у нас, на самом
[20:40.960 --> 20:45.280]  деле, система, как правило, переопределенная, если все хорошо, то есть, у нас больше объектов,
[20:45.280 --> 20:52.560]  чем признаков, то есть, у нас больше ограничений, чем наших свободных переменных. На самом деле,
[20:52.560 --> 20:58.160]  ответ правильный. Если наши признаки линией независимые, то вот в этой матрице появится,
[20:58.160 --> 21:02.640]  опять же, линия независимости X transponable X. Вы посмотрите, как она получается. Когда мы с вами
[21:02.640 --> 21:08.000]  считаем X transponable X, по сути что? Мы берем и, по сути, transponable X, мы говорим, вот этот признак
[21:08.000 --> 21:12.680]  равен вот таким величинам у этих объектов, правильно? И другой признак равен таким-то
[21:12.680 --> 21:17.680]  величинам у этих объектов. Если у нас два признака, ну для простоты, например, одинаковые или они
[21:17.680 --> 21:24.800]  пропорциональны друг к другу, то получается, что для всех объектов признак 1 будет равен 0,4,2,7,5,
[21:24.800 --> 21:30.080]  а признак 2, который с ним линией независим, например, будет ровно всегда в два раза больше.
[21:30.080 --> 21:37.760]  Согласны? Соответственно, мы умножаем эти строки сами на себя, и получается что? Что у нас,
[21:37.760 --> 21:41.480]  как раз таки, два признака линии независимые, у нас получается матрица выраженная, потому что
[21:41.480 --> 21:45.040]  у нее теперь строки, ну или столбцы, потому что она, вообще говоря, квадратная линия независимая,
[21:45.040 --> 21:49.840]  и у нас проблем с этим. Обратить мы ее не можем. Тобственно, когда у нас появляются с вами
[21:49.840 --> 21:55.040]  зависимые признаки, мы не можем найти решения. Но тут, на самом деле, на это можно посмотреть
[21:55.040 --> 21:59.840]  и с точки зрения просто физического смысла. Смотрите, предположим, у нас с вами есть два
[21:59.840 --> 22:06.200]  признака, которые зависимы друг с другом, например, для самого выраженного случая, но просто с ним
[22:06.200 --> 22:11.600]  проще, мне не надо в уме какие-то преобразования делать. Представим себе, что у нас в выборке два
[22:11.600 --> 22:18.040]  признака, вес 1 и вес 2. Это абсолютно один и тот же признак. Окей? И мы, на самом деле, знаем,
[22:18.040 --> 22:23.480]  что вес человека влияет на солевую переменную, допустим, с коэффициентом 5. Вот омега для веса,
[22:23.480 --> 22:29.720]  где она тут, вот здесь, будет равна 5. Но у нас с вами два раза вес присутствует. Значит, соответственно,
[22:29.720 --> 22:35.440]  первый элемент и второй элемент веса суммарно должны делать склад равной 5. Согласны? Но потому
[22:35.440 --> 22:39.280]  что первая половинка делает там вклад какой-то, вторая кое-то, вместе они влияют как 5, потому что
[22:39.280 --> 22:45.400]  вес-то один, просто он почему-то был продублирован. А теперь внимание, проблема. Если я возьму веса,
[22:45.400 --> 22:53.560]  да, веса, параметр значения, омега 1 для веса 1 будет равно 10, а омега 2 для веса 2 будет равно
[22:53.560 --> 23:00.840]  минус 5, в сумме у них что получится? 5, 6 и минус 1, 7 и минус 2. У вас получается континуальное
[23:00.840 --> 23:04.560]  множество решений, у вас континум решений, вы можете любую пару чисел, которые в сумме дают вам
[23:04.560 --> 23:10.680]  5, взять и получить абсолютно тот же самый ответ. Так что то, что мы здесь не можем на самом деле
[23:10.680 --> 23:15.720]  найти аналитическое решение, а у нас оно в любом случае не единственное. Так что это на самом деле
[23:15.720 --> 23:20.960]  нормально и физическая обоснованность тоже имеет. Улавливаете пока? То есть линейные зависимые
[23:20.960 --> 23:26.280]  признаки аналитической решения мы найти не можем, плюс мы не можем найти единственное решение, в принципе,
[23:26.280 --> 23:35.560]  его не существует. Классный вопрос и классное предложение, если признаки линейных зависимых,
[23:35.560 --> 23:39.400]  их можно выкинуть. Если мы об этом знаем, конечно надо выкинуть. Но проблема в том,
[23:39.400 --> 23:43.720]  что я вам привел пример совсем игрушечный, а на практике нам абсолютно неважно, у нас
[23:43.720 --> 23:49.360]  линейная зависимость, два признака равно друг другу или признак номер 328 это линейная комбинация
[23:49.360 --> 23:55.960]  предыдущих 327. Плюс у нас есть шум, плюс признаков может быть там 10 тысяч. Так что найти
[23:55.960 --> 23:59.840]  какие из них линейные зависимые друг с другом может быть большой большой проблемой. Просто
[23:59.840 --> 24:04.120]  дорого и мы просто мы не можем этого явно увидеть. Поэтому выкинуть в общем случае нам
[24:04.120 --> 24:09.640]  плузновато. Окей, с этой проблемой разобрались? Ну что ж, давайте тогда пытаться это чинить.
[24:09.640 --> 24:38.400]  Вы имеете для одного и того же признаков описание два разных ответа? Ну окей.
[24:38.400 --> 24:45.280]  Ну как бывает. Например, есть понятие выброса, где у вас для нормального объекта абсолютно
[24:45.280 --> 24:48.960]  неправильный ответ, например. Или у объекта в принципе неправильные признаки описания.
[24:48.960 --> 24:53.000]  С этим придется работать, так как мы если мы не можем их отфильтровать, значит придется
[24:53.000 --> 24:57.360]  строить такие модели, которые устойчивы к наличию выбросов. Правильно тоже поговорим. То есть в
[24:57.360 --> 25:01.800]  общем случае, смотрите, когда мы решаем с вами оптимизационную задачу, вот методом оптимизации
[25:01.800 --> 25:06.320]  в принципе оптимизируем функционалу абсолютно по барабану, что у нас там с данными. Грязное,
[25:06.320 --> 25:11.480]  нечистое. Мы засунули на вход выборку, по сути мы засунули систему линейных уравнений,
[25:11.480 --> 25:17.520]  ответов по сути ограничения мы дали. Оно нашло ответ. Всё. Так что если у нас плохие данные,
[25:17.520 --> 25:22.360]  это наши с вами проблемы. Мы с этим будем жить. На самом деле принцип, его обычно по-английски
[25:22.360 --> 25:26.200]  называют garbage in, garbage out, он работает почти везде. Если у вас плохие данные,
[25:26.200 --> 25:30.920]  у вас могут быть хоть идеальные модели, у вас будет плохое предсказание. Так что нам придется
[25:30.920 --> 25:35.360]  как-то чистить данные, строить более устойчивые модели и вообще оценивать, насколько плохие у
[25:35.360 --> 25:41.300]  нас данные. Ответил наш вопрос? Супер. Ладно, матрица может быть выраженной. Давайте я вам на примере
[25:41.300 --> 25:47.120]  как раз покажу. Вот собственно ситуация, где у нас два признака или там несколько признаков, короче,
[25:47.120 --> 25:51.640]  зависимые между собой, в простейшем случае два признака линейне зависимы. Матрица x и x соответственно
[25:51.640 --> 25:56.400]  выраженная. Вот у нас есть истинное значение ω true, мы с вами на семинаре сегодня ровно проделаем,
[25:56.400 --> 26:03.360]  это оттуда скриншоты. Допустим вот 268, минус 0.52, минус 1.12. Второй и третий признаки между
[26:03.360 --> 26:07.280]  собой зависимы, то есть это почти один и тот же признак источницы, дам до какого-то маленького-маленького
[26:07.280 --> 26:13.760]  шума. То есть мы все еще можем обратить матрицу x и x, но ошибка у нас большая. Вот аналитическое
[26:13.760 --> 26:18.720]  решение, которое мы получаем буквально формулой за пистом. x и x минус 1 и x и y и так далее. Смотрите,
[26:18.720 --> 26:23.720]  первый член абсолютно правильно определен, с точностью до того, что у нас присутствует небольшой
[26:23.720 --> 26:30.000]  шум данных, то есть 268, два знака после запятой правильно. Второй и третий член, минус 186, 184,
[26:30.000 --> 26:34.720]  ровно то, о чем я вам говорил. Но обратите внимание, сложите второй и третий признаки,
[26:34.720 --> 26:44.960]  ну веса точнее их. Получится примерно минус 0.6, минус 1.6, согласны? Здесь сложите. Получится
[26:44.960 --> 26:49.240]  то же самое. У нас одно ограничение, потому что у нас по сути признак один, но он дважды
[26:49.240 --> 26:53.600]  продублирован. У нас одно ограничение, поэтому на их сумму у нас ограничение есть. На каждой
[26:53.600 --> 26:59.400]  подельности у нас ограничений нет, у нас любая комбинация может этому быть равна. Ну, собственно,
[26:59.400 --> 27:03.960]  возникает вопрос, а что с этим делать? Там уже маленький спойлер был, где-то полсекунды.
[27:03.960 --> 27:09.480]  Что делать, если мы с вами не можем эту матрицу обратить? Что делать с матрицей, если она не
[27:09.480 --> 27:21.320]  обратима? Пошуметь? Классно, зашуметь. Мы с вами зашумели матрицу, но мы с вами при этом потеряли
[27:21.320 --> 27:26.400]  достаточно много информации из-за этого, потому что мы шум добавили прямо явно туда. А чем больше
[27:26.400 --> 27:30.840]  у нас, грубо говоря, чтобы у нас матрица была точно не выраженная, шум должен быть достаточно
[27:30.840 --> 27:34.840]  большой. Потому что если у нас шум будет крайне маленький, то он мало повлияет на определитель,
[27:34.840 --> 27:40.400]  и соответственно у вас матрица все еще будет очень близка к выраженной. Что еще можно сделать?
[27:40.400 --> 27:48.600]  Убрать скоррелированные признаки можно, но если мы их знаем. Если у нас там 10 тысяч признаков
[27:48.600 --> 27:52.760]  и у них зависимости крайне сложные, то сложновато их убрать. Что еще можно сделать?
[27:52.760 --> 28:04.960]  Классный вопрос. А в чем вообще беда? У нас получилось два признака, они в сумме дают что-то
[28:04.960 --> 28:11.160]  там правильное. Они же все равно правильную сумму-то дают. А ответ на самом деле простой. У нас данные на
[28:11.160 --> 28:17.360]  самом деле всегда содержат в себе шум. И, во-первых, у нас вот эти параметры конкретно, значения
[28:17.360 --> 28:22.240]  параметров подобраны под шум в обучающей выборке. И, как правило, очень часто у нас будут получаться
[28:22.240 --> 28:28.440]  значения, которые почти противоположны друг другу. Потому что они как раз подстраиваются под шум,
[28:28.440 --> 28:33.240]  характерное значение шума маленькое относительно признаков, поэтому пытаются настроиться именно
[28:33.240 --> 28:39.000]  на шум, поэтому эти значения большие. Но на новых данных, которых мы не видели на этапе обучения,
[28:39.000 --> 28:43.680]  шум будет другой. Потому что он случайный, он каждый раз случайно генерируется. И у нас может
[28:43.680 --> 28:48.880]  казаться, что вот на этот признак условно прилетело значение шума 0.1, а вот на это не прилетело. И у
[28:48.880 --> 28:55.120]  вас целевая переменная завышена на 186,0 на 10. Потому что мы вот за счет вот этих вот
[28:55.120 --> 29:00.520]  величин начинаем переобучаться под обучающую выборку. Потому что у нас два параметра для того,
[29:00.520 --> 29:05.200]  чтобы запомнить только одно значение. Соответственно у нас по сути один параметр есть, чтобы запомнить
[29:05.200 --> 29:12.800]  что-то специфичное только обучающей выборке. Абсолютное значение не надо уменьшить. Классное
[29:12.800 --> 29:18.240]  замечание. Собственно, мы к нему сейчас придем. Коллеги, смотрите, на это очень полезно смотреть,
[29:18.720 --> 29:22.560]  и я неспроста вот так медленно это все разжевываю. Я хочу, чтобы вы до этого прямо сами додумались,
[29:22.560 --> 29:27.840]  а не просто я вам это сказал. Мы с вами изначально сказали, у нас проблем в чем? У нас с вами решений,
[29:27.840 --> 29:32.720]  множество, вообще континуум. У нас не единственное решение, правильно? Если у нас не единственное
[29:32.720 --> 29:38.800]  решение, вы бы хотели все-таки одно решение как-то получить. Согласны? Решать другую задачу. Классно.
[29:38.800 --> 29:45.840]  Какую? Смещенную. Ну классно, вы понимаете о чем речь. Супер. Давайте тогда наложим какое-то
[29:45.840 --> 29:50.000]  дополнительное ограничение на наш вектор весов. Вот тут было классное предложение. Раз у нас
[29:50.000 --> 29:55.760]  присутствует шум, шум случайный, особенно на тестовых данных, тогда давайте потребуем, чтобы,
[29:55.760 --> 30:01.480]  например, этот вектор был наименьший по какой-нибудь норме. Ну, например, тогда если у вектора маленькая
[30:01.480 --> 30:06.080]  норма, значит у него все члены тоже маленькие. Согласны? Но норма это все равно, если ограничиваем
[30:06.080 --> 30:11.880]  норму, ограничим значение членов. Я с этим согласен. Можно это посмотреть, на самом деле, с другой
[30:11.880 --> 30:16.600]  стороны. Можно посмотреть вот на эту матрицу. На самом деле, как из выраженной матрицы сделать
[30:16.600 --> 30:24.520]  невыраженную. Вот проще, чем зашумить. Мысль более детерминирована. Ну, например, прибавить
[30:24.520 --> 30:28.720]  единичную. Она явно диагональная, тогда у нас получится явно невыраженная матрица. Заметьте,
[30:28.720 --> 30:34.040]  эта матрица квадратная всегда. Согласны? Так что, если мы добавим к ней диагональную матрицу,
[30:34.040 --> 30:39.920]  то мы получим невыраженную матрицу. На самом деле, то, что мы сказали, это плюс-минус одно и то же,
[30:39.920 --> 30:45.320]  и это, по сути, называется регуляризация по-тихнему. Мы можем с вами всегда добавить единичную матрицу,
[30:45.320 --> 30:50.040]  ну, домноженную коэффициент лямбда. И на самом деле, тогда вот эта матричка у нас явно будет
[30:50.040 --> 30:56.680]  невыраженная. Ее всегда можно обратить. Согласны? И это у нас будет решением вот такой задачи
[30:56.680 --> 31:03.760]  оптимизации. Вот этот вывод мы вас сейчас попросим сделать самостоятельно. Это можно вывести на
[31:03.760 --> 31:09.200]  семинаре, но практика, он отличается от этого на две строчки, поэтому я вас прошу сделать
[31:09.200 --> 31:14.960]  самостоятельно. Окей? Можете считать частью домашней. Собственно, вот наш функционал,
[31:14.960 --> 31:20.920]  и теперь мы помимо средней квадратической ошибки минимизируем вторую норму вектора весов в квадрате
[31:20.920 --> 31:30.080]  с коэффициентом лямбда. Вот, что такое лямбда? Лямбда, по сути, это гиперпараметр, который говорит
[31:30.080 --> 31:35.360]  нам, насколько важно для нас, чтобы норма вектора весов была маленькая. То, что мы теперь, по сути,
[31:35.360 --> 31:40.200]  два функционала минимизируем. У нас вот этот функционал хочет лямбду как можно более подходящей,
[31:40.200 --> 31:45.320]  чтобы ошибка на выборке обучающей была минимальной. А этот функционал говорит, что не, слушай,
[31:45.320 --> 31:49.640]  давай-ка Омега у нас будет как можно меньше, потому что иначе штраф большой. По сути, мы пытаемся
[31:49.640 --> 31:54.280]  с вами решить две задачи, которые в разные стороны нас на самом деле тянут. Но здесь на самом деле есть
[31:54.280 --> 32:00.520]  еще один большой плюс. Если у нас ситуация невыраженная и все хорошо, то у нас вот эта задача имеет
[32:00.520 --> 32:05.960]  единственное решение. Все прекрасно. Если у нас решение здесь не единственное, у нас вот эта задача
[32:05.960 --> 32:10.320]  минимизации все равно имеет единственное решение. То есть среди всего континуума значений, которые
[32:10.320 --> 32:14.920]  заставляют одинаковые значения функций потерь, только одна пара даст нам, не одна пара, одно
[32:14.920 --> 32:20.040]  подможество значений весов даст нам минимальное значение функции ошибки. Но почему? Что такое вторая
[32:20.040 --> 32:27.040]  норма квадрата? Это сумма квадрата. Это выпуклый функционал. Согласны? Согласны.
[32:31.280 --> 32:34.720]  Тишина. У пара был один минимум. Точно?
[32:34.720 --> 32:44.720]  Ну короче, мы делаем задачу выпуклой, грубо говоря, за счет этой регуляризации. И собственно отсюда и
[32:44.720 --> 32:49.880]  появляется вообще у нас в курсе термин регуляризация. На самом деле регуляризацию называют, опять же,
[32:49.880 --> 32:56.120]  от английского языка regularize, ограничивать. Мы по сути дополняем нашу задачу некоторыми ограничениями,
[32:56.120 --> 33:00.600]  потому что мы не можем решить исходную задачу единственным образом или таким образом, чтобы это
[33:00.600 --> 33:06.440]  удовлетворяло нашим интересам. Мы не знаем, какое именно решение выбрать, поэтому мы дополнительное
[33:06.440 --> 33:11.040]  ограничение кладем. Например, накладываем, чтобы у нас норма вектора весов была минимальна, тогда
[33:11.040 --> 33:15.120]  решение единственное, плюс мы уже знаем, какие свойства ожидать от нормы нашего вектора весов.
[33:15.120 --> 33:24.920]  И от вектора весов соответственно. Смотрите, почему такая функция потерь? Ровно потому, что у нас
[33:24.920 --> 33:32.560]  просто по сути добавляется вторая норма вектора весов. Что второй нормой вторая норма вектора весов,
[33:32.560 --> 33:37.320]  а лямбда это просто коэффициент, в котором мы ее вкладываем. По сути это коэффициент регуляризации.
[33:37.320 --> 33:41.400]  Чем больше лямбда? Ну, предположим, мы лямбду стремили в бесконечность. Там где-то вот она очень
[33:41.400 --> 33:49.400]  большое число. Тогда для решения задачи оптимизации, какое решение будет оптимальным? Ну, примерно да,
[33:49.400 --> 33:54.280]  нулевые веса, потому что у нас клад вот этого члена будет много больше, чем вклад вот этого члена.
[33:54.280 --> 34:01.400]  Если лямбда близка к нулю, то какое решение оптимально? То, которое позволяет переобучиться под
[34:01.400 --> 34:06.160]  этот функционал. Лямбда это просто наш, грубо говоря, регулятор, насколько важно нам получить
[34:06.160 --> 34:17.280]  невыраженное решение. Это схема работает всегда, потому что если вы к невыраженной матрице добавить
[34:17.280 --> 34:27.600]  диагонально, она выраженной не станет. О, кстати, кстати, кстати, кстати, кстати, вот тут квадрат не хватает.
[34:27.600 --> 34:33.360]  Ну, в смысле, лямбда должна быть положительная. В такой формулировке, вот, прошу прощения,
[34:33.360 --> 34:38.040]  с опечатком. Ну, тут или ограничение, что лямбда больше или равна нулю, или соответственно здесь
[34:38.040 --> 34:42.000]  надо везде квадрат писать, окей, чтобы вас не запутать.
[34:42.000 --> 34:54.080]  Не, смотрите, мы реальное значение с вами уже никогда не узнаем почему,
[34:54.080 --> 35:00.000]  потому что если у нас с вами два признака, грубо говоря, дублируют друг друга, то вот у вас 2,
[35:00.000 --> 35:06.960]  вес 1, вес 2. Вы только знаете, что вклад вес 1 и вес 2 вместе равен 1. Нет никакого
[35:06.960 --> 35:13.080]  правильного значения признака веса для веса 1 и вес 2. Поэтому мы с вами собственно и разрешаем
[35:13.080 --> 35:16.800]  эту неопределенность, говоря, окей, выберем тот, который доставляет на меньшую норму векторовесов.
[35:16.800 --> 35:30.120]  Если лямбда больше или равна нуля, матрица не станет выраженной. Почему?
[35:30.120 --> 35:37.960]  Смотрите, что у нас здесь стоит? X транспонированное x. На диагонали что стоит?
[35:37.960 --> 35:45.800]  Что? Это что вообще? X транспонированное x, что задается в точке верения там аналита или налог?
[35:45.800 --> 35:56.200]  Вам слово квадратичная форма что-нибудь говорит? Ну, смотрите, у нас собственно здесь на диагонали
[35:56.200 --> 36:00.880]  будут стоять квадраты х. Мы к ним добавляем строг не отрицательную диагональ все остальной нули.
[36:00.880 --> 36:18.000]  Невыраженная она будет. Так, хорошо. Собственно, еще раз повторяю. Какую? Смотрите, тут можно зайти
[36:18.000 --> 36:21.680]  с двух сторон, собственно они на самом деле эквивалентны. Мы можем сказать, эта матрица
[36:21.680 --> 36:26.760]  выраженная, сделаем ее невыраженной как? Добавим к ней диагональную матрицу. Ну, например, лямбда,
[36:26.760 --> 36:31.520]  опять же, здесь лямбда не отрицательная, здесь тогда квадрат по-хорошему надо добавить, это опечатка.
[36:31.520 --> 36:37.920]  Тогда матрица невыраженная. Но это эквивалентное решение вот такой задачи. На самом деле я не уверен,
[36:37.920 --> 36:42.840]  как было изначально, то есть можно просто добавить ограничение на нашу норму векторовесов и получится,
[36:42.840 --> 36:47.880]  что есть аналитическое решение, которое ровно такой формат имеет. То есть с двух сторон приходим к
[36:47.880 --> 36:53.960]  одному и тому же. Вот. Ладно. И почему это называется еще раз регуляризация? Ограничение,
[36:53.960 --> 36:59.280]  которое мы наложили. На всякий случай, вот эта вот норма вторая вектора версов, это далеко не единственный
[36:59.280 --> 37:04.560]  способ регуляризации, мы с вами их десятки увидим. Можно ограничивать количество признаков модели,
[37:04.560 --> 37:08.840]  можно ограничивать ее структуру в нейронках, глубину дерева в деревьях, другую норму взять,
[37:08.840 --> 37:14.080]  что угодно. Любое ваше априорное предположение, которое вы накладываете на решение задачи, это
[37:14.080 --> 37:19.560]  ваше ограничение, ваше регуляризация. И с регуляризацией, как и со всеми предположениями,
[37:19.560 --> 37:23.920]  работает какое правило? Если у вас хорошее предположение, которое подходит под решение
[37:23.920 --> 37:29.820]  для задачи, вы получаете хороший результат. Если ваши решения противоречат задачи, то от них
[37:29.820 --> 37:39.080]  станет хуже. Поэтому нет такого правила, что с регуляризацией всегда лучше. Окей? Да. Лямду
[37:39.080 --> 37:44.640]  выбирать из соображений того, что у вас есть... Это гиперпараметры, гиперпараметры, как правило,
[37:44.640 --> 37:49.120]  подбираются на кросс-валидации. Про кросс-валидацию мы в конце лекции как раз поговорим. То есть гиперпараметры
[37:49.120 --> 37:54.120]  мы не можем автоматически подобрать, мы их выбираем, например, по качеству поведения нашей модели на
[37:54.120 --> 38:01.640]  валидирующей выборке, то есть на отложенной выборке. Хорошо, еще вопросы тут есть? Окей,
[38:01.640 --> 38:07.880]  я тогда сразу маленький спойлер делаю. Я думаю, многим из вас очевидно, что вторая норма, это далеко
[38:07.880 --> 38:13.480]  не единственная норма, которая здесь может стоять. Согласны? С второй нормой есть только одно
[38:13.480 --> 38:19.960]  важное замечание. Для второй нормы у нас есть аналитическое решение, что с регуляризацией,
[38:19.960 --> 38:24.360]  что без регуляризации. Это, наверное, единственный случай, где в линейной регрессии есть аналитическое
[38:24.360 --> 38:29.280]  решение. Вы можете сюда первую норму подставить, решаться это будет, но только градиентным методами
[38:29.280 --> 38:34.960]  аналитического решения уже не будет. И вторая важная вещь про вторую норму. Теориям Гаусс Маркова.
[38:34.960 --> 38:40.960]  Теория Гаусс Маркова дает нам очень важные гарантии, на самом деле, и это та причина,
[38:40.960 --> 38:45.880]  почему среднюю квадратичную ошибку вообще любят, в том числе. Вот у нас есть наша целевая
[38:45.880 --> 38:49.840]  переменная. Заметьте, это уже не наша оценка, это вот наше предположение, что есть целевая
[38:49.840 --> 38:55.040]  переменная, которая зависит от икса линейным образом, но в наличии также какой-то шум. Идет?
[38:55.040 --> 39:04.360]  Все согласны. Так, на всякий случай, а кто вообще знает теорию Гаусс Маркова? Никто. Классно. Не,
[39:04.480 --> 39:11.120]  ладно, бывает. Поэтому мы ее разбираем. Еще раз, ε это у нас случайно величины, это наш шум.
[39:11.120 --> 39:17.440]  В данных присутствует шум. Всем окей это, правильно? Наше предположение, во-первых, эти у нас
[39:17.440 --> 39:23.000]  величины независимы, на самом деле, в идеальном случае, когда мы хотим. То есть в идеале наш
[39:23.000 --> 39:27.080]  шум должен быть независимый. Почему? Потому что если наш шум зависимый, то, в смысле, у нас
[39:27.080 --> 39:32.560]  переменная как-то там зависит от иксов, например, то что-то это на шум не похоже, на самом деле. Но
[39:32.560 --> 39:36.560]  теория Гаусс Маркова делает на самом деле три очень маленьких и простых предположения. Во-
[39:36.560 --> 39:41.400]  первых, у нас шум не смещенный, то есть мат ожидания каждого ε равно нулю, то есть мы можем как
[39:41.400 --> 39:48.440]  завысить наше предсказание, так и занизить. Хорошо? Целевой пример. Во-вторых, у нас есть какая-то
[39:48.440 --> 39:53.040]  дисперсия, которая конечна, то есть у нас нету случайных величин здесь, которые имеют бесконечную
[39:53.040 --> 39:58.800]  дисперсию. Мы не можем получить, как бы, с какой-то значимой вероятностью там отклонения в десятки
[39:58.800 --> 40:08.280]  миллионов. Да? Да, конечно. Хорошо. И третье, собственно, у нас для всех и не равно 0, ковриация между
[40:08.280 --> 40:20.600]  ними равна нулю. Шутка ковриации все помнят? Хорошо. Вар дисперсия варианс. Ну можно еще написать вот
[40:20.600 --> 40:29.080]  такую д, но лотех в ней не умеет. Поэтому написано вот так. Вот, три условия. Все условия понятны, правильно?
[40:29.080 --> 40:37.160]  Еще раз, у нас все ошибки не смещенные относительно нуля, у нас есть конечная дисперсия и ковриация
[40:37.160 --> 40:48.400]  наших ошибок равна нулю. Да, в данном случае давайте будем считать, что просто фиксированная дисперсия,
[40:48.400 --> 40:53.240]  окей? Которая конечная, главное. На самом деле можно пойти на какие-то обобщения, чтобы даже
[40:53.240 --> 40:58.200]  были разные дисперсии, лишь бы они были все конечные, но пока вот просто равно сигн. Хорошо.
[40:58.200 --> 41:03.120]  И, собственно, в этих предположениях вот это решение, которое мы с вами только что видели,
[41:03.120 --> 41:09.160]  омега транспанированная х, у х транспанированной х-1, у х транспанированной у, то есть решение
[41:09.160 --> 41:15.400]  задачи на меньше квадратов является оптимальным среди несмещенных. Говоря по-русски или по-английски,
[41:15.400 --> 41:22.920]  best linear unbiased estimator. Наилучшей среди несмещенных. Вопрос, собственно, такой. Во-первых,
[41:22.920 --> 41:37.280]  что значит несмещенных? Без лямбда. Хорошо, а на практике что значит? В мат ожидании кого?
[41:37.280 --> 41:45.000]  Бинго. Наша оценка является несмещенной, у нас же омега с крышкой, что будет? Она же у нас зависит
[41:45.000 --> 41:49.720]  от каких-то случайных величин, правильно? Вот у нас тэпсилоны сидят, так что это, по сути,
[41:49.720 --> 41:56.280]  тоже случайная величина. Мат ожидания омеги с крышкой равно омеги. Это значит, что наша
[41:56.280 --> 42:02.960]  оценка несмещенная. Вот, это решение заставляет нам несмещенную оценку. Это раз. Просто обращаю
[42:02.960 --> 42:07.840]  ваше внимание, потому что иногда начинают говорить, что у нас несмещенная оценка у. Тоже вариант,
[42:07.840 --> 42:12.520]  но в данном случае теорема гласит именно обоим. На что значит все это наилучшее?
[42:12.520 --> 42:29.240]  Ладно, что такое линейное, понятно? А что про эффективную говорили? Окей, что такое эффективная
[42:29.240 --> 42:42.080]  оценка? Да, на самом деле. Можете еще рассказать? Опять же, наименьшую дисперсию имеет кто? Бинго.
[42:42.080 --> 42:46.840]  Смотрите, наилучшая или оптимальна в средине смещенная. Омега с крышкой, во-первых, несмещенная,
[42:46.840 --> 42:53.120]  то есть мат ожидания омеги, равно омега с крышкой. И у нее наименьшая дисперсия, опять же, у омеги с
[42:53.120 --> 42:58.600]  крышкой. Не у какой-то там ошибки, у чего-то. Окей? Собственно, у нас есть гарантия. Эта теорема,
[42:58.600 --> 43:02.440]  на самом деле, оказывается в три строчки. Можем доказать или на семинаре, или сделаю самостоятельно.
[43:02.440 --> 43:08.840]  Опять же, на лекцию я стараюсь доказательств теорем не вытаскивать. Это решение у нас является
[43:08.840 --> 43:19.400]  оптимальным в средине смещенных. Окей? Не, как раз-таки мы-то на практике с вами не знаем реальное
[43:19.400 --> 43:25.600]  значение, но мы с вами знаем выборку. У нас есть выборка. И мы можем получить оценку наших параметров
[43:25.600 --> 43:30.920]  на основании выборки. Вот, с такими предположениями, тремя, оценка, которую мы получаем вот отсюда,
[43:30.920 --> 43:41.200]  будет оптимальна средина смещения. Вот как раз-таки это очень хороший вопрос. Здесь мы предполагаем,
[43:41.200 --> 43:45.500]  конечно, что она обратима. Если она не обратима, то мы это просто посчитать не можем. И вторая
[43:45.500 --> 43:50.360]  проблема, если она, скажем так, обратима, но почти выражена, тут возникает другая проблема,
[43:50.360 --> 43:55.360]  что у нас вычислительная точность наших машин конечна. Поэтому чем ближе она к сингулярной,
[43:55.360 --> 43:59.720]  тем больше у нас здесь будет вычислительная ошибка. То есть это, конечно, работа, когда мы можем
[43:59.720 --> 44:03.600]  посчитать омегу. Если у нас матрица выраженная, то у нас вообще не существует такой оценки,
[44:03.600 --> 44:10.760]  потому что она не единственная. Окей? Хорошо. Ну и теперь, собственно, пара замечаний. Смотрите,
[44:10.760 --> 44:17.800]  возвращаемся назад. Вот у нас с вами минимизация среднего третичной ошибки. Задача линейной
[44:17.800 --> 44:24.000]  регрессии. Теория Магауса Маркова работает? Предположение о тех свойствах шумов наших данных.
[44:24.000 --> 44:30.560]  Ещё раз. Минимизируем вот эту функцию ошибки. Теория Магауса Маркова работает,
[44:30.560 --> 44:39.360]  если эти три предположения выполняются? Да, классно. Минимизируем вот такую, вот
[44:39.360 --> 44:55.000]  такой функциональный амперический риск. Теория Магауса Маркова работает? Почему? Ага, классно.
[44:55.000 --> 45:05.760]  А решение у нас какое? Бинго. Ребят, ещё раз. Теория Магауса Маркова гласит, что вот это решение
[45:05.760 --> 45:10.240]  является оптимальным решением для задачи на меньше квадратов. То есть для минимизации среднего
[45:10.240 --> 45:16.800]  третичной функции ошибки только. Всё. Если вы добавляете вот сюда любую другую функцию,
[45:16.800 --> 45:20.680]  любой другой функционал, если у вас не прост среднего третичной ошибка, у вас уже посылка
[45:20.680 --> 45:25.160]  теоремы неправильно. Вы не задачи на меньше квадратов рассматриваете. Поэтому теорема Магауса
[45:25.160 --> 45:29.360]  Маркова работает тогда только, когда у вас минимизируется среднего третичной функции ошибки.
[45:29.360 --> 45:34.160]  В данном случае у вас уже появился второй член, и здесь на самом деле можно увидеть ещё одну вещь.
[45:34.160 --> 45:40.040]  У вас оценка будет априори смещенной. Почему? Потому что для минимизации среднего третичной
[45:40.040 --> 45:46.960]  функции ошибки вы явно не хотите минимизировать норму вектора весов. У нас добавление регуляризации
[45:46.960 --> 45:51.080]  как правило даёт нам смещенное решение, потому что теперь относительно изначального оптимума
[45:51.080 --> 45:56.240]  нашего функционала мы ищем другой оптимум, который обладает заданными нами свойствами.
[45:56.240 --> 46:07.960]  Нет. А как? Мы с вами сделали по сути что? У нас была изначальная оптимизационная задача. Мы
[46:07.960 --> 46:12.920]  сказали, что мы не можем её решить либо технически, либо у неё не единственное решение. Мы придумали
[46:12.920 --> 46:17.840]  по сути как поставить новую оптимизационную задачу. Вот она теперь, вот её мы решаем. И
[46:17.840 --> 46:24.160]  собственно для этой оптимизационной задачи найти решение. Вот её мы решили. Мы не решали другую
[46:24.160 --> 46:28.960]  оптимизационную задачу. Мы не знаем какое у неё решение. У нас нет обратного хода. Мы решили ту
[46:28.960 --> 46:46.400]  задачу, которую мы поставили. Да? Да. Там В с крышкой был набор случайных величин в кеории, так как у вас
[46:46.400 --> 46:50.960]  Х то в общем случае это всегда реализация, как и Y реализация случайной величины. Вы получили
[46:50.960 --> 46:59.840]  точно так же оценку. Это набор чисел. Конечно. Смотрите, вы собственно и получаете на самом деле,
[46:59.840 --> 47:05.040]  собственно когда вы средний квадрат ошибки минимизируете, вы считаете в краске мат ожидания
[47:05.040 --> 47:26.480]  вот этой величины. Всё правильно. Вот. Добавляя по одному признаку. Да, смотрите, вы предлагаете
[47:26.480 --> 47:36.160]  замечательный комментарий. Собственно, коллега ваш предложил отфильтровать признаки, то есть отобрать
[47:36.160 --> 47:40.840]  только те, которые образуют некоторые линии независимые под выборку. Да, так можно делать.
[47:40.840 --> 47:45.920]  Такие подходы тоже есть отбора признаков, итеративные. Минус в том, что когда у нас опять же очень много
[47:45.920 --> 47:50.360]  признаков, например там десятки тысяч, это очень дорого вычислительно. Вам для каждого надо всё
[47:50.360 --> 47:54.760]  равно тогда решить оптимизационную задачу. Причем для каждого под множество выбрать оптимальный,
[47:54.760 --> 48:01.080]  повторить, это просто дорого. Вот. Собственно, а с регуляризацией плюс в чём? Да, мы получаем
[48:01.080 --> 48:04.960]  смещённое решение. Но, во-первых, у нас от лямбда зависит, собственно, насколько сильно оно будет
[48:04.960 --> 48:10.040]  смещённым. Чем меньше лямбда, тем меньше у нас смещение. Во-вторых, решение вы всё равно получаете,
[48:10.040 --> 48:14.520]  которое обладает заданными вами свойствами. Даже если там есть краски зависимые признаки,
[48:14.520 --> 48:19.960]  вы точно знаете, что их суммарный вес будет ближе к оптимальному, а каждый вес подельности будет
[48:19.960 --> 48:26.240]  вообще минимально из возможных. То, что начав в квадрате, он сразу будет выше. Вот и всё. Так,
[48:26.240 --> 48:38.720]  понятно? Ещё раз, что такое оптимальная средина смещённых, понятно? Обладает наименьшей дисперсией.
[48:38.720 --> 48:44.320]  То есть любую другую оценку вашей матрицы весов вы можете сделать, но дисперсия вот этой
[48:44.320 --> 48:48.840]  случайной величины это случайный вектор, потому что он зависит от случайных величин. У неё дисперсия
[48:48.840 --> 49:01.720]  будет выше. Настолько, насколько подходит линейный модель для вашей задачи. То есть если вы
[49:01.720 --> 49:07.160]  пытаетесь, условно, параболу линейной моделью описать, то у вас в принципе не очень подходящий
[49:07.160 --> 49:12.600]  класс моделей используется для решения задач. Ещё раз, это теоретический результат. То есть это
[49:12.600 --> 49:18.160]  обосновывает нам, что в хорошем признаковом описании у нас решение задач наименьших квадратов
[49:18.160 --> 49:24.880]  обладает хорошими свойствами, что это оптимальная оценка средней смещённых. Всё. Нет, смотрите, вот,
[49:24.880 --> 49:29.840]  речь только про линейные модели. Всё, вот изначальная постановка. Если у нас зависимость не линейной
[49:29.840 --> 49:34.040]  модели, не линейной, никаких опять же договорённостей у нас нет, у нас условия теремы не выполняются.
[49:34.040 --> 49:47.360]  Вот. Хорошо. Да. И да, и нет. Смотрите, ещё раз. А теперь, собственно, когда брать какую норму.
[49:47.360 --> 49:52.280]  Давайте сейчас покажу, что бывает, например, первая норма, что вы понимаете и сами, но какие у неё
[49:52.280 --> 49:57.520]  есть свойства. Но, во-вторых, норм можно брать раз. Собственно, здесь мы подходим с вами опять к
[49:57.520 --> 50:06.640]  очень важным факту. Так, вам видно там справа? Я не сгораживаю. Смотрите, если модель смещённая,
[50:06.640 --> 50:12.240]  значит она решает не ту задачу, которая у вас, вот, тогда она решает, например, не минимизацию,
[50:12.240 --> 50:19.080]  как бы, той функции ошибки. Короче, если у вас модель смещённая, значит она не минимизирует
[50:19.080 --> 50:26.000]  чистой функции ошибки. Что такое? Давайте я ещё пару тогда этих терминов веду. Вот есть функция
[50:26.000 --> 50:30.240]  ошибки. Её, на самом деле, очень часто путают в интернете друг с другом. Давайте называть так.
[50:30.240 --> 50:35.280]  Функция ошибки — это именно вот ошибка предсказательной нашей модели. То есть, насколько мы ошибаемся,
[50:35.280 --> 50:41.280]  предсказывая целевую величину, вот будет эта функция ошибки. Хорошо? Вот всё вместе можно назвать там,
[50:41.280 --> 50:46.880]  например, функционал имперического риска. Потому что это ошибка плюс какая-то регулиризация. Это
[50:46.880 --> 50:51.880]  ровно тот функционал, который мы с вами минимизируем. Не смещённая у нас оценка тогда, когда минимизируем
[50:51.880 --> 50:56.520]  чистую ошибку. Если мы минимизируем что-то там ещё, то к правилам получаем смещённую, потому что на
[50:56.520 --> 51:01.640]  этом можно на самом деле посмотреть абсолютно банально со второй стороны. Смотрите. Вот у вас раз функционал,
[51:01.640 --> 51:06.440]  два функционал, да? Те же помнят, что градиент можно посчитать. И мы на самом деле сейчас про это
[51:06.440 --> 51:10.840]  говорим, можем градиентными методами найти оптимальное значение матрицы Омега, ну или вектор Омега
[51:10.840 --> 51:18.200]  в данном случае. У вас градиент, производная сумма чему равна? В сумме производных. Поэтому у вас
[51:18.200 --> 51:23.320]  для Омеги будет отсюда идти градиент, который тянет её в сторону оптимальной оценки краски. И отсюда
[51:23.320 --> 51:27.600]  будет идти градиент, который тянет её в ноль. Вот откуда у вас появляется смещение. У вас оценка
[51:27.600 --> 51:31.800]  Омеги по сути съезжает в сторону вот этого градиента. Это совсем на пальцах, если.
[51:31.800 --> 51:42.440]  Ну так это и есть то же самое. У вас то, что Омега с крышкой теперь тянется в сторону нуля,
[51:42.440 --> 51:47.720]  это смещает её относительно мат ожидания. Потому что иначе бы она к краске была в мат ожиданиях,
[51:47.720 --> 51:57.600]  которая вот эту штуку минимизирует. Окей, так на всякий случай это тогда тоже можно проделать
[51:57.600 --> 52:04.400]  дополнительно. Так коллеги, давайте так. Вот что такое оценка максимального правдоподобия? Кто
[52:04.400 --> 52:10.080]  помнит вообще знает? Понял, тогда про это можно прямо отдельно допсеминар провести собственно.
[52:10.080 --> 52:18.120]  О чем речь? Мы с вами на самом деле, ну вот как раз, когда обсудите, мы здесь на семинаре тоже
[52:18.120 --> 52:21.680]  можем обсудить. По сути, когда мы с вами минимизируем среднюю квадратичную ошибку,
[52:21.680 --> 52:27.840]  мы по сути ищем оценку максимального правдоподобия для средней квадратичной ошибки.
[52:27.840 --> 52:32.200]  Когда минимизируем среднюю квадратичную ошибку, мы ищем оценку максимального правдоподобия
[52:32.200 --> 52:38.720]  в предположении, что у нас краски и ошибки нормально распределены. Вот и все. Эта краска будет средняя.
[52:40.080 --> 52:46.480]  Нет, это на статистике прямо отдельно выводится где-то полсеминара. Я поэтому и говорю, кто знает,
[52:46.480 --> 52:51.120]  мы это можем на допсеминаре провести, потому что сейчас у меня записи нет, я это буду изымал
[52:51.120 --> 52:58.840]  вводить минут 20, наверное. Извините. Да? А, это да, сейчас я его потрогаю. Вот, хорошо. Собственно,
[52:58.840 --> 53:03.400]  нормы у нас бывают разные. И возвращаясь к вопросу, а правильно ли, чтобы всегда брать вторую норму
[53:03.400 --> 53:09.560]  и не выпендриваться? Вообще говоря, нет. Потому что выбор того функционала, который вы будете
[53:09.560 --> 53:13.640]  минимизировать, опять же, зависит от вас. Это, на самом деле, одна из самых важных частей в любой
[53:13.640 --> 53:18.240]  задаче машинного обучения. Не уметь там строить классные нейронки, деревья и так далее, ансамбли.
[53:18.240 --> 53:23.480]  Умение правильно из неформальной задачи, ну, например, вон там научрук говорит, что надо сделать
[53:23.480 --> 53:28.040]  то-то. Или на работе вам говорит, не знаю, там менеджер, надо сделать что-то другое. Короче, как это
[53:28.040 --> 53:32.760]  обычно говорят, из бизнес задачи, ну или из научной задачи, сформулировать уже математическую
[53:32.760 --> 53:36.960]  оптимационную задачу. Выбор функционала, который вы будете оптимизировать, это вообще краеугольный
[53:36.960 --> 53:41.480]  камень. Потому что неправильно выбранный функционал, неправильно поставленная задача,
[53:41.480 --> 53:45.880]  будет по-любому давать неправильное решение. Все, конец. Если вы неправильную задачу поставили,
[53:45.880 --> 53:49.600]  вы не то получили. Собственно, поэтому функционал, который вы минимизируете,
[53:49.600 --> 53:53.680]  должен исходить именно из того, что вы хотите получить. Вообще, я пока просто декларативно
[53:53.680 --> 53:58.920]  скажу. Минимизация квадратичной ошибки дает нам среднее, минимизация абсолютной ошибки,
[53:58.920 --> 54:04.000]  то есть, например, первой нормы, будет давать нам медиану. Как-то доказать краски можем с вами
[54:04.000 --> 54:08.640]  сделать сегодня, например, после там семинара в конце. И, собственно, нормы бывают разные. На
[54:08.640 --> 54:13.240]  практике, как правило, в дозаче регрессии применяют в основном две. Ну ладно, три. Первая — это
[54:13.240 --> 54:18.000]  квадратичная ошибка, это, наверное, 70% всех случаев. Вторая — это средняя абсолютная ошибка,
[54:18.000 --> 54:25.320]  или же их называют МСК и МАЕ, на том, что mean squared error, mean absolute error. МАЕ — это сумма
[54:25.320 --> 54:30.120]  модулей отклонений, и, соответственно, для нее мы точно также можем все посчитать, но для нее нет
[54:30.120 --> 54:35.480]  аналитического решения. Градиентная, хотя все еще присутствует. И, собственно, третья функция
[54:35.480 --> 54:39.720]  ошибки, которую часто еще используют, это так называемая квантильная функция ошибки. Короче,
[54:39.720 --> 54:44.920]  там у вас в зависимости от того, какой у вас квантиль важен, например, вам сильно важнее там
[54:44.920 --> 54:49.120]  не завышать, чем не занижать, у вас может функция ошибки иметь какой-нибудь вид, типа вот такая
[54:49.120 --> 54:55.720]  галочка, здесь она плавно, растется здесь быстро. Но все понимают, что у нас для модуля график, как
[54:56.720 --> 55:02.200]  отклонения. Причем с углом 90 градусов. У вас угол, на самом деле, может быть другой. Такой
[55:02.200 --> 55:06.160]  функция ошибки тоже можно подобрать, она иногда тоже подходит. Причем этот угол даже является
[55:06.160 --> 55:12.840]  гиперпараметром, он там можно подбирать и так далее. Вот. Все. Вот у нас есть раз-два именно для
[55:12.840 --> 55:17.960]  функций ошибок. Но также у нас есть регуляризация, которую точно также можно использовать со второй
[55:17.960 --> 55:22.920]  нормой, с первой, с четвертой, если вам вдруг захотелось. На практике обычно используют. И так
[55:22.920 --> 55:28.920]  далее. И на всякий случай, только вот этот случай МСЕ без регуляризации работает с теориями
[55:28.920 --> 55:32.680]  Гаусса-Маркова. То, что он в условии теориями Гаусса-Маркова стоит. Мы решаем задачи на
[55:32.680 --> 55:38.880]  меньше квадрат. Все. Линейная модель. Все остальное, как бы, это что-то там другое. Так что, если вы
[55:38.880 --> 55:44.000]  добавили регуляризацию, вы добавили свои ограничения на задачу, но вы ушли от изначальной задачи
[55:44.000 --> 55:48.680]  найти оптимальную оценку, чтобы минимизировать ошибку. Вы что-то другое уже решаете. Поэтому оценку
[55:48.680 --> 55:52.800]  у вас априори совмещенная, вы другую задачу решаете. За это вы получаете какие-то хорошие
[55:52.800 --> 55:58.240]  свойства. Например, она у вас более устойчивая. Устойчивые краски под устойчивостью будем
[55:58.240 --> 56:02.360]  понимать следующее. Это такое, так сказать, определение. Оценку будем называть устойчивой,
[56:02.360 --> 56:08.480]  если малое изменение обучающей выбраки приводит к малому изменению вектора параметров. То есть,
[56:08.480 --> 56:13.360]  если условно мы на Эпсилон, грубо говоря, изменили выбраку, ну там шум чуть-чуть поменяли, у нас
[56:13.360 --> 56:18.320]  матрица весов тоже поменялась не больше, чем на Эпсилон, например. Потому что, если у нас,
[56:18.320 --> 56:24.040]  например, наша ситуация выраженная, наша матрица выраженная регуляризации нет, вы можете чуть-чуть
[56:24.040 --> 56:28.720]  шум поменять, мы это с вами на семинаре опять же сделаем, здесь может оказаться 50-50, например.
[56:28.720 --> 56:36.760]  Видите, шум там будет меняться на 10-1, 10-2, 10-2, а здесь будет меняться на 10-2. Вот, значит,
[56:36.760 --> 56:45.320]  оценка неустойчивая. Поняли? Согласились? Ловили? Вопросы? Комментарии? Ну, собственно,
[56:45.320 --> 56:51.640]  как это происходит? Во-первых, если у вас есть возможность на некоторых, на различных выбраках
[56:51.640 --> 56:55.920]  ее посчитать, ну, например, у вас там есть две выбраки, вы можете посчитать оценку на первой
[56:55.920 --> 57:00.360]  выбраке и на второй, если у вас какие-то признаки получать сильно разные веса, скорее всего у вас
[57:00.360 --> 57:04.720]  оценка неустойчивая или у вас очень разные выбраки. То есть, надо или надо проверять распределение
[57:04.720 --> 57:10.800]  у этих выборок по признакам или, значит, у вас оценка неустойчивая. Второе, если вы, сейчас мы
[57:10.800 --> 57:15.440]  дойдем до градиентов краски, до градиентов меттов, если у вас для каких-то признаков градиенты, для весов
[57:15.440 --> 57:22.360]  признаков градиенты здоровые и они постоянно туда-сюда меняются, это тоже проблема. Вот. Так, на всякий
[57:22.360 --> 57:27.680]  случай, там звук работает, он не сел еще. Супер. Если он вдруг перестанет, вы, пожалуйста, это
[57:27.680 --> 57:36.560]  голосуйте. Хорошо? Вот. Ну и, собственно, вот наши сравнения, наши параболы замечательные и наши
[57:36.560 --> 57:42.560]  галки. На всякий случай, внимание, вопрос. А у нас же модуль, функция вроде как в нуле не дифференцируемая.
[57:42.560 --> 57:51.920]  А что делать? Как мы вообще можем его оптимизировать? Да, то, что у вас вроде как эта функция не совсем
[57:51.920 --> 57:56.560]  дифференцируемая, на практике нас абсолютно не волнует. Почему? У нас производная слева от нуля
[57:56.560 --> 58:00.960]  минус один, производная справа от нуля один, производную в нуле доопределяем нулем и радуемся.
[58:00.960 --> 58:06.960]  Если ошибка ноль, никакого градиента нам не надо. Все. Все счастливы, все работает. Соответственно,
[58:06.960 --> 58:13.280]  MSE дает нам Blue Best Linear Ambiance Estimator, оптимально средний смещен к оценку, работает с теориями
[58:13.280 --> 58:17.800]  Гаустамарковой, соответственно, дифференцируемо и она чувствительна к шуму. Потому что, если у
[58:17.800 --> 58:22.640]  вас, например, шум достаточно большой, то есть у вас Y, это краски, истинный сигнал, плюс какой-то
[58:22.640 --> 58:27.560]  шум. Чем больше у вас отклонения, парабола вам еще и в квадрат возведет это самое отклонение.
[58:27.560 --> 58:32.240]  Ошибка будет большая. А чем больше ошибка, уже с точки зрения, например, градиентных методов,
[58:32.240 --> 58:38.920]  вы давайте сюда внимательно посмотрим. Градиент у вас здесь слева будет что? Минус два х, а справа
[58:38.920 --> 58:47.080]  2х. Согласны? Но если минус два модули х, а справа 2х. Вот. Короче, у вас х там присутствует.
[58:47.080 --> 58:51.120]  То есть, чем больше у вас отклонения, тем больше у вас будет градиент, тем больше у вас каждый
[58:51.120 --> 58:56.840]  объект будет влиять на решение. Она чувствительна к шуму поэтому. Моя, собственно, у нее что?
[58:56.840 --> 59:02.440]  Отклонения слева дают нам градиент минус один, справа плюс один. Абсолютно все равно сильная
[59:02.440 --> 59:07.360]  там ошибка, слабая. У нас просто не попали точно в цель. Поэтому она гораздо менее чувствительна
[59:07.360 --> 59:20.200]  к шумам. Улавливаете почему, правильно? Все, классно. Тут всем тоже понятно. Что? Еще раз,
[59:20.200 --> 59:24.360]  она не дифференцируема с точки зрения вот мотона. Нам не нужно, чтобы она была дифференцирована
[59:24.360 --> 59:29.600]  всюду. Она дифференцируема на r+, то, что там всегда производная плюс один, на r-, а в нуле
[59:29.600 --> 59:33.680]  просто определяем производную нулем и все. То, что у нее производная как бы не является гладкой
[59:33.680 --> 59:37.800]  функцией, а нам не нужна гладкость. Нам надо, чтобы мы в каждой точке могли посчитать производную.
[59:37.800 --> 59:44.200]  Мы можем. И, соответственно, с регуляризацией. С регуляризацией тоже любопытная вещь. Во-первых,
[59:44.200 --> 59:49.760]  L2-регуляризация это тот случай, когда у нас есть аналитическое решение. Все замечательно. Она
[59:49.760 --> 59:52.840]  дает нам более устойчивое решение, потому что у нас теперь есть единственный вектор,
[59:52.840 --> 59:59.320]  который минимизирует норму вектора весов. Устойчивый, значит, при малом изменении нашей выборки
[59:59.320 --> 01:00:05.640]  у нас будет мало изменения нашего вектора весов. А вот L1-регуляризация. Она не дифференцируема,
[01:00:05.640 --> 01:00:10.040]  но нам опять же все равно абсолютно, потому что доопределяем в нуле нулем. То, что она не является
[01:00:10.040 --> 01:00:22.120]  гладкой, нам это не требуется. Типа того, когда мы в функции ошибки добавляем норму вектора весов.
[01:00:22.120 --> 01:00:29.080]  Можем взять вторую норму, можем взять первую норму. Все, собственно. Как вы понимаете, третью
[01:00:29.080 --> 01:00:33.760]  норму брать немножко странновато, потому что она может быть отрицательной. Нам это вообще не надо.
[01:00:33.760 --> 01:00:42.960]  Окей? Ой, простите, третью норму, господи, третью. Ладно, вы меня поняли. Да. В смысле,
[01:00:42.960 --> 01:00:53.360]  с функции ошибки третьей степени не надо брать, я договорился. Да. Значение весов или значение параметров?
[01:01:03.760 --> 01:01:24.320]  Да. Смотрите, я сейчас на ваш вопрос отвечу, как бы предполагаю. Так, смотрите, собственно, L2-регуляризация,
[01:01:24.320 --> 01:01:29.560]  как вы понимаете, работает и с МСЕ, и с МОЕ, верно? Если бы и другой функции ошибки на самом деле.
[01:01:29.560 --> 01:01:32.880]  Например, на следующей неделе мы поговорим про задачи классификации, там у нас будет
[01:01:32.880 --> 01:01:37.760]  log-loss, логистическая функция потери. Туда точно также можно запекать вторую норму вектор весов,
[01:01:37.760 --> 01:01:42.880]  ничего не поменять, все хорошо. Возникает вопрос, зачем нам L1-регуляризация? Во-первых,
[01:01:42.880 --> 01:01:49.080]  она дает нам чуть другие свойства, потому что, например, она отбирает признаки. Что это значит,
[01:01:49.080 --> 01:01:56.080]  я сейчас скажу. Во-вторых, у нас не всегда есть требования получить наименьшее значение вектор
[01:01:56.080 --> 01:02:00.680]  весов, это зависит опять же от ваших предположений. А касательно того, что у вас нет аналитической
[01:02:00.680 --> 01:02:05.920]  формулы, а нам все равно, даже если считать градиентное решение, у вас будет решение,
[01:02:05.920 --> 01:02:09.960]  грубо говоря, не единственное, все равно вы сойдетесь согласно шуму к какому-то решению,
[01:02:09.960 --> 01:02:14.760]  если вы используете выраженную матрицу, просто градиентным методом, но она у вас, собственно,
[01:02:14.760 --> 01:02:18.880]  получится тоже каким-то непонятным, каким-то вот таким может получиться и любым другим,
[01:02:18.880 --> 01:02:23.280]  потому что вы под шум переобучитесь. То, что шум у вас все равно в данных присутствует. А когда вы
[01:02:23.280 --> 01:02:27.920]  используете L2-регуляризацию, вы все равно получите решение, которое гораздо ближе вот сюда. Но не вот
[01:02:27.920 --> 01:02:38.000]  сюда. Тут будет примерно минус 0.7, 0.8 и минус 0.8. Они будут гораздо ближе друг к другу. Да, это
[01:02:38.000 --> 01:02:43.160]  с L2. С L1 примерно то же самое. Заметьте, это тоже функция, по сути, выпуклая. Поэтому она все равно
[01:02:43.160 --> 01:02:48.320]  дает вам какое-то единственное решение. Но оно работает чуть по-другому. И здесь обычно начинаются
[01:02:48.320 --> 01:02:57.160]  какая-то очень странные разговоры. К сожалению, анимашка почему-то померла. Может, на нее можно
[01:02:57.160 --> 01:03:03.720]  посмотреть. Нет, нельзя. Ладно, потом покажу. Ну, тут просто это все должно ездить. Я вам лучше сейчас
[01:03:03.720 --> 01:03:10.080]  на пальцах объясню. Смотрите, L1 отбирает признаки. Вот это очень такая нестандартная вещь. От нее,
[01:03:10.080 --> 01:03:14.720]  к правилу, все начинают немножко зависать. Что значит L1 отбирает признаки? Я вам давайте сейчас
[01:03:14.720 --> 01:03:22.440]  даже попробую это нарисовать. Благо, у нас есть доска. Пока она загружается, собственно, что сделаем?
[01:03:22.440 --> 01:03:28.120]  Что значит L1 отбирает признаки? Давайте пока что подумаем. Вот мы с вами, когда решаем
[01:03:28.120 --> 01:03:33.040]  оптимизационную задачу, нам нужно найти решение, которое минимизирует сумму квадратов отклонений,
[01:03:33.040 --> 01:03:37.120]  например, в СМСЕ. Но при этом мы хотим еще и минимизировать первую норму вектор весов.
[01:03:37.120 --> 01:03:42.960]  Правильно? Давайте на это посмотрим с точки зрения градентных методов.
[01:03:42.960 --> 01:03:57.320]  Что? Рисовать на черном фоне классно, мне нравится. По-моему, так даже лучше видно. Итак, смотрите.
[01:03:57.320 --> 01:04:06.320]  Вот у нас с вами есть, собственно, первая норма, вектор омега. То есть это у нас получается сумма
[01:04:06.320 --> 01:04:14.920]  омега итой по модулю. Согласны? Давайте мы теперь попробуем эту штуку минимизировать.
[01:04:14.920 --> 01:04:26.960]  Причем градентный метод? Когда у нас получается d омега первая норма по d омега и будет чему равна?
[01:04:26.960 --> 01:04:42.240]  Ну по факту это будет плюс один, если омега и больше нуля, ноль если омега равна нулю и
[01:04:42.240 --> 01:04:57.760]  минус один, если омега и. Нет. Если омега и меньше нуля. Или, что на самом деле эквивалентно,
[01:04:57.760 --> 01:05:10.400]  так просто проще записать, это просто сигнум омега и. Хорошо? О, спасибо. Современные
[01:05:10.400 --> 01:05:17.240]  технологии. Ладно, с этим все согласны. А теперь давайте посмотрим, как он. Предположим,
[01:05:17.240 --> 01:05:21.320]  что у нас какой-то признак, например, вообще не влияет на решение. То есть он не нужен,
[01:05:21.320 --> 01:05:25.640]  он шумовой. Он неважно, там положительный, отрицательный вес имеет, он на решение влияет.
[01:05:25.640 --> 01:05:32.520]  Тогда что мы получим? У нас, когда мы с вами пытаемся минимизировать нашу функцию имперического
[01:05:32.520 --> 01:05:40.320]  риска, q от омега, которая в общем случае равна l, это наша функция потерь. Что с тобой не так?
[01:05:40.320 --> 01:05:48.440]  Залепло, наверное. У нас есть l, это именно функция ошибки, ошибка предсказания нашей
[01:05:48.440 --> 01:05:56.160]  целевопеременной. Плюс первая норма, вектор весов. Согласны, когда мы с вами считаем производную,
[01:05:56.160 --> 01:06:07.360]  у нас получается dq по d омега. Это что? Это у нас dl по d омега плюс d omega 1 по d омега.
[01:06:07.360 --> 01:06:18.240]  С этим все согласны? Вот. Давайте добавлю лямду. Я сейчас показываю именно тот факт,
[01:06:18.240 --> 01:06:24.360]  почему при лямду равной 1 или лямду равной 0,5 ничего не поменяется. Если у нас какой-то
[01:06:24.520 --> 01:06:32.440]  признак не влияет на функцию ошибки. Что значит будет с этой производной? Она будет равна 0,
[01:06:32.440 --> 01:06:36.800]  потому что она от нее не зависит. Ну или в среднем равна 0, потому что у нас есть шум, одни объекты
[01:06:36.800 --> 01:06:45.240]  будут тянуть наверх, другие вниз, чтобы не засыпали и так далее. Да что с ними не так сегодня?
[01:06:45.440 --> 01:06:53.280]  Ой, ладно. Короче, довки устали. Короче, эта штука примерно равна нулю.
[01:06:53.280 --> 01:07:06.080]  Все, да ладно, мы почти закончили. Короче, все согласны, я надеюсь, что вот эта штуковина,
[01:07:06.080 --> 01:07:12.480]  она примерно равна нулю, если признак не зависит. Что у нас когда остается от градиента? У нас
[01:07:13.080 --> 01:07:18.200]  вот этот вклад, а когда вот этот вклад будет нулевым. То есть когда у нас градиент не будет тянуть
[01:07:18.200 --> 01:07:24.720]  данный вес признака куда-нибудь, когда он равен нулю. Потому что всегда когда у нас
[01:07:24.720 --> 01:07:29.760]  признак больше нуля, у нас градиент плюс 1. Соответственно антиградиент будет минус 1.
[01:07:29.760 --> 01:07:34.520]  Всегда когда признак меньше нуля, антиградиент будет плюс 1, то есть у нас градиент будет
[01:07:34.520 --> 01:07:39.160]  всегда толкать значение нашего веса в 0? Да и вы меня
[01:07:39.160 --> 01:07:43.640]  достали, господи. Не вы в смысле о доске. Соответственно,
[01:07:43.640 --> 01:07:47.400]  всегда когда значение нашего признака не равно 0, веса
[01:07:47.400 --> 01:07:51.440]  нашего признака не равно 0, если он неважен, то, соответственно,
[01:07:51.440 --> 01:07:53.560]  он будет просто загнан в 0. Потому что вот эта штука
[01:07:53.560 --> 01:07:56.320]  компенсировать это не будет, а эта штука будет его толкать
[01:07:56.320 --> 01:08:00.120]  в 0. Собственно, ровно поэтому и говорят, что L1 регуляризация
[01:08:00.120 --> 01:08:03.000]  отбирает признаки в смысле того, что если у вас какой-то
[01:08:03.000 --> 01:08:07.160]  признак неважен, он получит на любой вес в конце кону.
[01:08:07.160 --> 01:08:11.680]  Его просто, собственно, первая норма загонит в 0. Более того,
[01:08:11.680 --> 01:08:15.560]  на самом деле тут даже что можно сказать? Если у вас
[01:08:15.560 --> 01:08:18.120]  вот этот коэффициент регуляризации достаточно большой, то
[01:08:18.120 --> 01:08:19.920]  есть признак все равно имеет там какой-то мизерный
[01:08:19.920 --> 01:08:22.280]  градиент, то есть очень маленький градиент, он возможно
[01:08:22.280 --> 01:08:26.000]  как-то очень мало значен для решения задачи, а здесь
[01:08:26.000 --> 01:08:29.640]  коэффициент лямбда большой, то получается, что вот этот
[01:08:29.720 --> 01:08:35.160]  он же всегда как бы пропорционален единице, потому что лямбде
[01:08:35.160 --> 01:08:37.360]  лямбда на единице, потому что здесь или плюс или минус
[01:08:37.360 --> 01:08:42.720]  один. Поэтому если у вас какой-то признак мало влияет
[01:08:42.720 --> 01:08:45.280]  на решение, допустим, там есть какая-то связь между
[01:08:45.280 --> 01:08:48.640]  признаком и целевой переменной, но она крайне маленькая,
[01:08:48.640 --> 01:08:50.680]  допустим, там влияет на нее в четвертом знаке после
[01:08:50.680 --> 01:08:56.000]  запятой. У вас L1 регуляризация с лямбдой больше, чем 10-4,
[01:08:56.000 --> 01:08:57.760]  все равно загонит его в 0, потому что отсюда у вас
[01:08:57.760 --> 01:09:02.680]  градиент будет порядка 10-4, отсюда порядка 1. Уловили?
[01:09:02.680 --> 01:09:11.840]  Все уловили? Картиночка? Ну, визуализировать, так я
[01:09:11.840 --> 01:09:15.440]  собственно и пытался вам это визуализировать. Смотрите,
[01:09:15.440 --> 01:09:17.840]  вот вам картиночка. У вас получается, что производная
[01:09:17.840 --> 01:09:22.120]  здесь всегда плюс 1, минус 1 здесь плюс 1. Если у вас
[01:09:22.120 --> 01:09:25.560]  производная от функции потери меньше, чем вот эта штука,
[01:09:25.560 --> 01:09:28.320]  она ее доминирует, значит, она загоняет 0. То, что если
[01:09:28.320 --> 01:09:30.960]  она вышла из 0, она сразу получает пинка в сторону
[01:09:30.960 --> 01:09:35.560]  0 силой 1, а функция потери силы меньше 1. Все, она зайдет
[01:09:35.560 --> 01:09:42.000]  именно туда, где минимум. Все, он нам больше не нужен.
[01:09:42.000 --> 01:09:48.120]  Ага. А теперь посмотрим на L2. Смотрите, L2 у нас какую
[01:09:48.120 --> 01:09:52.360]  производную будет давать? 2 Омега. Поэтому если Омега
[01:09:52.360 --> 01:09:56.000]  около 0, то у нее градиент становится уже меньше, чем
[01:09:56.000 --> 01:09:58.840]  вот этот самый условно Эпсилон, который нам дает функция
[01:09:58.840 --> 01:10:01.440]  потерь, поэтому она просто загоняет их близко к нулю,
[01:10:01.440 --> 01:10:04.600]  но не ровно в 0. L1, собственно, за счет вот этой ступеньки
[01:10:04.600 --> 01:10:07.560]  у нас производная, либо минус 1, 0, 1. У нас ступенька
[01:10:07.560 --> 01:10:10.600]  резкий переход. За счет этого он ровно в 0 загоняет,
[01:10:10.600 --> 01:10:14.720]  а у L2 плавно убывает производная. Чем ближе к нулю, тем меньше
[01:10:14.720 --> 01:10:17.080]  производная. Поэтому он их просто догоняет примерно
[01:10:17.080 --> 01:10:21.120]  до 0, но не выкидывает полностью. Именно в этом смысле говорят,
[01:10:21.120 --> 01:10:24.560]  что L1 регуляризация отбирает признаки. На всякий случай,
[01:10:24.560 --> 01:10:26.280]  когда вы используете L1 регуляризации, вы просто
[01:10:26.280 --> 01:10:28.320]  в векторе весов можете получить некоторые нулевые
[01:10:28.320 --> 01:10:32.040]  элементы, но вектор весов будет точно такой же размеренский.
[01:10:32.040 --> 01:10:37.240]  Это частая ошибка на самом начале работы с машинным
[01:10:37.240 --> 01:10:40.920]  обучением, когда ожидают, что вы применили L1 регуляризацию
[01:10:40.920 --> 01:10:43.280]  и у вас почему-то вектор весов стал меньше. Нет,
[01:10:43.280 --> 01:10:45.320]  конечно, у вас просто такие члены будут равны нулю.
[01:10:45.320 --> 01:10:49.080]  Все. И именно их мы считаем выкинутыми признаками,
[01:10:49.160 --> 01:10:51.760]  но после этого, как правило, есть еще подход, что после
[01:10:51.760 --> 01:10:55.160]  этого вы перезапускаете вашу модель, оптимизацию
[01:10:55.160 --> 01:10:57.640]  выкинув эти признаки из обучающей выбраки, можете
[01:10:57.640 --> 01:11:01.000]  даже после этого уже L2 регуляризацию взять и уже оставшиеся признаки
[01:11:01.000 --> 01:11:04.680]  просто подогнать поближе к нулю. Вопросы, пожелания,
[01:11:04.680 --> 01:11:08.920]  комментарии? Те поняли, что L1 делает с признаками,
[01:11:08.920 --> 01:11:15.120]  с весами? Тут тоже. Зануляет, если они не важны для решения
[01:11:15.120 --> 01:11:18.120]  задачи. То есть, если они достаточно не важны относительно
[01:11:18.120 --> 01:11:19.840]  коэффициента лямбда. И, собственно, здесь коэффициент лямбда
[01:11:19.840 --> 01:11:22.880]  как раз явно видно, что делает. Если лямбда большая,
[01:11:22.880 --> 01:11:25.080]  то если наш признак влияет на ошибку, на градиент
[01:11:25.080 --> 01:11:28.480]  ошибки меньше, чем на лямбду, то он будет выкинут. Если
[01:11:28.480 --> 01:11:31.680]  больше, чем на лямбду, то он не будет выкинут. Все.
[01:11:31.680 --> 01:11:35.240]  Вот. И это, на самом деле, штука, я ее потом подвигаю.
[01:11:35.240 --> 01:11:39.280]  Это второе объяснение, собственно, что это такое. Это шар,
[01:11:39.280 --> 01:11:44.200]  но во второй норме. Это шар в первой норме. Заметьте,
[01:11:44.200 --> 01:11:47.120]  у вас вот это, по сути, что здесь, я потом, она подвигается,
[01:11:47.120 --> 01:11:49.120]  на пальцах, опять же, покажу. Собственно, что вот это за
[01:11:49.120 --> 01:11:52.360]  эллипс такой? Это линии уровня квадратичной функции
[01:11:52.360 --> 01:11:55.600]  потерь. То есть, линии уровня, это эквипотенциальная
[01:11:55.600 --> 01:11:57.560]  поверхность. То есть, на линии уровня у вас одинаковые
[01:11:57.560 --> 01:12:00.280]  значения функции ошибки. Оптимальное решение у нас
[01:12:00.280 --> 01:12:03.520]  где-то вот в этой точке. Мы именно в нее хотим попасть.
[01:12:03.520 --> 01:12:06.120]  Но когда мы, например, говорим, что рассмотрим все решения,
[01:12:06.120 --> 01:12:10.160]  у которых норма вектор весов первая равна единице,
[01:12:10.160 --> 01:12:13.400]  у нас в среднем вот эта точка, вот эти точки, так как они
[01:12:13.400 --> 01:12:15.880]  у нас по первой норме на шаре лежат, по сути, они в вершинах
[01:12:15.880 --> 01:12:19.080]  нашего четырехугольника, они в среднем будут ближе
[01:12:19.080 --> 01:12:22.880]  к центру, чем те, которые лежат между ними. Опять
[01:12:22.880 --> 01:12:25.800]  же, это можно формально доказать, эта штука по-хорошему
[01:12:25.800 --> 01:12:33.640]  должна двигаться. Ладно, давайте я вам ее покажу. Я
[01:12:33.640 --> 01:12:51.440]  надеюсь, ссылка еще работает. Во! Да понятно, она сейчас
[01:12:51.440 --> 01:13:01.800]  откроется. Вот, давайте посмотрим, скажем так
[01:13:01.800 --> 01:13:06.280]  за этим, как его. Глядите за нахождением желтой точки.
[01:13:06.280 --> 01:13:08.840]  Видите, здесь она ездит совершенно свободно. Она находится
[01:13:08.840 --> 01:13:13.200]  всегда на окружности радиуса единицы от центра, и абсолютно
[01:13:13.200 --> 01:13:16.240]  все равно у нас есть центральная симметрия. Здесь точка почти
[01:13:16.240 --> 01:13:20.160]  всегда находится в вершинах. А это ровно есть та ситуация,
[01:13:20.160 --> 01:13:23.800]  когда у вас один из весов от собственно оси, ω1, ω2,
[01:13:23.800 --> 01:13:28.640]  один из весов равен нулю. Понимаете, потому что его
[01:13:28.640 --> 01:13:32.680]  вклад в качестве ошибки вот здесь гораздо меньше,
[01:13:32.680 --> 01:13:36.040]  чем его вклад в качестве регуляризации здесь. Только
[01:13:36.040 --> 01:13:41.720]  и всего. Это такая визуальная картинка. Мне, честно говоря,
[01:13:41.720 --> 01:13:43.680]  вот это объяснение было непонятно, наверное, первые
[01:13:43.680 --> 01:13:46.440]  года три, как я на него смотрел, с градиентами было гораздо
[01:13:46.440 --> 01:13:54.040]  проще понять. Но кому-то возможно так понять. Окей, тут вопрос
[01:13:54.040 --> 01:14:09.480]  есть? Ну почему? У вас производная тогда чему равна? 2 ω всегда.
[01:14:09.480 --> 01:14:13.640]  Так, ребят, сейчас дайте мне еще пять минут, пожалуйста.
[01:14:13.640 --> 01:14:16.200]  Мы тут чуть позже начали, поэтому мы не до конца уложились.
[01:14:16.200 --> 01:14:25.680]  Что? Да, да. Ну ровно поэтому, чем ближе вы к нулю, тем меньше
[01:14:25.680 --> 01:14:33.360]  у вас вклад регуляризации в решение. Ну да. Я вас
[01:14:33.360 --> 01:14:35.360]  плохо слышу, тут уже пошел, давайте в перерыве тогда
[01:14:35.360 --> 01:14:39.120]  отвечу. Хорошо. Ладно, ребят, собственно, бывают различные
[01:14:39.120 --> 01:14:42.080]  еще способы по мере качества в регрессии не только
[01:14:42.080 --> 01:14:44.640]  МСЕ и МАЕ. Например, есть коэффициент детерминации
[01:14:45.480 --> 01:14:48.480]  есть МОПЕ, средняя абсолютная процентная ошибка, есть
[01:14:48.480 --> 01:14:51.040]  самопе симметричная, на них проще на семинаре посмотреть
[01:14:51.040 --> 01:14:54.480]  и не в принципе. Это все мое МСЕ, только чуть-чуть
[01:14:54.480 --> 01:14:58.120]  преобразованное. Ну и последняя, но тем не менее очень важная
[01:14:58.120 --> 01:15:01.240]  вещь, потому что уже второе занятие, скажем так, чисто
[01:15:01.240 --> 01:15:04.560]  по машинке, у нас была еще лекция по линалу. И чтобы
[01:15:04.560 --> 01:15:07.320]  все модели строить, надо уметь оценивать их качество.
[01:15:07.320 --> 01:15:10.520]  И как сказал очень умный, талантливый человек, вы
[01:15:10.520 --> 01:15:13.360]  ровно настолько можете доверять своей модели, насколько
[01:15:13.360 --> 01:15:16.760]  вы доверяете своему пайплайну валидации. Вот шутку валидации
[01:15:16.760 --> 01:15:21.400]  я сейчас скажу. Давайте скажем, что у нас есть выборка.
[01:15:21.400 --> 01:15:23.600]  Мы можем решать задачу регрессии, можем задачу
[01:15:23.600 --> 01:15:26.480]  бинарной классификации, абсолютно неважно. У нас
[01:15:26.480 --> 01:15:28.800]  есть некоторая модель, которая предсказывает значение
[01:15:28.800 --> 01:15:33.240]  целевой перемены для каждого объекта. Наша задача – минимизировать
[01:15:33.240 --> 01:15:37.040]  вот эту самую функцию имперического риска, ну или функцию потери.
[01:15:37.040 --> 01:15:38.480]  Опять же, на практике ее просто всегда называют
[01:15:38.480 --> 01:15:40.480]  функцией потерь, главное понимать, что вы в это
[01:15:40.480 --> 01:15:42.840]  вкладываете. Включает она в себя регуляризацию
[01:15:42.840 --> 01:15:48.040]  или нет. И у нас может быть три ситуации. Недо обучения
[01:15:48.040 --> 01:15:50.640]  модель слишком простая для решения задач. Линейная
[01:15:50.640 --> 01:15:53.840]  модель, например, пытается, мы пытаемся ее использовать
[01:15:53.840 --> 01:15:57.680]  в явно нелинейной задаче. Тогда все плохо. Может быть
[01:15:57.680 --> 01:15:59.960]  хорошая модель, вот то, что мы всегда хотим, может
[01:15:59.960 --> 01:16:02.200]  быть переобучение, когда модель по сути запомнила
[01:16:02.200 --> 01:16:05.760]  ответ на каждом объекте и явно вот такая поверхность
[01:16:05.760 --> 01:16:08.840]  решений как-то больно сложной выглядит для такой задачи.
[01:16:09.480 --> 01:16:11.640]  Что такое оптимальная сложность модели, на самом деле можно
[01:16:11.640 --> 01:16:14.040]  долго говорить, и это зависит от того, что мы сформулируем
[01:16:14.040 --> 01:16:16.600]  как оптимальная модель. Но есть вот эта классная
[01:16:16.600 --> 01:16:22.520]  картинка из книжки Гудфеллу и Бенджоу, и Арен Курвилла,
[01:16:22.520 --> 01:16:25.120]  она у нас вторая в списке рекомендованной литературы.
[01:16:25.120 --> 01:16:28.720]  Собственно, здесь у нас по оси х ошибка, по оси у
[01:16:28.720 --> 01:16:32.800]  у нас сложность модели, то есть количество степеней
[01:16:32.800 --> 01:16:35.360]  свободы модели, грубо говоря. Можете очень грубо это
[01:16:35.360 --> 01:16:38.600]  оценить как количество параметров модели. Хорошо?
[01:16:38.600 --> 01:16:40.440]  В двух данных она может запомнить. И, собственно,
[01:16:40.440 --> 01:16:43.080]  у нас есть две ошибки. Тиненькая, это ошибка на обучающей
[01:16:43.080 --> 01:16:46.600]  выборке, она у нас снижается примерно к нулю, она может
[01:16:46.600 --> 01:16:50.040]  достичь нуля, если у нас нету явно шума в наших данных,
[01:16:50.040 --> 01:16:51.760]  она не может его достичь, если у нас, например, два
[01:16:51.760 --> 01:16:54.280]  объекта одинаковых с разными предсказаниями, тогда
[01:16:54.280 --> 01:16:58.440]  не сможешь ничего сделать. А вот у нас ошибка обобщающая,
[01:16:58.440 --> 01:17:01.880]  ошибка обобщения или Generalization Error, или ошибка на отложенных
[01:17:01.880 --> 01:17:04.360]  данных. Как видите, она до какого-то момента снижается
[01:17:04.360 --> 01:17:07.680]  вместе с ним, а потом начинает краски расти. Это, собственно,
[01:17:07.680 --> 01:17:10.440]  как можно сделать переобучением, когда наша модель переобучается
[01:17:10.440 --> 01:17:12.880]  под обучающую выборку, то есть запоминает специфичные
[01:17:12.880 --> 01:17:15.800]  вещи только для обучающей выборки. Но это красивая
[01:17:15.800 --> 01:17:18.400]  теория. Тут, на самом деле, есть еще у этой картинки
[01:17:18.400 --> 01:17:21.360]  продолжение, о нем говорят последние годы, и все еще
[01:17:21.360 --> 01:17:23.920]  идут научные изыскания, непонятно, что происходит.
[01:17:23.920 --> 01:17:26.920]  Называется она Double Decay. В некоторых случаях вот эта
[01:17:26.920 --> 01:17:29.680]  штука может начать расти, а потом опять начать падать.
[01:17:29.680 --> 01:17:32.120]  Но как бы это вот конец второго семестра, возможно,
[01:17:32.120 --> 01:17:34.080]  про это поговорим, если будет именно как это обосновать,
[01:17:34.080 --> 01:17:36.400]  потому что есть просто несколько работ на эту тему научную,
[01:17:36.400 --> 01:17:40.760]  которые говорят, ну, вообще так бывает. Вот, собственно,
[01:17:40.760 --> 01:17:44.240]  недообучение и переобучение. Проблем в чем? Если вы модель
[01:17:44.240 --> 01:17:46.720]  сделанной слишком простой, усложнить ее вы всегда
[01:17:46.720 --> 01:17:49.840]  можете. Если слишком сложной, можете ее всегда упростить.
[01:17:49.840 --> 01:17:52.480]  Но, собственно, как понять, насколько ваша модель
[01:17:52.480 --> 01:17:55.400]  переобучена? Что она недообучена, в принципе, можно понять
[01:17:55.400 --> 01:17:57.680]  достаточно легко. У вас просто качество в модели недостаточно
[01:17:57.680 --> 01:18:00.880]  хорошее. У вас либо данные плохие, либо модель недообучается.
[01:18:00.880 --> 01:18:03.240]  Именно на обучающей выборке. Что делать, если модель
[01:18:03.280 --> 01:18:06.920]  переобучается? Вы для этого сначала должны обнаружить
[01:18:06.920 --> 01:18:08.840]  переобучение, потому что вы не знаете, как ваша модель
[01:18:08.840 --> 01:18:12.320]  себя ведет. Где-то, кроме обучающей выборки, покажут.
[01:18:12.320 --> 01:18:15.080]  Поэтому можно использовать, скажем так, метод отложенной
[01:18:15.080 --> 01:18:18.320]  выборки. И я не рекомендую его использовать сейчас,
[01:18:18.320 --> 01:18:20.480]  когда мы будем говорить про какие-то крупные нейронки.
[01:18:20.480 --> 01:18:23.280]  У нас не будет выбора другого. Но, тем не менее. Как правило,
[01:18:23.280 --> 01:18:28.520]  вот есть такое понятие обучающей выборке, train, тестовой выборке,
[01:18:28.520 --> 01:18:31.520]  test. И я сразу скажу, что есть, собственно, валидационная
[01:18:31.520 --> 01:18:35.680]  выборка еще, чтобы вас сразу не консьюдить, скажем так, не запутывать.
[01:18:35.680 --> 01:18:41.080]  Собственно, в хорошей нотации тестовая выборка — это то, что у вас вообще вот
[01:18:41.080 --> 01:18:44.960]  лежит где-то вне, и вы к нему либо никогда не получите доступа, либо вы
[01:18:44.960 --> 01:18:48.640]  получите к нему доступа, грубо говоря, пройдя точку невозврата. Ну, словно вот
[01:18:48.640 --> 01:18:51.800]  тестовая выборка — это как будто вы сдали работу преподавателю на экзамене, и
[01:18:51.800 --> 01:18:57.120]  уже преподаватель вам говорит ответ, хорошо вы решили экзамен или нет. Вот это
[01:18:57.120 --> 01:19:00.600]  ваш тестовая выборка. Вы не можете сказать преподавателю, а, я здесь ошибся, дайте
[01:19:00.600 --> 01:19:04.640]  перепишу и опять ему сдать. Некоторые пытаются, как правило, от нее работать.
[01:19:04.640 --> 01:19:08.200]  Валидационная выборка — это все то, что вы делаете у себя локально, это ваши
[01:19:08.200 --> 01:19:11.640]  локальные тесты на ваш код где-нибудь там на проге, это ваша проверка своей
[01:19:11.640 --> 01:19:15.720]  домашки или попытка соседа попросить проверить вашу домашку и так далее. То
[01:19:15.720 --> 01:19:19.000]  есть валидационная выборка — это то, что вы сами используете. Поэтому у нас есть
[01:19:19.000 --> 01:19:23.520]  собственно train validation test framework. Train — это то, на чем вы настраиваете параметры
[01:19:23.520 --> 01:19:27.680]  вашей модели. Вот именно параметр. Валидация — это то, на чем вы выбираете
[01:19:27.680 --> 01:19:32.440]  гиперпараметры и то, на чем вы проверяете качество вашей модели. Обращаю ваше внимание,
[01:19:32.440 --> 01:19:35.800]  если вы подбираете параметры на валидационной выборке, вы уже
[01:19:35.800 --> 01:19:39.440]  посредством взаимодействия самостоятельно с параметрами, гиперпараметрами точнее,
[01:19:39.440 --> 01:19:43.880]  переобучаетесь под валидационную выборку. И вы можете под нее переобучиться, если будете
[01:19:43.880 --> 01:19:49.400]  долго это делать. То, что у вас по сути утечка данных идет через вас в вашу гиперпараметру.
[01:19:49.400 --> 01:19:53.320]  Если достаточно долго будете делать, вы подберете оптимально для вашей валидационной выборки,
[01:19:53.320 --> 01:20:00.200]  но не факт для всего распределения, откуда приходят данные. Что можно делать? Варианты,
[01:20:00.200 --> 01:20:05.200]  где вы просто от Рейна отрубили тест один раз, на нем проверились — идея плохая. Почему? Потому
[01:20:05.200 --> 01:20:09.640]  что как минимум вы можете случайно выбрать плохую под выборку, на которую будете тестироваться.
[01:20:09.640 --> 01:20:15.320]  Например, у вас сюда попали только объекты класса 1. Тогда если у вас абсолютно тупой классикатор,
[01:20:15.320 --> 01:20:20.080]  который умеет предсказывать только объекты класса 1, у вас 100% accuracy, у вас все правильно,
[01:20:20.080 --> 01:20:25.760]  а по факту ваш классикатор — мусор. Или у вас здесь просто класс не сбалансированный. Или здесь у вас
[01:20:25.760 --> 01:20:30.480]  только студента, например, у вас задача предсказания среднего дохода, и у вас несколько социальных
[01:20:30.480 --> 01:20:35.640]  групп. Короче, как дробить данные, там будет различная процедура стратисикации, нормализация,
[01:20:35.640 --> 01:20:41.120]  и так далее. Про это мы тоже поговорим. Как делать, пока у вас маленькие модели? Это примерно до
[01:20:41.120 --> 01:20:46.040]  восьмого занятия нашего курса, пока мы делаем так. Мысли так делают всегда, когда маленькие модели,
[01:20:46.040 --> 01:20:50.600]  когда не слишком много данных, когда можно себе это позволить. Можно, собственно, или разделить
[01:20:50.600 --> 01:20:56.000]  выборку на train, validation и test. Вы можете либо выбрать валидацию очень хорошо, проверить все
[01:20:56.000 --> 01:21:00.200]  статистики. Классно. Пока выборка маленькая, можно сделать сильно проще. Можно использовать cross
[01:21:00.200 --> 01:21:05.440]  валидацию. А именно, вы сразу имеете какую-то отложенную выборку. Вот тест — это то, что у вас или
[01:21:05.440 --> 01:21:10.600]  вам вообще недоступно, или вы изначально ее отложили. Как правило, тест у вас… Короче, вы, как
[01:21:10.600 --> 01:21:15.240]  правило, тест себе не выделяете. То есть тест — это либо то, что вы будете сдавать в контест, либо то,
[01:21:15.240 --> 01:21:19.680]  что вы издаете на соревнования, либо то, что вы завтра тестируетесь на работе, на проде и так далее.
[01:21:19.680 --> 01:21:24.840]  Если все упадет — плохо. Что делать с validation? У вас есть все ваши данные обучающие. Вы их можете
[01:21:24.840 --> 01:21:31.760]  побить на различные чанки, кусочки, и повторить много раз процедуру. Выбираем все фолды, кроме
[01:21:31.760 --> 01:21:37.200]  одного, фолд, краски в данном случае. Все куски, на котором побили, на них обучаемся, на последнем
[01:21:37.200 --> 01:21:41.120]  валидируемся. На всякий случай тут написано test fault. Я когда-нибудь это все-таки перепишу.
[01:21:41.120 --> 01:21:45.960]  Валидационный фолд. На последнем проверяемся. И повторяем так, чтобы в качестве валидационного
[01:21:45.960 --> 01:21:49.800]  использовался каждый из фолдов, который был. То есть, по сути, мы, если у нас 10 фолдов,
[01:21:49.800 --> 01:21:56.000]  10 раз обучаем нашу модель на различных подвыборках. 10 раз тестируем на, опять же, различных подвыборках
[01:21:56.000 --> 01:22:01.120]  валидационных. У нас получается на самом деле 10 моделей. И 10 раз мы посчитали ошибку. Потом
[01:22:01.120 --> 01:22:05.920]  можем усреднить. И вот эта усредненная ошибка есть наиболее, наверное, качественная оценка качества
[01:22:05.920 --> 01:22:13.160]  нашей модели. Да, это, конечно, все умеет. Мы вас будем явно просить делать именно так,
[01:22:13.160 --> 01:22:18.280]  потому что один раз побить пополам – плохая идея. Плюс это вам позволяет экономить на самом деле
[01:22:18.280 --> 01:22:22.080]  данные, потому что теперь вы, по сути, пробежались, протестировались на все обучающие выборки,
[01:22:22.080 --> 01:22:28.720]  и вы обучились на все обучающие выборки. Тут маленькое замечание. Что делать, когда у вас
[01:22:28.720 --> 01:22:32.960]  получил здесь модели на выходе, ведь у вас здесь, здесь, здесь и здесь, получил здесь разных моделей.
[01:22:32.960 --> 01:22:38.960]  Тут есть два варианта. Первая – модель линейная. Допустим, это линейная регрессия. Можете банально
[01:22:38.960 --> 01:22:44.160]  усреднить их веса и все. Долго не думая. Вторая – модели какие-то сложные, например, это какие-то
[01:22:44.160 --> 01:22:49.640]  деревья. Тогда потом одиннадцатый раз обучаете модель с нужными вам гиперпараметрами. Гиперпараметры
[01:22:49.640 --> 01:22:53.600]  вы тоже можете подбирать по кросс-валидации. Просто в качестве скоров вы выбираете усредненность.
[01:22:53.600 --> 01:22:59.360]  И тогда, собственно, вы просто-напросто потом берете всю обучающую выборку, нужные вам гиперпараметры,
[01:22:59.360 --> 01:23:05.600]  обучаете модель целиком. Еще раз, но уже с теми гиперпараметрами, которые у вас были. А как обнаружить
[01:23:05.600 --> 01:23:16.000]  переобучение? Нет, две вещи. Первое – вы можете так посмотреть. Вы подкрутили гиперпараметры,
[01:23:16.000 --> 01:23:21.760]  посмотрели на кросс-валидации, как поменялся скор. Стало лучше или хуже на отложенных данных. Второе – вы
[01:23:21.760 --> 01:23:27.240]  тем самым детектируете переобучение. Если у вас начинает резко расходиться качество на обучающей
[01:23:27.240 --> 01:23:32.720]  выборке и на валидационной, то тут два варианта. Либо валидационная выборка плохая, сильно хуже,
[01:23:32.720 --> 01:23:37.000]  чем обучающая, либо вы переобучаете. Кросс-валидация вас, по сути, спасает от того, что вы
[01:23:37.000 --> 01:23:42.360]  переобучаетесь, от того, что она плохая, потому что вы пробежались по всем под выборком. Если на
[01:23:42.360 --> 01:23:53.760]  всех стало хуже, значит вы переобучились под обучающую выборку. Да, конечно. То есть по-хорошему,
[01:23:53.760 --> 01:23:58.240]  если вы смотрите просто на отклонения, на значение ошибки, вы смотрите на его средние. По-хорошему,
[01:23:58.240 --> 01:24:02.040]  посмотрите именно на статистики ошибки, возможно, у вас на каких-то классах ошибка большая и так далее.
[01:24:02.040 --> 01:24:07.040]  Анализировать ошибку более глубоко – абсолютно верно. Но, по крайней мере, вот базовая вещь – вы
[01:24:07.040 --> 01:24:11.840]  по кросс-валидации можете выбрать себе, скажем так, не выбрать, а заметить, что у вас на отложенной
[01:24:11.840 --> 01:24:16.000]  выборке всегда ошибка сильно выше, чем на тройновой, на обучающей. Значит, скорее всего,
[01:24:16.000 --> 01:24:21.040]  у вас слишком сложная модель, она просто-напросто переобучается под ваши данные, и вы находитесь
[01:24:21.040 --> 01:24:30.120]  где-то вот здесь. Надо делать модель попроще. И так далее. Вот. Десять – это просто количество
[01:24:30.120 --> 01:24:35.240]  фолдов. Это, скажем так, чем больше, тем лучше, но чем больше, тем дороже. У вас сложность линейно
[01:24:35.240 --> 01:24:40.920]  растет с увеличением количества фолдов. Ранее, вот совсем давным-давно, когда данные были маленькие,
[01:24:40.920 --> 01:24:48.240]  а трава зеленая, даже был отдельно называемый метод LOO, leave one out. Короче, оставь один. То есть
[01:24:48.240 --> 01:24:53.120]  там, в качестве трест, обучающий выборке использовал вся выборка с заключением одного объекта,
[01:24:53.120 --> 01:24:57.720]  соответственно, на одном объект тестировались. Но если у вас выборка размером там миллион, то
[01:24:57.720 --> 01:25:02.600]  миллион раз обучаете вашу модель, и вы замучаетесь. Поэтому, условно, обычно будет там на 5-10-20 фолдов,
[01:25:02.600 --> 01:25:07.520]  в зависимости от сложности. То есть, грубо говоря, сколько вы можете себе подволить. И именно поэтому
[01:25:07.520 --> 01:25:11.440]  кросс-валидация с каким-то uber-неронками не используется, потому что обычно там одна
[01:25:11.440 --> 01:25:17.240]  модель обучается месяца так два, так кластеры из ГПУ. Если вы будете это 5 раз делать, то это
[01:25:17.240 --> 01:25:23.440]  почти год. Дорого, просто дорого. Вот. Поэтому, когда мы дойдем до сложных моделей, мы краски
[01:25:23.440 --> 01:25:28.840]  поговорим еще раз, как нам выбирать уже хорошую одну валидационную выборку. Там должны совпадать
[01:25:28.840 --> 01:25:34.120]  распределение по классам или каким-то категориальным переменным, по ошибкам, короче, по всему. Они
[01:25:34.120 --> 01:25:38.760]  должны визуально выглядеть как выборки из одной и той же, из одного и того же распределения. Тогда
[01:25:38.760 --> 01:25:43.960]  все хорошо. Ну что ж, а на этом мы подходим к финалу нашей лекции. Собственно, линейные модели — это
[01:25:43.960 --> 01:25:48.840]  замечательные модели, которые работают в половине случаев. И как говорит мой, например, научный
[01:25:48.840 --> 01:25:52.800]  руководитель, это так называемый метод палки и веревки. Он очень простой, он не может не работать.
[01:25:52.800 --> 01:25:56.680]  Он может не работать, только если у вас слишком сложная задача для линейной модели. Там ломаться
[01:25:56.680 --> 01:26:01.280]  нечем. Понимание линейных моделей позволит вам работать и с нейронными сетями, и на самом деле
[01:26:01.280 --> 01:26:05.120]  с деревьями, потому что, как мы с вами увидим, деревья — это тоже в некотором смысле линейные модели,
[01:26:05.120 --> 01:26:10.240]  просто над очень странными признаками. И, собственно, стройте валидацию так, чтобы вы могли и доверять.
[01:26:10.240 --> 01:26:14.160]  Если вы не уверены в вашей валидации, значит, ровно настолько не уверены в том, что ваша модель
[01:26:14.160 --> 01:26:18.320]  вообще делает нефигню. То есть вот валидация и постановка задач и оптимизация — это, наверное,
[01:26:18.320 --> 01:26:22.760]  два таких кругольных камня, вокруг которых строится все остальное. Если у вас ошибка там или здесь,
[01:26:22.760 --> 01:26:28.360]  все остальное бесполезно. Вот. Ну, на этом лекция завершается. Перерыв 15 минут. Дальше семинар.

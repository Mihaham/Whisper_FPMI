[00:00.000 --> 00:14.400]  Всем привет! У нас с вами две пары назад начался MapReduce, потом мы сделали такое отступление в
[00:14.400 --> 00:26.480]  сторону джамвы и сегодня снова продолжим MapReduce. Сейчас я расшарю экран. Всем экран виден, да?
[00:30.000 --> 00:45.520]  Отлично. Сначала повторим немного MapReduce, потому что было давно базовые вещи. То есть
[00:45.520 --> 00:54.800]  парадигма MapReduce возникла в 2004 году на основе статьи от Google и работает она примерно так. То
[00:54.800 --> 01:02.000]  есть работаем с парами ключ значения и эти пары проходят несколько стадий. Первая стадия это стадия
[01:02.000 --> 01:10.320]  мап независимая обработка ключей значений. Потом эти пары сортируются и группируются по ключам.
[01:10.320 --> 01:18.960]  Эти группы передаются на редьюсеры и на редьюсере уже происходит какая-то агрегация по этим группам.
[01:19.200 --> 01:26.560]  Можно посмотреть, например, то есть мап работает примерно так. Берем элемент один и что-то с ним
[01:26.560 --> 01:32.160]  делаем, а на редьюсере берем всю группу и, например, складываем все элементы ее.
[01:32.160 --> 01:49.920]  Если посмотрим на эту задачу, то вспомним задачу на подсчет чистоты встречаемости слов в тексте,
[01:49.920 --> 01:55.840]  то с помощью MapReduce мы это будем делать вот так. Сначала сформируем пары слова единичка,
[01:55.840 --> 02:02.240]  потом от сортируем и сгруппируем их по словам и потом схлопнем вот эти группы.
[02:02.240 --> 02:18.400]  Вот эта схема, к которой мы в прошлый раз часто возвращались, что есть такая особенность,
[02:18.400 --> 02:25.040]  что на одном редьюсере может быть или один ключ или больше, чем один ключ, но если на одном
[02:25.040 --> 02:31.120]  редьюсере уже попал ключ 1, то нигде на другом редьюсере ключ K1 уже не будет.
[02:38.960 --> 02:44.440]  Кто помнит из позапрошлой лекции, почему здесь написано вот это, то есть почему блок,
[02:44.440 --> 02:49.880]  который подается на маппер, примерно равен блоку HDFS, но не в точности равен.
[02:49.880 --> 03:00.040]  Потому что они чуть меньше, может быть, или что-то такое?
[03:00.040 --> 03:07.040]  Может быть и чуть больше, может и чуть меньше, а почему? Да, вопрос от Максима,
[03:07.040 --> 03:13.960]  если K1 не вмещается в память, то это плохо, то как бы наш редьюсер будет падать,
[03:13.960 --> 03:17.800]  мы должны с этим как-то справиться. Как справиться будет чуть позже,
[03:17.800 --> 03:23.040]  но это как бы на стороне программиста. Карпухин Сергей говорит, потому что сплиты,
[03:23.040 --> 03:28.680]  а более подробно, ну сплиты, там мы называли блоки, здесь называли сплиты, что поменялось.
[03:47.800 --> 04:08.800]  Ну а что значит, зачем нам сплиты, почему нельзя просто работать с блоками,
[04:08.800 --> 04:16.000]  и почему они отличаются по размеру? Мы хотим какие-то целостные файлы
[04:16.000 --> 04:22.360]  передавать, не просто рандомно, как-то поделенные. Да, то есть блок это как бы,
[04:22.360 --> 04:28.600]  мы взяли файл и разбили его с точностью до бита, и поэтому если формат хранения
[04:28.600 --> 04:34.160]  какой-то необычный, то мы можем сломать этот формат и не сможем прочитать файл.
[04:34.160 --> 04:47.440]  Да, вот все правильно Сергей написал. Ну и давайте перед тем, как мы пойдем дальше,
[04:47.440 --> 04:56.880]  разберем несколько таких вот задачек. Есть задача grab, ну это как бы фильтрация. Как бы вы
[04:56.880 --> 05:13.520]  решали такую задачу с помощью mapReduce? Фильтровали бы в мапе. Да, то есть мап это был контент,
[05:13.520 --> 05:22.840]  здесь контент фильтра, то есть тот текст, который содержит слово, вот мы его в мапе
[05:22.840 --> 05:31.040]  проверили, и в принципе все, reducer у нас нету, shuffled sort у нас нету, а слово передается в distributed
[05:31.040 --> 05:45.440]  cache. Еще посмотрим на group by, тут как бы возможно два варианта. Если у нас агрегация есть,
[05:45.440 --> 05:53.840]  а обычно она есть, то как бы мы на мапе не делаем ничего, вот в отличие от grab,
[05:53.840 --> 06:02.840]  где мы все делали на мапе, тут мы не делаем ничего, зато мы на этапе shuffled sort группируем,
[06:02.840 --> 06:08.840]  и на reducer делаем агрегацию, если она есть, то есть если от нас требуется просто сгруппировать
[06:08.840 --> 06:14.880]  вот так, то тогда reducer не будет, а если от нас требуется сделать вот так, как здесь на этой
[06:14.880 --> 06:27.480]  схеме, то есть было три записи, мы их сломнули в одну, тогда нужно делать eluse тоже. Вот,
[06:27.480 --> 06:34.760]  ну и теперь посмотрим на вот такой вот кейс. Смотрите, это наша задача word count,
[06:34.760 --> 06:41.680]  слово единичка, вот, и мы видим, что вот таких пар у нас много с одинаковыми словами,
[06:41.800 --> 06:51.800]  их надо передать на reducer. Для того, чтобы меньше передавать данных по сети, мы можем еще на этапе
[06:51.800 --> 07:00.480]  мапера вот эти вот пары с одинаковыми ключами просуммировать, и такая функция в mapp reducer
[07:00.480 --> 07:11.160]  есть, она называется combiner, и работает она вот так, то есть если вы посмотрите на эту вот схемку,
[07:11.160 --> 07:21.200]  то она вам напомнит обычный mapp reducer, ну как бы mapper, сортировка, reducer, но на самом деле вот эта
[07:21.200 --> 07:28.440]  часть, она выполняется локально на одной ноде, то есть там, где отработал mapper, у нас выполняется
[07:28.440 --> 07:37.400]  там же сортировка, и там же выполняется combiner. Вот, и после этого уже идет глобальный shell
[07:37.400 --> 07:49.640]  class and sort, и потом reducer. Вот давайте посмотрим на эту схему еще раз с прошлого занятия, точнее.
[07:59.640 --> 08:05.320]  Что у нас тут есть? У нас отработал mapper, результаты мы записали в буфер в памяти,
[08:05.480 --> 08:10.760]  но мы помним, что я прошлый раз рассказывал, если буфера не хватает, мы скидываем это все на диск,
[08:10.760 --> 08:17.160]  но так или иначе у нас результат хранится на этой моде. Тут же мы его можем отсортировать,
[08:17.160 --> 08:22.080]  получить вот такие вот группы, и место для combiner оно вот здесь,
[08:22.080 --> 08:30.680]  то есть прежде чем мы понесем вот эти группы дальше в reducer, мы их уменьшим, мы их схлопнем.
[08:30.680 --> 08:53.640]  Вот, ну и чем отличается combiner от reducer, то есть он по сути делает похожие вещи,
[08:53.800 --> 09:01.000]  но во-первых он сортирует результат работы одного mapper, поэтому данных мало,
[09:01.000 --> 09:10.520]  и все происходит на той же ноде, значит нет передачи данных по сети. И вот combiner может
[09:10.520 --> 09:15.240]  применяться несколько раз. Чтобы понять почему так, давайте я открою другую схему.
[09:23.640 --> 09:51.360]  Это схема с предыдущей презентацией, и посмотрите как у нас работает MapReduce в деталях.
[09:51.360 --> 09:58.120]  Вот mapper, вот у нас результат хранится в буфере, но этого буфера может не хватать.
[09:58.120 --> 10:08.360]  Тогда мы делаем спилы, то есть мы скидываем лишние данные на диск.
[10:21.360 --> 10:27.720]  То есть скинули один раз, скинули второй раз, и у нас получается много вот таких кусочков данных,
[10:27.720 --> 10:34.200]  и мы начинаем их схлопывать, combining. Выполняем combiner для одного кусочка, для второго,
[10:34.200 --> 10:41.480]  для третьего. В результате вот эти спилы мы схлопываем в один файл, вот merge,
[10:41.480 --> 10:54.120]  еще раз выполняем combiner. А еще может быть такой кейс, когда у нас несколько мапперов отработали
[10:54.120 --> 11:01.880]  на одной и той же ноде. Поэтому мы обработали данные с одного маппера от combiner, но видим,
[11:01.880 --> 11:08.520]  что еще на этой же ноде у нас выполнялся другой маппер, и мы тоже можем не тащить эти два файла
[11:08.520 --> 11:15.440]  на reduce, а обработать их combiner вот здесь. Поэтому вот merge final,
[11:15.440 --> 11:26.720]  все обработали, и дальше уже вот этот померженный файл отправляем по сети на Shaft and Sort.
[11:26.720 --> 11:34.680]  То есть, как вы видите, combiner может применяться сам к себе несколько раз, а может быть такое,
[11:34.680 --> 11:45.240]  что он вообще не будет применяться. Здесь написано выраженный случай, когда у нас одна запись,
[11:45.240 --> 11:51.360]  ну или даже не одна запись, а если там две или три записи в результате маппера, то Hadoop умный,
[11:51.360 --> 11:57.320]  и он понимает, что стартовать combiner — это по сути стартовать контейнер, то есть нужны ресурсы в
[11:57.320 --> 12:03.760]  ярне, нужно стартовать виртуальную машину Java, то есть это долго и сложно. Если записей мало,
[12:03.760 --> 12:09.000]  то он запускать combiner не будет, а сразу прямиком отдаст это дело в наше отсутствие.
[12:09.000 --> 12:23.440]  Вот главная вещь, что так как combiner у нас может применяться несколько раз к одному,
[12:23.440 --> 12:32.880]  как бы к самому себе, то мы не должны менять тип ключ значения. Просто если мы поменяем и захотим
[12:32.880 --> 12:38.080]  еще раз применить combiner, то входные данные уже будут другие, и combiner может упасть.
[12:38.080 --> 12:48.320]  Так, хорошо, есть ли вопросы по combiner какие-нибудь?
[13:02.880 --> 13:21.960]  Что происходит в том случае, если combiner не применяется? Ну как бы, как мы сказали,
[13:21.960 --> 13:29.000]  combiner не меняет тип ключа значения, поэтому он не применился, мы просто дальше пошли с этим
[13:29.000 --> 13:41.200]  блоком данных работать, так как будто combiner нет. Вот здесь он не применился,
[13:41.200 --> 13:45.920]  допустим, и мы вот эти блоки не обработанные отдали дальше на reduce.
[13:59.000 --> 14:04.080]  Хорошо, есть еще какие-нибудь вопросы по combiner?
[14:04.080 --> 14:19.200]  Сейчас я найду схему.
[14:19.200 --> 14:35.640]  Если вопросов нет, давайте тогда поработаем с combiner более так практически и попробуем
[14:35.640 --> 14:43.040]  решить вот такие задачи. То есть у нас есть какие-то пары или просто может не пары,
[14:43.200 --> 14:50.360]  числа и мы хотим на reduce посчитать сумму, среднюю и медиану. Давайте подумаем,
[14:50.360 --> 14:52.560]  какой будет combiner в каждом из этих случаях.
[14:52.560 --> 15:13.040]  Вот давайте начнем с суммы. Какие тут идеи есть?
[15:13.040 --> 15:26.600]  Мы видим, после того как отсортировали, у нас подряд идут одинаковые ключи с разными
[15:26.600 --> 15:29.800]  значениями и мы просто подряд их идущие просуммируем локально.
[15:29.800 --> 15:38.360]  Да, вот все правильно, то есть по сути combiner повторяет reducer, мы считаем сумму ту же самую.
[15:38.360 --> 15:48.480]  У нас не меняется тип ключа значения, то есть было слово число и стало слово число тоже.
[15:48.480 --> 16:01.440]  Число увеличилось, но стало другое, но все равно число. А среднее надо хранить количество и сумму.
[16:01.440 --> 16:10.440]  То есть да, у нас был маппер, в маппере мы хранили word count, но если мы будем считать
[16:10.440 --> 16:16.120]  среднее и сохранять его в count, то нам получится так, что нужно считать среднее от среднего.
[16:16.120 --> 16:26.000]  А среднее от среднего это уже неправильное среднее, поэтому вот здесь должно быть слово
[16:26.000 --> 16:32.000]  единичка, то есть вместо одного каунта должно быть два числа. Я это сейчас покажу тоже на схеме,
[16:32.000 --> 16:39.280]  поэтому если кто-то не понял, то не страшно. И давайте подумаем вот здесь, что будет в такой
[16:39.280 --> 16:57.560]  компании. Разве медиана от медианы будет медианой?
[17:09.280 --> 17:19.360]  Да, компанер не может изменить типы данных, но значение это он может изменить.
[17:27.720 --> 17:33.360]  Но медиана от медианы не будет равна медиане, поэтому тут не так.
[17:40.040 --> 17:48.760]  Вот, то есть вот что такое медиана, если кто-то забыл, то мы упорядочиваем наше число и берем среднее по
[17:48.760 --> 17:58.440]  позиции. Если у нас число элементов четное, то берем два соседних и берем между ними среднее.
[17:58.440 --> 18:02.480]  То есть главное, что нам нужно данные как-то отсортировать.
[18:02.480 --> 18:16.200]  Вот, поэтому действительно на этапе мапера, на этапе комбайнера мы еще не можем глобально
[18:16.200 --> 18:22.440]  отсортировать данные, поэтому для медианы комбайнер сделать не получится. Комбайнер,
[18:22.440 --> 18:32.160]  который ничего не делает, в нем смысла нету. Ну мы просто, если он ничего не делает, то он нам
[18:32.160 --> 18:37.560]  и не нужен, мы его не подключаем просто. Вот, на комбайнере можем прочитать сумму,
[18:37.560 --> 18:44.960]  Святослав говорит, но нам нужно медиану, то есть нам не надо терять числа. Нам надо их
[18:44.960 --> 18:56.520]  отсортировать и взять среднюю позицию. Поэтому если мы сделаем сумму, то мы потеряем порядок,
[18:56.520 --> 19:02.000]  но надо еще порядок где-то хранить. Можно так что-нибудь попробовать, то есть уже с двумя
[19:02.000 --> 19:08.240]  значениями, но в принципе комбайнер нам только усложнит все. То есть комбайнер нужно применять
[19:08.240 --> 19:33.640]  не всегда. То есть видите, в редюсере комбайнер это сумма, в AVG в среднем комбайнер это сумма
[19:33.640 --> 19:38.720]  и каунд, и при этом нам надо изменить маппер. То есть если у нас было маппер слово единичка,
[19:38.720 --> 19:45.080]  то теперь чтобы у нас комбайнер не изменил ключ значения, то есть в результате комбайнера
[19:45.080 --> 20:00.480]  получилось два числа, значит тут тоже должно быть два числа. Да, можно использовать формулу для
[20:00.480 --> 20:05.520]  квантилий, ну то есть всякие статистические примочки есть, но как правильно говорит Сергей,
[20:05.520 --> 20:12.200]  это будет уже не точно оценочно. Да, ну мы пока говорим про точный подсчет, а в принципе в реальной
[20:12.200 --> 20:20.160]  жизни в таких вот задачах big data нам нужно часто что-нибудь оценить. У нас будет через пару лекций
[20:20.160 --> 20:28.120]  хайф, и там мы в хайве будем использовать приблизительные подсчеты. Вот если приблизительно,
[20:28.120 --> 20:47.600]  тогда да, тогда можно. Есть еще такая штука как in-mapper-combiner. Что это такое? Если мы до этого
[20:47.600 --> 20:55.000]  смотрели на вот эту схему, и комбайнер это был запуск какого-то процесса, какого-то java.
[20:55.000 --> 21:03.000]  Но в принципе, если мы знаем, что у нас данные устроены так, что прямо много повторяющихся слов,
[21:03.000 --> 21:10.240]  то можем уже на маппере, то есть мы знаем, что записей у нас много пар ключ значения,
[21:10.240 --> 21:18.120]  а уникальных слов мало, поэтому мы можем уже на маппере сделать дикт локальный, в этот дикт
[21:18.120 --> 21:26.440]  по записывать слова, и уже здесь будет вот такая штука, то есть приходит одинаковое слово единичкой,
[21:26.440 --> 21:35.920]  мы их тут же слопываем в слово n. Вот это очень удобно, не надо даже писать свой комбайнер,
[21:35.920 --> 21:42.120]  не надо думать про то, что ключи значения не меняются, но у такого подхода есть минус,
[21:42.120 --> 21:43.280]  кто подскажет какой.
[22:04.760 --> 22:09.880]  Вот чем плохо, если мы будем делать функционал комбайнера на маппере?
[22:09.880 --> 22:27.400]  Да, именно так. То есть вот тут, как я сказал, нам нужно хранить дикт дикшнри, чтобы по каждому
[22:27.400 --> 22:34.760]  слову считать n, и может оказаться так, что у нас таких слов очень много, и до этого словаря не
[22:34.760 --> 22:38.720]  будет хватать памяти, поэтому вот таким лучше не увлекаться в целом.
[22:38.720 --> 22:47.480]  Вот с комбайнером пока все, появились ли вопросы по поводу комбайнера?
[22:56.160 --> 23:01.880]  Так, если нет, идем дальше. Еще один элемент, это компаратор. Вот мы с вами на семинаре
[23:01.880 --> 23:07.360]  Памаприюс на ближайшем разбирали задачу, где нужно что-то отсортировать в обратном
[23:07.360 --> 23:13.000]  порядке, и оказалось, что ходоп у нас сортирует лексикографически все, как текст,
[23:13.000 --> 23:19.480]  плюс он нас сортирует только в прямом порядке, поэтому чтобы сортировать в обратном, надо
[23:19.480 --> 23:22.520]  писать вот такой костыль. Но костыль писать не хочется.
[23:22.520 --> 23:33.360]  Что можно сделать? Можно сделать свой компаратор.
[23:33.360 --> 23:43.360]  Свой компаратор можно написать только на джаме, или можно использовать уже готовые.
[23:43.360 --> 23:50.840]  Для тех, кто знает джаву, надо написать вот такой простой класс, который реализует этот
[23:50.840 --> 23:56.520]  интерфейс, и у этого класса, по сути, нужно реализовать только один метод, вот такой.
[23:56.520 --> 24:01.120]  То есть компаратор для сравнения двух объектов.
[24:01.120 --> 24:07.640]  Но это для джавистов. Для не джавистов можно использовать
[24:07.640 --> 24:09.720]  кейфилд-бейст-компаратор.
[24:37.840 --> 24:40.840]  Что здесь указано про этот компаратор?
[24:40.840 --> 24:48.880]  То, что мы его можем подключить к нашей МАП-рью задачи, но помните, вот код на стриминге,
[24:48.880 --> 24:53.960]  где мы подключали минус маппер, минус комбайнер, то есть можем подключить вот этот готовый
[24:53.960 --> 25:00.520]  компаратор и использовать всякие опции с ним. То есть по какому полю сортируем?
[25:00.520 --> 25:03.040]  Можно по нескольким полям даже.
[25:03.040 --> 25:09.600]  Дальше как сортируем, так числа. В каком порядке сортируем?
[25:09.600 --> 25:16.080]  Вот, ну тут еще есть дополнительные опции. В принципе, если хотите понять,
[25:16.080 --> 25:21.200]  какие опции есть у кейфилд-бейст-компаратора, то просто сделайте man sort.
[25:21.200 --> 25:28.560]  Вот, и опции, они в принципе совпадают с опциями команды sort.
[25:28.560 --> 25:44.960]  Вот это что касается компаратора. То есть пишем только на джаве,
[25:44.960 --> 25:49.880]  реализуем вот этот метод или используем готовый какой-то класс.
[25:49.880 --> 26:05.480]  Еще один элемент – это партишнир. Вот кто помнит с прошлой пары,
[26:05.480 --> 26:07.960]  вообще зачем нужен партишнир, что он делает?
[26:19.880 --> 26:28.000]  Ну про это точно было. Никто не помнит?
[26:28.000 --> 26:49.640]  Да, делит данные перед сортировкой. То есть нам нужно как-то обеспечить
[26:49.640 --> 26:59.080]  вот это вот. Вот, то есть чтобы данные разбились по редьюсерам, причем разбились
[26:59.080 --> 27:02.920]  правильно, чтобы у нас не были ключи разбросаны по всем редьюсерам.
[27:02.920 --> 27:08.440]  Вот, и за это отвечает именно партишнир. Что он делает?
[27:08.440 --> 27:19.600]  По умолчанию он работает вот так. Он берет хешет ключа, делит на количество редьюсеров,
[27:19.600 --> 27:28.160]  и на выходе получается номер редьюсера. Но бывает так, что нам нужно использовать
[27:28.160 --> 27:35.560]  какой-то свой партишнир. Например, у нас есть какой-то сложный ключ.
[27:35.560 --> 27:53.080]  Вот здесь какой-нибудь ключ типа field1, field2, field3 и value. Мы будем редьюсить по вот этому
[27:53.080 --> 28:00.080]  ключу всему, но мы хотим сделать так, чтобы вот все ключи, у которых одинаковый f1, попали на одну
[28:00.080 --> 28:14.160]  ноду. Вот, что при этом можно сделать? Можно сделать свой партишнир, который будет работать вот так.
[28:14.160 --> 28:35.480]  Хешет ключа 0, делит на r. То есть вот такой партишнир новый, он не нарушит вот эту нашу схему,
[28:35.480 --> 28:49.560]  просто потому что не может быть так, чтобы хешет ключа 0 был разный, а вот эти значения все были
[28:49.560 --> 28:56.280]  одинаковые. Поэтому с схемой у нас вот это не нарушится, но мы просто с помощью партишнира можем
[28:56.280 --> 29:04.240]  сделать дополнительное условие, что вот эти k2, k3, k4 будут не просто какие-то ключи, а вот ключи с
[29:04.240 --> 29:17.720]  одинаковым f1. Вот, ну и как его реализовать? Его реализовать можно тоже только на джаве или использовать
[29:17.720 --> 29:25.000]  готовый. Для тех, кто это реализует на джаве, вам нужно реализовать всего одну функцию. Вот такую,
[29:25.000 --> 29:34.000]  то есть на вход подается пара ключ значения и константа r количество редьюсеров. И дальше
[29:34.000 --> 29:40.960]  нужно на основе этих трех значений выдать номер редьюсера. Как мы это будем делать? Это вот на
[29:40.960 --> 29:52.720]  наше усмотрение. Вот, то есть мы с вами разобрали три дополнительных элемента mapReduce, то есть
[29:52.720 --> 29:58.480]  есть не только map и есть. То есть можно чтобы элементы с одинаковыми ключами попали в разные
[29:58.480 --> 30:06.560]  редьюсеры? Теоретически мы так можем сделать, но это будет нарушать mapReduce и ходу не гарантирует,
[30:06.560 --> 30:13.480]  что в этом случае мы посчитаем правильно. А тогда можно сделать, можно вообще внутри партишнира
[30:13.480 --> 30:28.200]  реализовать просто какой-нибудь рандом. Вот такую функцию условно int getPartition. Тут kvalue r и мы
[30:28.200 --> 30:40.960]  делаем там вот так. То есть вообще какой-то рандомный редьюсер, мы это раскидываем как попало,
[30:41.120 --> 30:49.480]  но потом-то у нас все сломается, потому что мы будем считать по ключам, а у нас ключ 1 есть и здесь,
[30:49.480 --> 30:54.600]  и здесь у нас будут какие-то частичные результаты, которые нужно будет потом доагрегировать.
[31:03.800 --> 31:08.720]  Ну да, написать такой код можно и он даже заработает.
[31:08.720 --> 31:19.080]  Я правильно понимаю, что это вопрос к тому, что если у нас ключей k1 очень много, вот как здесь,
[31:19.080 --> 31:24.240]  у нас много ключей k1 и они не влазят в память, что делать с ними? Ну давайте их рандомно раскидаем,
[31:24.240 --> 31:37.680]  как вариант конечно да, но нам потом придется еще отлавливать эти ключи по всем редьюсерам и как бы
[31:37.680 --> 31:43.560]  придется запускать по сути еще одну редьюсер задачу поверх этой первой. Поэтому если мы хотим
[31:43.560 --> 31:52.920]  бороться с жирным вот этим ключом, то лучше сделать не так, лучше сделать еще на этапе мапера какой-нибудь
[31:52.920 --> 32:03.680]  seed, то есть сделать k1 random например 4. Вот и все, у нас получается, что мы этот жирный ключ разбили
[32:03.680 --> 32:12.280]  на 4 других ключа и ходу будет думать, что это разные ключи. Он их посчитает, а нам потом останется
[32:12.280 --> 32:34.080]  как-нибудь вручную сложить пары типа k1 n1 и там k1 2, вот здесь k1 1 например. Вот такие пары нам
[32:34.080 --> 32:39.400]  останется сложить вручную, но их уже будет немного, мы точно знаем сколько, точно знаем как они
[32:39.400 --> 32:51.680]  выглядят, поэтому тут уже не мапредьюс, а питон нам в помощь. Да, вот если мы хотим бороться с
[32:51.680 --> 32:55.480]  несбалансированными данными, то остается вот так только подсаливать ключи.
[33:09.400 --> 33:32.280]  Хорошо, давайте разберем вот такую задачку. Это упрощенный обратный индекс. Что вообще такой
[33:32.280 --> 33:37.200]  обратный индекс? Это алгоритм, который часто используется в всяких поисковых сервисах,
[33:37.200 --> 33:45.080]  то есть у нас на вход подается список документов и какое-нибудь слово, и нам
[33:45.080 --> 33:51.560]  нужно выдать поисковую выточу слова и список документов, в которых оно нашлось. В реальной
[33:51.560 --> 33:57.520]  жизни мы там еще сортируем по встречательности этих слов, но это уже следующий этап. Давайте
[33:57.520 --> 34:04.800]  пока посмотрим на эту задачу и подумаем, как бы вы делали ее с помощью мапредьюса.
[34:34.800 --> 34:42.760]  Возможно, в маппере разделил бы контент по отдельным термам и далее уже передавал
[34:42.760 --> 34:53.600]  просто сочетание обратное, типа пару из терм и doc ID. Да, все именно так. То есть в маппе мы
[34:53.600 --> 35:04.440]  проверяем, если терм встретился, то мы формируем такую пару. Потом нам остается
[35:04.440 --> 35:11.960]  это дело сгруппировать, потому что терма может быть несколько, и у нас будут вот такие группы,
[35:11.960 --> 35:19.320]  терм и куча ID. На комбайнере мы, в принципе, делаем то же самое. То есть мы на комбайнере
[35:19.320 --> 35:25.120]  делаем группировку из вот этих пар и на редьюсере продолжаем делать группировку.
[35:25.120 --> 35:37.600]  Вот, ну и теперь давайте посмотрим на настоящий обратный индекс. То есть тут
[35:37.600 --> 35:45.640]  появляется еще TF Term Frequency, и мы не просто формируем группу из терма и кучи ID,
[35:45.640 --> 35:52.200]  а мы еще считаем у каждого документа Term Frequency и сортируем по этой Term Frequency.
[35:52.200 --> 36:08.440]  Вот, как считать Term Frequency? То есть по сути мы на вот эту вот задачу, которую здесь решили,
[36:08.440 --> 36:13.520]  мы еще накручиваем сверху word count. У нас получается на маппере Term Doc ID единичка,
[36:13.520 --> 36:22.800]  на редьюсере мы делаем суммирование Term Doc ID. Вот, но на выходе-то у нас должно быть
[36:22.800 --> 36:30.760]  выход вот такой. Term и список пар Doc ID TF Doc ID TF. То есть несмотря на то, что тут ключ Doc ID
[36:30.760 --> 36:36.440]  и Term, вот видите, то есть по сути, если мы не будем смотреть на эту последнюю строчку,
[36:36.440 --> 36:43.840]  то вот этот мап и вот этот редьюс это что? Это по сути word count. Только ключ тут стал больше,
[36:43.840 --> 36:49.760]  он стал из двух элементов, но так это word count. А дальше, чтобы получить нам вот такой выход,
[36:49.760 --> 36:56.640]  нам нужно обеспечить то, что пары с одинаковым термом придут на один редьюсер. То есть вот
[36:56.640 --> 37:09.880]  вот это самое уже partitioner, когда у нас есть Term ID и сумма, и мы пишем partitioner,
[37:09.880 --> 37:23.480]  который здесь сделает hash от терма делить на R. И тогда все пары с одинаковым термом,
[37:23.480 --> 37:30.000]  они придут на один редьюсер. Это ничего не нарушает, но мы сможем составить из них вот такие вот
[37:30.000 --> 37:38.720]  кортежи. Вот это как бы реальный случай, в котором нам нужен свой самописный partitioner.
[37:38.720 --> 37:56.400]  Если какие-то вопросы по этой части, нам еще сегодня осталось две темы, одна маленькая,
[37:56.400 --> 38:02.840]  другая большая. Если успеем, еще и третью рассмотрим, и тогда в теории мап редьюс будет закончен.
[38:08.720 --> 38:31.920]  Хорошо, тогда идем дальше.
[38:31.920 --> 38:43.800]  Вот данные на выходе мапера и редьюсера можем хранить двумя способами. Вот даже если рассмотрим
[38:43.800 --> 38:50.200]  эту задачу с термом и Doc ID, у нас Doc ID может быть много, поэтому мы их можем хранить двумя вот
[38:50.200 --> 38:57.880]  такими способами. Подход называется pairs и stripes. Первый способ, это когда мы храним терм,
[38:57.880 --> 39:06.960]  и вот тут хранится коллекция из каких-то ключей. И второй способ, когда у нас на каждое значение
[39:06.960 --> 39:14.360]  формируется своя пара, и получается много пар. Можете сказать, какие плюсы и минусы у каждого подхода?
[39:14.360 --> 39:32.520]  Первое, по памяти меньше занимает? По памяти меньше, stripes это меньше по памяти. Ну вроде
[39:32.520 --> 39:39.640]  когда, потому с ней нужно хоть раз терм повторять. Терм не надо повторять, но у нас одна запись,
[39:39.640 --> 39:47.240]  вот эта вот, она может быть очень длинной. То есть как раз в плане памяти pairs лучше,
[39:47.240 --> 39:53.360]  потому что да, записей много, но они короткие. И мы помним вот эту схему мапредьюса, и помним,
[39:53.360 --> 39:57.760]  что мапредьюст не боится того, что у нас закончится бушер. То есть мы будем эти пары
[39:57.760 --> 40:02.920]  скидывать, скидывать, скидывать в бушер. В какой-то момент бушер скажет, я переполнился, скидываю на
[40:02.920 --> 40:09.480]  диск, и будем дальше скидывать. То есть тут можно как угодно, а вот что делать с тем,
[40:09.480 --> 40:19.120]  что у нас огромная пара, на одну пару не хватило памяти, вот тут непонятно. Тут иметь в виду лучше
[40:19.120 --> 40:36.080]  данные хранить для input в маппер? Для output в маппер. Что касается памяти, stripes в плане
[40:36.080 --> 40:44.160]  памяти даже больше нагрузку дает. Но зато мы меньше передаем данных по сети, меньше пишем на диск,
[40:44.160 --> 40:51.680]  то есть пары большие, но редко. То есть взяли одну пару, забакапили вторую, и их будет не очень много.
[40:51.680 --> 40:58.680]  А что касается pairs, больше записей, то есть да, меньше нагрузка на память, потому что записи
[40:58.680 --> 41:05.520]  маленькие, и то есть тут еще плохо то, что мы вот эту большую пару собираем и храним прямо внутри
[41:05.520 --> 41:14.040]  маппера. А здесь у нас такого нет, мы просто маленькие пары скидываем в буфер, память не
[41:14.040 --> 41:21.640]  грузим, но зато мы грузим диски, потому что надо все время backup-ить данные на диск, и грузим сеть,
[41:21.640 --> 41:27.840]  потому что вот эти пары, то есть маппер отработал, он их забакапил, но потом вот эту всю кучу пар
[41:27.840 --> 41:35.160]  придется передавать дальше на редьюсер. Поэтому вот такие вот особенности.
[41:35.160 --> 41:51.360]  Хорошо, и теперь нам осталось, осталась самая большая боль маппредьюса, это joining. Именно
[41:51.360 --> 41:57.120]  реализация joining в маппредьюсе привела к тому, что стали появляться какие-то новые системы,
[41:57.120 --> 42:04.240]  в которых joining сделан проще, например там системы Hype, Spark, потому что как мы сейчас увидим,
[42:04.240 --> 42:10.160]  joining это в маппредьюсе настоящая боль, и у меня к вам два вопроса. Первое, какие типы joining вы
[42:10.160 --> 42:26.760]  помните? Left join, inner join. Да, left join, inner join, right join и full join, 4 join.
[42:26.760 --> 42:46.160]  Вот такие joining. И второй вопрос, какая самая большая проблема, если мы захотим реализовать
[42:46.160 --> 42:54.800]  join в маппредьюсе? С чем вы сразу столкнетесь? Что вам там не нравится? Есть какие-то две таблицы
[42:54.800 --> 42:58.960]  или два депо сета, и мы хотим их поджоинить.
[43:16.160 --> 43:38.800]  Есть ли какие-нибудь идеи? Какие будут проблемы вот сразу, как только мы начнем пытаться сделать
[43:38.800 --> 43:39.800]  joining в маппредьюсе?
[43:39.800 --> 44:09.720]  Хорошо, основная проблема тут в том, что депо сета два, то есть до этого
[44:09.720 --> 44:16.800]  мы получали какой-то один депо сет, у него одна структура, и мы его дальше гнали через мапп,
[44:16.800 --> 44:21.960]  потом через комбайн, потом через редьюс, а тут два депо сета совершенно разной структурой,
[44:21.960 --> 44:28.880]  и мы не можем, вообще в маппредьюсе мы не можем сказать, что вот у нас два разных маппера,
[44:28.880 --> 44:36.720]  и поэтому на таких нодах работает этот маппер, а на таких нодах работает другой маппер.
[44:36.720 --> 44:44.320]  Так как так сделать нельзя, приходится вот так вот извращаться, то есть что мы делаем?
[44:44.320 --> 44:55.120]  У нас два депо сета, ну и давайте, например, сделаем какой-то более реальный пример.
[44:55.120 --> 45:10.240]  Ну, например, у нас есть таблица студентов, ID, name, фамилия, ну и что-нибудь еще, какие-нибудь
[45:10.240 --> 45:21.240]  строковые данные. Ну и еще, что тут важно, это группа.
[45:21.240 --> 45:34.440]  И есть отдельная группа. Ну и тут тоже какие-нибудь еще строковые данные, то есть
[45:34.440 --> 45:40.480]  есть две таблички студенты группа, и когда мы хотим их поджойнить, даже без какой-то агрегации,
[45:40.480 --> 45:47.080]  просто сделать join от этих двух таблиц, мы видим, что у нас разная структура. Здесь, например,
[45:47.080 --> 45:58.600]  у нас четыре поля, а здесь два. А у нас разными ID-шниками, только в единственном экземпляре в депо сете?
[45:58.600 --> 46:04.760]  Давайте считать, что да. Ну, то есть обычная таблица уникальные ID-шники.
[46:04.760 --> 46:15.560]  То есть здесь ID-шник группа ID уникальный, а здесь ID-шник ID уникальный.
[46:15.560 --> 46:20.000]  А вот группа ID в этой таблице не уникальна.
[46:20.000 --> 46:35.320]  Да, мы можем сделать какой-то маппер, который будет эти структуры рассматривать как ID и
[46:35.320 --> 46:44.320]  какой-то объект. В объект можно запихать и вот это все, и вот это, не важно, что будет.
[46:44.320 --> 46:56.360]  Но получается другая проблема. Да, мы вот так записали, у нас отработал маппер, но дальше как
[46:56.360 --> 47:02.200]  из этого объекта нам восстановить данные? Как понять, где хранится вот это, где хранится вот это?
[47:02.200 --> 47:08.160]  Опять кастить, опять пытаться этот объект распарсить не очень хорошо, поэтому мы еще храним тег.
[47:08.160 --> 47:18.080]  И в этом теге мы укажем, все-таки откуда пришла эта стака. Из первого датасета или
[47:18.080 --> 47:21.840]  из второго. Тег это A и B, тут мы видим.
[47:35.840 --> 47:43.800]  И получается, вот это будет группа ID, как раз таки. Да, это будет группа ID.
[47:48.800 --> 47:55.640]  Вот, то есть добавили тег. Ну и теперь нам нужно эти данные отсортировать, группировать, то есть
[47:55.640 --> 48:05.920]  запустить обычную стадию mapReduce. После этого у нас вот эти пары приходят на Reducer, сгруппированные
[48:05.920 --> 48:14.280]  по группой ID. То есть у нас придут пары из тега равно A, из тега равно B, и нам останется уже
[48:14.280 --> 48:26.680]  на стороне Reducer реализовать join. То есть, по сути, у нас на один Reducer придет группа ID
[48:26.680 --> 48:37.080]  равно, допустим, один и куча вот таких пар. И значит, на выходе у нас будет, что у нас будет
[48:37.080 --> 48:43.960]  на выходе, у нас будет куча строк, группа ID равно один. И также вот эти поля.
[48:43.960 --> 48:54.520]  Зависит же от join. В смысле, зависит от join? В смысле, какой join реализован, left join?
[48:54.520 --> 49:01.800]  Ну, не важно, все равно будет такая структура, то есть все равно останется ID, и вот какие-то
[49:01.880 --> 49:08.040]  здесь вещи. Left join, right join и inner join, он влияет только на количество записей,
[49:08.040 --> 49:16.920]  которые у нас будут на выходе. Тут реализуется уже логика в Reducer,
[49:16.920 --> 49:26.200]  логика join. Как мы напишем код, так он и будет. Я к тому, что в одном join у нас,
[49:26.200 --> 49:37.240]  получается, будет просто один результат. Еще раз, не совсем понял. Но у нас же в одной таблице только
[49:37.240 --> 49:43.960]  единственный элемент групп ID и номер. Получается, если при каком-то join у нас будет единственный.
[49:43.960 --> 49:59.800]  Там скорее связано с тем, что у нас где-то будут нулы, то есть где-то может быть группа,
[49:59.800 --> 50:05.960]  у которой нет студентов, или студент, у которого нет группы, и там будет null. И мы вот эти нулы выкидываем.
[50:14.200 --> 50:22.840]  Хорошо, а есть еще такая оптимизация join, когда у нас датасет, один из датасетов маленький. Сейчас я
[50:22.840 --> 50:34.960]  тоже покажу. Если у нас оба датасета большие, вот такие, то нам ничего не остается, кроме как их
[50:34.960 --> 50:42.760]  тегать, потом сортировать и редюсить на Reducer. А если у нас один из датасетов маленький, и мы
[50:42.760 --> 50:48.480]  можем целиком его считать в памяти, то тогда получится, что мы имеем кусочек датасета A и
[50:48.480 --> 50:54.720]  весь датасет B, и уже здесь мы можем этот кусочек поджоинить. Потом здесь поджоинили, здесь поджоинили.
[50:54.720 --> 51:02.560]  Получили такой частичный join, и редюсера у нас нету, но в принципе можно сделать редюсер,
[51:02.560 --> 51:11.160]  который просто сделает union. Хотя union можно сделать и без редюсера, то есть просто записи
[51:11.160 --> 51:19.080]  у нас пишутся в HDFS, и потом они все пишутся в одну папку, у нас по сути union делается автоматически.
[51:19.080 --> 51:29.240]  Вопрос всегда ли можно реализовать вот такой mapsite join, точнее все ли типы join можно так
[51:29.240 --> 51:34.560]  реализовать. Вот у нас есть один большой датасет, другой маленький, и можно ли здесь реализовать
[51:34.720 --> 51:37.600]  write join, live join, или будут какие-то проблемы.
[51:37.600 --> 52:07.080]  Давайте придумаем какой-нибудь пример. Есть датасет A, и у него пары типа 1,
[52:07.080 --> 52:29.600]  2, на другом мапере 3, 4, на третьем мапере 5, 6, и датасет B у которого
[52:29.600 --> 52:56.520]  1 value, например 3 value и 4 value. Да, Святослав написал, нельзя реализовать live join,
[52:56.520 --> 53:21.920]  делается это большой датасет. Почему? Ответ правильный, но хочется услышать пояснение.
[53:22.320 --> 53:38.360]  Да, только часть датасета из A будет joining с B, то есть что у нас получается? Если live join,
[53:38.360 --> 53:43.600]  это значит, что весь A должен быть под joining с B, и где-то будут нулы.
[53:43.600 --> 54:08.920]  То есть что у нас получается в случае с live join? У нас получается, вот мы например,
[54:08.920 --> 54:24.080]  joining B и первый мапер. У нас получается такая штука. 1 value, например под joining value и оттуда,
[54:24.080 --> 54:29.520]  дальше 2, а двух у нас нету.
[54:54.080 --> 55:16.360]  Да, давайте идти лучше по вот этому датасету, то есть 1 мы нашли, дальше 3, а 3 получается value null,
[55:16.360 --> 55:34.520]  потом 4 у нас снова value null, а дальше берем этот мапер, и тут у нас получается как раз 1 value null,
[55:34.520 --> 55:51.840]  а 3 и 4 есть. Вот и получается, что у нас будут вот такие вот пары,
[55:51.840 --> 56:00.280]  которые будут повторяться, где-то будут нулы, а где-то будет значение. И нам придется после вот
[56:00.280 --> 56:05.880]  такого live join еще делать маперию задачу, которая будет эти пары искать и схлопывать.
[56:05.880 --> 56:15.040]  Вот основная проблема в этом. Это кажется, что в live join просто нулы же не нужны,
[56:15.040 --> 56:21.760]  эти хранительные. Наоборот это будет проблема в right join и full join, или я что-то не понимаю.
[56:21.760 --> 56:35.400]  В full join тоже такая же проблема будет с нулами. Да, кажется, Саша прав, это не left join, а right join.
[56:35.400 --> 56:39.280]  А, ну да, точно, это right join, вот такая проблема тут есть.
[56:52.760 --> 56:59.880]  Ну с left join в принципе даже проще, у нас там нет этих нулов, и обрабатывать ничего не нужно будет.
[56:59.880 --> 57:14.160]  Есть еще одна оптимизация, называется bucket side join. То есть здесь было понятно, датасет большой,
[57:14.160 --> 57:20.160]  датасет маленький. А вот здесь датасет, конечно, маленький, но считать его в distributed cache
[57:20.160 --> 57:26.560]  целиком мы не можем, он не настолько маленький. Поэтому мы его бьем по частям и считаем каждую
[57:26.560 --> 57:32.960]  часть в distributed cache на разных маперах. Здесь первая часть, вторая и третья. А здесь тоже кусочки
[57:32.960 --> 57:38.640]  датасета. И давайте посмотрим теперь, а какой тип join вот здесь нельзя реализовать.
[57:38.640 --> 57:44.560]  Если у нас и там кусочки датасета, и там кусочки датасета.
[57:44.560 --> 57:50.280]  А имеется в виду, что мы эти кусочки делаем одинаковым partition, то есть типа у нас
[57:50.280 --> 57:56.040]  ключи падут из разных датасетов все равно в одного только мапера, или как?
[57:56.040 --> 58:05.240]  Ключи из разных датасетов нет, пока нет, пока просто мы разбили датасет как-то по сплетам,
[58:05.240 --> 58:07.040]  и тут как-то его посплетили.
[58:07.040 --> 58:33.520]  Какие типы join здесь нельзя реализовать? На самом деле никакие. Таким способом ни один join
[58:33.520 --> 58:38.800]  реализовать не получится, потому что у нас и в случае left join будут лишние,
[58:38.800 --> 58:45.280]  и в случае right join будут лишние нулы. Ничего не получится, поэтому для чего же нам этот bucket
[58:45.280 --> 58:50.800]  side join нужен? Вот как правильно сейчас кто-то из вас сказал, что нам нужно подготовить данные.
[58:50.800 --> 58:59.440]  И вот если у нас будут ключи, которые есть в кусочке одинцев, падать с ключами,
[58:59.440 --> 59:05.680]  которые есть в этом кусочке один, то тогда мы просто можем сделать маленькие join независимые
[59:05.680 --> 59:12.440]  на каждом мапере, и потом сделать union. Но нужно заранее подготовить данные и знать,
[59:12.440 --> 59:24.880]  как мы их разбиваем. Есть ли какие-то вопросы под join? Мне пока нужно минута,
[59:24.880 --> 59:36.480]  чтобы найти еще презентацию. Насколько вообще корректно делать join мы предьюсом,
[59:36.480 --> 59:42.440]  если в целом большие базы оптимизированы под то, чтобы делать join на огромных масштабах?
[59:42.440 --> 59:49.360]  Большие базы, ты имеешь в виду всякие там HBA из вот этого все, да?
[59:49.360 --> 59:57.440]  Вообще, ну я меньше опыта имел с ними, поэтому не знаю, но кажется, что люди ж продакшен не
[59:57.440 --> 01:00:01.760]  используют там какие-нибудь позгрессы и все остальное. Как-то им приходится, наверное,
[01:00:01.760 --> 01:00:09.760]  иногда join. Ну смотри, позгрессы, если мы говорим про одну машинку, то есть база,
[01:00:09.760 --> 01:00:15.960]  которая работает на одном сервере, там такой проблемы вообще нет. То есть там есть свои
[01:00:15.960 --> 01:00:22.240]  оптимизации, индексирование, и join на одной машинке сделать можно. А если у нас машинок несколько,
[01:00:22.240 --> 01:00:30.160]  и мы хотим сделать какой-то распределенный join, то там будет вот такой метод использовать.
[01:00:30.160 --> 01:00:37.480]  Ну зависимость от того, есть мапредьюс или нет, если это что-то поверх мапредьюса,
[01:00:37.480 --> 01:00:43.040]  тот же HBase, про который я сказал, или тот же Hive, там будет использоваться вот такой вот
[01:00:43.040 --> 01:00:50.360]  сложный метод. Но правда, в продакшене мы вот это все писать сами не будем, мы этого даже
[01:00:50.360 --> 01:00:56.200]  видеть не будем. Мы напишем команду join, а она уже сама поставит теги, сделает мапредьюс,
[01:00:56.200 --> 01:01:10.120]  и мы этого не увидим. Хорошо, есть какие-нибудь еще вопросы? Пока я ищу презу.
[01:01:26.200 --> 01:01:40.360]  Хорошо, если вопросов нет, тогда идем дальше, мы сегодня успеем как раз последнюю тему,
[01:01:40.360 --> 01:01:49.560]  это планировка задач. Вот есть такая система yarn, мы про нее уже немного говорили на семинарах,
[01:01:49.560 --> 01:01:58.360]  когда смотрели на вот эти вот application UI. В общем, очередь задач, и у нас получается много
[01:01:58.360 --> 01:02:05.680]  пользователей, много нод на кластере, надо как-то организовать очередь так, чтобы задачи между
[01:02:05.680 --> 01:02:13.400]  собой не конфликтовали. А тот yarn, который в Javascript, это другой yarn? Совсем другой,
[01:02:13.560 --> 01:02:25.280]  это просто совпали аббревиатуры, ничего общего нет. Ну и как бы у нас постановка задачи вот такая,
[01:02:25.280 --> 01:02:33.280]  у нас приходит одно приложение, потом приходит второе приложение, и нужно как-то решить вопрос
[01:02:33.280 --> 01:02:39.680]  с ресурсами. Первое самое простое, что приходит в голову, это сделать очередь. То есть первое
[01:02:39.680 --> 01:02:49.240]  приложение пришло, работает-работает-работает, пришло второе и оно ждет, пока первое не отработает.
[01:02:49.240 --> 01:02:58.960]  Какие плюсы и минусы вот такого подхода, как вы думаете? Ну плюс, что просто реализовать,
[01:02:58.960 --> 01:03:04.320]  минус в том, что вторая задача может на очень долго зависнуть, если первая там застакает,
[01:03:04.320 --> 01:03:10.960]  или просто долго. Да, именно так. Реализовать очень просто, а у разных программ могут быть разные
[01:03:10.960 --> 01:03:16.080]  требования, то есть вот этой второй программке может считаться одну минуту, а она будет ждать
[01:03:16.080 --> 01:03:25.120]  два дня, пока посчитается первая, висеть в очередь. Вот поэтому второй вариант, это capacity
[01:03:25.120 --> 01:03:33.960]  scheduler, и здесь у нас каждому приложению дается определенное capacity, то есть какое-то выделенное
[01:03:33.960 --> 01:03:39.080]  количество ресурсов. Вот мы, например, знаем, что первое приложение у нас жирное, мы ему дали
[01:03:39.080 --> 01:03:44.800]  75 процентов, а второму дали 25 процентов, и оно будет спокойно работать и никому не мешать.
[01:03:44.800 --> 01:04:01.960]  Какие здесь плюсы и минусы? Не до конца железа тратим. Да, то есть у нас вот эти белые куски есть,
[01:04:01.960 --> 01:04:10.400]  которые по сути это простой. То есть на самом деле вот такой capacity scheduler, он подходит для того,
[01:04:10.400 --> 01:04:16.320]  когда у нас программа работает постоянно. То есть есть вот какой-то сервис, который мы запустили,
[01:04:16.320 --> 01:04:26.520]  и он постоянно работает в реал тайме, отжирает 25 процентов ресурсов своих и все. То есть для
[01:04:26.520 --> 01:04:32.800]  варианта он подходит. Сделать тоже несложно, но присвоил capacity каждому приложению и работаешь.
[01:04:32.800 --> 01:04:41.320]  Вот, но минусы это то, что вот есть вот эти белые куски, которые простаивают. Ну и еще сказано,
[01:04:41.320 --> 01:04:50.960]  что разные требования в разное время. Например, у нас может быть в задаче 90 блоков, значит будет
[01:04:50.960 --> 01:04:58.880]  90 сплитов и будет 90 мапперов. Потом мы сделаем какой топ и после этого все погоним на сортировку
[01:04:58.880 --> 01:05:05.360]  на один reducer. Например, такая задача. И у нас будет использоваться то 90 контейнеров, то один.
[01:05:05.360 --> 01:05:15.600]  И получится, что нам нужно какое capacity сделать? Если capacity сделать маленькое, то что будет? У нас
[01:05:15.600 --> 01:05:21.000]  будут вот эти 90 мапперов очень долго ждать. Например, capacity позволяет только 10 мапперов.
[01:05:21.000 --> 01:05:29.840]  Как при этом будет работать задачка? Мапперы все будут работать, все в порядке, но только они будут
[01:05:29.840 --> 01:05:36.360]  идти порциями по 10. Первые 10 отработали, закончили, вторые 10 и вот так будет вот такая локальная очередь
[01:05:36.360 --> 01:05:43.760]  образовываться с мапперов. А если очень большое capacity поставим, да мапперы отработают быстро,
[01:05:43.760 --> 01:05:56.480]  но зато на этапе reducer все будет остаивать. И третий планировщик, это так называемый честный
[01:05:56.480 --> 01:06:04.480]  планировщик, он умеет динамически выделять ресурсы. То есть вот пришло первое приложение,
[01:06:04.480 --> 01:06:12.640]  ну пока никого нет, бери все. Потом пришло второе и отбирает на себя часть ресурсов. Все, конечно,
[01:06:12.640 --> 01:06:21.400]  хорошо и fairshedule, он самый такой, самый гибкий. Но вот если посмотреть именно на эту схему,
[01:06:21.400 --> 01:06:34.880]  какие вот тут вы проблемы видите? Ну а как мы вообще возьмем посреди вышесрения отберем ресурс?
[01:06:34.880 --> 01:06:40.040]  Вот, именно так. То есть то, что у нас есть в этой схеме, это значит, что у нас работало,
[01:06:40.040 --> 01:06:45.600]  работало первое приложение, потом второе пришло и говорит, хочу ресурсов. Планировщик
[01:06:45.600 --> 01:06:54.520]  берет и убивает половину мапперов. То, что они до этого момента работали и отработали почти там
[01:06:54.520 --> 01:07:02.200]  на 90 процентов, это никого не волнует и мы просто убьем. И по сути это все равно, что если бы было
[01:07:02.200 --> 01:07:09.120]  вот так, белое. То есть все равно, вычисления, которые были здесь, их прибили, результата нет,
[01:07:09.120 --> 01:07:14.760]  и они получаются бесполезные. Что в этом случае можно сделать? Можно использовать вот такой подход.
[01:07:14.760 --> 01:07:23.360]  То есть мы выделяем ресурсы не сразу, а вот такими ступеньками. То есть мы не убиваем мапперы,
[01:07:23.360 --> 01:07:29.520]  а мы просто ждем, отработал один маппер, значит все, этой задачей ресурсы больше не даем,
[01:07:29.520 --> 01:07:35.760]  отдаем другой. Отработал следующий маппер, опять забрали, опять забрали. Именно так у нас сделано
[01:07:35.760 --> 01:07:45.080]  на кластере. Вы тоже можете увидеть, что вот у нас запустили какое-то приложение и будет написано,
[01:07:45.080 --> 01:07:50.440]  сначала она использует там 10 контейнеров, потом 8, 7, 6 и так далее, меньше и меньше,
[01:07:50.440 --> 01:07:53.680]  особенно если на кластере живет много пользователей.
[01:08:06.680 --> 01:08:11.240]  Вот хорошо. Какие плюсы и минусы у этого подхода? Они тоже есть.
[01:08:28.560 --> 01:08:32.880]  Плюс понятно, что гибкое планирование, а вот какие минусы тут есть?
[01:08:35.760 --> 01:08:53.720]  Может быть какая-нибудь гранулярность ресурсов, то есть у нас как-то из-за маленького
[01:08:53.720 --> 01:08:59.200]  количества ресурсов, но не очень. Мы не сможем всю джобу сразу запустить,
[01:08:59.200 --> 01:09:02.440]  но не джобу, таску сегодня сможем запустить, к которому хотим.
[01:09:02.440 --> 01:09:13.640]  Всю таску? Ну да, мы сможем запустить ее частями, то есть мы будем постепенно выделять ресурсы,
[01:09:13.640 --> 01:09:21.320]  но по-другому никак не сделаешь. То есть да, постепенно, но это лучше, чем сразу прибить пол
[01:09:21.320 --> 01:09:33.400]  таски или сидеть в очереди и долго ждать. По поводу гранулярности, то есть yarn работает в
[01:09:33.400 --> 01:09:39.040]  терминах контейнеров, то есть вот эти все ступенечки это работа с контейнерами. Что
[01:09:39.040 --> 01:09:45.360]  такое контейнер? Это некая такая абстракция, на которую выделяется, например, одно ядро
[01:09:45.360 --> 01:09:52.320]  процессора и один гига оперативки, или там два ядра процессора, два гига оперативки. То есть
[01:09:52.320 --> 01:09:58.240]  вот все ресурсы, которые есть у нас на кластере, они бьются на такие контейнеры в терминах yarn,
[01:09:58.240 --> 01:10:05.240]  и yarn работает именно с ними. Размер ресурсов на самом контейнере, он тоже может меняться.
[01:10:05.240 --> 01:10:13.320]  Например, один маппер работает в одном контейнере, один рьюсер работает в одном контейнере. Если у
[01:10:13.320 --> 01:10:19.400]  нас в маппере такой код, что ему в процессе работы нужно чуть больше ресурсов, он будет до
[01:10:19.400 --> 01:10:26.160]  какого-то момента эти ресурсы давать в один контейнер. Если этот один контейнер хочет очень
[01:10:26.160 --> 01:10:34.120]  жирный ресурсов, то он его просто прибьет. Мы такое будем встречать в Spark. Очень часто встречается,
[01:10:34.120 --> 01:10:44.480]  что вот у нас контейнер, в контейнере работает какое-нибудь преобразование над Spark'овским датасетом,
[01:10:44.480 --> 01:10:56.760]  и мы в процессе преобразования просим еще ресурсы, просим-просим, нам всего не хватает,
[01:10:56.760 --> 01:11:02.200]  и yarn в какой-то момент говорит контейнеру, ты слишком много попросил ресурсов, я тебя убью.
[01:11:02.200 --> 01:11:09.920]  Убивает контейнер, рестартует, ну а после рестарта мы же запускаем на этом контейнере все равно один и тот же код,
[01:11:09.920 --> 01:11:16.400]  поэтому этот код снова начинает просить ресурсы, снова жирнет-жирнет, его снова убивают. Ну и так
[01:11:16.400 --> 01:11:26.760]  происходит несколько раз, пока не упадет все приложение. Такой подход есть. Ну и вот что еще
[01:11:26.760 --> 01:11:32.680]  из этого следует, раз у нас четко так разбито по контейнерам, то непонятно, что делать вот в таком
[01:11:32.680 --> 01:11:40.440]  случае, что ресурсы бывают разные. Например, у нас какой-то маппер хочет много процессоров и мало
[01:11:40.440 --> 01:11:51.360]  памяти, потом мы переходим на reduce, а там работает по-другому. Много памяти, мало процессор. То есть
[01:11:51.360 --> 01:11:57.000]  получается, что мы пробили данные на контейнеры, но в контейнерах у нас все время что-нибудь будет
[01:11:57.000 --> 01:12:05.560]  простаивать. Или процессоры, или память. А есть вот еще, в одной из новых версий Hadoop появились
[01:12:05.560 --> 01:12:13.280]  еще видеокарты, то есть еще один такой ресурс, и нам надо как-то между этим балансировать. Как
[01:12:13.280 --> 01:12:21.440]  это сделать? На самом деле тут уже идет ручная настройка. То есть мы говорим, что на маппер мы
[01:12:21.440 --> 01:12:29.080]  выделяем контейнеры с таким количеством процессовой памяти на редьюсере, вот с таким количеством. Для
[01:12:29.080 --> 01:12:38.520]  этого есть специальные настройки, то есть там можно указать map minimum bytes, reduce максимум
[01:12:38.520 --> 01:12:44.000]  ram. Вот все это можно указать и вот сделать такую тонкую настройку для контейнеров.
[01:12:44.000 --> 01:12:49.880]  Вот есть ли какие-нибудь вопросы еще?
[01:13:08.520 --> 01:13:26.760]  Мы можем как-то динамически писать приложения, которые пытаются там, допустим, выделить память
[01:13:26.760 --> 01:13:34.040]  и выделяют ее до тех пор, как это возможно. А как только операционная система или контейнер ответила,
[01:13:34.040 --> 01:13:45.120]  что все хватит, она использует то, что ей дали? Да, так можно. То есть мы же знаем, до какого предела
[01:13:45.120 --> 01:13:49.200]  наш контейнер может расти, и мы можем на этот предел повесить проверку.
[01:14:04.040 --> 01:14:11.840]  Окей, есть ли какие-нибудь еще вопросы?

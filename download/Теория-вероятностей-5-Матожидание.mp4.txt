[00:00.000 --> 00:18.880]  На чём мы с вами в прошлый раз остановились? Мы с вами говорили про многомерное распределение,
[00:18.880 --> 00:24.160]  про тиремовый в конце, я напомнил про тиремовый вине, который мы применили,
[00:24.160 --> 00:28.720]  чтобы доказать форму свёрки. В общем, про распределение простучальной величины,
[00:28.720 --> 00:34.200]  простучальные векторы, я самое главное сказал. Давайте теперь будем говорить про некоторые
[00:34.200 --> 00:37.360]  характеристики распределения, которые нам впоследствии пригодятся, когда мы
[00:37.360 --> 00:43.880]  всякие предельные теории будем доказывать. В частности, ещё эти характеристики распределения,
[00:43.880 --> 00:49.240]  о которых мы будем сегодня говорить, они важны со статистической точки зрения. В частности,
[00:49.240 --> 00:54.320]  математическое ожидание – это, грубо говоря, среднее значение, которое принимает случайная
[00:54.320 --> 01:06.520]  величина. Как это можно понять? Можно это понять имперически, имея в виду, что среднее значение
[01:06.520 --> 01:11.600]  случайной величины – это если бы вы могли это случайную величину много-много раз независимо
[01:11.600 --> 01:19.160]  реализовать, провести много-много экспериментов, независимо в ходе которых ваш случайная величина
[01:19.160 --> 01:24.640]  приняла бы то или иное значение. То есть, если проводить аналоги с температурой,
[01:24.640 --> 01:28.520]  допустим, вы измеряете температуру при одних и тех же условиях в одном и том же месте много-много
[01:28.520 --> 01:33.320]  раз, и получаете разные какие-то значения, и потом вы считаете просто империческое среднее,
[01:33.320 --> 01:41.680]  то есть складываете и делите на количество. Оказывается, математическое ожидание,
[01:41.680 --> 01:48.560]  о котором мы сегодня будем говорить, очень хорошо этим империческим приближается. Если вы будете в
[01:48.560 --> 01:53.160]  реальность проводить какой-то эксперимент, и у вас будет правильное соображение на тему того,
[01:53.160 --> 01:57.480]  что за распределение вашей случайной величины, если вы посчитаете империческое среднее и
[01:57.480 --> 02:03.480]  теоретическое, они окажутся очень близки. Что же такое это самое теоретическое среднее,
[02:03.480 --> 02:16.320]  ну то есть математическое ожидание. Но это, на самом деле, если что иное,
[02:16.320 --> 02:23.840]  как просто интеграл либега от вашей случайной величины по вероятностной мере. То есть,
[02:23.840 --> 02:32.880]  у вас есть вероятностное пространство омега фп, есть какая-то случайная величина,
[02:32.880 --> 02:43.520]  которая действует из омега вр, на этом вероятностном пространстве. Ну и вы полагаете,
[02:43.520 --> 02:50.560]  просто по определению, что мат ожидания кси, это есть не что иное, как интеграл по омега
[02:50.560 --> 03:02.000]  от кси dp. То есть, просто интеграл либега от вашей функции кси по мере p.
[03:02.000 --> 03:17.920]  Значит, оказывается, есть такая теорема либега, теорема, прошу прощения, о замене переменных
[03:17.920 --> 03:26.840]  интеграль либега, которая утверждает, что это есть не что иное, как интеграл по r от x dp кси.
[03:26.840 --> 03:35.920]  Да, то есть, на самом деле, вы можете интегрировать, вы можете перенести вероятностную меру вр,
[03:35.920 --> 03:43.720]  и интегрировать просто x по соответственно этой мере будет тоже самое. Иван Генрихович рассказывал,
[03:43.720 --> 03:53.560]  что такое теорема о замене переменных в интеграле либега? У нас на мата не было.
[03:53.560 --> 04:01.600]  Можно просто, почему этот интеграл, почему вообще кси интегрируема по мере p?
[04:01.600 --> 04:07.480]  Она может быть, конечно, не интегрируема, но если она интегрируема, то это с томат
[04:08.480 --> 04:17.360]  Если она не интегрируема, то мат ожидания не существует. То есть, если интеграл не существует, то мат ожидания не существует тоже.
[04:27.800 --> 04:35.480]  Как у нас вообще определяется интеграл либега? Мы говорим, что мы сначала его определяем для не отрицательных функций.
[04:35.480 --> 04:44.480]  То есть, мы можем представить кси как кси плюс минус кси минус, и где и кси плюс и кси минус это не отрицательные функции,
[04:44.480 --> 05:01.480]  которые определяются вот так. Кси плюс это максимум из кси нуля, а кси минус это максимум из минус кси нуля.
[05:05.480 --> 05:15.480]  И находим, понятно, что у не отрицательных функций, в случае случайных величин, интеграл либега существует, но может быть бесконечный.
[05:15.480 --> 05:26.480]  В том случае, когда только одна из этих величин бесконечна, если обе конечны, если там и интеграл от кси плюс,
[05:26.480 --> 05:38.480]  то это означает, что и мат ожидания кси тоже бесконечна.
[05:38.480 --> 05:51.480]  Если один из них бесконечен, ну скажем первый, а второй конечен, то это означает, что мат ожидания равна бесконечности.
[05:51.480 --> 06:01.480]  В этом случае будет плюс бесконечность, а в случае, когда интеграл кси плюс ДП конечен, а второй интеграл бесконечен,
[06:01.480 --> 06:09.480]  то это значит будет минус бесконечность. Ну и в случае, когда оба интеграла бесконечны, то есть,
[06:09.480 --> 06:16.480]  иными словами, интеграл либега не существует, то и мат ожидания кси тоже не существует.
[06:16.480 --> 06:25.480]  Ну короче говоря, это просто интеграл либега, он существует или не существует точно тогда же, когда и существует или не существует интеграл либега.
[06:25.480 --> 06:34.480]  Это тут просто терема о замене переменных. Она утверждает следующее.
[06:34.480 --> 06:42.480]  Ну в общем-то это и есть утверждение теремы о замене переменных. У вас есть какая-то функция кси от Омега, и вы ее заменили на х.
[06:46.480 --> 06:51.480]  Вы сделали такую замену, и в результате у вас интеграл стал по r.
[06:51.480 --> 06:58.480]  Вместо интегрирования по мере p, вы интегрируете по той мере, которая при такой замене у вас просто получается.
[06:58.480 --> 07:04.480]  У вас мера p переходит в распределение случайного чдх7.
[07:04.480 --> 07:10.480]  Ну хорошо, есть какие-то вопросы?
[07:10.480 --> 07:15.480]  Ничего нового я пока не сказал, просто сказал, что мы должны посчитать интеграл либега, чтобы получить мат ожидания.
[07:15.480 --> 07:20.480]  Давайте в каких-то частных случаях посмотрим что будет происходить.
[07:20.480 --> 07:25.480]  У нас интересуют случаи дискретных распределений, и случаи абсолютнепрерывных распределений.
[07:25.480 --> 07:31.480]  В archive по поводу распределения я не помню говорил ли мне это, я на всякий случай еще повторю.
[07:31.480 --> 07:35.480]  Есть еще phr обłbyку.
[07:35.480 --> 07:44.480]  Есть еще так называемое сингулярное распределение.
[07:44.480 --> 07:48.880]  что такое сегулярное распределение? Оно очень похоже на дискретно, то есть на самом деле дискретно это частный случай.
[07:48.880 --> 07:57.800]  То есть это такое распределение, что мером множества, на котором сосредоточена вся вероятность,
[07:57.800 --> 08:05.800]  то есть вы нашли такое множество х, что его мера равна единице, при этом его мера либега равна нулю.
[08:05.800 --> 08:10.560]  Когда мы говорим про дискретное распределение, вот этот носитель множества х, оно конечное или счётное.
[08:10.560 --> 08:15.560]  Но понятно, что этими ситуациями не ограничиваются множество, у которых либега мера равна нулю.
[08:15.560 --> 08:20.560]  Можно взять какое-то континуальное множество, у которого мера либега равна нулю.
[08:20.560 --> 08:28.560]  И тогда, если вы задаете такое распределение вероятностей, что оно будет полностью сконцентрировано в этом множестве,
[08:28.560 --> 08:31.560]  то есть вероятность этого множества на единице, то это распределение будет сегулярным.
[08:31.560 --> 08:34.560]  Дискретное распределение получается частный случай.
[08:34.560 --> 08:41.560]  Почему это важно отметить?
[08:41.560 --> 08:47.560]  Потому что вообще все распределения представляются как линейная комбинация абсолютно непрерывного и сингулярного.
[08:47.560 --> 08:54.560]  Поэтому на самом деле с помощью абсолютно непрерывных и сингулярных распределений можно классифицировать вообще все.
[08:54.560 --> 08:59.560]  Но это такая теоретическая вещь, у нас в курсе совершенно нигде не будет возникать.
[08:59.560 --> 09:04.560]  Эти распределения мы говорить не будем, поэтому оставим это за кадром.
[09:04.560 --> 09:07.560]  Хорошо, так вот.
[09:07.560 --> 09:12.560]  Когда мы считаем мат ожидания, нам важно понять, как оно считается в дискретном случае и в абсолютно непрерывном случае.
[09:12.560 --> 09:22.560]  Давайте начнём с простейшего, с дискретного.
[09:22.560 --> 09:24.560]  Пусть это кси, это дискретная случайная величина.
[09:24.560 --> 09:26.560]  Что это значит?
[09:26.560 --> 09:29.560]  Это значит, что есть какой-то носитель.
[09:29.560 --> 09:41.560]  Х, носитель этого распределения, он не более чем счёт.
[09:41.560 --> 09:46.560]  Всё распределение случайной величины кси сосредоточено на этом множестве.
[09:46.560 --> 09:48.560]  Тогда во что превращается наш интеграл либега?
[09:48.560 --> 09:51.560]  Понятно, это просто есть сумма.
[09:51.560 --> 09:56.560]  Давайте смотреть на второй интеграл.
[09:56.560 --> 10:01.560]  Так как вот это распределение является дискретным, то интеграл либега – это просто…
[10:01.560 --> 10:06.560]  То есть вы интегрируете простую функцию данную или функцию, которая принимает счётное количество значений.
[10:06.560 --> 10:17.560]  То есть вы на самом деле получаете сумму по всем х маленьким из х большого, х умножить на вероятность того, что кси равняется х.
[10:18.560 --> 10:30.560]  Ну вот, в общем-то и всё. Давайте в каких-то ситуациях посчитаем, чему будет равно от ожидания.
[10:30.560 --> 10:36.560]  Разберём примеры распределения.
[10:36.560 --> 10:43.560]  Ну, во-первых, распределение Бернули.
[10:43.560 --> 10:45.560]  То есть, что такое распределение Бернули?
[10:45.560 --> 10:50.560]  Что я имею в виду, когда я пишу кси – волна Бернули с параметром p.
[10:50.560 --> 10:57.560]  Я имею в виду, что распределение случайной величины кси, то есть вот это вот p кси, это вероятность на меры,
[10:57.560 --> 10:59.560]  которая устроена следующим образом.
[10:59.560 --> 11:09.560]  p кси от единицы – это p, а p кси от нуля – это 1-п.
[11:09.560 --> 11:12.560]  То есть вероятность того, что случайная величина приняла значение 1, равняется p.
[11:12.560 --> 11:16.560]  Вероятность того, что случайная величина приняла значение 0, равняется 1-п.
[11:16.560 --> 11:25.560]  Поэтому от ожидания кси – это, конечно, просто 1 умножить на p, плюс 0 умножить на 1-п, то есть p.
[11:30.560 --> 11:35.560]  Ну окей, давайте посмотрим на биномиальное распределение с параметром p.
[11:35.560 --> 11:42.560]  Я напоминаю, что это означает, что вероятность того, что случайная величина попала в точку k,
[11:42.560 --> 11:49.560]  это в точности csn по k на p в степени k на 1-p в степени аминус k.
[11:49.560 --> 11:55.560]  При этом k пробегает значение от 0 до n, иначе вероятность равна 0.
[11:55.560 --> 12:01.560]  Ну понятно, тогда можем опять же по нашей форме посчитать от ожидания кси, это будет сумма просто по всем k от 0 до n.
[12:01.560 --> 12:06.560]  k умножить на csn по k на p в степени k на 1-p в степени аминус k.
[12:06.560 --> 12:15.560]  Ну, сходу кто-то, конечно, понимает, чему равна эта сумма, кто-то нет, я сейчас ее посчитаю.
[12:15.560 --> 12:19.560]  Но на самом деле, этому ожиданию можно посчитать сильно проще, и я об этом позже скажу.
[12:19.560 --> 12:26.560]  Есть более простой способ вычисления от ожидания биномиального случайной величины, нежели вот этого сумму считать.
[12:26.560 --> 12:32.560]  Как сумму можно посчитать? Ну, разные способы. Ну, например, можно сделать следующее.
[12:32.560 --> 12:37.560]  Можно заметить, что csn по k по определению от n факториал поделить на k факториал, поделить на минус k факториал,
[12:37.560 --> 12:43.560]  и сократить k факториал из номинателя вот этим k, то есть из номинателя, что получился k минус 1 факториал.
[12:43.560 --> 12:50.560]  Но для этого нам нужно начать суммировать с единицы. Понятно, что мы это можем делать, потому что в 0 это штука просто 0.
[12:50.560 --> 12:56.560]  Эта штука равна 0, если k равно 0. Поэтому мы ничего не потеряем, если просто здесь вместо 0 напишем единицу,
[12:56.560 --> 13:01.560]  у нас сумма не изменится. Давайте так и сделаем.
[13:01.560 --> 13:15.560]  А далее сократим этот k, то есть вместо csn по k напишем n факториал поделить на k минус 1 факториал, и на n минус k факториал.
[13:15.560 --> 13:22.560]  Далее давайте вынесем за знак суммы np.
[13:22.560 --> 13:32.560]  Если мы вынесем отсюда m, то вместо m получится n минус 1 факториал.
[13:32.560 --> 13:43.560]  И тогда вот вся эта дробь, это есть не что иное, как csn минус 1 по k минус 1.
[13:43.560 --> 13:53.560]  А это бином Ньютона просто, это в точности единица.
[13:53.560 --> 14:02.560]  То есть на самом деле вы вместо k можете, как это понять, давайте заменим k минус 1 на s.
[14:02.560 --> 14:07.560]  Вот такую вот сделаем замену, тогда будет здесь сумма по s от 0 до m минус 1.
[14:07.560 --> 14:16.560]  Тогда здесь будет вместо k минус 1 здесь будет s, здесь будет соответственно тоже s, а здесь будет n минус 1 минус s.
[14:16.560 --> 14:25.560]  Поэтому вся эта сумма, это просто есть не что иное, как бином Ньютона, то есть p плюс единица минус p в степени n минус 1,
[14:25.560 --> 14:33.560]  что равно единице, и получаем просто np.
[14:33.560 --> 14:40.560]  Если есть вопросы.
[14:40.560 --> 14:47.560]  Так, окей, давайте теперь возьмем равномерное распиление на множестве каком-то.
[14:47.560 --> 14:55.560]  Ну скажем, можно взять просто подряд числа от одного до m.
[14:55.560 --> 15:01.560]  То есть что это означает? Это означает, что ваше распиление, распиление ваших случайночных сил,
[15:01.560 --> 15:05.560]  примерно следующим образом, оно принимает значения от одного до n,
[15:05.560 --> 15:09.560]  никакие другие, у всех основных значений вероятно 0.
[15:09.560 --> 15:15.560]  И для чисел от одного до n она равна, ну она должна быть равномерна.
[15:15.560 --> 15:20.560]  То есть вероятно, должна быть одинаковая, так как чисел у вас n, она просто равна одной n-той.
[15:20.560 --> 15:26.560]  Для всех k от единицы до n, да, иначе 0.
[15:26.560 --> 15:37.560]  Вот, это мат ожидания кси, это есть сумма, у k от 0 до n, извиняюсь, от единицы до n,
[15:37.560 --> 15:44.560]  k умножить на одну n-тую, ну и это равно просто np.
[15:49.560 --> 15:54.560]  Ну и последнее дискетное распиление, которое мы с вами разбирали в качестве примеров дискетных распилений,
[15:54.560 --> 15:59.560]  плацоновская, с параметром лямда.
[15:59.560 --> 16:05.560]  Я напомню, что это означает, что случайночина ваш принимает только целое неотрицательное значение,
[16:05.560 --> 16:10.560]  и вероятность каждого к целого неотрицательного, это лямда степени k,
[16:10.560 --> 16:13.560]  на е степени минус лямда выделить на k факториал.
[16:18.560 --> 16:23.560]  Вот, тогда мат ожидания кси, это сумма по k от 0 до бесконечности,
[16:23.560 --> 16:29.560]  k умножить на лямда степени k, на е степени минус лямда и выделить на k факториал.
[16:31.560 --> 16:38.560]  Опять, при k равно 0, эта штука просто обращается в 0, и поэтому можно насуммировать, начиная с единицы.
[16:40.560 --> 16:44.560]  И сделаем такой же точно трюк, который мы делали с беремиальным распредлением,
[16:44.560 --> 16:49.560]  давайте сокращаем k факториал с беременателем, остается просто k минус 1 факториал.
[16:49.560 --> 16:56.560]  Давайте теперь лямду умножить на е в степени минус лямда вынесем за знак суммы,
[16:56.560 --> 17:03.560]  и получим вот такую штуку, лямда степени k минус 1 поделить на k минус 1 факториал.
[17:03.560 --> 17:07.560]  Это в точности ряд Тейлора для экспонента.
[17:07.560 --> 17:15.560]  То есть, если вы опять вместо k минус единицы напишите s, то есть обозначите s равно k минус 1,
[17:15.560 --> 17:21.560]  то у вас будет сумма по всем s или от бесконечности, здесь будет лямда в степени s, а в знаменателе будет s факториал.
[17:21.560 --> 17:26.560]  Поэтому это в точности ряд Тейлора для экспонента от лямда.
[17:26.560 --> 17:33.560]  То есть, получаем лямда е в степени минус лямда умножить на е в степени лямда, что просто равно 0.
[17:38.560 --> 17:40.560]  Так, есть ли какие-то вопросы?
[17:45.560 --> 17:49.560]  Хорошо, давайте теперь рассмотрим ситуацию к такси.
[17:49.560 --> 17:52.560]  Это абсолютно неприютно, случайно, вечно.
[17:52.560 --> 17:59.560]  Возвращаемся к определению математического ожидания вот к этому.
[17:59.560 --> 18:06.560]  В тех ситуациях, когда мы интегрируем патолий, вот мы делаем в этом случае,
[18:06.560 --> 18:12.560]  то есть мы делаем в этом случае дизайнерский университет,
[18:12.560 --> 18:17.560]  то есть мы делаем в этом случае дизайнерский университет,
[18:17.560 --> 18:24.240]  В тех ситуациях, когда мы интегрируем по абсолютно неприродному распределению,
[18:24.240 --> 18:29.920]  у него есть плотность, то есть означает, что вы плотность из-под дифференциала можете вынести,
[18:29.920 --> 18:33.640]  и у вас получится просто плотность кси умножить на dx.
[18:33.640 --> 18:44.040]  То есть этот интеграл превращается в интеграл по r от x пкси от x dx.
[18:44.040 --> 18:57.960]  Вообще, давайте запишем такое утверждение, что если g произвольная баррельская функция zrvr,
[18:57.960 --> 19:19.720]  то если у вас есть случайное распределение и пкси абсолютно непрерывно,
[19:19.720 --> 19:34.120]  то тогда интеграл от g от x d пкси, это есть просто интеграл от g от x пкси от x dx.
[19:34.120 --> 19:45.000]  Вот, и это утверждение доказать несложно, я не знаю, доказывает вам его Иван Генрихович или нет.
[19:45.000 --> 19:50.040]  Давайте, что ли, быстренько коротко обсудим его доказательства, чтобы вы понимали,
[19:50.040 --> 19:54.120]  это очень важный момент, почему для абсолютно неприродных распределений мы можем выносить
[19:54.120 --> 20:01.320]  плотность из-под дифференциала. Понятно, что мы можем это делать для индикаторов просто по
[20:01.320 --> 20:08.800]  определению абсолютно неприродного распределения. То есть если g от x – это индикатор того,
[20:08.800 --> 20:22.840]  что x помежит кому-то баррельскому множеству b, то тогда интеграл от g от x d пкси – это просто есть
[20:22.840 --> 20:32.000]  интеграл по множеству b d пкси. Ну, то есть на самом деле это есть не что иное, как пкси от b,
[20:32.000 --> 20:40.400]  по определению интеграла Лебега. А по определению плотности, по определению
[20:40.400 --> 20:43.600]  абсолютно неприродного распределения, вероятность любого баррельского множества – это есть просто
[20:43.600 --> 20:57.240]  интеграл плотности. Ну или иными словами, интеграл по r от g от x на пкси от x d x. То есть для
[20:57.240 --> 21:02.200]  индикаторов баррельских множеств – это просто есть не что иное, как определение абсолютно
[21:02.200 --> 21:07.280]  неприродного распределения. И нам нужно увидеть, что это продолжается на вообще все баррельские
[21:07.280 --> 21:18.280]  функции. Понятно, что это продолжается на простые баррельские функции. То есть если g простая,
[21:18.280 --> 21:25.080]  что значит простая? Это значит, что она равна сумме по i от единицы до n, то есть в конечной сумме,
[21:25.080 --> 21:31.200]  каких-то значений g it, умножительный индикатор того, что x принадлежит
[21:31.200 --> 21:44.160]  Боите, а Боите – это какие-то баррельские множества. Ну полинейность интеграла Лебега тогда, конечно,
[21:44.160 --> 21:57.960]  все следует. Полинейность интеграла Лебега тогда все следует. То есть интеграл по r g от x d пкси – это
[21:57.960 --> 22:05.240]  есть интеграл от суммы, а интеграл Лебега линейен, поэтому получаем сумму по i от единицы до n g it
[22:05.240 --> 22:12.280]  умножить на интеграл по r, индикатор от x принадлежит Боите, умножить на d пкси. Но для индикаторов мы уже
[22:12.280 --> 22:19.320]  знаем, что наше утверждение это правда. Поэтому мы получаем теперь интеграл Лебега по классической
[22:19.320 --> 22:30.640]  мере Лебега, и внутри интеграла еще появляется плотность. Обратно по линейности вносим сумму по
[22:30.640 --> 22:40.920]  интегралу и получаем то, что нужно. Вот, ну а дальше осталось просто по
[22:40.920 --> 22:53.200]  по там Лебега в жаревой сходимости, что-нибудь подобным, приблизить любую функцию простой,
[22:53.200 --> 23:04.560]  приблизить любую функцию простой. Что нам нужно? Ну, понятно, что нам нужно, чтобы интеграл существовал,
[23:04.560 --> 23:11.200]  чтобы вообще это равенство было верным. Нам нужно, чтобы интеграл существовал, поэтому...
[23:19.840 --> 23:26.480]  Сейчас. Я просто пытаюсь понять. Ну, давай для не отрицательных сначала. Ладно, я думаю, что можно сразу
[23:26.480 --> 23:37.400]  для произвольных. Давай сначала не отрицательных. Если g не отрицательна, то приблизим ее простыми,
[23:37.400 --> 23:47.680]  причем приближать можно вот так, тоже не отрицательными. Ну, то есть, я имею в виду,
[23:47.680 --> 24:00.280]  что последность g-н не убывает и стремится к g-поточечному. Вот, тогда интеграл от g от x
[24:00.280 --> 24:09.280]  dpx-и по определению интеграла Лебега, именно по определению. Это есть предел
[24:09.280 --> 24:18.240]  предель смещения бесконечности, интеграл по r от g-н от x dpx-и. Для простых мы умеем выносить плотность,
[24:18.240 --> 24:44.040]  получаем предел интеграл по r g-н от x dpx-и. Вот. А дальше, так как эти функции не отрицательны,
[24:44.040 --> 24:50.960]  и плотность не отрицательна, и g-н не отрицательны, то их произведение тоже не отрицательны. А значит,
[24:50.960 --> 25:02.680]  мы можем внести предел внутри интеграла и получим интеграла g от x px-и от x dx, что требуется.
[25:02.680 --> 25:16.920]  Да, то есть не отрицательны здесь нам сильно в помощь. Так просто мы бы здесь не смогли
[25:16.920 --> 25:22.880]  воспользоваться тиремой Лебега, потому что непонятно, что с этим делать. Если мы не предположили
[25:22.880 --> 25:27.640]  изначально, что g-н не отрицательна, тогда g-н тоже бы не отрицательны. Вот это произведение
[25:27.720 --> 25:34.000]  нифига не простая функция, потому что g-н это простая, а px-и от x совсем нет, поэтому их произведение
[25:34.000 --> 25:41.680]  не простая. Зато есть не отрицательность, поэтому мы можем вносить предел под интеграл. Ну и теперь,
[25:41.680 --> 25:48.920]  если g произвольная, то представляем ее как g-плюс минус g-минус, то есть g-плюс это максимум
[25:48.920 --> 25:55.960]  из g и ноль, g- это максимум из минус g и ноль, и пользуемся просто линейностью.
[25:55.960 --> 26:11.720]  Пользуемся просто линейностью. Значит, интеграл от g от x dpx-и, если он существует,
[26:11.720 --> 26:17.600]  то есть предполагаем, что он существует, тогда это интеграл от g-плюс dpx-и минус интеграл от g-плюс
[26:17.600 --> 26:26.400]  dpx-и. Значит, если он существует, то это означает, что ровно один из этих интегралов, прошу прощения,
[26:26.400 --> 26:33.600]  не более одного из этих интегралов, не более чем один из этих интегралов бесконечно. Да, то есть
[26:33.600 --> 26:41.280]  два интеграла бесконечно быть не могут, только один. В любом случае, для не отрицательных функций
[26:41.280 --> 26:46.240]  так или иначе мы умеем заменять уже интеграл на интеграл по классическому мере Либега,
[26:46.240 --> 26:59.520]  поэтому получаем интеграл от g-плюс x-и dx. Так как между этими интегралами равенство стоит,
[26:59.520 --> 27:04.640]  то раз только один из этих, не более чем один из этих интегралов может быть бесконечен,
[27:04.640 --> 27:08.760]  то же самое верно и для этих двух интегралов. Не более чем один из этих двух интегралов может
[27:08.760 --> 27:22.200]  быть бесконечен. А значит, это равно по линейности интегралу от g-пxy dx, что и требуется. Да, то есть
[27:22.200 --> 27:27.840]  утверждение доказывается более-менее очевидно, просто основываясь на предельном переходе вот
[27:27.840 --> 27:43.740]  здесь и на определение. Есть какие-то вопросы? Прекрасно, да. То есть теперь мы понимаем,
[27:43.740 --> 27:50.920]  действительно, что просто из того, что мы умеем выносить плотность из-под дифференциала для
[27:50.920 --> 27:58.200]  индикаторов, ну, то есть иными словами из-за того, что мы знаем вот это, из этого следует,
[27:58.200 --> 28:02.120]  что внутри интеграла можно выносить плотность вообще для приинтегрирования любых баррельских
[28:02.120 --> 28:08.560]  функций. Хорошо, теперь давайте примеры разберем. То есть я сейчас оправдал вот эту формулу,
[28:08.560 --> 28:13.680]  и теперь я буду применять для каких-нибудь конкретных абсолютно неправильных распределений.
[28:20.920 --> 28:31.560]  Так, какие у нас там были абсолютно неправильные? Ну, наверное, равномерные. Пусть x имеет
[28:31.560 --> 28:43.480]  равномерное распределение на отрезки от А до B. Скажем. Что это значит? Это означает,
[28:43.480 --> 28:48.040]  что плотность распределения этой случайной величины, которую мы обозначаем как x от x,
[28:48.040 --> 28:55.240]  это есть единица девять на B минус А, множительный индикатор принадлежности отрезку от А до B.
[28:55.240 --> 29:07.920]  По нашей формуле, в отождании x, это есть интеграл по r от x пекси от x до x. То есть
[29:07.920 --> 29:18.560]  интеграл от A до B, x поделить на B минус А до x. Легко видеть, что это есть, когда мы интегрируем x,
[29:18.560 --> 29:24.840]  мы получаем x в раз пополам. То есть, поставляя B и A, мы в итоге получаем B плюс A пополам.
[29:24.840 --> 29:42.560]  Есть какие-то вопросы? Окей. Какой теперь? Давайте экспоненциально.
[29:42.560 --> 29:51.080]  Экспоненциально распределение с параметром лямбда, это такое распределение, у которого плотность равна лямбда
[29:51.080 --> 29:54.840]  е в степени минус лямбда x, умножить на индикатор того, что x не отрицает.
[29:54.840 --> 30:13.040]  Ну тогда, от ожидания x, это есть интеграл по r от x, множить на пекси от x до x. И это равно интеграл
[30:13.040 --> 30:20.960]  от нуля, потому что у нас есть такой индикатор. То есть, функция плотность вне луча на лямбда
[30:20.960 --> 30:27.840]  бесконечна просто нулю равна, поэтому там можно интегрировать. Здесь будет лямбда x е в степени минус лямбда x dx.
[30:27.840 --> 30:36.840]  Эта штука легко интегрируется по частям. Занося под дифференциал экспонента, мы получим
[30:36.840 --> 30:50.360]  минус x умножить на d от е в степени минус лямбда x. Вот, значит, тогда интегрируя по частям,
[30:50.360 --> 31:02.720]  мы получаем такую вот вещь. Первая слагаемая это ноль, первая слагаемая это ноль, остается только
[31:02.720 --> 31:12.760]  вторая слагаемая. Ну, этот интеграл легко найти. Это минус е в степени минус лямбда x поддевить на лямбда.
[31:12.760 --> 31:21.440]  И в итоге получаем просто 1 поддевить на лямбда. Значит, мат ожидания экспоненциально
[31:21.440 --> 31:25.080]  случайно вещественные с параметром лямбда равна единицей на лямбда. Есть какие-то вопросы?
[31:25.080 --> 31:37.080]  Ну и еще давайте для нормального распределения с параметром a и c в квадрате посчитаем. Значит,
[31:37.080 --> 31:42.240]  забегая вперед, скажу сразу, что я бы мог сперва сформулировать какие-то полезные свойства
[31:42.240 --> 31:48.920]  мат ожидания. И после этого мне было это мат ожидания такой случайно вещественной читать проще.
[31:48.920 --> 31:54.480]  Я бы сначала посчитал мат ожидания стандартного нормального, то есть когда параметры равны 0 и 1,
[31:54.480 --> 31:59.080]  это как бы технически проще, хотя идеологически ничем не отличается, но технически проще,
[31:59.080 --> 32:04.360]  а потом бы легко бы из этого вывел мат ожидания вот такой случайно вещественной. Но для того,
[32:04.360 --> 32:10.320]  чтобы наработать технику, я поступлю по-другому. Я сначала вас научу в сложном случае считать,
[32:10.320 --> 32:16.200]  то есть чтобы техника была в вычислении таких интегралов, а потом еще раз вернусь к этому,
[32:16.200 --> 32:24.440]  когда говорю про свойства математического ожидания. Значит, плотность такого распределения равна
[32:24.440 --> 32:30.400]  по определению 1 петель на корень из 2 пи сима в квадрате, е в течение минус х минус а в квадрате
[32:30.400 --> 32:34.840]  петель на 2 сима в квадрате. И здесь отличие от предыдущих в том, что носит все множество
[32:34.840 --> 32:38.760]  издательных чисел. Здесь видно нет никаких индикаторов, у нас везде плотность нулевая.
[32:38.760 --> 32:51.000]  По определению мат ожидания к сте это есть интеграл по r от x по x от x до x. И в нашем
[32:51.000 --> 32:59.080]  случае получаем интеграл по r от x поделить на корень из 2 пи сима в квадрате. Экспонента
[32:59.080 --> 33:10.960]  от минус х минус а в квадрате поделить на 2 сима в квадрате dx. Что с этим делать?
[33:10.960 --> 33:19.280]  С этим можно сделать следующее. Давайте мы вот в этой дроби в числителе вычтем и добавим а. То есть
[33:19.280 --> 33:25.240]  представим на самом деле в виде суммы двух интегралов 1 от х минус а поделить на корень из 2
[33:25.240 --> 33:33.960]  пи сима в квадрате. Экспонента от минус х минус а в квадрате поделить на 2 сима в квадрате dx.
[33:33.960 --> 33:42.960]  И плюс а интегралов по r от единицы поделить на корень из 2 пи сима в квадрате е в степени
[33:42.960 --> 33:49.560]  минус х минус а в квадрате поделить на 2 сима в квадрате dx. Я иногда пишу е и степень,
[33:49.560 --> 33:56.640]  иногда и экспонент. Теперь смотрите, вот какое наблюдение. Вот эта функция,
[33:56.640 --> 34:06.200]  которая здесь написана, если сместить в точку а, то она окажется нечетной.
[34:06.200 --> 34:14.840]  То есть иными словами, если вы нарисуете график график этой функции, тут у вас 0,
[34:14.840 --> 34:20.720]  а вот тут у вас точка а. Если вы нарисуете график этой функции, то она в точке а равна нулю,
[34:20.720 --> 34:28.640]  и относительно этой точки она будет симметрична. Справа положительно, слева отрицательно.
[34:28.640 --> 34:35.600]  При этом интеграл, конечно, существует. Эта экспонента при больших х, при х мячности
[34:35.600 --> 34:40.680]  бесконечности, эта экспонента убывает супер быстро, гораздо быстрее, чем эта штука возрастает.
[34:40.720 --> 34:46.200]  То есть она их доминирует. Интеграл поэтому существует, он конечен. А раз он существует
[34:46.200 --> 34:53.140]  и конечен, то интеграл от нечетной функции равен нулю. Просто интеграл от положительной части
[34:53.140 --> 34:57.900]  интеграла от happy Negro mastеч率 равны. И по модулю вы получаете равный, podía типа у нас 0, 2000.
[35:04.300 --> 35:08.560]  Здесь у нас к интегралу плотность написана. Это вмысленность плотности нашего
[35:08.560 --> 35:13.380]  нормального распределения, тогда плотность равен единице и значит наш весь,
[35:13.380 --> 35:21.900]  наш вся сумма просто равна а. Есть ли какие-то вопросы?
[35:27.440 --> 35:32.220]  Вопросов нет, прекрасно. Давай теперь тогда, в общем, примеры основных
[35:32.220 --> 35:35.940]  распределений мы взобрали, для них мы математические ожидания посчитали,
[35:35.940 --> 35:38.580]  давайте поговорим про свойства математического ожидания.
[35:48.580 --> 35:55.780]  Это на самом деле просто свойства интеграла Либега, так как математожжение просто интеграла
[35:55.780 --> 35:59.060]  Либега, это те свойства, которые я перечитаю, просто стандартные свойства интеграла Либега,
[35:59.060 --> 36:04.020]  которые вам Иван Геррихович, я надеюсь, тоже рассказывал, поэтому я их буду давать без доказательства,
[36:04.020 --> 36:14.660]  может докажу, что только специфическое, именно то, что полезно именно для теории вероятности,
[36:14.660 --> 36:18.660]  может быть, вам эти свойства Иван Геррихович не рассказывал, я их докажу, а стандартные
[36:18.660 --> 36:24.620]  свойства интеграла Либега доказывать не стану. Итак, во-первых, если две функции,
[36:24.620 --> 36:32.060]  две случайной величины, таковы, что одна больше, чем другая, и оба мат ожидания существуют,
[36:32.060 --> 36:47.820]  они могут быть бесконечной, то мат ожидания СЕ больше не очень мат ожидания ЭТО, да, здесь,
[36:47.820 --> 36:51.660]  мы когда пишем неравенство, имеется в виду, что бесконечности мы тоже умеем сравнивать,
[36:51.660 --> 36:57.100]  то есть плюс бесконечность больше, чем минус бесконечность, и плюс бесконечность равно плюс
[36:57.100 --> 37:01.620]  бесконечности, а минус бесконечности, все равно минус бесконечности. То есть здесь никого не
[37:01.620 --> 37:06.180]  противоречит тем, что они могут быть бесконечными, мы их, тем не менее, тоже можем сравнить.
[37:06.180 --> 37:16.560]  Значит второе, если кси не отрицательно, кси не отрицательно случайно вылечена, то и
[37:16.560 --> 37:23.620]  моджетание кси тоже не отрицательно, но это просто следствие предыдущего свойства. Явное, да,
[37:23.620 --> 37:29.380]  моджетание от нуля это ноль. Здесь мы напишем, здесь моджетание с двух сторон, мы получим ровно,
[37:29.380 --> 37:50.420]  получим ровно. Вот это утверждение. Кроме того, если, если к тому же, не только кси больше
[37:50.420 --> 37:56.060]  нуля, еще известно, что моджетание кси равно нулю, если кси больше нуля и моджетание кси равно нулю,
[37:56.060 --> 37:59.300]  то тогда вероятность того, что кси равно нулю, равна единице.
[37:59.300 --> 38:20.180]  Ну вот, вторая часть, это, наверное, специфическое свойство, именно для, который мы используем
[38:20.180 --> 38:27.300]  категорию при стучании вечен, давайте его докажем. Значит, ну, если первое свойство очевидно,
[38:27.300 --> 38:33.220]  первая часть второго свойства очевидна, ну, кроме того, она просто следует из определения
[38:33.220 --> 38:39.700]  математического, из определения интеграла, что интеграл от не отрицательных функций является
[38:39.700 --> 38:45.500]  не отрицательным величиной, то со вторым, казалось бы, сегодня так просто, но, тем не менее,
[38:45.500 --> 38:51.660]  не должно быть сложно. Смотрите, значит, итак, пусть кси не отрицательно, тогда его можно приблизить
[38:51.660 --> 39:00.060]  снизу простыми случайными величинами. То есть существует последовательность не отрицательных
[39:00.060 --> 39:11.580]  простых случайных величин кси-н, которые снизу стремятся к си снизу, то есть они
[39:11.580 --> 39:24.180]  последовательность не убывает и стремится поточь на кси. Вот, значит, раз они не превосходят кси,
[39:24.180 --> 39:35.620]  то их мат ожидания тоже не превосходит мат ожидания кси. При этом мат ожидания кси равно 0,
[39:35.620 --> 39:44.100]  и следовательно, мат ожидания кси-н тоже равно 0. Но для простых случайных величин не отрицательных,
[39:44.100 --> 39:52.060]  если уж так вышло, что у них мат ожидания 0, то они равны 0 без вариантов. Почему? Потому что
[39:52.060 --> 40:05.860]  простая случайная величина, это есть что такое? Это есть сумма каких-то там окатых, умножить на
[40:05.860 --> 40:15.300]  вероятность того, что кси-н попал в какое-то там множество б-каты. Эта сумма хонечная, и вот эти все
[40:15.300 --> 40:22.260]  окаты и не отрицательные. То есть все слагаемые в этой сумме не отрицательные, так этому мат ожидания,
[40:22.260 --> 40:28.660]  и она равна 0. Сумма не отрицательных чисел равна 0 тогда и только тогда, когда все окаты равны 0.
[40:28.660 --> 40:34.260]  А это означает, что действительно просто кси-н в точности равно 0.
[40:34.260 --> 40:47.140]  Но раз кси-н стремится по точечкам кси, и они все равны 0, то это означает,
[40:47.140 --> 40:52.740]  что вероятность того, что кси равно 0, тоже равна 1. Что это елос.
[40:52.740 --> 41:00.180]  Так, есть какие-то вопросы?
[41:04.260 --> 41:11.940]  Хорошо. Третье тоже, наверное, довольно специфическое свойство. Давайте мы его тоже докажем.
[41:11.940 --> 41:30.140]  Значит, если существует мат ожидания кси, то для любого события ASF существует мат ожидания кси на
[41:30.140 --> 41:39.820]  индикатор А. То же самое верно с конечностью. То есть, если модульма от ожидания кси меньше
[41:39.820 --> 41:47.940]  бесконечности, то для любого ASF модульма от ожидания кси на индикатор А тоже меньше бесконечности.
[41:47.940 --> 42:01.300]  Но не выглядит сложным. Смотрите, что значит, что существует мат ожидания кси.
[42:01.300 --> 42:11.460]  Существует мат ожидания кси. Значит, либо мат ожидания кси плюс меньше бесконечности,
[42:11.460 --> 42:17.220]  либо мат ожидания кси минус меньше бесконечности. Но давайте без ограничений
[42:17.220 --> 42:22.820]  обществе, это случая симметричная, без ограничений обществе будем считать, что мат ожидания кси плюс меньше бесконечности.
[42:22.820 --> 42:34.660]  Значит, не ограничивая без ограничения обществе, мат ожидания кси плюс меньше бесконечности.
[42:34.660 --> 42:41.460]  Вот, теперь, что происходит, когда мы тоже представляем в таком виде, в виде разницы двух
[42:41.660 --> 42:49.660]  неотрицательных величин, кси умножительного и��икатора. Ну смотрите, понятно, что можно вот так поступить,
[42:49.660 --> 42:55.160]  можно сказать, ну кси, это же кси плюс минус кси минус.
[42:55.160 --> 43:01.420]  Дальше раскрыть скобки. Мы получим кси плюс на индикатор А минус кси минус на индикатор А,
[43:01.420 --> 43:07.540]  и обе величины будут неотрицательны. И вообще понятно, что кси плюс на индикатор А φbahn(?)
[43:07.540 --> 43:08.720]  как Ciaoce bare
[43:08.720 --> 43:10.040]  confirming plum as well
[43:10.040 --> 43:11.380]  animals qil meant it.
[43:11.380 --> 43:12.460]  bilcongfu
[43:12.460 --> 43:12.700]  use
[43:12.700 --> 43:13.340]  Yes.
[43:13.340 --> 43:15.020] fy
[43:15.020 --> 43:15.840]  fee plus
[43:15.840 --> 43:16.720]  fifty
[43:16.720 --> 43:18.160]  what's going
[43:18.160 --> 43:19.720]  what's
[43:19.720 --> 43:24.600]  род
[43:24.600 --> 43:26.040]  뭐 это же binary
[43:26.040 --> 43:27.760]  скидка
[43:27.780 --> 43:29.320]  золото на SE
[43:29.320 --> 43:29.820]  плюс неBrief
[43:29.820 --> 43:30.160] aley
[43:30.160 --> 43:32.000]  тем более
[43:32.000 --> 43:32.520]  Да, well,
[43:32.520 --> 43:33.200]  вовсе говоря,
[43:33.200 --> 43:34.040]  не превосходит
[43:34.040 --> 43:34.880]  отзезания
[43:34.880 --> 43:36.240]  все плюс
[43:36.240 --> 43:41.680]  так как кси-плюс на индикатора а не превосходит кси-плюс, да вот случайно
[43:41.680 --> 43:45.040]  величину множество индикатора уменьшится, то есть для каких-то
[43:45.040 --> 43:51.720]  положительных значений они просто обнудились, вот поэтому по свойству 1 из
[43:51.720 --> 43:57.080]  вот этого следует нерадость для математических ожиданий, ну и все, а значит и
[43:57.080 --> 44:03.640]  мат-зани кси-плюс на индикатора тоже конечна и тогда существует мат-ожидание.
[44:07.040 --> 44:15.620]  существует... вот понятно, что для конечности всё то же самое, то есть
[44:15.620 --> 44:20.240]  рассуждение, для случая когда модуль мат-ожидание кси меньше бесконечности, оно просто
[44:20.240 --> 44:31.380]  аналогично, вы там скажете, ну окей, раз мат-ожидание кси конеч
[44:31.380 --> 44:36.620]  кси минус меньше бесконечности, а дальше вы проделываете вот это рассуждение и для кси плюс,
[44:36.620 --> 44:41.620]  и для кси минус. То есть вы делаете вывод, что ага, ну значит им от ожидания кси плюс на индикатора
[44:41.620 --> 44:51.380]  конечна, им от ожидания кси минус на индикатора тоже конечна. Вот собственно и всё. Значит,
[44:51.380 --> 45:08.940]  доказательства в случае модуль кси меньше бесконечности аналогично. Так, есть какие-то
[45:08.940 --> 45:19.060]  вопросы. Можно вот начнить, кси с минусом это отрицательные числа. Еще раз. Кси с минусом это
[45:19.060 --> 45:25.340]  отрицательные функции. Ну, сейчас. Кси с минусом это тоже не отрицательная функция. Значит, кси с
[45:25.340 --> 45:33.700]  минусом это максимум из минус кси и ноль. Да, то есть это соответствует отрицательным значениям
[45:33.700 --> 45:41.060]  случайно величины кси, но сама функция является не отрицательной. Спасибо. Ещё какие-то вопросы?
[45:41.060 --> 45:57.020]  Хорошо, давайте двигаться дальше. Значит, следующее свойство. Следующее свойство.
[45:57.020 --> 46:12.220]  Значит, если существует от ожидания кси, то модуль от ожидания меньше, чем раньше от
[46:12.220 --> 46:18.860]  ожидания модуля. Да, но это как обычно классическое свойство интеграла Либерго. Я думаю, что Иван
[46:18.860 --> 46:28.460]  Генрихович вам рассказывал. Дальше, линейность. Во-первых, что можно выносить константу из-под
[46:28.460 --> 46:36.380]  математического ожидания. Опять надо сказать, что если моджедание кси существует, то для любого
[46:36.380 --> 46:46.180]  действительно у числа c, мод ожидания от c кси равно c на мод ожидания кси. Да, но здесь понятно и дело,
[46:46.300 --> 46:51.100]  что в случае, когда моджедание бесконечно, нужно понимать, что знак может меняться. То есть,
[46:51.100 --> 46:57.420]  если у вас от ожидания кси равно плюс бесконечности, а c отрицательное число,
[46:57.420 --> 47:03.220]  то значит, это будет тогда уже минус бесконечности. Я думаю, что вы понимаете, как мы тут
[47:03.220 --> 47:10.740]  арифметические операции с бесконечностью производим. Хорошо, теперь с суммой. Ну,
[47:10.740 --> 47:15.220]  можно какое-то более общее утверждение сформулировать, но нам это не потребуется.
[47:15.220 --> 47:23.300]  Давайте считать для простоты, что оба мат ожидания конечны. Значит, если мат ожидания кси по модулю
[47:23.300 --> 47:31.460]  и мат ожидания от по модулю конечны, то мат ожидания от суммы равна сумме мат ожиданий.
[47:31.460 --> 47:41.940]  Тоже обычное свойство про линейность интеграла Ребега, которую вам Иван Генривович обязан был
[47:41.940 --> 47:52.540]  рассказать. Что еще? Что еще? Давайте еще одну способность сформулируем. Значит, пусть существует
[47:52.540 --> 48:03.740]  мат жаних си и мат ожиданий этой, пусть они конечны опять. А в шестом пункте, если у нас
[48:03.740 --> 48:12.500]  мат ожидания бесконечная, но одного знака, в чем проблема? Давайте так. Понятно, что это шестой пункт,
[48:12.500 --> 48:18.140]  его можно обобщать. В максимальной обществе нужно сформулировать несколько разных ситуаций.
[48:18.140 --> 48:27.260]  То есть плохая ситуация, это когда оба бесконечны, причем разных знаков. То есть если они оба плюс
[48:27.260 --> 48:32.780]  бесконечность, все ок, если они оба минус бесконечность, тоже все ок. Дальше, если один из них бесконечно,
[48:32.780 --> 48:43.980]  второй констант, это тоже все ок. Давайте я сформулирую в более общем виде, как-то сказать коротко,
[48:43.980 --> 48:56.060]  сейчас попробуем. Значит, нам не подходит только случай, пусть мат ожиданий си и мат жаних существует.
[48:56.060 --> 49:19.580]  И к тому же, к тому же, что еще надо сказать? Надо сказать, что они, если и бесконечно, то одинаковых знаков.
[49:19.580 --> 49:32.100]  Значит, и либо
[49:39.300 --> 49:45.700]  не более одной, сейчас, как же просто сказать.
[49:50.300 --> 50:00.060]  И так надо сказать, значит, и если они оба бесконечны, то одинаковых знаков. И если оба бесконечны,
[50:05.060 --> 50:07.740]  то одинаковых знаков.
[50:07.740 --> 50:20.340]  Тогда мат ожидания сумма равна сумму мат ожиданий. Наверное, так. Хорошо.
[50:20.340 --> 50:27.340]  Значит, пусть теперь случайные величины имеют конечные математические ожидания,
[50:27.940 --> 50:40.940]  и пусть для любого а из f мат ожидания кси на индикатор а меньше набрано, чем мат ожидания эт на индикатор а.
[50:42.940 --> 50:47.940]  Тогда кси меньше набрано, чем эт с вероятностью 1.
[50:48.540 --> 50:59.540]  Тоже довольно специфическое свойство, поэтому давайте его докажем.
[51:06.540 --> 51:15.540]  Давайте докажем. Давайте рассмотрим такое вот множество b или a.
[51:16.140 --> 51:27.140]  Обычно множество событий ab означает множество таких омега, что кси от омега больше, чем это от омега.
[51:28.140 --> 51:35.140]  Значит, понятно, что это элемент f. Кстати, кто может объяснить, почему это элемент f?
[51:41.140 --> 51:43.140]  Почему это событие?
[51:43.140 --> 51:45.140]  Потому что разность измерима.
[51:46.140 --> 51:51.140]  Yes. То есть вы из случайных величин кси это составляете вектор,
[51:51.140 --> 51:56.140]  потом применяете к нему баррельскую функцию разность и получаете снова измеримую функцию.
[51:56.140 --> 51:58.140]  То есть кси минус это, это измеримая функция.
[51:58.140 --> 52:04.140]  У вас здесь интересует не что иное, а как прообраз луча от нуля до бесконечности.
[52:06.140 --> 52:10.140]  Нам надо на самом деле доказать, что вероятность этого множества это 0.
[52:12.140 --> 52:15.140]  Нам надо доказать, что вероятность этого множества это 0.
[52:16.140 --> 52:21.140]  Ну вот у нас есть вот это утверждение, которое верно для любого a из f.
[52:21.140 --> 52:24.140]  В частности, мы можем взять именно это a, которое мы только что выбрали.
[52:24.140 --> 52:30.140]  Мы знаем, что мы от ожидания кси на индикатор a меньше набрано, чем от ожидания это на индикатор a.
[52:34.140 --> 52:39.140]  Ну вот давайте теперь по линейности выясним, что это означает на самом деле,
[52:39.140 --> 52:44.140]  что мы от ожидания это минус кси на индикатор a больше набрал нуля.
[52:46.140 --> 52:50.140]  Теперь давайте поймем, что это за случайная величина такая.
[52:50.140 --> 52:52.140]  Это минус кси на индикатор a.
[52:58.140 --> 53:05.140]  Смотрите, значит индикатор a равен нулю, если кси больше чем это.
[53:09.140 --> 53:14.140]  То есть это либо 0, если кси больше чем это.
[53:16.140 --> 53:18.140]  Если же кси меньше набрано, чем это.
[53:26.140 --> 53:30.140]  Значит, если же кси меньше набрано, чем это, то...
[53:30.740 --> 53:32.740]  Сейчас.
[53:44.740 --> 53:46.740]  А, наоборот, глупости говорю.
[53:46.740 --> 53:49.740]  Значит, здесь же у нас индикатор того, что кси больше, чем это.
[53:49.740 --> 53:51.740]  Значит, это 0 наоборот, если это не выполнено.
[53:51.740 --> 53:53.740]  То есть, если кси меньше, но чем это.
[53:55.740 --> 53:56.740]  Прекрасно.
[53:56.740 --> 54:03.740]  А если этот индикатор выполнен, то есть, если все-таки кси больше, чем это,
[54:03.740 --> 54:06.740]  то вот эта разность, она отрицательна.
[54:07.740 --> 54:12.740]  То есть меньше нуля, если кси больше, чем это.
[54:14.740 --> 54:18.740]  Ну короче говоря, это не отрицательно, это неположительная случайная величина.
[54:19.740 --> 54:22.740]  В любом случае она не может быть положительна.
[54:22.740 --> 54:25.740]  А значит, мы от ожидания от нее тоже не может быть положительной.
[54:26.740 --> 54:32.040]  У нас есть теперь с вами два нераеста.
[54:32.040 --> 54:34.280]  С одной стороны, мотождание не отрицательное, с другой
[54:34.280 --> 54:38.200]  стороны оно неположительное, значит оно ровно нулю.
[54:38.200 --> 54:40.840]  Ничего не остается, кроме как ему равняться нулю.
[54:40.840 --> 54:48.360]  А у нас есть свойство номер два, которое мы вот здесь
[54:48.360 --> 54:50.440]  сейчас применим, которое говорит, что если случайно
[54:50.440 --> 54:53.760]  миличная не отрицательная, и мотождание от нее ноль,
[54:53.760 --> 54:55.200]  то она просто равна нулю.
[54:55.940 --> 54:58.060]  У нас здесь именно такая ситуация, но только не положительная,
[54:58.060 --> 55:00.060]  но неважно, свойства все равно работают.
[55:00.060 --> 55:03.260]  У нас случайная величина не положительная, а мотождание
[55:03.260 --> 55:04.760]  от нее равно ноль.
[55:04.760 --> 55:07.840]  Значит по свойству два.
[55:07.840 --> 55:09.700]  Вероятность того, что это случайная величина равна
[55:09.700 --> 55:10.700]  нулю равна единице.
[55:10.700 --> 55:19.380]  Значит в каком случае эта случайная величина равна
[55:19.380 --> 55:20.380]  нулю?
[55:20.380 --> 55:25.140]  вот у меня здесь написано это случайно равна, случайно числа равна нулю только
[55:25.140 --> 55:30.220]  если кси меньше 0 чем это, вот только в этом случае она равна нулю, то есть это в
[55:30.220 --> 55:39.700]  точности вероятность того что кси меньше 0 чем это, что и требовалось, так
[55:39.700 --> 55:42.900]  есть ли вопросы?
[55:50.380 --> 56:02.020]  хорошо давайте теперь независимые случайные величины, последнее наверное
[56:02.020 --> 56:11.100]  свойство и потом я напомню всякие теоремы которые у вас были в курсе с Иваном
[56:11.100 --> 56:16.500]  Генриховичем, которые мы будем тоже использовать поэтому я их напомню
[56:16.500 --> 56:32.020]  значит итак, если кси не зависит от этом и ну боксом давайте пусть конечным от
[56:32.020 --> 56:35.460]  ожидания кси, мы от ожидания это
[56:35.460 --> 56:48.540]  важно ли конечный, сейчас дайте я собираюсь в кучку
[56:51.460 --> 57:00.900]  наверное неважно, давайте просто они будут существовать, давайте они просто будут
[57:00.900 --> 57:09.620]  существовать, тогда мы от ожидания произведения
[57:16.060 --> 57:20.540]  не очень понятно что такое, сейчас, сейчас секунду
[57:21.540 --> 57:28.780]  нет, давайте они будут конечны, прошу прощения, пусть они будут все-таки
[57:28.780 --> 57:34.380]  конечны, значит тогда мы от ожидания произведения равно произведению мы от
[57:34.380 --> 57:42.140]  ожидания, это на самом деле, я докажу сейчас, это просто явное следствие
[57:42.900 --> 57:50.860]  Фубини, значит смотрите, мы от ожидания произведения
[57:51.700 --> 58:01.260]  это есть не что иное, как интеграл
[58:06.580 --> 58:07.060]  от
[58:09.500 --> 58:14.540]  кси отомега, это отомега dp
[58:20.860 --> 58:28.180]  а это есть не что иное, как интеграл
[58:28.180 --> 58:40.100]  значит мы можем заменить здесь кси отомега на х и это отомега на у, интегрировать
[58:40.100 --> 59:01.260]  пр2, интегрировать пр2 d от декартового произведения по кси по это, ну это теорема
[59:01.620 --> 59:12.540]  о замене переменов интегралили бега в точности, значит, то есть на самом деле
[59:12.540 --> 59:23.540]  здесь должен стоять мера вектора кси это, распределение вектора кси это, но в силу
[59:23.540 --> 59:30.060]  независимости распределение вектора это езде картового произведения, распределение
[59:30.060 --> 59:35.860]  случайных величин кси и случайных величин это, то есть мы вот в этот момент применяем
[59:35.860 --> 59:43.380]  независимость, говоря, что распределение вектора кси это, это в точности
[59:43.380 --> 59:51.220]  это в точности декартового произведения распределения по кси по это
[59:51.220 --> 01:00:00.260]  я сейчас докажу это свойство, потом я еще про теорему о замене переменных и
[01:00:00.260 --> 01:00:06.340]  попробую в максимально общем виде сформулировать, в частности для функций
[01:00:06.340 --> 01:00:16.540]  многих переменных, как это здесь происходит тоже, так, хорошо, значит, теперь мы применяем
[01:00:16.540 --> 01:00:26.740]  теорему фубини и говорим, что это просто интеграл по r от x dp кси, множительно интеграл
[01:00:26.740 --> 01:00:31.780]  по r от y dp это, это есть просто мы от ожидания кси нам от ожидания это
[01:00:31.780 --> 01:00:42.940]  вот, ну давайте сразу тогда я сформулирую теорему о замене
[01:00:42.940 --> 01:00:53.140]  переменных, а после этого еще напомню тему либега, а мы же о реме сходимости
[01:00:53.940 --> 01:01:00.860]  теорема о замене переменных
[01:01:00.860 --> 01:01:28.300]  так, значит, пусть у нас есть, пусть у нас есть, сейчас я собажу, пусть есть случайный вектор
[01:01:28.300 --> 01:01:49.300]  x, размерности n или давайте лучше k, размерности k и пусть phi это баррельовская функция,
[01:01:49.300 --> 01:02:18.780]  которая действует из rqr, тогда если существует мот ожидания от phi от x, то оно равно интегралу
[01:02:18.780 --> 01:02:34.780]  по r степени k от phi от x dpх, так сейчас, я прошу прощения, одну секунду
[01:02:48.780 --> 01:03:11.780]  прошу прощения, да, вот, собственно,
[01:03:11.780 --> 01:03:21.780]  мы ее здесь и применяли, то есть мы говорили, что возьмем случайный вектор x равный кси это
[01:03:21.780 --> 01:03:32.780]  и применю к нему функцию phi, которая равна просто произведению, да, тогда мот ожидания от phi от x
[01:03:32.780 --> 01:03:44.780]  это в точности вот это вот мот ожидания от кси это, и по теореме о замене переменных мы получаем
[01:03:44.780 --> 01:03:56.780]  вот такого интеграла, но я даже не знаю, доказывается эта теорема в точности также, как мы доказывали
[01:03:56.780 --> 01:04:08.780]  аналогичное утверждение вот здесь, да, когда мы говорили, что плотность можно вынести из-под дифференциала
[01:04:08.780 --> 01:04:16.780]  доказательственно аналогична, но давайте как-то коротко, что ли, по нему пробежимся, если Иван Генрихович
[01:04:16.780 --> 01:04:26.780]  его не рассказывал, видимо, не рассказывал, раз вы спрашиваете, значит, давайте попробуем коротко
[01:04:26.780 --> 01:04:36.780]  пробежаться, ну, во-первых, если phi это индикатор, пусть phi от x это индикатор того, что x принадлежит
[01:04:36.780 --> 01:04:42.780]  баррельскому множеству b для некоторого баррельского множества rk
[01:04:42.780 --> 01:04:56.780]  вот, тогда что такое мот ожидания phi от x, большое, это мот ожидания индикатора того, что x большое
[01:04:56.780 --> 01:05:06.780]  то есть это в точности вероятность того, что x принадлежит b, значит, по определению интеграла
[01:05:06.780 --> 01:05:16.780]  Лебега теперь по px, это есть не что иное, как интеграл от индикатора того, что x маленькое принадлежит b
[01:05:16.780 --> 01:05:26.780]  умножить на, ну, для простоты давайте напишу так, интеграл по b от dpx
[01:05:26.780 --> 01:05:36.780]  да, это тоже определение интеграла Лебега, то есть когда мы находим интеграл Лебега от индикатора того, что x принадлежит b
[01:05:36.780 --> 01:05:42.780]  по какой-то мере, мы получаем просто меру этого множества
[01:05:42.780 --> 01:05:52.780]  чтобы было еще понятнее, я давайте еще добавлю одно выражение, значит, по определению p от x принадлежит b, это просто в точности px от b
[01:05:52.780 --> 01:06:02.780]  да, вот так должно быть понятнее, что и требовалось, собственно, да, это и есть интеграл от phi от x dpx
[01:06:02.780 --> 01:06:08.780]  хорошо, теперь если phi простая, да, по линейности
[01:06:08.780 --> 01:06:16.780]  я не буду расписывать с вашего позволения, понятно, что надо просто применить линейность как для левой части, так и для правой части
[01:06:20.780 --> 01:06:24.780]  по линейности интеграл Лебега
[01:06:24.780 --> 01:06:40.780]  вот, теперь если phi неотрицательная, тогда можем приблизить простыми, неотрицательными снизу
[01:06:44.780 --> 01:06:48.780]  и дальше просто переходим в предел, давайте это аккуратно проделаем
[01:06:48.780 --> 01:07:04.780]  значит, в ожидании phi от x, так как phi простая функция, phi от x будет простая случайная величина, она будет принимать только конечное количество значений
[01:07:04.780 --> 01:07:10.780]  phi n, прошу прощения, phi n будет простая случайная величина, которая принимает конечное количество значений
[01:07:10.780 --> 01:07:14.780]  при определении интеграла Лебега, или то же самое правление вматоожидания
[01:07:14.780 --> 01:07:20.780]  вматоожидание phi от x, это есть просто предел при ностримящейся бесконечности, вматоожидание phi от x
[01:07:20.780 --> 01:07:28.780]  для phi n мы умеем заменять на этот интеграл вматоожидания, получаем интеграл от phi от x dpx
[01:07:28.780 --> 01:07:38.780]  опять, сил тоже phi n простые функции, мы можем занести предел внутри интеграла и получить интеграл от phi от x dpx
[01:07:40.780 --> 01:07:47.300]  Ну и, наконец, для произвольных функций по линейности мы представляем фи как фи плюс минус фи минус,
[01:07:53.300 --> 01:07:55.940]  где фи плюс, фи минус не отрицательные.
[01:07:55.940 --> 01:08:13.060]  И понятно, что фи от х плюс, так я снизу написал, давайте я сверху напишу как обычно,
[01:08:13.060 --> 01:08:25.860]  фи плюс минус фи минус, фи от х плюс это просто фи плюс от х, а фи от х минус это просто фи минус,
[01:08:26.220 --> 01:08:34.820]  поэтому мот ожидания от фи от х, это просто мот ожиданne от фи плюс от х, минус мот ожидания
[01:08:34.820 --> 01:08:40.540]  от фи минус от х, для фи плюс и для фи минус мы всё доказали, поэтому это есть
[01:08:40.540 --> 01:08:53.560]  интеграл от фи плюс от х dpx, минус интеграл с фи минус от х dpx по определению, интеграл
[01:08:53.560 --> 01:09:07.800]  лебего это интеграл от phi dx, что и требовалось, есть какие-то вопросы. Хорошо, значит, все, что осталось,
[01:09:07.800 --> 01:09:16.800]  это я давайте напомню теорему лебего по мажорируемой сходимости, просто напомню,
[01:09:16.800 --> 01:09:25.200]  тоже как свойства математического ожидания, и потом, если успеем, хотелось бы еще успеть вернуться к
[01:09:25.200 --> 01:09:30.160]  двум примерам, во-первых, для биномиального распления и нормального распления, и увидеть вот то,
[01:09:30.160 --> 01:09:38.720]  что я сказал, что можно посчитать от ожидания проще, уже имея то, что те свойства, которые я сформулировал.
[01:09:38.720 --> 01:09:56.920]  Что она говорит? Она говорит, что, предположим, у вас есть последовательность случайных величин
[01:09:56.920 --> 01:10:09.040]  ксен, которые мажорируются некоторым случайным величиной r, то есть модулем ожидания ксен меньше
[01:10:09.040 --> 01:10:18.520]  чем какая-то случайная величина r, это мажорит, который конечен. Значит, тогда, если вероятность того,
[01:10:18.520 --> 01:10:24.680]  что ксен стремится кси равна единице, ну, значит, что такое вероятность того, что ксен стремится кси
[01:10:24.680 --> 01:10:30.640]  равна единице, это вероятность события, да, множество таких омега, что ксен от омега стремится кси от омега.
[01:10:30.640 --> 01:10:40.400]  Если это вероятность равна единице, то тогда и мажорит ксен стремится к мажорит кси,
[01:10:40.400 --> 01:10:46.040]  при этом мажорит кси, конечно, конечна.
[01:10:46.040 --> 01:10:56.440]  И еще можно сказать, что мы от ожидания модуля разности между ксеноксистами и с к0.
[01:10:56.440 --> 01:11:07.040]  Вот, это те ремы ли вега мы уже идем сходить, из которых вы прекрасно знаете. И теперь давайте вернемся к
[01:11:07.040 --> 01:11:12.720]  двум примерам, к нормальному распределению и к бенминальному распределению, к нормальному
[01:11:12.720 --> 01:11:16.800]  распределению. Поговорим, как проще найти мы от ожидания для этих двух распределений.
[01:11:16.800 --> 01:11:27.120]  Во-первых, пусть кси имеет бенминальное распределение с параметром n и p. Заметим вот
[01:11:27.120 --> 01:11:31.320]  что. Заметим, что что такое бенминальное распределение с параметром n и p? Это количество
[01:11:31.320 --> 01:11:36.880]  успехов при n подбросках монетки. Вот вы n раз подброски независимо вероятность решки равна p,
[01:11:36.880 --> 01:11:43.240]  ваша случайная величина это количество решек. Тогда вы можете сказать, ну окей, давайте возьмем n
[01:11:43.240 --> 01:11:49.720]  случайных величин, кси1, кси n, которые имеют распределение Bernoulli с параметром p и которые
[01:11:49.720 --> 01:11:58.200]  независимы. Это индикаторы решки. Да, кси1 это индикатор того, чтобы при первом подброске получили
[01:11:58.200 --> 01:12:03.680]  решку, кси2 это индикатор того, чтобы при втором подброске получили решку и так далее. Понятно,
[01:12:03.680 --> 01:12:09.320]  что если вы их сложите, вы получите ровно количество решек при n независимых подбросках.
[01:12:09.320 --> 01:12:15.480]  То есть, иными словами, их сумма имеет то же самое бенминальное распределение с параметром n и p.
[01:12:15.480 --> 01:12:23.280]  А когда у вас две случайные величины, имеют одно и то же распределение, у них одинаковый
[01:12:23.280 --> 01:12:30.360]  мат ожидания. Поэтому мат ожидания кси, это в точности мат ожидания суммы ваших ксиитых.
[01:12:30.360 --> 01:12:35.440]  По линейности мат ожидания, мат ожидания суммы всегда равна сумме мат ожидания. Вам даже
[01:12:35.440 --> 01:12:40.720]  независимость нужна. Если бы они были зависимы, было бы все то же самое. Мат ожидания суммы равна
[01:12:40.720 --> 01:12:47.280]  сумме мат ожиданий. У Bernoulli распределение мат ожидания равно p, то есть у всех этих ксиитых
[01:12:47.280 --> 01:12:53.160]  мат ожидания равно p, значит получаем n, p. Да, вот без всяких вычислений, никакие вот эти трюки
[01:12:53.160 --> 01:12:58.640]  с биномиальным коэффициентом не нужно производить, чтобы получить ответ. Есть какие-то вопросы?
[01:13:04.640 --> 01:13:10.440]  А теперь возьмем нормальную случайную величину с параметром i7 в квадрате. И заметьте, вот какую
[01:13:10.440 --> 01:13:20.080]  интересную вещь, что если мы возьмем теперь случайную величину, стандартную нормальную,
[01:13:20.080 --> 01:13:33.840]  обозначим ее это, и возьмем и умножим это на корень i7 в квадрате и прибавим а, то мы получим
[01:13:33.840 --> 01:13:42.240]  нормальное распределение с параметром i7 в квадрате. Почему так? Ну по определению, если вы посмотрите на
[01:13:42.240 --> 01:13:50.840]  функцию распределения, вот этой случайной величины, корень i7 в квадрате на это плюс а в точке x,
[01:13:50.840 --> 01:13:57.960]  это есть не что иное, как вероятность того, что корень i7 в квадрате на это плюс а меньше 0,
[01:13:57.960 --> 01:14:05.000]  чем x. А это есть вероятность того, что это меньше 0, чем x-a поделить на корень i7 в квадрате.
[01:14:05.000 --> 01:14:12.040]  Дальше, подставляя в определение функции распределения, которая для нормального
[01:14:12.040 --> 01:14:18.600]  распределения есть просто интеграл, от минус бесконечности до вот этой вот точки x-a поделить на
[01:14:18.600 --> 01:14:24.920]  корень i7 в квадрате, от плотности 1 поделить на корень i7 из 2 pi e в степени минус t в квадрате
[01:14:24.920 --> 01:14:36.440]  пополам dt. Если вы сделаете замену, обозначите t за y-a поделить на корень i7 в квадрате,
[01:14:36.440 --> 01:14:45.800]  то получите в точности интеграл от минус бесконечности до x от 1 поделить на корень из
[01:14:45.800 --> 01:14:56.440]  2 pi e в степени минус y-a в квадрате поделить на 2 si в квадрате dy. А это есть как раз функция
[01:14:56.440 --> 01:15:01.520]  распределения случайно-вечной кси в точке x. То есть, иными словами, вот это линейное
[01:15:01.520 --> 01:15:05.680]  преобразование, которое вы можете проделать стандартным нормальным распределением, вам дает
[01:15:05.680 --> 01:15:11.440]  любые параметры нормального распределения. То есть, умножив на корень из второго параметра и
[01:15:11.440 --> 01:15:15.080]  прибавив первый параметр, вы получите нормальное распределение с параметром i7 в квадрате.
[01:15:15.080 --> 01:15:21.640]  Это позволяет считать мотожидание только для стандартного нормального распределения,
[01:15:21.640 --> 01:15:36.880]  потому что если вы знаете, так как вы знаете, что мотожидание это равно нулю, то из этого сразу
[01:15:36.880 --> 01:15:42.320]  следует, что мотожидание кси совпадает с мотожиданием от корня i7 в квадрате это плюс а,
[01:15:42.320 --> 01:15:50.120]  но совпадает то, что у них распределение одинаковое. И у кси нормальная с параметром i7 в квадрате,
[01:15:50.120 --> 01:15:55.560]  и вот этой вот суммой тоже нормальная с параметром i7 в квадрате. А здесь по линейности вы получаете
[01:15:55.560 --> 01:16:01.160]  корень из i7 в квадрате на мотожидание это и плюс а, и так как мотожидание это это ноль,
[01:16:01.160 --> 01:16:06.760]  вы получаете а. То есть, достаточно было считать мотожидание только для стандартного нормального,
[01:16:06.760 --> 01:16:12.280]  это чуть проще. Давайте вспомним, как вы там считали мотожидание для просто нормального
[01:16:12.280 --> 01:16:23.240]  распределения. Вот нам тут пришлось представлять вот эти суммы. Но если бы у нас не было вот этого а,
[01:16:23.240 --> 01:16:28.680]  то мы бы сразу сказали, что это функция нечетная. Вот эта вот функция под интегралом нечетный,
[01:16:28.680 --> 01:16:34.360]  просто сразу сказали, что это ноль. А когда у нас есть а, ну вот нам пришлось вот так вот вычитать
[01:16:34.360 --> 01:16:39.800]  а и добавлять а, и потом замечать, что вот это ноль. Так бы у нас вот этой операции бы не,
[01:16:39.800 --> 01:16:45.760]  мы бы не проделывали, если бы мы умели бы сразу выводить из стандартного нормального распределения
[01:16:45.760 --> 01:16:54.440]  любое другое. Так, ну на этом все, что я хотел рассказать сегодня. Если какие-то вопросы,
[01:16:54.440 --> 01:17:09.840]  пожалуйста задавайте. Пока временно, к сожалению, лекции я вынужден перенести в онлайн-формат.
[01:17:09.840 --> 01:17:18.400]  В общем, посмотрим, что будет происходить дальше. Смогу ли я выходить в аудиторию в какой-то
[01:17:18.400 --> 01:17:23.320]  момент или нет. В общем, ближайшая лекция будет тоже в онлайне. Прошу за это прощение и надеюсь,
[01:17:23.320 --> 01:17:29.720]  что это не сильно влияет на качество. Я буду своевременно выкладывать видеозаписи и конспекты
[01:17:29.720 --> 01:17:36.720]  лекций, которые я пишу во время лекции. Надеюсь, что это будет даже удобнее, чем обычный формат.
[01:17:36.720 --> 01:17:40.880]  Пожалуйста, не стесняйтесь во время лекции задавать. Можно больше вопросов, если они у вас есть,
[01:17:40.880 --> 01:17:46.320]  не откладывайте это делом. Но потом всегда проще разобраться во время лекции, задать какие-то
[01:17:46.320 --> 01:17:53.720]  вопросы, чем потом не понимать какие-то места при просмотре видео. До встречи в следующую субботу.

[00:00.000 --> 00:12.000]  Метод Ньютона и квазинтонские методы. Метод Ньютона сходится очень быстро, но требует больших ресурсов.
[00:12.000 --> 00:18.000]  Квазинтонские методы сходятся тоже чуть-чуть медленнее, но требует существенно меньше ресурсов.
[00:18.000 --> 00:27.000]  Сейчас будем понимать, как это все дело строится, в чем там специфика работы и как это все можно вызывать из стандартных пакетов
[00:27.000 --> 00:31.000]  и каких-то задач, которые могут возникнуть на практике.
[00:31.000 --> 00:41.000]  Метод Ньютона. Смотрим на задачи минимизации выпуклой функции. Идея в чем. Метод является методом второго порядка.
[00:41.000 --> 00:50.000]  То есть для его получения, для заказа и выражения, который будет давать пересчет в следующей точке, надо знать дисян.
[00:50.000 --> 00:54.000]  То есть функция должна быть дважды деференцируемой.
[00:54.000 --> 01:03.000]  Это важно. Если это так, то мы можем построить в данной точке x квадратичную аппроксимацию с крышкой.
[01:03.000 --> 01:09.000]  Такой вид имеет. Просто второй порядок.
[01:09.000 --> 01:18.000]  Если дисян строго положительно определен, то есть функция является строго выпуклой,
[01:18.000 --> 01:27.000]  тогда мы можем записать вот такую задачу минимизации, то есть заменить функцию исходную на квадратичную аппроксимацию
[01:27.000 --> 01:34.000]  и сказать, что давайте мы будем в данной точке минимизировать эту функцию по h, тем самым надеясь получить направление,
[01:34.000 --> 01:38.000]  по которому надо двигаться для ее уменьшения.
[01:38.000 --> 01:45.000]  Ну и соответственно, поскольку это все выпукло, то условие первого порядка дает нам, что оно нам дает.
[01:45.000 --> 01:52.000]  То есть он дает то, что в точке минимума градиент равен нулю.
[01:52.000 --> 01:56.000]  И это также является достаточным условием.
[01:56.000 --> 02:03.000]  То есть мы записываем градиент вот этой функции, приравним его к нулю и выражаем аж на звездочку.
[02:03.000 --> 02:13.000]  Отсюда аж на звездочку имеет такой вид, что это в общем эта штука является процедурой решения систем линейных уравнений.
[02:13.000 --> 02:21.000]  И метод Ньютона записывается вот таким вот образом в рамочке, что мы из текущей точки перемещаемся в следующую,
[02:21.000 --> 02:32.000]  по направлению, которая является решением линейной системы с матрицы Гесиана и минус градиент правой части.
[02:32.000 --> 02:34.000]  Понятно ли это?
[02:34.000 --> 02:38.000]  Этот слайд. Или есть какие-то вопросы?
[02:38.000 --> 02:41.000]  Прекрасно.
[02:41.000 --> 02:50.000]  Важный момент, что вот это направление нужно считать не как вычисление обратной матрицы и умножение ее на вектор.
[02:50.000 --> 02:55.000]  Для того, чтобы решать линейную систему, не нужно считать обратную матрицу.
[02:55.000 --> 03:01.000]  Нужно сделать иллю-разложение, так называемое, которое факторизует матрицу на нижней верхней треугольной.
[03:01.000 --> 03:08.000]  И потом решает набор систем с такого рода матрицы.
[03:08.000 --> 03:12.000]  Это делается за 1 ватт на квадрат.
[03:12.000 --> 03:17.000]  Но понятно, что само разложение будет стоить 1 куб.
[03:17.000 --> 03:23.000]  Как и стандартная процедура решения линейной системы.
[03:23.000 --> 03:29.000]  Это канонический метод Ньютона, про который будут следующие несколько слайдов.
[03:29.000 --> 03:38.000]  После чего мы перейдем к его модификациям и, соответственно, к Возюмтовским методам.
[03:38.000 --> 03:43.000]  После того, как поймем, в чем проблема метод Ньютон.
[03:43.000 --> 03:49.000]  Метод Ньютон также вы могли встретить в теме про решение системы нелинейных уравнений.
[03:49.000 --> 03:55.000]  Например, у вас есть вот такая вот система, и вам надо найти X.
[03:55.000 --> 04:00.000]  Тут была немножко другая история. Вы считали приближение линейное.
[04:00.000 --> 04:04.000]  То есть тут G штрих этой матрице якобы.
[04:04.000 --> 04:09.000]  И получали delta X. И по таким же ровным правилам.
[04:09.000 --> 04:16.000]  Только вместо решения системы с градиентом, тут было решение системы с якобы.
[04:16.000 --> 04:20.000]  И соответственно, метод Ньютона был таким.
[04:20.000 --> 04:24.000]  Какая у этого метода связь с нашей задачей?
[04:24.000 --> 04:31.000]  Очень простая. Если у нас срива функция выпукла, то условия оптимальности записываются вот таким вот образом.
[04:31.000 --> 04:36.000]  А это есть не что иное, как нелинейных уравнений.
[04:36.000 --> 04:47.000]  Поэтому, когда мы ищем направление, чтобы получить направление движения метод Ньютона,
[04:47.000 --> 04:55.000]  то это то же самое, что и направление для поиска корня функции G.
[04:55.000 --> 05:01.000]  Тем не менее, несложно заметить, что метод Ньютона для решения уравнений является более общим.
[05:01.000 --> 05:08.000]  Как вы думаете, почему? То есть это более общая штука, чем метод Ньютона для решения задачи оптимизации.
[05:08.000 --> 05:18.000]  Возглядя на этот слайд, можно внимательно на него посмотреть и увидеть, в чем там разница.
[05:18.000 --> 05:22.000]  Есть идеи? Не очень.
[05:22.000 --> 05:24.000]  Окей. Смотрите, в чем дело.
[05:24.000 --> 05:38.000]  Когда здесь мы записываем условно икебян, то в случае, когда у нас система, то этот икебян вырождается в гессианах.
[05:38.000 --> 05:45.000]  Однако никто не сказал, что... Ну и про гессиан мы знаем, что это симметричная матрица, как минимум.
[05:45.000 --> 05:49.000]  Положить на определенный, в случае выпуковом случае, все понятно.
[05:49.000 --> 05:56.000]  Но когда мы рассматриваем эту задачу, то, что вот тут матрицейкой будет быть такими свойствами, никто не сказал.
[05:56.000 --> 06:03.000]  То есть, в общем случае, тут может быть какая-то несимметричная матрица, которая будет довольно хитро устроена.
[06:03.000 --> 06:17.000]  Поэтому анализ метода Ньютон для решения нелинейной системы уравнений более сложная задача, чем его анализ для решения задачи оптимизации.
[06:19.000 --> 06:27.000]  В общем случае, сходимость нетривиальна исследования.
[06:27.000 --> 06:31.000]  И можно... Тут есть ссылочка на фракталы Ньютона.
[06:31.000 --> 06:36.000]  Это такая штука, которая возникает в процессе интеррирования методом Ньютона для самых простых уравнений.
[06:36.000 --> 06:40.000]  Если интересно, можете посмотреть, тут довольно красивая картина.
[06:40.000 --> 06:45.000]  Что известно про сходимость метода Ньютона?
[06:45.000 --> 06:53.000]  Первое результат, который мы будем получать, это предположение, что у нас положительный определенный кисян, строго.
[06:53.000 --> 07:02.000]  Если это неправда, то метод перестает работать, имеется в виду, что надо его как-то подкручивать так, чтобы он не...
[07:02.000 --> 07:06.000]  Ну, в случае, если функция не выпукла, он вообще может оказаться в...
[07:06.000 --> 07:13.000]  Ну, не выпукла и не ограничена снизу, он может вообще улететь без бесконечности, и ничто его не спасет.
[07:13.000 --> 07:19.000]  Если же функция просто выпукла, и там есть некоторые критические точки, где кисян 0,
[07:19.000 --> 07:26.000]  то там можно его некоторым образом регулиризовать так, чтобы двигаться в epsilon-окрестности этой точки,
[07:26.000 --> 07:32.000]  где кисян всегда положительно определен, но довольно плохо обусловлен, скажем так.
[07:32.000 --> 07:41.000]  Значит, сходимость локальная, то есть в зависимости от выбора x0, метод может сходиться, расходиться или эсценировать.
[07:41.000 --> 07:48.000]  То есть тут такой довольно капризный метод, который не для любого x0 будет сходиться,
[07:48.000 --> 07:55.000]  в отличие от градиентного спуска, который в случае выпуклой функции у нас сходился из любой точки.
[07:55.000 --> 08:05.000]  Для того, чтобы заставить его сходиться из любой точки, его специально подправляют, а именно добавляют шаг,
[08:05.000 --> 08:11.000]  который также подбирается некоторыми адаптивными техниками, и это все называют демпфированный мета-нютн.
[08:11.000 --> 08:17.000]  Но создемпфированный, потому что направление немножко шкалируется на этот самый альфа-кат.
[08:17.000 --> 08:25.000]  И выбирать этот шаг, выбирать его по наложенному спуску, чтобы там было достаточно существенного убывания,
[08:25.000 --> 08:31.000]  правил армии, все эти истории, они в этом случае также начинают работать.
[08:31.000 --> 08:39.000]  Введение шага существенно расширяет обысходимости, то есть он начинает сходиться из любой точки,
[08:39.000 --> 08:45.000]  но скорая исходимость начинает страдать. Сейчас посмотрим, как именно это происходит.
[08:45.000 --> 08:53.000]  Во-первых, при всех этих несильно обремительных условиях можно получить локальную сверхлинную исходимость.
[08:53.000 --> 08:59.000]  Как это сделать? Пусть в локальном минимуме градиент 0, 10 положительно определен,
[08:59.000 --> 09:11.000]  тогда, расписав ряд Тейлора в точке их со звездочкой, мы получим вот такое приближение для функции, которая равна градиенту.
[09:11.000 --> 09:19.000]  Понятно, откуда это взялось? Почему это так выглядит?
[09:19.000 --> 09:27.000]  Вот тогда, поскольку у нас есть строгая положительная определенность,
[09:27.000 --> 09:37.000]  то умножим обе части на обратный гессиан, мы получим слева, перенеся все необходимое в левую часть,
[09:37.000 --> 09:45.000]  мы получим здесь разность xk и x со звездочкой, а здесь получим решение системы матрицы гессиана и минус градиент правой части.
[09:45.000 --> 09:51.000]  И эта штука равна умалому от нормы разности x со звездочкой и xк.
[09:51.000 --> 09:59.000]  Поскольку мы знаем, что тут у нас по сути дела записана одна итерация метода Ньютона.
[09:59.000 --> 10:09.000]  Таким образом, мы видим, что у нас справедливо следующее равенство, что xk плюс 1 минус x со звездочкой это умалое от нормы разности x со звездочкой и предыдущей точки.
[10:09.000 --> 10:15.000]  Это значит, что предел отношения равен нулю.
[10:15.000 --> 10:24.000]  Это значит, что сходимость сверхлинейная, потому что если бы она была линейная, то предел был бы равен Константику, которая фигурировала в линейной сходимости.
[10:24.000 --> 10:31.000]  Это понятно? Почему так произошло?
[10:31.000 --> 10:38.000]  Хорошо. Получили пока что сверхлинейную сходимость при его несильно обремительных условиях.
[10:38.000 --> 10:43.000]  Теперь давайте получим локальную квадратичную сходимость, более сильный результат.
[10:43.000 --> 10:49.000]  Пусть тут уже поскольку результат более сильный, то и условий надо побольше.
[10:49.000 --> 11:04.000]  Требуется локальная сильная выпуклость с константы мю, то есть чтобы в x со звездочкой гессиан был отделен, его спектр был отделен от нуля на константу мю.
[11:04.000 --> 11:13.000]  Также нужно липшить его с гессианом с константы м, и близость x0 к x со звездочкой вот настолько.
[11:14.000 --> 11:27.000]  Тогда есть квадратичная сходимость, то есть норма между x ка плюс 1 и x со звездочкой меньше либо равна, чем констант, умноженный на норму в квадрате между x ка и x со звездочкой.
[11:27.000 --> 11:37.000]  То есть вот этот квадрат как раз таки гарантирует квадратичную сходимость и удвоение числозначих цифр в записи на каждой итерации.
[11:37.000 --> 11:45.000]  На графике скоро, буквально здесь, можно увидеть как именно отличается сходимость градиентного спуска и метода Ньютона.
[11:45.000 --> 11:56.000]  То есть градиентный спуск сходится линейно, вот она линейная сходимость, метод Ньютона сходится квадратично, локально и линейно в удалении от точки мимо.
[11:56.000 --> 11:58.000]  То есть смотрите, что происходит.
[11:58.000 --> 12:06.000]  Тут график для демпфированного метода Ньютона, поэтому здесь мы шаг подбираем, и он не равен единице, то есть он меньше единицы.
[12:06.000 --> 12:10.000]  Поскольку мы его начинаем итерировать, начинаем подбирать единицы.
[12:10.000 --> 12:20.000]  И потихоньку, если у нас наше направление приводит к возрастанию значений, то мы делим шаг на два условия, на три.
[12:20.000 --> 12:23.000]  Умножаем на какое-то число меньше единицы.
[12:23.000 --> 12:33.000]  Что дальше? Дальше мы пришли в некоторую окрестность, и уже в этой окрестности мы начинаем сходиться квадратично.
[12:33.000 --> 12:45.000]  То есть видите, вот уже начиная отсюда, тут было условно 10 в первой, тут стало 10 в минус первой, здесь стало 10 в минус третий, четвертый, и здесь стало 10 в минус восьмой.
[12:45.000 --> 12:51.000]  То есть каждый раз норма градиента становится меньше в квадрат раз.
[12:53.000 --> 13:02.000]  В отличие от того, как выглядела сходимость для градиентного спуска, когда мы просто эту норму умножали на какое-то число меньше единицы.
[13:02.000 --> 13:06.000]  Понятно ли в чем разница линии неквадратической сходимости?
[13:12.000 --> 13:15.000]  Теперь доказательства. Доказательства 9 шагов.
[13:15.000 --> 13:20.000]  Значит первое, введем необходимое определение.
[13:20.000 --> 13:32.000]  rk плюс первое это будет наша ошибка, которая будет связана с rk вот таким вот образом, потому что если подставить, то станет ясно, что здесь стоит просто rk.
[13:32.000 --> 13:36.000]  А тут остается выражение для направлений.
[13:37.000 --> 13:45.000]  Теория Манюттона Лейблица. Напоминаю про интегралы производную и значение.
[13:45.000 --> 13:50.000]  Ну в общем то, как интеграл от функции на отрезке можно выразить через значение на краях.
[13:50.000 --> 13:54.000]  То же самое можно записать для градиентов.
[13:54.000 --> 14:01.000]  Градиент в точке xk это градиент в точке xk минус ноль, то есть градиент в точке x звездочкой.
[14:01.000 --> 14:09.000]  И это интеграл от f2' вот это вот ровно то же самое, что и вот это.
[14:09.000 --> 14:23.000]  То есть тут теперь f2' точка где-то между xk и x звездочкой, и это умножается на rk, то есть на xk минус x звездочка.
[14:23.000 --> 14:28.000]  Вот там общение формулы Манюттона Лейблица на многомерный случай.
[14:28.000 --> 14:33.000]  Отставляем вот сюда. Вот и получаем следующий результат.
[14:33.000 --> 14:46.000]  Что rk плюс первое это идентичная матрица, минус обратный гисян, умноженный на вот такую штуку интеграл и умноженный на rk.
[14:46.000 --> 14:51.000]  В силу того, что такое матричная векторная норма, у нас есть свойство публикативности.
[14:51.000 --> 14:57.000]  То есть норма rk плюс первого ограничена сверхпроведением норм ЖКТового и РКТового.
[14:57.000 --> 15:02.000]  Все ли понятно, что было на этом слайде?
[15:02.000 --> 15:04.000]  Да.
[15:04.000 --> 15:10.000]  Прекрасно. Теперь нам осталось разобраться с тем, как норму ЖКТ оценить.
[15:10.000 --> 15:15.000]  Тут нам пригодится Лейблица с гисяном. ЖКТ у нас вот такая вот штука.
[15:15.000 --> 15:23.000]  Поэтому, что мы делаем?
[15:23.000 --> 15:30.000]  Мы вот эту идентичную матрицу переписываем как произведение гисяна и обратного.
[15:30.000 --> 15:37.000]  То есть обратного и гисяна. После вынесения обратного гисяна мы получаем интеграл такой разности,
[15:37.000 --> 15:46.000]  что, соответственно, после взятия нормы превратится в произведение нормы вот этого на интеграл от нормы разности.
[15:46.000 --> 15:50.000]  А эта штука фигурирует у нас в липчество из гисяна.
[15:50.000 --> 15:53.000]  То есть это m на норму разности аргументов.
[15:53.000 --> 15:59.000]  А норма разности аргументов это по сути rk минус t на rk.
[15:59.000 --> 16:06.000]  Отсюда получаем оценку.
[16:06.000 --> 16:10.000]  Тут, по-моему, квадрат где-то потерялся.
[16:10.000 --> 16:14.000]  Нет, квадрат не потерялся, все в порядке.
[16:14.000 --> 16:19.000]  Получаем вот такую вот оценку.
[16:19.000 --> 16:21.000]  Смотрите, что получилось.
[16:21.000 --> 16:28.000]  Тут фигурирует норма rk, а ЖК мы оценили сверху вот так.
[16:28.000 --> 16:31.000]  Оценили, по крайней мере, вот тот кусочек.
[16:31.000 --> 16:33.000]  То есть тут еще одна rk появилась.
[16:33.000 --> 16:38.000]  Теперь осталась норма гисяна в минус 1 оценить снизу и все будет хорошо.
[16:41.000 --> 16:47.000]  Это нам позволяет сделать липчество или гисяна и сильный выпуск.
[16:47.000 --> 16:54.000]  Поскольку у нас справедлива следующая оценка на гисян,
[16:54.000 --> 17:04.000]  то это следует напрямую из-за липчества с гисяном.
[17:04.000 --> 17:11.000]  Вот что, раз у нас норма разности меньше либо равна, чем m на rk t,
[17:11.000 --> 17:19.000]  значит сами гисяны в тех точках, которые рассматриваются, связаны вот таким вот образом.
[17:25.000 --> 17:31.000]  Что в свою очередь к силу сильной выпуклости можно записать вот так.
[17:31.000 --> 17:39.000]  То есть мы вот это вот выражение заменяем на mu умножить на единичную матрицу.
[17:39.000 --> 17:44.000]  Соответственно, когда мы будем оценивать норму,
[17:44.000 --> 17:48.000]  нам нужно оценить норму обратной матрицы к вот этой,
[17:48.000 --> 17:56.000]  то мы получим вот такой вот неразь, просто единицы делить на норму вот этой штуки.
[17:56.000 --> 18:00.000]  Ну, собственно, вот все, что нам нужно было получить.
[18:00.000 --> 18:04.000]  То есть складывая все вместе, мы получаем итоговую оценку,
[18:04.000 --> 18:09.000]  что тут у нас m на r квадрат и тут деление на 2 умножить на вот это выражение.
[18:09.000 --> 18:14.000]  Собственно, вот эта вот оценка, она откуда берется?
[18:14.000 --> 18:20.000]  Она берется из требований, чтобы вот в начальной точке у нас
[18:20.000 --> 18:25.000]  величина, на которую умножается сама норма ошибки,
[18:25.000 --> 18:30.000]  так это надо написать лучше на доске.
[18:33.000 --> 18:35.000]  Сейчас я напишу.
[18:35.000 --> 18:36.000]  Оп.
[18:41.000 --> 18:44.000]  Так, это не то, это не то.
[19:01.000 --> 19:03.000]  Так, так, так, так, так, так.
[19:05.000 --> 19:06.000]  Ой.
[19:10.000 --> 19:12.000]  Так, все получилось.
[19:15.000 --> 19:16.000]  Share screen.
[19:16.000 --> 19:19.000]  Так, сейчас плагин надо установить.
[19:19.000 --> 19:22.000]  Я надеюсь, не придется сейчас никуда переходить,
[19:22.000 --> 19:23.000]  перевыходить и все заработать.
[19:23.000 --> 19:25.000]  О, все заработало, ура.
[19:26.000 --> 19:29.000]  Ой, что такое?
[19:30.000 --> 19:34.000]  Так, да, я забыл, что тут никогда ничего не работает.
[19:34.000 --> 19:39.000]  Так, сейчас секундочку, я в другой части подключусь.
[19:49.000 --> 19:52.000]  Так, ага, вот так.
[19:54.000 --> 19:55.000]  И...
[19:55.000 --> 19:56.000]  Вот так.
[19:56.000 --> 19:59.000]  Сейчас, я надеюсь, может, немножко пропаду, но надеюсь,
[19:59.000 --> 20:00.000]  немного долго.
[20:01.000 --> 20:03.000]  Если даже и пропаду.
[20:04.000 --> 20:06.000]  О, все, сейчас все точно будет хорошо.
[20:07.000 --> 20:08.000]  Так.
[20:09.000 --> 20:10.000]  Ага.
[20:16.000 --> 20:19.000]  Убирайся, ничего не работает.
[20:20.000 --> 20:21.000]  Убирайся.
[20:23.000 --> 20:25.000]  Убирайся, ничего не работает.
[20:41.000 --> 20:43.000]  О, ура, все заработало.
[20:44.000 --> 20:48.000]  В итоге, наше исходное выражение имело вот такой вид,
[20:49.000 --> 21:18.000]  Сейчас, если вы ничего не видите, ничего страшного, я сейчас пишу с слайда, mu-m, вот так, вот, так, тут стереть, конечно, вот, и нам надо, чтобы, вот, в итоге-то, вот эта штука, это что такое, это m на норму xk-x'
[21:18.000 --> 21:28.000]  умножается на норму xk-x'
[21:28.000 --> 21:36.000]  вот, вот эта штука должна быть меньше единицы, вот, если мы явным образом расписываем это требование
[21:36.000 --> 22:05.000]  вот, и потом сделаем необходимое преобразование, ну, то есть, тут как бы вместо k поставим x0, например, да, что на первой итерации мы хотим, чтобы у нас x1-x' было меньше, либо равно, чем там что-то умножить на x0-x'
[22:05.000 --> 22:14.000]  и вот это что-то должно быть меньше единицы, это довольно естественное желание, чтобы расстояние все-таки уменьшилось к x'
[22:14.000 --> 22:27.000]  вот, тогда, перенеся влево-вправо, получим, что x0-x' меньше, либо равно, чем 2 mu делить на 3 m
[22:27.000 --> 22:31.000]  понятно? откуда это взялось? да
[22:31.000 --> 22:36.000]  хорошо, возвращаемся к слайдам тогда
[22:36.000 --> 22:40.000]  так, это была теорема про сходимость метода Ньютона
[22:40.000 --> 22:43.000]  значит, какие у метода Ньютона достоинства и недостатки?
[22:43.000 --> 22:58.000]  достоинства, трагичная сходимость, высокая точность решения, и, ну, вот, афина и неврядность перечисленная, но, в общем, это история, которая говорит о том, что метод не зависит, не меняется при афином преобразовании координат
[22:58.000 --> 23:03.000]  то есть, в градиентном спуске это не выполняется, вот, в метод Ньютона это выполняется
[23:03.000 --> 23:13.000]  вот, что плохого?
[23:13.000 --> 23:15.000]  довольно много всего плохого
[23:15.000 --> 23:18.000]  во-первых, надо гессен хранить, это инклады по памяти
[23:18.000 --> 23:21.000]  надо системы линейных уравнений решать, это n-куп
[23:22.000 --> 23:29.000]  ну и, там, гессен может быть вырожденным, но это вроде с какими-то некоторыми специальными решениями лечится
[23:29.000 --> 23:35.000]  но все равно нельзя сказать, что это прям легко и просто
[23:35.000 --> 23:38.000]  как это чинить?
[23:38.000 --> 23:42.000]  чинить надо следующим образом
[23:42.000 --> 23:48.000]  поскольку мы знаем метод, который лишен всех этих недостатков, а именно этой градиентной спуску
[23:48.000 --> 23:53.000]  то давайте посмотрим, что у них общего
[23:53.000 --> 24:00.000]  и попробуем найти некоторый баланс между достоинствами и недостатками этих двух методов
[24:00.000 --> 24:06.000]  значит, когда мы получали градиентный спуск, то
[24:06.000 --> 24:10.000]  мы основывались на следующей ассоциации сверху
[24:10.000 --> 24:14.000]  тут вместо единицы налихо была L
[24:14.000 --> 24:19.000]  ну и мы получали, что максимальный постоянный шаг это единица налихо
[24:19.000 --> 24:24.000]  то есть тут, по сути дела, тоже была использована квадратичная оценка
[24:24.000 --> 24:27.000]  но с единичной матрицы
[24:27.000 --> 24:32.000]  тогда у нас, с нашей звездочки, это просто минус альфа на градиент, получаем градиентность
[24:32.000 --> 24:38.000]  метод единицы был все то же самое, только вместо единичной матрицы стоял 10
[24:38.000 --> 24:41.000]  и мы получали метод налихо
[24:41.000 --> 24:44.000]  можно налиф поправить
[24:44.000 --> 24:51.000]  и нам нужно что-то, что будет по скорости сходимости лучше, чем градиентный спуск
[24:51.000 --> 24:54.000]  то есть будет больше, чем линейная сходимость
[24:54.000 --> 24:58.000]  но по времени одной итерации будет быстрее, чем мета ньютона
[25:01.000 --> 25:06.000]  понятно ли идея и то, на чем она будет основана?
[25:06.000 --> 25:10.000]  ну, на чем, собственно говоря, как мы будем строить?
[25:12.000 --> 25:16.000]  непонятно то, что мы хотим, а как строить, пока непонятно
[25:16.000 --> 25:21.000]  ну, погодите, да, сейчас как только понятно, что хотим, то и как строить станет, я скажу
[25:21.000 --> 25:24.000]  возьмем толстые методы, общая схема
[25:24.000 --> 25:30.000]  оцениваем нашу функцию квадратично, но подставляемся на некоторую матрицу B
[25:30.000 --> 25:34.000]  которая что-то среднее между единичной и 10
[25:34.000 --> 25:39.000]  чтобы быть лучше, чем градиентный спуск
[25:39.000 --> 25:46.000]  но быстрее, чем метод ньютона, поскольку матрица B будет обладать специальными свойствами, которыми мы сейчас получим
[25:46.000 --> 25:52.000]  значит, соответственно, минимум достигается в той же самой точке, минус Bk, минус первый градиент
[25:52.000 --> 25:56.000]  и квазинцовский метод – это вот такая вот штука
[25:56.000 --> 26:01.000]  то есть как смартфон BkT, собственно, будет посвящена оставшейся несколько слайдов
[26:01.000 --> 26:05.000]  ну, там, не оставшиеся, но следующие несколько слайдов, скажем так
[26:05.000 --> 26:08.000]  обозначение, которое тут используется
[26:08.000 --> 26:13.000]  BkT – это оценка на градиент, hkT – это оценка на гессиан
[26:13.000 --> 26:17.000]  hkT – это оценка на обратный гессиан
[26:17.000 --> 26:20.000]  далее будем это использовать
[26:20.000 --> 26:24.000]  знаете, какие требования к BkT мы предъявляем?
[26:24.000 --> 26:29.000]  нужно уметь быстро пересчитывать Bk плюс первое на основе BkT
[26:29.000 --> 26:34.000]  при условии, что у нас есть только градиенты, то есть мы хотим ограничиться первым порядком
[26:34.000 --> 26:38.000]  то есть вычислять только градиенты, никаких гессианов нам не надо
[26:38.000 --> 26:44.000]  но на основании градиентов строить оценку на гессиан
[26:44.000 --> 26:47.000]  это первое
[26:47.000 --> 26:53.000]  второе – быстрый поиск направления, то есть вот эта вот система должна решаться быстрее, чем за n-куп
[26:53.000 --> 26:58.000]  тогда мы удовлетворим нашим требованиям, что
[26:58.000 --> 27:04.000]  наш квазинтулский метод будет работать быстрее, чем метод Ньютона
[27:04.000 --> 27:08.000]  по скорости одной итерации
[27:08.000 --> 27:11.000]  и надеяться, что
[27:11.000 --> 27:14.000]  и с верхней линии сходить еще должно быть
[27:14.000 --> 27:19.000]  ну и BkT еще желательно как-то компактно хранить, но про это будет ближе к концу лекции
[27:19.000 --> 27:23.000]  отдельная модификация про компактное хранение
[27:24.000 --> 27:29.000]  ну давайте потихоньку эти требования думать, как их удовлетворить
[27:29.000 --> 27:35.000]  да, ну тут сайт про немного истории, я обычно рассказываю про то, что
[27:35.000 --> 27:39.000]  сначала его предложили физики, потому что у них ничего не работало
[27:39.000 --> 27:42.000]  потом хотели вопубликовать, ничего не опубликовалось
[27:42.000 --> 27:47.000]  30 лет было припринтом, все уже стали активно использовать, кучу модификаций получили
[27:47.000 --> 27:52.000]  в итоге только в 1991 году опубликовали в первом номере, вот я вам скажу, журнал по оптимизации эту работу
[27:52.000 --> 27:55.000]  в общем, такая вот хитрая история этих методов
[27:55.000 --> 27:58.000]  значит, обновление Bk
[27:58.000 --> 28:01.000]  правила двух градиентов
[28:01.000 --> 28:06.000]  звучит так, что, во-первых, градиент нашей квадратической оценки
[28:06.000 --> 28:12.000]  вот этой, в нуле, должен быть равен точному градиенту в точке xk
[28:12.000 --> 28:15.000]  ну и это так получается просто по построению
[28:15.000 --> 28:22.000]  потому что градиент fq равен вот этот градиент плюс Bk на h
[28:22.000 --> 28:27.000]  ну h равен нулю, Bk на h да нуляется, это просто градиент, это по построению квадрата
[28:27.000 --> 28:30.000]  но, поскольку мы знаем предыдущую точку
[28:30.000 --> 28:35.000]  то мы можем сказать, что если мы, имея эту функцию, шагнем по направлению
[28:35.000 --> 28:38.000]  которая приведет нас к предыдущую точку
[28:38.000 --> 28:42.000]  то мы должны получить точный градиент в этой предыдущей точке
[28:42.000 --> 28:45.000]  то есть, если мы шагнем назад
[28:45.000 --> 28:50.000]  то нам надо получить значение в точке xk
[28:50.000 --> 28:53.000]  значение градиента в точке xk
[28:53.000 --> 28:56.000]  отсюда, после подстановки
[28:56.000 --> 28:59.000]  получаем, что разность градиентов
[28:59.000 --> 29:02.000]  ну, получаем такое уравнение, короче говоря
[29:02.000 --> 29:06.000]  или, ну и в общем-то, квазинтонским уравнением
[29:06.000 --> 29:10.000]  которое записывается с помощью введения следующих обозначений
[29:10.000 --> 29:12.000]  с
[29:12.000 --> 29:14.000]  это разность х
[29:14.000 --> 29:15.000]  двух соседних
[29:15.000 --> 29:18.000]  а y это разность градиентов в соответствующих точках
[29:18.000 --> 29:21.000]  поэтому
[29:21.000 --> 29:23.000]  уравнение на B вот такое
[29:23.000 --> 29:27.000]  то есть Bk плюс 1 на h должно быть равно y
[29:27.000 --> 29:29.000]  вопрос
[29:29.000 --> 29:30.000]  софт 2
[29:30.000 --> 29:32.000]  всегда ли оно имеет решение
[29:32.000 --> 29:35.000]  и всегда ли оно единственное?
[29:35.000 --> 29:37.000]  как вы думаете?
[29:37.000 --> 29:40.000]  ну, это зависит от матрицы B
[29:40.000 --> 29:45.000]  давайте еще раз посмотрим, что это уравнение задает
[29:50.000 --> 29:53.000]  ну, что вот здесь известно, а что неизвестно?
[29:53.000 --> 29:55.000]  без каты неизвестно
[29:55.000 --> 29:56.000]  почему?
[29:56.000 --> 29:59.000]  мы же знаем текущую и следующую точку
[29:59.000 --> 30:03.000]  точнее, текущую и предыдущую
[30:03.000 --> 30:06.000]  а, оно относительно B, что ли, получается?
[30:06.000 --> 30:09.000]  ну да, это уравнение на B, совершенно верно
[30:12.000 --> 30:15.000]  ну, хорошо, теперь давайте на вопрос ответим
[30:15.000 --> 30:19.000]  ну, оно всегда имеет решение, но не единственное может быть
[30:19.000 --> 30:22.000]  ну, смотрите, про всегда этот вопрос
[30:22.000 --> 30:24.000]  поскольку он на матрицу B
[30:24.000 --> 30:27.000]  помните, было еще такое вот условие?
[30:29.000 --> 30:30.000]  а, точно
[30:30.000 --> 30:33.000]  поэтому давайте, когда все может сломаться?
[30:33.000 --> 30:39.000]  ну, когда там s-каты на y-каты, если там меньше нуля будет получаться?
[30:39.000 --> 30:40.000]  да, именно так
[30:40.000 --> 30:49.000]  то есть вот это вот требование того, чтобы угол между s-катом и y-катом был тупым, да?
[30:49.000 --> 30:51.000]  то есть острым
[30:52.000 --> 30:56.000]  то есть если вдруг эта штука меньше нуля, это произведение
[30:56.000 --> 30:58.000]  то все плохо
[30:58.000 --> 31:05.000]  поэтому нам нужно специально шаг альфа подбирать так, чтобы вот это вот скалярное произведение было правильного знака
[31:05.000 --> 31:07.000]  это отдельная история
[31:07.000 --> 31:09.000]  да, допустим, это выполнено
[31:09.000 --> 31:11.000]  значит, теперь про единственность
[31:12.000 --> 31:13.000]  да, нет?
[31:13.000 --> 31:14.000]  алло?
[31:14.000 --> 31:15.000]  вы тут?
[31:15.000 --> 31:16.000]  да
[31:16.000 --> 31:17.000]  хорошо
[31:17.000 --> 31:19.000]  что думаете про единственность?
[31:20.000 --> 31:23.000]  как вообще понять, будет ли решение единство или нет?
[31:24.000 --> 31:26.000]  что надо посмотреть?
[31:26.000 --> 31:30.000]  если решение не единственное, то выражение будет получаться
[31:31.000 --> 31:33.000]  кто выраженный?
[31:33.000 --> 31:36.000]  я забыл, что мы решаем
[31:36.000 --> 31:41.000]  вы еще не проснулись, что ли? У вас же перед этим пока лекция еще должна быть
[31:42.000 --> 31:43.000]  да
[31:44.000 --> 31:46.000]  давайте, соберитесь
[31:47.000 --> 31:49.000]  уже почти 12 часов
[31:49.000 --> 31:50.000]  ну
[31:51.000 --> 31:55.000]  на что надо посмотреть, чтобы понять единственность или нет?
[31:57.000 --> 31:59.000]  что при этой единственности с решением?
[32:02.000 --> 32:04.000]  допустим, у вас обычность тем или иным уравнения
[32:05.000 --> 32:07.000]  а х равно b
[32:10.000 --> 32:12.000]  когда решение единственное?
[32:15.000 --> 32:16.000]  относительно х?
[32:16.000 --> 32:17.000]  да
[32:17.000 --> 32:18.000]  когда?
[32:20.000 --> 32:22.000]  ну, когда они выражены
[32:24.000 --> 32:26.000]  а если выражено, то что?
[32:27.000 --> 32:30.000]  если выражено, если есть хоть одно решение, то там их бесконечно много будет
[32:31.000 --> 32:32.000]  ага
[32:33.000 --> 32:35.000]  а теперь давайте, то есть
[32:35.000 --> 32:39.000]  тут как бы, наверное, поскольку у нас-то задача на b
[32:42.000 --> 32:44.000]  что там? b
[32:48.000 --> 32:50.000]  bs равно y
[32:54.000 --> 32:56.000]  но считайте, что b это х
[33:00.000 --> 33:02.000]  то есть эта штука
[33:02.000 --> 33:04.000]  n на n
[33:04.000 --> 33:06.000]  это n, это n
[33:09.000 --> 33:12.000]  какие системы линиевых уравнений имеют единственное решение?
[33:12.000 --> 33:14.000]  а какие нет?
[33:14.000 --> 33:18.000]  ну тут я не понимаю, это относительно b, это непонятно
[33:18.000 --> 33:20.000]  ну а что непонятного?
[33:20.000 --> 33:22.000]  давайте распишем
[33:22.000 --> 33:24.000]  что b11s1
[33:24.000 --> 33:26.000]  плюс там b12s2
[33:26.000 --> 33:28.000]  там b13s3
[33:28.000 --> 33:30.000]  например, да?
[33:31.000 --> 33:34.000]  ну вот тут у вас было a11x1
[33:34.000 --> 33:36.000]  плюс a12x2
[33:36.000 --> 33:38.000]  плюс a13x3
[33:38.000 --> 33:41.000]  а, ну если vs0 есть, то, кажется, не единственное
[33:42.000 --> 33:43.000]  чего-чего?
[33:43.000 --> 33:45.000]  если vs0 есть, то не единственное
[33:47.000 --> 33:52.000]  потому что мы можем соответствующие элементы на диагонали увеличивать там, сколько хотим
[33:53.000 --> 33:57.000]  кажется, никакой положительной определенности там не испортим
[33:59.000 --> 34:01.000]  ну или что-нибудь такое
[34:01.000 --> 34:03.000]  ну, на самом деле, ответ более...
[34:03.000 --> 34:05.000]  опираюсь на более общий факт
[34:05.000 --> 34:09.000]  о том, что если у вас число уравнений меньше, чем число неизвестных
[34:09.000 --> 34:13.000]  ну, типа, ax равно b, но матрица у вас вот такая вот
[34:19.000 --> 34:21.000]  то есть число уравнений, условно, там 10
[34:21.000 --> 34:23.000]  а матрица 100 на 100
[34:23.000 --> 34:25.000]  ой, тут я криво нарисовал, вот так надо нарисовать
[34:30.000 --> 34:32.000]  то, что тут будет, единственное или не единственное?
[34:32.000 --> 34:33.000]  ну не единственное
[34:33.000 --> 34:35.000]  ну не единственное, конечно
[34:35.000 --> 34:37.000]  ну вот так вот
[34:37.000 --> 34:39.000]  конечно, конечно
[34:39.000 --> 34:41.000]  но вот это наша ситуация
[34:41.000 --> 34:45.000]  у нас n квадрат, ну не n квадрат, а n-1,5, по-моему, да?
[34:45.000 --> 34:47.000]  или плюс один?
[34:47.000 --> 34:49.000]  минус один, помню
[34:49.000 --> 34:51.000]  почему?
[34:51.000 --> 34:53.000]  ну, от одного до n
[34:55.000 --> 34:57.000]  от одного до... да, плюс один, правда?
[34:57.000 --> 35:01.000]  неизвестных сил симметричности
[35:01.000 --> 35:03.000]  и n уравнений
[35:04.000 --> 35:06.000]  ладно, я понял, кажется
[35:06.000 --> 35:09.000]  как бы, вот абсолютно вот этот случай
[35:11.000 --> 35:13.000]  вот, в итоге
[35:13.000 --> 35:15.000]  решение единственное, действительно
[35:15.000 --> 35:18.000]  чтобы сделать его единственным, надо немножко подкрутить
[35:18.000 --> 35:20.000]  как обычно
[35:20.000 --> 35:23.000]  этот подход, а именно сказать, что
[35:23.000 --> 35:25.000]  новая цена 50 должна быть как текущей
[35:25.000 --> 35:27.000]  то есть у нас
[35:27.000 --> 35:29.000]  получается
[35:29.000 --> 35:32.000]  дополнительная задача оптимизации
[35:32.000 --> 35:34.000]  которая минимизирует
[35:34.000 --> 35:37.000]  норму B-BKT
[35:37.000 --> 35:39.000]  и требует, чтобы новая матрица
[35:39.000 --> 35:41.000]  удовлетворяла вот этому уравнению
[35:41.000 --> 35:44.000]  вот, сейчас будем потихонечку это формализовывать
[35:44.000 --> 35:47.000]  значит, необходим B0 задать нам
[35:47.000 --> 35:51.000]  обычно B0 задать как гамма на единичную матрицу
[35:51.000 --> 35:54.000]  параметр в процедуре поиска шага
[35:54.000 --> 35:57.000]  значит, все вычисления, которые мы будем проводить
[35:57.000 --> 36:00.000]  надо сделать так, чтобы они имели сложность
[36:00.000 --> 36:02.000]  меньше, чем n-1
[36:02.000 --> 36:04.000]  потому что мы хотим быть быстрее
[36:04.000 --> 36:06.000]  на каждой итерации метода ньютона
[36:06.000 --> 36:08.000]  ассигнатической
[36:08.000 --> 36:10.000]  ну и, собственно, примеры, которые мы сейчас
[36:10.000 --> 36:12.000]  пытаемся успеть разобрать
[36:12.000 --> 36:14.000]  это Бразилай Бурвейн, ДФП и БФГС
[36:14.000 --> 36:18.000]  и его версия заграничной памяти
[36:18.000 --> 36:20.000]  метод Бразилая Бурвейн
[36:20.000 --> 36:22.000]  очень простой метод
[36:22.000 --> 36:25.000]  опроксимируем гессиан диагональной матрицей
[36:25.000 --> 36:28.000]  направление нашего гридетом методе было вот таким
[36:28.000 --> 36:30.000]  его можно записать как
[36:30.000 --> 36:33.000]  альфа, кан и единичную матрицу на гридиан
[36:33.000 --> 36:36.000]  вот эта штука, в свою очередь, есть не что иное
[36:36.000 --> 36:39.000]  как обратная матрица к вот такой матрице
[36:42.000 --> 36:45.000]  вот, а это так, тут опять лишняя скобка, я все никак не поправлю
[36:45.000 --> 36:46.000]  18 слайд
[36:46.000 --> 36:49.000]  ну, значит, не лишняя, не хватает вот этой стороны скобки
[36:49.000 --> 36:53.000]  то есть, это примерно обратный гессиан на гридиан
[36:53.000 --> 36:56.000]  ну, значит, давайте подставим вот это вот условие
[36:56.000 --> 36:58.000]  вот это вот модель нашего гессиана
[36:58.000 --> 37:00.000]  в квазинтуловском уравнении
[37:00.000 --> 37:04.000]  мы получим вот такое вот уравнение на альфу
[37:07.000 --> 37:11.000]  то есть, нам надо, по сути дела, минимизировать вот норму такой разности
[37:11.000 --> 37:15.000]  эта задача решается аналитически, она вообще-то ну сколяр
[37:15.000 --> 37:18.000]  то есть, она просто в квазинтуловский метод
[37:18.000 --> 37:20.000]  выродился в метод поиска шага
[37:20.000 --> 37:22.000]  представляете, какая прелесть
[37:22.000 --> 37:24.000]  что
[37:24.000 --> 37:26.000]  строили-строили
[37:26.000 --> 37:28.000]  в простейшем случае
[37:28.000 --> 37:31.000]  оказывается, это все то же самое, что и шаг подобрать правильный
[37:31.000 --> 37:33.000]  вот, и
[37:33.000 --> 37:35.000]  он имеет аналитическое решение
[37:35.000 --> 37:36.000]  вот такое
[37:36.000 --> 37:38.000]  то есть, шаг
[37:38.000 --> 37:40.000]  надо ОЛАТН операции сделать
[37:40.000 --> 37:43.000]  вообще, конечное число, все детерминировано
[37:43.000 --> 37:46.000]  никаких там параметров в процедуре подбора шага
[37:46.000 --> 37:47.000]  ничего этого нет
[37:47.000 --> 37:49.000]  и все вполне себе замечательно работает
[37:51.000 --> 37:53.000]  понятен ли подход?
[37:54.000 --> 37:55.000]  да
[37:56.000 --> 37:57.000]  отлично
[37:57.000 --> 38:00.000]  да, значит, храническая модификация, статья на ней 2016 года
[38:00.000 --> 38:03.000]  на ней все, в общем, рекомендую посмотреть, если интересно
[38:03.000 --> 38:07.000]  метод DFP, собственно говоря, явно образом решает задачу минимизации
[38:07.000 --> 38:10.000]  то есть, поиску ближайшей матрицы
[38:10.000 --> 38:12.000]  текущей
[38:12.000 --> 38:14.000]  для которой выполняется квазинтуловское уравнение
[38:14.000 --> 38:18.000]  обычно тут стоит пробедение своего нормы, но это, в общем, как это решается
[38:18.000 --> 38:20.000]  оставим за скобками
[38:20.000 --> 38:22.000]  вот, и запишем явное решение
[38:22.000 --> 38:25.000]  вот оно, оно общеизвестное в литературе
[38:25.000 --> 38:27.000]  приводится
[38:27.000 --> 38:30.000]  вот, и, значит
[38:30.000 --> 38:34.000]  замечательно то, что такая структура матрицы
[38:34.000 --> 38:37.000]  то есть, смотрите, тут матрица B с левой и с правой обкладывается
[38:37.000 --> 38:39.000]  типа проекторами
[38:39.000 --> 38:41.000]  в каком-то смысле
[38:41.000 --> 38:43.000]  вот, то есть они
[38:45.000 --> 38:49.000]  выглядят как единичный минус РО на некоторую матрицу РАНГ-1
[38:49.000 --> 38:51.000]  плюс матрица РАНГ-1
[38:51.000 --> 38:55.000]  и эту матрицу такой структуры можно обратить
[38:55.000 --> 38:57.000]  аналитически
[38:57.000 --> 38:59.000]  по формуле Шерману Моррисона Вудбеля
[38:59.000 --> 39:00.000]  вот она тут приведена
[39:00.000 --> 39:01.000]  точнее приведен ответ
[39:01.000 --> 39:03.000]  то есть, смотрите, чтобы
[39:03.000 --> 39:05.000]  допустим, у нас есть матрица B0
[39:05.000 --> 39:07.000]  что нам надо сделать?
[39:07.000 --> 39:10.000]  нам надо посчитать B0-1
[39:10.000 --> 39:14.000]  ну, учитывая, что B0 это единичная матрица на константу
[39:14.000 --> 39:16.000]  обратно считается просто
[39:16.000 --> 39:18.000]  вот, это наш H0
[39:18.000 --> 39:21.000]  дальше наш H1 высчитывается вот так вот
[39:21.000 --> 39:25.000]  давайте посчитаем, сколько операций нужно сделать, чтобы посчитать H1
[39:25.000 --> 39:27.000]  значит, во-первых
[39:27.000 --> 39:30.000]  тут идут какие-то сложения вычитания
[39:30.000 --> 39:32.000]  это все ОАТН квадрат
[39:32.000 --> 39:33.000]  далее
[39:33.000 --> 39:35.000]  знаменатель считается за ОАТН
[39:35.000 --> 39:38.000]  тут матрица РАНГ-1 ОАТН квадрат
[39:38.000 --> 39:39.000]  посчитать
[39:39.000 --> 39:42.000]  здесь тоже ОАТН квадрат
[39:42.000 --> 39:44.000]  потому что сложится самая сложная операция
[39:44.000 --> 39:47.000]  это умножение обратного гессиана на вектор H0
[39:47.000 --> 39:53.000]  здесь произведение тоже матрица РАНГ-1
[39:53.000 --> 39:57.000]  потому что вот эта штука равна вот этой штуке транспонированной
[39:57.000 --> 40:00.000]  поэтому мы один раз считаем Hk на yk
[40:00.000 --> 40:03.000]  и берем матрицу РАНГ-1 здесь
[40:03.000 --> 40:09.000]  умножаем вот этот вектор на yk
[40:09.000 --> 40:13.000]  и после этого проведения сложения получаем новое приближение для H1
[40:13.000 --> 40:15.000]  и все это за ОАТН квадрат
[40:16.000 --> 40:18.000]  успели?
[40:18.000 --> 40:20.000]  да
[40:20.000 --> 40:22.000]  хорошо
[40:22.000 --> 40:25.000]  собственно, модификация метода DFP
[40:25.000 --> 40:28.000]  метод BFGS, вот почему он так называется
[40:28.000 --> 40:31.000]  Бройден, Флэчер, Гордмар, Пшанно
[40:31.000 --> 40:36.000]  и идея в том, что давайте мы не будем приближать B
[40:36.000 --> 40:38.000]  давайте мы сразу H приблизим
[40:38.000 --> 40:40.000]  у нас квадринуск уровень уже поменяется
[40:40.000 --> 40:43.000]  но решение задачи будет записываться точно так же
[40:43.000 --> 40:47.000]  только где-то поменяется местами y и s
[40:47.000 --> 40:51.000]  почти теоремы
[40:51.000 --> 40:54.000]  на основании ущереджих соображений можно показать, что
[40:54.000 --> 40:57.000]  если функция сильного последующего гессиана
[40:57.000 --> 41:00.000]  то BFGS будет сходить сверх линей
[41:00.000 --> 41:04.000]  более точный анализ ходимости довольно затруднительным
[41:04.000 --> 41:07.000]  в силу сложности формул, которые
[41:07.000 --> 41:11.000]  довольно их трудоемкости для какого-то исследования
[41:11.000 --> 41:14.000]  и получения оценок
[41:14.000 --> 41:19.000]  основные комментарии про BFGS
[41:19.000 --> 41:22.000]  очень хорошо работают на практике
[41:22.000 --> 41:25.000]  и бывает свойством самой коррекции
[41:25.000 --> 41:28.000]  то есть если он на какой-то итерации
[41:28.000 --> 41:33.000]  приближение матрицы H оказывается достаточно плохим
[41:33.000 --> 41:39.000]  то на следующих итерациях это плохое качество будет исправляться
[41:39.000 --> 41:42.000]  сейчас на примере посмотрим, как это работает
[41:42.000 --> 41:46.000]  это я, наверное, сильно комментировать не буду
[41:46.000 --> 41:49.000]  следующий этап
[41:49.000 --> 41:52.000]  это квазинтонский мест с ограниченной памяти
[41:52.000 --> 41:55.000]  у нас пока что осталась проблема, что
[41:55.000 --> 41:59.000]  сложность хранения обновления гессиана это n квадрат
[41:59.000 --> 42:02.000]  при этом мы понимаем, что на самом деле
[42:02.000 --> 42:05.000]  нам вся матрица не нужна
[42:05.000 --> 42:08.000]  нам нужно всего лишь эффективно ее навестору умножить
[42:08.000 --> 42:11.000]  и даже грандианам
[42:11.000 --> 42:17.000]  и еще мы понимаем, что значения векторов y и s
[42:17.000 --> 42:20.000]  получены на первых итерациях
[42:20.000 --> 42:23.000]  они по-прежнему остаются включенными
[42:23.000 --> 42:26.000]  в значение матрицы H на сотой итерации
[42:26.000 --> 42:29.000]  потому что она просто обновляется
[42:29.000 --> 42:32.000]  через сумму и вычитание
[42:32.000 --> 42:35.000]  но при этом они могут портить эту самую оценку
[42:35.000 --> 42:38.000]  поэтому давайте мы будем хранить
[42:38.000 --> 42:41.000]  только последние m значений
[42:41.000 --> 42:44.000]  корректировать hm0 на каждой итерации
[42:44.000 --> 42:50.000]  и умножать на такое приближение гессиана рекурсивно
[42:50.000 --> 42:53.000]  тогда сложность будет mn
[42:53.000 --> 42:56.000]  и для хранения последних m пар
[42:56.000 --> 42:59.000]  нам нужно будет списание статуру данных,
[42:59.000 --> 43:02.000]  которая называется deck
[43:02.000 --> 43:05.000]  метод лучше всего работает на практике
[43:05.000 --> 43:08.000]  заранее определяем m и обновляем рекурсивно
[43:08.000 --> 43:11.000]  у нас были такие формулы
[43:11.000 --> 43:14.000]  если расписать m шагов, то мы получим
[43:14.000 --> 43:17.000]  что здесь присутствует набор из умножений
[43:17.000 --> 43:20.000]  из скалярных произведений
[43:20.000 --> 43:23.000]  и потом умножение на hm0
[43:23.000 --> 43:26.000]  в итоге мы умножаем вот эту матрицу
[43:26.000 --> 43:29.000]  в виде на вектор
[43:29.000 --> 43:32.000]  за o от n
[43:32.000 --> 43:35.000]  потому что все, что здесь используется
[43:35.000 --> 43:38.000]  оно представляется таким вот образом
[43:38.000 --> 43:41.000]  а умножить на матрицу v вектор
[43:41.000 --> 43:44.000]  это всего лишь o от n
[43:44.000 --> 43:47.000]  потому что нам надо взять этот вектор
[43:47.000 --> 43:50.000]  посчитать скалярное произведение с s
[43:50.000 --> 43:53.000]  и вычесть из него вектор y умноженный на row
[43:53.000 --> 43:56.000]  и на вот это скалярное произведение
[43:56.000 --> 43:59.000]  это нам дает быстрый способ
[43:59.000 --> 44:02.000]  получения результата умножения матрицы
[44:02.000 --> 44:05.000]  обратного гессиана на вектор
[44:05.000 --> 44:08.000]  без формирования самой матрицы
[44:08.000 --> 44:11.000]  мы просто храним набор векторов, с которых
[44:11.000 --> 44:14.000]  эта матрица складывается
[44:14.000 --> 44:17.000]  понятно ли идею?
[44:17.000 --> 44:20.000]  окей, эксперименты
[44:20.000 --> 44:23.000]  смотреть, что происходит
[44:23.000 --> 44:26.000]  опять рассмотрим эту же самую задачу
[44:26.000 --> 44:29.000]  видим, что метод dfpm
[44:29.000 --> 44:32.000]  вот в этом месте сломался
[44:32.000 --> 44:35.000]  и перестал сходиться
[44:35.000 --> 44:38.000]  вместе с тем
[44:38.000 --> 44:41.000]  да, тут немножко кряя картинка
[44:41.000 --> 44:44.000]  ладно, я ее потом обновлю
[44:44.000 --> 44:47.000]  вместе с тем метод bfgs
[44:47.000 --> 44:50.000]  в этом месте тоже перестал сходиться
[44:50.000 --> 44:53.000]  но в силу своей самой коррекции
[44:53.000 --> 44:56.000]  через несколько итераций он выправился
[44:56.000 --> 44:59.000]  и продолжил сходиться в таком же стиле
[44:59.000 --> 45:02.000]  видно, что bfgs сошел быстрее всех
[45:02.000 --> 45:05.000]  при этом бразилай и бурвин
[45:05.000 --> 45:08.000]  примерно до столько же итераций
[45:08.000 --> 45:11.000]  при этом bfgs и sci-fi сходятся достаточно долго
[45:11.000 --> 45:14.000]  потому что там хитрый процедуровый барышан
[45:14.000 --> 45:17.000]  который не является оптимальным для этой задачи
[45:17.000 --> 45:20.000]  в итоге выводы по сегодняшней лекции
[45:20.000 --> 45:23.000]  сложность одной итерации
[45:23.000 --> 45:26.000]  по ньютоновской методу от n квадрат
[45:26.000 --> 45:29.000]  в строении санкубом в методе ньютона
[45:29.000 --> 45:32.000]  для метода bfgs требуется линейное количество памяти
[45:32.000 --> 45:35.000]  у bfgs есть самая коррекция
[45:35.000 --> 45:38.000]  и сверхленность сходилась
[45:38.000 --> 45:41.000]  проблема этого сомнительства метода в том, что
[45:41.000 --> 45:44.000]  на классический случай
[45:44.000 --> 45:47.000]  все начнет ломаться
[45:47.000 --> 45:50.000]  если у вас градиент не точный, то
[45:50.000 --> 45:53.000]  ничего не работает
[45:53.000 --> 45:56.000]  начальное приближение b0 или h0 это некая пристика
[45:56.000 --> 45:59.000]  с теорией сходимости все не очень хорошо
[45:59.000 --> 46:02.000]  это пока что в стадии разработки все находится
[46:02.000 --> 46:05.000]  и не любой способ хорошего шага будет гарантировать вам
[46:05.000 --> 46:08.000]  положительность скалярного произведения y на s
[46:08.000 --> 46:11.000]  для задач большой размерности
[46:11.000 --> 46:14.000]  точный градиент известен
[46:14.000 --> 46:17.000]  bfgs является стандартом
[46:17.000 --> 46:20.000]  все прикладные работы, которые что-то минимизируют
[46:20.000 --> 46:23.000]  и задача является, безусловно, чаще всего
[46:23.000 --> 46:26.000]  использовать именно его
[46:26.000 --> 46:29.000]  во многих пакетах он реализован
[46:29.000 --> 46:32.000]  скорее всего, на любом языке
[46:32.000 --> 46:35.000]  на котором вы что-то пишете
[46:35.000 --> 46:38.000]  и который маломальски приспособлен
[46:38.000 --> 46:41.000]  решение каких-либо вычислительных задач
[46:41.000 --> 46:44.000]  там это все будет
[46:44.000 --> 46:47.000]  следующий раз у нас будут максимальные методы
[46:47.000 --> 46:50.000]  последней уже лекции
[46:50.000 --> 46:53.000]  посмотрим на то, как решать задач с ограничениями
[46:53.000 --> 46:56.000]  когда эти ограничения достаточно просты
[46:56.000 --> 46:59.000]  есть ли какие-то вопросы? Нет
[46:59.000 --> 47:02.000]  очень хорошо, надеюсь, что все было понятно
[47:02.000 --> 47:05.000]  тогда спасибо за внимание, и на этом мы заканчиваем
[47:05.000 --> 47:08.000]  всем большое спасибо
[47:08.000 --> 47:11.000]  и до следующего раза

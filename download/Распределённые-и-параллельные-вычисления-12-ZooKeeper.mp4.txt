[00:00.000 --> 00:10.800]  Сегодня наша цель — зукиппер, и нужно как-то мотивировать его появление.
[00:10.800 --> 00:20.760]  В прошлый раз мы с вами говорили о том, что я рассказывал про то, что существует такая каптиария,
[00:20.760 --> 00:25.920]  которая говорит, что если вы пишете распределённую систему, то вы должны выбрать три буквы из трёх,
[00:25.920 --> 00:33.960]  вы должны выбрать не все возможные варианты, конечно, но вы должны, если у вас есть партишны,
[00:33.960 --> 00:45.800]  они у вас, разумеется, есть. Просто потому что такова жизнь, потому что такой сбой может
[00:45.800 --> 00:51.880]  случиться в сети, то вы должны выбирать, чему ваша система даёт приоритет. Она остаётся согласованной
[00:51.880 --> 00:59.960]  или остаётся доступной. Не то чтобы это какая-то теорема, разумеется, это скорее просто инженерная
[00:59.960 --> 01:06.720]  интуиция. Вы должны выбрать один из двух вариантов дизайна. Как ваша система будет вести себя,
[01:06.720 --> 01:13.400]  когда сеть расколется на два сегмента и между сегментами связи не будет, а сами сегменты будут
[01:13.400 --> 01:19.240]  рабочими внутри себя и даже могут обслуживать клиентов, которые с этими сегментами могут
[01:19.240 --> 01:24.960]  поддерживать связь. Один вариант, если ваша система в таком случае готова обслуживать запросы,
[01:24.960 --> 01:30.040]  готова отвечать на них и готова менять состояние, то есть система остаётся высокодоступной.
[01:30.040 --> 01:35.640]  И вторая альтернатива, когда система говорит, что я не могу работать в таких условиях, я не
[01:35.640 --> 01:40.920]  могу независимо принимать решения в двух сегментах сети, я должна в одной части заблокироваться,
[01:40.920 --> 01:46.800]  а в другой продолжить обслуживать пользователей и давать им уже согласованный ответ. Вот оба
[01:46.800 --> 01:54.080]  варианта, обе альтернативы, они по-своему разумны. Но вот если мы говорим про согласованность,
[01:54.080 --> 01:58.560]  то в общем тут никаких сомнений нет. Система не может обеспечить согласованность с двух сторон
[01:58.560 --> 02:06.800]  partition, поэтому выбирает одну половину, а в другой блокируется. В случае с доступностью это тоже
[02:06.800 --> 02:12.240]  в некоторых случаях разумно, потому что не во всех задачах, не во всех приложениях нам вот эта самая
[02:12.240 --> 02:20.520]  согласованность нужна. У нас, кажется, еще будет лекция про Кассандру и про Amazon Dynamo. Так вот,
[02:20.520 --> 02:26.760]  там мотивирующий пример, который, мотивирующая задача, которая стояла перед инженером Amazon,
[02:26.760 --> 02:35.280]  состояла в том, чтобы сделать корзину товаров для их сайта. Так вот, для такой задачи согласованность
[02:35.280 --> 02:41.760]  не очень-то важна. В такой задачи можно вполне себе добавить товар в корзину, потом он оттуда
[02:41.760 --> 02:46.800]  исчезнет, потом снова появится. Это не очень большая беда, можно было бы это пережить. А поскольку
[02:46.800 --> 02:51.760]  две эти альтернативы взаимоисключающие, ну то есть понятно, что если у вас есть два сегмента сети,
[02:51.760 --> 02:55.920]  между ними нет коммуникации, то вы не можете записать с одной стороны и потом прочистить с другой
[02:55.920 --> 03:02.360]  половины сегмента, из другого сегмента. Так вот, если вы должны выбирать, то вот в некоторых условиях
[03:02.360 --> 03:08.480]  вам разумно выбрать высокую доступность системы. То есть дизайн, в котором система доступна на
[03:08.480 --> 03:15.520]  запись, при котором система доступна на запись даже в меньшей части partition. Ну просто потому,
[03:15.520 --> 03:21.760]  что там тоже есть клиенты. Но это будет в следующий раз, в другой раз, а сегодня мы поговорим все же
[03:21.760 --> 03:27.880]  про системы, которые выбирают согласованность. Ну и что значит согласованность? Они, видимо, выбирают,
[03:27.880 --> 03:34.560]  как правило, линьализуемость. Линьализуемость — это самая высокая степень согласованности,
[03:34.560 --> 03:37.800]  которая система может предоставить. Линьализуемость — это гарантия про то,
[03:37.800 --> 03:43.520]  что пользователь, клиент, вообще говоря, не наблюдает распределенности системы. То есть
[03:43.520 --> 03:47.640]  система, конечно, состоит из разных узлов, которые связаны проводами и которые не могут быть
[03:47.640 --> 03:55.680]  полностью синхронные, но при этом пользователь может думать о системе как о таком одном отказоустойчивом,
[03:55.680 --> 04:02.320]  высокодоступном компьютере. Пока в системе остается большинство узлов, пользователь эту систему,
[04:03.240 --> 04:09.120]  может работать как просто с одной надежной машиной. Это очень сильная гарантия, она очень удобна
[04:09.120 --> 04:12.240]  пользователю, но понятно, что есть более слабые гарантии, мы сейчас про них не очень хотим
[04:12.240 --> 04:18.200]  говорить. Мы под согласованностью понимаем, ну, по крайней мере, CAP-теорема в своей строгой
[04:18.200 --> 04:24.320]  интерпретации под согласованностью принимает линьализуемость. Это вот еще один повод попинать
[04:24.320 --> 04:30.880]  CAP-теорема, потому что сама теорема в кавычках, она плохо формализует, что такое согласованность,
[04:30.880 --> 04:38.120]  а вариантов согласованности довольно много. Вот мы под согласованностью будем понимать
[04:38.120 --> 04:44.920]  линьализуемость и в прошлый раз и с субботом уже давно мы обсуждали, что вот эта самая
[04:44.920 --> 04:50.680]  линьализуемость, которая говорит о том, что система ведет себя атомарно и выполняет как будто бы все
[04:50.680 --> 04:55.960]  операции в некотором глобальном порядке, причем с соблюдением предшествования операции в реальном
[04:55.960 --> 05:01.320]  времени, то есть с соблюдением причинности для пользователя. Так вот, система обеспечивает эту
[05:01.320 --> 05:08.080]  гарантию с помощью вспомогательного примитива, который называется Atomic Broadcast. Если вы умеете
[05:08.080 --> 05:14.800]  строить Atomic Broadcast, то вы можете реплицировать произвольное состояние линьализуемое, и вот
[05:14.800 --> 05:20.800]  это означает согласованность для пользователей. Atomic Broadcast, напомню, это примитив коммуникации,
[05:20.800 --> 05:28.800]  которая позволяет узлам в определенной системе коммуницировать друг с другом и при этом иметь
[05:28.800 --> 05:33.640]  некоторый общий глобальный порядок доставки сообщений. Разумеется, сеть вам такого не
[05:33.640 --> 05:40.360]  позволяет. В сети это сложная компутационная фабрика, которая связывает разные машины
[05:40.360 --> 05:45.080]  огромным количеством разнообразных маршрутов, обеспечивает высокую пропускную способность
[05:45.080 --> 05:50.360]  любого разреза и никакого порядка, конечно, не гарантирует. Но вот вам порядок нужен,
[05:50.360 --> 05:55.320]  когда вы собираетесь решать задачу репликации. Вам удобно, если бы все команды от всех
[05:55.320 --> 06:01.160]  пользователей, которые выступают в вашу систему, были бы каким-то образом упорядочен на всех
[06:01.160 --> 06:08.280]  репликах. Давайте я, для тех, кто не слушал меня по субботам, быстро нарисую картинку и мы вернемся
[06:08.280 --> 06:20.280]  к звукитеру. Примитив Atomic Broadcast или Total Reorder Broadcast. Это инструмент, с помощью которого
[06:20.280 --> 06:24.680]  распространённую систему достигается согласованности. Если мы в CAPC-ареме выбираем
[06:24.680 --> 06:39.320]  Google C, то мы неизбежно должны использовать в своем дизайне Atomic Broadcast. У вас есть,
[06:39.320 --> 06:45.440]  допустим, три узла и у нас есть два клиента, которые приходят в систему со своими запросами.
[06:45.440 --> 07:03.360]  Вот клиент 1 с какой-то командой 1 и клиент 2. И вот реплики системы, они должны быть у
[07:03.360 --> 07:08.800]  возможности синхронны. И вот мы сейчас этого с помощью Atomic Broadcast пытаемся достичь.
[07:08.800 --> 07:13.760]  Разумеется, система может быть сложнее, чем просто три реплики, в ней может быть очень много узлов,
[07:13.760 --> 07:22.040]  но как правило происходит следующее. Либо ваша система это гигантская база данных или гигантская
[07:22.040 --> 07:27.720]  таблица, тогда у вас просто много-много компьютеров делится на шарды. Каждый шар включает
[07:27.720 --> 07:33.120]  сохранение части данных и каждый шард сам по себе является набором реплик. То есть,
[07:33.120 --> 07:38.120]  можно вот опуститься до такого уровня. Ну а в некоторых системах все не так. Там есть
[07:38.120 --> 07:43.280]  некоторая иерархия в смысле, ну не знаю, можно представить себе распределенную файловую систему,
[07:43.280 --> 07:48.920]  с которыми вы уже работали, и вот там есть уровень данных и уровень метаданных. Ну вот уровень данных
[07:48.920 --> 07:52.680]  может быть устроен одним образом, а уровень метаданных может быть устроен как раз с помощью
[07:52.680 --> 07:58.760]  репликации через Atomic Broadcast, потому что там есть сложное состояние, которое мутируется
[07:58.760 --> 08:04.440]  комплектно разными клиентами. Вот так или иначе согласованность где-то в таком месте достигается,
[08:04.440 --> 08:10.400]  она достигается с помощью этого инструмента. А именно, когда реплика получает команду от клиента,
[08:10.400 --> 08:21.520]  то она становится координатором этой команды и выполняет процедуру, которую мы назовем Atomic Broadcast.
[08:21.520 --> 08:34.000]  Она иницирует широковещательную рассылку команды C2 на вот этот набор реплик. Клиент
[08:34.000 --> 08:48.960]  один делает то же самое. Он иницирует Atomic Broadcast для своей команды. И Atomic Broadcast,
[08:48.960 --> 08:56.920]  вот этот протокол, неизвестный нам, гарантирует, что если у нас реплика будет жить достаточно долго,
[08:56.920 --> 09:02.240]  то она получит обе эти команды, причем каждая реплика получит эти команды в одном и том же
[09:02.240 --> 09:06.160]  порядке. Но какая-то реплика может получить не все команды, потому что просто она откажет,
[09:06.160 --> 09:11.280]  но если она будет работать достаточно долго, то она получит красную и синюю команду в одном и
[09:11.280 --> 09:16.680]  том же порядке. И каждая реплика будет поддерживать копию состояния некоторого,
[09:16.680 --> 09:21.440]  ну не знаю, дерево, файловая система, кусочек, таблица, и будет в одном и том же порядке
[09:21.440 --> 09:26.280]  применять одни и те же апдейты. Разумеется, реплики не синхронные, но при этом они проходят
[09:26.280 --> 09:33.680]  через общую серию, через одну и ту же серию изменений, серию мутаций, и этого достаточно,
[09:33.680 --> 09:39.160]  чтобы поверх вот такого протокола, чтобы поверх Atomic Broadcast получили линиаризуемость. Как
[09:39.160 --> 09:45.920]  только координатор отправляет, как только координатор отправив, поставок подотправил
[09:45.920 --> 09:51.080]  команду пользователя, сам получит Atomic Broadcast, а это, конечно, мгновенная операция, потому что
[09:51.080 --> 09:56.160]  между репликами должна быть какая-то координация довольно сложная, чтобы говорится о порядке. Так вот,
[09:56.160 --> 10:01.520]  в основном, он получит эту команду сам, он применит ее к своему состоянию и может вернуть
[10:01.520 --> 10:09.200]  это пользователю. Вот такой протокол по модулю того, что мы не знаем, как Atomic Broadcast реализован,
[10:09.200 --> 10:15.000]  обеспечивает линиаризуемость, обеспечивает согласованность. Ну а чтобы этот Atomic Broadcast
[10:15.000 --> 10:26.040]  реализовать, я в прошлый раз говорил, нам нужно решать задачу консенсуса. Задача консенсуса — это
[10:26.040 --> 10:31.800]  очень простая задача, где у нас есть узлы, у них есть на входе какие-то значения, и задача этих
[10:31.800 --> 10:37.720]  узлов — просто договориться об общем выборе и завершиться, сделать этот выбор. Если узел завершается,
[10:37.720 --> 10:43.440]  то он должен выбирать одно из предложенных значений и должен, и все завершившиеся, сделавшие
[10:43.440 --> 10:49.080]  выбор узлы должны выбирать одно и то же. Вот эти две задачи, не то чтобы эта задача сводится к
[10:49.080 --> 10:55.320]  этой, они вообще позволяют по сложности, это одна и та же по сложности задача. Вот, и на практике
[10:55.320 --> 11:00.960]  обе эти задачи решаются с помощью, ну как правило, промышленных системах решаются с помощью двух
[11:00.960 --> 11:11.640]  алгоритмов. Это алгоритм Multipaxas и алгоритм Round. Это два похожих алгоритма, можно сказать, что это
[11:11.640 --> 11:18.800]  примерно один и тот же алгоритм. Но вот они существуют, реализация деталей этих алгоритмов,
[11:18.800 --> 11:24.280]  конечно, сегодня в лекцию они не могут пронести, это не наша цель. Но важно, что если вы пишете
[11:24.280 --> 11:32.760]  распределённую систему, то, как правило, вы уже знаете, что вы уже выбрали в CAP теориями букву C,
[11:32.760 --> 11:38.200]  вы знаете, что вы хотите достичь согласованности, то есть линия резуемости, вы это делаете по общему
[11:38.200 --> 11:42.960]  рецепту с помощью реализации Atomic Broadcast, и Atomic Broadcast, как правило, вы пишете даже не голыми
[11:42.960 --> 11:50.840]  руками, потому что он уже написан. Но вот есть протокол RAFT, и это, наверное, сейчас такой
[11:50.840 --> 11:56.960]  стандартный протокол для большинства open source распределённых систем. Вот если вы open source,
[11:56.960 --> 12:05.040]  если вы пишете open source распределённую систему в 2021 году, то, скорее всего, вы начнёте с RAFTA.
[12:05.040 --> 12:10.240]  Ну, точнее, если вы будете использовать consensus в своём коде, то, скорее всего,
[12:10.240 --> 12:15.720]  вы возьмёте RAFT просто потому, что протокол достаточно хорошо описан, у него очень много,
[12:15.720 --> 12:21.040]  есть, во-первых, очень много спецификаций, документаций, там какие-то ториллов, но,
[12:21.040 --> 12:25.800]  а кроме того, что, наверное, важно в первую очередь для разработчиков, есть open source реализации
[12:25.800 --> 12:32.320]  очень высокого уровня. Причём есть вот для самых разных языков, для Go, для C++, для Rastia,
[12:32.320 --> 12:38.200]  вот эти реализации, они используются вот в промышленных распределённых системах. То есть,
[12:38.200 --> 12:45.200]  это не просто какой-то такой академический проект, это вот именно код, который работает,
[12:45.200 --> 12:49.960]  в конце концов, в существующих системах под большой нагрузкой, и вы можете его переиспользовать.
[12:49.960 --> 13:00.040]  Но, тем не менее, это не единственный способ подходить к реализации распределённых систем,
[13:00.040 --> 13:07.480]  которые выбирают C и используют внутри себя только broadcast, протокол consensus. Альтернативный подход
[13:07.480 --> 13:12.880]  состоит в том, чтобы, один подход, это взять библиотеку, которую этот consensus реализует,
[13:12.880 --> 13:19.600]  которая решает реализовать приметивный broadcast, и встроить её в свой код. А есть совершенно
[13:19.600 --> 13:28.600]  альтернативный дизайн, который был предложен, который выбрали в Google ещё в 2006 году. Идея состояла
[13:28.600 --> 13:37.160]  в том, чтобы вместо того, чтобы consensus помещать, решение consensus в виде библиотеки помещать в код
[13:37.160 --> 13:43.480]  вашей системы, в код ваших узлов, вместо этого можно вынести задачу консенсуса, решение задачи
[13:43.480 --> 13:50.400]  консенсуса, приметив atomic broadcast в отдельный сервис. То есть, сделать consensus как сервис.
[13:50.400 --> 13:57.000]  Консенсус необходим, если вы хотите достичь буквы C в каптеореме, если вы хотите получить
[13:57.000 --> 14:03.640]  линеризуемость в качестве модели согласованности. Так вот, этот consensus можно вынести за пределы
[14:03.640 --> 14:09.520]  ваших узлов, за пределы вашей системы, и пользоваться им как сервисом. И этот сервис с точки
[14:09.520 --> 14:16.440]  зрения Google, с точки зрения Google в 2006 году, выглядит как сервис блокировок. То есть, это сервис,
[14:16.440 --> 14:23.560]  в котором вы можете прийти и взять блокировку, и что ты под этой блокировкой что-то делаешь. Ну,
[14:23.560 --> 14:30.040]  то есть, это такой распределённый, отказоостойчивый mutex. Насколько такая модель данных вообще
[14:30.040 --> 14:36.000]  адекватна. Что такое распределённые блокировки, это отдельная история. Мы к ней сегодня придём,
[14:36.000 --> 14:42.600]  довольно завысоватым путём. Но, просто сам point, что вместо того, чтобы встраивать консенсус в
[14:42.600 --> 14:50.360]  код ваших узлов, вы можете использовать консенсус в виде внешнего сервиса. Ну,
[14:50.360 --> 14:56.400]  аргументы Google были довольно странные, что там блокировки более привычные пользователям. На
[14:56.440 --> 15:00.320]  самом деле, конечно же, это не так, потому что распределённые блокировки, как мы сегодня увидим,
[15:00.320 --> 15:06.800]  они сильно отличаются от блокировок, которые вы используете по точным коде. Но, тем не менее,
[15:06.800 --> 15:13.440]  вот появилась такая система. Она появилась в 2006 году, если мне память не изменяет. И спустя
[15:13.440 --> 15:21.560]  несколько лет в Yahoo написали другую систему по мотивам Google-чаба. Ну, то есть, замысел был тот
[15:21.560 --> 15:29.040]  же реализовать консенсус как сервис, вынести его в внешнюю систему, но реализация, точнее,
[15:29.040 --> 15:38.800]  не реализация, а модель данных была выбрана другой. Ну, вот мы сейчас посмотрим на то,
[15:38.800 --> 15:44.840]  какая модель данных была выбрана в этой системе. Она называется ZooKeeper. И поговорим немного про то,
[15:44.840 --> 15:49.840]  как она внутри устроена, потому что внутри и на границе с клиентом, потому что это всё важно.
[15:49.840 --> 15:57.720]  ZooKeeper — это система, которая принадлежит классу систем, которая называется сервис координации.
[15:57.720 --> 16:04.440]  Вот в этом курсе вы обычно говорите про системы, которые хранят данные пользователя или обрабатывают
[16:04.440 --> 16:10.120]  данные пользователя, и которые там масштабируются, которые отказоустойчивы. Так вот, ZooKeeper не
[16:10.120 --> 16:15.240]  предназначен для того, чтобы с ним работали непосредственно пользователи. ZooKeeper не хранит
[16:15.240 --> 16:22.200]  большой объем данных, он хранит маленький объем данных, и он не предназначен там для
[16:22.200 --> 16:27.400]  какой-то обработки. Короче, пользователи с ним взаимодействовать не должны. ZooKeeper — это
[16:27.400 --> 16:34.880]  система, которая служит таким вот кубиком, с помощью которого строятся другие распределенные
[16:34.880 --> 16:42.040]  системы. То есть, если вы пишете там, не знаю, очередь сообщений, если вы пишете там очередь
[16:42.040 --> 16:47.440]  задач, если вы пишете там условные какие-то мы-предьюстеры, что-то подобное, то вы в дизайне
[16:47.440 --> 16:53.160]  своей системы для того, чтобы добиться согласованности, для того, чтобы решать разные задачи, которые
[16:53.160 --> 17:00.720]  в системах возникают, можете использовать вот этот самый ZooKeeper. Какую модель данных он вам
[17:00.720 --> 17:09.680]  предоставляет? ZooKeeper хранит в себе дерево. Дерево с узлами, и у вас есть, ну давайте на опись
[17:09.680 --> 17:18.080]  сразу посмотрим, операции, которые выглядят так. Create — вы создаете путь в некотором узел по пути
[17:18.080 --> 17:23.920]  в этом дереве. Вы можете этот узел удалить, вы можете проверить, существует ли он, вы можете
[17:23.920 --> 17:34.600]  перечислить потомков какого-то узла. Вы можете, ну давайте пока остановимся на этом и сразу скажем,
[17:34.600 --> 17:41.360]  сразу оговоримся, что несмотря на такой опи, у нас есть дерево, там есть пути, узлы, их можно создавать,
[17:41.360 --> 17:47.320]  удалять и проверять, существует ли они. Так вот, это ни в коем случае не файловая система. Ну
[17:47.320 --> 17:54.760]  файловая система, про файловая система вы и мы уже говорили в разных, по разным причинам. Так вот,
[17:54.760 --> 18:01.760]  это не файловая система, это всего лишь иерархическая организация данных. Сами узлы в этом дереве — это
[18:01.760 --> 18:08.600]  не файлы. Вообще, все данные, которые хранит зукипер, должны умещаться в одну машину. То есть,
[18:08.600 --> 18:17.480]  очень небольшие данные. И отдельный узел тоже очень маленький. Размер отдельного узла может быть,
[18:17.480 --> 18:23.720]  ну вот, ограничен там какими-то единицами в мегабайт. То есть, вы там можете хранить только небольшой
[18:23.720 --> 18:30.920]  блок с данными. Сам зукипер никак не интерпретирует эту задачу вашего приложения. И у нас нет
[18:30.920 --> 18:37.280]  операций типа append. То есть, мы не можем это точечно модифицировать узлы. У нас в API есть только
[18:37.280 --> 18:44.840]  операции getData и setData, которые просто читают содержимое узла и перезаписывают содержимое узла.
[18:44.840 --> 18:58.920]  Я бы сказал, что вот на зукипер, на данные в зукипере стоит смотреть, как на набор атомиков. Но вот
[18:58.920 --> 19:03.520]  зукипер выполняет в распределенных системах ту же функцию, которую выполняет в многопроточном
[19:03.520 --> 19:11.120]  приложении атомики. Зукипер нужен для синхронизации узлов в распределенной системе. И так же,
[19:11.120 --> 19:17.080]  как у атомиков, у зукипера есть более сложные операции, чем get и set, то есть, чем просто
[19:17.080 --> 19:24.160]  чтение и запись. Но об этом мы чуть позже поговорим. Главное, что не стоит думать об этом как в файловой
[19:24.160 --> 19:29.160]  системе, стоит думать об этом как просто об иерархическом наборе атомиков, через которые
[19:29.160 --> 19:33.840]  можно синхронизировать узлы распределенной системы или даже какие-то сервисы распределенной системы.
[19:33.840 --> 19:46.320]  Но, значит, это первое приближение. Но есть у зукипера API интереснее, чем просто создать узел,
[19:46.320 --> 19:52.520]  прочесть данные из узла, записать узел. Когда вы создаете узел, у вас есть разные режимы. Как
[19:52.520 --> 20:03.800]  именно вы можете это сделать? Если мы посмотрим на команду create, то, смотрите, вы создаете по
[20:03.800 --> 20:11.880]  такому пути узел вот с такими данными, и у вас еще есть create mode. Вот посмотрим на create mode.
[20:11.880 --> 20:20.400]  Вы можете создать, во-первых, персистентный узел, то есть узел, который появится в дереве и будет
[20:20.400 --> 20:27.720]  жить там вечно до тех пор, пока его явно не удалят операции delete. Но у вас есть еще и другие
[20:27.720 --> 20:33.360]  варианты. У вас есть два флага, которые можно комбинировать между собой. А именно, вы можете
[20:33.360 --> 20:41.800]  создать узел, который называется sequential. Смысл такой, когда вы создаете sequential узлы в зукипере,
[20:41.800 --> 20:50.040]  то в некоторой директории, то сам зукипер для этой директории поддерживает счетчик,
[20:50.040 --> 21:00.000]  и каждый новый узел получает себе порядковый номер. При создании узла в директории в режиме
[21:00.000 --> 21:10.280]  sequential к имени узла, который вы приложили в операции create, добавляется еще один компонент,
[21:10.280 --> 21:15.840]  а именно порядковый номер этого узла. Вот, и не помню, где-то здесь есть хороший пример.
[21:25.840 --> 21:28.040]  Возможно, он есть.
[21:33.560 --> 21:39.720]  Вот. Если вы создаете узел по какому-то пути, то внутри директории накременится счетчик,
[21:39.720 --> 21:44.800]  и создается узел с таким номером. Пока вы не создаете узел, вы, конечно, не знаете,
[21:44.800 --> 21:53.600]  какой номер он получит, но вот, создав его, вы можете это потом узнать. Это первая возможность.
[21:53.600 --> 22:03.600]  Если думать в сторону, если пытаться дальше проводить аналогию с атомиками, что мне кажется
[22:04.480 --> 22:10.440]  очень разумным, то это вот некоторый аналог атомарной операции FetchEd. Если вы когда-то изучали
[22:10.440 --> 22:15.480]  атомики, то вы, наверное, знаете, что просто операции store и load для координации узлов,
[22:15.480 --> 22:20.800]  для координации потоков мало. Нужны еще какие-то более сложные операции, но вот одна из них – это
[22:20.800 --> 22:27.320]  FetchEd, и sequential узлы эту функцию выполняет. На самом деле, в зукипере есть даже операция CAS,
[22:27.320 --> 22:33.080]  хотя она вот так довольно незаметно выглядит, потому что она называется всего лишь setData,
[22:33.080 --> 22:41.520]  как будто бы она просто перезаписывает данные в узле. Но при этом у нее есть аргумент версия,
[22:41.520 --> 22:53.960]  и семантика этого setData такая. Каждый узел, когда вы в него что-то пишете, каждая запись в зукипере на
[22:53.960 --> 22:59.920]  узле, каждая перезапись узла меняет его версию, увеличивает, представляет новую версию. Сам
[22:59.920 --> 23:05.320]  зукипер отвечает за то, чтобы эти версии венатонно росли. Так вот, если вы пишете данные в узел,
[23:05.320 --> 23:10.800]  вы можете поставить себе, поставить зукиперу ограничение, что я хочу записать в узел по этому
[23:10.800 --> 23:19.520]  пути вот эти данные, только если сейчас версия узла равна вот этой. Это вот буквально compare
[23:19.520 --> 23:28.400]  exchange в атомиках, только даже более мощный, потому что вот тут нет ABA, потому что вы сравниваете
[23:28.400 --> 23:42.520]  несодержимое, а версию. Ну вот такой API. Но даже это еще не все, потому что чем должен отличаться
[23:42.520 --> 23:53.480]  зукипер от атомиков? Зукипер предоставляет вам атомики вам клиентам, но при этом сами клиенты,
[23:53.480 --> 23:58.840]  они отличаются от потоков в многопоточной программе, потому что поток он запустился и работает,
[23:58.840 --> 24:03.720]  а клиент в распределенной системе, он подвержен отказам, он может просто взорваться и больше
[24:03.720 --> 24:10.400]  ничего не делать. Поэтому это не просто набор атомиков, организованных иерархически, а набор
[24:10.400 --> 24:18.520]  атомиков, которые еще учитывают возможные отказы клиента. И по этой причине, когда вы создаете
[24:18.520 --> 24:31.280]  узел, у вас есть дополнительная опция. Сейчас мы вернемся в... Давайте закроем лишнее. Вернемся
[24:31.280 --> 24:37.320]  в режимы создания узла. У вас есть еще один режим, который называется ephemeral. И для того,
[24:37.320 --> 24:43.480]  чтобы объяснить, что это такое, нужно немного поговорить про то, как сам зукипер реализован
[24:43.480 --> 24:55.800]  и как с ним работает клиент. Зукипер это... Разумеется, зукипер это система надежная. Зукипер
[24:55.800 --> 25:07.920]  это набор реплик, которые общаются между собой и реплицируют некоторое состояние. И реплицируют
[25:07.920 --> 25:15.840]  это состояние, а не как раз с помощью протокола atomic broadcast. Но это не совсем RSM, если вы помните,
[25:15.840 --> 25:21.000]  знаете, что это такое. Там не протокол RAFT используется, потому что зукипер был написан до того,
[25:21.000 --> 25:25.640]  как придумали RAFT. Там используется не multiplex, там схема немного другая, по смыслу похожая,
[25:25.640 --> 25:31.600]  но все-таки другая. Когда мы говорим про multiplex или RAFT, то у нас есть клиент, он коммитит
[25:31.600 --> 25:35.440]  команду в реплицированный лог, копия которого есть на каждой реплике, и потом эта команда
[25:35.440 --> 25:42.200]  применяется к реплицированному состоянию. В зукипере используется подход, который называется
[25:42.200 --> 25:48.360]  primary backup, а именно, у вас есть лидер в кластере, он получает команду от клиента, он применяет
[25:48.360 --> 25:54.960]  ее к своему состоянию, скажем, increment, и после этого реплицируют уже такие вот идомпатентные
[25:54.960 --> 26:02.000]  апдейты на другие реплики. Схема похожая, но все же другая, и там у нее есть некоторые интересные
[26:02.000 --> 26:07.800]  особенности, например, гораздо легче делать снапшоты, но это уже глубина реализации, нам сейчас не
[26:07.800 --> 26:16.640]  очень важно. Важно, что зукипер внутри себя, зукипер предоставляет вам модель согласованности,
[26:16.640 --> 26:23.880]  гарантирует, что он упорядочивает все апдейты, упорядочивает их с помощью примитива Atomic
[26:23.880 --> 26:28.600]  Broadcast, и для этого использует собственный протокол, который называется зукипер Atomic Broadcast.
[26:28.600 --> 26:35.680]  То есть это буквально протокол, который был придуман по мотивам мультипаксиса для зукипера,
[26:35.680 --> 26:42.280]  то есть это тоже протокол, который оптимизирован, который выполняет одну фазу на быстром пути для
[26:42.280 --> 26:51.520]  комита команды пользователя, простите, немножко простужен, тоже выбирает лидера, но в общем это
[26:51.520 --> 26:58.720]  некоторый рафт, который был придуман специально вот для этой системы, и разумеется, чтобы протокол
[26:58.720 --> 27:03.840]  работал, чтобы система оставалась доступна, в этой системе должны оставаться живыми большинство
[27:03.840 --> 27:13.280]  реплик. Если у нас зукипер, инсталляция зукипера из трех узлов, то зукипер будет доступен и будет
[27:13.280 --> 27:20.520]  обслуживать ваши команды, ваши операции до сих пор, пока в нем доступны два узла. Вот, как правило,
[27:20.520 --> 27:29.920]  зукипер инсталирует не на трех узла, но есть разные варианты. Зукипер можно размещать, если в одном
[27:29.920 --> 27:35.240]  датацентре можно взять и пять узлов, пять или семь узлов, такие стандартные цифры, если вы
[27:35.240 --> 27:41.880]  размещаете зукипер, реплики зукипера в разных датацентрах, то вот три датацентра, три узла этого
[27:41.880 --> 27:49.160]  будет кажется достаточно для того, чтобы повысить доступность этого компонента. Но сам зукипер
[27:49.160 --> 27:53.440]  является, разумеется, CP-системой, то есть он использует внутри Atomic Broadcast, решает задачу
[27:53.440 --> 28:01.720]  консенсуса, поэтому в случае partition в части сети, где осталось меньшинство реплик зукипера, эта
[28:01.720 --> 28:13.000]  система окажется недоступной для пользователей. Так реализован зукипер, там внутри некоторый
[28:13.000 --> 28:18.640]  Atomic Broadcast, некоторые условные паксы с оптимизированной, с выбором лидера и с
[28:18.640 --> 28:28.320]  однофазным комитом, и дальше клиент в эту систему, в этот Atomic Broadcast отправляет свои команды create,
[28:28.320 --> 28:34.880]  delete, вот те команды setdata, которые что-то в состоянии зукипера меняют. Поэтому на всех репликах
[28:34.880 --> 28:39.880]  зукипера это состояние меняется согласованно, меняется, проходит через одну и ту же серию
[28:39.880 --> 28:59.040]  update. Что еще нужно знать про устройство зукипера, про то, как с ним взаимодействует клиент.
[28:59.040 --> 29:19.480]  В зукипере вы можете помимо, вот есть, скажем, три узла этого самого зукипера, есть клиент и есть
[29:19.480 --> 29:34.760]  дерево, которое сейчас хранит каждый узел зукипера. Ну, во-первых, немного нотации. Для того,
[29:34.760 --> 29:40.520]  чтобы не путать вот эти кружочки и вот эти, то есть узлы зукипера и узлы дерева, в зукипере узлы
[29:40.520 --> 29:59.160]  называются Zeno. Ну вот, у вас здесь есть корень. Вот здесь какая-то директория, ну, условная
[29:59.160 --> 30:04.000]  директория, тут нет такого понятия именно узлы. То есть про директорию нужно, потому что директория
[30:04.000 --> 30:11.120]  это что-то про файловую систему, это не файловая система. Так вот, вы клиент, вы помимо того,
[30:11.120 --> 30:18.960]  что можете создать просто персистентный узел по 200 по другому узлу, или вы можете построить узел
[30:18.960 --> 30:24.800]  с автоматической нумерацией, с некоторым аналогичным, вы еще можете создать эфемерный узел.
[30:24.800 --> 30:38.160]  Давайте это по-особенному нарисуем. Вот у нас есть синий клиент, и он может создать синий
[30:38.160 --> 30:59.640]  эфемерный узел. Смысл такой, вот есть клиент и он может... Почему я обо всем этом говорю? Потому
[30:59.640 --> 31:05.080]  что, то есть почему мы добавляем еще один тип узлов? Потому что клиенты могут отказывать. В
[31:05.080 --> 31:09.240]  атомиках потоки не отказывались. Здесь клиенты могут отказывать, поэтому для отказывающих
[31:09.240 --> 31:12.840]  клиентов, для того чтобы оборватывать такие сценарии тоже, мы заводим новый тип узлов
[31:12.840 --> 31:21.000]  эфемерного. Если клиент создает этот эфемерный узел, и вот он в принципе такой же узел,
[31:21.000 --> 31:27.240]  как и все остальные, но с некоторой поправкой в нем нельзя создавать потомков. Вот чем этот
[31:27.240 --> 31:38.000]  узел отличается от других, тем своей судьбой. Вот если клиент умирает, то если клиент успел
[31:38.000 --> 31:42.720]  создать персистентный узел в этом дереве, то этот узел остается жить. А вот эфемерный узел
[31:42.720 --> 31:52.120]  исчезает вместе с потерей клиента. Каким образом это реализуется? Для того чтобы создавать
[31:52.120 --> 31:56.600]  эфемерный узел, для того чтобы работать с зукипером, мало просто иметь, скажем,
[31:56.600 --> 32:04.000]  такой тип API, по которому вы можете посылать какие-то запросы. Нет, клиент зукипера это
[32:04.000 --> 32:10.600]  полноценный участник всей этой системы. Клиент зукипера использует довольно сложную
[32:10.600 --> 32:16.480]  клиентскую библиотеку. Когда он работает с системой, он устанавливает с ней логическую сессию.
[32:16.480 --> 32:30.720]  Он открывает сессию и в рамках этой сессии, сессия идентифицируется некоторой уникальной
[32:30.720 --> 32:38.480]  строчкой. Вот в рамках этой сессии клиент выполняет все свои операции, в том числе создание эфемерного
[32:38.480 --> 32:45.600]  узла. Сессия считается живой, пока жив клиент, то есть пока он управляет регулярно в зукеперы
[32:45.600 --> 32:52.960]  такие свои операции. Либо, если ему нечего делать прямо сейчас, он сам библиотеку клиентской
[32:52.960 --> 33:06.000]  отправляет в этой сессии узлу системы, узлу зукипера Harbit. Зукиперы называются pings. То есть
[33:06.000 --> 33:13.200]  клиентская библиотека говорит зукиперу, что клиент все еще жив. Вот если сессия протухла,
[33:13.200 --> 33:21.360]  то есть клиент показал, допустим, и перестал посоветовать pings эти Harbit, то зукипера понимает
[33:21.360 --> 33:32.080]  и автоматически стирает эфемерный узел, который был создан этим клиентом. Ну и флаг эфемерности и
[33:32.080 --> 33:37.400]  флаг sequential можно комбинировать. То есть вы можете создать узел с автоматической нумерацией,
[33:37.400 --> 33:44.080]  который исчезнет, если исчез клиент. То есть смотрите аналогия с автоматиками еще раз. Вот у нас
[33:44.080 --> 34:00.360]  есть SerData плюс версии. Это просто CAS. У нас есть sequential узлы. Это
[34:00.360 --> 34:10.920]  перчерк. И на случай умирающих узлов у нас еще есть плюс к этому всему эфемерность.
[34:10.920 --> 34:18.920]  Ну вот здесь на самом деле ситуация хитрее, чем я рассказываю. Сложнее устроено. Гарантии тут
[34:18.920 --> 34:28.120]  более... короче, нужно быть аккуратным. Смотрите, что такое сессия? Ну сессия такое логическое
[34:28.120 --> 34:34.480]  соединение с системой. Разумеется, внутри реализации, внутри клиентской библиотеки написан какой-то
[34:34.480 --> 34:43.120]  код, который работает с обычными соединениями. И в конце концов сессия, это некоторая логическая
[34:43.120 --> 34:47.600]  сессия, реализуется в виде некоторого DCP соединения с системой, через которое клиент
[34:47.600 --> 34:53.320]  отправляет свои операции в систему и клиентская библиотека отправляет пилинги. Так вот, разрыв
[34:53.320 --> 35:00.320]  этого соединения это не разрыв сессии. Ну скажем, у вас какой-то узел самого зукипера может оказаться.
[35:00.320 --> 35:09.960]  Это нормальная ситуация, потому что зукипер внутри себя использует протокол atomic broadcast,
[35:09.960 --> 35:14.880]  а этому протоколу atomic broadcast для того, чтобы работать, достаточно собирать хворумы,
[35:14.880 --> 35:21.400]  пока живо большинство узлов зукипер может обслуживать пользователями. Но при этом вот это
[35:21.400 --> 35:38.160]  соединение разрывалось, и клиент переконнектится к другому узлу. Но при этом сам зукипер сессию не
[35:38.160 --> 35:44.040]  потеряет, потому что сессия, но зукипер с помощью atomic broadcast реплицирует с одной стороны дерево,
[35:44.040 --> 35:54.520]  а с другой стороны сессия реплицирует. То есть, если вы знаете про мультипансис прорав, то сессия это
[35:54.520 --> 36:01.640]  часть персистентного состояния. Оно реплицируется вместе с данными, которые в этом деле живут.
[36:01.640 --> 36:14.320]  Вот это не беда, но может быть другая беда, а именно, что клиент, ну допустим, залит на сборке
[36:14.320 --> 36:19.840]  мусора, мы видим, что вот нативный API для зукипер написан на джаве. В джаве есть сборщик мусора,
[36:19.840 --> 36:23.400]  он может остановить вашу программу в любом момент времени, в том числе клиентскую библиотеку,
[36:23.400 --> 36:29.040]  которая отправляет инги, поэтому сессия протухнет уже по-честному, и потому что у нее есть таймал,
[36:29.040 --> 36:38.560]  и клиент должен, разумеется, об этом получить уведомление. Вот протухание сессии — это самая
[36:38.560 --> 36:45.320]  большая неприятность, которая может случиться с клиентом, и это такая не unrecoverable error. Из этой
[36:45.320 --> 36:52.200]  ошибки нет способа адекватным образом восстановиться, потому что вы не понимаете,
[36:52.200 --> 36:58.960]  теперь в каком системе состояния. Вы утратили сессию, у вас исчезли все эфемерные узлы, вы должны,
[36:58.960 --> 37:04.560]  ну не знаю, начать сначала с чистого листа, забыть, как будто бы вы перезагрузились, можно об этом,
[37:04.560 --> 37:19.880]  можно к этому так относиться. Пока все понятно, скажите. Понятно. Если примерно понятно,
[37:19.880 --> 37:26.240]  как зукипер внутри устроен, какую модель данных он предоставляет, с какими гарантиями.
[37:26.240 --> 37:32.480]  С гарантиями у зукипера довольно сложно, потому что, с одной стороны, все запросы на запись
[37:32.480 --> 37:37.840]  упорядочиваются через atomic broadcast, а вот с чтениями чуть хитрее. На самом деле,
[37:37.840 --> 37:44.280]  зукипер и чтение могут читать немного старые данные, но в принципе для большинства приложений
[37:44.280 --> 37:50.040]  это не очень важно, но это очень сложная история, если хотите, можно потом углубиться в документацию.
[37:50.040 --> 37:58.040]  Окей, вот если все это понятно, то теперь можно подумать, а почему нам зукипер полезен,
[37:58.040 --> 38:08.840]  то есть какие задачи мы собираемся решать с помощью этого зукипера. Итак, какие задачи мы хотим
[38:08.840 --> 38:16.240]  решать в первую очередь? Прежде чем решать задачи, маленькое замечание важное. Чем зукипер
[38:16.240 --> 38:30.720]  отличается от протокола консенсуса, то есть такое забавное наблюдение и правильный подход к этой
[38:30.720 --> 38:36.960]  системе. Если вы используете какую-то библиотеку для консенсуса для atomic broadcast в своем коде,
[38:36.960 --> 38:43.080]  то она вырешает консенсус внутри вашей системы. Для каждой системы, которая использует консенсус,
[38:43.080 --> 38:50.760]  будут свои узлы, свои алгоритмы, своя библиотека, свои независимые консенсусы. Так вот, зукипер
[38:50.760 --> 38:59.080]  предназначен для того, чтобы он был общим. Вот если вы пишете одну систему, потом другие
[38:59.080 --> 39:04.960]  люди вашей компании большой пишут другую систему, и вам и им нужен сервис координации, то вы можете
[39:04.960 --> 39:11.400]  переиспользоваться на тот же зукипер. Иерархия нужна для того, чтобы разные проекты разнести по
[39:11.400 --> 39:17.680]  разным узлам, по разным поддеревьям. Или у вас есть одна система, просто разные инсталляции
[39:17.680 --> 39:26.480]  на разных машинах, на разных кластерах. Опять, эти системы могут использовать один общий зукипер,
[39:26.480 --> 39:32.720]  просто разместив свои данные в разных поддеревьях. У каждой системы будет свой
[39:32.720 --> 39:41.400]  собственный путь, и там будут храниться все метаданные этой системы. Итак, первая задача,
[39:41.400 --> 39:45.760]  которую мы будем сегодня решать. Они все будут довольно абстрактными, но, в принципе,
[39:45.760 --> 39:51.320]  вы, надеюсь, поймете, что они неизбежно возникают более-менее в любой системе,
[39:51.320 --> 39:58.600]  в любой распределенной системе, чем бы она ни занималась. Итак, представим, что, ну, я не знаю,
[39:58.600 --> 40:11.040]  вы пишете каком-то условно, и у вас есть много машин, например, 9, которые готовы выполнить что-то
[40:11.040 --> 40:18.560]  полезное. Там запускаются узлы вашей системы, и, разумеется, каждому узлу, чтобы запуститься,
[40:18.560 --> 40:26.880]  нужно иметь некоторую конфигурацию. Так, я не знаю, какие настройки кэшей, настройки rate limit,
[40:26.880 --> 40:32.520]  но все, что вы можете, какие-то порты, по которым нужно слушать сообщения из сети, все,
[40:32.520 --> 40:42.000]  что вы можете представить. То есть, на каждом узле должен быть какой-то файл с конфигурацией. И есть
[40:42.000 --> 40:50.960]  проблема теперь. Вот есть администратор, и он подготовил, тут на каждом узле лежит какой-то файл,
[40:50.960 --> 41:02.920]  он называется, ну так вот, очень условно, config.json. И вот есть первая версия, которая лежит, нулевая
[41:02.920 --> 41:07.620]  версия, то есть, которая была на старте каждого узла. А теперь у администратора есть
[41:07.620 --> 41:19.360]  config.json версии 2, версии 1. И он хочет этот конфиг положить на все узлы, обновить его на каждом
[41:19.360 --> 41:26.680]  узле. Понятно, что он не может так сделать, потому что все узлы, не все узлы сейчас доступны. Но его
[41:26.680 --> 41:39.040]  цель, чтобы конфигурация родается на каждом узле. Вот такая очень простая распределённая задача.
[41:39.040 --> 41:49.440]  Узнается, если у вас пластер большой, то нет никакой надежды, что все узлы будут живы и все
[41:49.440 --> 41:54.560]  узлы будут, можно будет собрать все узлы в каком-то временном промежутке и положить на них config.
[41:54.560 --> 42:00.680]  Если у вас там тысячи машин, то каждый день у вас сломаются какие-то там единицы, десятки
[42:00.680 --> 42:05.440]  дисков, вы постоянно что-то меняете, что-то отключено, поэтому нельзя просто так пройтись
[42:05.440 --> 42:18.760]  по всем машинам и обновить там конфигурацию. Как же быть? Как поддерживать узла в конфиге так,
[42:18.760 --> 42:30.280]  чтобы их можно было легко обновлять? Легко и надежно обновлять. Есть идеи,
[42:30.400 --> 42:43.880]  потому что вопрос простой, у Zookieper у нас есть. Вот вместо того, чтобы хранить на каждом узле
[42:43.880 --> 42:49.520]  конфигурацию какую-то сложную, мы на каждом узле будем хранить конфигурацию, которая называется
[42:49.520 --> 43:02.760]  bootstrap.json и все, что вот в этом файле будет написано, это адрес Zookieper, то есть реплики и порты,
[43:02.760 --> 43:09.200]  на которых работают узлы Zookieper, так чтобы просто каждый узел системы мог к этому Zookieper прийти.
[43:09.200 --> 43:16.000]  Зачем? Затем, что именно в Zookieper мы положим конфигурацию для каждого узла. Ну, давайте
[43:16.320 --> 43:23.480]  пропишем такую условную домашнюю, свою собственную распределенную систему абстрактную и вот в Zookieper мы
[43:23.480 --> 43:53.040]  заведем для нее узел. И здесь у нас будет узел config. И просто узлы знают, что это можно положить
[43:53.040 --> 44:03.120]  вот сюда. Этот путь меняться в будущем не планирует. Когда администратор хочет обновить конфигурацию
[44:03.120 --> 44:11.920]  каждого узла, он не пытается это сделать буквально на каждом узле, потому что он не знает, когда каждый
[44:11.920 --> 44:18.960]  отдельный узел станет доступным. Он пишет эту конфигурацию, обновляет ее здесь. Он вместо
[44:18.960 --> 44:24.680]  нулевой конфигурации записывает здесь первую конфигурацию. И эту операцию выполнить легко,
[44:24.680 --> 44:30.480]  потому что сам Zookieper является доступным, он может переживать отказа. Так что администратору
[44:30.480 --> 44:36.880]  скорее всего это удастся. И пусть каждый узел вместо того, чтобы администратор обновления конфигурации,
[44:36.880 --> 44:48.720]  он просто пользует узел Zookieper. Простой, тупой вариант изначальный. Пусть там каждый узел раз в
[44:48.720 --> 44:58.720]  30 секунд перечитывает эту конфигурацию. Ну и смотрите, я уже говорил, что когда мы обновляем данные
[44:58.720 --> 45:10.400]  Zookieper, когда мы говорим set data, то Zookieper бампает версию узла. Поэтому узлу легко понять,
[45:10.400 --> 45:18.680]  что конфигурация изменилась. Ему не нужно что-то там перечитывать и сравнивать. Он читает версию
[45:18.680 --> 45:25.280]  этого узла, и если она увеличилась, то значит администратор обновил конфигурацию и нужно сам файл перечитать.
[45:25.280 --> 45:36.760]  Идея понятна? То есть мы сосредоточили наше глобальное знание, вот некоторую глобальную истину,
[45:36.760 --> 45:43.720]  какова сейчас конфигурация в одной отказоустойчивой системе, а все узлы просто к ней обращаются. То есть мы
[45:43.720 --> 45:49.400]  вот эту коммуникацию Admin общается со всеми, заменили на Admin общается только с Zookieper,
[45:49.400 --> 45:54.600]  и узлы общаются с Zookieper. То есть мы прямую связь между ними разорвали, заменили ее на
[45:54.600 --> 46:05.560]  коммуникацию с общим отказоустойчивым компонентом. И для того, чтобы эту конструкцию
[46:05.560 --> 46:11.560]  запустить, достаточно просто иметь bootstrap-конфиг, который позволит узлу вообще стартовать. То есть
[46:11.560 --> 46:21.240]  он стартует, из этого конфига получает адрес Zookieper, получает путь конфига в Zookieper, и дальше из этого
[46:21.240 --> 46:30.040]  Zookieper, по этому пути вычитывает уже свой конфиг и поднимается. В чем проблема такого подхода?
[46:38.040 --> 46:41.240]  Узлы могут с разными версиями в один момент работать.
[46:42.240 --> 46:44.240]  Но я бы сказал, что это неизбежно.
[46:47.240 --> 46:52.240]  Если ты прям хочешь сделать все синхронно, то, боюсь, у тебя один способ – все выключить, а потом включить.
[46:53.240 --> 46:59.240]  Тут задача не в том, чтобы они все как-то синхронно переключились, это недостижимо все равно,
[46:59.240 --> 47:07.240]  а в том, чтобы, если конфиг меняется, то все узлы его рано или поздно перечитали и начали работать с ним.
[47:07.920 --> 47:15.920]  Тут не то, чтобы что-то плохое произойдет, просто конструкция очень неэффективна, а именно каждый узел полит Zookieper.
[47:15.920 --> 47:24.920]  А если у вас их там, не знаю, тысячи или десять тысяч, не дай бог, то они будут каждый раз в 10 секунд идти в Zookieper,
[47:24.920 --> 47:28.920]  то это просто большая нагрузка на сам Zookieper. Можно ли ее оптимизировать?
[47:28.920 --> 47:33.920]  Конечно же, мы этого хотим, потому что конфиг не будет обновляться каждый 10 секунд.
[47:34.600 --> 47:40.600]  Ну не знаю, раз в неделю может будет обновляться или раз в месяц вообще, а нагрузка будет постоянная у нас.
[47:40.600 --> 47:49.600]  Поэтому в апизу Zookieper есть еще одна фича очень важная, которая позволяет такие сценарии оптимизировать.
[47:49.600 --> 47:53.600]  Давайте вернемся на экран.
[48:04.600 --> 48:12.600]  На API у нас есть метод, который называется Exist.
[48:12.600 --> 48:20.600]  Мы туда передаем путь к узлу и получаем в ответ структуру, которая называется Stat.
[48:20.600 --> 48:27.600]  В общем, информация про то, в каком состоянии сейчас узел, и тут можно у него версию узнать.
[48:28.280 --> 48:33.280]  Много разных версий, много нюансов, но короче, сейчас не суть.
[48:33.280 --> 48:37.280]  То есть это мета информация про Zenoat внутри Zookieper.
[48:37.280 --> 48:42.280]  Но смотрите, в этом API есть еще один параметр.
[48:42.280 --> 48:46.280]  В этом вызове. Ой, не здесь, это структура.
[48:46.280 --> 48:51.280]  Вызов Exist. Вы передаете путь, а еще флажок Watch.
[48:51.960 --> 49:06.960]  Смотрите, когда вы создаете клиента Zookieper, вы помимо того, что указываете строчку, в которой перечислены реплики с портами, чтобы соединяться с ними,
[49:06.960 --> 49:16.960]  и помимо указания тайм-аута для вашей сессии, сколько секунд без кинга Zookieper будет считать вас живым.
[49:17.640 --> 49:20.640]  Вы передаете еще реализацию Watcher.
[49:20.640 --> 49:25.640]  Watcher – это интерфейс.
[49:25.640 --> 49:28.640]  Ключ оставите внизу.
[49:28.640 --> 49:31.640]  Хорошо, спасибо.
[49:31.640 --> 49:36.640]  Когда вы подключаетесь к кластеру, вы передаете еще Watcher.
[49:36.640 --> 49:41.640]  Watcher нужен для того, чтобы не полить, а заменить полных нотификаций.
[49:42.320 --> 49:47.320]  Вот у Watcher есть всего лишь один метод.
[49:47.320 --> 49:50.320]  Watcher реализует один метод процесс.
[49:50.320 --> 49:57.320]  И ему сваливается вот такое вот событие, что по некоторому пути случилось что-то.
[49:57.320 --> 50:01.320]  Ну, например, по некоторому пути был создан узел.
[50:01.320 --> 50:04.320]  В смысле, в некоторой директории был создан узел.
[50:04.320 --> 50:07.320]  Или удален узел.
[50:08.000 --> 50:12.000]  Или модифицирован узел.
[50:12.000 --> 50:16.000]  И вместо того, чтобы вот в этой конструкции, которая сзади меня была нарисована,
[50:16.000 --> 50:19.000]  каждый узел полил конфигурацию.
[50:19.000 --> 50:24.000]  Нет, каждый узел перечитывает конфигурацию и вешает Watch.
[50:24.000 --> 50:26.000]  Но нужно сделать это аккуратно.
[50:26.000 --> 50:29.000]  Нельзя просто прочесть конфигурацию, а потом сделать Exist с Watch.
[50:29.000 --> 50:32.000]  Понятно ли почему?
[50:38.000 --> 50:41.000]  Потому что между этими вызовами могло случиться изменение.
[50:41.000 --> 50:45.000]  Так что, смотрите, любая читающая операция может этот Watch поставить.
[50:45.000 --> 50:47.000]  И протокол такой.
[50:47.000 --> 50:49.000]  Вы стартуете на месте узла.
[50:49.000 --> 50:54.000]  Вы читаете с помощью GetData.
[50:54.000 --> 50:58.000]  Вы читаете ваш Bootstrap-конфиг статический, который не меняется.
[50:58.000 --> 51:01.000]  Там, где написан адрес зукипера.
[51:01.000 --> 51:06.000]  Вы из этого конфига получаете путь до вашего конфига в зукипере.
[51:06.680 --> 51:09.680]  Вы с помощью GetData этот конфиг читаете.
[51:09.680 --> 51:14.680]  И вместе с этим вы ставите Watch в рамках вашей сессии.
[51:14.680 --> 51:15.680]  То есть вы клиент.
[51:15.680 --> 51:18.680]  Вот права зукипера знают, что это не просто отдельный запрос,
[51:18.680 --> 51:20.680]  а что вы именно клиент сессии.
[51:20.680 --> 51:23.680]  И вы в рамках этой сессии вешаете Watch.
[51:23.680 --> 51:27.680]  И когда администратор обновит конфигурацию...
[51:27.680 --> 51:30.680]  Давайте я это нарисую сейчас на доске.
[51:36.680 --> 51:39.680]  У нас есть с одной стороны зукипер.
[51:39.680 --> 51:46.680]  Есть какие-то узлы вашей системы.
[51:46.680 --> 51:49.680]  И есть администратор.
[51:49.680 --> 51:58.680]  Вот на старте узлы системы обращаются к зукиперу
[51:58.680 --> 52:01.680]  с помощью с операцией GetData.
[52:02.360 --> 52:05.360]  Они читают себе конфиг.
[52:09.360 --> 52:16.360]  После этого, что у них есть сессия 1, сессия 2.
[52:16.360 --> 52:19.360]  И у зукипера теперь есть два Watch.
[52:19.360 --> 52:22.360]  То есть эти узлы теперь...
[52:22.360 --> 52:25.360]  Ну давайте такие вот очки нарисуем маленькие.
[52:25.360 --> 52:26.360]  Не знаю, видите, вы клиент.
[52:26.360 --> 52:29.360]  Они ждут изменений.
[52:30.040 --> 52:34.040]  Потом появляется администратор, и он будет разноцветным.
[52:41.040 --> 52:44.040]  Он обновляет конфиг с помощью SetData.
[52:48.040 --> 52:51.040]  После чего зукипер, понимая, что данные изменились,
[52:51.040 --> 52:54.040]  за которыми наблюдали два узла,
[52:54.720 --> 52:57.720]  сообщит им об этом с помощью Watch.
[53:04.720 --> 53:11.720]  И после этого, вот эти два узла пойдут и еще раз перечитают конфиги.
[53:17.720 --> 53:20.720]  Но при этом, что могло случиться?
[53:21.400 --> 53:24.400]  Пока они получат уведомление, пересчитают,
[53:24.400 --> 53:29.400]  в этом интервале вполне могло случиться еще одно обновление конфиг.
[53:30.400 --> 53:35.400]  Вот сам по себе Watch не говорит, что данные изменились на такие.
[53:35.400 --> 53:38.400]  Это просто событие, что что-то поменялось.
[53:38.400 --> 53:42.400]  И вот между этой нотификацией, когда Watch одноразовый,
[53:42.400 --> 53:45.400]  вы повесили его, вы получили один notification,
[53:45.400 --> 53:48.400]  и после этого он отменяется.
[53:48.400 --> 53:50.400]  Больше уже узел зачем не следит.
[53:51.080 --> 53:54.080]  Так вот, вы в такой схеме пропустите одну версию конфига.
[53:54.080 --> 53:56.080]  Но для задачей конфигурации это не страшно.
[53:56.080 --> 53:59.080]  То есть вы здесь перечитаете уже не первую версию, а вторую версию.
[54:01.080 --> 54:02.080]  Понятно?
[54:06.080 --> 54:07.080]  Понятно.
[54:07.080 --> 54:10.080]  Окей, тогда идем.
[54:10.080 --> 54:12.080]  Это такой метод замечания.
[54:12.080 --> 54:15.080]  Вот Watch в зукепере, это еще одна важная фича.
[54:15.760 --> 54:18.760]  Немерные узлы и Watch.
[54:22.760 --> 54:25.760]  Вот Watch это механизм каширования на самом деле.
[54:25.760 --> 54:30.760]  И вот эти нотификации это как протокол инвалидации кашей.
[54:30.760 --> 54:34.760]  Клиенты системы не хотят постоянно по любому поводу
[54:34.760 --> 54:37.760]  в зукепер ходить и перечитывать данные.
[54:37.760 --> 54:40.760]  Они хотят поддерживать локальную копию этих данных.
[54:40.760 --> 54:43.760]  Но для того, чтобы она была согласована,
[54:44.440 --> 54:48.440]  нужно, чтобы в случае перезаписи в зукепере ваша копия инвалидировалась.
[54:48.440 --> 54:51.440]  Ну вот это буквально как в протоколе конгерентности кашей.
[54:51.440 --> 54:55.440]  Если кто-то пишет систему, то он посылает другим запрос на инвалидацию.
[54:55.440 --> 55:00.440]  Вот в GoogleChab прямо так в зукепере немного по-другому.
[55:00.440 --> 55:01.440]  Но смысл такой же.
[55:01.440 --> 55:06.440]  Если кто-то подписался, а потом кто-то другой перезаписал данные,
[55:06.440 --> 55:11.440]  то эта перезапись приводит к триггеру нотификации
[55:12.120 --> 55:16.120]  у вас на клиенте вызывается процесс адвента какого-то
[55:16.120 --> 55:19.120]  и вы можете инвалидировать уже свой каш.
[55:19.120 --> 55:22.120]  То есть смысл ровно такой же.
[55:22.120 --> 55:27.120]  Это еще одна аналогия с атомиками и с процессорами.
[55:27.120 --> 55:30.120]  Это протокол конгерентности кашей.
[55:30.120 --> 55:33.120]  И не то, чтобы это было удивительно.
[55:33.120 --> 55:38.120]  Это как раз очень разумно, потому что мы решаем задачу координации.
[55:38.800 --> 55:41.800]  И поэтому инструменты такие же.
[55:44.800 --> 55:49.800]  Хорошо, с обновлением конфигурации мы разобрались.
[55:49.800 --> 55:55.800]  Давайте решим еще одну задачу, которая на системах возникает неизбежно.
[55:55.800 --> 55:58.800]  Это задача обнаружения спойных узлов.
[56:08.800 --> 56:11.800]  Как с ней поможет ZooKeeper?
[56:19.800 --> 56:22.800]  Представьте себе, мы снова пишем наш собственный маленький
[56:22.800 --> 56:25.800]  reproduce и у нас есть pool машин.
[56:25.800 --> 56:29.800]  И вот на этих машинах должны запускаться какие-то джабы,
[56:29.800 --> 56:34.800]  которые перемалывают часть каких-нибудь данных, файлов, таблиц, чего угодно.
[56:35.480 --> 56:40.480]  Мы считаем, что каждая машина – это некоторые вычислительные ресурсы.
[56:40.480 --> 56:45.480]  Это набор процессоров, которые могут выполнять reproduce и джабы.
[56:45.480 --> 56:49.480]  Но нам, конечно, нужно понимать, какие сейчас машины доступны, какие нет.
[56:54.480 --> 56:57.480]  Это еще один способ ZooKeeper поменять.
[56:57.480 --> 56:58.480]  Еще один очень простой способ.
[56:59.160 --> 57:03.160]  Если у нас есть наша система и там есть узлы Worker,
[57:03.160 --> 57:09.160]  то мы в этой конструкции, в нашей поддереве ZooKeeper,
[57:09.160 --> 57:13.160]  для нашей системы заведем еще один узел,
[57:13.160 --> 57:17.160]  который будет называться Workers.
[57:21.160 --> 57:24.160]  И каждый узел, когда он стартует, он просто приходит в ZooKeeper,
[57:24.160 --> 57:27.160]  потому что у него есть адрес ZooKeeper,
[57:27.840 --> 57:34.840]  и он в нем создает эфемерный узел со своим FQD.
[57:44.840 --> 57:48.840]  И теперь, если у нас есть такой условный компонент планировщик,
[57:48.840 --> 57:51.840]  который получает задачи от пользователя,
[57:51.840 --> 57:55.840]  делит данные на части, отправляет эти части с этими задачами на узлы,
[57:56.520 --> 57:59.520]  то вот этот планировщик будет что делать?
[57:59.520 --> 58:05.520]  Он должен будет следить за вот этой директорией.
[58:08.520 --> 58:11.520]  Он вешает на нее Watch,
[58:13.520 --> 58:18.520]  и если какой-то узел умирает, то у него протухает сессия,
[58:18.520 --> 58:21.520]  эфемерный узел исчезает,
[58:22.200 --> 58:27.200]  триггерится нацификация на этой директории у планировщика,
[58:27.200 --> 58:30.200]  и планировщик получит уведомление от системы,
[58:30.200 --> 58:33.200]  что директория изменилась,
[58:33.200 --> 58:38.200]  перечидывает ее и обновляет список живых узлов,
[58:38.200 --> 58:41.200]  список узлов в своей собственной памяти,
[58:41.200 --> 58:44.200]  которые он считает живыми.
[58:45.200 --> 58:48.200]  Ну и дальше, если он успел на какой-то узел задачу назначить,
[58:48.200 --> 58:51.200]  он ее переназначает на другой узел.
[58:51.880 --> 58:56.880]  То есть тут снова Watch,
[58:56.880 --> 58:59.880]  и снова эфемерные узлы,
[58:59.880 --> 59:02.880]  впервые эфемерные узлы, и снова Watch.
[59:04.880 --> 59:08.880]  Ну и снова этот планировщик кэширует на себя список живых машин.
[59:08.880 --> 59:11.880]  То есть если мы используем Watch, это означает,
[59:11.880 --> 59:14.880]  что мы видимо поддерживаем копию этого списка у себя,
[59:14.880 --> 59:18.880]  и если этот список меняется здесь, то он меняется и через донатификацию, и у нас.
[59:19.560 --> 59:23.560]  Если мы его перечитываем, обновляем после донатификации от ZooKeeper.
[59:28.560 --> 59:29.560]  Окей.
[59:29.560 --> 59:32.560]  Ну да, я забыл сказать про самое базовое применение ZooKeeper,
[59:32.560 --> 59:34.560]  а это сервис Discovery.
[59:34.560 --> 59:37.560]  То есть если у вас в системе довольно много разных сервисов,
[59:37.560 --> 59:40.560]  много компонентов, микросервисов,
[59:40.560 --> 59:43.560]  то для того, чтобы они просто могли находить друг друга,
[59:43.560 --> 59:46.560]  вы вместо установленного DNS можете использовать,
[59:46.560 --> 59:48.560]  ну вы можете использовать ZooKeeper как DNS,
[59:49.240 --> 59:52.240]  вы можете в нем создавать узлы,
[59:52.240 --> 59:54.240]  которые отвечают,
[59:54.240 --> 59:56.240]  имя которых это имя сервиса,
[59:56.240 --> 01:00:00.240]  и уже в рамках этих узлов записывать данные о том,
[01:00:00.240 --> 01:00:03.240]  какие из каких машин состоит, к кому можно приходить.
[01:00:05.240 --> 01:00:08.240]  Но вам нужен некоторая статическая для нас запись,
[01:00:08.240 --> 01:00:10.240]  а именно вот этот конфиг для того,
[01:00:10.240 --> 01:00:13.240]  чтобы просто найти сам ZooKeeper, потом же через него находится все остальное.
[01:00:13.920 --> 01:00:16.920]  Итак, идем дальше.
[01:00:19.920 --> 01:00:22.920]  Следующая очень естественная задача,
[01:00:22.920 --> 01:00:25.920]  которая возникает в распределенных системах,
[01:00:25.920 --> 01:00:28.920]  это задача выбора координатора.
[01:00:29.920 --> 01:00:31.920]  Вот если мы надеюсь,
[01:00:31.920 --> 01:00:34.920]  будем успеем поговорить с вами про капку,
[01:00:34.920 --> 01:00:36.920]  это все мести вроде должны,
[01:00:36.920 --> 01:00:38.920]  если я все правильно понимаю,
[01:00:38.920 --> 01:00:40.920]  то, ну это точно,
[01:00:41.600 --> 01:00:43.600]  тоже распределенная система,
[01:00:43.600 --> 01:00:45.600]  там есть какие-то узлы,
[01:00:45.600 --> 01:00:47.600]  которые хранят в копии логов,
[01:00:47.600 --> 01:00:49.600]  копии партий, что бы это ни значило пока,
[01:00:49.600 --> 01:00:51.600]  и выбирается среди всех узлов
[01:00:51.600 --> 01:00:53.600]  некоторый узел, который управляет другими узлами,
[01:00:53.600 --> 01:00:55.600]  который следит за тем, что в системе происходит,
[01:00:55.600 --> 01:00:57.600]  кто что хранит, кто жив, кто умер,
[01:00:57.600 --> 01:01:02.600]  и разумеется, этот узел должен быть в системе 1.
[01:01:04.600 --> 01:01:06.600]  Вот если у нас такая задача возникает,
[01:01:06.600 --> 01:01:10.600]  то очень разумно подумать в сторону взаимного исключения.
[01:01:11.600 --> 01:01:13.600]  Мы хотим сделать,
[01:01:13.600 --> 01:01:15.600]  мы хотим выбрать узел,
[01:01:15.600 --> 01:01:17.600]  который будет координатором,
[01:01:17.600 --> 01:01:19.600]  просто сделав распределенную блокировку,
[01:01:19.600 --> 01:01:21.600]  то есть пусть узел логит блокировку,
[01:01:21.600 --> 01:01:23.600]  и дальше будет руководить
[01:01:23.600 --> 01:01:25.600]  остальными узлами,
[01:01:25.600 --> 01:01:27.600]  потому что он один.
[01:01:27.600 --> 01:01:29.600]  Вот с такой мыслью,
[01:01:29.600 --> 01:01:31.600]  в Google придумали Chubby,
[01:01:31.600 --> 01:01:33.600]  именно поэтому Chubby был
[01:01:33.600 --> 01:01:35.600]  сервисом блокировок.
[01:01:35.600 --> 01:01:38.600]  В ZooKeeper от модели блокировок
[01:01:38.600 --> 01:01:40.600]  на уровне API отказались,
[01:01:40.600 --> 01:01:42.600]  то есть у нас нет методов
[01:01:42.600 --> 01:01:44.600]  лог, анлок, подобного,
[01:01:44.600 --> 01:01:46.600]  acquired risk. У нас есть операция
[01:01:46.600 --> 01:01:48.600]  атомиков, потому что понятно,
[01:01:48.600 --> 01:01:50.600]  что из атомиков можно сделать блокировку,
[01:01:50.600 --> 01:01:52.600]  то есть можно сделать спинлог.
[01:01:52.600 --> 01:01:54.600]  Так вот, мало шансов,
[01:01:54.600 --> 01:01:56.600]  что вы где-нибудь в своей жизни
[01:01:56.600 --> 01:01:58.600]  будете писать, ну не то,
[01:01:58.600 --> 01:02:00.600]  что мало шансов, но
[01:02:00.600 --> 01:02:02.600]  вряд ли вы в своей жизни будете писать
[01:02:02.600 --> 01:02:04.600]  спинлог для процессора.
[01:02:04.600 --> 01:02:06.600]  Скорее всего, он уже написан за вас,
[01:02:06.600 --> 01:02:08.600]  или вы просто используете.
[01:02:08.600 --> 01:02:10.600]  Но вот знание про спинлог
[01:02:10.600 --> 01:02:12.600]  внезапно полезно, если вы используете ZooKeeper,
[01:02:12.600 --> 01:02:14.600]  потому что в ZooKeeper нет
[01:02:14.600 --> 01:02:16.600]  в чистом виде блокировок,
[01:02:16.600 --> 01:02:18.600]  но через модель данных
[01:02:18.600 --> 01:02:20.600]  вы можете эти блокировки реализовать.
[01:02:20.600 --> 01:02:22.600]  Вот давайте представим,
[01:02:22.600 --> 01:02:24.600]  что мы хотим сделать,
[01:02:24.600 --> 01:02:26.600]  решить взаимные исключения
[01:02:26.600 --> 01:02:28.600]  с помощью ZooKeeper. У нас есть разные узлы,
[01:02:28.600 --> 01:02:30.600]  и каждый из них готов
[01:02:30.600 --> 01:02:32.600]  стать главным координатором,
[01:02:32.600 --> 01:02:34.600]  лидером, мастером
[01:02:34.600 --> 01:02:36.600]  всей нашей системы.
[01:02:38.600 --> 01:02:40.600]  Как бы мы могли такую задачу
[01:02:40.600 --> 01:02:42.600]  решить с помощью ZooKeeper?
[01:02:52.600 --> 01:02:54.600]  Давайте у нас опять будет
[01:02:54.600 --> 01:02:56.600]  директория с лучшим,
[01:02:56.600 --> 01:02:58.600]  и там в ней один
[01:02:58.600 --> 01:03:00.600]  эфемерный узел.
[01:03:00.600 --> 01:03:02.600]  У нас будет директория с лучшим,
[01:03:02.600 --> 01:03:04.600]  а в ней эфемерный узел.
[01:03:04.600 --> 01:03:06.600]  Вот.
[01:03:06.600 --> 01:03:08.600]  Когда он умирает,
[01:03:08.600 --> 01:03:10.600]  всем посылается сигнал,
[01:03:10.600 --> 01:03:12.600]  и всем вы пытаетесь делать касс.
[01:03:12.600 --> 01:03:14.600]  Вот у нас есть,
[01:03:14.600 --> 01:03:16.600]  давай скажем, что у нас есть
[01:03:16.600 --> 01:03:18.600]  персистентный узел
[01:03:18.600 --> 01:03:20.600]  slash, моя распределенная система
[01:03:20.600 --> 01:03:22.600]  slash master.
[01:03:22.600 --> 01:03:24.600]  И вот в этом,
[01:03:24.600 --> 01:03:26.600]  на этом узле мы создаем,
[01:03:26.600 --> 01:03:28.600]  в этой директории
[01:03:28.600 --> 01:03:30.600]  условной мы создаем потомка
[01:03:30.600 --> 01:03:32.600]  slash log.
[01:03:32.600 --> 01:03:34.600]  Вот этот узел эфемерный.
[01:03:36.600 --> 01:03:38.600]  Каждый узел,
[01:03:38.600 --> 01:03:40.600]  который хочет стать мастером системы,
[01:03:40.600 --> 01:03:42.600]  хочет стать координатором,
[01:03:42.600 --> 01:03:44.600]  пытается вот по этому пути
[01:03:44.600 --> 01:03:46.600]  создать потомку slash log.
[01:03:48.600 --> 01:03:50.600]  ZooKeeper, напомню, упорядочивает
[01:03:50.600 --> 01:03:52.600]  все операции, которые к нему приходят.
[01:03:52.600 --> 01:03:54.600]  В том числе все креаты.
[01:03:54.600 --> 01:03:56.600]  Поэтому среди креатов будет
[01:03:56.600 --> 01:03:58.600]  первый успешный
[01:03:58.600 --> 01:04:00.600]  и остальные неуспешные
[01:04:00.600 --> 01:04:02.600]  которые получат исключение,
[01:04:02.600 --> 01:04:04.600]  что узел уже существует.
[01:04:04.600 --> 01:04:06.600]  И вот когда мы создаем узел,
[01:04:06.600 --> 01:04:08.600]  мы вот это,
[01:04:08.600 --> 01:04:10.600]  мы, во-первых, создаем его
[01:04:10.600 --> 01:04:12.600]  атомарно, если его еще не было.
[01:04:12.600 --> 01:04:14.600]  Это, ну вот, буквально
[01:04:14.600 --> 01:04:16.600]  create это
[01:04:16.600 --> 01:04:18.600]  операция
[01:04:18.600 --> 01:04:20.600]  exchange в атомике,
[01:04:20.600 --> 01:04:22.600]  правда?
[01:04:22.600 --> 01:04:24.600]  Записать, если там пуст.
[01:04:24.600 --> 01:04:26.600]  Ну вот, create это вот создать,
[01:04:26.600 --> 01:04:28.600]  если ничего не было еще.
[01:04:28.600 --> 01:04:30.600]  И в этом create мы сразу
[01:04:30.600 --> 01:04:32.600]  положим внутрь этого узла
[01:04:32.600 --> 01:04:34.600]  в качестве данных
[01:04:34.600 --> 01:04:36.600]  просто свой адрес,
[01:04:36.600 --> 01:04:38.600]  чтобы другие узлы, которые
[01:04:38.600 --> 01:04:40.600]  провалят свой create,
[01:04:40.600 --> 01:04:42.600]  после этого понимали,
[01:04:42.600 --> 01:04:44.600]  что, ну, кто сейчас
[01:04:44.600 --> 01:04:46.600]  мастером выбрал.
[01:04:46.600 --> 01:04:48.600]  Если они пытались создать узел,
[01:04:48.600 --> 01:04:50.600]  а он уже существует, они его перечитают
[01:04:50.600 --> 01:04:52.600]  и узнают мастера.
[01:04:52.600 --> 01:04:54.600]  Ну, если вдруг это им нужно вообще.
[01:04:58.600 --> 01:05:00.600]  Хорошо, если у этого, да,
[01:05:00.600 --> 01:05:02.600]  мы создали этот узел,
[01:05:02.600 --> 01:05:04.600]  стали мастером, а что делают остальные,
[01:05:04.600 --> 01:05:06.600]  которые проиграли?
[01:05:06.600 --> 01:05:08.600]  Они, видимо, с помощью
[01:05:08.600 --> 01:05:10.600]  Exist вешают Watch
[01:05:12.600 --> 01:05:14.600]  и этот Exist может
[01:05:14.600 --> 01:05:16.600]  вернуться либо с,
[01:05:16.600 --> 01:05:18.600]  ну, либо этот файл все еще существует
[01:05:18.600 --> 01:05:20.600]  и они подвисят Watch.
[01:05:20.600 --> 01:05:22.600]  Либо вдруг,
[01:05:22.600 --> 01:05:24.600]  пока они вешали Watch, то есть между create
[01:05:24.600 --> 01:05:26.600]  и Exist файл был убален,
[01:05:26.600 --> 01:05:28.600]  потому что узел умер.
[01:05:28.600 --> 01:05:30.600]  И тогда они об этом опять же узнают
[01:05:30.600 --> 01:05:32.600]  и попробуют снова.
[01:05:32.600 --> 01:05:34.600]  Но если они успели создать Watch,
[01:05:34.600 --> 01:05:36.600]  если файл все еще существует
[01:05:36.600 --> 01:05:38.600]  и они подвисели Watch,
[01:05:38.600 --> 01:05:40.600]  то они просто ждут, пока не получат
[01:05:40.600 --> 01:05:42.600]  нотификацию о том, что
[01:05:42.600 --> 01:05:44.600]  узел slash log исчез из диктории
[01:05:44.600 --> 01:05:46.600]  мастер и попробуют снова.
[01:05:48.600 --> 01:05:50.600]  Ну, вот такой простой протокол.
[01:05:50.600 --> 01:05:52.600]  В чем его проблема?
[01:05:56.600 --> 01:05:58.600]  А мы Watch вешаем на мастер
[01:05:58.600 --> 01:06:00.600]  или на лог?
[01:06:00.600 --> 01:06:02.600]  Мы Watch вешаем на мастера
[01:06:02.600 --> 01:06:04.600]  или на нот?
[01:06:04.600 --> 01:06:06.600]  Тут, мне кажется, не важно уже.
[01:06:06.600 --> 01:06:08.600]  Как угодно можно сниматься.
[01:06:14.600 --> 01:06:16.600]  Ну, опять, наверное,
[01:06:16.600 --> 01:06:18.600]  неаккуратно вопрос задаю.
[01:06:18.600 --> 01:06:20.600]  В чем нерефективность
[01:06:20.600 --> 01:06:22.600]  такого решения?
[01:06:22.600 --> 01:06:24.600]  Но эта нефективность называется
[01:06:24.600 --> 01:06:26.600]  Sundering Hair.
[01:06:32.600 --> 01:06:34.600]  Проблема следующая.
[01:06:34.600 --> 01:06:36.600]  Вот это вот,
[01:06:36.600 --> 01:06:38.600]  вот это вот,
[01:06:38.600 --> 01:06:40.600]  вот это вот,
[01:06:40.600 --> 01:06:42.600]  вот это вот,
[01:06:42.600 --> 01:06:44.600]  вот это вот,
[01:06:44.600 --> 01:06:46.600]  вот это вот,
[01:06:46.600 --> 01:06:48.600]  вот это вот,
[01:06:48.600 --> 01:06:50.600]  вот это вот,
[01:06:50.600 --> 01:06:52.600]  проблема следующая.
[01:06:52.600 --> 01:06:54.600]  Вот у вас мастер был, а потом
[01:06:54.600 --> 01:06:56.600]  он отказал.
[01:06:56.600 --> 01:06:58.600]  И в этом случае
[01:06:58.600 --> 01:07:00.600]  узел исчез.
[01:07:00.600 --> 01:07:02.600]  У многих подписчиков, которые готовы были
[01:07:02.600 --> 01:07:04.600]  его заменить, сработала
[01:07:04.600 --> 01:07:06.600]  нотификация, пришло сообщение
[01:07:06.600 --> 01:07:08.600]  о том, что узел удален.
[01:07:08.600 --> 01:07:10.600]  И каждый из них посылает
[01:07:10.600 --> 01:07:12.600]  create-систему после этого.
[01:07:14.600 --> 01:07:16.600]  Выглядит не очень
[01:07:16.600 --> 01:07:18.600]  страшно. Выглядит
[01:07:18.600 --> 01:07:20.600]  не очень страшно, потому что
[01:07:20.600 --> 01:07:22.600]  вряд ли мастер будет умирать часто.
[01:07:22.600 --> 01:07:24.600]  Но с другой стороны, зависит
[01:07:24.600 --> 01:07:26.600]  от того, какой узел кипера используете.
[01:07:26.600 --> 01:07:28.600]  Во-первых, узел кипера могут пользоваться
[01:07:28.600 --> 01:07:30.600]  много систем. Во-вторых,
[01:07:30.600 --> 01:07:32.600]  в вашей системе просто могут быть
[01:07:32.600 --> 01:07:34.600]  много компонентов, которые выбирают
[01:07:34.600 --> 01:07:36.600]  среди себя какого-то главного.
[01:07:36.600 --> 01:07:38.600]  Это можно оптимизировать в кавке.
[01:07:38.600 --> 01:07:40.600]  Я покажу, как это сделано.
[01:07:40.600 --> 01:07:42.600]  Через неделю, видимо, или через две.
[01:07:42.600 --> 01:07:44.600]  Но потенциально
[01:07:44.600 --> 01:07:46.600]  это может стать неэффективным.
[01:07:46.600 --> 01:07:48.600]  Что сразу много клиентов приходят
[01:07:48.600 --> 01:07:50.600]  и начинают ломиться в
[01:07:50.600 --> 01:07:52.600]  зу кипер, создавать у него
[01:07:52.600 --> 01:07:54.600]  тот же узел. Хотя удастся
[01:07:54.600 --> 01:07:56.600]  только одному.
[01:07:58.600 --> 01:08:00.600]  Можно ли этот сценарий оптимизировать?
[01:08:00.600 --> 01:08:02.600]  Разумеется, может.
[01:08:02.600 --> 01:08:04.600]  И даже вы знаете как.
[01:08:06.600 --> 01:08:08.600]  Для этого вы
[01:08:08.600 --> 01:08:10.600]  в прошлом семестре
[01:08:10.600 --> 01:08:12.600]  писали тикет-лог.
[01:08:12.600 --> 01:08:14.600]  Было такое?
[01:08:16.600 --> 01:08:18.600]  Да, было такое.
[01:08:18.600 --> 01:08:20.600]  Но я не знаю,
[01:08:20.600 --> 01:08:22.600]  что такое тикет-лог.
[01:08:22.600 --> 01:08:24.600]  Я не знаю, что такое тикет-лог.
[01:08:24.600 --> 01:08:26.600]  Я не знаю, что такое тикет-лог.
[01:08:26.600 --> 01:08:28.600]  Было такое?
[01:08:32.600 --> 01:08:34.600]  Вот тикет-лог можно сделать
[01:08:34.600 --> 01:08:36.600]  в зу кипере. Причем
[01:08:36.600 --> 01:08:38.600]  я бы сказал, что это такой
[01:08:38.600 --> 01:08:40.600]  рекомендуемый способ
[01:08:40.600 --> 01:08:42.600]  изготовления блокировки в зу кипере.
[01:08:42.600 --> 01:08:44.600]  Вообще в зу кипере можно делать самые
[01:08:44.600 --> 01:08:46.600]  разные вещи.
[01:08:46.600 --> 01:08:48.600]  И поскольку
[01:08:48.600 --> 01:08:50.600]  зу кипер, у него AP слишком низкоуровневая,
[01:08:50.600 --> 01:08:52.600]  то есть он не решает некую конкретную задачу.
[01:08:52.600 --> 01:08:54.600]  Он скорее дает инструменты для того, чтобы это все сделать.
[01:08:54.600 --> 01:08:56.600]  То есть так же, как в C++, у вас есть атомики,
[01:08:56.600 --> 01:08:58.600]  а дальше крутитесь, как хотите.
[01:08:58.600 --> 01:09:00.600]  Так вот, зу кипере тоже обливает атомики
[01:09:00.600 --> 01:09:02.600]  и говорят, что вы с помощью атомиков
[01:09:02.600 --> 01:09:04.600]  можете с помощью атомиков и вочей
[01:09:04.600 --> 01:09:06.600]  обвочить такой аналог фьютекса,
[01:09:06.600 --> 01:09:08.600]  где вы можете заснуть.
[01:09:08.600 --> 01:09:10.600]  Но тут все очень
[01:09:10.600 --> 01:09:12.600]  сходство совершенно прямое.
[01:09:12.600 --> 01:09:14.600]  Вы можете из всего этого делать
[01:09:14.600 --> 01:09:16.600]  разные примитивы коммуникации.
[01:09:16.600 --> 01:09:18.600]  Вы можете делать разные локи, можно делать барьеры,
[01:09:18.600 --> 01:09:20.600]  можно делать очереди.
[01:09:20.600 --> 01:09:22.600]  Но, наверное, мы не успеем уже про очереди поговорить.
[01:09:22.600 --> 01:09:24.600]  Но давайте хотя бы с локами разберемся.
[01:09:24.600 --> 01:09:26.600]  Так вот, предлагается сделать
[01:09:26.600 --> 01:09:28.600]  немного пооптимальный протокол,
[01:09:28.600 --> 01:09:30.600]  чтобы, если
[01:09:30.600 --> 01:09:32.600]  кто-то лог отпускает,
[01:09:32.600 --> 01:09:34.600]  то после этого не все ломились
[01:09:34.600 --> 01:09:36.600]  в зу кипер, а только один узел, который
[01:09:36.600 --> 01:09:38.600]  стоит в очереди.
[01:09:38.600 --> 01:09:40.600]  Для этого, когда мы пытаемся
[01:09:40.600 --> 01:09:42.600]  захватить лог, то мы берем
[01:09:42.600 --> 01:09:44.600]  директорию мьютекс,
[01:09:44.600 --> 01:09:46.600]  директорию лока
[01:09:46.600 --> 01:09:48.600]  и создаем там
[01:09:48.600 --> 01:09:50.600]  эфемерный sequential узел
[01:09:50.600 --> 01:09:52.600]  logDefice.
[01:09:52.600 --> 01:09:54.600]  Напомню, что sequential означает, что
[01:09:54.600 --> 01:09:56.600]  к этому имени прибавится еще
[01:09:56.600 --> 01:09:58.600]  некоторый порядковый номер, который будет присвоен
[01:09:58.600 --> 01:10:00.600]  самой системой.
[01:10:00.600 --> 01:10:02.600]  Ну и когда
[01:10:02.600 --> 01:10:04.600]  вы вызываете create, то
[01:10:04.600 --> 01:10:06.600]  смотрим на API,
[01:10:06.600 --> 01:10:08.600]  вы получаете путь, который вы
[01:10:08.600 --> 01:10:10.600]  создали. Ну потому что если вы создали персистентный
[01:10:10.600 --> 01:10:12.600]  узел, просто персистентный, то это будет
[01:10:12.600 --> 01:10:14.600]  ваш пас. Если вы создали sequential
[01:10:14.600 --> 01:10:16.600]  узел, то вам вернется порядковый номер
[01:10:16.600 --> 01:10:18.600]  его.
[01:10:18.600 --> 01:10:20.600]  Вот, создали
[01:10:20.600 --> 01:10:22.600]  эфемерный sequential узел.
[01:10:22.600 --> 01:10:24.600]  То есть вы таким образом получили
[01:10:24.600 --> 01:10:26.600]  тикет в тикетлоке, заняли
[01:10:26.600 --> 01:10:28.600]  какой-то порядковый номер, заняли
[01:10:28.600 --> 01:10:30.600]  свою очередь. Но
[01:10:30.600 --> 01:10:32.600]  чем отличается наш протокол от тикетлока
[01:10:32.600 --> 01:10:34.600]  тем, что у нас клиенты могут отказывать
[01:10:34.600 --> 01:10:36.600]  и
[01:10:36.600 --> 01:10:38.600]  поэтому из этой очереди
[01:10:38.600 --> 01:10:40.600]  тикетов какие-то номера могут
[01:10:40.600 --> 01:10:42.600]  выпадать. Но может быть вы
[01:10:42.600 --> 01:10:44.600]  уже выиграли, на самом деле, непонятно. Поэтому
[01:10:44.600 --> 01:10:46.600]  первое, что вы делаете, вы говорите getChildren.
[01:10:46.600 --> 01:10:48.600]  И таким образом узнаете
[01:10:48.600 --> 01:10:50.600]  как вы упорядочились относительно
[01:10:50.600 --> 01:10:52.600]  других. То есть либо вы
[01:10:52.600 --> 01:10:54.600]  первый в директории, и тогда уже
[01:10:54.600 --> 01:10:56.600]  новый узел, который может
[01:10:56.600 --> 01:10:58.600]  быть создан, получит
[01:10:58.600 --> 01:11:00.600]  порядковый номер больше, чем вы.
[01:11:00.600 --> 01:11:02.600]  Так что если вы видите
[01:11:02.600 --> 01:11:04.600]  с помощью getChildren, что вы
[01:11:04.600 --> 01:11:06.600]  наименьший узел в директории,
[01:11:06.600 --> 01:11:08.600]  графически, то
[01:11:08.600 --> 01:11:10.600]  вы можете быть уверенными, что вы
[01:11:10.600 --> 01:11:12.600]  первой в очереди на блокировку.
[01:11:12.600 --> 01:11:14.600]  В этом случае
[01:11:14.600 --> 01:11:16.600]  вы уходите и вы довольны.
[01:11:16.600 --> 01:11:18.600]  Что если вы узнали,
[01:11:18.600 --> 01:11:20.600]  что вы не первой? Ну вы должны
[01:11:20.600 --> 01:11:22.600]  ждать. Ждать кого?
[01:11:24.600 --> 01:11:26.600]  Вы можете повесить watch
[01:11:26.600 --> 01:11:28.600]  на всю директорию.
[01:11:28.600 --> 01:11:30.600]  И когда какой-то узел
[01:11:30.600 --> 01:11:32.600]  будет появляться, или какой-то узел
[01:11:32.600 --> 01:11:34.600]  будет освобождать
[01:11:34.600 --> 01:11:36.600]  блокировку, даже если перед вами другие
[01:11:36.600 --> 01:11:38.600]  узлы еще есть, вы будете
[01:11:38.600 --> 01:11:40.600]  получать нотификацию и перечитывать всю
[01:11:40.600 --> 01:11:42.600]  директорию. Это будет неэффективно.
[01:11:42.600 --> 01:11:44.600]  Поэтому вы вешаете
[01:11:44.600 --> 01:11:46.600]  watch на
[01:11:46.600 --> 01:11:48.600]  узел перед вами.
[01:11:48.600 --> 01:11:50.600]  Вы говорите exists
[01:11:50.600 --> 01:11:52.600]  на предшествующем
[01:11:52.600 --> 01:11:54.600]  лексикографическом порядке узле.
[01:11:54.600 --> 01:11:56.600]  Этот exists
[01:11:56.600 --> 01:11:58.600]  мог вернуть вам false, потому что
[01:11:58.600 --> 01:12:00.600]  пока вы все это делали, ваша очередь
[01:12:00.600 --> 01:12:02.600]  прям подошла.
[01:12:02.600 --> 01:12:04.600]  Ну тогда ваше представление
[01:12:04.600 --> 01:12:06.600]  об очереди устарели, и вы
[01:12:06.600 --> 01:12:08.600]  откатываетесь в пункту 2.
[01:12:08.600 --> 01:12:10.600]  Но если вы подписались,
[01:12:10.600 --> 01:12:12.600]  вот здесь вот, успешно,
[01:12:12.600 --> 01:12:14.600]  с помощью exists
[01:12:14.600 --> 01:12:16.600]  повесили watch
[01:12:16.600 --> 01:12:18.600]  на предыдущий файл,
[01:12:18.600 --> 01:12:20.600]  то вы просто ждете, пока он
[01:12:20.600 --> 01:12:22.600]  не исчезнет.
[01:12:22.600 --> 01:12:24.600]  Непонятно, почему он исчезнет, потому что
[01:12:24.600 --> 01:12:26.600]  узел умер и выпал из очереди.
[01:12:26.600 --> 01:12:28.600]  Клиент умер и выпал из очереди.
[01:12:28.600 --> 01:12:30.600]  Или потому что ваша очередь просто
[01:12:30.600 --> 01:12:32.600]  подошла. Но в обоих случаях
[01:12:32.600 --> 01:12:34.600]  вы
[01:12:34.600 --> 01:12:36.600]  откатываетесь в пункту 2
[01:12:36.600 --> 01:12:38.600]  и перечитываете
[01:12:38.600 --> 01:12:40.600]  список узлов и
[01:12:40.600 --> 01:12:42.600]  обновляете свое
[01:12:42.600 --> 01:12:44.600]  понимание происходящее.
[01:12:44.600 --> 01:12:46.600]  То есть это вот буквально ticket lock
[01:12:46.600 --> 01:12:48.600]  с блокирующим
[01:12:48.600 --> 01:12:50.600]  ожиданием.
[01:12:50.600 --> 01:12:52.600]  И здесь вы делаете get children
[01:12:52.600 --> 01:12:54.600]  тогда, когда
[01:12:54.600 --> 01:12:56.600]  умирает узел в очереди
[01:12:56.600 --> 01:12:58.600]  перед вами.
[01:12:58.600 --> 01:13:00.600]  То есть если
[01:13:00.600 --> 01:13:02.600]  допустим все живые берут блокировку в
[01:13:02.600 --> 01:13:04.600]  каком-то порядке, то в принципе ничего страшного
[01:13:04.600 --> 01:13:06.600]  не происходит. Каждый
[01:13:06.600 --> 01:13:08.600]  то есть стирание вот этого вашего узла
[01:13:08.600 --> 01:13:10.600]  приводит к всего лишь к пробуждению,
[01:13:10.600 --> 01:13:12.600]  то есть к иноцификации
[01:13:12.600 --> 01:13:14.600]  только одного ждущего узла,
[01:13:14.600 --> 01:13:16.600]  одного ждущего клиента.
[01:13:20.600 --> 01:13:22.600]  Я не знаю,
[01:13:22.600 --> 01:13:24.600]  на самом деле вот мы
[01:13:24.600 --> 01:13:26.600]  минутка забавных
[01:13:26.600 --> 01:13:28.600]  фактов.
[01:13:28.600 --> 01:13:30.600]  Я кажется весной
[01:13:30.600 --> 01:13:32.600]  перестал это рассказывать, потому что все не помещается,
[01:13:32.600 --> 01:13:34.600]  но вообще в
[01:13:34.600 --> 01:13:36.600]  линуксе,
[01:13:36.600 --> 01:13:38.600]  вообще в жизни
[01:13:38.600 --> 01:13:40.600]  есть разные спинлоки, и вот есть
[01:13:40.600 --> 01:13:42.600]  спинлоки, которые оптимизированы
[01:13:42.600 --> 01:13:44.600]  для протокололога
[01:13:44.600 --> 01:13:46.600]  генетности кэшей, чтобы запись не инвалидировала
[01:13:46.600 --> 01:13:48.600]  многих других узлов.
[01:13:48.600 --> 01:13:50.600]  Так вот в линуксе есть вот примерно
[01:13:50.600 --> 01:13:52.600]  такой спинлок, который я вам рассказал, только он
[01:13:52.600 --> 01:13:54.600]  реализован с помощью там weight-free
[01:13:54.600 --> 01:13:56.600]  очередей.
[01:13:56.600 --> 01:13:58.600]  Это такая сложная реализация, есть там попроще
[01:13:58.600 --> 01:14:00.600]  по-моему.
[01:14:04.600 --> 01:14:06.600]  Если я найду его за 5 секунд,
[01:14:06.600 --> 01:14:08.600]  то хорошо, не найду, то сдамся.
[01:14:08.600 --> 01:14:10.600]  А нет, вот же он.
[01:14:10.600 --> 01:14:12.600]  Это называется
[01:14:12.600 --> 01:14:14.600]  mcs-spinlock,
[01:14:14.600 --> 01:14:16.600]  очень простой, очень истроумный,
[01:14:16.600 --> 01:14:18.600]  и вот
[01:14:18.600 --> 01:14:20.600]  идея похожая.
[01:14:20.600 --> 01:14:22.600]  Но
[01:14:22.600 --> 01:14:24.600]  если мы поговорим, то то есть
[01:14:24.600 --> 01:14:26.600]  вот так вот можно делать распределенные
[01:14:26.600 --> 01:14:28.600]  блокировки, можно выбирать мастер,
[01:14:28.600 --> 01:14:30.600]  короче, чего мы научились делать?
[01:14:30.600 --> 01:14:32.600]  Мы научились поддерживать конфигурации
[01:14:32.600 --> 01:14:34.600]  мы научились делать
[01:14:34.600 --> 01:14:36.600]  failure detection и
[01:14:36.600 --> 01:14:38.600]  просто
[01:14:38.600 --> 01:14:40.600]  понимать, в каком состоянии сейчас кластер находится,
[01:14:40.600 --> 01:14:42.600]  с каких узлов он состоит.
[01:14:42.600 --> 01:14:44.600]  Мы научились выбирать
[01:14:44.600 --> 01:14:46.600]  мастер с помощью блокировок.
[01:14:48.600 --> 01:14:50.600]  Ну вот про блокировки
[01:14:50.600 --> 01:14:52.600]  разговор еще не закончен, потому что
[01:14:52.600 --> 01:14:54.600]  сами распределенные блокировки
[01:14:54.600 --> 01:14:56.600]  это довольно хрупкая вещь, потому что
[01:14:56.600 --> 01:14:58.600]  они, в отличие от блокировок
[01:14:58.600 --> 01:15:00.600]  многопоточных, не
[01:15:00.600 --> 01:15:02.600]  гарантируют взаимного исключения.
[01:15:02.600 --> 01:15:04.600]  Вот про это есть
[01:15:04.600 --> 01:15:06.600]  замечательная статья Мартина Клепмана,
[01:15:06.600 --> 01:15:08.600]  вот она.
[01:15:08.600 --> 01:15:10.600]  Мартин Клепман, как говорит этот картинка справа,
[01:15:10.600 --> 01:15:12.600]  автор всем известной
[01:15:12.600 --> 01:15:14.600]  книжки с Кабаном,
[01:15:14.600 --> 01:15:16.600]  в которой, вот такая огромная
[01:15:16.600 --> 01:15:18.600]  толстая книга, в которой в перемешку,
[01:15:18.600 --> 01:15:20.600]  мне кажется, без всякой системы
[01:15:20.600 --> 01:15:22.600]  описаны разные идеи
[01:15:22.600 --> 01:15:24.600]  алгоритмические, инженерные,
[01:15:24.600 --> 01:15:26.600]  которые возникают при проектировании
[01:15:26.600 --> 01:15:28.600]  больших систем обработки
[01:15:28.600 --> 01:15:30.600]  и хранения данных.
[01:15:30.600 --> 01:15:32.600]  Но мне кажется, что там
[01:15:32.600 --> 01:15:34.600]  книга не самым лучшим образом
[01:15:34.600 --> 01:15:36.600]  декомпозирована, там нет отдельно
[01:15:36.600 --> 01:15:38.600]  не отделены инженерные
[01:15:38.600 --> 01:15:40.600]  аспекты от какой-то теории,
[01:15:40.600 --> 01:15:42.600]  очень сложно по ней какое-то понимание
[01:15:42.600 --> 01:15:44.600]  построить. Но
[01:15:44.600 --> 01:15:46.600]  тем не менее, там очень много
[01:15:46.600 --> 01:15:48.600]  всего, и книгу
[01:15:48.600 --> 01:15:50.600]  я определенно рекомендую.
[01:15:50.600 --> 01:15:52.600]  Так вот, этот человек написал замечательный пост
[01:15:52.600 --> 01:15:54.600]  про распределенные блокировки, про то, что
[01:15:54.600 --> 01:15:56.600]  идея вообще довольно сомнительная, потому что
[01:15:56.600 --> 01:15:58.600]  если вы пишете такой код, я иду
[01:15:58.600 --> 01:16:00.600]  в сервис блокировок, это Чаби в Google
[01:16:00.600 --> 01:16:02.600]  или это ZooKeeper, где я сделал свой тикет-лог
[01:16:02.600 --> 01:16:04.600]  и беру там блокировку по какому-то пути,
[01:16:04.600 --> 01:16:06.600]  а потом
[01:16:06.600 --> 01:16:08.600]  под этой блокировкой
[01:16:08.600 --> 01:16:10.600]  я работаю
[01:16:10.600 --> 01:16:12.600]  с каким-то внешним состоянием,
[01:16:12.600 --> 01:16:14.600]  а потом лог отпускаю,
[01:16:14.600 --> 01:16:16.600]  то в многопоточном варианте
[01:16:16.600 --> 01:16:18.600]  этот код, конечно, будет правильным, а
[01:16:18.600 --> 01:16:20.600]  в распределенном он не будет работать, потому
[01:16:20.600 --> 01:16:22.600]  что, ну опять,
[01:16:22.600 --> 01:16:24.600]  у вас есть вы
[01:16:24.600 --> 01:16:26.600]  клиент системы, и вот вы думаете, что
[01:16:26.600 --> 01:16:28.600]  вы владеете блокировкой, а есть система,
[01:16:28.600 --> 01:16:30.600]  которая вам эту блокировку
[01:16:30.600 --> 01:16:32.600]  выдала, и
[01:16:32.600 --> 01:16:34.600]  ваше понимание о том, владеете вы блокировкой
[01:16:34.600 --> 01:16:36.600]  или нет, оно может
[01:16:36.600 --> 01:16:38.600]  с системой расходиться. Вот тут есть
[01:16:38.600 --> 01:16:40.600]  замечательная диаграмма, можно ее проиллюстрировать,
[01:16:40.600 --> 01:16:42.600]  наверное, это то, что нам нужно.
[01:16:42.600 --> 01:16:44.600]  Вы взяли
[01:16:44.600 --> 01:16:46.600]  блокировку, она называется Лиза,
[01:16:46.600 --> 01:16:48.600]  а не блокировка, у нас, к сожалению,
[01:16:48.600 --> 01:16:50.600]  нет сейчас силы разбираться в тонкостях,
[01:16:50.600 --> 01:16:52.600]  точнее я потом расскажу. Взяли
[01:16:52.600 --> 01:16:54.600]  блокировку, получили ответ,
[01:16:54.600 --> 01:16:56.600]  и эта блокировка
[01:16:56.600 --> 01:16:58.600]  привязана, ну эта блокировка,
[01:16:58.600 --> 01:17:00.600]  это эфемерный узел, как мы уже выяснили,
[01:17:00.600 --> 01:17:02.600]  и жизнь этого эфемерного узла
[01:17:02.600 --> 01:17:04.600]  привязана к жизни сессии.
[01:17:04.600 --> 01:17:06.600]  Вот, блок-сервис поддерживает вашу сессию,
[01:17:06.600 --> 01:17:08.600]  у него есть тайм-аут.
[01:17:08.600 --> 01:17:10.600]  Если этот тайм-аут истекает,
[01:17:10.600 --> 01:17:12.600]  а вы не посылаете пинги системе, то она считает,
[01:17:12.600 --> 01:17:14.600]  что вы, разумно считает, что вы отказали.
[01:17:14.600 --> 01:17:16.600]  После этого блокировку
[01:17:16.600 --> 01:17:18.600]  вас отнимает, поэтому это не блокировка,
[01:17:18.600 --> 01:17:20.600]  называется Лиза. То есть вы
[01:17:20.600 --> 01:17:22.600]  арендуете блокировку, а не захватываете его
[01:17:22.600 --> 01:17:24.600]  владение.
[01:17:24.600 --> 01:17:26.600]  У вас блокировку отнимает,
[01:17:26.600 --> 01:17:28.600]  но вы на самом деле живы, потому что
[01:17:28.600 --> 01:17:30.600]  вы просто заснули
[01:17:30.600 --> 01:17:32.600]  на паузе сборки мусора.
[01:17:32.600 --> 01:17:34.600]  И появляется другой клиент,
[01:17:34.600 --> 01:17:36.600]  и он эту блокировку перехватывает.
[01:17:36.600 --> 01:17:38.600]  И после этого он под этой
[01:17:38.600 --> 01:17:40.600]  блокировкой пишет что-то вот сюда.
[01:17:42.600 --> 01:17:44.600]  Какое-то внешнее хранилище.
[01:17:44.600 --> 01:17:46.600]  А потом просыпаетесь вы,
[01:17:46.600 --> 01:17:48.600]  и вы заснули просто вот между этой
[01:17:48.600 --> 01:17:50.600]  строчкой,
[01:17:50.600 --> 01:17:52.600]  после этой строчки и до вот этой.
[01:17:52.600 --> 01:17:54.600]  Вы все еще думаете,
[01:17:54.600 --> 01:17:56.600]  что вы владеете блокировкой.
[01:17:56.600 --> 01:17:58.600]  И вы пишете в третью
[01:17:58.600 --> 01:18:00.600]  систему, и вот тут-то вы
[01:18:00.600 --> 01:18:02.600]  нарушили.
[01:18:02.600 --> 01:18:04.600]  Это не то чтобы до-после,
[01:18:04.600 --> 01:18:06.600]  а вот у вас есть
[01:18:06.600 --> 01:18:08.600]  два конкурентных вызова, и они
[01:18:08.600 --> 01:18:10.600]  неопорядочны блокировкой, получается, теперь.
[01:18:10.600 --> 01:18:12.600]  Это же та проблема, которая у нас была
[01:18:12.600 --> 01:18:14.600]  в кундуаре,
[01:18:14.600 --> 01:18:16.600]  которую мы решали.
[01:18:16.600 --> 01:18:18.600]  На кундуар похоже.
[01:18:18.600 --> 01:18:20.600]  Нам нужно проверять каждый раз,
[01:18:20.600 --> 01:18:22.600]  что мы все еще...
[01:18:22.600 --> 01:18:24.600]  Ну да,
[01:18:24.600 --> 01:18:26.600]  если мы что делаем
[01:18:26.600 --> 01:18:28.600]  во внешней системе, то мы должны
[01:18:28.600 --> 01:18:30.600]  убедиться, что мы все еще владеем
[01:18:30.600 --> 01:18:32.600]  блокировкой.
[01:18:32.600 --> 01:18:34.600]  Беда в том, что у нас здесь две системы,
[01:18:34.600 --> 01:18:36.600]  они друг про друга работают,
[01:18:36.600 --> 01:18:38.600]  а мы все еще не владеем блокировкой.
[01:18:38.600 --> 01:18:40.600]  Да, здесь две системы, они друг про друга
[01:18:40.600 --> 01:18:42.600]  ничего не знают, но и не могут знать.
[01:18:42.600 --> 01:18:44.600]  Поэтому тут есть
[01:18:44.600 --> 01:18:46.600]  разные варианты, как можно поступать.
[01:18:46.600 --> 01:18:48.600]  Вариантов
[01:18:48.600 --> 01:18:50.600]  много.
[01:18:50.600 --> 01:18:52.600]  Для этой лекции важно, скорее,
[01:18:52.600 --> 01:18:54.600]  что
[01:18:54.600 --> 01:18:56.600]  ZooKeeper сам по себе
[01:18:56.600 --> 01:18:58.600]  такую проблему допускает.
[01:18:58.600 --> 01:19:00.600]  Вот отдельный сервис блокировок
[01:19:00.600 --> 01:19:02.600]  такую проблему допускает.
[01:19:02.600 --> 01:19:04.600]  Как можно эту проблему решить?
[01:19:04.600 --> 01:19:06.600]  Можно решить ее,
[01:19:06.600 --> 01:19:08.600]  поместив данные,
[01:19:08.600 --> 01:19:10.600]  с которыми вы работаете,
[01:19:10.600 --> 01:19:12.600]  и блокировку в одну и ту же систему,
[01:19:12.600 --> 01:19:14.600]  то есть хранить в ZooKeeper и данные,
[01:19:14.600 --> 01:19:16.600]  и блокировку.
[01:19:16.600 --> 01:19:18.600]  Конечно, в ZooKeeper много данных не поместится,
[01:19:18.600 --> 01:19:20.600]  но какое-то количество данных все-таки поместится.
[01:19:20.600 --> 01:19:22.600]  И вы можете аккуратно
[01:19:22.600 --> 01:19:24.600]  обновлять эти данные,
[01:19:24.600 --> 01:19:26.600]  только если вы все еще владеете блокировкой.
[01:19:26.600 --> 01:19:28.600]  Если у вас
[01:19:28.600 --> 01:19:30.600]  отнимут блокировку, значит у вас протухло сессия,
[01:19:30.600 --> 01:19:32.600]  значит вы и данные обновлять не можете.
[01:19:32.600 --> 01:19:34.600]  Но это удобный
[01:19:34.600 --> 01:19:36.600]  удобный дизайн системы,
[01:19:36.600 --> 01:19:38.600]  когда у вас такие...
[01:19:38.600 --> 01:19:40.600]  Когда у вас можно
[01:19:40.600 --> 01:19:42.600]  часть состояния поместить ZooKeeper.
[01:19:42.600 --> 01:19:44.600]  Но это не всегда возможно,
[01:19:44.600 --> 01:19:46.600]  поэтому в общем случае у вас проблемы.
[01:19:46.600 --> 01:19:48.600]  Ну а некоторые... Простите,
[01:19:48.600 --> 01:19:50.600]  он простудился.
[01:19:50.600 --> 01:19:52.600]  Некоторые сложные системы, ну скажем,
[01:19:52.600 --> 01:19:54.600]  для тех,
[01:19:54.600 --> 01:19:56.600]  кто слушал меня по субботам,
[01:19:56.600 --> 01:19:58.600]  я рассказывал про YandexDB.
[01:19:58.600 --> 01:20:00.600]  Это
[01:20:00.600 --> 01:20:02.600]  большая распределенная база данных
[01:20:02.600 --> 01:20:04.600]  с детерминированными транзакциями,
[01:20:04.600 --> 01:20:06.600]  которые написаны в Yandex для Yandex облака,
[01:20:06.600 --> 01:20:08.600]  но не только для Yandex облака.
[01:20:08.600 --> 01:20:10.600]  Так вот, там есть такой же...
[01:20:10.600 --> 01:20:12.600]  Там похожий дизайн.
[01:20:12.600 --> 01:20:14.600]  В смысле, у вас есть подсистема,
[01:20:14.600 --> 01:20:16.600]  в которой можно хранить и блокировки,
[01:20:16.600 --> 01:20:18.600]  то есть и...
[01:20:20.600 --> 01:20:22.600]  Я остановлюсь,
[01:20:22.600 --> 01:20:24.600]  отмотаю себя в прошлое
[01:20:24.600 --> 01:20:26.600]  на 30 секунд.
[01:20:26.600 --> 01:20:28.600]  И скажу, что вот как такие проблемы
[01:20:28.600 --> 01:20:30.600]  хотя бы можно замечать?
[01:20:30.600 --> 01:20:32.600]  В этом хранилище.
[01:20:32.600 --> 01:20:34.600]  Можно просто локи версионировать.
[01:20:34.600 --> 01:20:36.600]  То есть вы не просто владеете локом,
[01:20:36.600 --> 01:20:38.600]  вы владеете локом,
[01:20:38.600 --> 01:20:40.600]  я не знаю, пятым по очереди,
[01:20:40.600 --> 01:20:42.600]  или шестым по очереди.
[01:20:42.600 --> 01:20:44.600]  То есть вы с блокировками связываете
[01:20:44.600 --> 01:20:46.600]  еще и поколения.
[01:20:46.600 --> 01:20:48.600]  Поэтому,
[01:20:48.600 --> 01:20:50.600]  если мы говорим про Google Chabi,
[01:20:50.600 --> 01:20:52.600]  это снова не он,
[01:20:52.600 --> 01:20:54.600]  это вот Google Chabi,
[01:20:54.600 --> 01:20:56.600]  то в API Google Chabi у вас есть
[01:20:56.600 --> 01:20:58.600]  помимо acquire и release,
[01:20:58.600 --> 01:21:00.600]  то есть взять себе отпускание блокировки,
[01:21:00.600 --> 01:21:02.600]  у вас есть метод getSequencer,
[01:21:02.600 --> 01:21:04.600]  который возвращает вам
[01:21:04.600 --> 01:21:06.600]  некоторый порядковый номер ваш.
[01:21:06.600 --> 01:21:08.600]  Ваш порядковый номер относительно других
[01:21:08.600 --> 01:21:10.600]  критических секций того же Mutex.
[01:21:10.600 --> 01:21:12.600]  Ну а в случае с зукипером
[01:21:12.600 --> 01:21:14.600]  все еще проще,
[01:21:14.600 --> 01:21:16.600]  потому что у вас есть
[01:21:18.600 --> 01:21:20.600]  потому что
[01:21:22.600 --> 01:21:24.600]  у вас есть
[01:21:24.600 --> 01:21:26.600]  стат
[01:21:28.600 --> 01:21:30.600]  и через этот стат
[01:21:30.600 --> 01:21:32.600]  из узла можно получить
[01:21:32.600 --> 01:21:34.600]  его версию, его глобальный порядковый номер
[01:21:36.600 --> 01:21:38.600]  за xid.
[01:21:40.600 --> 01:21:42.600]  В общем, вы с самого зукипера можете
[01:21:42.600 --> 01:21:44.600]  порядковый номер блокировки изъять
[01:21:44.600 --> 01:21:46.600]  и пользоваться им для того,
[01:21:46.600 --> 01:21:48.600]  чтобы сообщить вот этой самой
[01:21:48.600 --> 01:21:50.600]  третьей системе
[01:21:50.600 --> 01:21:52.600]  о том, что вы
[01:21:52.600 --> 01:21:54.600]  первой
[01:21:54.600 --> 01:21:56.600]  обладателем Mutex.
[01:21:56.600 --> 01:21:58.600]  Если система поймет, что к ней уже приходил второй,
[01:21:58.600 --> 01:22:00.600]  то она ваш запрос проигнорирует.
[01:22:00.600 --> 01:22:02.600]  Вот такое упорядочивание
[01:22:02.600 --> 01:22:04.600]  этих самых блокировок,
[01:22:04.600 --> 01:22:06.600]  это называется fencing.
[01:22:06.600 --> 01:22:08.600]  Ну вот зря я картинку
[01:22:08.600 --> 01:22:10.600]  не промотал, она здесь была.
[01:22:10.600 --> 01:22:12.600]  Вот это называется здесь токен. То есть вы берете блокировку
[01:22:12.600 --> 01:22:14.600]  и получаете токен. И каждый
[01:22:14.600 --> 01:22:16.600]  следующий клиент,
[01:22:16.600 --> 01:22:18.600]  который получил блокировку, получает себе
[01:22:18.600 --> 01:22:20.600]  больше токен. И система,
[01:22:20.600 --> 01:22:22.600]  просто запоминая, наибольший токен, который она видела, может
[01:22:22.600 --> 01:22:24.600]  отвергнуть
[01:22:24.600 --> 01:22:26.600]  более старый токен.
[01:22:26.600 --> 01:22:28.600]  Но если вы вдруг знаете Rafter, Mutex,
[01:22:28.600 --> 01:22:30.600]  Axis или что-нибудь про консенсоры,
[01:22:30.600 --> 01:22:32.600]  то идея знакомая. И в общем, тут попытка
[01:22:32.600 --> 01:22:34.600]  вынести эту логику
[01:22:34.600 --> 01:22:36.600]  из вашего кода в
[01:22:36.600 --> 01:22:38.600]  внешнюю систему.
[01:22:38.600 --> 01:22:40.600]  И такая идея она используется и в большем
[01:22:40.600 --> 01:22:42.600]  масштабе. Все-таки в зукипер
[01:22:42.600 --> 01:22:44.600]  данные и блокировки
[01:22:44.600 --> 01:22:46.600]  положить нельзя, потому что данных много.
[01:22:46.600 --> 01:22:48.600]  Но вот если вы пишете систему с чистого
[01:22:48.600 --> 01:22:50.600]  перистата, вот в Янекс есть ЯнексDB,
[01:22:50.600 --> 01:22:52.600]  и там архитектура такая.
[01:22:52.600 --> 01:22:54.600]  Есть подсистема
[01:22:54.600 --> 01:22:56.600]  хранения, называется там Distribute Storage,
[01:22:56.600 --> 01:22:58.600]  Block Storage. И у нее
[01:22:58.600 --> 01:23:00.600]  API такое. С одной стороны, она хранит
[01:23:00.600 --> 01:23:02.600]  неимоверное количество данных,
[01:23:02.600 --> 01:23:04.600]  любое количество. Она там десятки,
[01:23:04.600 --> 01:23:06.600]  сотни петабайт. А с другой стороны, она
[01:23:06.600 --> 01:23:08.600]  внутри реализует консенсус и
[01:23:08.600 --> 01:23:10.600]  выдает узлам
[01:23:10.600 --> 01:23:12.600]  поколения.
[01:23:12.600 --> 01:23:14.600]  Так что, если вы заранее
[01:23:14.600 --> 01:23:16.600]  позаботились об этой проблеме,
[01:23:16.600 --> 01:23:18.600]  то вы можете
[01:23:18.600 --> 01:23:20.600]  избежать,
[01:23:20.600 --> 01:23:22.600]  аккуратно побороть
[01:23:22.600 --> 01:23:24.600]  проблему
[01:23:24.600 --> 01:23:26.600]  отсутствия взаимного исключения
[01:23:26.600 --> 01:23:28.600]  в распределенных блокировках.
[01:23:28.600 --> 01:23:30.600]  Да, действительно,
[01:23:30.600 --> 01:23:32.600]  два узла могут
[01:23:32.600 --> 01:23:34.600]  в определенной системе думать, что не владеют блокировкой,
[01:23:34.600 --> 01:23:36.600]  это неизбежно.
[01:23:36.600 --> 01:23:38.600]  Но в таком случае можно
[01:23:38.600 --> 01:23:40.600]  по крайней мере защитить,
[01:23:40.600 --> 01:23:42.600]  можно по крайней мере заблокировать
[01:23:42.600 --> 01:23:44.600]  старый узел, который на самом деле,
[01:23:44.600 --> 01:23:46.600]  блокировка которого уже устарела,
[01:23:46.600 --> 01:23:48.600]  не дать ему модифицировать
[01:23:48.600 --> 01:23:50.600]  какое-то состояние.
[01:23:50.600 --> 01:23:52.600]  То есть, локальное понимание,
[01:23:52.600 --> 01:23:54.600]  локальную веру, что я узел, владею блокировкой,
[01:23:54.600 --> 01:23:56.600]  мы победить не можем,
[01:23:56.600 --> 01:23:58.600]  это просто
[01:23:58.600 --> 01:24:00.600]  свойство распределенности,
[01:24:00.600 --> 01:24:02.600]  с которым мы должны смириться.
[01:24:02.600 --> 01:24:04.600]  Но мы можем заблокировать модификацию
[01:24:04.600 --> 01:24:06.600]  мутации вот такого узла
[01:24:08.600 --> 01:24:10.600]  с помощью токенов поколений,
[01:24:10.600 --> 01:24:12.600]  как они там называются, по-разному.
[01:24:14.600 --> 01:24:16.600]  Ну что,
[01:24:16.600 --> 01:24:18.600]  это, наверное,
[01:24:18.600 --> 01:24:20.600]  все, что можно за ограниченное время
[01:24:20.600 --> 01:24:22.600]  про зукипер же сказать.
[01:24:22.600 --> 01:24:24.600]  Если у вас вопросы есть,
[01:24:24.600 --> 01:24:26.600]  то давайте мы их обсудим.
[01:24:30.600 --> 01:24:32.600]  Маленький анонс,
[01:24:32.600 --> 01:24:34.600]  когда мы будем говорить про кавку,
[01:24:34.600 --> 01:24:36.600]  то в кавке зукипер
[01:24:36.600 --> 01:24:38.600]  является как раз
[01:24:38.600 --> 01:24:40.600]  вот тем самым консенсусом, как сервис.
[01:24:40.600 --> 01:24:42.600]  То есть, это компонент, который
[01:24:42.600 --> 01:24:44.600]  гарантирует, что данные
[01:24:44.600 --> 01:24:46.600]  обновляются в кавке согласованно,
[01:24:46.600 --> 01:24:48.600]  что они упорядочиваются
[01:24:48.600 --> 01:24:50.600]  и там ничего не теряется.
[01:24:50.600 --> 01:24:52.600]  Вот когда мы будем говорить про кавку, мы увидим,
[01:24:52.600 --> 01:24:54.600]  как зукипер там используется.
[01:24:56.600 --> 01:24:58.600]  Применение зукипера
[01:24:58.600 --> 01:25:00.600]  по назначению ПиМО, то есть
[01:25:00.600 --> 01:25:02.600]  консенсус как сервис.
[01:25:12.600 --> 01:25:14.600]  Попросы?
[01:25:14.600 --> 01:25:16.600]  Пожелания?
[01:25:16.600 --> 01:25:18.600]  Что-нибудь?
[01:25:20.600 --> 01:25:22.600]  Я не знаю, будет ли у вас
[01:25:22.600 --> 01:25:24.600]  семинар по зукеперу. Если
[01:25:24.600 --> 01:25:26.600]  будет, то там можно
[01:25:26.600 --> 01:25:28.600]  обсудить очень простую задачу,
[01:25:28.600 --> 01:25:30.600]  которая внутри себя комбинирует
[01:25:30.600 --> 01:25:32.600]  все, что мы сегодня
[01:25:32.600 --> 01:25:34.600]  обсудили. Вот задача,
[01:25:34.600 --> 01:25:36.600]  где можно собрать все вместе
[01:25:36.600 --> 01:25:38.600]  и детектор сбоев,
[01:25:38.600 --> 01:25:40.600]  и конфигурации, наверное, можно
[01:25:40.600 --> 01:25:42.600]  собрать и взаимные
[01:25:42.600 --> 01:25:44.600]  исключения, и еще что-нибудь. А именно
[01:25:44.600 --> 01:25:46.600]  задача такая – сделать сервис, который
[01:25:46.600 --> 01:25:48.600]  очередь задач,
[01:25:48.600 --> 01:25:50.600]  тредпул распределенный,
[01:25:50.600 --> 01:25:52.600]  у вас есть пул машин,
[01:25:52.600 --> 01:25:54.600]  они могут, разумеется, отказывать,
[01:25:54.600 --> 01:25:56.600]  и есть клиенты,
[01:25:56.600 --> 01:25:58.600]  которые в этот пул бросают
[01:25:58.600 --> 01:26:00.600]  задачи. Ну, то есть вот какие-то, не знаю,
[01:26:00.600 --> 01:26:02.600]  маленькие башискрипты,
[01:26:02.600 --> 01:26:04.600]  которые должны запуститься
[01:26:04.600 --> 01:26:06.600]  на любой машине, что-то сделать, и вот
[01:26:06.600 --> 01:26:08.600]  неизвестно что, просто вот
[01:26:08.600 --> 01:26:10.600]  какие-то команды, которые мы можем системам
[01:26:10.600 --> 01:26:12.600]  помещать. Вот задача в том, чтобы
[01:26:12.600 --> 01:26:14.600]  эти задачи
[01:26:14.600 --> 01:26:16.600]  исполнялись распределенно параллельно
[01:26:16.600 --> 01:26:18.600]  на разных машинах, и чтобы
[01:26:18.600 --> 01:26:20.600]  каждая задача, которую клиент отправил в систему,
[01:26:20.600 --> 01:26:22.600]  исполнилось, ну, по крайней мере, один раз.
[01:26:24.600 --> 01:26:26.600]  Тут уже можно разные дизайны придумывать,
[01:26:26.600 --> 01:26:28.600]  более или менее эффективные, но вот
[01:26:28.600 --> 01:26:30.600]  хороший дизайн, он сочетает
[01:26:30.600 --> 01:26:32.600]  примерно все техники, которые мы сегодня
[01:26:32.600 --> 01:26:34.600]  обсудили.
[01:26:38.600 --> 01:26:40.600]  Ну что, если вопросов больше нет,
[01:26:40.600 --> 01:26:42.600]  то спасибо, что зашли.
[01:26:42.600 --> 01:26:44.600]  Заходите в следующий раз, мы поговорим
[01:26:44.600 --> 01:26:46.600]  либо про Кассандру, либо про Кавку.

[00:00.000 --> 00:10.520]  Коллег, один из студентов, я бы хотел вам его задать.
[00:10.520 --> 00:14.720]  Я предлагаю это даже на запись уже включить, все, скринкаст
[00:14.720 --> 00:18.280]  запись экрана тоже пошла, 3-2 раз, 1-2-3, потому что вопрос
[00:18.280 --> 00:19.280]  достаточно актуальный.
[00:19.280 --> 00:24.080]  А именно, вот есть у вас лабораторная работа, там как раз есть
[00:24.080 --> 00:27.080]  задачка применить PCA, куда-то все спроецировать, и тут
[00:27.080 --> 00:30.400]  скорее вопрос не про лабу, а про PCA.
[00:30.400 --> 00:32.560]  Если у вас нет главных компонентов, применяем
[00:32.560 --> 00:35.680]  к самой выборке, к самому датсету.
[00:35.680 --> 00:36.680]  Внимание, вопрос.
[00:36.680 --> 00:40.320]  Вот мы взяли с вами PCA, взяли датсет, и выбрали
[00:40.320 --> 00:43.480]  количество главных компонентов, равное количеству признаков
[00:43.480 --> 00:44.480]  в нашем датсете.
[00:44.480 --> 00:48.400]  Условно у нас была 10-мерная выборка, 10 признаков, мы
[00:48.400 --> 00:52.520]  взяли 10 главных компонентов, применили, PCA соответственно
[00:52.520 --> 00:55.520]  выучили все главные компоненты, на них отобразились.
[00:55.600 --> 00:57.840]  Получили ли мы тождественное преобразование или нет?
[00:57.840 --> 00:58.840]  Нет.
[00:58.840 --> 01:03.320]  Кто за да, кто за нет, подумайте, пожалуйста.
[01:03.320 --> 01:12.600]  Вопрос, есть ли метод главных компонентов, в качестве
[01:12.600 --> 01:15.920]  главных компонентов используют столько же главных компонентов,
[01:15.920 --> 01:20.160]  сколько у нас координатных осей в байсе пространства?
[01:20.160 --> 01:21.920]  Получим ли мы тождественное преобразование или нет?
[01:21.920 --> 01:31.120]  Ну вот, коллеги с правой части для меня зала считают,
[01:31.120 --> 01:32.120]  что нет.
[01:32.120 --> 01:34.520]  Коллеги с левой части зала.
[01:34.520 --> 01:46.280]  Ну да, домножаем, вопрос, эта матричка будет единична
[01:46.280 --> 01:48.800]  или нет?
[01:48.800 --> 01:49.800]  Хороший вопрос.
[01:50.040 --> 01:55.200]  Ну давайте, кто за то, что да, оно тождественное?
[01:55.200 --> 01:57.800]  Даже те, кто спрашивали, говорят, что уже нет.
[01:57.800 --> 02:00.800]  Кто за то, что нет?
[02:00.800 --> 02:03.480]  Большая часть аудитории воздержал, я понял.
[02:03.480 --> 02:06.720]  Ну хорошо, давайте те, кто за то, что нет, дадут какое-нибудь
[02:06.720 --> 02:07.720]  обоснование.
[02:07.720 --> 02:08.720]  Ну пожалуйста, вот вы говорили в начале.
[02:08.720 --> 02:15.680]  Ага, классно, пример с овальчиком, работает.
[02:15.680 --> 02:18.280]  Если вы эллипс нарисуете, у вас первая главная компонента
[02:18.280 --> 02:21.360]  будет направлена вдоль первой главной полуаси.
[02:21.360 --> 02:24.080]  Вообще не обязательно, что она совпадает с осью
[02:24.080 --> 02:25.080]  координат.
[02:25.080 --> 02:26.440]  Теперь как это обосновать в общем случае?
[02:26.440 --> 02:29.360]  У вас главные компоненты – это направление наибольшей
[02:29.360 --> 02:32.800]  дисперсии по убыванию, если вы их отсортируете.
[02:32.800 --> 02:35.560]  Соответственно, у вас направление наибольшей дисперсии вовсе
[02:35.560 --> 02:39.000]  не обязательно координатной оси вашего пространства,
[02:39.000 --> 02:40.680]  тем более в том же порядке.
[02:40.680 --> 02:43.480]  Поэтому, конечно же, в общем случае PCA все равно меняет
[02:43.480 --> 02:45.560]  ваше пространство признаковое.
[02:45.560 --> 02:47.400]  По сути, вы его поворачиваете.
[02:47.400 --> 02:49.860]  Так что, если вы примените к выборке PCA и сохраните
[02:49.860 --> 02:52.760]  количество компонентов, равным количеству координат
[02:52.760 --> 02:56.000]  осей вашей выборки, вы просто-напросто в другой
[02:56.000 --> 02:58.400]  базе спирейете в том же пространстве.
[02:58.400 --> 03:00.600]  Вы не потеряете ни йода информации, у вас все останется
[03:00.600 --> 03:04.200]  на месте, но тем не менее база у вас будет другой.
[03:04.200 --> 03:05.200]  Данный вопрос понятен?
[03:05.200 --> 03:06.200]  Супер.
[03:06.200 --> 03:07.200]  Хорошо.
[03:07.200 --> 03:10.760]  Еще какие-то вопросы по предыдущим занятиям есть?
[03:10.760 --> 03:13.320]  Мало ли там что-то еще в голове сло, мы можем с вами их
[03:13.320 --> 03:14.320]  обсудить вначале.
[03:14.320 --> 03:25.240]  Три, два, один, ну ладно, тогда предлагаю продолжать.
[03:25.240 --> 03:29.480]  И сегодня мы с вами поговорим о градиентном бустинге,
[03:29.480 --> 03:33.800]  о ужас, о той технике, которая немножко, скажем так, затмила
[03:33.800 --> 03:37.600]  нейронные сети в начале нулевых, в 2001 году был представлен
[03:37.600 --> 03:41.420]  GBM, Grading Boosting Machine, и за счет этого в том числе нейронные
[03:42.260 --> 03:44.980]  сети на 10 лет ушли в подполье и сидели там крайне тихо
[03:44.980 --> 03:45.980]  развивались.
[03:45.980 --> 03:48.860]  Собственно, градиентный бустинг это на данный момент
[03:48.860 --> 03:54.380]  наверное некоторая максимально высокая точка развития
[03:54.380 --> 03:58.020]  классических моделей, в каком смысле, он все еще актуален,
[03:58.020 --> 04:00.860]  все еще широко применим, все еще крайне любим и
[04:00.860 --> 04:04.860]  не сдает позиции нейронным сетям в том смысле, в смысле
[04:04.860 --> 04:06.660]  подходу, где мы используем какие-то дифференцируемые
[04:06.660 --> 04:09.220]  преобразования, каскадом их выстраиваем и так далее.
[04:09.380 --> 04:12.100]  То, что нейронные сети и линии на модели суть примерно
[04:12.100 --> 04:16.340]  одно и то же, можем их отнести примерно в одно семейство.
[04:16.340 --> 04:18.580]  Отдельно могут стоять метрические алгоритмы, тот же самый
[04:18.580 --> 04:21.740]  KNN и так далее, отдельно может стоять дерево и их
[04:21.740 --> 04:22.740]  ансамбли.
[04:22.740 --> 04:25.060]  Сегодня мы с вами поговорим про ансамбли деревьев, про
[04:25.060 --> 04:27.420]  градиентный бустинг, не обязательно на самом деле
[04:27.420 --> 04:29.900]  деревьев, но исторически он используется в основном
[04:29.900 --> 04:32.380]  на неглубоких деревьях, но опять же это исторически
[04:32.380 --> 04:35.020]  сложилось, его можно и с другими моделями использовать.
[04:35.020 --> 04:38.660]  И сегодня у нас в меню 4 основных шага.
[04:39.220 --> 04:41.900]  Первое, мы с вами постараемся интуитивно понять, что такое
[04:41.900 --> 04:43.780]  бустинг и как он вообще работает.
[04:43.780 --> 04:47.500]  Второе, мы с вами попытаемся математически объяснить,
[04:47.500 --> 04:49.940]  что происходит и как мы можем градиентные методы
[04:49.940 --> 04:54.020]  использовать вроде как деревьями, которые недеференцируемы.
[04:54.020 --> 04:56.220]  Ну а дальше поговорим еще чуть-чуть про другие техники
[04:56.220 --> 04:59.140]  ансамблирования, будь то стекинг, блендинг и про
[04:59.140 --> 05:00.140]  что-нибудь еще.
[05:00.140 --> 05:02.460]  Вопросы сегодня приветствуются, в какой-то момент придется
[05:02.460 --> 05:06.060]  чуть-чуть расширить сознание и понять, что такое градиентный
[05:06.060 --> 05:08.900]  спуск в пространстве моделей, если так грубо
[05:08.900 --> 05:09.900]  говорить.
[05:09.900 --> 05:11.580]  Если у вас был уже функкан, то вам, наверное, будет
[05:11.580 --> 05:13.940]  чуть проще, если у вас его не было, вам все равно
[05:13.940 --> 05:14.940]  будет нормально.
[05:14.940 --> 05:17.300]  Функкан нам сильно не понадобится, но понимать, что у нас это
[05:17.300 --> 05:19.420]  выражение в разных местах работает полезно.
[05:19.420 --> 05:22.940]  Но перед этим давайте коротенько вспомним, что было на
[05:22.940 --> 05:26.300]  прошлом занятии, конкретно про random forest поговорим,
[05:26.300 --> 05:28.940]  про случайный лес и про ансамблирование деревьев
[05:28.940 --> 05:29.940]  в лоб.
[05:29.940 --> 05:32.820]  Вот у вас есть куча деревьев, вы взяли их, усреднили.
[05:32.820 --> 05:34.820]  Что можно сказать первое?
[05:34.820 --> 05:36.700]  Во-первых, все деревья были построены независимо
[05:36.700 --> 05:37.700]  друг от друга.
[05:37.700 --> 05:38.700]  Правильно?
[05:38.700 --> 05:40.660]  Вы можете одно строить на одной машине, другое
[05:40.660 --> 05:44.260]  на другой, на разных датсетах, на разных признаках, подможествах.
[05:44.260 --> 05:45.260]  Они все независимы.
[05:45.260 --> 05:49.740]  Но тут, если вспомнить опять же, не знаю, там, электричество,
[05:49.740 --> 05:52.140]  электродинамика, то же самое, то у нас есть последовательная
[05:52.140 --> 05:55.460]  сеть, какие-то вот элементы последовательные и параллельные.
[05:55.460 --> 05:58.460]  Вот здесь мы явно все строим параллельно, поэтому оно
[05:58.460 --> 05:59.460]  независимо друг от друга.
[05:59.460 --> 06:01.620]  Можете сразу напроситься вопросом, а что если строить
[06:01.620 --> 06:02.620]  последовательно?
[06:02.620 --> 06:04.260]  Это мы как раз к бусинку и подойдем.
[06:04.260 --> 06:06.900]  Во-вторых, какие основные свойства у Рэндом Фореста
[06:06.900 --> 06:10.420]  и ему же подобных бэггинг-моделей, но Рэндом Форест, наверное,
[06:10.420 --> 06:13.780]  как некоторая высшая ступень развития этого подхода.
[06:13.780 --> 06:15.180]  Давайте еще раз повторим.
[06:15.180 --> 06:17.780]  Во-первых, деревья сами по себе хорошо работают
[06:17.780 --> 06:20.900]  с пропусками раз и со скоррелированными признаками два.
[06:20.900 --> 06:22.980]  Рэндом Форест это все у них перенимает, потому
[06:22.980 --> 06:25.300]  что скоррелированные признаки абсолютно по барабану мы
[06:25.300 --> 06:27.660]  их раздельно рассматриваем.
[06:27.660 --> 06:28.660]  Пропуски в данных.
[06:28.660 --> 06:31.260]  Если есть пропуск, то мы используем оба поддерева,
[06:31.260 --> 06:33.340]  потом усредняем ответ в зависимости от мощности
[06:33.340 --> 06:36.420]  левого и правого поддерева в смысле объема выборки,
[06:36.420 --> 06:39.140]  который туда и сюда пошел на этапе обучения.
[06:39.140 --> 06:42.340]  И в-третьих, мы можем использовать out-of-back оценку для того,
[06:42.340 --> 06:45.980]  чтобы получать оценку на не увиденных ранее данных.
[06:45.980 --> 06:50.260]  Короче, валитационная выборка бесплатна, нам не нужно
[06:50.260 --> 06:53.060]  делить наши данные на две части.
[06:53.060 --> 06:55.340]  Тогда мы получаем с вами оценку сверху на ошибку
[06:55.340 --> 06:57.420]  модели, потому что если мы работаем out-of-back, у нас
[06:57.420 --> 06:59.140]  лишь часть ансамбля работает.
[06:59.140 --> 07:01.940]  Вот вы здесь можете видеть, что мы все модели, у которых
[07:01.940 --> 07:05.180]  данный объект не попал в обучающую выборку используемую.
[07:05.180 --> 07:07.460]  Соответственно, размер ансамбля меньше, эффективность
[07:07.460 --> 07:10.940]  ранним фореста меньше, поэтому качество тоже ниже.
[07:10.940 --> 07:11.940]  Ошибка выше.
[07:11.940 --> 07:14.940]  Ну и также я говорил коротко в конце занятия, что мы с
[07:14.940 --> 07:18.620]  вами можем использовать другие версии вот этого
[07:18.620 --> 07:20.020]  самого случайного леса.
[07:20.020 --> 07:23.660]  Extremely randomized trees, там мы вообще доводим до абсурда
[07:23.660 --> 07:25.500]  случайность, если у нас краски сильно скоррелированы
[07:25.500 --> 07:26.500]  и множество.
[07:26.500 --> 07:30.620]  Или Isolation forest, Isolation forest — это техника поиска аномалий,
[07:30.620 --> 07:31.620]  как я уже говорил.
[07:31.620 --> 07:33.260]  Повторить про нее еще раз, или все уже все помнят
[07:33.260 --> 07:34.260]  и движемся дальше.
[07:34.260 --> 07:35.260]  Повторить.
[07:35.260 --> 07:39.860]  Ну хорошо, давайте тогда еще раз, тогда это было в
[07:39.860 --> 07:40.860]  конце занятия.
[07:40.860 --> 07:41.860]  Еще раз формулируем.
[07:41.860 --> 07:44.580]  Что такое вот аномалия или же выброс с точки зрения
[07:44.580 --> 07:46.500]  именно признаков описания, не будем пока смотреть
[07:46.500 --> 07:47.500]  на таргеты вообще.
[07:47.500 --> 07:50.180]  Вот у вас есть множество точек, какие-то из них какие-то
[07:50.180 --> 07:52.340]  аномальные, какие-то нестандартные.
[07:52.340 --> 07:54.700]  Вы можете какие-то свойства их описать самостоятельно,
[07:54.700 --> 07:55.700]  не глядя на выборку.
[07:55.700 --> 08:01.540]  Ну кто-нибудь, давайте, не смейте.
[08:01.540 --> 08:11.180]  Легко от других точек, классно, что-нибудь еще?
[08:11.180 --> 08:12.180]  Легко отделить.
[08:12.180 --> 08:13.180]  Хорошо.
[08:13.180 --> 08:15.540]  Ну по сути, если мы с вами нарисуем какое-то наше
[08:15.540 --> 08:18.940]  множество, мы с вами краски заметим, что у нас наши данные
[08:18.940 --> 08:22.700]  могут образовывать какую-то там достаточно плотную
[08:22.700 --> 08:25.060]  группу, причем она вообще не обязательно какая-то
[08:25.060 --> 08:27.180]  правильная условно, у нас может быть просто какое-то
[08:27.180 --> 08:29.460]  облако точек круглое, а может быть какая-нибудь
[08:30.460 --> 08:34.940]  непонятной формы, вот бука ФКС, и внутри у него у нас
[08:34.940 --> 08:35.940]  накиданы точки.
[08:35.940 --> 08:39.940]  И, допустим, вот эта точка, она в среднем-то не так
[08:39.940 --> 08:41.940]  уж далеко до всех, но она будет выбраться от того,
[08:41.940 --> 08:44.340]  что у нас плотность как-то вот таким образом распределена,
[08:44.340 --> 08:47.860]  а здесь у нас краски низкая плотность объектов, а туда
[08:47.860 --> 08:48.860]  кто-то попал.
[08:48.860 --> 08:49.860]  Вот все.
[08:49.860 --> 08:52.940]  Соответственно выборку мы можем, точнее не выборку,
[08:52.940 --> 08:56.220]  а выбросами или аномалиями мы можем называть те точки,
[08:56.220 --> 08:59.300]  которые визуально выбиваются из вот этого нашего общего
[08:59.940 --> 09:02.140]  распределения, из генералит с вакуумности, откуда все
[09:02.140 --> 09:03.140]  пришло.
[09:03.140 --> 09:05.260]  Грубо говоря, если вы нарисуете многомерную функцию плотности,
[09:05.260 --> 09:06.980]  то они краски будут торчать там, где у вас плотность
[09:06.980 --> 09:08.100]  вашего распределения очень низкая.
[09:08.100 --> 09:12.780]  Ну, с простейшим случаем для гауссовского распределения,
[09:12.780 --> 09:14.700]  например, слева-справа, вот там, не знаю, за двумя
[09:14.700 --> 09:16.740]  с половиной сигмами, можете считать, что это у вас какие-то
[09:16.740 --> 09:19.380]  аномалии, так обычно и поступают в статистике.
[09:19.380 --> 09:21.900]  Типа, если у нас там три сигма, это вот почти, наверное,
[09:21.900 --> 09:22.900]  все хорошо.
[09:22.900 --> 09:26.900]  Там 99,7 нам уже достаточно, в принципе, из 100.
[09:26.900 --> 09:29.820]  Так вот, isolation forest работает ровно на этом принципе.
[09:29.820 --> 09:32.460]  Давайте попытаемся тогда отделить наши объекты от
[09:32.460 --> 09:35.380]  всего остального и попробуем это сделать наименьшим
[09:35.380 --> 09:36.860]  числом разделений.
[09:36.860 --> 09:39.780]  Тогда мы можем это с вами делать каким образом?
[09:39.780 --> 09:45.940]  Ну, вот чтобы эту точку отделить, можем там 1, 2, 3, 4, все, условно
[09:45.940 --> 09:46.940]  как-то так.
[09:46.940 --> 09:51.660]  Но при этом, если вы попытаетесь отделить ее с самого начала,
[09:51.660 --> 09:53.420]  вас, наверное, вызовет какие-то проблемы, гораздо
[09:53.420 --> 09:57.180]  проще будет выделить вот такую точку, ее вообще
[09:57.180 --> 10:01.980]  там можно одним ихом отделить и так далее.
[10:01.980 --> 10:04.520]  А те точки, которые сидят здесь внутри, их будет отделять
[10:04.520 --> 10:06.940]  гораздо сложнее, потому что их нужно вообще прям
[10:06.940 --> 10:10.700]  очень точно от всех остальных отводить и так далее.
[10:10.700 --> 10:12.340]  Понятная идея, правильно?
[10:12.340 --> 10:15.540]  То есть, чем меньше точка похожа на другие, тем меньше
[10:15.540 --> 10:18.500]  нам в среднем нужно шагов, чтобы ее откинуть от остальных.
[10:18.500 --> 10:20.820]  Плюс там точно также используется идея того, что мы используем
[10:20.820 --> 10:24.420]  случайно признаками подможества и какие-то там буддстрапированные
[10:24.420 --> 10:25.900]  выборки используем и так далее, чтобы деревья были
[10:25.900 --> 10:26.900]  не похожи.
[10:26.900 --> 10:28.900]  Соответственно, тогда мы имеем для каждой точки,
[10:28.900 --> 10:31.300]  мы строим дерево, например, до какой-то глубины, для
[10:31.300 --> 10:34.620]  каждой точки мы имеем глубину, на которую мы ее выделили
[10:34.620 --> 10:36.860]  в каждом дереве, или что мы ее вообще не выделили
[10:36.860 --> 10:37.860]  отдельно.
[10:37.860 --> 10:39.460]  И тогда вот скор для каждой точки, это что?
[10:39.460 --> 10:41.660]  Это средняя глубина, на которой она была отделена.
[10:41.660 --> 10:43.860]  Тогда у вас получится, если отранжировать точки вот
[10:43.860 --> 10:46.780]  по этой средней глубине, получится кто-то сидит слева,
[10:46.780 --> 10:48.540]  все остальные где-то там справа или вообще делены
[10:48.540 --> 10:49.540]  не были.
[10:49.540 --> 10:51.220]  А те, кто сидят слева, мы говорим, ну хорошо, это
[10:51.220 --> 10:52.220]  вроде как аномалия.
[10:52.220 --> 10:55.020]  Понятное дело, это тоже не всегда хорошо подходит
[10:55.020 --> 10:56.020]  и так далее.
[10:56.020 --> 11:00.140]  Ну представь себе, я не знаю, там завернутую, вот представь
[11:00.140 --> 11:03.180]  себе, что у вас все точки лежат на эдаком многообразии,
[11:03.180 --> 11:06.980]  вот у вас есть двумерная плоскость, там все точки,
[11:06.980 --> 11:09.180]  потом вы взяли и ее свернули в рулу, вот коврик для йоги
[11:09.180 --> 11:10.180]  мы так сворачиваем.
[11:10.180 --> 11:12.380]  Теперь это все в сверхмерном пространстве.
[11:12.380 --> 11:14.420]  И теперь там все точки, по сути, лежат вот на этой
[11:14.420 --> 11:16.820]  закрученной поверхности, а некоторые между слоями
[11:16.820 --> 11:17.820]  лежат.
[11:17.980 --> 11:20.220]  В точке зрения расстояния до стальных расстояние
[11:20.220 --> 11:21.220]  будет маленькое.
[11:21.220 --> 11:23.540]  В точке зрения отделения тем же самым изволоченным
[11:23.540 --> 11:26.020]  форстом проблем будет много.
[11:26.020 --> 11:28.260]  Тем не менее, это тоже точки являются аномалиями, они
[11:28.260 --> 11:30.660]  как раз не лежат на вот этой общей гиперповерхности.
[11:30.660 --> 11:33.660]  Это нормально, этот метод не универсальный и он не
[11:33.660 --> 11:34.660]  всегда подходит.
[11:34.660 --> 11:36.700]  Просто чтобы вы понимали, это некая церебряная пуля,
[11:36.700 --> 11:38.860]  что вот мы ее запустили и все работает.
[11:38.860 --> 11:41.980]  Но с этим вот сложным многообразием вообще бывают проблемы.
[11:41.980 --> 11:42.980]  Тут вопросы есть?
[11:45.300 --> 11:47.020]  Три, два, один.
[11:47.100 --> 11:48.100]  Хорошо.
[11:48.100 --> 11:50.140]  Ну и в целом, если у вас есть какое-то желание работать
[11:50.140 --> 11:54.020]  с данными, я надеюсь, у вас оно есть, раз вы ходите
[11:54.020 --> 11:56.980]  даже очень на лекции, то изволоченный, ой, изволоченный,
[11:56.980 --> 11:59.580]  random forest это классный такой подход, который можно в
[11:59.580 --> 12:01.900]  копилку себе положить и всегда его использовать.
[12:01.900 --> 12:02.900]  Одно маленькое но.
[12:02.900 --> 12:07.420]  Пожалуйста, будьте осторожны, если вы пытаетесь его использовать
[12:07.420 --> 12:11.340]  а, с данными, которые упорядочены во времени, потому что вам
[12:11.340 --> 12:14.780]  нужно все-таки четко понимать, что у вас валидация работает
[12:14.780 --> 12:15.780]  как.
[12:15.860 --> 12:17.820]  Харехно с точки зрения течения времени, потому
[12:17.820 --> 12:20.060]  что вот эта ваша аутов бэк оценка может казаться
[12:20.060 --> 12:22.980]  так, что вы оцениваете, по сути, на прошлых точках,
[12:22.980 --> 12:26.100]  а лес обучался на будущем, если у вас точки упорядочены
[12:26.100 --> 12:27.100]  во времени.
[12:27.100 --> 12:30.740]  Это не очень хорошо, потому что у вас распределение
[12:30.740 --> 12:34.540]  х новый, у новый при условии х, у предыдущего.
[12:34.540 --> 12:36.820]  Если вы пытаетесь построить наоборот х, у предыдущий
[12:36.820 --> 12:39.460]  при условии новых, вы по сути что-то другое делаете,
[12:39.460 --> 12:42.540]  вы нарушаете ход событий и ваша модель учится чему-то
[12:42.540 --> 12:43.540]  не тому.
[12:43.540 --> 12:45.500]  Поэтому, пожалуйста, всегда, когда данные упорядочены,
[12:45.500 --> 12:48.380]  будьте очень осторожны с этими всеми случайными
[12:48.380 --> 12:50.060]  подвыбреками и так далее.
[12:50.060 --> 12:51.060]  Тут вопрос есть?
[12:51.060 --> 12:56.260]  Все понятно, все просто, едем дальше, правильно?
[12:56.260 --> 12:58.660]  Или ничего не понятно, ничего не просто спасите, помогите.
[12:58.660 --> 13:11.340]  Вы про изволейшн форс сейчас или про кого?
[13:11.340 --> 13:12.340]  Вот про эту штуку.
[13:12.340 --> 13:17.700]  Сейчас, я вопрос не понял, к сожалению.
[13:17.700 --> 13:29.180]  Не, не, я говорил, что изволейшн форс работает не всегда,
[13:29.180 --> 13:32.260]  он все равно смотрит на то, что у вас точка находится
[13:32.260 --> 13:34.180]  как-то подаль от всех остальных.
[13:34.180 --> 13:38.380]  Ну, условно доведите вот этот пример до абсурда,
[13:38.380 --> 13:40.060]  заверните эту спираль очень сильно, у вас все лежит
[13:40.060 --> 13:41.060]  на спирали.
[13:41.660 --> 13:43.860]  А какие-то точки лежат не на спирали, а между слоев
[13:43.860 --> 13:44.860]  спирали.
[13:44.860 --> 13:46.900]  В точке зрения какой-то закономерности, они выбиваются
[13:46.900 --> 13:47.900]  из этой закономерности.
[13:47.900 --> 13:50.940]  В точке зрения расстояний, они все равно рядом со всеми.
[13:50.940 --> 13:53.860]  Поэтому этот мет просто не сможет их отделить.
[13:53.860 --> 13:58.940]  Это нормально, он не рассчитан на все, он в среднем работает
[13:58.940 --> 13:59.940]  неплохо.
[13:59.940 --> 14:00.940]  Половили?
[14:00.940 --> 14:04.300]  Так, больше вопросов нет, едем дальше.
[14:04.300 --> 14:08.860]  Ребят, вы какие-то скучающие, вы меня пугаете, у вас уже
[14:08.860 --> 14:11.660]  все, подкрадывается сессия, в середину семестра все
[14:11.660 --> 14:12.660]  сложно?
[14:12.660 --> 14:15.660]  Или вам просто скучно?
[14:15.660 --> 14:21.500]  Снег не выпал, рано ботать, а чего вы тогда это делаете?
[14:21.500 --> 14:22.500]  Хорошо, ладно.
[14:22.500 --> 14:28.220]  Ну и давайте тогда чуть-чуть поднимем градус настроения
[14:28.220 --> 14:29.220]  в аудитории.
[14:29.220 --> 14:32.460]  Мы с вами чуть-чуть говорили про bias-variance, я решил его
[14:32.460 --> 14:37.660]  вынести на отдельный доп-семинар, чтобы мы с вами могли более
[14:37.660 --> 14:38.660]  развернуто поговорить.
[14:38.660 --> 14:41.380]  Да, кстати, на всякий случай онлайн-семинары идут по
[14:41.380 --> 14:43.820]  средам и субботам, все, туда можно ходить, это особенно
[14:43.820 --> 14:47.580]  всем тем, кто в онлайне нас смотрит, приходите, подключайтесь,
[14:47.580 --> 14:50.060]  все ссылки есть в чате, каждый раз они публикуют
[14:50.060 --> 14:51.060]  заранее.
[14:51.060 --> 14:57.140]  Ну, давайте по одному, там будет то же самое, но с маленькими
[14:57.140 --> 14:58.500]  оговорками в каком-то смысле.
[14:58.500 --> 15:02.500]  Материал опорный, то есть ноутбук и тема, они те же самые,
[15:02.500 --> 15:04.900]  но во-первых, там учитывается, что у вас уже есть запись
[15:04.900 --> 15:07.300]  этого семинара и вы можете его посмотреть, то есть там
[15:07.300 --> 15:09.580]  как правило разбираются больше вопросы, которые
[15:09.580 --> 15:14.220]  идут уже от аудитории, потому что они пришли уже после
[15:14.220 --> 15:16.620]  лекции, возможно, люди что-то смотрели, обдумали и так
[15:16.620 --> 15:17.620]  далее.
[15:17.620 --> 15:19.580]  Во-вторых, все равно каждый преподаватель чуть по-своему
[15:19.580 --> 15:22.540]  подает материал, например, Валерий, он больше расписывает
[15:22.540 --> 15:24.740]  все руками на планшете, потому что у него есть такая
[15:24.740 --> 15:26.980]  возможность, потому что у него рядом все книжки,
[15:26.980 --> 15:29.380]  у него рядом планшет, у него есть время это дело,
[15:29.380 --> 15:31.580]  круговоря, заранее вопросы, видите, и так далее.
[15:31.580 --> 15:34.620]  Альбина, я думаю, наоборот, будет делать это все попроще
[15:34.620 --> 15:35.620]  и с красивым визуализажкой.
[15:36.580 --> 15:39.460]  То есть я больше стараюсь захватить теорию и практику,
[15:39.460 --> 15:42.060]  чтобы держать максимальный баланс между тем и другим.
[15:42.060 --> 15:43.980]  То есть там, грубо говоря, больше уклон в одну или в
[15:43.980 --> 15:44.980]  другую сторону.
[15:44.980 --> 15:46.980]  В принципе, можете посмотреть запись, ей будет лучше.
[15:46.980 --> 15:47.980]  Да.
[15:49.980 --> 15:51.980]  Да, мои семинары остаются до конца симметрии.
[15:51.980 --> 15:54.660]  Ну, с оговоркой, что в какой-то из дней я могу оказаться
[15:54.660 --> 15:55.940]  в отъезде, и тогда меня кто-то заменит.
[15:57.940 --> 15:58.940]  Так, хорошо.
[15:58.940 --> 15:59.940]  Ладно.
[15:59.940 --> 16:04.180]  И давайте тогда коротенько поговорим про bias variance.
[16:04.740 --> 16:07.540]  Ладно, давайте про него в конце поговорим, я представлю.
[16:07.540 --> 16:10.220]  Давайте тогда перейдем к бустингу, цель наша сегодняшняя.
[16:10.220 --> 16:11.780]  Тобственно, что такое бустинг?
[16:11.780 --> 16:15.060]  Ну, понятное дело, что бустинг, опять же, от английского
[16:15.060 --> 16:19.220]  boost, как-то усиливать, улучшать, я думаю, вам знакома библиотека
[16:19.220 --> 16:20.780]  boost, правильно?
[16:20.780 --> 16:24.820]  Ну, по крайней мере, те, кто на плюсах писали, и все
[16:24.820 --> 16:27.540]  они знакомы, те знают, что это такое.
[16:27.540 --> 16:28.540]  Вот.
[16:28.540 --> 16:31.260]  Давайте тогда подумаем, как нам перейти от идеи
[16:31.260 --> 16:33.620]  параллельности, где у нас все независимо, к идее
[16:33.700 --> 16:35.940]  последовательности наших моделей, чтобы каждое следующее
[16:35.940 --> 16:38.020]  делало предыдущую лучше.
[16:38.020 --> 16:40.860]  Тогда у нас получается некоторый каскад моделей, которые
[16:40.860 --> 16:43.260]  все вместе работают как единое целое.
[16:43.260 --> 16:45.380]  Прям замечательно.
[16:45.380 --> 16:48.020]  Но возникает вопрос, а как нам обучать каждую следующую
[16:48.020 --> 16:49.020]  модель?
[16:49.020 --> 16:51.340]  То, что в принципе с тем же самым решающим деревом
[16:51.340 --> 16:53.140]  мы с вами можем сказать, что взяли мы решающее
[16:53.140 --> 16:55.700]  дерево, мы его можем просто обучать до упора, оно всю
[16:55.700 --> 16:59.020]  выборку обучающую запомнит, переобучится под нее, ошибок
[16:59.020 --> 17:01.540]  не будет, исправлять нечем.
[17:01.540 --> 17:03.540]  Почему нам нужно строить каскад модель?
[17:03.620 --> 17:05.660]  Также можно заметить, что вот здесь предлагается
[17:05.660 --> 17:08.780]  самая простая версия, просто линейная комбинация моделей.
[17:08.780 --> 17:11.260]  Как вы думаете, если у нас в качестве базовых моделей,
[17:11.260 --> 17:14.020]  вот эти вот, ашитый, аш-1 и так далее, аш-н, будет
[17:14.020 --> 17:15.020]  линейная регрессия?
[17:15.020 --> 17:18.020]  Смысл это вообще какой-то имеет?
[17:18.020 --> 17:21.060]  Не имеет, потому что линейная комбинация линейных отображений
[17:21.060 --> 17:22.540]  есть линейное отображение.
[17:22.540 --> 17:23.540]  Все, конец.
[17:23.540 --> 17:28.540]  А если логистическая регрессия?
[17:28.540 --> 17:29.540]  И сверчок такой на фоне.
[17:29.540 --> 17:34.540]  Да, и что?
[17:34.540 --> 17:37.540]  Зачем?
[17:37.540 --> 17:43.540]  Ну, если я сюда просто линейные модели засуну, тоже нейросеть
[17:43.540 --> 17:44.540]  можно обозвать.
[17:44.540 --> 17:51.540]  Линейное регрессие тоже однослойное нейросеть без нелинейности.
[17:51.540 --> 17:56.540]  Ладно, хорошо, смотрите, тогда в конце к этому вопросу
[17:56.540 --> 17:58.540]  вернемся, если коротко, да, логистические регрессии
[17:58.540 --> 18:00.540]  бусить можно, даже иногда работает.
[18:00.540 --> 18:03.540]  Хорошо, вот вам выборка.
[18:03.540 --> 18:04.540]  Вот вам задачка.
[18:04.540 --> 18:08.540]  Давайте-ка попытаемся ее решить с помощью решающих
[18:08.540 --> 18:11.540]  деревьев, а точнее, решающих деревьев, доведенных до
[18:11.540 --> 18:14.540]  иступления, решающих пней, то есть единых ифов.
[18:14.540 --> 18:16.540]  Каждом модели будет решающий пень.
[18:16.540 --> 18:18.540]  Она говорит, налево или направо, вверх или вниз.
[18:18.540 --> 18:19.540]  Больше она ничего делать не умеет.
[18:19.540 --> 18:22.540]  Как видите, у нас выборка явно не делится ни одним
[18:22.540 --> 18:24.540]  пнем поровну, правильно?
[18:24.540 --> 18:27.540]  Давайте попробуем это сделать последовательностью
[18:27.540 --> 18:28.540]  пней.
[18:28.540 --> 18:30.540]  Ну вот, первый пень мы с вами обучаем.
[18:30.540 --> 18:31.540]  Ладно, он все три показал.
[18:31.540 --> 18:33.540]  Хорошо, первый пень мы с вами обучаем.
[18:33.540 --> 18:36.540]  Отделили правые две точки и сказали, справа все синие.
[18:36.540 --> 18:38.540]  Слева тогда логично, что все красно.
[18:38.540 --> 18:41.540]  Тогда у нас появились синие точки, вот эти три рите,
[18:41.540 --> 18:44.540]  они специально пожирнее нарисованы, на которых ошибка
[18:44.540 --> 18:45.540]  выше.
[18:45.540 --> 18:46.540]  Согласны?
[18:46.540 --> 18:47.540]  То мы можем сделать.
[18:47.540 --> 18:50.540]  Мы можем с вами на каждом шаге перевзвешивать наш
[18:50.540 --> 18:53.540]  обучающий выборку, отдавая больше вес тем объектам,
[18:53.540 --> 18:55.540]  на которых мы совершили ошибку классификации.
[18:55.540 --> 18:58.540]  Например, погрессия абсолютно все та же самая.
[18:58.540 --> 18:59.540]  Давайте показывай классификации.
[18:59.540 --> 19:01.540]  Теперь у нас соответственно, на этих точках, видите,
[19:01.540 --> 19:04.540]  они стали поменьше, вес маленький, потому что у нас
[19:04.540 --> 19:06.540]  ошибка маленькая, а на точках с левой синень instruction
[19:06.540 --> 19:09.540]  вес большой, потому что они были ошибочно классифицированы.
[19:09.540 --> 19:11.540]  Теперь мы строим вторую модель.
[19:11.540 --> 19:14.540]  Бац, теперь все что с левой красное, все, что справа
[19:14.540 --> 19:15.540]  синее.
[19:15.540 --> 19:17.540]  Теперь у нас получается, что вот эти точки получили
[19:17.540 --> 19:20.540]  вес поменьше, эти вес поменьше, потому что они тоже синие,
[19:20.540 --> 19:23.900]  синие, красный вес побольше, потому что их неправильно классифицировали, правильно?
[19:23.900 --> 19:29.500]  Строим третью модель. Соответственно, те, кто сверху синие, те, кто снизу красные. Хорошо,
[19:29.500 --> 19:34.300]  супер. И в итоге у нас получаются вот такие три модели, которые, если с какими-то весами
[19:34.300 --> 19:39.540]  сложить, возникает вопрос, с какими весами, то у нас получится вот такая нелинейная разделяющая
[19:39.540 --> 19:45.620]  поверхность. По сути такая ступенчатая. На самом деле, пример абсолютно игрушечный, но в чем суть?
[19:45.620 --> 19:52.660]  Здесь мы с вами только что увидели, что мы с вами из множества простых моделей, вот у нас три
[19:52.660 --> 19:57.700]  решающих пня, которые могут построить вам только линейную разделяющую поверхность, да более того,
[19:57.700 --> 20:03.740]  она должна быть параллельна оси координатной какой-то. Мы из трех моделей, которые очень простые,
[20:03.740 --> 20:09.180]  получили гораздо более сложную модель. То есть три простых модели объединились в одну более сильную.
[20:09.180 --> 20:18.860]  Логично? Ро-1, Ро-2, Ро-3 это ровно те веса, с которыми мы их объединяем. Откуда их брать,
[20:18.860 --> 20:24.740]  мы сейчас тоже с вами поговорим. Ну мы с вами эти классификаторы, вот у нас есть 1, 2, 3, соответственно.
[20:24.740 --> 20:34.020]  Чего? Ну у вас каждый классификатор предсказывает какую-то чиселку, например, вероятность или там,
[20:34.020 --> 20:39.100]  в общем случае, логит. Вы строите линейную комбинацию вашей модели. Линейная комбинация бывает
[20:39.100 --> 20:47.260]  взвешенной. Вот это веса, с которым вы их добавляете. То есть мы с вами можем из трех получить более
[20:47.260 --> 20:51.580]  сложную модель, но на самом деле гораздо больше, чем из трех. Вы можете 25 штук построить, но тут
[20:51.580 --> 20:57.100]  3 достаточно. И давайте теперь вернемся вот к этой вот красивой картинке. Вы ее видели на третьей
[20:57.100 --> 21:03.220]  лекции, на четвертой лекции. Что у нас здесь нарисовано? У нас здесь нарисованы все вот эти наши
[21:03.220 --> 21:08.460]  верхние оценки на истинную, в кавычках, функция потерь, на ошибку классификации. И тут у нас с вами
[21:08.460 --> 21:14.220]  сидела еще экспоненциальная функция потери. Вот она. Давайте попробуем ей заодно и воспользоваться.
[21:14.220 --> 21:18.660]  Когда мы на самом деле переизобретем алгоритм, который был предложен в 97, что ли, а допустим
[21:18.660 --> 21:24.900]  назывался или 99, и сделаем что? Давайте скажем, что у нас функция потерь с вами экспоненциальна.
[21:24.900 --> 21:31.580]  Е в степени минус маржина. Логично, тогда она у нас справа убывает, нормально слева она растет
[21:31.580 --> 21:36.740]  экспоненциально. Вон она. То есть чем глубже объект оказывается вне своего класса, тем больше у нас
[21:36.740 --> 21:42.260]  ошибка, причем растет экспоненциально. Сразу можно заметить, что так себе результат, экспоненциальный
[21:42.260 --> 21:45.700]  рост вообще не очень хорошее дело. Примерно везде. Все может сломаться, взорваться,
[21:45.700 --> 21:55.140]  вычислительная точность конечная и так далее. Мы сейчас по сути придумаем. Смотрите, мы с вами
[21:55.140 --> 22:01.980]  выбрали пока что простую функцию потерь экспоненциальную. Почему? Сейчас увидите. И для нее как раз-таки попробуем
[22:01.980 --> 22:06.780]  вот это перевзвешивание, переизобрести. А потом, в общем случае, уже просто по стопам Фридмана
[22:06.780 --> 22:14.580]  пройдем, которые как раз-таки... Фридман же. Я надеюсь, Фридман. Я в начале лекции говорил имя,
[22:14.580 --> 22:22.100]  сейчас я уже запутался. Просто есть еще классный автор, в MIT ведет лекции, на ютубе у него прикольный
[22:22.100 --> 22:28.660]  подкаст Lex Friedman. Сын, собственно, выпускника физтеха, если мне не изменяет память. И у него классный
[22:28.660 --> 22:35.940]  подкаст. По-моему, все-таки тоже Фридман. Давайте скажем, что у нас с вами функция потерь экспоненциальная,
[22:35.940 --> 22:42.820]  то есть E в степени минус маржин. Хорошо, тогда у нас есть с вами над этом шаге наш алгоритм. Это
[22:42.820 --> 22:49.620]  ансамбль из предыдущих первого по Т-большого алгоритма. Вот он. И вот наша функция потерь L от Y
[22:49.620 --> 22:56.980]  и предсказание F с крышкой от T. Это что? Экспоненты минус Y на F с крышкой от T. Согласны? Пока просто
[22:56.980 --> 23:03.260]  маржин написали. Супер. То есть мы можем это расписать как экспонента минус Y на сумму вот этих
[23:03.260 --> 23:09.260]  штук. А что мы с вами знаем про экспоненту от суммы? Логично, что если у нас в степени экспонента
[23:09.260 --> 23:15.340]  стоит сумма, мы это можем разделить на произведение экспонентов. Логично. Работаем. И мы отсюда с вами
[23:15.340 --> 23:20.500]  можем совершенно спокойно взять и вытащить последний элемент. Давайте скажем, что мы сейчас находимся
[23:20.500 --> 23:24.980]  над этом шаге, и у нас есть все модели, кроме последней, обученные. То есть у нас уже есть
[23:24.980 --> 23:30.700]  с первой по T-большой M1 модели, которую мы обучили, они отлиты в бетоне, их трогать нельзя. И есть
[23:30.700 --> 23:36.700]  последняя H-T-большая модель, которую мы сейчас должны обучить. Ну, собственно, вот она. И получается,
[23:36.700 --> 23:42.100]  что на шаге T вот это у нас константа. У нас ничего не меняется. И мы должны минимизировать ровно
[23:42.100 --> 23:46.620]  вот эту штуку. Потому что все предыдущие модели уже обучены в ансамбле. Ну, замечательно. Но раз это
[23:46.620 --> 23:50.580]  константа, получается, что у нас для каждого объекта функция потерь приобретает вот такой вид.
[23:50.580 --> 23:56.860]  Возникает вопрос, зачем нам здесь экспоненциальная функция потерь? Потому что по ней краске очевидно,
[23:56.860 --> 24:01.860]  что такое взвешивание объекта. Смотрите, вот ваш вес. На каждом шаге ошибка нашего алгоритма,
[24:01.860 --> 24:06.860]  который был построен до текущего шага, это и есть вес объекта. Если объект был классифицирован
[24:06.860 --> 24:12.100]  правильно, ошибка у него маленькая, вес маленький. Если ошибка большая, то есть объект был
[24:12.100 --> 24:17.420]  классифицирован неправильно, мы хотим на нем учиться. Понятно, что происходит? Понятно, зачем здесь
[24:17.420 --> 24:23.340]  экспонента? Потому что если бы не был экспонент, мы не могли бы вот так факторизоваться и выделить
[24:23.340 --> 24:30.860]  отдельный член. Ну, назвали это дело Adaboost, от слова адаптивный boosting, adaptive boosting, и вроде как
[24:30.860 --> 24:35.900]  казалось, что вот классно. Мы придумали, как работать, но у Adaboost была куча проблем, он
[24:35.900 --> 24:42.100]  переобучался в лед, у него вот эта экспонента мешала вычислительно всем процессам, и далеко
[24:42.100 --> 24:46.460]  не всегда экспоненциальная функция потерь хорошо подходила. В том числе у нас с вами есть не только
[24:46.460 --> 24:51.420]  задача классификации, а еще и задача регрессии. Там не очень понятно, как сходу это исправить. Тоже
[24:51.420 --> 24:57.380]  можно, но тем не менее. Но на самом деле, в 90-х было множество различных подходов. Adaboost,
[24:57.380 --> 25:04.540]  Brownboost еще был, многие там называли фамилия автора Boost, и так далее. А потом в 2001 году к краске
[25:04.540 --> 25:08.820]  пришел градиентный boosting, который показал, что все вот это вот многообразие, это на самом деле
[25:08.820 --> 25:14.300]  частный случай общей истории градиентного boosting. И с ним давайте краски сейчас и попробуем
[25:14.300 --> 25:21.060]  разобраться. Во-первых, я сейчас сразу вас немножко попрошу, наверное, напрячь внимание. Сейчас
[25:21.060 --> 25:25.740]  будет немного больше математики, чем на предыдущих слайдах. И здесь будет, наверное, тема, которая
[25:25.740 --> 25:34.580]  в первом семестре кажется одной из наиболее сложных, наряду с свмом, который часто вызывает
[25:34.580 --> 25:38.220]  вопросы. В этом году мы оттуда почти полностью выполнили двойственную задачу, поэтому он стал, кажется,
[25:38.220 --> 25:44.060]  очень простым. Но, тем не менее, и вместе с, не знаю, кому-то первые лекции сложные, там всякие
[25:44.060 --> 25:48.380]  правдоподобия вылазят, кому-то всякие нейронки становятся сложными. Короче, давайте сейчас
[25:48.380 --> 25:54.220]  напряжемся на 10-15 минут, все осознаем. Итак, шаг номер один. Как всегда, постановка задачи. У нас
[25:54.220 --> 25:59.820]  с вами есть выборка, пара X, Y, причем абсолютно все равно. Регрессия, классификация, что-нибудь еще
[25:59.820 --> 26:06.380]  там придумайте, неважно. У вас есть множество пар объект-ответ, и у вас есть функция потеря. Мы
[26:06.380 --> 26:11.860]  хотели бы с вами найти такую модель, которая достигает оптимума на данной выборке, минимума,
[26:11.860 --> 26:18.420]  нашей функции потерь или что-то же самое, минимума эмпирического риска. Ну, что мы хотим? Мы, на самом
[26:18.420 --> 26:22.260]  деле, не имеем доступа ко всей генеральной совокупности, ко всему распределению, откуда пришли наши данные.
[26:22.260 --> 26:26.580]  У нас есть только наша выборка, поэтому вместо нашего вот этого отождания по факту мы с вами
[26:26.580 --> 26:30.860]  будем брать что? Среднее по всей выборке. То есть мы хотим получить модель, которая в среднем
[26:30.860 --> 26:36.500]  получает наименьшую ошибку. И пусть у нас с вами не просто какое-то семейство моделей, с которых мы
[26:36.500 --> 26:42.260]  ищем оптимальную, потому что среди всех, типа среди линейных, метрических, деревьев, их ансамблей,
[26:42.260 --> 26:46.700]  непараметрических и так далее, сложно найти оптимальную, надо все перебирать. Давайте пусть
[26:46.700 --> 26:50.700]  будет какая-то параметрическая патрика моделей, параметрическая семейство моделей. Что это значит?
[26:50.700 --> 26:56.340]  Что модель параметризуется вот этим вектором тета, неважно, что это это вектор параметров
[26:56.340 --> 27:03.100]  нашей линейной регрессии, множество параметров неровной сети, просто множество плитов вот этих
[27:03.100 --> 27:08.300]  трешолдов и фичей для дерева или что-нибудь еще. Главное, что мы вот эту запихнули все наши параметры,
[27:08.300 --> 27:13.500]  которые мы можем каким-то там образом менять, градиентным, неградиентным. Все нормально. Тут понятно?
[27:13.500 --> 27:36.100]  Вот здесь? Ну, я так скажу, здесь это скорее... Хорошо, здесь, наверное, стоит сказать,
[27:36.100 --> 27:40.660]  что х приходит из какой-то генерально-совокупности и здесь мы, собственно, по нему мат ожидаем. Короче,
[27:40.660 --> 27:47.100]  вот это просто выкиньте и все. Выкиньте второй член. Грубо говоря, это эквалентные записи в том
[27:47.100 --> 27:51.220]  смысле, что у нас с вами х и у приходят из вот этого нашего распределения, откуда пришли данные.
[27:51.220 --> 27:56.260]  Тут просто мы явно это написали. Тут, наверное, да, не совсем корректно написано. Наверное,
[27:56.260 --> 28:02.900]  надо поправить. f это краски. Смотрите, мы говорим, у нас вот что такое оптимальная модель. А,
[28:02.900 --> 28:08.740]  которая достигает минимума мат ожидаемости нашей функции потерь. Далее мы говорим, нам ее надо как-то
[28:08.740 --> 28:12.980]  найти, поэтому пусть наша f будет из код параметрических 8s. То есть мы можем каким-то
[28:12.980 --> 28:17.140]  векторам параметров ее определить. Пока у нас ее нет. Пока мы договорились, что...
[28:17.140 --> 28:30.500]  Ф от... Ну, функция потери от у и от нашего предсказания. Предсказания. Ну, смотрите,
[28:30.500 --> 28:34.820]  функция потери у вас обычно что считает? У вас есть ваше истинное значение и предсказанное значение.
[28:34.820 --> 28:39.300]  Средне квадратичная ошибка. Разница квадрат. Квадрат разницы течения.
[28:39.300 --> 28:46.700]  log loss. p log q. И так далее. То есть это ваша функция потерь. Вы ловили, нет?
[28:46.700 --> 28:56.940]  Так, коллеги, кто-нибудь вообще записывает за вашим вопросом. Я в тот раз еще на самом деле просил,
[28:56.940 --> 29:03.500]  если вы вот такие штуки видите. Вот да, здесь надо написать f от x. Так будет лучше. Можете,
[29:03.500 --> 29:07.020]  пожалуйста, после занятия, если кто-то это логирует, кидать в чат, тогда это будет все
[29:07.020 --> 29:11.860]  оперативно поправлено. Я, к сожалению, забываю через два с половиной часа, после того, как я это
[29:11.860 --> 29:20.260]  услышал. Спасибо. Хорошо. Опечатки вроде починили. Еще что-то понятно? Не понятно, точнее.
[29:20.260 --> 29:34.260]  Вроде все окей. Хорошо. Давайте тогда разобьем нашу вот эту текущую модель краски в том смысле,
[29:34.260 --> 29:38.740]  что мы знаем, что у нас с вами последовательность наших моделек. Классикаторов, например,
[29:38.740 --> 29:44.820]  или регрессоров. Последовательность алгоритмов. И мы знаем, что на текущем шаге у нас есть,
[29:44.820 --> 29:50.300]  допустим, с нулевого по t минус первое уже обученные. Все. Вот мы их обучили каким-то образом,
[29:50.300 --> 29:55.180]  не важно, но вспомните мат индукции. То есть тут мы сейчас говорим про шаг индукции. У нас уже
[29:55.180 --> 30:02.220]  что-то обучено, мы хотим еще одну модель дообучить, номер t. То есть нам, по сути, надо для модели на
[30:02.220 --> 30:08.380]  шаге t, ρt это ее вес и ттt это ее параметры, решить вот такую задачу минимизации. У нас есть y,
[30:08.380 --> 30:13.580]  истинный, который у нас есть. И, собственно, f от x, причем с крышкой вот все, что было раньше,
[30:13.580 --> 30:19.460]  плюс ρ на h от x. Вот это вот h от xt это наша новая модель, которую мы хотим добавить.
[30:19.460 --> 30:31.420]  Чего? Говорите погромче, пожалуйста, я вас плохо слышу отсюда. Еще раз, мы с вами, вот это было,
[30:31.420 --> 30:36.020]  в общем случае, написано это работать для всего. Теперь мы говорим, давайте будем строить модели
[30:36.020 --> 30:40.460]  последовательно. Каждое следующее будет улучшать результат предыдущего ансамбля. Значит у нас
[30:40.460 --> 30:47.580]  модель общая, вот алгоритм, ансамбль, давайте вот так называть, представим в виде суммы наших
[30:47.580 --> 30:53.700]  моделей. Каждое следующее, это все модели вместе что-то предсказали, получили результат. Согласны?
[30:53.700 --> 31:01.180]  Вот мы с вами пока что прошли t-1 шаг, и у нас есть вот эта вот f с крышкой, она уже каким-то там
[31:01.180 --> 31:05.140]  образом обучена. Пока не важно каким, мы, собственно, сейчас с вами на шаге индукции покажем,
[31:05.140 --> 31:10.300]  как мы ее обучили. Соответственно, на новом шаге нам нужно дообучить еще один кусочек ансамбля,
[31:10.300 --> 31:20.620]  вот он. Все согласны? Что? Базу индукции сейчас мы до нее дойдем. Пока давайте сначала шаг
[31:20.620 --> 31:26.100]  индукции опишем, потом базу, окей? Если у нас уже есть с вами обученная часть ансамбля,
[31:26.100 --> 31:32.380]  как нам к нему добавить еще один элемент? Вот что мы сейчас делаем. Понятно? Хорошо. Тогда мы с
[31:32.380 --> 31:37.100]  вами можем разбить в нашей функции потерь вот это на две части. Наше текущее предсказание и,
[31:37.100 --> 31:43.820]  по сути, поправка к тому, что есть. Верно? Все вот это понимают, правильно? Тут, тут, тут, тут.
[31:43.820 --> 31:50.580]  Нормально. И тогда краски оптимальная, наша t-t модель, вот f с крышкой t, которую мы должны
[31:50.580 --> 31:57.380]  сюда добавить, это будет ρt на h х θ t. Нам надо вот этот дел найти. Теперь у меня есть вопрос,
[31:57.380 --> 32:03.980]  как нам найти ρt и tt? Это основной вопрос. И здесь вступают краски в силу гридент и бусик.
[32:03.980 --> 32:09.260]  Вопрос, вы видите уже на экране. А что если бы мы могли использовать гридентный спуск не в
[32:09.260 --> 32:16.620]  пространстве как-то там параметров и так далее, а в пространстве моделей? Что это значит? Помните,
[32:16.620 --> 32:20.700]  вот эту картинку она у нас была на втором занятии, если исключить занятия по повторению линии на
[32:20.700 --> 32:26.660]  регрессии. Это вот у нас линии уровня или же эквипутенциальной поверхности, тут оптимум,
[32:26.660 --> 32:31.580]  вот наша начальная инициализация, вот первый гридентный шаг, второй, третий и так далее. Вот
[32:31.580 --> 32:37.680]  мы куда-то там шагаем. Но мы с вами привыкли это делать, где в пространстве параметры. У нас
[32:37.680 --> 32:42.940]  есть гиперповерхности, эквипутенциальной поверхности функции потерь, и мы с вами шагаем туда-сюда,
[32:42.940 --> 32:48.460]  пытаясь понизить значение функции потерь. Верно? А кто сказал, что только в пространстве параметров
[32:48.460 --> 32:53.340]  мы можем это делать? Мы это можем делать в пространстве модели и вообще где угодно. На самом деле,
[32:53.340 --> 32:57.740]  я поэтому говорил, что если у вас был функкан, вам будет проще. У вас есть какое-то пространство,
[32:57.740 --> 33:02.540]  а потом есть какие-то эквапутенциальные поверхности, и в котором, допустим, каждая точка
[33:02.540 --> 33:09.140]  соответствует какому-то отображению. В модели это и есть отображение. Можем так сделать? Можем.
[33:09.140 --> 33:15.220]  Остается вопрос, где нам взять вот этот самый гридентный спуск? И вся красота в том, что нам
[33:15.220 --> 33:20.340]  достаточно с вами одного единственного допущения. Давайте договоримся, что вот эта функция потерь,
[33:20.340 --> 33:30.940]  которую мы с вами выбрали, дифференцируема. Хорошо? Чего? Да, например. Вот. Каждая модель — это,
[33:30.940 --> 33:34.140]  по сути, некоторый оператор, сдающий отображение из пространства исходного,
[33:34.140 --> 33:40.780]  в пространство таргетов. Идет? Все, супер. Поэтому я и говорил, что функкан здесь полезен,
[33:40.780 --> 33:44.540]  просто чтобы вы понимали, что пространство операторов тоже можно задать, оно тоже существует.
[33:44.540 --> 33:49.780]  Хоть и не столь привычно. Так вот, единственное ограничение. Пусть вот эта функция потерь
[33:49.780 --> 33:54.860]  дифференцируема. Договорились? Но, я думаю, слово «градиентный бустинг», вот «градиентный» намекает,
[33:54.860 --> 34:02.140]  что где-то там будут градиенты. Нам нужны градиенты. Вот. Функция L — дифференцируема. Что мы тогда
[34:02.140 --> 34:07.700]  можем видеть? Ну, тогда давайте шагом. Вот наша текущая с вами константа, которая предсказывает
[34:07.700 --> 34:13.740]  на каждом шаге. Она у нас есть. Тогда теперь мы с вами что можем сделать? На каждом объекте из обучающей
[34:13.740 --> 34:22.820]  выборки у нас с вами есть истинный ответ. Правильно? И у нас с вами есть что? У нас с вами есть какое-то
[34:22.820 --> 34:29.420]  предсказание нашей текущей модели. Правильно? Ну, давайте я назад лисну. Вот оно. Предсказание
[34:29.420 --> 34:34.140]  нашей текущей модели для каждого х тоже есть. Она же у нас уже обучена. Плюс у нас есть какое-то
[34:34.140 --> 34:45.740]  ρ на h х тета. Правильно? Вот давайте вот эту штуку обзаваем просто ритой. В данном случае где-то он у нас. Вот. А, нет. Вот. Df от х тета.
[34:45.740 --> 34:54.660]  Короче, вот это наша добавка. ρ на h х тета. Это краткий f. Наговорились? Согласитесь, что если вот эта штука
[34:54.660 --> 35:01.860]  дифференцируема, L от y, f плюс какая-то поправка. Мы же можем с вами по этой поправке продиференцироваться.
[35:01.860 --> 35:08.100]  Это проф наш свободный член. Вспоминаем производную сложные функции. dl по d вот эта поправка. Это что?
[35:08.100 --> 35:16.860]  Это dl по d сумма и d сумма по вот этой штуке. Согласились все? Все согласны. И внезапно у нас получается,
[35:16.860 --> 35:22.700]  что для каждого объекта обучающей выборки, обратите внимание, для каждой пары x и y мы можем
[35:22.700 --> 35:28.140]  посчитать с вами функцию потерь, предсказание и соответственно значение производной. И получается,
[35:28.140 --> 35:34.700]  что для каждого объекта из нашей обучающей выборки, для каждой пары у нас есть поправка. Причем
[35:34.700 --> 35:38.860]  это не просто поправка. Это ровно то значение градиента, которое показывается. Вот настолько надо
[35:38.860 --> 35:43.580]  здесь поменять предсказание, чтобы стало лучше. Что такое градиент? Направление нескорейшего
[35:43.580 --> 35:48.540]  возрастания функции. Антиградиентное направление нескорейшего убывания. То есть ровно вот так мы
[35:48.540 --> 35:54.020]  должны поменять предсказания на этом объекте, над этом шаге нашего ансамбля, чтобы ошибка
[35:54.020 --> 36:02.060]  в ансамбле стала меньше. Вот это понятно? Сейчас еще раз проговорю. То есть мы с вами посчитали
[36:02.060 --> 36:07.260]  производные функции потерь по предсказанию нашей новой модели, которую мы пока не обучили. Это
[36:07.260 --> 36:15.060]  наш свободный член. Вы ловливаетесь? Давайте попробуем это на доске тоже написать. Вот у вас
[36:15.060 --> 36:30.980]  по сути L. Белый. L у вас зависит от Y этого, потом F от X этого. Вот с крышкой. Это все то, что мы
[36:30.980 --> 36:41.780]  сейчас обучили. Давайте. Вот. Да. X у нас есть. Вот. Взяли один объект, обучающий выборке. Только
[36:41.780 --> 36:49.300]  один. Взяли и с ним работаем. Все. Соответственно, у нас вот эта штука есть. Правильно? Давайте
[36:49.300 --> 37:00.020]  вот эту штуку я просто для удобства обзову кси. Хорошо? Тогда мы говорим с вами DL по DL. От Y и
[37:00.020 --> 37:10.220]  кси по D кси и опять же. Чему-то равна. Согласны? То есть это вот та величина, это наш градиент.
[37:10.220 --> 37:17.420]  Это то, как нам надо поменять предсказание, чтобы стало лучше. С этим все согласны? Ну так мы ж с
[37:17.420 --> 37:21.900]  вами только что говорили, что мы будем наше предсказание менять ровно вот на эту величину.
[37:21.900 --> 37:30.580]  А вот нам примерно что надо его менять. Все. Мы с вами получили оценку градиента в точке. То есть
[37:30.580 --> 37:37.080]  насколько нам нужно поменять предсказание на каждом объекте. А теперь фентушами. Мы для каждой
[37:37.080 --> 37:41.680]  точки из нашей обучающей выборки или, грубо говоря, для каждой точки в признаковом пространстве,
[37:41.680 --> 37:47.760]  до которой мы знаем значение таргета, мы понимаем, как нужно исправить предсказание. Вот наш градиент
[37:47.760 --> 37:53.680]  для градиент Loss по предсказанию. То есть надо идти по антиградиенту. Верно? Так давайте на него
[37:53.680 --> 38:00.320]  и обучаться. Давайте наша новая модель будет апроксимировать не ответ сам по себе, а антиградиент
[38:00.320 --> 38:06.560]  функции ошибки по предыдущим предсказаниям. То есть наша новая модель, мы когда добавляем новую
[38:06.560 --> 38:10.880]  модель, мы по сути делаем градиентный шаг пространства модели. У нас была какая-то модель, мы
[38:10.880 --> 38:18.520]  добавили новую, которая апроксимирует градиент. Уловили, что произошло? Мы не параметры поменяли,
[38:18.520 --> 38:29.160]  у нас новая модель апроксимирует градиенты, вы ее туда добавляете. Все. Ну смотрите, у вас модель,
[38:29.160 --> 38:35.800]  вот ваш ансамбль, это последовательность моделей. Вот сумма. На новом шаге вы к ней добавляете еще
[38:35.800 --> 38:41.520]  одну модель. Вот это все у вас уже обучено, оно уже где-то есть, оно уже прибито гвоздями. Все,
[38:41.520 --> 38:46.880]  вы его трогать не можете. На N плюс первом шаге вы говорите, хочу найти новую добавку к моему
[38:46.880 --> 38:53.120]  ансамблю, чтобы стало лучше. Как мне эту добавку обучать? Мы ищем антиградиент ошибки по текущим
[38:53.120 --> 38:57.880]  предсказаниям ансамбля, понимаем, что надо подменять предсказания ансамбля вот так и на них
[38:57.880 --> 39:04.800]  учимся. То есть наш новый элемент ансамбля апроксимирует антиградиент предыдущих ошибок
[39:04.800 --> 39:14.840]  в ансамбле. Наш новый элемент что-то, ну вот смотрите, вот наша H X TETA, вот он ваш. Новая
[39:14.840 --> 39:22.040]  модель, которая пока не обучена. Еще раз, вот ваш текущий ансамбль, вот вы к нему добавите
[39:22.040 --> 39:28.920]  РО на H X TETA. РО это learning rate просто, это градиентный шаг, размерово. Все, теперь вы нашли
[39:28.920 --> 39:35.040]  антиградиент, говорите, вот мой антиградиент. Вот ровно на него вы вашу новую модель, поправку
[39:35.040 --> 39:46.480]  будете обучать. Кто? H малая, это ваш алгоритм, который вы будете учить. Какой он будет, от вас
[39:46.480 --> 39:53.640]  абсолютно зависит, может быть, линейная модель, КНН и так далее, по барабану. Нет, H это просто
[39:53.640 --> 39:58.240]  модель из какого-то семейства. Единственное, что вам нужно, вам нужно уметь H обучать по выборке.
[39:58.240 --> 40:03.320]  То есть, если у вас есть множество пар, объекты, ответы, вы можете по ним настроить H.
[40:03.320 --> 40:12.480]  Ну да, то есть H приходит из какого-то параметрического семейства. То есть,
[40:12.480 --> 40:17.360]  вам не надо ее искать везде. Например, вы сказали, H это решающие деревья. Все, тогда у вас для каждого
[40:17.360 --> 40:23.160]  объекта есть X, признаковое описание, есть новый R-итый, который оценка антиградиента. На парах
[40:23.160 --> 40:32.520]  X и R-итый, X и R-итый, вы обучаете ваше новое дерево. Все. Почему именно квадратичное? Смотрите,
[40:32.520 --> 40:37.160]  потому что, по сути, у вас теперь от задачи регрессии, правильно? И вы сюда можете абсолютно,
[40:37.160 --> 40:41.120]  на самом деле, ошибку запихнуть. Квадратичные краски потому, что, когда вы минимизируете квадрат,
[40:41.120 --> 40:46.520]  вы получаете именно оценку среднего, и у вас там под капотом зашито гауссовое распределение. Все.
[40:46.520 --> 40:56.840]  Бинго. Абсолютно неважно, если у вас функция потерь есть дифференцируемая. Регрессия,
[40:56.840 --> 41:00.680]  классификация, какой-нибудь там ранжирование придумайте, вам вообще все равно. Есть дифференцированная
[41:00.680 --> 41:06.680]  функция потерь, вы знаете, как сделать ее лучше. Все. Вам вообще не дифференцируем. Более того,
[41:06.680 --> 41:11.880]  у вас изначально вот эти вот модели, опять же, в чем вся прелесть границного бустинга, они могут
[41:11.880 --> 41:16.120]  быть недифференцируемыми. Тот же самый градиентный бустинг надо решающим деревьями. Деревья сами
[41:16.120 --> 41:20.240]  по себе градиентной оптимизации не поддаются. Каждое дерево это кусочек непостоянной функции,
[41:20.240 --> 41:27.040]  но вы их можете использовать для опроксимации градиентов в каждой точке пространства. Все. То
[41:27.040 --> 41:32.400]  есть у вас, получается, в ней дифференцируемые модели сидят как базовые алгоритмы в ансамбле типа
[41:32.400 --> 41:53.720]  градиентный бустинг. Да. Это не очень логично, а дальше что? Почему это не очень логично? Смотрите,
[41:53.800 --> 42:05.720]  давайте я это напишу тогда на доске. Смотрите. Не, здесь у нас любые модели абсолютно. То есть
[42:05.720 --> 42:10.160]  про деревья я просто говорю, что даже дерево и даже какой-нибудь КНН здесь будет работать. Смотрите,
[42:10.160 --> 42:18.720]  у вас есть текущая модель, вот F от X крышкой. Это ваша текущая модель. Мы говорим, хочу получить
[42:18.720 --> 42:26.040]  новую модель, которая лучше. Вот это у меня на шаге T. Мы хотим получить F T плюс 1 с крышкой
[42:26.040 --> 42:32.080]  опять же от X. Тогда у нас здесь есть два допущения. Первое, мы говорим, что F T плюс 1 с крышкой,
[42:32.080 --> 42:41.200]  это что? Это F T с крышкой от X плюс какое-то РО на какое-то H от X. Ну, с параметрами тета. Вот,
[42:41.200 --> 42:48.840]  чтобы как там было. Согласны? Это мы пока сказали. И теперь, соответственно, вот это наша первая,
[42:48.840 --> 42:53.840]  по сути, посылка. И второе, мы говорим, хорошо, как мне, вот у нас есть F, да, и у нас есть L,
[42:53.840 --> 43:00.680]  функция. Как нам лучше всего исправить ошибку нашего текущего ансамбля? Ну, давайте посчитаем,
[43:00.680 --> 43:15.320]  D L от Y и F с крышкой T от X по кому? По, ну, даже может T плюс 1, по D краске F с крышкой T плюс 1 от X.
[43:15.320 --> 43:20.000]  Всё, вы говорите, как мне лучше всего исправить предсказание? Чистая математика, вот она вам.
[43:20.000 --> 43:25.480]  Вы поняли, как лучше всего исправить предсказание? Ну, теперь добавьте, вот вам ваш,
[43:25.480 --> 43:31.520]  по сути, шаг исправления предсказаний. То есть, вроде как, логично. Мы хотим оценить,
[43:31.520 --> 43:34.840]  насколько нам нужно подвинуть предсказания на каждом объекте, чтобы стало лучше.
[43:34.840 --> 43:57.720]  Чего? Я понял. Ой, хорошо, давайте посчитаем. У вас L считается от Y, да, и от F с крышкой T
[43:57.720 --> 44:08.840]  плюс РО, H, X и T по. Вот с этим согласны? Давайте я для начала вот это обозначу просто как
[44:08.840 --> 44:18.120]  кси, просто чтобы было удобнее писать. Тогда D L Y кси по D кси чему равно? Ну, вот чему-то там равно,
[44:18.120 --> 44:21.600]  мы не знаем, это от функции потери зависит. Хорошо, но мы это дифференцировать умеем.
[44:21.600 --> 44:29.520]  Это у нас есть. Что мы можем дальше сделать? Это у нас есть галка. Дальше мы с вами можем
[44:29.520 --> 44:38.920]  посчитать D кси по кому? По D РО. Ой, ну D тета я забежал вперед. По D РО можем продиференцировать?
[44:38.920 --> 44:52.120]  Чему это будет равно? H X тета. Все, хорошо. Соответственно D кси по D тета. Можем посчитать?
[44:52.120 --> 45:11.440]  Это соответственно будет D кси по D H на D H по D тета. И тут еще РО будет сидеть. Согласны?
[45:11.440 --> 45:26.920]  Вот вы градиенты посчитали в лоб. Супер. Да, спасибо. Я его просто потом явно написал.
[45:26.920 --> 45:41.160]  Мысли зачем? Не, в смысле вот вам все, вот ваши все градиенты, откуда они взялись. Если вот
[45:41.160 --> 45:52.320]  отсюда получили по сути оценку D тета или просто вам достаточно D кси по D H нарисовать, вы знаете,
[45:52.320 --> 45:57.200]  чему оно будет равно? У вас будет вот эта штука, умноженная на РО, это ваша D кси по D H. Это то,
[45:57.200 --> 46:18.880]  как надо настроить H. Все. Чего непонятно, ребята, я не понимаю. Ну да. Не, погодите,
[46:18.880 --> 46:25.640]  я вам всего лишь показываю, что вот у вас эта штука F от X, правильно? Она в себя включает
[46:25.640 --> 46:29.560]  константную часть, ваше текущее предсказание, вы по константе не можете продеференцировать,
[46:29.560 --> 46:34.440]  она константа. Плюс ваше новое предсказание, которое вы краски будете на него обучать.
[46:34.440 --> 46:48.000]  Так, что я сейчас сделал? По шагам. Первое, у нас есть текущая модель, которая предсказывает
[46:48.000 --> 46:54.840]  что-то, правильно? Вот. Сейчас она уже, вот эта часть, она обучена. Вот скажите мне, кто-нибудь
[46:54.840 --> 47:01.360]  может продеференцировать лост по константному предсказанию? Оно константное, вы его поменять
[47:01.360 --> 47:10.240]  не можете. Есть смысл деференцироваться по константе? Не очень. Хорошо. То есть мы говорим,
[47:10.240 --> 47:14.280]  хорошо, давайте в целом по предсказаниям, считая, что оно может линейно как-то меняться,
[47:14.280 --> 47:18.920]  продеференцируемся. Вот я только что это писал. Вот это у вас констант, а это у вас туда краски
[47:18.920 --> 47:26.120]  входят каким-то образом. По нему продеференцируемся и получаем, что вот это та же производная. Потому
[47:26.120 --> 47:31.520]  что у нас есть член, который свободный, он обладает крепким свободным, мы его менять можем. Мы на
[47:31.520 --> 47:46.280]  него краски обучаем кого? Нашу новую модель, которую у нас есть. Есть модель, она делает
[47:46.280 --> 47:52.720]  предсказания. У вас вы берете предсказания модели как просто какую-то свободную переменную,
[47:52.720 --> 48:04.160]  кси. Все, вот она. Дл по дкси. Можете посчитать? Можете. В каком месте я вас запутал, ребят?
[48:13.040 --> 48:19.760]  Хорошо, ладно, это было лишнее. Ну, в плане, сейчас ладо для прав, для понимания скорее вредит. Вы правы.
[48:19.760 --> 48:25.640]  Я всего лишь хотел показать, что это все дифференцируемая операция, то есть у вас нет
[48:25.640 --> 48:42.620]  того, что вы дифференцируете по какому-то константному предсказанию. Потому что
[48:42.620 --> 48:45.140]  дифференцируемся по свободному члену, получаем как надо
[48:45.140 --> 48:48.580]  изменить текущие предсказания, обучаем новую модель ровно
[48:48.580 --> 48:49.820]  на то, как надо его изменить.
[48:49.820 --> 48:55.340]  Логично то, что у нас, скорее всего, модель не сможет
[48:55.340 --> 48:59.100]  идеально аппроксимировать градиент в каждой точке.
[48:59.100 --> 49:01.860]  Поэтому мы с вами не получим идеальную поправку, мы
[49:01.860 --> 49:04.220]  получим какое-то движение в сторону антиградиента
[49:04.220 --> 49:06.300]  ровно как здесь у нас указано, мы за один шаг не попадем
[49:06.300 --> 49:07.300]  в оптимум.
[49:07.300 --> 49:09.140]  В том числе потому, что чтобы попасть даже в случае
[49:09.140 --> 49:11.580]  выпуклой задачи в оптимум за один шаг, нам нужны методы
[49:11.580 --> 49:13.700]  второго порядка применять, метод ньютнеравсн и так
[49:13.700 --> 49:14.700]  далее.
[49:14.700 --> 49:17.820]  А егебьяны считать и гессианы никто не любит, особенно
[49:17.820 --> 49:18.820]  на больших водах.
[49:18.820 --> 49:24.860]  Нет, это все для любой задачи, смотрите.
[49:24.860 --> 49:39.140]  Именно поэтому я вам уже пятую лекцию говорю, четвертую.
[49:40.140 --> 49:42.900]  Мы предсказываем вероятность, все, забудьте про метки
[49:42.900 --> 49:44.100]  класса, это слишком детский сад.
[49:44.100 --> 49:47.700]  У нас там вектор вероятности, по нему можно совершенно
[49:47.700 --> 49:48.700]  дифференцировать.
[49:48.700 --> 49:52.660]  Так, еще вопросы есть?
[49:52.660 --> 49:53.660]  Да.
[49:53.660 --> 50:05.740]  Сейчас, смотрите, мы должны аппроксимировать наши
[50:05.740 --> 50:06.740]  антиградиенты.
[50:07.620 --> 50:11.020]  Это уже просто какой-то вектор в общем случае в
[50:11.020 --> 50:13.300]  каком-то пространстве, правильно?
[50:13.300 --> 50:15.500]  Если мы решаем задачу, где надо аппроксимировать
[50:15.500 --> 50:16.980]  вектор, это какая задача?
[50:16.980 --> 50:17.980]  Регрессии.
[50:17.980 --> 50:21.140]  Регрессию можно решать с помощью минимизации МСЕ,
[50:21.140 --> 50:25.940]  МАЕ, МАЕ и кучу других там всяких функций ошибок.
[50:25.940 --> 50:28.420]  Когда мы с вами минимизируем среднюю квадратичную ошибку,
[50:28.420 --> 50:29.980]  мы на самом деле делаем оценку максимального
[50:29.980 --> 50:32.180]  правдоподобия, если у нас гауссовский прайор.
[50:32.180 --> 50:36.380]  Ну, это из статов просто вытекает.
[50:36.460 --> 50:40.020]  Оценка максимального правдоподобия при предположении,
[50:40.020 --> 50:42.380]  что у нас нормальное распределение, это называется эквивалент
[50:42.380 --> 50:43.820]  минимизации среднего квадратичной ошибки.
[50:43.820 --> 50:44.820]  Вот.
[50:44.820 --> 50:47.420]  Поэтому я и сказал, что по сути это говорит, что
[50:47.420 --> 50:48.420]  у нас где-то там гауссовский прайор.
[50:48.420 --> 50:51.060]  Запихнуть сюда МАЕ, у вас будет лапласовский прайор.
[50:51.060 --> 50:52.060]  Все.
[50:52.060 --> 50:55.060]  Так, у вас тоже был вопрос.
[50:55.060 --> 50:58.060]  Вот, смотрите, что такое РО, давайте вспомним.
[50:58.060 --> 51:00.060]  РО – это просто наш размер градиентного шага.
[51:00.060 --> 51:02.060]  Помните, мы с вами когда говорили про градиентный
[51:02.060 --> 51:06.060]  спуск, у нас там была какая-то тета, это на размер градиента.
[51:06.060 --> 51:07.060]  Вот.
[51:07.060 --> 51:10.060]  У нас функция наша с вами, она умная, она аппроксимирует
[51:10.060 --> 51:11.060]  чисто наш градиент.
[51:11.060 --> 51:12.060]  Вот.
[51:12.060 --> 51:13.060]  Вот.
[51:13.060 --> 51:14.060]  Вот.
[51:14.060 --> 51:15.060]  Вот.
[51:15.060 --> 51:16.060]  Вот.
[51:16.060 --> 51:17.060]  Вот.
[51:17.060 --> 51:18.060]  Вот.
[51:18.060 --> 51:19.060]  Вот.
[51:19.060 --> 51:20.060]  Вот.
[51:20.060 --> 51:21.060]  Вот.
[51:21.060 --> 51:22.060]  Вот.
[51:22.060 --> 51:23.060]  Вот.
[51:23.060 --> 51:24.060]  Вот.
[51:24.060 --> 51:25.060]  Вот.
[51:25.060 --> 51:26.060]  Вот.
[51:26.060 --> 51:27.060]  Вот.
[51:27.060 --> 51:28.060]  Вот.
[51:28.060 --> 51:29.060]  Вот.
[51:29.060 --> 51:30.060]  Вот.
[51:30.060 --> 51:31.060]  Вот.
[51:31.060 --> 51:32.060]  Вот.
[51:32.060 --> 51:33.060]  Вот.
[51:33.060 --> 51:34.060]  Вот.
[51:34.060 --> 51:35.060]  Вот.
[51:35.060 --> 51:36.060]  Вот.
[51:36.060 --> 51:37.060]  Вот.
[51:37.060 --> 51:38.060]  Вот.
[51:38.060 --> 51:39.060]  Вот.
[51:39.060 --> 51:40.060]  Вот.
[51:40.060 --> 51:41.060]  Вот.
[51:41.060 --> 51:42.060]  Вот.
[51:42.060 --> 51:43.060]  Вот.
[51:43.060 --> 51:44.060]  Вот.
[51:44.060 --> 51:45.060]  Вот.
[51:45.060 --> 51:46.060]  Вот.
[51:46.060 --> 51:47.060]  Вот.
[51:47.060 --> 51:48.060]  Вот.
[51:48.060 --> 51:49.060]  Вот.
[51:49.060 --> 51:50.060]  Вот.
[51:50.060 --> 51:51.060]  Вот.
[51:51.060 --> 51:52.060]  Вот.
[51:52.060 --> 51:53.060]  Вот.
[51:53.060 --> 51:54.060]  Вот.
[51:54.060 --> 51:55.060]  Вот.
[51:55.060 --> 51:56.060]  Вот.
[51:56.060 --> 51:57.560]  Да, это believing that.
[51:57.560 --> 51:58.560]  Да.
[51:58.560 --> 52:00.500]  Смотрите, вы сначала аппроксимируете антиградиент каким-то
[52:00.500 --> 52:01.500]  там образом.
[52:01.500 --> 52:04.420]  А потом вы, уже зная, что идём в этом направлении,
[52:04.420 --> 52:06.420]  ищете оптимальное значение ρ.
[52:06.420 --> 52:09.140]  Потому, что ρ – это, грубо говоря, растяжение и сжатия
[52:09.140 --> 52:10.900]  в этом направлении – куда именно нам шаг consid Ricardo.
[52:10.900 --> 52:13.020]  Стильно или слабо?
[52:13.020 --> 52:18.420]  Ну, слушайте, по сравнению с тем, что на каждом шаге
[52:18.420 --> 52:20.900]  вы какой-то модели обучаете подобрать коэффициент,
[52:20.900 --> 52:26.620]  У вас же тут, ну, сами посмотрите, линейны, по сути, все линейны.
[52:26.620 --> 52:29.300]  Производную вы уже считали, чтобы вот это получить,
[52:29.300 --> 52:31.460]  вам достаточно всего лишь сюда подставить в аналитическое
[52:31.460 --> 52:35.260]  выражение ваше новое предсказание и подобрать порожки-производную.
[52:35.260 --> 52:36.260]  Все.
[52:36.260 --> 52:37.260]  Так что ничего страшного.
[52:37.260 --> 52:40.900]  Не, ну, второй вариант вы можете, если совсем не
[52:40.900 --> 52:43.300]  хочется, может каким-то, не знаю, там, линейным поиском
[52:43.300 --> 52:44.300]  пробежаться и все.
[52:44.300 --> 52:45.300]  Тоже вариант.
[52:45.300 --> 52:46.300]  Да?
[52:46.300 --> 52:47.300]  Да?
[52:47.300 --> 52:48.300]  Сейчас.
[52:48.300 --> 52:49.300]  Что значит?
[52:49.300 --> 52:50.300]  А, смотрите, argmin, что такое?
[52:50.300 --> 52:51.860]  Ну, min – задача минимизации, argmin – это задача поиска
[52:51.860 --> 52:54.020]  аргументов, при которых доходится, достается минимум.
[52:54.020 --> 52:57.500]  То есть, минимум выражения – это его экстремум, argmin
[52:57.500 --> 53:00.220]  выражение – это значение аргумента, при котором достигается
[53:00.220 --> 53:01.220]  экстремум.
[53:01.220 --> 53:03.540]  Ну, экстремум минимум, в смысле argmax при котором
[53:03.540 --> 53:04.540]  достигается максимум, соответственно.
[53:04.540 --> 53:05.540]  Все.
[53:05.540 --> 53:06.540]  Продолжаем.
[53:06.540 --> 53:30.680]  Сначала вы нашли параметр θ, вы знаете, какая модель
[53:30.680 --> 53:32.100]  у вас новая на новом шаге.
[53:32.100 --> 53:34.540]  Все, модель зафиксировали.
[53:34.540 --> 53:36.340]  Теперь вы говорите, с каким весом мне добавить
[53:36.340 --> 53:37.340]  ОМАнсамбль.
[53:37.340 --> 53:38.340]  Все.
[53:38.340 --> 54:01.380]  Я еще этот вопрос-то не понимаю, почему мы в один
[54:01.380 --> 54:03.100]  шаг не нашли и роет эту одновременно?
[54:03.100 --> 54:11.260]  Смотрите, мы с вами с помощью дифференцирования нашли
[54:11.260 --> 54:12.260]  наш градиент.
[54:12.260 --> 54:15.260]  Градиент мы потом аппроксимировали каким-то там образом.
[54:15.260 --> 54:17.740]  Он неточный, это уже оценка градиента, она уже имеет
[54:17.740 --> 54:18.740]  ошибки.
[54:18.740 --> 54:21.340]  Теперь мы говорим, с такой оценкой градиента, насколько
[54:21.340 --> 54:22.660]  нам нужно сместиться.
[54:22.660 --> 54:23.660]  Все.
[54:23.660 --> 54:27.660]  Если бы у нас с вами был точный градиент, еще вопрос
[54:27.660 --> 54:28.660]  бы, наверное, имел смысл.
[54:28.660 --> 54:30.660]  У нас вообще оценка градиента, нам не понятно, насколько
[54:30.660 --> 54:31.660]  он вообще хороший.
[54:31.740 --> 54:35.820]  У вас градиент оценивается трехслойным деревом в глубиной
[54:35.820 --> 54:36.820]  3.
[54:36.820 --> 54:38.700]  Он очень грубо оценивается.
[54:38.700 --> 54:40.140]  Вопрос, насколько нам вообще надо идти.
[54:40.140 --> 54:43.020]  Может, нам вообще с таким градиентом идти никуда
[54:43.020 --> 54:44.460]  не надо, потому что слишком паршивая оценка.
[54:44.460 --> 54:45.460]  Все.
[54:45.460 --> 54:51.860]  Еще вопросы тут есть?
[54:51.860 --> 54:53.860]  Ну давайте тогда посмотрим на простой пример.
[54:53.860 --> 54:54.860]  Сразу дисклеймер.
[54:54.860 --> 54:58.300]  В случае регрессии и средней квадратической ошибки
[54:58.300 --> 55:01.020]  это все вырождается вообще в детский сад, это частный
[55:01.020 --> 55:02.260]  так работает не всегда.
[55:02.260 --> 55:03.260]  Хорошо?
[55:03.260 --> 55:04.260]  Поехали.
[55:04.260 --> 55:07.460]  Давайте посмотрим на среднюю квадратичную ошибку.
[55:07.460 --> 55:11.260]  У нас тогда лос это квадрат отклонения, правильно?
[55:11.260 --> 55:13.660]  То есть 2у с крышкой минус у.
[55:13.660 --> 55:16.460]  У с крышкой наша оценка, у то, что есть.
[55:16.460 --> 55:17.460]  Правильно?
[55:17.460 --> 55:22.260]  Ну видим, что по сути пропорционально с кефисентом минус 2у с крышкой
[55:22.260 --> 55:23.260]  минус у.
[55:23.260 --> 55:24.260]  Верно?
[55:24.260 --> 55:25.260]  А?
[55:25.260 --> 55:26.260]  Пропорционально.
[55:26.260 --> 55:41.660]  Ну ладно, ну наверное все-таки пропорционально, потому что
[55:41.660 --> 55:42.660]  я эти слайды составлял.
[55:42.660 --> 55:45.380]  Вывод про это.
[55:45.380 --> 55:53.420]  Так, какой символ вам не нравится?
[55:54.420 --> 56:04.300]  Ребят, ну нам же основная суть какая, мы должны условиться
[56:04.300 --> 56:05.300]  об обозначениях.
[56:05.300 --> 56:07.980]  Я хоть горшок на доске нарисовать могу, если вы понимаете,
[56:07.980 --> 56:08.980]  что это такое, абсолютно неважно.
[56:08.980 --> 56:11.180]  Что мы здесь с вами видим?
[56:11.180 --> 56:14.460]  Что у нас градиент по сути пропорционально чему?
[56:14.460 --> 56:17.860]  Реально отклонению нашего предсказания от истинного
[56:17.860 --> 56:19.860]  значения, правильно?
[56:20.500 --> 56:22.980]  То есть мы на каждом шаге с вами будем пытаться
[56:22.980 --> 56:23.980]  оценить что?
[56:23.980 --> 56:26.380]  Мы на каждом шаге будем с вами пытаться краски
[56:26.380 --> 56:29.780]  оценить, насколько мы ошиблись и каждая новая модель будет
[56:29.780 --> 56:32.700]  предсказывать именно ошибку предыдущей модели в прямом
[56:32.700 --> 56:33.700]  смысле.
[56:33.700 --> 56:36.700]  То есть надо было предсказать 25 овец, мы предсказали 21,
[56:36.700 --> 56:37.700]  надо поправить на 4 овца.
[56:37.700 --> 56:39.220]  Я понять не имею причем здесь овца, мне только что
[56:39.220 --> 56:40.220]  в голову пришли.
[56:40.220 --> 56:42.820]  Извините, я вспомнил мимачик, который я вчера видел.
[56:42.820 --> 56:48.420]  Да, именно, ну то есть смотрите, это именно предсказание
[56:48.420 --> 56:49.420]  в каждой точке.
[56:49.460 --> 56:51.060]  Я просто говорю, почему я это расписывал, потому
[56:51.060 --> 56:53.700]  что если у вас уже модель фиксирована, все в ней все
[56:53.700 --> 56:55.980]  параметры фиксированы, вы не можете по константе
[56:55.980 --> 56:59.100]  продиференцироваться, чтобы вы понимали, что это
[56:59.100 --> 57:00.100]  значит.
[57:00.100 --> 57:01.100]  Понятно?
[57:01.100 --> 57:02.100]  Супер.
[57:02.100 --> 57:03.980]  То есть в случае с линейной регрессией градиентный
[57:03.980 --> 57:07.020]  бустинг вырождается в то, простите, с минимализацией
[57:07.020 --> 57:09.940]  среднего трагичной ошибки, задачей регрессии, градиентный
[57:09.940 --> 57:12.020]  бустинг вырождается в том, что каждая следующая
[57:12.020 --> 57:14.940]  модель обучается на ошибке предыдущей буквально.
[57:14.940 --> 57:19.340]  Все, он просто, вот вы предсказали, 25 надо было, 30, он очень
[57:19.340 --> 57:23.020]  обучится на 5, но это работает, потому что у вас производная
[57:23.020 --> 57:25.020]  будет вот такая линейная.
[57:25.020 --> 57:27.180]  Условно с какой-нибудь средней абсолютной ошибкой это
[57:27.180 --> 57:28.180]  уже не работает.
[57:28.180 --> 57:30.460]  То, что производом будет какая?
[57:30.460 --> 57:31.460]  Сигнум вот этой величины.
[57:31.460 --> 57:32.460]  Согласны?
[57:32.460 --> 57:33.460]  Уловили?
[57:33.460 --> 57:38.140]  Ну и давайте посмотрим на классную демку, которую
[57:38.140 --> 57:42.820]  в далеком 2016 году запилил мой тогда еще коллега, мы
[57:42.820 --> 57:45.300]  с ним последние годы точно вообще не общались, но
[57:46.300 --> 57:49.340]  классный следователь, классный преподаватель приложил
[57:49.340 --> 57:52.620]  руку к различным задачкам и преподаванию и в шаге
[57:52.620 --> 57:53.620]  много где.
[57:53.620 --> 57:56.700]  И в принципе достаточно, смотрите, во-первых, вот
[57:56.700 --> 57:59.100]  наше решающее дерево, давайте, наверное, на него сначала
[57:59.100 --> 58:00.100]  посмотрим.
[58:00.100 --> 58:02.780]  Вот у нас с вами гиперповерхность, видите, да, такая волна.
[58:02.780 --> 58:08.500]  Вот, что у нас делает решающее дерево, глубины 1, глубины
[58:08.500 --> 58:13.300]  2, 3, 4, 5, 6, вот такая вот ступенчатая функция у нас получается.
[58:13.300 --> 58:26.860]  Погодите, пока что одно дерево, сейчас, или что?
[58:26.860 --> 58:27.860]  Линейная комбинация, да.
[58:27.860 --> 58:32.380]  Погодите, вот же мы с вами только что это писали, вот
[58:32.380 --> 58:35.540]  каждое с вами дерево, вы просто добавляете его
[58:36.540 --> 58:39.540]  находится в гридентном какой-то оптимизации, опять же, маленькую
[58:39.540 --> 58:40.540]  подзадачу решаем.
[58:40.540 --> 58:48.380]  Погодите, у вас всегда будет ступенчатая функция при
[58:48.380 --> 58:51.780]  сумме, а ступенчатая функция не линейная.
[58:51.780 --> 58:55.020]  Ну, смотрите, давайте я вам это нарисую.
[58:55.020 --> 59:03.740]  Вот у вас одна модель, вот у вас вторая модель.
[59:03.740 --> 59:10.220]  Давайте их просуммируем, соответственно, тут у нас
[59:10.220 --> 59:15.740]  был 0, тут 1, тут опять 0, 1, 1, 0, получается у вас будет
[59:15.740 --> 59:22.180]  0, потом 1, потом 2, потом опять 1, вот, всё, просуммировал.
[59:22.180 --> 59:26.380]  Ну, вот этого края, правда, нет.
[59:26.380 --> 59:27.380]  Уловили?
[59:27.380 --> 59:30.900]  Короче, вот у нас одно дерево может вот так максимум
[59:30.900 --> 59:33.100]  сделать, дерево глубиной 6.
[59:33.100 --> 59:36.180]  Возникает вопрос, а что если у нас с вами гораздо
[59:36.180 --> 59:38.980]  больше деревьев, но они все глупенькие.
[59:38.980 --> 59:41.300]  Вот вам гридентный бустинг из ста решающих деревьев.
[59:41.300 --> 59:44.860]  И давайте посмотрим, вот у нас 100 деревьев глубиной
[59:44.860 --> 59:45.860]  3.
[59:45.860 --> 59:48.260]  Ну, согласитесь, кажется, гораздо более плавно описывает
[59:48.260 --> 59:50.940]  нашу вот эту гиперповерхность по сравнению с одним деревом
[59:50.940 --> 59:53.980]  глубиной 6, ступенник меньше и так далее.
[59:53.980 --> 59:58.420]  Можно даже сделать глубину поменьше и видеть, что всё
[59:58.420 --> 01:00:00.740]  равно достаточно плавно он его описывает, хотя,
[01:00:00.900 --> 01:00:03.300]  на сами пике не может добраться, почему?
[01:00:03.300 --> 01:00:05.780]  Потому что глубина дерева слишком маленькая, деревьев
[01:00:05.780 --> 01:00:06.780]  маловато.
[01:00:06.780 --> 01:00:10.460]  Если увеличим глубину дерева, даже он, решающие пни, они
[01:00:10.460 --> 01:00:12.580]  из-за того, что каждый раз только по одному признаку
[01:00:12.580 --> 01:00:14.460]  они вторую ось не распознают.
[01:00:14.460 --> 01:00:15.460]  Всё.
[01:00:15.460 --> 01:00:18.700]  Ну, соответственно, вот вам два признака уже есть,
[01:00:18.700 --> 01:00:19.700]  вот он хорошо работает.
[01:00:19.700 --> 01:00:26.260]  3, и того краше 4, ну, практически на таком, скажем так, удалении
[01:00:26.260 --> 01:00:28.460]  не видно вообще гладкой поверхности или куча
[01:00:28.460 --> 01:00:29.940]  ступенек, по факту это куча ступенек.
[01:00:29.940 --> 01:00:34.100]  Ну, соответственно, то дерево глубиной 6, ну, вот
[01:00:34.100 --> 01:00:37.620]  вам замечательная опроксимация той самой вашей гиперповерхности
[01:00:37.620 --> 01:00:38.620]  работает.
[01:00:38.620 --> 01:00:41.180]  Хотя одно дерево глубиной 6, как видите, даже рядом
[01:00:41.180 --> 01:00:43.580]  не лежало, и тут очень такое всё дискретно и не очень
[01:00:43.580 --> 01:00:44.580]  красивое.
[01:00:44.580 --> 01:00:48.340]  Понятно, что происходит, правильно?
[01:00:48.340 --> 01:00:49.340]  Вот.
[01:00:49.340 --> 01:00:52.180]  И ещё раз, собственно, возможно вам тут станет понятно,
[01:00:52.180 --> 01:00:54.860]  вот ваш ансамбль, вот первое дерево, второе дерево и
[01:00:54.860 --> 01:00:57.820]  так далее, и все их мы обучаем.
[01:00:57.900 --> 01:01:01.460]  На четвертом шаге три дерева уже построены, мы говорим,
[01:01:01.460 --> 01:01:04.900]  что это три дерева плюс четвертое, это наша f от x.
[01:01:04.900 --> 01:01:07.260]  Давайте поймём, как нам поправить предсказания
[01:01:07.260 --> 01:01:09.820]  и обучим на этой краске наше четвертое дерево.
[01:01:09.820 --> 01:01:10.820]  Всё?
[01:01:10.820 --> 01:01:11.820]  Вот оно.
[01:01:11.820 --> 01:01:12.820]  Вот, смотрите.
[01:01:12.820 --> 01:01:18.260]  Вот наша целевая функция, и теперь, кстати, да, вернёмся
[01:01:18.260 --> 01:01:20.660]  сразу к краске с вами, к самом начале, помните, я
[01:01:20.660 --> 01:01:23.260]  говорил, что мы только шаг индукции с вами обозначили.
[01:01:23.260 --> 01:01:26.380]  Где взять базу индукции?
[01:01:26.380 --> 01:01:29.100]  Эдман, предложивший градиентный бусинг, предложил очень просто.
[01:01:29.100 --> 01:01:30.900]  Абсолютно неважно, с чего вы начинаете, потому что
[01:01:30.900 --> 01:01:33.140]  каждая следующая модель будет всё равно всё поправлять.
[01:01:33.140 --> 01:01:35.220]  Давайте всегда у нас в начальной модели будет просто среднее
[01:01:35.220 --> 01:01:36.220]  значение таргета.
[01:01:36.220 --> 01:01:37.220]  Констант.
[01:01:37.220 --> 01:01:38.220]  Всё.
[01:01:38.220 --> 01:01:40.980]  Соответственно, вот у вас слева, вот наш таргет.
[01:01:40.980 --> 01:01:42.700]  Видите, просто ноль.
[01:01:42.700 --> 01:01:45.260]  Вот эта гиперпло… вот эта плоскость, это наша текущая
[01:01:45.260 --> 01:01:46.260]  оценка.
[01:01:46.260 --> 01:01:48.260]  Каждая следующая модель будет её исправлять.
[01:01:48.260 --> 01:01:49.260]  Чего?
[01:01:49.260 --> 01:01:52.980]  Среднее в данном случае это интеграл у вас?
[01:01:52.980 --> 01:01:55.100]  Просто среднее значение y, просто выборочное среднее
[01:01:55.100 --> 01:01:56.100]  Взяли, да и всё.
[01:01:56.100 --> 01:01:59.540]  У нас так как нет распределения, у нас интеграла нет, просто
[01:01:59.540 --> 01:02:00.540]  среднее выборочное.
[01:02:00.540 --> 01:02:01.540]  Всё.
[01:02:01.540 --> 01:02:04.260]  Соответственно, вот у нас первое дерево, смотрите.
[01:02:04.260 --> 01:02:10.700]  Мы его построили, с каким-то весом его сюда добавили.
[01:02:10.700 --> 01:02:11.700]  Добавили второе дерево.
[01:02:11.700 --> 01:02:16.860]  Эх, жалко он нас, эти, не строит нам, как его называют,
[01:02:16.860 --> 01:02:17.860]  остатки.
[01:02:17.860 --> 01:02:22.260]  Третье, четвёртое, пятое, шестое… А, не, вот, смотрите,
[01:02:22.260 --> 01:02:23.940]  справа видно остатки.
[01:02:23.940 --> 01:02:28.580]  Посмотрите, слева полностью наш ансамбль, вот он нарисован,
[01:02:28.580 --> 01:02:29.660]  и наша целевая функция.
[01:02:29.660 --> 01:02:35.380]  Справа каждая новая дерева и та гиперповершенность,
[01:02:35.380 --> 01:02:37.860]  которую дерево должно опроксимировать, это ровно те самые наши
[01:02:37.860 --> 01:02:39.700]  градиенты, антиградиенты.
[01:02:39.700 --> 01:02:41.420]  Следите за гиперповершенностью справа.
[01:02:41.420 --> 01:02:45.220]  Пока нормально, одно дерево добавили.
[01:02:45.220 --> 01:02:48.060]  Заметьте, видите, уже тут появились какие-то неоднородности,
[01:02:48.060 --> 01:02:49.060]  вот их видно.
[01:02:50.060 --> 01:02:53.780]  Если идти дальше, видите, у нас уже, где появляются,
[01:02:53.780 --> 01:02:56.700]  тут у нас почти всё хорошо предсказано, поэтому тут
[01:02:56.700 --> 01:02:59.780]  у нас никаких впадин нет, тут почти константа грубая.
[01:02:59.780 --> 01:03:02.580]  Вот эта штука всё ещё торчит, потому что мы здесь, видите,
[01:03:02.580 --> 01:03:04.100]  сильную ошибку имеем, тут градиент большой.
[01:03:04.100 --> 01:03:08.540]  Едем дальше, заметьте, у нас опять гиперповерхность,
[01:03:08.540 --> 01:03:11.300]  которую наше дерево опроксимирует, она уже сильно отличается
[01:03:11.300 --> 01:03:12.300]  от исходной.
[01:03:12.300 --> 01:03:15.140]  Она уже больше на шум какой-то похожа по центру.
[01:03:15.140 --> 01:03:20.740]  6, 7, 8, 9, 10, вот, здесь в конце совсем хорошо видно.
[01:03:20.740 --> 01:03:23.300]  Видите, ошибки у нас большие только в районе пиков.
[01:03:23.300 --> 01:03:25.980]  В районе пиков у нас всё ещё есть градиенты, в остальных
[01:03:25.980 --> 01:03:28.940]  местах у нас скорее какие-то небольшие девиации.
[01:03:28.940 --> 01:03:33.620]  И отсюда мы, собственно, с вами видим, в чём суть
[01:03:33.620 --> 01:03:34.980]  градиентного бусинга.
[01:03:34.980 --> 01:03:37.200]  Если до этого момента мы с вами всегда, когда решали
[01:03:37.200 --> 01:03:40.620]  задачу какую-либо, мы пытались построить всё более-более-более
[01:03:40.620 --> 01:03:43.900]  информативное признаковое описание, мы пытались упростить
[01:03:43.900 --> 01:03:46.020]  признаковое описание, придумать новые признаки, придумать
[01:03:46.020 --> 01:03:49.260]  подходящее ядро, сделать нелинейную нашу модель,
[01:03:49.260 --> 01:03:50.260]  то же самое дерево.
[01:03:50.260 --> 01:03:52.900]  Градиентный бусинг заходит с другой стороны.
[01:03:52.900 --> 01:03:55.460]  У нас на каждом шаге признаки те же самые остаются, мы
[01:03:55.460 --> 01:03:56.700]  каждый раз упрощаем таргет.
[01:03:56.700 --> 01:04:00.700]  По сути мы говорим, вот смотри, мы уже вот здесь
[01:04:00.700 --> 01:04:02.660]  хорошо предсказываем, не обязательно на этом объекте,
[01:04:02.660 --> 01:04:04.660]  вот в этой области хорошо предсказываем, а вот тут
[01:04:04.660 --> 01:04:05.660]  у нас всё плохо.
[01:04:05.660 --> 01:04:07.980]  Поэтому надо фокусировать внимание вот здесь.
[01:04:07.980 --> 01:04:08.980]  Вот вам пример.
[01:04:08.980 --> 01:04:11.380]  Мы в центре уже всё достаточно хорошо предсказываем, а
[01:04:11.380 --> 01:04:12.980]  с краёв у нас всё плохо.
[01:04:12.980 --> 01:04:14.700]  Поэтому следующие модели будут всё больше и больше
[01:04:14.700 --> 01:04:17.180]  фокусироваться на краях, игнорировать центр.
[01:04:17.180 --> 01:04:19.020]  Если говорить с точки зрения дерева, дерево будет что-то
[01:04:19.020 --> 01:04:21.380]  вроде вот такого, иметь столбца, где-то там слева
[01:04:21.380 --> 01:04:25.140]  с краю, а по центру будет просто нулевой констант предсказывать.
[01:04:25.140 --> 01:04:26.140]  Улавливаете?
[01:04:26.140 --> 01:04:29.060]  И почему это важно?
[01:04:29.060 --> 01:04:33.060]  Потому что когда мы с вами говорим про градиентный
[01:04:33.060 --> 01:04:36.340]  бустинг или про линейные модели, про всё это остальное,
[01:04:36.340 --> 01:04:39.780]  это не какие-то два абсолютно разных мира, это просто
[01:04:39.780 --> 01:04:41.100]  две крайности.
[01:04:41.220 --> 01:04:43.940]  Мы можем либо только работать с признаками, либо только
[01:04:43.940 --> 01:04:45.100]  работать с таргетом.
[01:04:45.100 --> 01:04:47.860]  Как вы понимаете, мы можем работать и с тем, и с другим.
[01:04:47.860 --> 01:04:50.420]  И поэтому это занятие у нас предшествует введению
[01:04:50.420 --> 01:04:52.140]  в диплёринг, потому что когда мы доберёмся с вами
[01:04:52.140 --> 01:04:55.700]  до нейронных сетей, мы ровно открываем глаза на то, что
[01:04:55.700 --> 01:04:57.740]  мы на самом деле можем строить с вами некоторое
[01:04:57.740 --> 01:04:59.860]  отображение из пространства объектов, пространства
[01:04:59.860 --> 01:05:02.740]  ответов, и мы менять можем как одно, так и другое,
[01:05:02.740 --> 01:05:03.740]  причём последовательно.
[01:05:03.740 --> 01:05:07.420]  В чём проблема градиентного бустинга, если про него говорить?
[01:05:07.740 --> 01:05:12.500]  В том, что он строго последовательный, и попав на шаг n плюс 1, мы
[01:05:12.500 --> 01:05:14.620]  теряем возможность поменять предыдущий шаг.
[01:05:14.620 --> 01:05:16.980]  Это дерево, если там стоит дерево в качестве базового
[01:05:16.980 --> 01:05:18.660]  алгоритма, в основном деревья.
[01:05:18.660 --> 01:05:21.380]  В общем случае, вы всё равно построили модель, она
[01:05:21.380 --> 01:05:23.380]  фиксирована, вы от неё считаете градиенты и так далее,
[01:05:23.380 --> 01:05:24.860]  вы не имеете права предыдущей модели менять.
[01:05:24.860 --> 01:05:28.460]  Поэтому, когда вы попали на n плюс первый шаг, всё
[01:05:28.460 --> 01:05:31.900]  то, что было до шага n, отлитый в бетоне, больше его не трогайте.
[01:05:31.900 --> 01:05:34.300]  Проблема в том, что это не всегда корректно, и иногда
[01:05:34.300 --> 01:05:36.340]  нам хочется что-то в начале поменять, потому что ошибка
[01:05:36.340 --> 01:05:39.020]  в начале, она очень дорога, она потом всегда нам будет
[01:05:39.020 --> 01:05:40.020]  аукаться.
[01:05:40.020 --> 01:05:43.220]  Нейронные сети ровно краски этим нам потом и помогут,
[01:05:43.220 --> 01:05:46.660]  и мы сможем менять любые шаги, любые преобразования
[01:05:46.660 --> 01:05:47.660]  в любой момент.
[01:05:47.660 --> 01:05:49.140]  Но об этом на следующих рексах.
[01:05:49.140 --> 01:05:51.140]  Просто, чтобы у вас не было вот этого разделения,
[01:05:51.140 --> 01:05:53.460]  что вот бустинги, а тут нейронные сети, нет, одно к другому
[01:05:53.460 --> 01:05:54.460]  очень хорошо подводит.
[01:05:54.460 --> 01:05:56.620]  Более того, бустинг появился после появления нейронных
[01:05:56.620 --> 01:05:58.780]  сетей, просто он менее прожорливый с точки зрения
[01:05:58.780 --> 01:06:02.180]  данных, поэтому он в начале нулевых зашёл гораздо больше.
[01:06:02.180 --> 01:06:05.900]  Нейронные сети к тому моменту показали свои, скажем
[01:06:06.040 --> 01:06:08.920]  проют suburbanassa, свои группы уielaCal и стали переобучаться
[01:06:09.240 --> 01:06:11.060]  в лед на всё что только можно.
[01:06:11.060 --> 01:06:13.880]  Гråдиентный бустинг, казалось что о, Крулёва он не перебучается,
[01:06:14.200 --> 01:06:16.100]  ладно сделали просто побольше ансамбль тоже стал
[01:06:16.240 --> 01:06:17.980]  переобычаться, все опять перегарюнились.
[01:06:18.560 --> 01:06:20.520]  То же самое было с нейронными ст Service, что сначала нейронные
[01:06:20.520 --> 01:06:23.080]  сети были маленькие, а даце тоже накопили большие,
[01:06:23.120 --> 01:06:25.120]  казало-с что нейронные сети и это идеально ониback
[01:06:25.120 --> 01:06:26.660]  работать везде, они не переобучаются.
[01:06:26.660 --> 01:06:29.620]  Просто потому что вычислительные мощности на тот момент были
[01:06:29.620 --> 01:06:31.780]  слишком маленькие, чтобы построить достаточно
[01:06:31.780 --> 01:06:33.380]  большую сеть, чтобы все переобучилось.
[01:06:33.380 --> 01:06:35.380]  Построили большую сеть и всё переоб Laughing.
[01:06:35.380 --> 01:06:39.940]  Бабусинг не переобучается, переобучился. Потом опять неровные сети, в итоге пришли к выводу,
[01:06:39.940 --> 01:06:46.640]  что все переобучается, грузь, печаль, беда, обида, но при этом с этим можно бороться. Хорошо. Вопрос
[01:06:46.640 --> 01:06:54.260]  сейчас есть? Давайте как раз на примере еще вот простеньком, как это называется, регрессии
[01:06:54.260 --> 01:06:59.900]  разберемся. Вот что нам надо. Нам нужны данные, дифференцированная функция потерь. В чем, кстати,
[01:06:59.900 --> 01:07:04.220]  плюс? Вы можете сразу аналитически посчитать производную функции потерь по предсказаниям один
[01:07:04.220 --> 01:07:08.660]  раз и потом сразу эту формулу эксплуатировать. Вас никто не заставляет на каждом шаге градиента
[01:07:08.660 --> 01:07:12.300]  считать, вам не надо. У вас аналитическое значение градиента есть, если у вас функция
[01:07:12.300 --> 01:07:18.700]  дифференцируемая. Вот у нас какая-то семейство алгоритмов, число итераций и initial value. Давайте
[01:07:18.700 --> 01:07:23.660]  просто константой первой модель. У нас будет и все. Например, вот у нас будет вот такая зависимость,
[01:07:23.660 --> 01:07:29.780]  cos, plus epsilon, то есть каким-то шумом. Функция потерь будет MSE. Опять же, в случае MSE все слишком
[01:07:29.780 --> 01:07:34.420]  просто. Мы просто предсказываем ошибку предыдущих моделей. В случае с log loss это уже не работает.
[01:07:34.420 --> 01:07:40.220]  Будьте осторожны. У нас будут решающие деревни глубиной 2, уже не совсем пни. Три итерации в
[01:07:40.220 --> 01:07:47.460]  начале константа. Что мы видим? Слева опять же наш ансамбль полностью, справа наша текущая модель.
[01:07:47.460 --> 01:07:53.180]  На первом шаге у нас только константа и, соответственно, ее ошибки. Это уже ошибки модели,
[01:07:53.180 --> 01:08:00.140]  это наши эти градиенты. На втором шаге вот наша добавочная модель, которая на это была обучена,
[01:08:00.140 --> 01:08:05.060]  вот наш полный ансамбль. Потому что средняя была 0, соответственно, 0 плюс наша модель,
[01:08:05.060 --> 01:08:10.660]  получилось то же самое. Посмотрите на наши остатки на третьем шаге. Вот на что обучается
[01:08:10.660 --> 01:08:17.540]  вторая модель, ну третья, если с 0 считать. Это уже далеко не везде похоже на наш костюм. Вот общий
[01:08:17.540 --> 01:08:22.460]  ансамбль. Вот четвертый шаг. Вот на что похожи наши остатки, на которые учится четвертая модель.
[01:08:22.460 --> 01:08:28.020]  Вот наш полный ансамбль. То есть каждый раз у нас все ближе и ближе к какому-то шуму становится наш
[01:08:28.020 --> 01:08:34.420]  сигнал, таргет, на который пытается обучиться наша модель. Все. Стал понять, что происходит?
[01:08:34.420 --> 01:08:43.580]  Замечательно. Ну и как бы пара графиков. Вот вам пример при увеличении числа деревьев для
[01:08:43.580 --> 01:08:49.940]  бэддинга. Просто видим, что он выходит на переобучение в районе там 500 деревьев. Вот для рэндом
[01:08:49.940 --> 01:08:56.380]  пороста, синий, он гораздо лучше держится. Итоговая ошибка у него ниже, то что деревья менее
[01:08:56.380 --> 01:09:01.660]  скоррелированные, у нас больше эффект снижения вот этой самой дисперсии, но тем не менее он
[01:09:01.660 --> 01:09:06.180]  тоже уходит куда-то на насыщение. Вот наш градентный бустинг. Он выходит на насыщение только в районе
[01:09:06.180 --> 01:09:11.580]  тысячи деревьев, но заметь, итоговая ошибка еще ниже. Почему? Потому что все равно решающий лес,
[01:09:11.580 --> 01:09:17.940]  решающее дерево, оно может переобучиться. Когда мы с вами усредняем их между собой, мы просто-напросто
[01:09:17.940 --> 01:09:23.460]  избавляемся от ошибок остальных деревьев, но тем не менее мы итоговую сложность модели усреднением
[01:09:23.460 --> 01:09:27.580]  повысить не можем. Потому что у нас невзвешенные средние, они просто друг к другу пытаются,
[01:09:27.580 --> 01:09:31.580]  грубо говоря, скраховать везде. В градентном бустинге каждый следующий модель усиливает
[01:09:31.580 --> 01:09:37.580]  предыдущую, поэтому чем глубже в лес, чем больше моделей, тем больше у нас с того его будет качество
[01:09:37.580 --> 01:09:41.500]  по крайней мере на обучающей выборке. Понятное дело, на тестовый в какой-то момент у нас все переобучится и
[01:09:41.500 --> 01:09:46.300]  будет плохо. Но градентный бустинг замечательный, переобучается, будьте осторожны. Его можно
[01:09:46.300 --> 01:09:51.420]  переобучить в лед, и в чем проблема градентного бустинга? Если рано-форос вас кое-как страхует от
[01:09:51.420 --> 01:09:56.660]  переобучения в том плане, что переобучилось одно дерево, не беда, много деревьев, краски, проблемы
[01:09:56.660 --> 01:10:00.780]  с переобучением нивелируются, потому что они усредняют друг друга. С градентным бустингом,
[01:10:00.780 --> 01:10:07.140]  если одно дерево было обычно, ладно, одна модель была обучена, где-то в ходе, все что после нее это
[01:10:07.140 --> 01:10:12.300]  мусор. То, что у вас переобученная модель, повлияет на все оставшиеся граденты, соответственно,
[01:10:12.300 --> 01:10:16.820]  если у вас один элемент ансамбля переобучен, весь ансамбль с этого момента и далее это мусор,
[01:10:16.820 --> 01:10:21.860]  он бесполезен. Поэтому следить за градентным бустингом надо очень аккуратно и там смотреть на
[01:10:21.860 --> 01:10:30.540]  валидацию. А потому что у вас градент станет сильно меньше, у вас ошибок стало сильно меньше на
[01:10:30.540 --> 01:10:35.140]  обучающей выборке. То есть вы уже переподстроились под обучающий выборку, у вас граденты почти все
[01:10:35.140 --> 01:10:44.340]  слопнулись. А следующая модель пытается подстроиться под градент, которых нет. Вот как-то так.
[01:10:44.340 --> 01:10:49.700]  Ну и вот вам пример краски из начала. Вот вам опять две концентрические окружности, очень старые
[01:10:49.700 --> 01:10:56.380]  картинки надо перерисовать. Короче, снаружи красные точки, в центре синие точки. Ну, поверьте
[01:10:56.380 --> 01:11:00.940]  мне на слово, потом можете посмотреть записи и я перерисую, наверное. Синие точки в центре,
[01:11:00.940 --> 01:11:06.780]  красные по кругу. Линейная модель одна не способна отделить, правильно, то что линия разделима и выборка.
[01:11:06.780 --> 01:11:11.580]  Но мы можем вспомнить, что логистическая регрессия это ни разу не линейное отображение,
[01:11:11.580 --> 01:11:16.940]  это сегмойда от линейного отображения. Поэтому их линейная комбинация уже вполне себе может
[01:11:16.940 --> 01:11:22.540]  задавать что-то нелинейное. Вот вам градентный бустинг над логистическими регрессиями.
[01:11:22.540 --> 01:11:33.780]  Замечательно, ограничивает центр. Уловили? Каждая полоса соответствует какой-то логистической
[01:11:33.780 --> 01:11:38.620]  регрессии. То есть каждая из них дает линейную разделяющую поверхность, но все вместе они смогли
[01:11:38.620 --> 01:11:45.540]  отделить центр от всего остального. Да, в идеальном случае мы с вами можем три прямых провести и
[01:11:45.540 --> 01:11:50.900]  треугольником отделить все. Но так, у нас там шум присутствует и так далее, вот за 40 шагов он
[01:11:50.900 --> 01:11:55.340]  хорошо отделился. Хотя мы видим с вами, что за первые там 10, на самом деле он уже примерно
[01:11:55.340 --> 01:12:07.740]  перестал ошибку ронять. Вот, ну что, градентный бустинг вас вызвал страх и ненависть в БХИМе.
[01:12:07.740 --> 01:12:27.300]  Вот здесь. Ну, смотрите, это тестовая ошибка, поэтому то, что она себя ведет таким образом,
[01:12:27.300 --> 01:12:33.620]  абсолютно нормально, потому что обучаемся мы на трейне, а на тесте валидируем. То есть на трейне
[01:12:33.660 --> 01:12:40.220]  она, как правило, падает достаточно равномерно, если считать по всей выборке. Если считать по бачам,
[01:12:40.220 --> 01:12:43.740]  то она себя будет вести вот так, потому что у вас от бача зависит хорошо или плохо, вы там на этом
[01:12:43.740 --> 01:12:50.380]  баче конкретно отработали. Вот, смотрите, тут она, например, сглаженная, во времени, видите,
[01:12:50.380 --> 01:12:56.380]  она в среднем-то падает, достаточно стабильно. Тут сглаживание применено, чтобы мы видели,
[01:12:56.380 --> 01:13:02.020]  что вот эти вот всплески, они на самом деле в среднем, все равно идут вниз. Ну и коротенькое,
[01:13:02.020 --> 01:13:07.300]  наверное, утро. Собственно, рано-пороста у вас работает строго параллельно на уровне деревьев,
[01:13:07.300 --> 01:13:12.580]  поэтому вы его можете хорошо парализовать. И если у вас словно есть 40 потоков, 40 деревьев
[01:13:12.580 --> 01:13:18.500]  можете одновременно построить, будет быстро. Применять тоже можете быстро. Гридентный бусинг,
[01:13:18.500 --> 01:13:23.700]  он не парализуется в лоб, но при этом стоит помнить, что дерево можно тоже строить достаточно
[01:13:23.700 --> 01:13:28.940]  эффективно. Если вы строите, например, гридентный бусинг над деревьями, каждый дерево может строиться,
[01:13:28.940 --> 01:13:33.500]  грубо говоря, на два под дерево поделили, они могут параллельно строиться. Поэтому не стоит думать,
[01:13:33.500 --> 01:13:36.860]  что гридентный бусинг уж совсем медленная штуковина. В современном мире он, конечно,
[01:13:36.860 --> 01:13:42.140]  медленнее, чем рано-пороста, но при этом он тоже достаточно быстро строится и, как правило,
[01:13:42.140 --> 01:13:49.020]  сильно быстрее всяких нейронок. Окей, сложных нейронок. Ну что, тут какие-то вопросы есть?
[01:13:49.020 --> 01:14:01.420]  Ещё раз, я именно и говорил, что если он переобучится, то всё плохо. Именно поэтому в
[01:14:01.420 --> 01:14:05.500]  среднем исторически глубокие деревья не используются, потому что неглубокое дерево
[01:14:05.500 --> 01:14:10.380]  переобучить сложно. Глубокое дерево переобучить легко. Опять же, это в среднем, это не то,
[01:14:10.380 --> 01:14:15.460]  что только так и никак иначе. Скорее так, если вы не понимаете, что делать, лучше использовать
[01:14:15.460 --> 01:14:20.140]  неглубокие деревья с гридентным бусингом и глубоким и с рано-поростым. Если вы понимаете,
[01:14:20.140 --> 01:14:25.260]  что вы делаете, плак вам в руки, барабан на шею. Делать можете что угодно. Просто пока у вас нет
[01:14:25.260 --> 01:14:29.620]  понимания, как бы как в данной задаче отработать тот или иной алгоритм, например, сильно у вас там
[01:14:29.620 --> 01:14:35.140]  шум присутствует или слабый, лучше придерживаться каких-то общепринятых норм. Вот как-то так.
[01:14:35.140 --> 01:14:41.180]  Короче, моя главная цель была на самом деле, чтобы сегодня вы поняли простую вещь, что гридентный
[01:14:41.180 --> 01:14:48.820]  бусинг строит именно на каждом шаге аппроксимацию антигридиента вашей функции потерь по предсказаниям.
[01:14:48.820 --> 01:14:53.860]  По сути, в каждой точке пространства, где есть обучающие значения из пары из обучающей выборки,
[01:14:53.860 --> 01:14:59.140]  вы строите ваш антигридиент и его аппроксимируете. Вот это коровая вещь. Все остальное это всякие
[01:14:59.140 --> 01:15:10.300]  там свистопляски и так далее. Это осознали? Хорошо. Ну, смотрите, у меня на самом деле еще
[01:15:10.300 --> 01:15:15.020]  про тейкинг и блендинг, дополнительные слайдики. Я, наверное, предлагаю что сделать. Продолжаю даже
[01:15:15.020 --> 01:15:19.420]  прямо сейчас сделать перерыв, потому что не самая легкая была штуковина. Где-то 18-40
[01:15:19.420 --> 01:15:24.220]  мы продолжим тогда, то есть 15 минут перерыва. И там мы поговорим сначала на практике про деревья
[01:15:24.220 --> 01:15:29.140]  и про бустинг, а потом я вам еще про то, как стакать это все дело расскажу. Все, лекция over.
